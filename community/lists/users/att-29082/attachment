Dave,<div><br></div><div>yes, this is for two MPI tasks only.</div><div><br></div><div>the MPI subroutine could/should return with an error if the communicator is made of more than 3 tasks.</div><div>an other option would be to abort at initialization time if no collective modules provide a barrier implementation.</div><div>or maybe the tuned module should have not used the two_procs algorithm, but what should it do instead ? use a default one ? do not implement barrier ? warn/error the end user ?</div><div><br></div><div>note the error message might be a bit obscure.</div><div><br></div><div>I write &quot;could&quot; because you explicitly forced something that cannot work, and I am not convinced OpenMPI should protect end users from themselves, even when they make an honest mistake.<br><br>George, any thoughts ?</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles</div><div><br>On Wednesday, May 4, 2016, Dave Love &lt;<a href="mailto:d.love@liverpool.ac.uk">d.love@liverpool.ac.uk</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">With OMPI 1.10.2 and earlier on Infiniband, IMB generally spins with no<br>
output for the barrier benchmark if you run it with algorithm 5, i.e.<br>
<br>
  mpirun --mca coll_tuned_use_dynamic_rules 1 --mca coll_tuned_barrier_algorithm 5 IMB-MPI1 barrier<br>
<br>
This is &quot;two proc only&quot;.  Does that mean it will only work for two<br>
processes (which seems true experimentally)?  If so, should it report an<br>
error if used with more?<br>
_______________________________________________<br>
users mailing list<br>
<a href="javascript:;" onclick="_e(event, &#39;cvml&#39;, &#39;users@open-mpi.org&#39;)">users@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/05/29081.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/05/29081.php</a><br>
</blockquote></div>

