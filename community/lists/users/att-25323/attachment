<html><head><meta http-equiv="Content-Type" content="text/html charset=iso-8859-1"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">Hmmm...well, the info is there. There is an envar&nbsp;OMPI_COMM_WORLD_LOCAL_SIZE which tells you how many procs are on this node. If you tell your proc how many cores (or hwthreads) to use, it would be a simple division to get what you want.<div><br></div><div>You could also detect the number of cores or hwthreads via a call to hwloc, but I don't know if you want to link that deep and MPI doesn't have a function for that purpose. Could be that OpenMP provides a call for that purpose?</div><div><br></div><div><br><div><div>On Sep 12, 2014, at 7:22 AM, JR Cary &lt;<a href="mailto:cary@txcorp.com">cary@txcorp.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite">
  
    <meta content="text/html; charset=ISO-8859-1" http-equiv="Content-Type">
  
  <div bgcolor="#FFFFFF" text="#000000">
    <font face="Courier New, Courier, monospace"><font face="Courier
        New, Courier, monospace"><br>
      </font><br>
    </font>
    <div class="moz-cite-prefix">On 9/12/14, 7:27 AM, Tim Prince wrote:<br>
    </div>
    <blockquote cite="mid:5412F4B1.2060709@aol.com" type="cite">
      <meta content="text/html; charset=ISO-8859-1" http-equiv="Content-Type">
      <br>
      <div class="moz-cite-prefix">On 9/12/2014 6:14 AM, JR Cary wrote:<br>
      </div>
      <blockquote cite="mid:5412D578.1040703@txcorp.com" type="cite">
        <meta http-equiv="content-type" content="text/html;
          charset=ISO-8859-1">
        <font face="Courier New, Courier, monospace">This must be a very
          old topic.<br>
          <br>
          I would like to run mpi with one process per node, e.g., using<br>
          -cpus-per-rank=1.&nbsp; Then I want to use openmp inside of that.<br>
          But other times I will run with a rank on each physical core.<br>
          <br>
          Inside my code I would like to detect which situation I am in.<br>
          Is there an openmpi api call to determine that?<br>
        </font><br>
      </blockquote>
      <font face="Courier New, Courier, monospace">omp_get_num_threads()
        should work.&nbsp; Unless you want to choose a different non-parallel
        algorithm for this case, a single thread omp parallel region
        works fine.<br>
        You should soon encounter cases where you want intermediate
        choices, such as 1 rank per CPU package and 1 thread per core,
        even if you stay away from platforms with more than 12 cores per
        CPU.</font><br>
    </blockquote>
    <br>
    <font face="Courier New, Courier, monospace">I may not understand,
      so I will try to ask in more detail.<br>
      <br>
      Suppose I am running on a four-core processor (and my code likes
      one thread per core).<br>
      <br>
      In case 1 I do<br>
      <br>
      &nbsp; mpiexec -np 2 myexec<br>
      <br>
      and I want to know that each mpi process should use 2 threads.<br>
      <br>
      If instead I did</font><br>
    <font face="Courier New, Courier, monospace"><br>
        &nbsp; mpiexec -np 4 myexec<br>
        <br>
        I want to know that each mpi process should use one thread.<br>
        <br>
        Will omp_get_num_threads() should return a different value for
        those two cases?<br>
        <br>
        Perhaps I am not invoking mpiexec correctly. <br>
        I use MPI_Init_thread(&amp;argc, &amp;argv, MPI_THREAD_FUNNELED,
        &amp;threadSupport), and regardless<br>
        of what how I invoke mpiexec (-n 1, -n 2, -n 4), I see 2 openmp
        processes <br>
        and 1 openmp threads (have not called omp_set_num_threads).<br>
        When I run serial, I see 8 openmp processes and 1 openmp
        threads.<br>
        So I must be missing an arg to mpiexec?<br>
        <br>
        This is a 4-core haswell with hyperthreading to get 8.<br>
        <br>
        &nbsp; <br>
        <br>
        Thx.....</font><br>
  </div>

_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post: http://www.open-mpi.org/community/lists/users/2014/09/25322.php</blockquote></div><br></div></body></html>
