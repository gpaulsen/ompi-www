<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">Hi Vitorio,<div><br><div><div>On 11-Jul-09, at 8:40 PM, Luis Vitorio Cargnini wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">did you saw that, maybe, just maybe using:<div>xserve01.local slots=8 max-slots=8</div><div>xserve02.local slots=8 max-slots=8</div><div>xserve03.local slots=8 max-slots=8</div><div>xserve04.local slots=8 max-slots=8</div><div><br></div><div>it can set the&nbsp;number&nbsp;of&nbsp;process&nbsp;specifically&nbsp;for&nbsp;each&nbsp;node,&nbsp;the&nbsp;"slots"&nbsp;does&nbsp;this&nbsp;setting&nbsp;the&nbsp;configuration&nbsp;of&nbsp;slots&nbsp;per&nbsp;each&nbsp;node,&nbsp;try&nbsp;it&nbsp;with&nbsp;the&nbsp;old&nbsp;conf&nbsp;of&nbsp;Xgrid&nbsp;and&nbsp;also&nbsp;test&nbsp;with&nbsp;your&nbsp;new&nbsp;Xgrid&nbsp;conf.</div></div></blockquote><div><br></div><div>As per Ralph's message, the xgrid launcher ignores --hostfiles... &nbsp;Further, "max_slots=2" is the same as "slots=2 max_slots=2" according to the man page. &nbsp;</div><div><br></div><div>Xgrid does have a somewhat convoluted, and poorly documented, method of directing jobs to specified machines. &nbsp;Its called Scoreboard and it allows the scheduler to query each machine with a script that gathers info about the machine and compute a "score". &nbsp;Nodes with the highest score get the job. &nbsp;However, how one would implement that using openMPI is unclear to me. &nbsp;Does openMPI have the capability of passing arbitrary arguments to the resource managers? &nbsp;</div><div><br></div><div>Thanks, &nbsp;Jody</div><br><blockquote type="cite"><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><div><br></div><div>Regards.</div><div>Vitorio.</div><div><br></div><div><br><div><div><div>Le 09-07-11 à 18:11, Klymak Jody a écrit :</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">If anyone else is using&nbsp;xgrid,&nbsp;there&nbsp;is&nbsp;a&nbsp;mechanism&nbsp;to&nbsp;limit&nbsp;the&nbsp;processes&nbsp;per&nbsp;machine:<div><br></div><div><span class="Apple-style-span" style="font-family: monospace, 'BitStream vera Sans', Helvetica, sans-serif; color: rgb(85, 85, 85); font-size: 12px; line-height: 17px; ">sudo defaults write /Library/Preferences/com.apple.xgrid.agent MaximumTaskCount 8</span></div><div><font class="Apple-style-span" color="#555555" face="monospace, 'BitStream vera Sans', Helvetica, sans-serif"><span class="Apple-style-span" style="line-height: 17px;"><br></span></font></div><div style="font-size: 14px; "><font class="Apple-style-span" face="Helvetica, 'BitStream vera Sans', Helvetica, sans-serif"><span class="Apple-style-span" style="font-size: medium;">on each of the nodes and then restarting xgrid tells the controller to only send 8 processes to that node. &nbsp;For now that is fine solution for my need. &nbsp;I'll try and figure out how to specify hosts via xgrid and get back to the list...</span></font></div><div style="font-size: 14px; "><font class="Apple-style-span" face="Helvetica, 'BitStream vera Sans', Helvetica, sans-serif"><span class="Apple-style-span" style="font-size: medium;"><br></span></font></div><div style="font-size: 14px; "><font class="Apple-style-span" face="Helvetica, 'BitStream vera Sans', Helvetica, sans-serif"><span class="Apple-style-span" style="font-size: medium;">Thanks for everyone's help, &nbsp;</span></font></div><div style="font-size: 14px; "><font class="Apple-style-span" face="Helvetica, 'BitStream vera Sans', Helvetica, sans-serif"><span class="Apple-style-span" style="font-size: medium;"><br></span></font></div><div style="font-size: 14px; "><font class="Apple-style-span" face="Helvetica, 'BitStream vera Sans', Helvetica, sans-serif"><span class="Apple-style-span" style="font-size: medium;">Cheers, Jody</span></font></div><div><div><br><div><div>On 11-Jul-09, at 12:42 PM, Ralph Castain wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div>Looking at the code, you are correct in that the Xgrid launcher is ignoring hostfiles. I'll have to look at it to determine how to correct that situation - I didn't write that code, nor do I have a way to test any changes I might make to it.<br><br>For now, though, if you add --bynode to your command line, you should get the layout you want. I'm not sure you'll get the rank layout you'll want, though...or if that is important to what you are doing.<br><br>Ralph<br><br>On Jul 11, 2009, at 1:18 PM, Klymak Jody wrote:<br><br><blockquote type="cite">Hi Vitorio,<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">Thanks for getting back to me! &nbsp;My hostfile is<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">xserve01.local max-slots=8<br></blockquote><blockquote type="cite">xserve02.local max-slots=8<br></blockquote><blockquote type="cite">xserve03.local max-slots=8<br></blockquote><blockquote type="cite">xserve04.local max-slots=8<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">I've now checked, and this seems to work fine just using ssh. &nbsp;i.e. if I turn off the Xgrid queue manager I can submit jobs manually to the appropriate nodes using --hosts.<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">However, I'd really like to use Xgrid as my queue manager as it is already set up (though I'll happily take hints on how to set up other queue managers on an OS X cluster).<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite"><blockquote type="cite">So you have 4 nodes each one with 2 processors, each processor 4-core - quad-core.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">So you have capacity for 32 process in parallel.<br></blockquote></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">The new Xeon chips designate 2-processes per core, though at a reduced clock rate. &nbsp;This means that Xgrid believes I have 16 processors/node. &nbsp;For large jobs I expect that to be useful, but for my more modest jobs I really only want 8 processes/node.<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">It appears that the default way xgrid assigns the jobs is to fill all 16 slots on one node before moving to the next. &nbsp;OpenMPI doesn't appear to look at the hostfile configuration when using Xgrid, so it makes it hard for me to deprecate this behaviour.<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">Thanks, &nbsp;Jody<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite"><blockquote type="cite">I think that only using the hostfile is enough is how I use. If you to specify a specific host or a different sequence, the mpirun will obey the host sequence in your hostfile to start the process, also can you put how you configured your host files ? I'm asking this because you should have something like:<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"># This is an example hostfile. Comments begin with<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"># #<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"># The following node is a single processor machine:<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">foo.example.com<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"># The following node is a dual-processor machine:<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">bar.example.com slots=2<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"># The following node is a quad-processor machine, and we absolutely<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"># want to disallow over-subscribing it:<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">yow.example.com slots=4 max-slots=4<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">so in your case like mine you should have something like:<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">your.hostname.domain slots=8 max-slots=8 # for each node<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">I hope this will help you.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Regards.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Vitorio.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Le 09-07-11 à 10:56, Klymak Jody a écrit :<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">Hi all,<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">Sorry in advance if these are naive questions - I'm not experienced in running a grid...<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">I'm using openMPI on 4 &nbsp;duo Quad-core Xeon xserves. &nbsp;The 8 cores mimic 16 cores and show up in xgrid as each agent having 16 processors. &nbsp;However, the processing speed goes down as the used processors exceeds 8, so if possible I'd prefer to not have more than 8 processors working on each machine at a time.<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">Unfortunately, if I submit a 16-processor job to xgrid it all goes to "xserve03". &nbsp;Or even worse, it does so if I submit two separate 8-processor jobs. &nbsp;Is there anyway to steer jobs to less-busy agents?<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">I tried making a hostfile and then specifying the host, but I get:<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">/usr/local/openmpi/bin/mpirun -n 8 --hostfile hostfile --host xserve01.local ../build/mitgcmuv<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">Some of the requested hosts are not included in the current allocation for the<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">application:<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">../build/mitgcmuv<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">The requested hosts were:<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">xserve01.local<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">so I assume --host doesn't work with xgrid?<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">Is a reasonable alternative to simply not use xgrid and rely on ssh?<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">Thanks, &nbsp;Jody<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">--<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">Jody Klymak<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><a href="http://web.uvic.ca/~jklymak">http://web.uvic.ca/~jklymak</a><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">_______________________________________________<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">users mailing list<br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">_______________________________________________<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">users mailing list<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">_______________________________________________<br></blockquote><blockquote type="cite">users mailing list<br></blockquote><blockquote type="cite"><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br></blockquote><blockquote type="cite"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote><br><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br></div></blockquote></div><br></div></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote></div><br></div></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
