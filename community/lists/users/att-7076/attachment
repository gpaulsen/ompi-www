<div dir="ltr"><div>Hi,</div>
<div>&nbsp;</div>
<div>&nbsp;</div>
<div>If I understand you correctly the most suitable way to do it is by paffinity that we have in Open MPI 1.3 and the trank.</div>
<div>how ever usually OS is distributing processes evenly between sockets by it self.</div>
<div>&nbsp;</div>
<div>There still no formal FAQ due to a multiple reasons but you can read how to use it in the attached scratch ( there were few name changings of the params, so check with ompi_info )</div>
<div>&nbsp;</div>
<div>shared memory is used between processes that share same machine, and openib is used between different machines ( hostnames ), no special mca params are needed.</div>
<div>&nbsp;</div>
<div>Best Regards</div>
<div>Lenny,</div>
<p><br><br>&nbsp;</p>
<div class="gmail_quote">On Sun, Oct 19, 2008 at 10:32 AM, Gilbert Grosdidier <span dir="ltr">&lt;<a href="mailto:grodid@mail.cern.ch">grodid@mail.cern.ch</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="PADDING-LEFT: 1ex; MARGIN: 0px 0px 0px 0.8ex; BORDER-LEFT: #ccc 1px solid">&nbsp;Working with a CellBlade cluster (QS22), the requirement is to have one<br>instance of the executable running on each socket of the blade (there are 2<br>
sockets). The application is of the &#39;domain decomposition&#39; type, and each<br>instance is required to often send/receive data with both the remote blades and<br>the neighbor socket.<br><br>&nbsp;Question is : which specification must be used for the mca btl component<br>
to force 1) shmem type messages when communicating with this neighbor socket,<br>while 2) using openib to communicate with the remote blades ?<br>Is &#39;-mca btl sm,openib,self&#39; suitable for this ?<br><br>&nbsp;Also, which debug flags could be used to crosscheck that the messages are<br>
_actually_ going thru the right channel for a given channel, please ?<br><br>&nbsp;We are currently using OpenMPI 1.2.5 shipped with RHEL5.2 (ppc64).<br>Which version do you think is currently the most optimised for these<br>processors and problem type ? Should we go towards OpenMPI 1.2.8 instead ?<br>
Or even try some OpenMPI 1.3 nightly build ?<br><br>&nbsp;Thanks in advance for your help, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Gilbert.<br><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>

