<div>In that case, I have a small question concerning design:</div>
<div> </div>
<div>Suppose task-based parallellism where one node (master) distributes work/tasks to 2 other nodes (slaves) by means of an MPI_Put. The master allocates 2 buffers locally in which it will store all necessary data that is needed by the slave to perform the task. So I do an MPI_Put on each of my 2 buffers to send each buffer to a specific slave. Now I need to know when I can reuse one of my buffers to already store the next task (that I will MPI_Put later on). The only way to know this is call MPI_Complete. But since this is blocking and if this buffer is not ready to be reused yet, I can neither verify if the other buffer is already available to me again (in the same thread).</div>

<div> </div>
<div>I would very much appreciate input on how to solve such issue !</div>
<div> </div>
<div>thanks in advance,</div>
<div> </div>
<div>toon</div>
<div> </div>
<div class="gmail_quote">On Tue, Feb 22, 2011 at 7:21 PM, Barrett, Brian W <span dir="ltr">&lt;<a href="mailto:bwbarre@sandia.gov">bwbarre@sandia.gov</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="PADDING-LEFT: 1ex; MARGIN: 0px 0px 0px 0.8ex; BORDER-LEFT: #ccc 1px solid">
<div>
<div></div>
<div class="h5">On Feb 18, 2011, at 8:59 AM, Toon Knapen wrote:<br><br>&gt; (Probably this issue has been discussed at length before but unfortunately I did not find any threads (on this site or anywhere else) on this topic, if you are able to provide me with links to earlier discussions on this topic, please do not hesitate)<br>
&gt;<br>&gt; Is there an alternative to MPI_Win_complete that does not &#39;enforce completion of preceding RMS calls at the origin&#39; (as said on pag 353 of the mpi-2.2 standard) ?<br>&gt;<br>&gt; I would like to know if I can reuse the buffer I gave to MPI_Put but without blocking on it, if the MPI lib is still using it, I want to be able to continue (and use another buffer).<br>
<br><br></div></div>There is not.   MPI_Win_complete is the only way to finish a MPI_Win_start epoch, and is always blocking until local completion of all messages started during the epoch.<br><br>Brian<br><br>--<br> Brian W. Barrett<br>
 Dept. 1423: Scalable System Software<br> Sandia National Laboratories<br><br><br><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

