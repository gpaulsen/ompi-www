<br>Hi<br><br><span style="color: rgb(255, 0, 0);">Are you running on two processes (mpiexec -n 2)?</span><br>Yes<br><br><span style="color: rgb(255, 0, 0);">Have you tried to print Gsize?</span><br>Yes, I had checked my codes several times, and I thought the errors came from the OpenMpi. :)<br>
<br><span style="color: rgb(255, 0, 0);">The command line I used:</span> <br>&quot;mpirun -hostfile ./Serverlist -np 2 ./test&quot;. The &quot;Serverlist&quot; file include several computers in my network.<br><br><span style="color: rgb(255, 0, 0);">The command line that I used to build the openmpi-1.4.1:</span><br>
./configure --enable-debug --prefix=/usr/work/openmpi ; make all install;<br><br><span style="color: rgb(255, 0, 0);">What interconnect do you use?</span><br>It is normal TCP/IP interconnect with 1GB network card. when I debugged my codes(and the openmpi codes), I found the openMpi do call the &quot;mca_pml_ob1_send_request_start_rdma(...)&quot; function, but I was not quite sure which protocal was used when transfer 2BG data. Do you have any opinions? Thanks<br>
<br>Best Regards<br>Xianjun Meng<br><br><div class="gmail_quote">2010/12/7 Gus Correa <span dir="ltr">&lt;<a href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt;</span><br><blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">
Hi Xianjun<br>
<br>
Are you running on two processes (mpiexec -n 2)?<br>
I think this code will deadlock for more than two processes.<br>
The MPI_Recv won&#39;t have a matching send for rank&gt;1.<br>
<br>
Also, this is C, not MPI,<br>
but you may be wrapping into the negative numbers.<br>
Have you tried to print Gsize?<br>
It is probably -2147483648 in 32bit and 64bit machines.<br>
<br>
My two cents.<br>
Gus Correa<br>
<br>
Mike Dubman wrote:<br>
<blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;"><div class="im">
Hi,<br>
What interconnect and command line do you use? For InfiniBand openib component there is a known issue with large transfers (2GB)<br>
<br>
<a href="https://svn.open-mpi.org/trac/ompi/ticket/2623" target="_blank">https://svn.open-mpi.org/trac/ompi/ticket/2623</a><br>
<br>
try disabling memory pinning: <a href="http://www.open-mpi.org/faq/?category=openfabrics#large-message-leave-pinned" target="_blank">http://www.open-mpi.org/faq/?category=openfabrics#large-message-leave-pinned</a><br>
<br>
<br>
regards<br>
M<br>
<br>
<br></div>
2010/12/6 &lt;<a href="mailto:xjun.meng@gmail.com" target="_blank">xjun.meng@gmail.com</a> &lt;mailto:<a href="mailto:xjun.meng@gmail.com" target="_blank">xjun.meng@gmail.com</a>&gt;&gt;<div class="im"><br>
<br>
    hi,<br>
<br>
    In my computers(X86-64), the sizeof(int)=4, but the<br>
    sizeof(long)=sizeof(double)=sizeof(size_t)=8. when I checked my<br>
    mpi.h file, I found that the definition about the sizeof(int) is<br>
    correct. meanwhile, I think the mpi.h file was generated according<br>
    to my compute environment when I compiled the Openmpi. So, my codes<br>
    still don&#39;t work. :(<br>
<br>
    Further, I found when I called the collective routines(such as,<br>
    MPI_Allgatherv(...)) which are implemented by the Point 2 Point<br>
    don&#39;t work either when the data &gt; 2GB.<br>
<br>
    Thanks<br>
    Xianjun<br>
<br></div>
    2010/12/6 Tim Prince &lt;<a href="mailto:n8tm@aol.com" target="_blank">n8tm@aol.com</a> &lt;mailto:<a href="mailto:n8tm@aol.com" target="_blank">n8tm@aol.com</a>&gt;&gt;<div><div></div><div class="h5"><br>
<br>
        On 12/5/2010 7:13 PM, Xianjun wrote:<br>
<br>
            hi,<br>
<br>
            I met a question recently when I tested the MPI_send and<br>
            MPI_Recv<br>
            functions. When I run the following codes, the processes<br>
            hanged and I<br>
            found there was not data transmission in my network at all.<br>
<br>
            BTW: I finished this test on two X86-64 computers with 16GB<br>
            memory and<br>
            installed Linux.<br>
<br>
            1 #include &lt;stdio.h&gt;<br>
            2 #include &lt;mpi.h&gt;<br>
            3 #include &lt;stdlib.h&gt;<br>
            4 #include &lt;unistd.h&gt;<br>
            5<br>
            6<br>
            7 int main(int argc, char** argv)<br>
            8 {<br>
            9 int localID;<br>
            10 int numOfPros;<br>
            11 size_t Gsize = (size_t)2 * 1024 * 1024 * 1024;<br>
            12<br>
            13 char* g = (char*)malloc(Gsize);<br>
            14<br>
            15 MPI_Init(&amp;argc, &amp;argv);<br>
            16 MPI_Comm_size(MPI_COMM_WORLD, &amp;numOfPros);<br>
            17 MPI_Comm_rank(MPI_COMM_WORLD, &amp;localID);<br>
            18<br>
            19 MPI_Datatype MPI_Type_lkchar;<br>
            20 MPI_Type_contiguous(2048, MPI_BYTE, &amp;MPI_Type_lkchar);<br>
            21 MPI_Type_commit(&amp;MPI_Type_lkchar);<br>
            22<br>
            23 if (localID == 0)<br>
            24 {<br>
            25 MPI_Send(g, 1024*1024, MPI_Type_lkchar, 1, 1,<br>
            MPI_COMM_WORLD);<br>
            26 }<br>
            27<br>
            28 if (localID != 0)<br>
            29 {<br>
            30 MPI_Status status;<br>
            31 MPI_Recv(g, 1024*1024, MPI_Type_lkchar, 0, 1, \<br>
            32 MPI_COMM_WORLD, &amp;status);<br>
            33 }<br>
            34<br>
            35 MPI_Finalize();<br>
            36<br>
            37 return 0;<br>
            38 }<br>
<br>
        You supplied all your constants as 32-bit signed data, so, even<br>
        if the count for MPI_Send() and MPI_Recv() were a larger data<br>
        type, you would see this limit. Did you look at your &lt;mpi.h&gt; ?<br>
<br>
        --         Tim Prince<br>
<br>
        _______________________________________________<br>
        users mailing list<br></div></div>
        <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<div class="im"><br>
        <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
    _______________________________________________<br>
    users mailing list<br></div>
    <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<div class="im"><br>
    <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
------------------------------------------------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></blockquote><div><div></div><div class="h5">
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

