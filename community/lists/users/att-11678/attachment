<div dir="ltr"><div>have you tried IMB benchmark with Bcast,<br></div><div>I think the problem is in the app.</div><div>All ranks in the communicator should enter Bcast,</div><div>since you have</div><div>if (rank==0) </div>
<div></div><div>else state, not all of them enters the same flow.</div><div>  if (iRank == 0)<br> {<br>   iLength = sizeof (acMessage);<br>   MPI_Bcast (&amp;iLength, 1, MPI_INT, 0, MPI_COMM_WORLD);<br>   MPI_Bcast (acMessage, iLength, MPI_CHAR, 0, MPI_COMM_WORLD);<br>
   printf (&quot;Process 0: Message sent\n&quot;);<br> }<br>      else<br> {<br>   MPI_Bcast (&amp;iLength, 1, MPI_INT, 0, MPI_COMM_WORLD);<br>   pMessage = (char *) malloc (iLength);<br>   MPI_Bcast (pMessage, iLength, MPI_CHAR, 0, MPI_COMM_WORLD);<br>
   printf (&quot;Process %d: %s\n&quot;, iRank, pMessage);<br> }<br><br></div><div></div><div>Lenny.</div><br><div class="gmail_quote">On Mon, Jan 4, 2010 at 8:23 AM, Eugene Loh <span dir="ltr">&lt;<a href="mailto:Eugene.Loh@sun.com">Eugene.Loh@sun.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">


  
  

<div bgcolor="#ffffff" text="#000000">
If you&#39;re willing to try some stuff:<br>
<br>
1) What about &quot;-mca coll_sync_barrier_before 100&quot;?  (The default may be
1000.  So, you can try various values less than 1000.  I&#39;m suggesting
100.)  Note that broadcast has somewhat one-way traffic flow, which can
have some undesirable flow control issues.<br>
<br>
2) What about &quot;-mca btl_sm_num_fifos 16&quot;?  Default is 1.  If the
problem is trac ticket 2043, then this suggestion can help.<br>
<br>
P.S.  There&#39;s a memory leak, right?  The receive buffer is being
allocated over and over again.  Might not be that closely related to
the problem you see here, but at a minimum it&#39;s bad style.<br>
<br>
Louis Rossi wrote:
<blockquote cite="http://mid4B4184F0.3090205@math.udel.edu" type="cite"><div class="im">I am
having a problem with BCast hanging on a dual quad core Opteron (2382,
2.6GHz, Quad Core, 4 x 512KB L2, 6MB L3 Cache) system running FC11 with
openmpi-1.4.  The LD_LIBRARY_PATH and PATH variables are correctly
set.  I have used the FC11 rpm distribution of openmpi and built
openmpi-1.4 locally with the same results.  The problem was first
observed in a larger reliable CFD code, but I can create the problem
with a simple demo code (attached).  The code attempts to execute 2000
pairs of broadcasts.
  <br>
  <br>
The hostfile contains a single line
  <br>
&lt;machinename&gt; slots=8
  <br>
  <br>
If I run it with 4 cores or fewer, the code will run fine.
  <br>
  <br>
If I run it with 5 cores or more, it will hang some of the time after
successfully executing several hundred broadcasts.  The number varies
from run to run.  The code usually finishes with 5 cores.  The
probability of hanging seems to increase with the number of nodes.  The
syntax I use is simple.
  <br>
  <br>
mpiexec -machinefile hostfile -np 5 bcast_example
  <br>
  <br>
There was some discussion of a similar problem on the user list, but I
could not find a resolution.  I have tried setting the processor
affinity (--mca mpi_paffinity_alone 1).  I have tried varying the
broadcast algorithm (--mca coll_tuned_bcast_algorithm 1-6).  I have
also tried excluding (-mca oob_tcp_if_exclude) my eth1 interface (see
ifconfig.txt attached) which is not connected to anything.  None of
these changed the outcome.
  <br>
  <br>
Any thoughts or suggestions would be appreciated.
  <br>
  <br>
  </div><pre><hr size="4" width="90%"><div class="im">
_______________________________________________
users mailing list
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
  </div></pre></blockquote>
<br>
</div>

<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>

