<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Oct 26, 2014, at 9:56 PM, Brock Palen &lt;<a href="mailto:brockp@umich.edu" class="">brockp@umich.edu</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class="">We are starting to look at supporting MPI on our Hadoop/Spark YARN based cluster.</div></blockquote><div><br class=""></div>You poor soul…</div><div><br class=""><blockquote type="cite" class=""><div class=""> &nbsp;I found a bunch of referneces to Hamster, but what I don't find is if it was ever merged into regular OpenMPI, and if so is it just another RM integration? &nbsp;Or does it need more setup?<br class=""></div></blockquote><div><br class=""></div>When I left Pivotal, it was based on a copy of the OMPI trunk that sat somewhere in the 1.7 series, I believe. Last contact I had indicated they were trying to update, but I’m not sure they were successful.</div><div><br class=""><blockquote type="cite" class=""><div class=""><br class="">I found this:<br class=""><a href="http://pivotalhd.docs.pivotal.io/doc/2100/Hamster.html" class="">http://pivotalhd.docs.pivotal.io/doc/2100/Hamster.html</a><br class=""></div></blockquote><div><br class=""></div>Didn’t know they had actually (finally) released it, so good to know. Just so you are aware, there are major problems running MPI under Yarn as it just isn’t designed for MPI support. What we did back then was add a JNI layer so that ORTE could run underneath it, and then added a PMI-like service to provide the wireup support (since Yarn couldn’t be used to exchange the info itself). You also have the issue that Yarn doesn’t understand the need for all the procs to be launched together, and so you have to modify Yarn so it will ensure that the MPI procs are all running or else you’ll hang in MPI_Init.</div><div><br class=""><blockquote type="cite" class=""><div class=""><br class="">Which appears to imply extra setup required. &nbsp;Is this documented anywhere for OpenMPI?<br class=""></div></blockquote><div><br class=""></div>I’m afraid you’ll just have to stick with the Pivotal-provided version as the integration is rather complicated. Don’t expect much in the way of performance! This was purely intended as a way for “casual” MPI users to make use of “free” time on their Hadoop cluster, not for any serious technical programming.</div><div><br class=""><blockquote type="cite" class=""><div class=""><br class="">Brock Palen<br class=""><a href="http://www.umich.edu/~brockp" class="">www.umich.edu/~brockp</a><br class="">CAEN Advanced Computing<br class="">XSEDE Campus Champion<br class="">brockp@umich.edu<br class="">(734)936-1985<br class=""><br class=""><br class=""><br class="">_______________________________________________<br class="">users mailing list<br class="">users@open-mpi.org<br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2014/10/25593.php<br class=""></div></blockquote></div><br class=""></body></html>
