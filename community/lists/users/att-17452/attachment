<html><body><div style="color:#000; background-color:#fff; font-family:Courier New, courier, monaco, monospace, sans-serif;font-size:10pt"><div><span>Hello All,</span></div><div><br><span></span></div><div><span>I have just rebuilt openmpi-1.4-3 on our cluster, and I see this error:</span></div><div><br><span></span></div><div><span>It looks like MPI_INIT failed for some reason; your parallel process is<br>likely to abort.&nbsp; There are many reasons that a parallel process can<br>fail during MPI_INIT; some of which are due to configuration or environment<br>problems.&nbsp; This failure appears to be an internal failure; here's some<br>additional information (which may only be relevant to an Open MPI<br>developer):<br><br>&nbsp; orte_grpcomm_modex failed<br>&nbsp; --&gt; Returned "Data unpack would read past end of buffer" (-26) instead of "Success"
 (0)<br>--------------------------------------------------------------------------<br>--------------------------------------------------------------------------<br>mpirun noticed that process rank 0 with PID 25633 on node tik40x exited on signal 27 (Profiling timer expired).</span></div><div><br><span></span></div><div><br><span></span></div><div><span>[tik40x:25626] [[29400,0],0] odls:default:fork binding child [[29400,1],0] to slot_list 0:0<br>[tik40x:25633] [[29400,1],0] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file grpcomm_bad_module.c at line 535<br>*** The MPI_Init() function was called before MPI_INIT was invoked.<br>*** This is disallowed by the MPI standard.<br>*** Your MPI job will now abort.<br>[tik40x:25633] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!</span></div><div><br><span></span></div><div><br><span></span></div><div><span>I had already tested this
 application prior to rebuilding openmpi (same version, but without thread support), and it was running well.</span></div><div><br><span></span></div><div><span>I have some discussions on this error in the forum, but I am not getting any useful pointers.</span></div><div><br><span></span></div><div><span>Has anyone else also seen this error?</span></div><div><br><span></span></div><div><span>Best</span></div><div><span><br>Devendra Rai<br></span></div><div><br></div><div style="font-family: Courier New, courier, monaco, monospace, sans-serif; font-size: 10pt;"><div style="font-family: times new roman, new york, times, serif; font-size: 12pt;"><font face="Arial" size="2"><hr size="1"><b><span style="font-weight:bold;">From:</span></b> German Hoecht &lt;german.hoecht@googlemail.com&gt;<br><b><span style="font-weight: bold;">To:</span></b> Open MPI Users &lt;users@open-mpi.org&gt;; Rob Latham &lt;robl@mcs.anl.gov&gt;<br><b><span style="font-weight:
 bold;">Sent:</span></b> Wednesday, 28 September 2011, 10:09<br><b><span style="font-weight: bold;">Subject:</span></b> Re: [OMPI users] maximum size for read buffer in MPI_File_read/write<br></font><br>Hi Rob,<br><br>thanks for your comments. I understand that it's most probably not worth<br>the effort to find the actual reason.<br><br>Because I have to deal with very large files I preferred using<br>"std::numeric_limits&lt;int&gt;::max()" rather than a hard-coded value<br>to split the read in case an IO request exceeds this amount. (This is<br>not the usual case but can happen.)<br><br>So your advice to use a max IO buffer of 1GB is quite precious.<br><br>To be honest, I did not do the check before we observed strange<br>numbers... Usually, MPI/ROMIO read/write functions are very stable, the<br>concerned code has read several Terabytes in the meanwhile.<br><br>Best regards,<br>German<br><br>On 09/27/2011 10:01 PM, Rob Latham wrote:<br>&gt; On Thu, Sep
 22, 2011 at 11:37:10PM +0200, German Hoecht wrote:<br>&gt;&gt; Hello,<br>&gt;&gt;<br>&gt;&gt; MPI_File_read/write functions uses&nbsp; an integer to specify the size of<br>&gt;&gt; the buffer, for instance:<br>&gt;&gt; int MPI_File_read(MPI_File fh, void *buf, int count, MPI_Datatype<br>&gt;&gt; datatype, MPI_Status *status)<br>&gt;&gt; with:<br>&gt;&gt; count&nbsp; &nbsp;  Number of elements in buffer (integer).<br>&gt;&gt; datatype&nbsp; Data type of each buffer element (handle).<br>&gt;&gt;<br>&gt;&gt; However, using the maximum value of 32 bytes integers:<br>&gt;&gt; count = 2^31-1 = 2147483647 (and datatype = MPI_BYTE)<br>&gt;&gt; MPI_file_read only reads&nbsp; 2^31-2^12 = 2147479552 bytes.<br>&gt;&gt; This means that 4095 bytes are ignored.<br>&gt;&gt;<br>&gt;&gt; I am not aware of this specific limit for integers in (Open) MPI<br>&gt;&gt; function calls. Is this supposed to be correct?<br>&gt; <br>&gt; Hi.&nbsp; I'm the ROMIO maintainer.&nbsp;
 OpenMPI more or less rolls up ROMIO<br>&gt; into OpenMPI, so any problems with the MPI_File_* routines is in my<br>&gt; lap, not OpenMPI.<br>&gt; <br>&gt; I'll be honest with you: i've not given any thought to just how big<br>&gt; the biggest request could be.&nbsp; The independent routines, especially<br>&gt; with a simple type like MPI_BYTE, are going to almost immediately call<br>&gt; the underlying posix read() or write() call. <br>&gt; <br>&gt; I can confirm the behavior you observe with your test program.<br>&gt; Thanks much for providing one.&nbsp; I'll dig around but I cannot think of<br>&gt; something in ROMIO that would ignore these 4095 bytes.&nbsp;  I do think<br>&gt; it's legal by the letter of the standard to read or write less than<br>&gt; requested.&nbsp;  "Upon completion, the amount of data accessed by the<br>&gt; calling process is returned in a status."&nbsp;  <br>&gt; <br>&gt; Bravo to you for actually checking return values and the
 status.&nbsp; I<br>&gt; don't think many non-library codes do that :&gt;<br>&gt; <br>&gt; I should at least be able to explain the behavior, so I'll dig a bit.<br>&gt; <br>&gt; in general, if you plot "i/o performance vs blocksize", every file<br>&gt; system tops out around several tens of megabytes.&nbsp; So, we have given<br>&gt; the advice to just split up this nearly 2 gb request into several 1 gb<br>&gt; requests.&nbsp; <br>&gt; <br>&gt; ==rob<br>&gt; <br><br>_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><br></div></div></div></body></html>
