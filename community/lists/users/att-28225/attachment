<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">Hmmm…let me see if I can remember :-)<div class=""><br class=""></div><div class="">Procs-per-object is what it does, of course, but I honestly forget what that last “r” stands for!</div><div class=""><br class=""></div><div class="">So what your command line is telling us is:</div><div class=""><br class=""></div><div class="">map 2 processes on each socket, binding each process to 7 cpu’s (“pe” = processing element)</div><div class=""><br class=""></div><div class="">In this case, we have defaulted to using cores as cpu’s (or “pe’s”). You could also set —use-hwthread-cpus and then “pe” would indicate the number of HTs to use for each process.</div><div class=""><br class=""></div><div class="">For a hybrid app, you probably want to ensure you have at least one pe for each thread you intend to spawn - but that is something you can experiment with to find the best option. I had hoped to someday make that a little easier by integrating OpenMP with OMPI a little better, but that ran into some controversy and so I abandoned it.</div><div class=""><br class=""></div><div class="">HTH</div><div class="">Ralph</div><div class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Jan 6, 2016, at 12:33 PM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com" class="">fortran@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><div dir="ltr" class="">A ha! The Gurus know all. The map-by was the magic sauce:<div class=""><br class=""></div><div class=""><div class=""><font face="monospace, monospace" class="">(1176) $ env OMP_NUM_THREADS=7 KMP_AFFINITY=compact mpirun -np 4 -map-by ppr:2:socket:pe=7 ./hello-hybrid.x | sort -g -k 18</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 0 out of 7 from process 0 out of 4 on borgo035 on CPU 0</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 1 out of 7 from process 0 out of 4 on borgo035 on CPU 1</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 2 out of 7 from process 0 out of 4 on borgo035 on CPU 2</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 3 out of 7 from process 0 out of 4 on borgo035 on CPU 3</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 4 out of 7 from process 0 out of 4 on borgo035 on CPU 4</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 5 out of 7 from process 0 out of 4 on borgo035 on CPU 5</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 6 out of 7 from process 0 out of 4 on borgo035 on CPU 6</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 0 out of 7 from process 1 out of 4 on borgo035 on CPU 7</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 1 out of 7 from process 1 out of 4 on borgo035 on CPU 8</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 2 out of 7 from process 1 out of 4 on borgo035 on CPU 9</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 3 out of 7 from process 1 out of 4 on borgo035 on CPU 10</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 4 out of 7 from process 1 out of 4 on borgo035 on CPU 11</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 5 out of 7 from process 1 out of 4 on borgo035 on CPU 12</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 6 out of 7 from process 1 out of 4 on borgo035 on CPU 13</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 0 out of 7 from process 2 out of 4 on borgo035 on CPU 14</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 1 out of 7 from process 2 out of 4 on borgo035 on CPU 15</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 2 out of 7 from process 2 out of 4 on borgo035 on CPU 16</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 3 out of 7 from process 2 out of 4 on borgo035 on CPU 17</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 4 out of 7 from process 2 out of 4 on borgo035 on CPU 18</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 5 out of 7 from process 2 out of 4 on borgo035 on CPU 19</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 6 out of 7 from process 2 out of 4 on borgo035 on CPU 20</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 0 out of 7 from process 3 out of 4 on borgo035 on CPU 21</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 1 out of 7 from process 3 out of 4 on borgo035 on CPU 22</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 2 out of 7 from process 3 out of 4 on borgo035 on CPU 23</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 3 out of 7 from process 3 out of 4 on borgo035 on CPU 24</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 4 out of 7 from process 3 out of 4 on borgo035 on CPU 25</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 5 out of 7 from process 3 out of 4 on borgo035 on CPU 26</font></div><div class=""><font face="monospace, monospace" class="">Hello from thread 6 out of 7 from process 3 out of 4 on borgo035 on CPU 27</font></div></div><div class=""><br class=""></div><div class="">So, a question: what does "ppr" mean? The man page seems to accept it as an axiom of Open MPI:</div><div class=""><br class=""></div><div class=""><div class="">&nbsp; &nbsp; &nbsp; &nbsp;--map-by &lt;foo&gt;</div><div class="">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Map &nbsp;to &nbsp;the specified object, defaults to socket. Supported options include slot, hwthread, core, L1cache, L2cache, L3cache, socket, numa,</div><div class="">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; board, node, sequential, distance, and ppr. Any object can include modifiers by adding a : and any combination of PE=n (bind &nbsp;n &nbsp;processing</div><div class="">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; elements &nbsp;to &nbsp;each &nbsp;proc), SPAN (load balance the processes across the allocation), OVERSUBSCRIBE (allow more processes on a node than pro‐</div><div class="">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; cessing elements), and NOOVERSUBSCRIBE.&nbsp; This includes PPR, where the pattern would be terminated by another colon to separate it from &nbsp;the</div><div class="">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; modifiers.</div></div><div class=""><br class=""></div><div class="">Is it an acronym/initialism? From some experimenting it seems to be ppr:2:socket means 2 processes per socket? And pe=7 means leave 7 processes between them? Is that about right?</div><div class=""><br class=""></div><div class="">Matt</div></div><div class="gmail_extra"><br class=""><div class="gmail_quote">On Wed, Jan 6, 2016 at 3:19 PM, Ralph Castain <span dir="ltr" class="">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank" class="">rhc@open-mpi.org</a>&gt;</span> wrote:<br class=""><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word" class="">I believe he wants two procs/socket, so you’d need ppr:2:socket:pe=7<div class=""><br class=""></div><div class=""><br class=""><div class=""><blockquote type="cite" class=""><div class=""><div class="h5"><div class="">On Jan 6, 2016, at 12:14 PM, Nick Papior &lt;<a href="mailto:nickpapior@gmail.com" target="_blank" class="">nickpapior@gmail.com</a>&gt; wrote:</div><br class=""></div></div><div class=""><div class=""><div class="h5"><div dir="ltr" class="">I do not think KMP_AFFINITY should affect anything in OpenMPI, it is an MKL env setting? Or am I wrong?<div class=""><br class=""></div><div class="">Note that these are used in an environment where openmpi automatically gets the host-file. Hence they are not present.</div><div class="">With intel mkl and openmpi I got the best performance using these, rather long flags:</div><div class=""><br class=""></div><div class=""><div class="">export KMP_AFFINITY=verbose,compact,granularity=core<br class=""></div><div class="">export KMP_STACKSIZE=62M</div><div class="">export KMP_SETTINGS=1</div><div class=""><br class=""></div><div class="">def_flags="--bind-to core -x OMP_PROC_BIND=true --report-bindings"</div><div class="">def_flags="$def_flags -x&nbsp;KMP_AFFINITY=$KMP_AFFINITY"</div><div class=""><br class=""></div><div class=""># in your case 7:</div><div class="">ONP=7</div><div class="">flags="$def_flags -x MKL_NUM_THREADS=$ONP -x MKL_DYNAMIC=FALSE"<br class=""></div><div class="">flags="$flags -x OMP_NUM_THREADS=$ONP -x OMP_DYNAMIC=FALSE"</div><div class="">flags="$flags -x KMP_STACKSIZE=$KMP_STACKSIZE"</div><div class="">flags="$flags --map-by ppr:1:socket:pe=7"</div></div><div class=""><br class=""></div><div class="">then run your program:</div><div class=""><br class=""></div><div class="">mpirun $flags &lt;app&gt;&nbsp;</div><div class=""><br class=""></div><div class="">A lot of the option flags are duplicated (and strictly not needed), but I provide them for easy testing changes.</div><div class="">Surely this is application dependent, but for my case it was performing really well.&nbsp;</div><div class=""><br class=""></div></div><div class="gmail_extra"><br class=""><div class="gmail_quote">2016-01-06 20:48 GMT+01:00 Erik Schnetter <span dir="ltr" class="">&lt;<a href="mailto:schnetter@gmail.com" target="_blank" class="">schnetter@gmail.com</a>&gt;</span>:<br class=""><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Setting KMP_AFFINITY will probably override anything that OpenMPI<br class="">
sets. Can you try without?<br class="">
<br class="">
-erik<br class="">
<div class=""><div class=""><br class="">
On Wed, Jan 6, 2016 at 2:46 PM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com" target="_blank" class="">fortran@gmail.com</a>&gt; wrote:<br class="">
&gt; Hello Open MPI Gurus,<br class="">
&gt;<br class="">
&gt; As I explore MPI-OpenMP hybrid codes, I'm trying to figure out how to do<br class="">
&gt; things to get the same behavior in various stacks. For example, I have a<br class="">
&gt; 28-core node (2 14-core Haswells), and I'd like to run 4 MPI processes and 7<br class="">
&gt; OpenMP threads. Thus, I'd like the processes to be 2 processes per socket<br class="">
&gt; with the OpenMP threads laid out on them. Using a "hybrid Hello World"<br class="">
&gt; program, I can achieve this with Intel MPI (after a lot of testing):<br class="">
&gt;<br class="">
&gt; (1097) $ env OMP_NUM_THREADS=7 KMP_AFFINITY=compact mpirun -np 4<br class="">
&gt; ./hello-hybrid.x | sort -g -k 18<br class="">
&gt; srun.slurm: cluster configuration lacks support for cpu binding<br class="">
&gt; Hello from thread 0 out of 7 from process 2 out of 4 on borgo035 on CPU 0<br class="">
&gt; Hello from thread 1 out of 7 from process 2 out of 4 on borgo035 on CPU 1<br class="">
&gt; Hello from thread 2 out of 7 from process 2 out of 4 on borgo035 on CPU 2<br class="">
&gt; Hello from thread 3 out of 7 from process 2 out of 4 on borgo035 on CPU 3<br class="">
&gt; Hello from thread 4 out of 7 from process 2 out of 4 on borgo035 on CPU 4<br class="">
&gt; Hello from thread 5 out of 7 from process 2 out of 4 on borgo035 on CPU 5<br class="">
&gt; Hello from thread 6 out of 7 from process 2 out of 4 on borgo035 on CPU 6<br class="">
&gt; Hello from thread 0 out of 7 from process 3 out of 4 on borgo035 on CPU 7<br class="">
&gt; Hello from thread 1 out of 7 from process 3 out of 4 on borgo035 on CPU 8<br class="">
&gt; Hello from thread 2 out of 7 from process 3 out of 4 on borgo035 on CPU 9<br class="">
&gt; Hello from thread 3 out of 7 from process 3 out of 4 on borgo035 on CPU 10<br class="">
&gt; Hello from thread 4 out of 7 from process 3 out of 4 on borgo035 on CPU 11<br class="">
&gt; Hello from thread 5 out of 7 from process 3 out of 4 on borgo035 on CPU 12<br class="">
&gt; Hello from thread 6 out of 7 from process 3 out of 4 on borgo035 on CPU 13<br class="">
&gt; Hello from thread 0 out of 7 from process 0 out of 4 on borgo035 on CPU 14<br class="">
&gt; Hello from thread 1 out of 7 from process 0 out of 4 on borgo035 on CPU 15<br class="">
&gt; Hello from thread 2 out of 7 from process 0 out of 4 on borgo035 on CPU 16<br class="">
&gt; Hello from thread 3 out of 7 from process 0 out of 4 on borgo035 on CPU 17<br class="">
&gt; Hello from thread 4 out of 7 from process 0 out of 4 on borgo035 on CPU 18<br class="">
&gt; Hello from thread 5 out of 7 from process 0 out of 4 on borgo035 on CPU 19<br class="">
&gt; Hello from thread 6 out of 7 from process 0 out of 4 on borgo035 on CPU 20<br class="">
&gt; Hello from thread 0 out of 7 from process 1 out of 4 on borgo035 on CPU 21<br class="">
&gt; Hello from thread 1 out of 7 from process 1 out of 4 on borgo035 on CPU 22<br class="">
&gt; Hello from thread 2 out of 7 from process 1 out of 4 on borgo035 on CPU 23<br class="">
&gt; Hello from thread 3 out of 7 from process 1 out of 4 on borgo035 on CPU 24<br class="">
&gt; Hello from thread 4 out of 7 from process 1 out of 4 on borgo035 on CPU 25<br class="">
&gt; Hello from thread 5 out of 7 from process 1 out of 4 on borgo035 on CPU 26<br class="">
&gt; Hello from thread 6 out of 7 from process 1 out of 4 on borgo035 on CPU 27<br class="">
&gt;<br class="">
&gt; Other than the odd fact that Process #0 seemed to start on Socket #1 (this<br class="">
&gt; might be an artifact of how I'm trying to detect the CPU I'm on), this looks<br class="">
&gt; reasonable. 14 threads on each socket and each process is laying out its<br class="">
&gt; threads in a nice orderly fashion.<br class="">
&gt;<br class="">
&gt; I'm trying to figure out how to do this with Open MPI (version 1.10.0) and<br class="">
&gt; apparently I am just not quite good enough to figure it out. The closest<br class="">
&gt; I've gotten is:<br class="">
&gt;<br class="">
&gt; (1155) $ env OMP_NUM_THREADS=7 KMP_AFFINITY=compact mpirun -np 4 -map-by<br class="">
&gt; ppr:2:socket ./hello-hybrid.x | sort -g -k 18<br class="">
&gt; Hello from thread 0 out of 7 from process 0 out of 4 on borgo035 on CPU 0<br class="">
&gt; Hello from thread 0 out of 7 from process 1 out of 4 on borgo035 on CPU 0<br class="">
&gt; Hello from thread 1 out of 7 from process 0 out of 4 on borgo035 on CPU 1<br class="">
&gt; Hello from thread 1 out of 7 from process 1 out of 4 on borgo035 on CPU 1<br class="">
&gt; Hello from thread 2 out of 7 from process 0 out of 4 on borgo035 on CPU 2<br class="">
&gt; Hello from thread 2 out of 7 from process 1 out of 4 on borgo035 on CPU 2<br class="">
&gt; Hello from thread 3 out of 7 from process 0 out of 4 on borgo035 on CPU 3<br class="">
&gt; Hello from thread 3 out of 7 from process 1 out of 4 on borgo035 on CPU 3<br class="">
&gt; Hello from thread 4 out of 7 from process 0 out of 4 on borgo035 on CPU 4<br class="">
&gt; Hello from thread 4 out of 7 from process 1 out of 4 on borgo035 on CPU 4<br class="">
&gt; Hello from thread 5 out of 7 from process 0 out of 4 on borgo035 on CPU 5<br class="">
&gt; Hello from thread 5 out of 7 from process 1 out of 4 on borgo035 on CPU 5<br class="">
&gt; Hello from thread 6 out of 7 from process 0 out of 4 on borgo035 on CPU 6<br class="">
&gt; Hello from thread 6 out of 7 from process 1 out of 4 on borgo035 on CPU 6<br class="">
&gt; Hello from thread 0 out of 7 from process 2 out of 4 on borgo035 on CPU 14<br class="">
&gt; Hello from thread 0 out of 7 from process 3 out of 4 on borgo035 on CPU 14<br class="">
&gt; Hello from thread 1 out of 7 from process 2 out of 4 on borgo035 on CPU 15<br class="">
&gt; Hello from thread 1 out of 7 from process 3 out of 4 on borgo035 on CPU 15<br class="">
&gt; Hello from thread 2 out of 7 from process 2 out of 4 on borgo035 on CPU 16<br class="">
&gt; Hello from thread 2 out of 7 from process 3 out of 4 on borgo035 on CPU 16<br class="">
&gt; Hello from thread 3 out of 7 from process 2 out of 4 on borgo035 on CPU 17<br class="">
&gt; Hello from thread 3 out of 7 from process 3 out of 4 on borgo035 on CPU 17<br class="">
&gt; Hello from thread 4 out of 7 from process 2 out of 4 on borgo035 on CPU 18<br class="">
&gt; Hello from thread 4 out of 7 from process 3 out of 4 on borgo035 on CPU 18<br class="">
&gt; Hello from thread 5 out of 7 from process 2 out of 4 on borgo035 on CPU 19<br class="">
&gt; Hello from thread 5 out of 7 from process 3 out of 4 on borgo035 on CPU 19<br class="">
&gt; Hello from thread 6 out of 7 from process 2 out of 4 on borgo035 on CPU 20<br class="">
&gt; Hello from thread 6 out of 7 from process 3 out of 4 on borgo035 on CPU 20<br class="">
&gt;<br class="">
&gt; Obviously not right. Any ideas on how to help me learn? The man mpirun page<br class="">
&gt; is a bit formidable in the pinning part, so maybe I've missed an obvious<br class="">
&gt; answer.<br class="">
&gt;<br class="">
&gt; Matt<br class="">
&gt; --<br class="">
&gt; Matt Thompson<br class="">
&gt;<br class="">
&gt; Man Among Men<br class="">
&gt; Fulcrum of History<br class="">
&gt;<br class="">
&gt;<br class="">
</div></div>&gt; _______________________________________________<br class="">
&gt; users mailing list<br class="">
&gt; <a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">
&gt; Link to this post:<br class="">
&gt; <a href="http://www.open-mpi.org/community/lists/users/2016/01/28217.php" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2016/01/28217.php</a><br class="">
<span class=""><font color="#888888" class=""><br class="">
<br class="">
<br class="">
--<br class="">
Erik Schnetter &lt;<a href="mailto:schnetter@gmail.com" target="_blank" class="">schnetter@gmail.com</a>&gt;<br class="">
<a href="http://www.perimeterinstitute.ca/personal/eschnetter/" rel="noreferrer" target="_blank" class="">http://www.perimeterinstitute.ca/personal/eschnetter/</a><br class="">
_______________________________________________<br class="">
users mailing list<br class="">
<a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/01/28218.php" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2016/01/28218.php</a><br class="">
</font></span></blockquote></div><br class=""><br clear="all" class=""><div class=""><br class=""></div>-- <br class=""><div class=""><div dir="ltr" class=""><div class="">Kind regards Nick</div></div></div>
</div>
_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class=""></div></div>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/01/28219.php" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2016/01/28219.php</a></div></blockquote></div><br class=""></div></div><br class="">_______________________________________________<br class="">
users mailing list<br class="">
<a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/01/28221.php" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2016/01/28221.php</a><br class=""></blockquote></div><br class=""><br clear="all" class=""><div class=""><br class=""></div>-- <br class=""><div class="gmail_signature"><div dir="ltr" class=""><div class=""><div dir="ltr" class=""><div class="">Matt Thompson</div></div></div><blockquote style="margin:0 0 0 40px;border:none;padding:0px" class=""><div class=""><div class=""><div class="">Man Among Men</div></div></div><div class=""><div class=""><div class="">Fulcrum of History</div></div></div></blockquote></div></div>
</div>
_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2016/01/28223.php</div></blockquote></div><br class=""></div></body></html>
