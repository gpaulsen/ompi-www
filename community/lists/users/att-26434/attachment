<div dir="ltr">Hello Tom,<div><br></div><div>Actually I have tried using: <span style="font-size:12.8000001907349px">MPI_Type_Create_Hindexed but the same problem persisted for the same matrix dimensions.</span></div><div><span style="font-size:12.8000001907349px"><br></span></div><div><span style="font-size:12.8000001907349px">Displacements array values are not a problem. Matrix of a size 800x640x480 creates type that is a bit less then 4GB large in case of complex datatype. It definitely fits in 32 bit range. So it is not an 32-64 bit issue, at least not for the value of displacements in this case.</span></div><div><span style="font-size:12.8000001907349px"><br></span></div><div><span style="font-size:12.8000001907349px">Regards,</span></div></div><div class="gmail_extra"><br clear="all"><div><div class="gmail_signature"><div dir="ltr"><div><div dir="ltr"><div><div dir="ltr"><div><span style="background-color:rgb(243,243,243)"><font color="#0b5394" size="1">----</font></span></div><div><span style="background-color:rgb(243,243,243)"><font color="#0b5394" size="1"><br></font></span></div><div><span style="background-color:rgb(243,243,243)"><font color="#0b5394"><font>Bogdan Sataric</font><br></font></span></div><div><span style="background-color:rgb(243,243,243)"><font color="#0b5394" size="1"><br></font></span></div><div><span style="background-color:rgb(243,243,243)"><font color="#0b5394">email: <a href="mailto:bogdan.sataric@gmail.com" target="_blank">bogdan.sataric@gmail.com</a></font></span></div><div><font color="#0b5394"><span style="background-color:rgb(243,243,243)">phone: +381 21-485-</span><span style="font-family:Arial,Helvetica,sans-serif;font-size:13.3333330154419px;white-space:nowrap">2441</span></font></div><div><span style="background-color:rgb(243,243,243)"><font color="#0b5394"><br></font></span></div><div><span style="background-color:rgb(243,243,243)"><font color="#0b5394">Teaching &amp; Research Assistant</font></span></div><div><span style="background-color:rgb(243,243,243)"><font color="#0b5394">Chair for Applied Computer Science</font></span></div><div><span style="background-color:rgb(243,243,243)"><font color="#0b5394">Faculty of Technical Sciences, Novi Sad, Serbia<br></font></span></div></div></div></div></div></div></div></div>
<br><div class="gmail_quote">On Thu, Mar 5, 2015 at 8:13 PM, Tom Rosmond <span dir="ltr">&lt;<a href="mailto:rosmond@reachone.com" target="_blank">rosmond@reachone.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Actually, you are not the first to encounter the problem with<br>
&#39;MPI_Type_indexed&#39; for very large datatypes.  I also run with a 1.6<br>
release, and solved the problem by switching to<br>
&#39;MPI_Type_Create_Hindexed&#39; for the datatype.  The critical difference is<br>
that the displacements for &#39;MPI_type_indexed&#39; are type integer, i.e. 32<br>
bit values, while for &#39;MPI_Type_Create_Hindexed&#39; the displacements are<br>
in bytes, but with type &#39;MPI_Address_Kind&#39;, i.e. normally 64 bit, and<br>
therefore of effectively infinite size.  Otherwise the 2 &#39;types&#39; can be<br>
used identically.<br>
<br>
T. Rosmond<br>
<div><div class="h5"><br>
<br>
On Thu, 2015-03-05 at 12:31 -0500, George Bosilca wrote:<br>
&gt; Bogdan,<br>
&gt;<br>
&gt;<br>
&gt; As far as I can tell your code is correct, and the problem is coming<br>
&gt; from Open MPI. More specifically, I used alloca in the optimization<br>
&gt; stage in MPI_Type_commit, and as your arrays of length were too large,<br>
&gt; alloca failed and lead to a segfault. I fixed in the trunk (3c489ea),<br>
&gt; and this will get into our next release.<br>
&gt;<br>
&gt;<br>
&gt; Unfortunately there is no fix for the 1.6 that I can think of.<br>
&gt; Apparently, you are really the first that run into such kind of<br>
&gt; problems, so guess you are the first creating gigantic datatypes.<br>
&gt;<br>
&gt;<br>
&gt; Thanks for the bug report,<br>
&gt;   George.<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; On Thu, Mar 5, 2015 at 9:09 AM, Bogdan Sataric<br>
&gt; &lt;<a href="mailto:bogdan.sataric@gmail.com">bogdan.sataric@gmail.com</a>&gt; wrote:<br>
&gt;         I&#39;ve been having problems with my 3D matrix transpose program.<br>
&gt;         I&#39;m using MPI_Type_indexed in order to allign specific blocks<br>
&gt;         that I want to send and receive across one or multiple nodes<br>
&gt;         of a cluster. Up to few days ago I was able to run my program<br>
&gt;         without any errors. However several test cases on the cluster<br>
&gt;         in last few days exposed segmentation fault when I try to form<br>
&gt;         indexed type on some specific matrix configurations.<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;         The code that forms indexed type is as follows:<br>
&gt;<br>
&gt;<br>
&gt;         #include &lt;stdio.h&gt;<br>
&gt;         #include &lt;stdlib.h&gt;<br>
&gt;         #include &lt;mpi.h&gt;<br>
&gt;<br>
&gt;<br>
&gt;         int main(int argc, char **argv) {<br>
&gt;<br>
&gt;<br>
&gt;             int Nx = 800;<br>
&gt;             int Ny = 640;<br>
&gt;             int Nz = 480;<br>
&gt;             int gsize;<br>
&gt;             int i, j;<br>
&gt;<br>
&gt;<br>
&gt;             MPI_Init(&amp;argc, &amp;argv);<br>
&gt;             MPI_Comm_size(MPI_COMM_WORLD, &amp;gsize);<br>
&gt;<br>
&gt;<br>
&gt;             printf(&quot;GSIZE: %d\n&quot;, gsize);<br>
&gt;<br>
&gt;<br>
&gt;             MPI_Datatype double_complex_type;<br>
&gt;             MPI_Datatype block_send_complex_type;<br>
&gt;<br>
&gt;<br>
&gt;             int * send_displ = (int *) malloc(Nx * Ny/gsize *<br>
&gt;         sizeof(int));<br>
&gt;             int * send_blocklen = (int *) malloc(Nx * Ny/gsize *<br>
&gt;         sizeof(int));<br>
&gt;<br>
&gt;<br>
&gt;             MPI_Type_contiguous(2, MPI_DOUBLE, &amp;double_complex_type);<br>
&gt;             MPI_Type_commit(&amp;double_complex_type);<br>
&gt;<br>
&gt;<br>
&gt;             for (i = Ny/gsize - 1; i &gt;= 0 ; i--) {<br>
&gt;                 for (j = 0; j &lt; Nx; j++) {<br>
&gt;                     send_displ[(Ny/gsize - 1 - i) * Nx + j] = i * Nz +<br>
&gt;         j * Ny * Nz;<br>
&gt;                     send_blocklen[(Ny/gsize - 1 - i) * Nx + j] = Nz;<br>
&gt;<br>
&gt;                 }<br>
&gt;             }<br>
&gt;<br>
&gt;<br>
&gt;             MPI_Type_indexed(Nx * Ny/gsize, send_blocklen, send_displ,<br>
&gt;         double_complex_type, &amp;block_send_complex_type);<br>
&gt;             MPI_Type_commit(&amp;block_send_complex_type);<br>
&gt;<br>
&gt;<br>
&gt;             free(send_displ);<br>
&gt;             free(send_blocklen);<br>
&gt;<br>
&gt;<br>
&gt;             MPI_Finalize();<br>
&gt;         }<br>
&gt;<br>
&gt;<br>
&gt;         Values of the Nx, Ny and Nz respectively are 800, 640 and 480.<br>
&gt;         Value of gsize for this test was 1 (simulation of MPI program<br>
&gt;         on 1 node). The node has 32GB of RAM and no other memory has<br>
&gt;         been allocated (only this code has been run).<br>
&gt;<br>
&gt;<br>
&gt;         In code basically I&#39;m creating double_complex_type to<br>
&gt;         represent complex number (2 contiguous MPI_DOUBLE) values. The<br>
&gt;         whole matrix has 800 * 640 * 480 of these values and I&#39;m<br>
&gt;         trying to catch these values in the indexed type. One indexed<br>
&gt;         type block length is the whole Nz &quot;rod&quot; while ordering of<br>
&gt;         these &quot;rods&quot; in displacements array is given by the formula i<br>
&gt;         * Nz + j * Ny * Nz. Basically displacements start from top<br>
&gt;         row, and left column of the 3D matrix. Then I gradually sweep<br>
&gt;         to the right sight of that top row, then go to one row below<br>
&gt;         sweep to the right side and so on until the bottom row.<br>
&gt;<br>
&gt;<br>
&gt;         The strange thing is that this formula and algorithm WORK if I<br>
&gt;         put MPI_DOUBLE type instead of derived complex type (1 instead<br>
&gt;         of 2 in MPI_TYPE_CONTIGIOUS). Also this formula WORKS if I put<br>
&gt;         1 for Nz dimension instead of 480. However if I change Nz to<br>
&gt;         even 2 I get segmentation fault error in the MPI_Type_commit<br>
&gt;         call.<br>
&gt;<br>
&gt;<br>
&gt;         I checked all of the displacements and they seem fine. There<br>
&gt;         is no overlapping of displacements or going under 0 or over<br>
&gt;         extent of the formed indexed type. Also the size of the<br>
&gt;         datatype is below 4GB (which is I believe limit of MPI<br>
&gt;         datatypes (since MPI_Type_size function returns int * ). Also<br>
&gt;         I believe amount of memory is not an issue as even if I put Nz<br>
&gt;         to be 2, I get the same segmentation fault error, and the node<br>
&gt;         has 32GB of RAM just for this test.<br>
&gt;<br>
&gt;<br>
&gt;         What bothers me is that most of other indexed type<br>
&gt;         configurations (with plain MPI_DOUBLE type elements), or<br>
&gt;         complex type with smaller matrix (say 400 * 640 * 480) WORK<br>
&gt;         without segmentation fault. Also If I commit the indexed type<br>
&gt;         with MPI_DOUBLE type elements even larger matrices work (say<br>
&gt;         960 x 800 x 640) which have exactly the same type size as 800<br>
&gt;         x 640 x 480 complex indexed type (just under 4GB)! So<br>
&gt;         basically the type size is not an issue here, but somehow<br>
&gt;         either number of blocks, size of particular blocks, or size of<br>
&gt;         block elements create problems. I&#39;m not sure weather there is<br>
&gt;         problem in implementation of OpenMPI or something in my code<br>
&gt;         is wrong...<br>
&gt;<br>
&gt;<br>
&gt;         I would greatly appreciate any help as I&#39;ve been stuck on this<br>
&gt;         problem for days now and nothing in MPI documentation and the<br>
&gt;         examples I found on the internet is giving me a clue where the<br>
&gt;         error might be.<br>
&gt;<br>
&gt;<br>
&gt;         At the end I would like to say that code has been compiled<br>
&gt;         with Open-MPI version 1.6.5.<br>
&gt;<br>
&gt;<br>
&gt;         Thank you,<br>
&gt;<br>
&gt;<br>
&gt;         Bogdan Sataric<br>
&gt;         ----<br>
&gt;<br>
&gt;<br>
&gt;         Bogdan Sataric<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;         email: <a href="mailto:bogdan.sataric@gmail.com">bogdan.sataric@gmail.com</a><br>
&gt;         phone: <a href="tel:%2B381%2021-485-2441" value="+381214852441">+381 21-485-2441</a><br>
&gt;<br>
&gt;<br>
&gt;         Teaching &amp; Research Assistant<br>
&gt;         Chair for Applied Computer Science<br>
&gt;         Faculty of Technical Sciences, Novi Sad, Serbia<br>
&gt;<br>
&gt;<br>
&gt;         _______________________________________________<br>
&gt;         users mailing list<br>
&gt;         <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;         Subscription:<br>
&gt;         <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;         Link to this post:<br>
&gt;         <a href="http://www.open-mpi.org/community/lists/users/2015/03/26430.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/03/26430.php</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div>&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/03/26431.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/03/26431.php</a><br>
<span class=""><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</span>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/03/26432.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/03/26432.php</a><br>
</blockquote></div><br></div>

