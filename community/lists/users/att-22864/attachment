<html xmlns:v="urn:schemas-microsoft-com:vml" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:w="urn:schemas-microsoft-com:office:word" xmlns:m="http://schemas.microsoft.com/office/2004/12/omml" xmlns="http://www.w3.org/TR/REC-html40"><head><meta http-equiv=Content-Type content="text/html; charset=iso-8859-1"><meta name=Generator content="Microsoft Word 14 (filtered medium)"><!--[if !mso]><style>v\:* {behavior:url(#default#VML);}
o\:* {behavior:url(#default#VML);}
w\:* {behavior:url(#default#VML);}
.shape {behavior:url(#default#VML);}
</style><![endif]--><style><!--
/* Font Definitions */
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:Tahoma;
	panose-1:2 11 6 4 3 5 4 4 2 4;}
@font-face
	{font-family:Consolas;
	panose-1:2 11 6 9 2 2 4 3 2 4;}
/* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin:0in;
	margin-bottom:.0001pt;
	font-size:12.0pt;
	font-family:"Times New Roman","serif";}
a:link, span.MsoHyperlink
	{mso-style-priority:99;
	color:blue;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{mso-style-priority:99;
	color:purple;
	text-decoration:underline;}
p
	{mso-style-priority:99;
	margin:0in;
	margin-bottom:.0001pt;
	font-size:12.0pt;
	font-family:"Times New Roman","serif";}
pre
	{mso-style-priority:99;
	mso-style-link:"HTML Preformatted Char";
	margin:0in;
	margin-bottom:.0001pt;
	font-size:10.0pt;
	font-family:"Courier New";}
p.MsoAcetate, li.MsoAcetate, div.MsoAcetate
	{mso-style-priority:99;
	mso-style-link:"Balloon Text Char";
	margin:0in;
	margin-bottom:.0001pt;
	font-size:8.0pt;
	font-family:"Tahoma","sans-serif";}
span.HTMLPreformattedChar
	{mso-style-name:"HTML Preformatted Char";
	mso-style-priority:99;
	mso-style-link:"HTML Preformatted";
	font-family:Consolas;}
span.EmailStyle20
	{mso-style-type:personal-reply;
	font-family:"Calibri","sans-serif";
	color:#1F497D;}
span.BalloonTextChar
	{mso-style-name:"Balloon Text Char";
	mso-style-priority:99;
	mso-style-link:"Balloon Text";
	font-family:"Tahoma","sans-serif";}
.MsoChpDefault
	{mso-style-type:export-only;
	font-size:10.0pt;}
@page WordSection1
	{size:8.5in 11.0in;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
	{page:WordSection1;}
--></style><!--[if gte mso 9]><xml>
<o:shapedefaults v:ext="edit" spidmax="1026" />
</xml><![endif]--><!--[if gte mso 9]><xml>
<o:shapelayout v:ext="edit">
<o:idmap v:ext="edit" data="1" />
</o:shapelayout></xml><![endif]--></head><body lang=EN-US link=blue vlink=purple><div class=WordSection1><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>The CUDA-aware support is only available when running with the verbs interface to Infiniband.  It does not work with the PSM interface which is being used in your installation.<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>To verify this, you need to disable the usage of PSM.  This can be done in a variety of ways, but try running like this:<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>mpirun &#8211;mca pml ob1 &#8230;..<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>This will force the use of the verbs support layer (openib) with the CUDA-aware support.<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><div style='border:none;border-left:solid blue 1.5pt;padding:0in 0in 0in 4.0pt'><div><div style='border:none;border-top:solid #B5C4DF 1.0pt;padding:3.0pt 0in 0in 0in'><p class=MsoNormal><b><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'>From:</span></b><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'> users [mailto:users-bounces@open-mpi.org] <b>On Behalf Of </b>KESTENER Pierre<br><b>Sent:</b> Wednesday, October 30, 2013 12:07 PM<br><b>To:</b> users@open-mpi.org<br><b>Subject:</b> Re: [OMPI users] OpenMPI-1.7.3 - cuda support<o:p></o:p></span></p></div></div><p class=MsoNormal><o:p>&nbsp;</o:p></p><div><p class=MsoNormal style='margin-bottom:12.0pt'><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif";color:black'>Dear Rolf,<br><br>thank for looking into this.<br>Here is the complete backtrace for execution using 2 GPUs on the same node:<br><br>(cuda-gdb) bt<br>#0&nbsp; 0x00007ffff711d885 in raise () from /lib64/libc.so.6<br>#1&nbsp; 0x00007ffff711f065 in abort () from /lib64/libc.so.6<br>#2&nbsp; 0x00007ffff0387b8d in psmi_errhandler_psm (ep=&lt;value optimized out&gt;, <br>&nbsp;&nbsp;&nbsp; err=PSM_INTERNAL_ERR, error_string=&lt;value optimized out&gt;, <br>&nbsp;&nbsp;&nbsp; token=&lt;value optimized out&gt;) at psm_error.c:76<br>#3&nbsp; 0x00007ffff0387df1 in psmi_handle_error (ep=0xfffffffffffffffe, <br>&nbsp;&nbsp;&nbsp; error=PSM_INTERNAL_ERR, buf=&lt;value optimized out&gt;) at psm_error.c:154<br>#4&nbsp; 0x00007ffff0382f6a in psmi_am_mq_handler_rtsmatch (toki=0x7fffffffc6a0, <br>&nbsp;&nbsp;&nbsp; args=0x7fffed0461d0, narg=&lt;value optimized out&gt;, <br>&nbsp;&nbsp;&nbsp; buf=&lt;value optimized out&gt;, len=&lt;value optimized out&gt;) at ptl.c:200<br>#5&nbsp; 0x00007ffff037a832 in process_packet (ptl=0x737818, pkt=0x7fffed0461c0, <br>&nbsp;&nbsp;&nbsp; isreq=&lt;value optimized out&gt;) at am_reqrep_shmem.c:2164<br>#6&nbsp; 0x00007ffff037d90f in amsh_poll_internal_inner (ptl=0x737818, replyonly=0)<br>&nbsp;&nbsp;&nbsp; at am_reqrep_shmem.c:1756<br>#7&nbsp; amsh_poll (ptl=0x737818, replyonly=0) at am_reqrep_shmem.c:1810<br>#8&nbsp; 0x00007ffff03a0329 in __psmi_poll_internal (ep=0x737538, <br>&nbsp;&nbsp;&nbsp; poll_amsh=&lt;value optimized out&gt;) at psm.c:465<br>#9&nbsp; 0x00007ffff039f0af in psmi_mq_wait_inner (ireq=0x7fffffffc848)<br>&nbsp;&nbsp;&nbsp; at psm_mq.c:299<br>#10 psmi_mq_wait_internal (ireq=0x7fffffffc848) at psm_mq.c:334<br>#11 0x00007ffff037db21 in amsh_mq_send_inner (ptl=0x737818, <br>&nbsp;&nbsp;&nbsp; mq=&lt;value optimized out&gt;, epaddr=0x6eb418, flags=&lt;value optimized out&gt;, <br>&nbsp;&nbsp;&nbsp; tag=844424930131968, ubuf=0x1308350000, len=32768)<br>---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---<br>&nbsp;&nbsp;&nbsp; at am_reqrep_shmem.c:2339<br>#12 amsh_mq_send (ptl=0x737818, mq=&lt;value optimized out&gt;, epaddr=0x6eb418, <br>&nbsp;&nbsp;&nbsp; flags=&lt;value optimized out&gt;, tag=844424930131968, ubuf=0x1308350000, <br>&nbsp;&nbsp;&nbsp; len=32768) at am_reqrep_shmem.c:2387<br>#13 0x00007ffff039ed71 in __psm_mq_send (mq=&lt;value optimized out&gt;, <br>&nbsp;&nbsp;&nbsp; dest=&lt;value optimized out&gt;, flags=&lt;value optimized out&gt;, <br>&nbsp;&nbsp;&nbsp; stag=&lt;value optimized out&gt;, buf=&lt;value optimized out&gt;, <br>&nbsp;&nbsp;&nbsp; len=&lt;value optimized out&gt;) at psm_mq.c:413<br>#14 0x00007ffff05c4ea8 in ompi_mtl_psm_send ()<br>&nbsp;&nbsp; from /gpfslocal/pub/openmpi/1.7.3/lib/openmpi/mca_mtl_psm.so<br>#15 0x00007ffff1eeddea in mca_pml_cm_send ()<br>&nbsp;&nbsp; from /gpfslocal/pub/openmpi/1.7.3/lib/openmpi/mca_pml_cm.so<br>#16 0x00007ffff79253da in PMPI_Sendrecv ()<br>&nbsp;&nbsp; from /gpfslocal/pub/openmpi/1.7.3/lib/libmpi.so.1<br>#17 0x00000000004045ef in ExchangeHalos (cartComm=0x715460, <br>&nbsp;&nbsp;&nbsp; devSend=0x1308350000, hostSend=0x7b8710, hostRecv=0x7c0720, <br>&nbsp;&nbsp;&nbsp; devRecv=0x1308358000, neighbor=1, elemCount=4096) at CUDA_Aware_MPI.c:70<br>#18 0x00000000004033d8 in TransferAllHalos (cartComm=0x715460, <br>&nbsp;&nbsp;&nbsp; domSize=0x7fffffffcd80, topIndex=0x7fffffffcd60, neighbors=0x7fffffffcd90, <br>&nbsp;&nbsp;&nbsp; copyStream=0xaa4450, devBlocks=0x7fffffffcd30, <br>&nbsp;&nbsp;&nbsp; devSideEdges=0x7fffffffcd20, devHaloLines=0x7fffffffcd10, <br>&nbsp;&nbsp;&nbsp; hostSendLines=0x7fffffffcd00, hostRecvLines=0x7fffffffccf0) at Host.c:400<br>#19 0x000000000040363c in RunJacobi (cartComm=0x715460, rank=0, size=2, <br>---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---<br>&nbsp;&nbsp;&nbsp; domSize=0x7fffffffcd80, topIndex=0x7fffffffcd60, neighbors=0x7fffffffcd90, <br>&nbsp;&nbsp;&nbsp; useFastSwap=0, devBlocks=0x7fffffffcd30, devSideEdges=0x7fffffffcd20, <br>&nbsp;&nbsp;&nbsp; devHaloLines=0x7fffffffcd10, hostSendLines=0x7fffffffcd00, <br>&nbsp;&nbsp;&nbsp; hostRecvLines=0x7fffffffccf0, devResidue=0x1310480000, <br>&nbsp;&nbsp;&nbsp; copyStream=0xaa4450, iterations=0x7fffffffcd44, <br>&nbsp;&nbsp;&nbsp; avgTransferTime=0x7fffffffcd48) at Host.c:466<br>#20 0x0000000000401ccb in main (argc=4, argv=0x7fffffffcea8) at Jacobi.c:60<o:p></o:p></span></p><div><p class=MsoNormal><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif";color:black'>Pierre.<o:p></o:p></span></p><div><div><div><div><pre><span style='color:black'><o:p>&nbsp;</o:p></span></pre></div></div></div></div></div><div><div class=MsoNormal align=center style='text-align:center'><span style='color:black'><hr size=2 width="100%" align=center></span></div><div id=divRpF441810><p class=MsoNormal style='margin-bottom:12.0pt'><b><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif";color:black'>De :</span></b><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif";color:black'> KESTENER Pierre<br><b>Date d'envoi :</b> mercredi 30 octobre 2013 16:34<br><b>À&nbsp;:</b> <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><b>Cc:</b> KESTENER Pierre<br><b>Objet :</b> OpenMPI-1.7.3 - cuda support</span><span style='color:black'><o:p></o:p></span></p></div><div><div><p class=MsoNormal><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif";color:black'>Hello, &nbsp;&nbsp;&nbsp; <o:p></o:p></span></p><div><p class=MsoNormal style='margin-bottom:12.0pt'><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif";color:black'><br>I'm having problems running a simple cuda-aware mpi application; the one found at<br><a href="https://github.com/parallel-forall/code-samples/tree/master/posts/cuda-aware-mpi-example" target="_blank">https://github.com/parallel-forall/code-samples/tree/master/posts/cuda-aware-mpi-example</a><br><br>I have modified symbol ENV_LOCAL_RANK into OMPI_COMM_WORLD_LOCAL_RANK<br>My cluster has 2 K20m GPUs per node, with QLogic IB stack.<br><br>The normal CUDA/MPI application works fine;<br>&nbsp;but the cuda-ware mpi app is crashing when using 2 MPI proc over the 2 GPUs of the same node:<br>the error message is:<br>&nbsp;&nbsp;&nbsp; Assertion failure at ptl.c:200: nbytes == msglen<br>I can send the complete backtrace from cuda-gdb if needed.<br><br>The same app when running on 2 GPUs on 2 different nodes give another error:<br>&nbsp;&nbsp;&nbsp; jacobi_cuda_aware_mpi:28280 terminated with signal 11 at PC=2aae9d7c9f78 SP=7fffc06c21f8.&nbsp; &nbsp;&nbsp;&nbsp; Backtrace:<br>&nbsp;&nbsp;&nbsp; /gpfslocal/pub/local/lib64/libinfinipath.so.4(+0x8f78)[0x2aae9d7c9f78]<br><br><br>Can someone give me hints where to look to track this problem ?<br>Thank you.<br><br>Pierre Kestener.<o:p></o:p></span></p><div><div><div><div><pre><span style='color:black'><o:p>&nbsp;</o:p></span></pre></div></div></div></div></div></div></div></div></div></div></div>
<DIV>
<HR>
</DIV>
<DIV>This email message is for the sole use of the intended recipient(s) and may 
contain confidential information.&nbsp; Any unauthorized review, use, disclosure 
or distribution is prohibited.&nbsp; If you are not the intended recipient, 
please contact the sender by reply email and destroy all copies of the original 
message. </DIV>
<DIV>
<HR>
</DIV>
</body></html>

