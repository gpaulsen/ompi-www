<div dir="ltr"><br><div class="gmail_extra"><br><div class="gmail_quote">2016-05-05 9:27 GMT-05:00 Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles.gouaillardet@gmail.com" target="_blank">gilles.gouaillardet@gmail.com</a>&gt;</span>:<br><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex">Out of curiosity, can you try<div>mpirun --mca btl self,sm ...</div></blockquote><div>Same as before. Many MPI_Test calls. <br></div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"><div>and</div><div>mpirun --mca btl self,vader ...</div></blockquote><div>A requested component was not found, or was unable to be opened.  This<br>means that this component is either not installed or is unable to be<br>used on your system (e.g., sometimes this means that shared libraries<br>that the component requires are unable to be found/loaded).  Note that<br>Open MPI stopped checking at the first component that it did not find.<br><br>Host:      VirtualBox<br>Framework: btl<br>Component: vade<br>--------------------------------------------------------------------------<br>*** An error occurred in MPI_Init<br>--------------------------------------------------------------------------<br>It looks like MPI_INIT failed for some reason; your parallel process is<br>likely to abort.  There are many reasons that a parallel process can<br>fail during MPI_INIT; some of which are due to configuration or environment<br>problems.  This failure appears to be an internal failure; here&#39;s some<br>additional information (which may only be relevant to an Open MPI<br>developer):<br><br>  mca_bml_base_open() failed<br>  --&gt; Returned &quot;Not found&quot; (-13) instead of &quot;Success&quot; (0)<br>--------------------------------------------------------------------------<br>*** on a NULL communicator<br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>***    and potentially your MPI job)<br>[VirtualBox:2188] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!<br>-------------------------------------------------------<br>Primary job  terminated normally, but 1 process returned<br>a non-zero exit code.. Per user-direction, the job has been aborted.<br>-------------------------------------------------------<br>--------------------------------------------------------------------------<br>mpirun detected that one or more processes exited with non-zero status, thus causing<br>the job to be terminated. The first process to do so was:<br><br>  Process name: [[9235,1],0]<br>  Exit code:    1<br>--------------------------------------------------------------------------<br>[VirtualBox:02186] 1 more process has sent help message help-mca-base.txt / find-available:not-valid<br>[VirtualBox:02186] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br> </div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"><div><br></div><div>and see if one performs better than the other ?</div><div class=""><div class="h5"><div><br></div><div>Cheers,</div><div><br></div><div>Gilles</div><div><br>On Thursday, May 5, 2016, Zhen Wang &lt;<a href="mailto:toddwz@gmail.com" target="_blank">toddwz@gmail.com</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"><div dir="ltr">Gilles,<br><br>Thanks for your reply.<br><br>Best regards,<div class="gmail_extra"><div><div><div dir="ltr"><div>Zhen</div></div></div></div>
<br><div class="gmail_quote">On Wed, May 4, 2016 at 8:43 PM, Gilles Gouaillardet <span dir="ltr">&lt;<a>gilles.gouaillardet@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex">Note there is no progress thread in openmpi 1.10<div>from a pragmatic point of view, that means that for &quot;large&quot; messages, no data is sent in MPI_Isend, and the data is sent when MPI &quot;progresses&quot; e.g. call a MPI_Test, MPI_Probe, MPI_Recv or some similar subroutine.</div><div>in your example, the data is transferred after the first usleep completes.</div></blockquote><div>I agree.  <br></div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"><div><br></div><div>that being said, it takes quite a while, and there could be an issue.</div><div>what if you use MPI_Send instead () ?</div></blockquote><div>Works as expected.<br> <br></div><div>MPI 1: Recv of 0 started at 08:37:10.<br>MPI 1: Recv of 0 finished at 08:37:10.<br>MPI 0: Send of 0 started at 08:37:10.<br>MPI 0: Send of 0 finished at 08:37:10.<br> </div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"><div>what if you send/Recv a large message first (to &quot;warmup&quot; connections), MPI_Barrier, and then start your MPI_Isend ?</div></blockquote><div>Not working. For what I want to accomplish, is my code the right way to go? Is there an altenative method? Thanks.<br><br>MPI 1: Recv of 0 started at 08:38:46.<br>MPI 0: Isend of 0 started at 08:38:46.<br>MPI 0: Isend of 1 started at 08:38:46.<br>MPI 0: Isend of 2 started at 08:38:46.<br>MPI 0: Isend of 3 started at 08:38:46.<br>MPI 0: Isend of 4 started at 08:38:46.<br>MPI 0: MPI_Test of 0 at 08:38:46.<br>MPI 0: MPI_Test of 0 at 08:38:46.<br>MPI 0: MPI_Test of 0 at 08:38:46.<br>MPI 0: MPI_Test of 0 at 08:38:46.<br>MPI 0: MPI_Test of 0 at 08:38:46.<br>MPI 0: MPI_Test of 0 at 08:38:46.<br>MPI 0: MPI_Test of 0 at 08:38:46.<br>MPI 0: MPI_Test of 0 at 08:38:47.<br>MPI 0: MPI_Test of 0 at 08:38:47.<br>MPI 0: MPI_Test of 0 at 08:38:47.<br>MPI 0: MPI_Test of 0 at 08:38:47.<br>MPI 0: MPI_Test of 0 at 08:38:47.<br>MPI 0: MPI_Test of 0 at 08:38:47.<br>MPI 0: MPI_Test of 0 at 08:38:47.<br>MPI 0: MPI_Test of 0 at 08:38:47.<br>MPI 0: MPI_Test of 0 at 08:38:47.<br>MPI 0: MPI_Test of 0 at 08:38:47.<br>MPI 0: MPI_Test of 0 at 08:38:48.<br>MPI 0: MPI_Test of 0 at 08:38:48.<br>MPI 0: MPI_Test of 0 at 08:38:48.<br>MPI 0: MPI_Test of 0 at 08:38:48.<br>MPI 0: MPI_Test of 0 at 08:38:48.<br>MPI 0: MPI_Test of 0 at 08:38:48.<br>MPI 0: MPI_Test of 0 at 08:38:48.<br>MPI 0: MPI_Test of 0 at 08:38:48.<br>MPI 0: MPI_Test of 0 at 08:38:48.<br>MPI 0: MPI_Test of 0 at 08:38:48.<br>MPI 0: MPI_Test of 0 at 08:38:49.<br>MPI 0: MPI_Test of 0 at 08:38:49.<br>MPI 0: MPI_Test of 0 at 08:38:49.<br>MPI 0: MPI_Test of 0 at 08:38:49.<br>MPI 0: MPI_Test of 0 at 08:38:49.<br>MPI 0: MPI_Test of 0 at 08:38:49.<br>MPI 0: MPI_Test of 0 at 08:38:49.<br>MPI 0: MPI_Test of 0 at 08:38:49.<br>MPI 0: MPI_Test of 0 at 08:38:49.<br>MPI 0: MPI_Test of 0 at 08:38:49.<br>MPI 0: MPI_Test of 0 at 08:38:50.<br>MPI 0: MPI_Test of 0 at 08:38:50.<br>MPI 0: MPI_Test of 0 at 08:38:50.<br>MPI 0: MPI_Test of 0 at 08:38:50.<br>MPI 1: Recv of 0 finished at 08:38:50.<br>MPI 1: Recv of 1 started at 08:38:50.<br>MPI 0: MPI_Test of 0 at 08:38:50.<br>MPI 0: Isend of 0 finished at 08:38:50.<br>MPI 0: MPI_Test of 1 at 08:38:50.<br>MPI 0: MPI_Test of 1 at 08:38:50.<br>MPI 0: MPI_Test of 1 at 08:38:50.<br>MPI 0: MPI_Test of 1 at 08:38:50.<br>MPI 0: MPI_Test of 1 at 08:38:50.<br>MPI 0: MPI_Test of 1 at 08:38:51.<br>MPI 0: MPI_Test of 1 at 08:38:51.<br>MPI 0: MPI_Test of 1 at 08:38:51.<br>MPI 0: MPI_Test of 1 at 08:38:51.<br>MPI 0: MPI_Test of 1 at 08:38:51.<br>MPI 0: MPI_Test of 1 at 08:38:51.<br>MPI 0: MPI_Test of 1 at 08:38:51.<br>MPI 0: MPI_Test of 1 at 08:38:51.<br>MPI 0: MPI_Test of 1 at 08:38:51.<br>MPI 0: MPI_Test of 1 at 08:38:51.<br>MPI 0: MPI_Test of 1 at 08:38:52.<br>MPI 0: MPI_Test of 1 at 08:38:52.<br>MPI 0: MPI_Test of 1 at 08:38:52.<br>MPI 0: MPI_Test of 1 at 08:38:52.<br>MPI 0: MPI_Test of 1 at 08:38:52.<br>MPI 0: MPI_Test of 1 at 08:38:52.<br>MPI 0: MPI_Test of 1 at 08:38:52.<br>MPI 0: MPI_Test of 1 at 08:38:52.<br>MPI 0: MPI_Test of 1 at 08:38:52.<br>MPI 0: MPI_Test of 1 at 08:38:52.<br>MPI 0: MPI_Test of 1 at 08:38:53.<br>MPI 0: MPI_Test of 1 at 08:38:53.<br>MPI 0: MPI_Test of 1 at 08:38:53.<br>MPI 0: MPI_Test of 1 at 08:38:53.<br>MPI 0: MPI_Test of 1 at 08:38:53.<br>MPI 0: MPI_Test of 1 at 08:38:53.<br>MPI 0: MPI_Test of 1 at 08:38:53.<br>MPI 0: MPI_Test of 1 at 08:38:53.<br>MPI 0: MPI_Test of 1 at 08:38:53.<br>MPI 0: MPI_Test of 1 at 08:38:53.<br>MPI 0: MPI_Test of 1 at 08:38:54.<br>MPI 0: MPI_Test of 1 at 08:38:54.<br>MPI 0: MPI_Test of 1 at 08:38:54.<br>MPI 0: MPI_Test of 1 at 08:38:54.<br>MPI 0: MPI_Test of 1 at 08:38:54.<br>MPI 1: Recv of 1 finished at 08:38:54.<br>MPI 1: Recv of 2 started at 08:38:54.<br>MPI 0: MPI_Test of 1 at 08:38:54.<br>MPI 0: Isend of 1 finished at 08:38:54.<br>MPI 0: MPI_Test of 2 at 08:38:54.<br>MPI 0: MPI_Test of 2 at 08:38:54.<br>MPI 0: MPI_Test of 2 at 08:38:54.<br>MPI 0: MPI_Test of 2 at 08:38:55.<br>MPI 0: MPI_Test of 2 at 08:38:55.<br>MPI 0: MPI_Test of 2 at 08:38:55.<br>MPI 0: MPI_Test of 2 at 08:38:55.<br>MPI 0: MPI_Test of 2 at 08:38:55.<br>MPI 0: MPI_Test of 2 at 08:38:55.<br>MPI 0: MPI_Test of 2 at 08:38:55.<br>MPI 0: MPI_Test of 2 at 08:38:55.<br>MPI 0: MPI_Test of 2 at 08:38:55.<br>MPI 0: MPI_Test of 2 at 08:38:55.<br>MPI 0: MPI_Test of 2 at 08:38:56.<br>MPI 0: MPI_Test of 2 at 08:38:56.<br>MPI 0: MPI_Test of 2 at 08:38:56.<br>MPI 0: MPI_Test of 2 at 08:38:56.<br>MPI 0: MPI_Test of 2 at 08:38:56.<br>MPI 0: MPI_Test of 2 at 08:38:56.<br>MPI 0: MPI_Test of 2 at 08:38:56.<br>MPI 0: MPI_Test of 2 at 08:38:56.<br>MPI 0: MPI_Test of 2 at 08:38:56.<br>MPI 0: MPI_Test of 2 at 08:38:56.<br>MPI 0: MPI_Test of 2 at 08:38:57.<br>MPI 0: MPI_Test of 2 at 08:38:57.<br>MPI 0: MPI_Test of 2 at 08:38:57.<br>MPI 0: MPI_Test of 2 at 08:38:57.<br>MPI 0: MPI_Test of 2 at 08:38:57.<br>MPI 0: MPI_Test of 2 at 08:38:57.<br>MPI 0: MPI_Test of 2 at 08:38:57.<br>MPI 0: MPI_Test of 2 at 08:38:57.<br>MPI 0: MPI_Test of 2 at 08:38:57.<br>MPI 0: MPI_Test of 2 at 08:38:57.<br>MPI 0: MPI_Test of 2 at 08:38:58.<br>MPI 0: MPI_Test of 2 at 08:38:58.<br>MPI 0: MPI_Test of 2 at 08:38:58.<br>MPI 0: MPI_Test of 2 at 08:38:58.<br>MPI 0: MPI_Test of 2 at 08:38:58.<br>MPI 0: MPI_Test of 2 at 08:38:58.<br>MPI 0: MPI_Test of 2 at 08:38:58.<br>MPI 1: Recv of 2 finished at 08:38:58.<br>MPI 1: Recv of 3 started at 08:38:58.<br>MPI 0: MPI_Test of 2 at 08:38:58.<br>MPI 0: Isend of 2 finished at 08:38:58.<br>MPI 0: MPI_Test of 3 at 08:38:58.<br>MPI 0: MPI_Test of 3 at 08:38:58.<br>MPI 0: MPI_Test of 3 at 08:38:59.<br>MPI 0: MPI_Test of 3 at 08:38:59.<br>MPI 0: MPI_Test of 3 at 08:38:59.<br>MPI 0: MPI_Test of 3 at 08:38:59.<br>MPI 0: MPI_Test of 3 at 08:38:59.<br>MPI 0: MPI_Test of 3 at 08:38:59.<br>MPI 0: MPI_Test of 3 at 08:38:59.<br>MPI 0: MPI_Test of 3 at 08:38:59.<br>MPI 0: MPI_Test of 3 at 08:38:59.<br>MPI 0: MPI_Test of 3 at 08:38:59.<br>MPI 0: MPI_Test of 3 at 08:39:00.<br>MPI 0: MPI_Test of 3 at 08:39:00.<br>MPI 0: MPI_Test of 3 at 08:39:00.<br>MPI 0: MPI_Test of 3 at 08:39:00.<br>MPI 0: MPI_Test of 3 at 08:39:00.<br>MPI 0: MPI_Test of 3 at 08:39:00.<br>MPI 0: MPI_Test of 3 at 08:39:00.<br>MPI 0: MPI_Test of 3 at 08:39:00.<br>MPI 0: MPI_Test of 3 at 08:39:00.<br>MPI 0: MPI_Test of 3 at 08:39:00.<br>MPI 0: MPI_Test of 3 at 08:39:01.<br>MPI 0: MPI_Test of 3 at 08:39:01.<br>MPI 0: MPI_Test of 3 at 08:39:01.<br>MPI 0: MPI_Test of 3 at 08:39:01.<br>MPI 0: MPI_Test of 3 at 08:39:01.<br>MPI 0: MPI_Test of 3 at 08:39:01.<br>MPI 0: MPI_Test of 3 at 08:39:01.<br>MPI 0: MPI_Test of 3 at 08:39:01.<br>MPI 0: MPI_Test of 3 at 08:39:01.<br>MPI 0: MPI_Test of 3 at 08:39:01.<br>MPI 0: MPI_Test of 3 at 08:39:02.<br>MPI 0: MPI_Test of 3 at 08:39:02.<br>MPI 0: MPI_Test of 3 at 08:39:02.<br>MPI 0: MPI_Test of 3 at 08:39:02.<br>MPI 0: MPI_Test of 3 at 08:39:02.<br>MPI 0: MPI_Test of 3 at 08:39:02.<br>MPI 0: MPI_Test of 3 at 08:39:02.<br>MPI 0: MPI_Test of 3 at 08:39:02.<br>MPI 1: Recv of 3 finished at 08:39:02.<br>MPI 1: Recv of 4 started at 08:39:02.<br>MPI 0: MPI_Test of 3 at 08:39:02.<br>MPI 0: Isend of 3 finished at 08:39:02.<br>MPI 0: MPI_Test of 4 at 08:39:02.<br>MPI 0: MPI_Test of 4 at 08:39:03.<br>MPI 0: MPI_Test of 4 at 08:39:03.<br>MPI 0: MPI_Test of 4 at 08:39:03.<br>MPI 0: MPI_Test of 4 at 08:39:03.<br>MPI 0: MPI_Test of 4 at 08:39:03.<br>MPI 0: MPI_Test of 4 at 08:39:03.<br>MPI 0: MPI_Test of 4 at 08:39:03.<br>MPI 0: MPI_Test of 4 at 08:39:03.<br>MPI 0: MPI_Test of 4 at 08:39:03.<br>MPI 0: MPI_Test of 4 at 08:39:03.<br>MPI 0: MPI_Test of 4 at 08:39:04.<br>MPI 0: MPI_Test of 4 at 08:39:04.<br>MPI 0: MPI_Test of 4 at 08:39:04.<br>MPI 0: MPI_Test of 4 at 08:39:04.<br>MPI 0: MPI_Test of 4 at 08:39:04.<br>MPI 0: MPI_Test of 4 at 08:39:04.<br>MPI 0: MPI_Test of 4 at 08:39:04.<br>MPI 0: MPI_Test of 4 at 08:39:04.<br>MPI 0: MPI_Test of 4 at 08:39:04.<br>MPI 0: MPI_Test of 4 at 08:39:04.<br>MPI 0: MPI_Test of 4 at 08:39:05.<br>MPI 0: MPI_Test of 4 at 08:39:05.<br>MPI 0: MPI_Test of 4 at 08:39:05.<br>MPI 0: MPI_Test of 4 at 08:39:05.<br>MPI 0: MPI_Test of 4 at 08:39:05.<br>MPI 0: MPI_Test of 4 at 08:39:05.<br>MPI 0: MPI_Test of 4 at 08:39:05.<br>MPI 0: MPI_Test of 4 at 08:39:05.<br>MPI 0: MPI_Test of 4 at 08:39:05.<br>MPI 0: MPI_Test of 4 at 08:39:05.<br>MPI 0: MPI_Test of 4 at 08:39:06.<br>MPI 0: MPI_Test of 4 at 08:39:06.<br>MPI 0: MPI_Test of 4 at 08:39:06.<br>MPI 0: MPI_Test of 4 at 08:39:06.<br>MPI 0: MPI_Test of 4 at 08:39:06.<br>MPI 0: MPI_Test of 4 at 08:39:06.<br>MPI 0: MPI_Test of 4 at 08:39:06.<br>MPI 0: MPI_Test of 4 at 08:39:06.<br>MPI 0: MPI_Test of 4 at 08:39:06.<br>MPI 1: Recv of 4 finished at 08:39:06.<br>MPI 0: MPI_Test of 4 at 08:39:06.<br>MPI 0: Isend of 4 finished at 08:39:06.<br> <br></div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"><div><br></div><div>Cheers,</div><div><br></div><div>Gilles</div><div><div><div><br></div><div><br>On Thursday, May 5, 2016, Zhen Wang &lt;<a>toddwz@gmail.com</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"><div dir="ltr"><div><div>Hi,<br><br></div>I&#39;m having a problem with Isend, Recv and Test in Linux Mint 16 Petra. The source is attached.<br><br>Open MPI 1.10.2 is configured with<br>./configure --enable-debug --prefix=/home/&lt;me&gt;/Tool/openmpi-1.10.2-debug<br><br>The source is built with<br>~/Tool/openmpi-1.10.2-debug/bin/mpiCC a5.cpp<br><br>and run in one node with<br>~/Tool/openmpi-1.10.2-debug/bin/mpirun -n 2 ./a.out<br><br></div>The output is in the end. What puzzles me is why MPI_Test is called so many times, and it takes so long to send a message. Am I doing something wrong? I&#39;m simulating a more complicated program: MPI 0 Isends data to MPI 1, computes (usleep here), and calls Test to check if data are sent. MPI 1 Recvs data, and computes.<br><br>Thanks in advance.<br><div><br><div><br clear="all"><div><div><div><div dir="ltr">Best regards,<div>Zhen<br><br>MPI 0: Isend of 0 started at 20:32:35.<br>MPI 1: Recv of 0 started at 20:32:35.<br>MPI 0: MPI_Test of 0 at 20:32:35.<br>MPI 0: MPI_Test of 0 at 20:32:35.<br>MPI 0: MPI_Test of 0 at 20:32:35.<br>MPI 0: MPI_Test of 0 at 20:32:35.<br>MPI 0: MPI_Test of 0 at 20:32:35.<br>MPI 0: MPI_Test of 0 at 20:32:35.<br>MPI 0: MPI_Test of 0 at 20:32:36.<br>MPI 0: MPI_Test of 0 at 20:32:36.<br>MPI 0: MPI_Test of 0 at 20:32:36.<br>MPI 0: MPI_Test of 0 at 20:32:36.<br>MPI 0: MPI_Test of 0 at 20:32:36.<br>MPI 0: MPI_Test of 0 at 20:32:36.<br>MPI 0: MPI_Test of 0 at 20:32:36.<br>MPI 0: MPI_Test of 0 at 20:32:36.<br>MPI 0: MPI_Test of 0 at 20:32:36.<br>MPI 0: MPI_Test of 0 at 20:32:37.<br>MPI 0: MPI_Test of 0 at 20:32:37.<br>MPI 0: MPI_Test of 0 at 20:32:37.<br>MPI 0: MPI_Test of 0 at 20:32:37.<br>MPI 0: MPI_Test of 0 at 20:32:37.<br>MPI 0: MPI_Test of 0 at 20:32:37.<br>MPI 0: MPI_Test of 0 at 20:32:37.<br>MPI 0: MPI_Test of 0 at 20:32:37.<br>MPI 0: MPI_Test of 0 at 20:32:37.<br>MPI 0: MPI_Test of 0 at 20:32:37.<br>MPI 0: MPI_Test of 0 at 20:32:38.<br>MPI 0: MPI_Test of 0 at 20:32:38.<br>MPI 0: MPI_Test of 0 at 20:32:38.<br>MPI 0: MPI_Test of 0 at 20:32:38.<br>MPI 0: MPI_Test of 0 at 20:32:38.<br>MPI 0: MPI_Test of 0 at 20:32:38.<br>MPI 0: MPI_Test of 0 at 20:32:38.<br>MPI 0: MPI_Test of 0 at 20:32:38.<br>MPI 0: MPI_Test of 0 at 20:32:38.<br>MPI 0: MPI_Test of 0 at 20:32:38.<br>MPI 0: MPI_Test of 0 at 20:32:39.<br>MPI 0: MPI_Test of 0 at 20:32:39.<br>MPI 0: MPI_Test of 0 at 20:32:39.<br>MPI 0: MPI_Test of 0 at 20:32:39.<br>MPI 0: MPI_Test of 0 at 20:32:39.<br>MPI 0: MPI_Test of 0 at 20:32:39.<br>MPI 1: Recv of 0 finished at 20:32:39.<br>MPI 0: MPI_Test of 0 at 20:32:39.<br>MPI 0: Isend of 0 finished at 20:32:39.<br><br></div></div></div></div>
</div></div></div></div>
</blockquote></div>
</div></div><br>_______________________________________________<br>
users mailing list<br>
<a>users@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/05/29086.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/05/29086.php</a><br></blockquote></div><br></div></div>
</blockquote></div>
</div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/05/29097.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/05/29097.php</a><br></blockquote></div><br></div></div>

