<html><head><meta http-equiv="Content-Type" content="text/html charset=iso-8859-1"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">Just looking at this output, it would appear that Windows is configured in a way that prevents the procs from connecting to each other via TCP while on the same node, and shared memory is disqualifying itself - which leaves no way for two procs on the same node to communicate.<div class=""><br class=""></div><div class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Jun 7, 2016, at 12:16 PM, Roth, Christopher &lt;<a href="mailto:CRoth@aer.com" class="">CRoth@aer.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><div style="font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; direction: ltr; font-family: Tahoma; font-size: 12pt;" class=""><div style="direction: ltr; font-family: Tahoma; font-size: 12pt;" class="">I have developed a set of C++ MPI programs for performing a series of scientific calculations.&nbsp; The master 'scheduler' program spawns off sets of parallelized 'executor' programs using the MPI_Comm_spawn routine; these executors communicate back and forth with the scheduler (only small amounts of information) via MPI_Bcast, MPI_Recv and MPI_Send routines (the 'C' library versions).<br class=""><br class="">This software was originally developed on a multi-core Linux machine using OpenMpi v1.5.2, and works extremely well; now I'm attempting to port it to multi-core Windows 7 machine, using Visual Studios 2012 and the precompiled OpenMpi v1.6.2 Windows release.&nbsp; It all compiles and links properly under VS2012.<br class="">When attempting to run this software on the Windows machine, the scheduler program is able to spawn off the executor programs as intended, but everything chokes when scheduler sends its initial broadcast.&nbsp; There is slightly different behavior when launching the scheduler via 'mpirun', or just by itself, as shown in the logs below:<br class="">(the warning about the 127.0.0.1 address is benign - there is no loopback on Windows)<br class=""><br class="">C:\Users\cjr\Desktop\mpi_demo&gt;mpirun -np 1 mpi_scheduler.exe<br class="">&nbsp;scheduler: MPI_Init<br class="">--------------------------------------------------------------------------<br class="">WARNING: An invalid value was given for btl_tcp_if_exclude.&nbsp; This<br class="">value will be ignored.<br class=""><br class="">&nbsp; Local host: sweet1<br class="">&nbsp; Value:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 127.0.0.1/8<br class="">&nbsp; Message:&nbsp;&nbsp;&nbsp; Did not find interface matching this subnet<br class="">--------------------------------------------------------------------------<br class="">--&gt;MPI_COMM_WORLD size = 1<br class="">parent: MPI_UNIVERSE_SIZE = 1<br class="">scheduler: calling MPI_Comm_spawn for 4 instances of 'mpi_executor.exe'<br class="">&nbsp;executor: MPI_Init<br class="">&nbsp;executor: MPI_Init<br class="">&nbsp;executor: MPI_Init<br class="">&nbsp;executor: MPI_Init<br class="">[sweet1][[20141,1],0][..\..\..\openmpi-1.6.2\ompi\mca\btl\tcp\btl_tcp_proc.c:128:..\..\..\openmpi-1.6.2\ompi\mca\btl\tcp<br class="">\btl_tcp_proc.c] mca_base_modex_recv: failed with return value=-13<br class="">--------------------------------------------------------------------------<br class="">At least one pair of MPI processes are unable to reach each other for<br class="">MPI communications.&nbsp; This means that no Open MPI device has indicated<br class="">that it can be used to communicate between these processes.&nbsp; This is<br class="">an error; Open MPI requires that all MPI processes be able to reach<br class="">each other.&nbsp; This error can sometimes be the result of forgetting to<br class="">specify the "self" BTL.<br class=""><br class="">&nbsp; Process 1 ([[20141,1],0]) is on host: sweet1<br class="">&nbsp; Process 2 ([[20141,2],0]) is on host: sweet1<br class="">&nbsp; BTLs attempted: tcp sm self<br class=""><br class="">Your MPI job is now going to abort; sorry.<br class="">--------------------------------------------------------------------------<br class="">&nbsp;subtask rank = 1 out of 4<br class="">&nbsp;subtask rank = 2 out of 4<br class="">&nbsp;subtask rank = 0 out of 4<br class="">&nbsp;subtask rank = 3 out of 4<br class=""><br class="">scheduler: MPI_Comm_spawn completed<br class="">&nbsp;scheduler broadcasting function string length = 4<br class="">child: MPI_UNIVERSE_SIZE = 4<br class="">child: MPI_UNIVERSE_SIZE = 4<br class="">child: MPI_UNIVERSE_SIZE = 4<br class="">child: MPI_UNIVERSE_SIZE = 4<br class="">Proc0 wait for first broadcast<br class="">Proc1 wait for first broadcast<br class="">Proc2 wait for first broadcast<br class="">Proc3 wait for first broadcast<br class="">[sweet1:6800] *** An error occurred in MPI_Bcast<br class="">[sweet1:6800] *** on communicator<br class="">[sweet1:6800] *** MPI_ERR_INTERN: internal error<br class="">[sweet1:6800] *** MPI_ERRORS_ARE_FATAL: your MPI job will now abort<br class="">[sweet1:02412] [[20141,0],0]-[[20141,1],0] mca_oob_tcp_msg_recv: readv failed: Unknown error (108)<br class="">[sweet1:02412] 4 more processes have sent help message help-mpi-btl-tcp.txt / invalid if_inexclude<br class="">[sweet1:02412] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages<br class="">--------------------------------------------------------------------------<br class="">WARNING: A process refused to die!<br class=""><br class="">Host: sweet1<br class="">PID:&nbsp; 524<br class=""><br class="">This process may still be running and/or consuming resources.<br class=""><br class="">--------------------------------------------------------------------------<br class="">[sweet1:02412] [[20141,0],0]-[[20141,2],1] mca_oob_tcp_msg_recv: readv failed: Unknown error (108)<br class="">[sweet1:02412] [[20141,0],0]-[[20141,2],0] mca_oob_tcp_msg_recv: readv failed: Unknown error (108)<br class="">[sweet1:02412] [[20141,0],0]-[[20141,2],2] mca_oob_tcp_msg_recv: readv failed: Unknown error (108)<br class="">--------------------------------------------------------------------------<br class="">mpirun has exited due to process rank 0 with PID 488 on<br class="">node sweet1 exiting improperly. There are two reasons this could occur:<br class=""><br class="">1. this process did not call "init" before exiting, but others in<br class="">the job did. This can cause a job to hang indefinitely while it waits<br class="">for all processes to call "init". By rule, if one process calls "init",<br class="">then ALL processes must call "init" prior to termination.<br class=""><br class="">2. this process called "init", but exited without calling "finalize".<br class="">By rule, all processes that call "init" MUST call "finalize" prior to<br class="">exiting or it will be considered an "abnormal termination"<br class=""><br class="">This may have caused other processes in the application to be<br class="">terminated by signals sent by mpirun (as reported here).<br class="">--------------------------------------------------------------------------<br class="">[sweet1:02412] 3 more processes have sent help message help-odls-default.txt / odls-default:could-not-kill<br class=""><br class="">C:\Users\cjr\Desktop\mpi_demo&gt;<br class=""><br class="">====================================================<br class=""><br class="">C:\Users\cjr\Desktop\mpi_demo&gt;mpi_scheduler.exe<br class="">&nbsp;scheduler: MPI_Init<br class="">--------------------------------------------------------------------------<br class="">WARNING: An invalid value was given for btl_tcp_if_exclude.&nbsp; This<br class="">value will be ignored.<br class=""><br class="">&nbsp; Local host: sweet1<br class="">&nbsp; Value:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 127.0.0.1/8<br class="">&nbsp; Message:&nbsp;&nbsp;&nbsp; Did not find interface matching this subnet<br class="">--------------------------------------------------------------------------<br class="">--&gt;MPI_COMM_WORLD size = 1<br class="">parent: MPI_UNIVERSE_SIZE = 1<br class="">scheduler: calling MPI_Comm_spawn for 4 instances of 'mpi_executor.exe'<br class="">&nbsp;executor: MPI_Init<br class="">&nbsp;executor: MPI_Init<br class="">&nbsp;executor: MPI_Init<br class="">&nbsp;executor: MPI_Init<br class="">[sweet1:04400] 1 more process has sent help message help-mpi-btl-tcp.txt / invalid if_inexclude<br class="">[sweet1:04400] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages<br class="">&nbsp;subtask rank = 2 out of 4<br class="">&nbsp;subtask rank = 1 out of 4<br class="">&nbsp;subtask rank = 0 out of 4<br class="">&nbsp;subtask rank = 3 out of 4<br class=""><br class="">scheduler: MPI_Comm_spawn completed<br class="">&nbsp;scheduler broadcasting function string length = 4<br class=""><br class="">child: MPI_UNIVERSE_SIZE = 4<br class="">child: MPI_UNIVERSE_SIZE = 4<br class="">child: MPI_UNIVERSE_SIZE = 4<br class="">child: MPI_UNIVERSE_SIZE = 4<br class="">Proc0 wait for first broadcast<br class="">Proc1 wait for first broadcast<br class="">Proc2 wait for first broadcast<br class="">Proc3 wait for first broadcast<br class=""><br class="">[sweet1:04400] 3 more processes have sent help message help-mpi-btl-tcp.txt / invalid if_inexclude<br class=""><br class="">&lt;&lt;&lt;&lt;***mpi_executor.exe processes are running, but 'hung' while wating for first broadcast***&gt;&gt;&gt;&gt;<br class="">&lt;&lt;&lt;&lt;***manually killed one of the 'mpi_executor.exe' processes; others subsequently exited***&gt;&gt;&gt;&gt;<br class=""><br class="">[sweet1:04400] [[22257,0],0]-[[22257,2],3] mca_oob_tcp_msg_recv: readv failed: Unknown error (108)<br class="">--------------------------------------------------------------------------<br class="">WARNING: A process refused to die!<br class=""><br class="">Host: sweet1<br class="">PID:&nbsp; 568<br class=""><br class="">This process may still be running and/or consuming resources.<br class=""><br class="">--------------------------------------------------------------------------<br class="">[sweet1:04400] [[22257,0],0]-[[22257,2],0] mca_oob_tcp_msg_recv: readv failed: Unknown error (108)<br class="">[sweet1:04400] [[22257,0],0]-[[22257,2],1] mca_oob_tcp_msg_recv: readv failed: Unknown error (108)<br class="">[sweet1:04400] 2 more processes have sent help message help-odls-default.txt / odls-default:could-not-kill<br class=""><br class="">C:\Users\cjr\Desktop\mpi_demo&gt;<br class=""><br class="">================================================<br class=""><br class="">The addition of the mpirun option "-mca btl_tcp_if_exclude none" eliminates the benign 127.0.0.1 warning; the option "-mca btl_base_verbose 100" adds output that verifies that the tcp, sm and self btl modules are successfully initialized - nothing else seems to be amiss!<br class="">I have also tested this with the firewall completely disabled on the Windows machine, with no change in behavior.<br class=""><br class="">I have been unable to determine what the error codes indicate, and am puzzled why the behavior is different when using the 'mpirun' launcher.<br class="">I have attached the prototype scheduler and executor program source code files, as well as files containing the Windows installation ompi information.<br class=""><br class="">Please help me figure out what is needed to enable this MPI communication.<br class=""><br class="">Thanks,<br class="">CJ Roth<br class=""></div></div><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><hr style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><font face="Arial" color="Gray" size="1" style="font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><br class="">This email is intended solely for the recipient. It may contain privileged, proprietary or confidential information or material. If you are not the intended recipient, please delete this email and any attachments and notify the sender of the error.<br class=""></font><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class=""></span><span id="cid:E59D0D70-4035-4F3B-8716-3D8C811BA301">&lt;mpi_scheduler.cpp&gt;</span><span id="cid:01F750A5-28D4-4D9E-84BF-91A8C9EC47CC">&lt;mpi_executor.cpp&gt;</span><span id="cid:3396E384-451C-4648-92FD-8B6D1F16F19E">&lt;ompi_info-all.txt&gt;</span><span id="cid:170EA87A-7BFD-4AEE-8AAF-544A82C15EEC">&lt;ompi_btl_info.txt&gt;</span><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">_______________________________________________</span><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">users mailing list</span><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><a href="mailto:users@open-mpi.org" style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class="">users@open-mpi.org</a><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">Subscription:<span class="Apple-converted-space">&nbsp;</span></span><a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">Link to this post:<span class="Apple-converted-space">&nbsp;</span></span><a href="http://www.open-mpi.org/community/lists/users/2016/06/29395.php" style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class="">http://www.open-mpi.org/community/lists/users/2016/06/29395.php</a></div></blockquote></div><br class=""></div></body></html>
