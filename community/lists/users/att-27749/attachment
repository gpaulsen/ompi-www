<div dir="ltr"><span style="font-size:12.8px">Xing Feng,</span><br><div><span style="font-size:12.8px"><br></span></div><div><span style="font-size:12.8px">A more focused (and certainly </span><span style="font-size:12.8px">more detailed) </span><span style="font-size:12.8px">analysis of the cost of different algorithms for collective communications can be found in [1], and more recently in [2].</span></div><div><span style="font-size:12.8px"><br></span></div><div><span style="font-size:12.8px">  George.</span></div><div><span style="font-size:12.8px"><br></span></div><div><span style="font-size:12.8px">[1] <a href="http://icl.cs.utk.edu/projectsfiles/rib/pubs/Pjesivac-Grbovic_PMEO-PDS05.pdf">http://icl.cs.utk.edu/projectsfiles/rib/pubs/Pjesivac-Grbovic_PMEO-PDS05.pdf</a></span></div><div><span style="font-size:12.8px">[2] <a href="https://www.cs.utexas.edu/~echan/vpapers/CCPE2007.pdf">https://www.cs.utexas.edu/~echan/vpapers/CCPE2007.pdf</a></span></div><div><span style="font-size:12.8px"><br></span></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Wed, Sep 30, 2015 at 3:07 AM, Marc-Andre Hermanns <span dir="ltr">&lt;<a href="mailto:m.a.hermanns@grs-sim.de" target="_blank">m.a.hermanns@grs-sim.de</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Dear Xing Feng,<br>
<br>
there are different algorithms to implement collective communication<br>
patterns. Next to general Big-O complexity the concrete complexity<br>
also depends on the network topology, message length, etc..<br>
<br>
Therefore many MPI implementations switch between different algorithms<br>
depending on the concrete communication parameters in a call.<br>
<br>
A colleague of mine investigated some MPI implementations (though not<br>
OpenMPI) [1]. There you can see how different MPI implementations<br>
(IBM, ParaStation, Cray) scale differently for a selection of<br>
collective calls. Maybe that helps a little in understanding the<br>
performance of your application.<br>
<br>
Cheers,<br>
Marc-Andre<br>
<br>
[1] <a href="http://dl.acm.org/citation.cfm?doid=2751205.2751216" rel="noreferrer" target="_blank">http://dl.acm.org/citation.cfm?doid=2751205.2751216</a><br>
<div><div class="h5"><br>
<br>
On 30.09.15 07:53, XingFENG wrote:<br>
&gt; Hi, every one,<br>
&gt;<br>
&gt; I am working with open-mpi. When I tried to analyse performance of my<br>
&gt; programs, I find it is hard to understand the communication complexity<br>
&gt; of MPI routines.<br>
&gt;<br>
&gt; I have found some page on Internet such as<br>
&gt; <a href="http://stackoverflow.com/questions/10625643/mpi-communication-complexity" rel="noreferrer" target="_blank">http://stackoverflow.com/questions/10625643/mpi-communication-complexity</a><br>
&gt;<br>
&gt;<br>
&gt; This indicates that communication complexity of broadcasting an<br>
&gt; integer is O(log P) where P is the number of processes. But is it<br>
&gt; correct on different MPI implementations( OMPI, MPICH, etc.)? Is there<br>
&gt; an official document discussing such complexity?<br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Best Regards.<br>
&gt; ---<br>
&gt; Xing FENG<br>
&gt; PhD Candidate<br>
&gt; Database Research Group<br>
&gt;<br>
&gt; School of Computer Science and Engineering<br>
&gt; University of New South Wales<br>
&gt; NSW 2052, Sydney<br>
&gt;<br>
&gt; Phone: <a href="tel:%28%2B61%29%20413%20857%20288" value="+61413857288">(+61) 413 857 288</a><br>
&gt;<br>
&gt;<br>
</div></div>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/09/27719.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/09/27719.php</a><br>
&gt;<br>
<br>
--<br>
Marc-Andre Hermanns<br>
Jülich Aachen Research Alliance,<br>
High Performance Computing (JARA-HPC)<br>
German Research School for Simulation Sciences GmbH<br>
<br>
Schinkelstrasse 2<br>
52062 Aachen<br>
Germany<br>
<br>
Phone: <a href="tel:%2B49%202461%2061%202509" value="+492461612509">+49 2461 61 2509</a><br>
Fax: <a href="tel:%2B49%20241%2080%206%2099753" value="+4924180699753">+49 241 80 6 99753</a><br>
<a href="http://www.grs-sim.de/parallel" rel="noreferrer" target="_blank">www.grs-sim.de/parallel</a><br>
email: <a href="mailto:m.a.hermanns@grs-sim.de">m.a.hermanns@grs-sim.de</a><br>
<br>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/09/27720.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/09/27720.php</a><br></blockquote></div><br></div>

