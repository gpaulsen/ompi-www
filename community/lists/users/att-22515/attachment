<html><head><meta http-equiv="Content-Type" content="text/html charset=iso-8859-1"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">It only has to come after MPI_Init *if* you are telling mpirun to bind you as well. Otherwise, you could just not tell mpirun to bind (it doesn't by default) and then bind anywhere, anytime you like<div><br></div><div><br><div><div>On Aug 18, 2013, at 3:24 PM, Siddhartha Jana &lt;<a href="mailto:siddharthajana24@gmail.com">siddharthajana24@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr"><br><div class="gmail_extra"><div class="gmail_quote"><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">A process can always change its binding by "re-binding" to wherever it wants after MPI_Init completes.</div>

</blockquote><div>Noted. Thanks. I guess the important thing that I wanted to know was that the binding needs to happen *after* MPI_Init() completes.&nbsp;</div><div><br></div><div>Thanks all</div><div><br></div><div>-- Siddhartha</div>

<div><br></div><div>&nbsp;</div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word"><div class="h5"><div><br></div><div><br><div><div>On Aug 18, 2013, at 9:38 AM, Siddhartha Jana &lt;<a href="mailto:siddharthajana24@gmail.com" target="_blank">siddharthajana24@gmail.com</a>&gt; wrote:</div>

<br><blockquote type="cite"><div dir="ltr"><div>Firstly, I would like my program to dynamically assign it self to one of the cores it pleases and remain bound to it until it later reschedules itself.</div><i><div><i><br>
</i></div>
Ralph Castain wrote:</i><div>

<div><i>&gt;&gt; "If you just want mpirun to respect an external cpuset limitation, it already does so when binding - it will bind within the external limitation"</i><br></div><div><br></div><div>In my case, the limitation is enforced "internally", by the application once in begins execution. I enforce this during program execution, after the mpirun has finished "binding within the external limitation".&nbsp;</div>



<div><br></div><div><br></div><div><i>Brice Goglin said</i>:</div><div><i>&gt;&gt;&nbsp;<span style="font-family:arial,sans-serif;font-size:13px">&nbsp;"</span><span style="font-family:arial,sans-serif;font-size:13px">MPI can bind at two different times: inside mpirun after ssh before running the actual program (this one would ignore your cpuset), later at MPI_Init inside your program (this one will ignore your cpuset only if you call MPI_Init before creating the cpuset)."</span></i></div>



<div><div><br></div><div>Noted. In that case, during program execution, whose binding is respected - mpirun's or MPI_Init()'s? From the above, is my understanding correct? That MPI_Init() will be responsible for the 2nd round of attempting to bind processes to cores and can override what mpirun or the programmer had enforced before its call (using hwloc/cpuset/sched_load_balance()<font color="#006000" face="monospace, courier" size="3"><i>&nbsp;</i></font>and other <i>compatible</i> cousins) ?&nbsp;</div>



<div><br></div><div><br></div><div>--------------------------------------------</div><div>If this is so, in my case the flow of events is thus:</div></div><div><br></div><div>1. mpirun binds an MPI process which is yet to begin execution. So mpirun says: "Bind to some core - A" (I don't use any hostfile/rankfile. but I do use the --bind-to-core flag)&nbsp;</div>



<div><br></div><div>2. Process begins execution on core A</div><div><br></div><div>3. I enforce: "Bind to core B". (we must remember, it is only at runtime that &nbsp;I know what core I want to be bound to and not while launching the processes using mpirun). So my process shifts over to core B</div>



<div><br></div><div>4. MPI_Init() once again honors rankfile mapping(if any, default policy, otherwise ) and rebinds my process to core A</div><div><br></div><div>5. process finished execution and calls MPI_Finalize(), all the time on core A</div>



<div><br></div><div>6. mpirun exits</div><div>--------------------------------------------<br></div><div><br></div><div>So if I place step-3 above after step-4, my request will hold for the rest of the execution. Please do let me know, if my understanding is correct.</div>



<div><br></div><div>Thanks for all the help</div><div><br></div><div>Sincerely,</div><div>Siddhartha Jana</div><div>HPCTools</div><div><br></div><div><br></div><div><br></div><div><br></div><div><br></div><div>

<br></div><div class="gmail_extra"><br><br><div class="gmail_quote">On 18 August 2013 10:49, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>



<blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div style="word-wrap:break-word">If you require that a specific rank go to a specific core, then use the rankfile mapper - you can see explanations on the syntax in "man mpirun"<div>



<br></div><div>If you just want mpirun to respect an external cpuset limitation, it already does so when binding - it will bind within the external limitation</div><div><div><br></div><div><br><div><div>On Aug 18, 2013, at 6:09 AM, Siddhartha Jana &lt;<a href="mailto:siddharthajana24@gmail.com" target="_blank">siddharthajana24@gmail.com</a>&gt; wrote:</div>



<br><blockquote type="cite"><div dir="ltr">So my question really boils down to:<div><span style="font-family:arial,sans-serif;font-size:13px">How does one ensure that mpirun launches the processes on the "specific" cores that are expected of them to be bound to.&nbsp;</span></div>





<div><span style="font-family:arial,sans-serif;font-size:13px">As I mentioned, i</span><span style="font-family:arial,sans-serif;font-size:13px">f there were a way to specify the cores through the hostfile, this problem should be solved.&nbsp;</span></div>





<div><div class="gmail_extra"><br></div><div class="gmail_extra">Thanks for all the quick replies,</div><div class="gmail_extra">-- Sid<br><br><div class="gmail_quote">On 18 August 2013 09:04, Siddhartha Jana <span dir="ltr">&lt;<a href="mailto:siddharthajana24@gmail.com" target="_blank">siddharthajana24@gmail.com</a>&gt;</span> wrote:<br>





<blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div dir="ltr">Thanks John. But I have an incredibly small system. 2 nodes - 16 cores each.<div>





2-4 MPI processes. :-)<br></div><div><br></div><div class="gmail_extra"><div class="gmail_quote"><div>On 18 August 2013 09:03, John Hearns <span dir="ltr">&lt;<a href="mailto:hearnsj@googlemail.com" target="_blank">hearnsj@googlemail.com</a>&gt;</span> wrote:<br>






</div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div><p>You really should install a job scheduler.<br>






There are free versions.</p><p>I'm not sure about cpuset support in Gridengine. Anyone?</p>
<br></div><div>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></blockquote></div><br></div></div>
</blockquote></div><br></div></div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>



</div><br></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div></div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>

</div><br></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
