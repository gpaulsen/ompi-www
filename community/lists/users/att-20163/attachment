<div>Hi</div><div><includetail><div>I want to install a 64-bit vision openmpi on a cluster. This cluster already have a 32-bit openmpi-1.4.3 installed on it. I'm not administrator. So I can't uninstall the previous version. an I installed the 64 bit openmpi in my homo directory.</div><div>the configure and make step seem errorless.</div><div>when I set the environment variables and use the new installed openmpi(version 1.6.1, 1.4.3, 1.5.5 tested) to compile and run example files. I get the following error:</div><div><br></div><div><br></div><div>**********************************************************************************************</div><div><div>zhongc@node100:~/openmpi-1.4.5-install/examples&gt; make</div><div>mpicc -g &nbsp; &nbsp;hello_c.c &nbsp; -o hello_c</div><div>mpicc -g &nbsp; &nbsp;ring_c.c &nbsp; -o ring_c</div><div>mpicc -g &nbsp; &nbsp;connectivity_c.c &nbsp; -o connectivity_c</div><div>make[1]: Entering directory `/dawnfs/users/zhongc/openmpi-1.4.5-install/examples'</div><div>mpic++ -g &nbsp; &nbsp;hello_cxx.cc &nbsp; -o hello_cxx</div><div>mpic++ -g &nbsp; &nbsp;ring_cxx.cc &nbsp; -o ring_cxx</div><div>make[1]: Leaving directory `/dawnfs/users/zhongc/openmpi-1.4.5-install/examples'</div><div>make[1]: Entering directory `/dawnfs/users/zhongc/openmpi-1.4.5-install/examples'</div><div>mpif77 -g hello_f77.f -o hello_f77</div><div>mpif77 -g ring_f77.f -o ring_f77</div><div>make[1]: Leaving directory `/dawnfs/users/zhongc/openmpi-1.4.5-install/examples'</div><div>make[1]: Entering directory `/dawnfs/users/zhongc/openmpi-1.4.5-install/examples'</div><div>mpif90 -g hello_f90.f90 -o hello_f90</div><div>hello_f90.f90(17): error #6285: There is no matching specific subroutine for this generic subroutine call. &nbsp; [MPI_INIT]</div><div>&nbsp; &nbsp; call MPI_INIT(ierr)</div><div>---------^</div><div>hello_f90.f90(18): error #6285: There is no matching specific subroutine for this generic subroutine call. &nbsp; [MPI_COMM_RANK]</div><div>&nbsp; &nbsp; call MPI_COMM_RANK(MPI_COMM_WORLD, rank, ierr)</div><div>---------^</div><div>hello_f90.f90(19): error #6285: There is no matching specific subroutine for this generic subroutine call. &nbsp; [MPI_COMM_SIZE]</div><div>&nbsp; &nbsp; call MPI_COMM_SIZE(MPI_COMM_WORLD, size, ierr)</div><div>---------^</div><div>hello_f90.f90(21): error #6285: There is no matching specific subroutine for this generic subroutine call. &nbsp; [MPI_FINALIZE]</div><div>&nbsp; &nbsp; call MPI_FINALIZE(ierr)</div><div>---------^</div><div>compilation aborted for hello_f90.f90 (code 1)</div><div>make[1]: *** [hello_f90] Error 1</div><div>make[1]: Leaving directory `/dawnfs/users/zhongc/openmpi-1.4.5-install/examples'</div><div>make: *** [all] Error 2</div></div><div><br></div><div>*****************************************************************************************************</div><div><br></div><div><br></div><div>and if i try to run the comiled program I will get the following result£º</div><div>****************************************************************************************************</div><div>zhongc@node100:~/openmpi-1.4.5-install/examples&gt; mpirun -np 4 hello_f77</div><div>&nbsp;Hello, world, I am &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 &nbsp;of &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4</div><div>&nbsp;Hello, world, I am &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 &nbsp;of &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4</div><div>&nbsp;Hello, world, I am &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 &nbsp;of &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4</div><div>&nbsp;Hello, world, I am &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 &nbsp;of &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4</div><div>&nbsp;zhongc@node100:~/openmpi-1.4.5-install/examples&gt; mpirun -np 4 ring_c</div><div>Process 0 sending 10 to 1, tag 201 (4 processes in ring)</div><div>Process 0 sent to 1</div><div>Process 0 decremented value: 9</div><div>Process 0 decremented value: 8</div><div>Process 0 decremented value: 7</div><div>Process 0 decremented value: 6</div><div>Process 0 decremented value: 5</div><div>Process 0 decremented value: 4</div><div>Process 0 decremented value: 3</div><div>Process 0 decremented value: 2</div><div>Process 0 decremented value: 1</div><div>Process 0 decremented value: 0</div><div>Process 0 exiting</div><div>Process 1 exiting</div><div>Process 2 exiting</div><div>Process 3 exiting</div><div>*************************************************************************************************</div><div><br></div><div>following are my environment set in ~/.bashrc</div><div>****************************************************************</div><div><div>export MPI_ROOT=/dawnfs/users/zhongc/openmpi-1.4.5</div><div>export LD_LIBRARY_PATH=/dawnfs/users/zhongc/openmpi-1.4.5/lib:$LD_LIBRARY_PATH</div><div>export LD_LIBRARY_PATH=/dawnfs/users/zhongc/openmpi-1.4.5:$LD_LIBRARY_PATH</div><div>export OMPI_MPIF77="ifort"</div><div>export OMPI_MPIFC="ifort"</div><div>export OMPI_MPICC="icc"</div><div>export OMPI_MPICXX="icc"</div><div>export PATH=$MPI_ROOT/bin:$PATH</div></div><div>******************************************************************</div><div><br></div><div>if I use the openmpi that originally installed in this cluster. every thing is fine:</div><div>***********************************************************************************</div><div><div>zhongc@node100:~/openmpi-1.4.5-install/examples&gt; which mpirun</div><div>/dawnfs/software/mpi/openmpi1.4.3-intel/bin/mpirun</div><div>zhongc@node100:~/openmpi-1.4.5-install/examples&gt; make clean</div><div>rm -f hello_c hello_cxx hello_f77 hello_f90 ring_c ring_cxx ring_f77 ring_f90 connectivity_c *~ *.o</div><div>zhongc@node100:~/openmpi-1.4.5-install/examples&gt; make</div><div>mpicc -g &nbsp; &nbsp;hello_c.c &nbsp; -o hello_c</div><div>mpicc -g &nbsp; &nbsp;ring_c.c &nbsp; -o ring_c</div><div>mpicc -g &nbsp; &nbsp;connectivity_c.c &nbsp; -o connectivity_c</div><div>make[1]: Entering directory `/dawnfs/users/zhongc/openmpi-1.4.5-install/examples'</div><div>mpic++ -g &nbsp; &nbsp;hello_cxx.cc &nbsp; -o hello_cxx</div><div>mpic++ -g &nbsp; &nbsp;ring_cxx.cc &nbsp; -o ring_cxx</div><div>make[1]: Leaving directory `/dawnfs/users/zhongc/openmpi-1.4.5-install/examples'</div><div>make[1]: Entering directory `/dawnfs/users/zhongc/openmpi-1.4.5-install/examples'</div><div>mpif77 -g hello_f77.f -o hello_f77</div><div>mpif77 -g ring_f77.f -o ring_f77</div><div>make[1]: Leaving directory `/dawnfs/users/zhongc/openmpi-1.4.5-install/examples'</div><div>make[1]: Entering directory `/dawnfs/users/zhongc/openmpi-1.4.5-install/examples'</div><div>mpif90 -g hello_f90.f90 -o hello_f90</div><div>mpif90 -g ring_f90.f90 -o ring_f90</div><div>make[1]: Leaving directory `/dawnfs/users/zhongc/openmpi-1.4.5-install/examples'</div><div>zhongc@node100:~/openmpi-1.4.5-install/examples&gt; mpirun -np 4 hello_f77</div><div>&nbsp;Hello, world, I am &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1 &nbsp;of &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4</div><div>&nbsp;Hello, world, I am &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 &nbsp;of &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4</div><div>&nbsp;Hello, world, I am &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2 &nbsp;of &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4</div><div>&nbsp;Hello, world, I am &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3 &nbsp;of &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4</div><div>zhongc@node100:~/openmpi-1.4.5-install/examples&gt; mpirun -np 4 ring_c</div><div>Process 0 sending 10 to 1, tag 201 (4 processes in ring)</div><div>Process 0 sent to 1</div><div>Process 1 exiting</div><div>Process 2 exiting</div><div>Process 3 exiting</div><div>Process 0 decremented value: 9</div><div>Process 0 decremented value: 8</div><div>Process 0 decremented value: 7</div><div>Process 0 decremented value: 6</div><div>Process 0 decremented value: 5</div><div>Process 0 decremented value: 4</div><div>Process 0 decremented value: 3</div><div>Process 0 decremented value: 2</div><div>Process 0 decremented value: 1</div><div>Process 0 decremented value: 0</div><div>Process 0 exiting</div></div><div>********************************************************************************</div><div><br></div><div>version I have tried 1.4.5, 1.6.1. 1.5.5</div><div>here are the configures I have tried</div><div>**************************************************</div><div><div>--prefix=/dawnfs/users/zhongc/openmpi-1.4.5 CXX=icpc CC=icc F77=ifort FC=ifort FFLAGS=-i8 FCFLAGS=-i8</div><div>--prefix=/dawnfs/users/zhongc/openmpi-1.4.5 CXX=icpc CC=icc F77=ifort FC=ifort FFLAGS=-i8 FCFLAGS=-i8 --without-tm --without-lsf</div><div>--prefix=/dawnfs/users/zhongc/openmpi-1.4.5 CXX=icpc CC=icc F77=ifort FC=ifort FFLAGS=-i8 FCFLAGS=-i8 --enable-static --disable-shared --with-mpi-f90-size=large</div><div>--prefix=/dawnfs/users/zhongc/openmpi-1.4.5 CXX=g++ CC=gcc F77=gfortran FC=gfortran FFLAGS="-m64 -fdefault-integer-8" FCFLAGS="-m64 -fdefault-integer-8" CFLAGS=-m64 CXXFLAGS=-m64</div></div><div><font color="#323456" face="tahoma, MS Shell Dlg, Arial, Helvetica, verd"><span style="line-height: 25px;">*********************************************************</span></font></div><div><font color="#323456" face="tahoma, MS Shell Dlg, Arial, Helvetica, verd"><span style="line-height: 25px;"><br></span></font></div><div><font color="#323456" face="tahoma, MS Shell Dlg, Arial, Helvetica, verd"><span style="line-height: 25px;">gcc or icc i have used</span></font></div><div><font color="#323456" face="tahoma, MS Shell Dlg, Arial, Helvetica, verd"><span style="line-height: 25px;">**********************************************************************************</span></font></div><div><font color="#323456" face="tahoma, MS Shell Dlg, Arial, Helvetica, verd"><span style="line-height: 25px;"><div>zhongc@node100:~/openmpi-1.4.5-install&gt; gcc -v</div><div>Using built-in specs.</div><div>COLLECT_GCC=gcc</div><div>COLLECT_LTO_WRAPPER=/dawnfs/users/zhongc/gcc-4.7.0/libexec/gcc/x86_64-unknown-linux-gnu/4.7.0/lto-wrapper</div><div>Target: x86_64-unknown-linux-gnu</div><div>Configured with: ../configure --prefix=/dawnfs/users/zhongc/gcc-4.7.0</div><div>Thread model: posix</div><div>gcc version 4.7.0 (GCC) </div><div>zhongc@node100:~/openmpi-1.4.5-install&gt; icc -V</div><div>Intel(R) C Intel(R) 64 Compiler XE for applications running on Intel(R) 64, Version 12.0.3.174 Build 20110309</div><div>Copyright (C) 1985-2011 Intel Corporation. &nbsp;All rights reserved.</div><div>FOR NON-COMMERCIAL USE ONLY</div><div>******************************************************************************</div></span></font></div><div><font color="#323456" face="tahoma, MS Shell Dlg, Arial, Helvetica, verd"><span style="line-height: 25px;"><br></span></font></div><div><font color="#323456" face="tahoma, MS Shell Dlg, Arial, Helvetica, verd"><span style="line-height: 25px;">the ompi_info --all &nbsp;output of system openmpi (name old-ompi-info) and my openmpi (name new-ompi-info) and config.log are in the attachment</span></font></div><div><font color="#323456" face="tahoma, MS Shell Dlg, Arial, Helvetica, verd"><span style="line-height: 25px;"><br></span></font></div><div><font color="#323456" face="tahoma, MS Shell Dlg, Arial, Helvetica, verd"><span style="line-height: 25px;">this have tortured me for several days, can anybody helpme ?</span></font></div></includetail></div>