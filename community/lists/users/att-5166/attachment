<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
</head>
<body bgcolor="#ffffff" text="#000000">
Hi everybody!<br>
<br>
I try to get OpenMPI and Globus to cooperate. These are the steps i
executed in order to get OpenMPI working:<br>
<br>
<ol>
  <li>export PATH=/opt/openmpi/bin/:$PATH</li>
  <li>/opt/globus/setup/globus/setup-globus-job-manager-fork<br>
checking for mpiexec... /opt/openmpi/bin//mpiexec<br>
checking for mpirun... /opt/openmpi/bin//mpirun<br>
find-fork-tools: creating ./config.status<br>
config.status: creating fork.pm</li>
  <li>restart VDT (includes GRAM, WSGRAM, mysql, rls...)</li>
</ol>
As you can see the necessary OpenMPI-executables are recognized
correctly by setup-globus-job-manager-fork. But when i actually try to
execute a simple mpi-program using globus-job-run i get this:<br>
<br>
globus-job-run localhost -x '(jobType=mpi)' -np 2 -s ./hypercube 0<br>
[hydra:10168] [0,0,0] ORTE_ERROR_LOG: Error in file
runtime/orte_init_stage1.c at line 312<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here's some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
  orte_pls_base_select failed<br>
  --&gt; Returned value -1 instead of ORTE_SUCCESS<br>
<br>
--------------------------------------------------------------------------<br>
[hydra:10168] [0,0,0] ORTE_ERROR_LOG: Error in file
runtime/orte_system_init.c at line 42<br>
[hydra:10168] [0,0,0] ORTE_ERROR_LOG: Error in file runtime/orte_init.c
at line 52<br>
--------------------------------------------------------------------------<br>
Open RTE was unable to initialize properly.  The error occured while<br>
attempting to orte_init().  Returned value -1 instead of ORTE_SUCCESS.<br>
--------------------------------------------------------------------------<br>
<br>
The MPI-program itself is okey:<br>
<br>
which mpirun &amp;&amp; mpirun -np 2 hypercube 0<br>
/opt/openmpi/bin/mpirun<br>
Process 0 received broadcast message 'MPI_Broadcast with hypercube
topology' from Process 0<br>
Process 1 received broadcast message 'MPI_Broadcast with hypercube
topology' from Process 0<br>
<br>
<br>
>From what i read in the mailing list i think that something is wrong
with the pls and globus. But i have no idea what could be wrong not to
speak of how it could be fixed ;). so if someone would have an idea how
this could be fixed, i'd be glad to hear it.<br>
<br>
Regards,<br>
<br>
Christoph<br>
</body>
</html>

