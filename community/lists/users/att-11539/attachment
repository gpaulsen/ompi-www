<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta content="text/html;charset=ISO-8859-1" http-equiv="Content-Type">
</head>
<body bgcolor="#ffffff" text="#000000">
Hi,<br>
<br>
Thanks Reuti. These links were very useful when I did the integration
of BLCR with SGE. I will review them to check if there is more useful
information.<br>
<br>
Regards,<br>
Sergio<br>
<br>
Reuti escribi&oacute;:
<blockquote
 cite="mid:C30CE0C4-3BDA-4489-8EC0-2F90C1C870F8@staff.uni-marburg.de"
 type="cite">Hi,
  <br>
  <br>
no, I never tried Open MPI's checkpointing. But there are two Howto's
from which you may get some ideas to integrate it with SGE:
  <br>
  <br>
<a class="moz-txt-link-freetext" href="http://gridengine.sunsource.net/howto/checkpointing.html">http://gridengine.sunsource.net/howto/checkpointing.html</a>
  <br>
<a class="moz-txt-link-freetext" href="http://gridengine.sunsource.net/howto/APSTC-TB-2004-005.pdf">http://gridengine.sunsource.net/howto/APSTC-TB-2004-005.pdf</a> (but Open
MPI's checkpointing seems more to be like Condor's, as you don't have
to deal with any process list on your own AFAIK)
  <br>
  <br>
Included is also an example to integrate SGE with the Condor
checkpointing library in standalone mode.
  <br>
  <br>
Purpose of the checkpointing interface can be to copy the files from a
local (checkpointing) directory on a node to a shared space like
/home/checkpoint (the $SGE_CKPT_DIR [I even greated a subdirectory with
the $JOB_ID therein in the examples]). Later on the files can be copied
to the (maybe different) nodes again (either in a queue prolog or the
job script) when the job restarts.
  <br>
  <br>
  <br>
-- Reuti
  <br>
  <br>
  <br>
Am 14.12.2009 um 18:25 schrieb Sergio D&iacute;az:
  <br>
  <br>
  <blockquote type="cite">Hi Reuti,
    <br>
    <br>
Yes, I sent a job with SGE and I checkpointed the mpirun process, by
hand, entering into the mpi master node. Then I killed the job with
qdel and after that I did the ompi-restart.
    <br>
I will try to integrate with SGE creating a ckpt environment but I
think that it could be a bit difficult because:
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 -&nbsp; when I do checkpoint I can't specify a directory with a
name like checkpoint_jobid
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2 -&nbsp; I can't specify the scratch directory and I have to use
the /tmp instead of SGE's scratch directory.
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3 -&nbsp; I tried to restart the snapshot and it only works if I
use the same machinefile. That is, If the job ran in the c3-13 and
c3-14, I have to restart the job using a machinefile with these two
nodes.
    <br>
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [sdiaz@svgd ~]$ ompi-restart -v -machinefile
mpi_test/lanzar_pi3.sh.po3117822 ompi_global_snapshot_12554.ckpt
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [svgd.cesga.es:28836] Checking for the existence of
(/home/cesga/sdiaz/ompi_global_snapshot_12554.ckpt)
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [svgd.cesga.es:28836] Restarting from file
(ompi_global_snapshot_12554.ckpt)
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [svgd.cesga.es:28836]&nbsp;&nbsp;&nbsp; Exec in self
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tiempo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 110
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Process&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 :
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; compute-3-14.local
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; of&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; tiempo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 110
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Process&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 :
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; compute-3-13.local
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; of&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
--------------------------------------------------------------------------
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mpirun noticed that process rank 1 with PID
8477 on node compute-3-15 exited on signal 11 (Segmentation fault).
    <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
--------------------------------------------------------------------------
    <br>
    <br>
To solve problem 1, there is a feature opened by Josh.
(<a class="moz-txt-link-freetext" href="https://svn.open-mpi.org/trac/ompi/ticket/2098">https://svn.open-mpi.org/trac/ompi/ticket/2098</a>)
    <br>
To solve problem 2, there is a thread in which is talked ([OMPI users]
Changing location where checkpoints are saved) and also a bug opened by
Josh. <a class="moz-txt-link-freetext" href="https://svn.open-mpi.org/trac/ompi/ticket/2139">https://svn.open-mpi.org/trac/ompi/ticket/2139</a> . I think that it
could work... we will see.
    <br>
To solve problem 3, I didn't have time to search it. But if Josh or
anyone have an idea... please tell to us :-)
    <br>
    <br>
Reuti, Did you test it successfully? How do you solve these problems?
    <br>
    <br>
Regards,
    <br>
Sergio
    <br>
    <br>
    <br>
Reuti escribi&oacute;:
    <br>
    <blockquote type="cite"><br>
Hi,
      <br>
      <br>
Am 14.12.2009 um 17:05 schrieb Sergio D&iacute;az:
      <br>
      <br>
      <blockquote type="cite">I got a successful checkpoint with a
fresh installation and without use the trunk. I can't understand why it
is working now and before I could do a successful restart... Maybe
there was something wrong in the openmpi installation and then the
metadata was created in a wrong way.
        <br>
I will test it more and also I will test the trunk.
        <br>
        <br>
Regards,
        <br>
Sergio
        <br>
        <br>
[sdiaz@compute-3-13 ~]$ ompi-restart -machinefile
mpi_test/lanzar_pi3.sh.po3117822 ompi_global_snapshot_12554.ckpt
        <br>
&nbsp;tiempo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 110
        <br>
&nbsp;Process&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 :
        <br>
&nbsp;compute-3-14.local
        <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; of&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2
        <br>
&nbsp;tiempo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 110
        <br>
&nbsp;Process&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 :
        <br>
&nbsp;compute-3-13.local
        <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; of&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2
        <br>
&nbsp;tiempo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 120
        <br>
&nbsp;Process&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 :
        <br>
&nbsp;compute-3-14.local
        <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; of&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2
        <br>
&nbsp;tiempo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 120
        <br>
&nbsp;Process&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 :
        <br>
&nbsp;compute-3-13.local
        <br>
...
        <br>
...
        <br>
        <br>
[sdiaz@compute-3-14 ~]$ ps auxf |grep sdiaz
        <br>
sdiaz&nbsp;&nbsp;&nbsp; 26273&nbsp; 0.0&nbsp; 0.0 34676 1668 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp; 15:58&nbsp;&nbsp; 0:00 orted
--daemonize -mca ess env -mca orte_ess_jobid 1739128832 -mca
        <br>
      </blockquote>
      <br>
in a Tight Integration into SGE the daemon should get the argument
--no-daemonize. Are you restarting a job on the command line, which ran
before under SGE's supervision?
      <br>
      <br>
-- Reuti
      <br>
      <br>
      <blockquote type="cite">orte_ess_vpid 1 -mca orte_ess_num_procs 2
--hnp-uri 1739128832.0;tcp://192.168.4.148:45551 -mca
mca_base_param_file_prefix ft-enable-cr -mca mca_base_param_file_path
/opt/cesga/openmpi-1.3.3_bis/share/openmpi/amca-param-sets:/home_no_usc/cesga/sdiaz
-mca mca_base_param_file_path_force /home_no_usc/cesga/sdiaz
        <br>
sdiaz&nbsp;&nbsp;&nbsp; 26274&nbsp; 0.1&nbsp; 0.0 15984&nbsp; 504 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sl&nbsp;&nbsp; 15:58&nbsp;&nbsp; 0:00&nbsp; \_
cr_restart
/home/cesga/sdiaz/ompi_global_snapshot_12554.ckpt/0/opal_snapshot_1.ckpt/ompi_blcr_context.26047
        <br>
sdiaz&nbsp;&nbsp;&nbsp; 26047&nbsp; 1.5&nbsp; 0.0 99460 3624 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sl&nbsp;&nbsp; 15:58&nbsp;&nbsp; 0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_
./pi3
        <br>
        <br>
[sdiaz@compute-3-13 ~]$ ps auxf |grep sdiaz
        <br>
root&nbsp;&nbsp;&nbsp;&nbsp; 12878&nbsp; 0.0&nbsp; 0.0 90260 3000 pts/0&nbsp;&nbsp;&nbsp; S&nbsp;&nbsp;&nbsp; 15:55&nbsp;&nbsp; 0:00&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
\_ su - sdiaz
        <br>
sdiaz&nbsp;&nbsp;&nbsp; 12880&nbsp; 0.0&nbsp; 0.0 53432 1512 pts/0&nbsp;&nbsp;&nbsp; S&nbsp;&nbsp;&nbsp; 15:55&nbsp;&nbsp; 0:00&nbsp;
|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_ -bash
        <br>
sdiaz&nbsp;&nbsp;&nbsp; 13070&nbsp; 0.3&nbsp; 0.0 39988 2500 pts/0&nbsp;&nbsp;&nbsp; S+&nbsp;&nbsp; 15:58&nbsp;&nbsp; 0:00&nbsp;
|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_ mpirun -am ft-enable-cr --default-hostfile
mpi_test/lanzar_pi3.sh.po3117822 --app
/home/cesga/sdiaz/ompi_global_snapshot_12554.ckpt/restart-appfile
        <br>
sdiaz&nbsp;&nbsp;&nbsp; 13073&nbsp; 0.0&nbsp; 0.0 15988&nbsp; 508 pts/0&nbsp;&nbsp;&nbsp; Sl+&nbsp; 15:58&nbsp;&nbsp; 0:00&nbsp;
|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_ cr_restart
/home/cesga/sdiaz/ompi_global_snapshot_12554.ckpt/0/opal_snapshot_0.ckpt/ompi_blcr_context.12558
        <br>
sdiaz&nbsp;&nbsp;&nbsp; 12558&nbsp; 0.2&nbsp; 0.0 99464 3616 pts/0&nbsp;&nbsp;&nbsp; Sl+&nbsp; 15:58&nbsp;&nbsp; 0:00&nbsp;
|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_ ./pi3
        <br>
        <br>
        <br>
Sergio D&iacute;az escribi&oacute;:
        <br>
        <blockquote type="cite"><br>
Hi Josh
          <br>
          <br>
Here you go the file.
          <br>
          <br>
I will try to apply the trunk but I think that I broke-up my openmpi
installation doing "something" and I don't know what :-( . I was
modifying the mca parameters...
          <br>
When I send a job, the orted daemon expanded in the SLAVE host is
launched in a bucle till they spend all the reserved memory.
          <br>
It is very strange so I will compile it again, I will reproduce the bug
and then I will test the trunk.
          <br>
          <br>
Thanks a lot for the support and tickets opened.
          <br>
Sergio
          <br>
          <br>
          <br>
sdiaz&nbsp;&nbsp;&nbsp; 30279&nbsp; 0.0&nbsp; 0.0&nbsp; 1888&nbsp; 560 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ds&nbsp;&nbsp; 12:54&nbsp;&nbsp; 0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_
/opt/cesga/sge62/utilbin/lx24-x86/qrsh_starter
/opt/cesga/sge62/default/spool/compute
          <br>
sdiaz&nbsp;&nbsp;&nbsp; 30286&nbsp; 0.0&nbsp; 0.0 52772 1188 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; D&nbsp;&nbsp;&nbsp; 12:54&nbsp;&nbsp; 0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
\_ /bin/bash /opt/cesga/openmpi-1.3.3/bin/orted -mca ess env -mca
orte_ess_jobid 219
          <br>
sdiaz&nbsp;&nbsp;&nbsp; 30322&nbsp; 0.0&nbsp; 0.0 52772 1188 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; S&nbsp;&nbsp;&nbsp; 12:54&nbsp;&nbsp;
0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_ /bin/bash /opt/cesga/openmpi-1.3.3/bin/orted
          <br>
sdiaz&nbsp;&nbsp;&nbsp; 30358&nbsp; 0.0&nbsp; 0.0 52772 1188 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; D&nbsp;&nbsp;&nbsp; 12:54&nbsp;&nbsp;
0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_ /bin/bash /opt/cesga/openmpi-1.3.3/bin/orted
          <br>
sdiaz&nbsp;&nbsp;&nbsp; 30394&nbsp; 0.0&nbsp; 0.0 52772 1188 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; D&nbsp;&nbsp;&nbsp; 12:54&nbsp;&nbsp;
0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_ /bin/bash
/opt/cesga/openmpi-1.3.3/bin/orted
          <br>
sdiaz&nbsp;&nbsp;&nbsp; 30430&nbsp; 0.0&nbsp; 0.0 52772 1188 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; D&nbsp;&nbsp;&nbsp; 12:54&nbsp;&nbsp;
0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_ /bin/bash
/opt/cesga/openmpi-1.3.3/bin/orted
          <br>
sdiaz&nbsp;&nbsp;&nbsp; 30466&nbsp; 0.0&nbsp; 0.0 52772 1188 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; D&nbsp;&nbsp;&nbsp; 12:54&nbsp;&nbsp;
0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_ /bin/bash
/opt/cesga/openmpi-1.3.3/bin/orted
          <br>
sdiaz&nbsp;&nbsp;&nbsp; 30502&nbsp; 0.0&nbsp; 0.0 52772 1188 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; D&nbsp;&nbsp;&nbsp; 12:54&nbsp;&nbsp;
0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_ /bin/bash
/opt/cesga/openmpi-1.3.3/bin/orted
          <br>
sdiaz&nbsp;&nbsp;&nbsp; 30538&nbsp; 0.0&nbsp; 0.0 52772 1188 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; D&nbsp;&nbsp;&nbsp; 12:54&nbsp;&nbsp;
0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_ /bin/bash
/opt/cesga/openmpi-1.3.3/bin/orted
          <br>
sdiaz&nbsp;&nbsp;&nbsp; 30574&nbsp; 0.0&nbsp; 0.0 52772 1188 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; D&nbsp;&nbsp;&nbsp; 12:54&nbsp;&nbsp;
0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_ /bin/bash
/opt/cesga/openmpi-1.3.3/bin/orted
          <br>
....
          <br>
          <br>
          <br>
          <br>
Josh Hursey escribi&oacute;:
          <br>
          <blockquote type="cite"><br>
            <br>
On Nov 12, 2009, at 10:54 AM, Sergio D&iacute;az wrote:
            <br>
            <br>
            <blockquote type="cite">Hi Josh,
              <br>
              <br>
You were right. The main problem was the /tmp. SGE uses a scratch
directory in which the jobs have temporary files. Setting TMPDIR to
/tmp, checkpoint works!
              <br>
However, when I try to restart it... I got the following error (see
ERROR1). Option -v agrees these lines (see ERRO2).
              <br>
            </blockquote>
            <br>
It is concerning that ompi-restart is segfault'ing when it errors out.
The error message is being generated between the launch of the
opal-restart starter command and when we try to exec(cr_restart).
Usually the failure is related to a corruption of the metadata stored
in the checkpoint.
            <br>
            <br>
Can you send me the file below:
            <br>
&nbsp;ompi_global_snapshot_28454.ckpt/0/opal_snapshot_0.ckpt/snapshot_meta.data
            <br>
            <br>
I was able to reproduce the segv (at least I think it is the same one).
We failed to check the validity of a string when we parse the metadata.
I committed a fix to the trunk in r22290, and requested that the fix be
moved to the v1.4 and v1.5 branches. If you are interested in seeing
when they get applied you can follow the following tickets:
            <br>
&nbsp; <a class="moz-txt-link-freetext" href="https://svn.open-mpi.org/trac/ompi/ticket/2140">https://svn.open-mpi.org/trac/ompi/ticket/2140</a>
            <br>
&nbsp; <a class="moz-txt-link-freetext" href="https://svn.open-mpi.org/trac/ompi/ticket/2141">https://svn.open-mpi.org/trac/ompi/ticket/2141</a>
            <br>
            <br>
Can you try the trunk to see if the problem goes away? The development
trunk and v1.5 series have a bunch of improvements to the C/R
functionality that were never brought over the v1.3/v1.4 series.
            <br>
            <br>
            <blockquote type="cite"><br>
I was trying to use ssh instead of rsh but I was impossible. By default
it should use ssh and if it finds a problem, it will use rsh. It seems
that ssh doesn't work because always use rsh.
              <br>
If I change this MCA parameter, It still uses rsh.
              <br>
If I set OMPI_MCA_plm_rsh_disable_qrsh variable to 1, It try to use ssh
and doesn't works. I got --&gt; "bash: orted: command not found" and
the mpi process dies.
              <br>
The command which try to execute is the following and I haven't found
yet the reason why this command doesn't found orted because I set the
/etc/bashrc in order to get always the right path and I have the right
path into my application. (see ERROR4).
              <br>
            </blockquote>
            <br>
This seems like an SGE specific issue, so a bit out of my domain. Maybe
others have suggestions here.
            <br>
            <br>
-- Josh
            <blockquote type="cite"><br>
              <br>
Many thanks!,
              <br>
Sergio
              <br>
              <br>
P.S. Sorry about these long emails. I just try to show you useful
information to identify my problems.
              <br>
              <br>
              <br>
ERROR 1
              <br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
              <br>
&gt; [sdiaz@compute-3-18 ~]$ ompi-restart
ompi_global_snapshot_28454.ckpt
              <br>
&gt;
--------------------------------------------------------------------------
              <br>
&gt; Error: Unable to obtain the proper restart command to restart from
the
              <br>
&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; checkpoint file (opal_snapshot_0.ckpt). Returned -1.
              <br>
&gt;
              <br>
&gt;
--------------------------------------------------------------------------
              <br>
&gt;
--------------------------------------------------------------------------
              <br>
&gt; Error: Unable to obtain the proper restart command to restart from
the
              <br>
&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; checkpoint file (opal_snapshot_1.ckpt). Returned -1.
              <br>
&gt;
              <br>
&gt;
--------------------------------------------------------------------------
              <br>
&gt; [compute-3-18:28792] *** Process received signal ***
              <br>
&gt; [compute-3-18:28792] Signal: Segmentation fault (11)
              <br>
&gt; [compute-3-18:28792] Signal code:&nbsp; (128)
              <br>
&gt; [compute-3-18:28792] Failing at address: (nil)
              <br>
&gt; [compute-3-18:28792] [ 0] /lib64/tls/libpthread.so.0
[0x33bbf0c430]
              <br>
&gt; [compute-3-18:28792] [ 1] /lib64/tls/libc.so.6(__libc_free+0x25)
[0x33bb669135]
              <br>
&gt; [compute-3-18:28792] [ 2]
/opt/cesga/openmpi-1.3.3/lib/libopen-pal.so.0(opal_argv_free+0x2e)
[0x2a95586658]
              <br>
&gt; [compute-3-18:28792] [ 3]
/opt/cesga/openmpi-1.3.3/lib/libopen-pal.so.0(opal_event_fini+0x1e)
[0x2a9557906e]
              <br>
&gt; [compute-3-18:28792] [ 4]
/opt/cesga/openmpi-1.3.3/lib/libopen-pal.so.0(opal_finalize+0x36)
[0x2a9556bcfa]
              <br>
&gt; [compute-3-18:28792] [ 5] opal-restart [0x40312a]
              <br>
&gt; [compute-3-18:28792] [ 6]
/lib64/tls/libc.so.6(__libc_start_main+0xdb) [0x33bb61c3fb]
              <br>
&gt; [compute-3-18:28792] [ 7] opal-restart [0x40272a]
              <br>
&gt; [compute-3-18:28792] *** End of error message ***
              <br>
&gt; [compute-3-18:28793] *** Process received signal ***
              <br>
&gt; [compute-3-18:28793] Signal: Segmentation fault (11)
              <br>
&gt; [compute-3-18:28793] Signal code:&nbsp; (128)
              <br>
&gt; [compute-3-18:28793] Failing at address: (nil)
              <br>
&gt; [compute-3-18:28793] [ 0] /lib64/tls/libpthread.so.0
[0x33bbf0c430]
              <br>
&gt; [compute-3-18:28793] [ 1] /lib64/tls/libc.so.6(__libc_free+0x25)
[0x33bb669135]
              <br>
&gt; [compute-3-18:28793] [ 2]
/opt/cesga/openmpi-1.3.3/lib/libopen-pal.so.0(opal_argv_free+0x2e)
[0x2a95586658]
              <br>
&gt; [compute-3-18:28793] [ 3]
/opt/cesga/openmpi-1.3.3/lib/libopen-pal.so.0(opal_event_fini+0x1e)
[0x2a9557906e]
              <br>
&gt; [compute-3-18:28793] [ 4]
/opt/cesga/openmpi-1.3.3/lib/libopen-pal.so.0(opal_finalize+0x36)
[0x2a9556bcfa]
              <br>
&gt; [compute-3-18:28793] [ 5] opal-restart [0x40312a]
              <br>
&gt; [compute-3-18:28793] [ 6]
/lib64/tls/libc.so.6(__libc_start_main+0xdb) [0x33bb61c3fb]
              <br>
&gt; [compute-3-18:28793] [ 7] opal-restart [0x40272a]
              <br>
&gt; [compute-3-18:28793] *** End of error message ***
              <br>
&gt;
--------------------------------------------------------------------------
              <br>
&gt; mpirun noticed that process rank 0 with PID 28792 on node
compute-3-18.local exited on signal 11 (Segmentation fault).
              <br>
&gt;
--------------------------------------------------------------------------
              <br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
              <br>
              <br>
              <br>
ERROR 2
              <br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
              <br>
&gt; [sdiaz@compute-3-18 ~]$ ompi-restart -v
ompi_global_snapshot_28454.ckpt
              <br>
&gt;[compute-3-18.local:28941] Checking for the existence of
(/home/cesga/sdiaz/ompi_global_snapshot_28454.ckpt)
              <br>
&gt; [compute-3-18.local:28941] Restarting from file
(ompi_global_snapshot_28454.ckpt)
              <br>
&gt; [compute-3-18.local:28941]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Exec in self
              <br>
&gt; .......
              <br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
              <br>
              <br>
              <br>
ERROR3
              <br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
              <br>
&gt;[sdiaz@compute-3-18 ~]$ ompi_info&nbsp; --all|grep "plm_rsh_agent"
              <br>
&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; How many plm_rsh_agent instances to invoke concurrently
(must be &gt; 0)
              <br>
&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MCA plm: parameter "plm_rsh_agent" (current value: "ssh :
rsh", data source: default value, synonyms: pls_rsh_agent)
              <br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
              <br>
              <br>
ERROR4
              <br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
              <br>
&gt;/usr/bin/ssh -x compute-3-17.local&nbsp; orted --debug-daemons -mca ess
env -mca orte_ess_jobid 2152464384 -mca orte_ess_vpid 1 -mca
orte_ess_num_procs 2 --hnp-uri
&gt;"2152464384.0;tcp://192.168.4.143:59176" -mca
mca_base_param_file_prefix ft-enable-cr -mca mca_base_param_file_path
&gt;/opt/cesga/openmpi-1.3.3/share/openmpi/amca-param-sets:/home_no_usc/cesga/sdiaz/mpi_test
-mca mca_base_param_file_path_force /home_no_usc/cesga/sdiaz/mpi_test
              <br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
Josh Hursey escribi&oacute;:
              <br>
              <blockquote type="cite"><br>
On Nov 9, 2009, at 5:33 AM, Sergio D&iacute;az wrote:
                <br>
                <br>
                <br>
                <blockquote type="cite">Hi Josh,
                  <br>
                  <br>
The OpenMPI version is 1.3.3.
                  <br>
                  <br>
The command ompi-ps doesn't work.
                  <br>
                  <br>
[root@compute-3-18 ~]# ompi-ps -j 2726959 -p 16241
                  <br>
[root@compute-3-18 ~]# ompi-ps -v -j 2726959 -p 16241
                  <br>
[compute-3-18.local:16254] orte_ps: Acquiring list of HNPs and setting
contact info into RML...
                  <br>
[root@compute-3-18 ~]# ompi-ps -v -j 2726959
                  <br>
[compute-3-18.local:16255] orte_ps: Acquiring list of HNPs and setting
contact info into RML...
                  <br>
                  <br>
[root@compute-3-18 ~]# ps uaxf | grep sdiaz
                  <br>
root&nbsp;&nbsp;&nbsp;&nbsp; 16260&nbsp; 0.0&nbsp; 0.0 51084&nbsp; 680 pts/0&nbsp;&nbsp;&nbsp; S+&nbsp;&nbsp; 13:38&nbsp;&nbsp; 0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
\_ grep sdiaz
                  <br>
sdiaz&nbsp;&nbsp;&nbsp; 16203&nbsp; 0.0&nbsp; 0.0 53164 1220 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp; 13:37&nbsp;&nbsp; 0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_
-bash /opt/cesga/sge62/default/spool/compute-3-18/job_scripts/2726959
                  <br>
sdiaz&nbsp;&nbsp;&nbsp; 16241&nbsp; 0.0&nbsp; 0.0 41028 2480 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; S&nbsp;&nbsp;&nbsp; 13:37&nbsp;&nbsp; 0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
\_ mpirun -np 2 -am ft-enable-cr ./pi3
                  <br>
sdiaz&nbsp;&nbsp;&nbsp; 16242&nbsp; 0.0&nbsp; 0.0 36484 1840 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sl&nbsp;&nbsp; 13:37&nbsp;&nbsp;
0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_ /opt/cesga/sge62/bin/lx24-x86/qrsh -inherit
-nostdin -V compute-3-17.local&nbsp; orted -mca ess env -mca orte_ess_jobid
2769879040 -mca orte_ess_vpid 1 -mca orte_ess_num_procs 2 --hnp-uri
"2769879040.0;tcp://192.168.4.143:57010" -mca
mca_base_param_file_prefix ft-enable-cr -mca mca_base_param_file_path
/opt/cesga/openmpi-1.3.3/share/openmpi/amca-param-sets:/home_no_usc/cesga/sdiaz/mpi_test
-mca mca_base_param_file_path_force /home_no_usc/cesga/sdiaz/mpi_test
                  <br>
sdiaz&nbsp;&nbsp;&nbsp; 16245&nbsp; 0.1&nbsp; 0.0 99464 4616 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sl&nbsp;&nbsp; 13:37&nbsp;&nbsp;
0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_ ./pi3
                  <br>
                  <br>
[root@compute-3-18 ~]# ompi-ps -n c3-18
                  <br>
[root@compute-3-18 ~]# ompi-ps -n compute-3-18
                  <br>
[root@compute-3-18 ~]# ompi-ps -n
                  <br>
                  <br>
There is not directory on the /tmp of the node. However, if the
application is run without SGE, the directory is created
                  <br>
                  <br>
                </blockquote>
This may be the core of the problem. ompi-ps and other command line
tools (e.g., ompi-checkpoint) look for the Open MPI session directory
in /tmp in order to find the connection information to connect to the
mpirun process (internally called the HNP or Head Node Process).
                <br>
                <br>
Can you change the location of the temporary directory in SGE? The
temporary directory is usually set via an environment variable (e.g.,
TMPDIR, or TMP). So removing the environment variable or setting it to
/tmp might help.
                <br>
                <br>
                <br>
                <br>
                <blockquote type="cite">but if I do ompi-ps -j
MPIRUN_PID, it seems hanged and I interrupt it. Does it take long time?
                  <br>
                  <br>
                </blockquote>
It should not take a long time. It is just querying the mpirun process
for state information.
                <br>
                <br>
                <br>
                <blockquote type="cite">what means the option -j of
ompi-ps command? isn't it related to a batch system(like sge,
condor...), is it?
                  <br>
                  <br>
                </blockquote>
The '-j' option allows the user to specify the Open MPI jobid. This is
completely different than the jobid provided by the batch system. In
general, users should not need to specify the -j option. It is useful
when you have multiple Open MPI jobs, and want a summary of just one of
them.
                <br>
                <br>
                <br>
                <blockquote type="cite">Thanks for the ticket. I will
follow it.
                  <br>
                  <br>
Talking with Alan, I realized that there are few transport protocols
that are supported. And maybe it is the problem. Currently, SGE is
using qrsh to expand mpi process. I can change this protocol and use
ssh. So, I'm going to test it this afternoon and I will comment to you
the results.
                  <br>
                  <br>
                </blockquote>
Try 'ssh' and see if that helps. I suspect the problem is with the
session directory location though.
                <br>
                <br>
                <br>
                <blockquote type="cite">Regards,
                  <br>
Sergio
                  <br>
                  <br>
                  <br>
Josh Hursey escribi&oacute;:
                  <br>
                  <br>
                  <blockquote type="cite">On Oct 28, 2009, at 7:41 AM,
Sergio D&iacute;az wrote:
                    <br>
                    <br>
                    <br>
                    <blockquote type="cite">Hello,
                      <br>
                      <br>
I have achieved the checkpoint of an easy program without SGE. Now, I'm
trying to do the integration openmpi+sge but I have some problems...
When I try to do checkpoint of the mpirun PID, I got an error similar
to the error gotten when the PID doesn't exit. The example below.
                      <br>
                      <br>
                    </blockquote>
I do not have any experience with the SGE environment, so I suspect
that there may something 'special' about the environment that is
tripping up the ompi-checkpoint tool.
                    <br>
                    <br>
First of all, what version of Open MPI are you using?
                    <br>
                    <br>
Somethings to check:
                    <br>
&nbsp;- Does 'ompi-ps' work when your application is running?
                    <br>
&nbsp;- Is there an /tmp/openmpi-sessions-* directory on the node where
mpirun is currently running? This directory contains information on how
to connect to the mpirun process from an external tool, if it's missing
then this could be the cause of the problem.
                    <br>
                    <br>
                    <br>
                    <blockquote type="cite">Any ideas?
                      <br>
Somebody have a script to do it automatic with SGE?. For example I have
one to do checkpoint each X seconds with BLCR and non-mpi jobs. It is
launched by SGE if you have configured the queue and the ckpt
environment.
                      <br>
                      <br>
                    </blockquote>
I do not know of any integration of the Open MPI checkpointing work
with SGE at the moment.
                    <br>
                    <br>
As far as time triggered checkpointing, I have a feature ticket open
about this:
                    <br>
&nbsp; <a class="moz-txt-link-freetext" href="https://svn.open-mpi.org/trac/ompi/ticket/1961">https://svn.open-mpi.org/trac/ompi/ticket/1961</a>
                    <br>
                    <br>
It is not available yet, but in the works.
                    <br>
                    <br>
                    <br>
                    <br>
                    <blockquote type="cite">Is it possible choose the
name of the ckpt folder when you do the ompi-checkpoint? I can't find
the option to do it.
                      <br>
                      <br>
                    </blockquote>
Not at this time. Though I could see it as a useful feature, and
shouldn't be too hard to implement. I filed a ticket if you want to
follow the progress:
                    <br>
&nbsp; <a class="moz-txt-link-freetext" href="https://svn.open-mpi.org/trac/ompi/ticket/2098">https://svn.open-mpi.org/trac/ompi/ticket/2098</a>
                    <br>
                    <br>
-- Josh
                    <blockquote type="cite">Regards,
                      <br>
Sergio
                      <br>
                      <br>
                      <br>
--------------------------------
                      <br>
                      <br>
[sdiaz@compute-3-17 ~]$ ps auxf
                      <br>
....
                      <br>
root&nbsp;&nbsp;&nbsp;&nbsp; 20044&nbsp; 0.0&nbsp; 0.0&nbsp; 4468 1224 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; S&nbsp;&nbsp;&nbsp; 13:28&nbsp;&nbsp; 0:00&nbsp; \_
sge_shepherd-2645150 -bg
                      <br>
sdiaz&nbsp;&nbsp;&nbsp; 20072&nbsp; 0.0&nbsp; 0.0 53172 1212 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp; 13:28&nbsp;&nbsp; 0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_
-bash /opt/cesga/sge62/default/spool/compute-3-17/job_scripts/2645150
                      <br>
sdiaz&nbsp;&nbsp;&nbsp; 20112&nbsp; 0.2&nbsp; 0.0 41028 2480 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; S&nbsp;&nbsp;&nbsp; 13:28&nbsp;&nbsp; 0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
\_ mpirun -np 2 -am ft-enable-cr pi3
                      <br>
sdiaz&nbsp;&nbsp;&nbsp; 20113&nbsp; 0.0&nbsp; 0.0 36484 1824 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sl&nbsp;&nbsp; 13:28&nbsp;&nbsp;
0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_ /opt/cesga/sge62/bin/lx24-x86/qrsh -inherit
-nostdin -V compute-3-18..........
                      <br>
sdiaz&nbsp;&nbsp;&nbsp; 20116&nbsp; 1.2&nbsp; 0.0 99464 4616 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sl&nbsp;&nbsp; 13:28&nbsp;&nbsp;
0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_ pi3
                      <br>
                      <br>
                      <br>
[sdiaz@compute-3-17 ~]$ ompi-checkpoint 20112
                      <br>
[compute-3-17.local:20124] HNP with PID 20112 Not found!
                      <br>
                      <br>
[sdiaz@compute-3-17 ~]$ ompi-checkpoint -s 20112
                      <br>
[compute-3-17.local:20135] HNP with PID 20112 Not found!
                      <br>
                      <br>
[sdiaz@compute-3-17 ~]$ ompi-checkpoint -s --term 20112
                      <br>
[compute-3-17.local:20136] HNP with PID 20112 Not found!
                      <br>
                      <br>
[sdiaz@compute-3-17 ~]$ ompi-checkpoint --hnp-pid 20112
                      <br>
--------------------------------------------------------------------------
                      <br>
ompi-checkpoint PID_OF_MPIRUN
                      <br>
&nbsp; Open MPI Checkpoint Tool
                      <br>
                      <br>
&nbsp;&nbsp; -am &lt;arg0&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Aggregate MCA parameter set file list
                      <br>
&nbsp;&nbsp; -gmca|--gmca &lt;arg0&gt; &lt;arg1&gt;
                      <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pass global MCA parameters that are applicable
to
                      <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; all contexts (arg0 is the parameter name; arg1
is
                      <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; the parameter value)
                      <br>
-h|--help&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This help message
                      <br>
&nbsp;&nbsp; --hnp-jobid &lt;arg0&gt;&nbsp;&nbsp;&nbsp; This should be the jobid of the HNP
whose
                      <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; applications you wish to checkpoint.
                      <br>
&nbsp;&nbsp; --hnp-pid &lt;arg0&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This should be the pid of the mpirun
whose
                      <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; applications you wish to checkpoint.
                      <br>
&nbsp;&nbsp; -mca|--mca &lt;arg0&gt; &lt;arg1&gt;
                      <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pass context-specific MCA parameters; they are
                      <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; considered global if --gmca is not used and
only
                      <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; one context is specified (arg0 is the
parameter
                      <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name; arg1 is the parameter value)
                      <br>
-s|--status&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Display status messages describing the
progression
                      <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; of the checkpoint
                      <br>
&nbsp;&nbsp; --term&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Terminate the application after checkpoint
                      <br>
-v|--verbose&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Be Verbose
                      <br>
-w|--nowait&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Do not wait for the application to finish
                      <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; checkpointing before returning
                      <br>
                      <br>
--------------------------------------------------------------------------
                      <br>
[sdiaz@compute-3-17 ~]$ exit
                      <br>
logout
                      <br>
Connection to c3-17 closed.
                      <br>
[sdiaz@svgd mpi_test]$ ssh c3-18
                      <br>
Last login: Wed Oct 28 13:24:12 2009 from svgd.local
                      <br>
-bash-3.00$ ps auxf |grep sdiaz
                      <br>
                      <br>
sdiaz&nbsp;&nbsp;&nbsp; 14412&nbsp; 0.0&nbsp; 0.0&nbsp; 1888&nbsp; 560 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp; 13:28&nbsp;&nbsp; 0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_
/opt/cesga/sge62/utilbin/lx24-x86/qrsh_starter
/opt/cesga/sge62/default/spool/compute-3-18/active_jobs/2645150.1/1.compute-3-18
                      <br>
sdiaz&nbsp;&nbsp;&nbsp; 14419&nbsp; 0.0&nbsp; 0.0 35728 2260 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; S&nbsp;&nbsp;&nbsp; 13:28&nbsp;&nbsp; 0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
\_ orted -mca ess env -mca orte_ess_jobid 2295267328 -mca orte_ess_vpid
1 -mca orte_ess_num_procs 2 --hnp-uri
2295267328.0;tcp://192.168.4.144:36596 -mca mca_base_param_file_prefix
ft-enable-cr -mca mca_base_param_file_path
/opt/cesga/openmpi-1.3.3/share/openmpi/amca-param-sets:/home_no_usc/cesga/sdiaz/mpi_test
-mca mca_base_param_file_path_force /home_no_usc/cesga/sdiaz/mpi_test
                      <br>
sdiaz&nbsp;&nbsp;&nbsp; 14420&nbsp; 0.0&nbsp; 0.0 99452 4596 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sl&nbsp;&nbsp; 13:28&nbsp;&nbsp;
0:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \_ pi3
                      <br>
                      <br>
                      <br>
                      <br>
                      <br>
                      <br>
--&nbsp;<br>
Sergio D&iacute;az Montes
                      <br>
Centro de Supercomputacion de Galicia
                      <br>
Avda. de Vigo. s/n (Campus Sur) 15706 Santiago de Compostela (Spain)
                      <br>
Tel: +34 981 56 98 10 ; Fax: +34 981 59 46 16
                      <br>
email: <a class="moz-txt-link-abbreviated" href="mailto:sdiaz@cesga.es">sdiaz@cesga.es</a> ; <a class="moz-txt-link-freetext" href="http://www.cesga.es/">http://www.cesga.es/</a>
                      <br>
&lt;image002.jpg&gt;
                      <br>
------------------------------------------------
                      <br>
_______________________________________________
                      <br>
users mailing list
                      <br>
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
                      <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
                      <br>
                      <br>
                    </blockquote>
_______________________________________________
                    <br>
users mailing list
                    <br>
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
                    <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
                    <br>
                    <br>
                    <br>
                  </blockquote>
--&nbsp;<br>
Sergio D&iacute;az Montes
                  <br>
Centro de Supercomputacion de Galicia
                  <br>
Avda. de Vigo. s/n (Campus Sur) 15706 Santiago de Compostela (Spain)
                  <br>
Tel: +34 981 56 98 10 ; Fax: +34 981 59 46 16
                  <br>
email: <a class="moz-txt-link-abbreviated" href="mailto:sdiaz@cesga.es">sdiaz@cesga.es</a> ; <a class="moz-txt-link-freetext" href="http://www.cesga.es/">http://www.cesga.es/</a>
                  <br>
&lt;image002.jpg&gt;
                  <br>
------------------------------------------------
                  <br>
_______________________________________________
                  <br>
users mailing list
                  <br>
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
                  <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
                  <br>
                  <br>
                </blockquote>
                <br>
_______________________________________________
                <br>
users mailing list
                <br>
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
                <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
                <br>
                <br>
                <br>
              </blockquote>
              <br>
              <br>
--&nbsp;<br>
Sergio D&iacute;az Montes
              <br>
Centro de Supercomputacion de Galicia
              <br>
Avda. de Vigo. s/n (Campus Sur) 15706 Santiago de Compostela (Spain)
              <br>
Tel: +34 981 56 98 10 ; Fax: +34 981 59 46 16
              <br>
email: <a class="moz-txt-link-abbreviated" href="mailto:sdiaz@cesga.es">sdiaz@cesga.es</a> ; <a class="moz-txt-link-freetext" href="http://www.cesga.es/">http://www.cesga.es/</a>
              <br>
&lt;image002.jpg&gt;
              <br>
------------------------------------------------
              <br>
_______________________________________________
              <br>
users mailing list
              <br>
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
              <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
              <br>
            </blockquote>
            <br>
            <br>
_______________________________________________
            <br>
users mailing list
            <br>
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
            <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
            <br>
            <br>
          </blockquote>
          <br>
          <br>
--&nbsp;<br>
Sergio D&iacute;az Montes
          <br>
Centro de Supercomputacion de Galicia
          <br>
Avda. de Vigo. s/n (Campus Sur) 15706 Santiago de Compostela (Spain)
          <br>
Tel: +34 981 56 98 10 ; Fax: +34 981 59 46 16
          <br>
email: <a class="moz-txt-link-abbreviated" href="mailto:sdiaz@cesga.es">sdiaz@cesga.es</a> ; <a class="moz-txt-link-freetext" href="http://www.cesga.es/">http://www.cesga.es/</a>
          <br>
&lt;mime-attachment.jpeg&gt;
          <br>
------------------------------------------------
          <br>
_______________________________________________ users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a> <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
          <br>
        </blockquote>
        <br>
        <br>
--&nbsp;<br>
Sergio D&iacute;az Montes
        <br>
Centro de Supercomputacion de Galicia
        <br>
Avda. de Vigo. s/n (Campus Sur) 15706 Santiago de Compostela (Spain)
        <br>
Tel: +34 981 56 98 10 ; Fax: +34 981 59 46 16
        <br>
email: <a class="moz-txt-link-abbreviated" href="mailto:sdiaz@cesga.es">sdiaz@cesga.es</a> ; <a class="moz-txt-link-freetext" href="http://www.cesga.es/">http://www.cesga.es/</a>
        <br>
&lt;image002.jpg&gt;
        <br>
------------------------------------------------
        <br>
&lt;mime-attachment.jpeg&gt;&lt;image002.jpg&gt;
        <br>
_______________________________________________
        <br>
users mailing list
        <br>
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
        <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
        <br>
      </blockquote>
      <br>
      <br>
_______________________________________________
      <br>
users mailing list
      <br>
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
      <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
      <br>
      <br>
    </blockquote>
    <br>
    <br>
--&nbsp;<br>
Sergio D&iacute;az Montes
    <br>
Centro de Supercomputacion de Galicia
    <br>
Avda. de Vigo. s/n (Campus Sur) 15706 Santiago de Compostela (Spain)
    <br>
Tel: +34 981 56 98 10 ; Fax: +34 981 59 46 16
    <br>
email: <a class="moz-txt-link-abbreviated" href="mailto:sdiaz@cesga.es">sdiaz@cesga.es</a> ; <a class="moz-txt-link-freetext" href="http://www.cesga.es/">http://www.cesga.es/</a>
    <br>
&lt;image002.jpg&gt;
    <br>
------------------------------------------------
    <br>
&lt;image002.jpg&gt;
    <br>
_______________________________________________
    <br>
users mailing list
    <br>
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
    <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
    <br>
  </blockquote>
  <br>
  <br>
_______________________________________________
  <br>
users mailing list
  <br>
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
  <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
  <br>
  <br>
</blockquote>
<br>
<br>
<div class="moz-signature">-- <br>
Sergio D&iacute;az Montes<br>
Centro de Supercomputacion de Galicia<br>
Avda. de Vigo. s/n (Campus Sur) 15706 Santiago de Compostela (Spain)<br>
Tel: +34 981 56 98 10 ; Fax: +34 981 59 46 16 <br>
email: <a class="moz-txt-link-abbreviated" href="mailto:sdiaz@cesga.es">sdiaz@cesga.es</a> ; <a href="http://www.cesga.es/">http://www.cesga.es/</a><br>
<img src="cid:part1.02030202.08070901@cesga.es"><br>
------------------------------------------------ <br>
</div>
</body>
</html>

