<div dir="ltr"><div>Just an update for the list. Really only impacts folks running Open MPI under LSF.</div><div><br></div><div><br></div><div>The LSB_PJL_TASK_GEOMETRY changes what lbs_getalloc() returns regarding the allocation. It adjusts it to the mapping/ordering specified in that environment variable. However, since it is not set by LSF when the job starts the LSB_AFFINITY_HOSTFILE will show a broader mapping/ordering. The difference between these two requests is the core of the problem here.</div><div><br></div><div>Consider an LSB hostfile with the following:</div><div>=== LSB_AFFINITY_HOSTFILE ===</div><div>p10a33 0,1,2,3,4,5,6,7</div><div>p10a33 8,9,10,11,12,13,14,15</div><div>p10a33 16,17,18,19,20,21,22,23</div><div>p10a30 0,1,2,3,4,5,6,7</div><div>p10a30 8,9,10,11,12,13,14,15</div><div>p10a30 16,17,18,19,20,21,22,23</div><div>p10a58 0,1,2,3,4,5,6,7</div><div>p10a58 8,9,10,11,12,13,14,15</div><div>p10a58 16,17,18,19,20,21,22,23</div><div>=============================</div><div><br></div><div>This tells Open MPI to launch 3 processes per node with a particular set of bindings - so 9 processes total.<br></div><div><br></div><div>export LSB_PJL_TASK_GEOMETRY=&quot;{(5)(4,3)(2,1,0)}&quot;<br></div><div><br></div><div>The LSB_PJL_TASK_GEOMETRY variable (above) tells us to only launch 6 processes. So lbs_getalloc() will return to us (ras_lsf_module.c) a list of resources that match launching 6 processes. However, when we go to the rmaps_seq.c we tell it to pay attention to the LSB_AFFINITY_HOSTFILE. So it tries to map 9 processes even though we set the slots on the nodes to be a total of 6. So eventually we get an oversubscription issue.</div><div><br></div><div>Interesting difference between 1.10.2 and 1.10.3rc1 - using the LSB_AFFINITY_HOSTFILE, seen above.</div><div>In 1.10.2 RAS thinks it has the following allocation (with and without the LSB_PJL_TASK_GEOMETRY set):</div><div>======================   ALLOCATED NODES   ======================</div><div>        p10a33: slots=1 max_slots=0 slots_inuse=0 state=UP</div><div>=================================================================</div><div>In 1.10.3.rc1 RAS thinks it has the following allocation (with the LSB_PJL_TASK_GEOMETRY set)</div><div>======================   ALLOCATED NODES   ======================</div><div>        p10a33: slots=1 max_slots=0 slots_inuse=0 state=UP</div><div>        p10a30: slots=2 max_slots=0 slots_inuse=0 state=UP</div><div>        p10a58: slots=3 max_slots=0 slots_inuse=0 state=UP</div><div>=================================================================</div><div>In 1.10.3.rc1 RAS thinks it has the following allocation (without the LSB_PJL_TASK_GEOMETRY set)</div><div>======================   ALLOCATED NODES   ======================</div><div>        p10a33: slots=3 max_slots=0 slots_inuse=0 state=UP</div><div>        p10a30: slots=3 max_slots=0 slots_inuse=0 state=UP</div><div>        p10a58: slots=3 max_slots=0 slots_inuse=0 state=UP</div><div>=================================================================</div><div><br></div><div>The 1.10.3rc1 behavior is what I would expect to happen. The 1.10.2 behavior seems to be a bug when running under LSF.</div><div><br></div><div>The original error comes from trying to map 3 process on each of the nodes (since the affinity file wants to launch 9 processes), but the nodes having a more restricted set of slots (Due to the LSB_PJL_TASK_GEOMETRY variable).</div><div><br></div><div><br></div><div>I know a number of things have changed from 1.10.2 to 1.10.3 regarding how we allocate/map. Ralph, do you know offhand what might have caused this difference? It&#39;s not a big deal if not, just curious.</div><div><br></div><div><br></div><div>I&#39;m working with Farid on some options to work around the issue for 1.10.2. Open MPI 1.10.3 seems to be ok for basic LSF functionality (without the LSB_PJL_TASK_GEOMETRY variable).</div><div><br></div><div>-- Josh</div><div><br></div><div class="gmail_extra"><br><div class="gmail_quote">On Tue, Apr 19, 2016 at 8:57 AM, Josh Hursey <span dir="ltr">&lt;<a href="mailto:jjhursey@open-mpi.org" target="_blank">jjhursey@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div dir="ltr">Farid,<div><br></div><div>I have access to the same cluster inside IBM. I can try to help you track this down and maybe work up a patch with the LSF folks. I&#39;ll contact you off-list with my IBM address and we can work on this a bit.</div><div><br></div><div>I&#39;ll post back to the list with what we found.</div><span class=""><font color="#888888"><div><br></div><div>-- Josh</div><div><br></div></font></span></div><div class=""><div class="h5"><div class="gmail_extra"><br><div class="gmail_quote">On Tue, Apr 19, 2016 at 5:06 AM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><span>On Apr 18, 2016, at 7:08 PM, Farid Parpia &lt;<a href="mailto:parpia@us.ibm.com" target="_blank">parpia@us.ibm.com</a>&gt; wrote:<br>
&gt;<br>
&gt; I will try to put you in touch with someone in LSF development immediately.<br>
<br>
</span>FWIW: It would be great if IBM could contribute the fixes to this.  None of us have access to LSF resources, and IBM is a core contributor to Open MPI.<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" rel="noreferrer" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<span><br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</span>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/04/28963.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/28963.php</a><br>
</blockquote></div><br></div>
</div></div></blockquote></div><br></div></div>

