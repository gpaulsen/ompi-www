<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta content="text/html; charset=ISO-8859-1"
 http-equiv="Content-Type">
</head>
<body bgcolor="#ffffff" text="#000000">
Your problem may not be related to bandwidth. It may be latency or
division of the problem. We found significant improvements running wrf
and other atmospheric code (CFD) over IB. The problem was not so much
the amount of data communicated, but how long it takes to send it.
Also, is your model big enough to split up as much as you are trying?
If there is not enough work for each core to do between edge exchanges,
you will spend all of your time spinning waiting for the network. If
you are running a demo benchmark it is likely too small for the number
of processors. At least that is what we find with most weather model
demo problems. One other thing to look at is how it is being split up.
Depending on what the algorithm does, you may want more x points, more
y points or completely even divisions. We found that we can
significantly speed up wrf for our particular domain by not lett<br>
<br>
On 07/10/12 08:48, Dugenoux Albert wrote:
<blockquote
 cite="mid:1341935304.78875.YahooMailNeo@web29405.mail.ird.yahoo.com"
 type="cite">
  <div
 style="color: rgb(0, 0, 0); background-color: rgb(255, 255, 255); font-family: times new roman,new york,times,serif; font-size: 12pt;">
  <div style="right: auto;"><span style="right: auto;">Thanks for your
answer.You are right.</span></div>
  <div style="right: auto;"><span style="right: auto;">&nbsp;</span><span
 style="right: auto;">I've tried&nbsp;upon&nbsp;<var id="yui-ie-cursor"></var>4
nodes with 6 processes and things are worst.</span></div>
  <div style="right: auto;"><span style="right: auto;"></span>&nbsp;</div>
  <div style="right: auto;"><span style="right: auto;">So do you
suggest that unique thing to do is to order an infiniband switch or is
there a possibility to enhance</span></div>
  <div style="right: auto;"><span style="right: auto;">something </span><span
 style="right: auto;">by tuning mca parameters ?</span></div>
  <div style="right: auto;"><span style="right: auto;"></span>&nbsp;</div>
  <div style="right: auto;"><br style="right: auto;">
  </div>
  <div
 style="font-family: times new roman,new york,times,serif; font-size: 12pt;">
  <div
 style="font-family: times new roman,new york,times,serif; font-size: 12pt;">
  <div dir="ltr"><font face="Arial" size="2"><b><span
 style="font-weight: bold;">De&nbsp;:</span></b> Ralph Castain
<a class="moz-txt-link-rfc2396E" href="mailto:rhc@open-mpi.org">&lt;rhc@open-mpi.org&gt;</a><br>
  <b><span style="font-weight: bold;">&Agrave;&nbsp;:</span></b> Dugenoux Albert
<a class="moz-txt-link-rfc2396E" href="mailto:dugenouxa@yahoo.fr">&lt;dugenouxa@yahoo.fr&gt;</a>; Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a> <br>
  <b><span style="font-weight: bold;">Envoy&eacute; le :</span></b> Mardi 10
juillet 2012 16h47<br>
  <b><span style="font-weight: bold;">Objet&nbsp;:</span></b> Re: [OMPI
users] Bad parallel scaling using Code Saturne with openmpi<br>
  </font></div>
  <br>
  <div id="yiv210346608">
  <div>I suspect it mostly reflects communication patterns. I don't
know anything about Saturne, but shared memory is a great deal faster
than TCP, so the more processes sharing a node the better. You may also
be hitting some natural boundary in your model - perhaps with 8
processes/node you wind up with more processes that cross the node
boundary, further increasing the communication requirement.
  <div><br>
  </div>
  <div>Do things continue to get worse if you use all 4 nodes with 6
processes/node?</div>
  <div><br>
  </div>
  <div><br>
  <div>
  <div>On Jul 10, 2012, at 7:31 AM, Dugenoux Albert wrote:</div>
  <br class="yiv210346608Apple-interchange-newline">
  <blockquote type="cite">
    <div>
    <div
 style="background-color: rgb(255, 255, 255); font-family: times new roman,new york,times,serif; color: rgb(0, 0, 0); font-size: 12pt;">
    <div>Hi.</div>
    <div>&nbsp;</div>
    <div>I have recently built a cluster upon a Dell PowerEdge Server
with a Debian 6.0 OS. This server is composed of </div>
    <div>4 system board of 2 processors of hexacores. So it gives 12
cores&nbsp;per system board.</div>
    <div>The boards are linked with a local Gbits switch. </div>
    <div>&nbsp;</div>
    <div>In order to&nbsp;parallelize the software Code Saturne, which is a
CFD solver, I have configured&nbsp;the cluster</div>
    <div>such that there are&nbsp;a pbs server/mom on 1 system board and&nbsp;3
mom and the&nbsp;3 others cards. So this leads to </div>
    <div>48 cores dispatched on&nbsp;4 nodes of 12 CPU. Code saturne is
compiled with the openmpi 1.6 version.</div>
    <div>&nbsp;</div>
    <div>When I launch a simulation using 2 nodes&nbsp;with 12 cores,&nbsp;elapse
time is good and network traffic is not full.</div>
    <div>But when I launch the same simulation using 3 nodes with 8
cores, elapse time is 5 times the previous one.</div>
    <div>I&nbsp;both cases, I use 24 cores and network seems not to be
satured. </div>
    <div>&nbsp;</div>
    <div>I have tested several configurations : binaries in local file
system or&nbsp;on a NFS. But results are the same.</div>
    <div>I have visited severals forums (in particular <a
 moz-do-not-send="true"
 href="http://www.open-mpi.org/community/lists/users/2009/08/10394.php"
 rel="nofollow" target="_blank">http://www.open-mpi.org/community/lists/users/2009/08/10394.php</a>)</div>
    <div>and read lots of threads, but as I am not an expert at
clusters, I presently do not see where it&nbsp;is wrong !<var
 id="yiv210346608yui-ie-cursor"></var></div>
    <div>&nbsp;</div>
    <div>Is it a problem in the configuration of PBS (I have installed
it from the deb packages), a subtile compilation options</div>
    <div>of openMPI, or a bad&nbsp;network configuration&nbsp;?</div>
    <div>&nbsp;</div>
    <div>Regards.</div>
    <div>&nbsp;</div>
    <div>B. S.</div>
    </div>
    </div>
_______________________________________________<br>
users mailing list<br>
    <a moz-do-not-send="true" href="mailto:users@open-mpi.org"
 rel="nofollow" target="_blank" ymailto="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
  </div>
  <br>
  </div>
  </div>
  </div>
  <br>
  <br>
  </div>
  </div>
  </div>
  <pre wrap="">
<fieldset class="mimeAttachmentHeader"></fieldset>
_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></pre>
</blockquote>
</body>
</html>

