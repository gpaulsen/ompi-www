Haven&#39;t seen that before on any of our machines.<br><br>Could you do &quot;printenv | grep SLURM&quot; after the salloc and send the results?<br><br>What version of SLURM is this?<br><br>Please run &quot;mpirun --display-allocation hostname&quot; and send the results.<br>
<br>Thanks<br>Ralph<br><br><div class="gmail_quote">On Mon, Aug 24, 2009 at 11:30 AM,  <span dir="ltr">&lt;<a href="mailto:matthew.piehl@ndsu.edu">matthew.piehl@ndsu.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Hello,<br>
<br>
I&#39;ve seem to run into an interesting problem with openMPI. After<br>
allocating 3 processors and confirming that the 3 processors are<br>
allocated. mpirun on a simple mpitest program seems to run on 4<br>
processors. We have 2 processors per node. I can repeat this case with any<br>
odd number of nodes, openMPI seems to take any remaining processors on the<br>
box. We are running openMPI v1.3.3. Here is an example of what happens:<br>
<br>
node64-test ~&gt;salloc -n3<br>
salloc: Granted job allocation 825<br>
<br>
node64-test ~&gt;srun hostname<br>
node64-28.xxxx.xxxx.xxxx.xxxx<br>
node64-28.xxxx.xxxx.xxxx.xxxx<br>
node64-29.xxxx.xxxx.xxxx.xxxx<br>
<br>
node64-test ~&gt;MX_RCACHE=0<br>
LD_LIBRARY_PATH=&quot;/hurd/mpi/openmpi/lib:/usr/local/mx/lib&quot; mpirun<br>
mpi_pgms/mpitest<br>
MPI domain size: 4<br>
I am rank 000 - node64-28.xxxx.xxxx.xxxx.xxxx<br>
I am rank 003 - node64-29.xxxx.xxxx.xxxx.xxxx<br>
I am rank 001 - node64-28.xxxx.xxxx.xxxx.xxxx<br>
I am rank 002 - node64-29.xxxx.xxxx.xxxx.xxxx<br>
<br>
<br>
<br>
For those who may be curious here is the program:<br>
<br>
#include &lt;stdio.h&gt;<br>
#include &lt;stdlib.h&gt;<br>
#include &lt;mpi.h&gt;<br>
<br>
extern int main(int argc, char *argv[]);<br>
<br>
extern int main(int argc, char *argv[])<br>
<br>
{<br>
        auto int rank,<br>
                 size,<br>
                 namelen;<br>
<br>
        MPI_Status status;<br>
<br>
        static char processor_name[MPI_MAX_PROCESSOR_NAME];<br>
<br>
        MPI_Init(&amp;argc, &amp;argv);<br>
        MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);<br>
        MPI_Comm_size(MPI_COMM_WORLD, &amp;size);<br>
<br>
       if ( rank == 0 )<br>
        {<br>
                MPI_Get_processor_name(processor_name, &amp;namelen);<br>
                fprintf(stdout,&quot;My name is: %s\n&quot;,processor_name);<br>
                fprintf(stdout,&quot;Cluster size is: %d\n&quot;, size);<br>
<br>
        }<br>
        else<br>
        {<br>
                MPI_Get_processor_name(processor_name, &amp;namelen);<br>
                fprintf(stdout,&quot;My name is: %s\n&quot;,processor_name);<br>
        }<br>
<br>
        MPI_Finalize();<br>
        return(0);<br>
}<br>
<br>
<br>
I&#39;m curious if this is a bug in the way openMPI interprets SLURM<br>
environment variables. If you have any ideas or need any more information<br>
let me know.<br>
<br>
<br>
Thanks.<br>
Matt<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br>

