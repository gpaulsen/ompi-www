<div dir="ltr"><div>Jeff,<br><br></div>I have another question. I thought MPI_Test is a local call, meaning it doesn&#39;t send/receive message. Am I misunderstanding something? Thanks again.<br><div><div><div class="gmail_extra"><br clear="all"><div><div class="gmail_signature"><div dir="ltr">Best regards,<div>Zhen</div></div></div></div>
<br><div class="gmail_quote">On Thu, May 5, 2016 at 9:45 PM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">It&#39;s taking so long because you are sleeping for .1 second between calling MPI_Test().<br>
<br>
The TCP transport is only sending a few fragments of your message during each iteration through MPI_Test (because, by definition, it has to return &quot;immediately&quot;).  Other transports do better handing off large messages like this to hardware for asynchronous progress.<br>
<br>
Additionally, in the upcoming v2.0.0 release is a non-default option to enable an asynchronous progress thread for the TCP transport.  We&#39;re up to v2.0.0rc2; you can give that async TCP support a whirl, if you want.  Pass &quot;--mca btl_tcp_progress_thread 1&quot; on the mpirun command line to enable the TCP progress thread to try it.<br>
<div><div class="h5"><br>
<br>
&gt; On May 4, 2016, at 7:40 PM, Zhen Wang &lt;<a href="mailto:toddwz@gmail.com">toddwz@gmail.com</a>&gt; wrote:<br>
&gt;<br>
&gt; Hi,<br>
&gt;<br>
&gt; I&#39;m having a problem with Isend, Recv and Test in Linux Mint 16 Petra. The source is attached.<br>
&gt;<br>
&gt; Open MPI 1.10.2 is configured with<br>
&gt; ./configure --enable-debug --prefix=/home/&lt;me&gt;/Tool/openmpi-1.10.2-debug<br>
&gt;<br>
&gt; The source is built with<br>
&gt; ~/Tool/openmpi-1.10.2-debug/bin/mpiCC a5.cpp<br>
&gt;<br>
&gt; and run in one node with<br>
&gt; ~/Tool/openmpi-1.10.2-debug/bin/mpirun -n 2 ./a.out<br>
&gt;<br>
&gt; The output is in the end. What puzzles me is why MPI_Test is called so many times, and it takes so long to send a message. Am I doing something wrong? I&#39;m simulating a more complicated program: MPI 0 Isends data to MPI 1, computes (usleep here), and calls Test to check if data are sent. MPI 1 Recvs data, and computes.<br>
&gt;<br>
&gt; Thanks in advance.<br>
&gt;<br>
&gt;<br>
&gt; Best regards,<br>
&gt; Zhen<br>
&gt;<br>
&gt; MPI 0: Isend of 0 started at 20:32:35.<br>
&gt; MPI 1: Recv of 0 started at 20:32:35.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:35.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:35.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:35.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:35.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:35.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:35.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:36.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:36.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:36.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:36.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:36.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:36.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:36.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:36.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:36.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:37.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:37.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:37.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:37.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:37.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:37.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:37.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:37.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:37.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:37.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:38.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:38.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:38.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:38.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:38.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:38.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:38.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:38.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:38.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:38.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:39.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:39.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:39.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:39.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:39.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:39.<br>
&gt; MPI 1: Recv of 0 finished at 20:32:39.<br>
&gt; MPI 0: MPI_Test of 0 at 20:32:39.<br>
&gt; MPI 0: Isend of 0 finished at 20:32:39.<br>
&gt;<br>
</div></div>&gt; &lt;a5.cpp&gt;_______________________________________________<br>
<span class="">&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</span>&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/05/29085.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/05/29085.php</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" rel="noreferrer" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<span class=""><br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</span>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/05/29105.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/05/29105.php</a><br>
</blockquote></div><br></div></div></div></div>

