<div dir="ltr"><div>Mainly responding to Ralph&#39;s comments.  </div><div> </div><div>In HLA a federate (MPI process) can join and leave a federation (MPI collective) independently from other federates.  And rejoin later.</div>
<div> </div><div>---John</div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Mon, Apr 22, 2013 at 11:20 AM, George Bosilca <span dir="ltr">&lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank">bosilca@icl.utk.edu</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word"><div><div><div>On Apr 19, 2013, at 17:00 , John Chludzinski &lt;<a href="mailto:john.chludzinski@gmail.com" target="_blank">john.chludzinski@gmail.com</a>&gt; wrote:</div>
<br><blockquote type="cite"><div dir="ltr"><div>So the apparent conclusion to this thread is that an (Open)MPI based RTI is very doable - if we allow for the future development of dynamic joining and leaving of the MPI collective?</div>
</div></blockquote><div><br></div><div>John,</div><div><br></div><div>What do you mean by dynamically joining and leaving of the MPI collective? </div><div><br></div><div>There are quite a few functions in MPI to dynamically join and disconnect processes (MPI_Spawn, MPI_Connect, MPI_Comm_join). So if your processes __always__ leave cleanly (using the defined MPI pattern of comm_disconnect + comm_free), you might be lucky enough to have this working today. If you want to support processes leaving for reasons outside of your control (such as crash) you do not have an option today in MPI, you need to use some extension (such as ULFM).</div>
<span class="HOEnZb"><font color="#888888"><div><br></div><div>  George.</div></font></span><div><div class="h5"><div><div><br></div><div><br><div></div></div></div><br><blockquote type="cite"><div dir="ltr"><div> </div><div>

---John</div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Wed, Apr 17, 2013 at 12:45 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>

<blockquote style="margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid" class="gmail_quote"><div style="word-wrap:break-word">Thanks for the clarification - very interesting indeed! I&#39;ll look at it more closely.<div>

<div><div><br></div><div><br><div><div>On Apr 17, 2013, at 9:20 AM, George Bosilca &lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank">bosilca@icl.utk.edu</a>&gt; wrote:</div><br><blockquote type="cite"><div style="word-wrap:break-word">

<div><div>On Apr 16, 2013, at 15:51 , Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt; wrote:</div><br><blockquote type="cite"><div style="word-wrap:break-word">Just curious: I thought ULFM dealt with recovering an MPI job where one or more processes fail. Is this correct?</div>

</blockquote><div><br></div><div>It depends what is the definition of &quot;recovering&quot; you take. ULFM is about leaving the processes that remains (after a fault or a disconnect) in a state that allow them to continue to make progress. It is not about recovering processes, or user data, but it does provide the minimalistic set of functionalities to allow application to do this, if needed (revoke, agreement and shrink).</div>

<br><blockquote type="cite"><div style="word-wrap:break-word">HLA/RTI consists of processes that start at random times, run to completion, and then exit normally. While a failure could occur, most process terminations are normal and there is no need/intent to revive them. </div>

</blockquote><div><br></div><div>As I said above, there is no revival of processes in ULFM, and it was never our intent to have such feature. The dynamic world is to be constructed using MPI-2 constructs (MPI_Spawn or MPI_Connect/Accept or even MPI_Join).</div>

<br><blockquote type="cite"><div style="word-wrap:break-word"><div>So it&#39;s mostly a case of massively exercising MPI&#39;s dynamic connect/accept/disconnect functions.</div><div><br></div><div>Do ULFM&#39;s structures have some utility for that purpose?</div>

</div></blockquote><div><br></div><div>Absolutely. If the process that leaves instead of calling MPI_Finalize calls exit() this will be interpreted by the version of the runtime in ULFM as an event triggering a report. All the ensuing mechanisms are then activated and the application can react to this event with the most meaningful approach it can envision.</div>

<div><br></div>  George.</div><div><br><blockquote type="cite"><div style="word-wrap:break-word"><div><br></div><div><br><div><div>On Apr 16, 2013, at 3:20 AM, George Bosilca &lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank">bosilca@icl.utk.edu</a>&gt; wrote:</div>

<br><blockquote type="cite"><div style="word-wrap:break-word">There is an ongoing effort to address the potential volatility of processes in MPI called ULFM. There is a working version available at <a href="http://fault-tolerance.org/" target="_blank">http://fault-tolerance.org</a>. It supports TCP, sm and IB (mostly). You will find some examples, and the document explaining the additional constructs needed in MPI to achieve this.<div>

<div><br></div><div>  George.</div><div><br><div><div>On Apr 15, 2013, at 17:29 , John Chludzinski &lt;<a href="mailto:john.chludzinski@gmail.com" target="_blank">john.chludzinski@gmail.com</a>&gt; wrote:</div><br><blockquote type="cite">

<div dir="ltr"><div>That would seem to preclude its use for an RTI.  Unless you have a card up your sleeve?</div><div> </div><div>---John</div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Mon, Apr 15, 2013 at 11:23 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>


<blockquote style="margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid" class="gmail_quote"><div style="word-wrap:break-word">It isn&#39;t the fact that there are multiple programs being used - we support that just fine. The problem with HLA/RTI is that it allows programs to come/go at will - i.e., not every program has to start at the same time, nor complete at the same time. MPI requires that all programs be executing at the beginning, and that all call finalize prior to anyone exiting.<div>


<div><div><br></div><div><br></div><div><div><div>On Apr 15, 2013, at 8:14 AM, John Chludzinski &lt;<a href="mailto:john.chludzinski@gmail.com" target="_blank">john.chludzinski@gmail.com</a>&gt; wrote:</div><br>
<blockquote type="cite"><div dir="ltr"><div>I just received an e-mail notifying me that MPI-2 supports MPMD.  This would seen to be just what the doctor ordered?</div><div> </div><div>---John</div></div><div class="gmail_extra">


<br><br><div class="gmail_quote">
On Mon, Apr 15, 2013 at 11:10 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote style="margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid" class="gmail_quote">



FWIW: some of us are working on a variant of MPI that would indeed support what you describe - it would support send/recv (i.e., MPI-1), but not collectives, and so would allow communication between arbitrary programs.<br>




<br>
Not specifically targeting HLA/RTI, though I suppose a wrapper that conformed to that standard could be created.<br>
<div><br>
On Apr 15, 2013, at 7:50 AM, John Chludzinski &lt;<a href="mailto:john.chludzinski@gmail.com" target="_blank">john.chludzinski@gmail.com</a>&gt; wrote:<br>
<br>
&gt; This would be a departure from the SPMD paradigm that seems central to<br>
&gt; MPI&#39;s design. Each process would be a completely different program<br>
&gt; (piece of code) and I&#39;m not sure how well that would working using<br>
&gt; MPI?<br>
&gt;<br>
&gt; BTW, MPI is commonly used in the parallel discrete even world for<br>
&gt; communication between LPs (federates in HLA). But these LPs are<br>
&gt; usually the same program.<br>
&gt;<br>
&gt; ---John<br>
&gt;<br>
&gt; On Mon, Apr 15, 2013 at 10:22 AM, John Chludzinski<br>
&gt; &lt;<a href="mailto:john.chludzinski@gmail.com" target="_blank">john.chludzinski@gmail.com</a>&gt; wrote:<br>
&gt;&gt; Is anyone aware of an MPI based HLA/RTI (DoD High Level Architecture<br>
&gt;&gt; (HLA) / Runtime Infrastructure)?<br>
&gt;&gt;<br>
&gt;&gt; ---John<br>
</div>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>


</div><br></div></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>

</div><br></div></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>

</div><br></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>

</div><br></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>

</div><br></div></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div></div></div><br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>

