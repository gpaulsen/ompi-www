<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">“We” do check the available cores - which is why I asked for details :-)<div class=""><br class=""></div><div class=""><br class=""><div style=""><blockquote type="cite" class=""><div class="">On Sep 15, 2015, at 7:10 PM, Gilles Gouaillardet &lt;<a href="mailto:gilles.gouaillardet@gmail.com" class="">gilles.gouaillardet@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class="">Ralph,<div class=""><br class=""></div><div class="">my guess is that cupset is set by the batch manager (slurm?)</div><div class="">so I think this is an ompi bug/missing feature :</div><div class="">"we"&nbsp;should check the available cores (4 in this case because of cpuset)</div><div class="">instead of the online cores (8 in this case)</div><div class="">I wrote "we" because it could either be ompi or hwloc, or ompi should ask the correct info to hwloc if it is available in hwloc.</div><div class=""><br class=""></div><div class="">makes sense ?</div><div class=""><br class=""></div><div class="">Brice, can you please&nbsp;comment on hwloc and cpuset ?</div><div class=""><br class=""></div><div class="">Cheers,</div><div class=""><br class=""></div><div class="">Gilles</div><div class=""><br class=""><br class="">On Wednesday, September 16, 2015, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" class="">rhc@open-mpi.org</a>&gt; wrote:<br class=""><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word" class="">Not precisely correct. It depends on the environment.<div class=""><br class=""></div><div class="">If there is a resource manager allocating nodes, or you provide a hostfile that specifies the number of slots on the nodes, or you use -host, then we default to no-oversubscribe.<div class=""><br class=""></div><div class="">If you provide a hostfile that doesn’t specify slots, then we use the number of cores we find on each node, and we allow oversubscription.</div><div class=""><br class=""></div><div class="">What is being described sounds like more of a bug than an intended feature. I’d need to know more about it, though, to be sure. Can you tell me how you are specifying this cpuset?</div><div class=""><br class=""></div><div class=""><br class=""><div class=""><blockquote type="cite" class=""><div class="">On Sep 15, 2015, at 4:44 PM, Matt Thompson &lt;<a href="javascript:_e(%7B%7D,'cvml','fortran@gmail.com');" target="_blank" class="">fortran@gmail.com</a>&gt; wrote:</div><br class=""><div class=""><div dir="ltr" class="">Looking at the Open MPI 1.10.0 man page:<div class=""><br class=""></div><div class="">&nbsp; <a href="https://www.open-mpi.org/doc/v1.10/man1/mpirun.1.php" target="_blank" class="">https://www.open-mpi.org/doc/v1.10/man1/mpirun.1.php</a><br class=""></div><div class=""><br class=""></div><div class="">it looks like perhaps -oversubscribe (which was an option) is now the default behavior. Instead we have:</div><div class=""><br class=""></div><div class=""><dt style="font-family:verdana,arial,helvetica;font-size:12px" class=""><b class="">-nooversubscribe, --nooversubscribe</b></dt><dd style="font-family:verdana,arial,helvetica;font-size:12px" class="">Do not oversubscribe any nodes; error (without starting any processes) if the requested number of processes would cause oversubscription. This option implicitly sets "max_slots" equal to the "slots" value for each node.</dd></div><div class=""><br class=""></div><div class=""><div class="">It also looks like -map-by has a way to implement it as well (see man page).</div><div class=""><br class=""></div><div class="">Thanks for letting me/us know about this. On a system of mine I sort of depend on the -nooversubscribe behavior!</div><div class=""><br class=""></div><div class="">Matt</div><div class="">&nbsp;</div></div><div class=""><br class=""></div><div class="gmail_extra"><br class=""><div class="gmail_quote">On Tue, Sep 15, 2015 at 11:17 AM, Patrick Begou <span dir="ltr" class="">&lt;<a href="javascript:_e(%7B%7D,'cvml','Patrick.Begou@legi.grenoble-inp.fr');" target="_blank" class="">Patrick.Begou@legi.grenoble-inp.fr</a>&gt;</span> wrote:<br class=""><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">
  

    
  
  <div bgcolor="#FFFFFF" text="#000000" class="">
    Hi,<br class="">
    <br class="">
    I'm runing OpenMPI 1.10.0 built with Intel 2015 compilers on a Bullx
    System.<br class="">
    I've some troubles with the bind-to core option when using cpuset. <br class="">
    If the cpuset is less than all the cores of a cpu (ex: 4 cores
    allowed on a 8 cores cpus) OpenMPI 1.10.0 allows to overload these
    cores&nbsp; until the maximum number of cores of the cpu.<br class="">
    With this config and because the cpuset only allows 4 cores, I can
    reach 2 processes/core if I use:<br class="">
    <br class="">
    <tt class="">mpirun -np 8 --bind-to core my_application</tt><br class="">
    <br class="">
    OpenMPI 1.7.3 doesn't show the problem with the same situation:<br class="">
    <tt class="">mpirun -np 8 --bind-to-core my_application</tt><br class="">
    returns:<br class="">
    <i class="">A request was made to bind to that would result in binding more</i><i class=""><br class="">
    </i><i class="">processes than cpus on a resource</i><br class="">
    and that's okay of course.<br class="">
    <br class="">
    <br class="">
    Is there a way to avoid this oveloading with OpenMPI 1.10.0 ?<br class="">
    <br class="">
    Thanks<span class=""><font color="#888888" class=""><br class="">
    <br class="">
    Patrick<br class="">
    <br class="">
    <pre cols="80" class="">-- 
===================================================================
|  Equipe M.O.S.T.         |                                      |
|  Patrick BEGOU           | <a href="javascript:_e(%7B%7D,'cvml','Patrick.Begou@grenoble-inp.fr');" target="_blank" class="">mailto:Patrick.Begou@grenoble-inp.fr</a> |
|  LEGI                    |                                      |
|  BP 53 X                 | Tel 04 76 82 51 35                   |
|  38041 GRENOBLE CEDEX    | Fax 04 76 82 52 71                   |
===================================================================
</pre>
  </font></span></div>

<br class="">_______________________________________________<br class="">
users mailing list<br class="">
<a href="javascript:_e(%7B%7D,'cvml','users@open-mpi.org');" target="_blank" class="">users@open-mpi.org</a><br class="">
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/09/27575.php" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/09/27575.php</a><br class=""></blockquote></div><br class=""><br clear="all" class=""><div class=""><br class=""></div>-- <br class=""><div class=""><div dir="ltr" class=""><div class=""><div dir="ltr" class=""><div class="">Matt Thompson</div></div></div><blockquote style="margin:0px 0px 0px 40px;border:none;padding:0px" class=""><div class=""><div class=""><div class="">Man Among Men</div></div></div><div class=""><div class=""><div class="">Fulcrum of History</div></div></div></blockquote></div></div>
</div></div>
_______________________________________________<br class="">users mailing list<br class=""><a href="javascript:_e(%7B%7D,'cvml','users@open-mpi.org');" target="_blank" class="">users@open-mpi.org</a><br class="">Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/09/27579.php" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/09/27579.php</a></div></blockquote></div><br class=""></div></div></div></blockquote></div>
_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2015/09/27581.php</div></blockquote></div><br class=""></div></body></html>
