<div dir="ltr"><div><span id="result_box" class="" lang="en"><span class="">My</span> <span class="">operating system is</span> <span class="">opensuse</span> <span class="">12.3</span> <span class="">X86_64</span> </span><span id="result_box" class="" lang="en"><span class="">I use</span></span><span id="result_box" class="" lang="en"> <span class="">openmpi</span> <span class=""> is</span> <span class="">1.6-3.1.2<br>
</span></span></div><span id="result_box" class="" lang="en"><span class="">the message is<br>Open RTE was unable to open the hostfile:<br>    /usr/lib64/mpi/gcc/openmpi/etc/openmpi-default-hostfile<br>Check to make sure the path and filename are correct.<br>
--------------------------------------------------------------------------<br>[linux-mwmw.site:01535] [[25215,0],0] ORTE_ERROR_LOG: Not found in file base/ras_base_allocate.c at line 200<br>[linux-mwmw.site:01535] [[25215,0],0] ORTE_ERROR_LOG: Not found in file base/plm_base_launch_support.c at line 99<br>
[linux-mwmw.site:01535] [[25215,0],0] ORTE_ERROR_LOG: Not found in file plm_rsh_module.c at line 1167<br><br></span></span></div><div class="gmail_extra"><br><br><div class="gmail_quote">2013/5/28 Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span><br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Per the email that you forwarded below, I replied to you off list saying that we could figure it out without bothering people, and then post the final resolution back to the list (I do this sometimes when figuring out a problem is going to take a bunch of back-and-forth).  On May 25th, I replied to your mail asking for more information that you did not provide.<br>

<br>
Do you want to go reply to that mail from May 25?<br>
<br>
<br>
<br>
On May 27, 2013, at 7:05 AM, Hayato KUNIIE &lt;<a href="mailto:kuni255@oita.email.ne.jp">kuni255@oita.email.ne.jp</a>&gt; wrote:<br>
<br>
&gt; Hello<br>
&gt;<br>
&gt; I posted this topic in last week.<br>
&gt; But Information about this problem was few.<br>
&gt; And I post again with more information.<br>
&gt;<br>
&gt; I build bewulf type PC Cluster (Cent OS release 6.4). And I studing<br>
&gt; about MPI.(Open MPI Ver.1.6.4) I tried following sample which using<br>
&gt; MPI_REDUCE (FORTRAN).<br>
&gt;<br>
&gt; Then, following Error occured.<br>
&gt; --------------------------------------<br>
&gt; [bwslv01:30793] *** An error occurred in MPI_Reduce: the reduction<br>
&gt; operation MPI_SUM is not defined on the MPI_INTEGER datatype<br>
&gt; [bwslv01:30793] *** on communicator MPI_COMM_WORLD<br>
&gt; [bwslv01:30793] *** MPI_ERR_OP: invalid reduce operation<br>
&gt; [bwslv01:30793] *** MPI_ERRORS_ARE_FATAL: your MPI job will now abort<br>
&gt; -------------------------------------- All informaion is showd in attached file err.log. and source file is attached as main.f<br>
&gt;<br>
&gt; This cluster system consist of one head node and 2 slave nodes.<br>
&gt; And sharing home directory in head node by NFS. so Open MPI is installed<br>
&gt; each nodes.<br>
&gt;<br>
&gt; When I test this program on only head node, program is run correctly.<br>
&gt; and output result.<br>
&gt; But When I test this program on only slave node, same error occured.<br>
&gt;<br>
&gt; Please tell me, good idea or advise.<br>
&gt;<br>
&gt; Other information is included attached file.<br>
&gt; Following is construction of directories .<br>
&gt; ompiReport<br>
&gt; ├── head<br>
&gt; │ ├── config.log // Item 3 on help page<br>
&gt; │ ├── ifocnfig.txt // Item 8 on help page<br>
&gt; │ ├── lstopo.txt // Item 5 on help page<br>
&gt; | |---- PATH.txt // Item 7 on help page<br>
&gt; | |---- LD_LIBRARY_PATH.txt // Item 7 on help page<br>
&gt; │ └── ompi_info_all.txt // Item 4 on help page<br>
&gt; ├── ompi_info_full.txt // Item 6 on help page<br>
&gt; |---- main.f // source file<br>
&gt; |---- err.log // error message<br>
&gt; ├── slv01<br>
&gt; │ ├── config.log // Item 3 on help page<br>
&gt; │ ├── ifconfig.txt // Item 8 on help page<br>
&gt; │ ├── lstopo.txt // Item 5 on help page<br>
&gt; | |---- PATH.txt // Item 7 on help page<br>
&gt; | |---- LD_LIBRARY_PATH.txt // Item 7 on help page<br>
&gt; │ └── ompi_info_all.txt // Item 4 on help page<br>
&gt; └── slv02<br>
&gt; ├── config.log // Item 3 on help page<br>
&gt; ├── ifconfig.txt // Item 8 on help page<br>
&gt; ├── lstopo.txt // Item 5 on help page<br>
&gt; |---- PATH.txt // Item 7 on help page<br>
&gt; |---- LD_LIBRARY_PATH.txt // Item 7 on help page<br>
&gt; └── ompi_info_all.txt // Item 4 on help page<br>
&gt;<br>
&gt; 3 directories, 13 files<br>
&gt;<br>
&gt; Best regards<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; (2013/05/16 23:24), Jeff Squyres (jsquyres) wrote:<br>
&gt;&gt; (OFF LIST)<br>
&gt;&gt;<br>
&gt;&gt; Let&#39;s figure this out off-list and post the final resolution back to the list.<br>
&gt;&gt;<br>
&gt;&gt; This is quite odd.<br>
&gt;&gt;<br>
&gt;&gt; You launched this mpirun from a single node, right?  I&#39;m trying to make sure that you&#39;re doing non-interactive logins on the remote nodes to find the ompi_info&#39;s, because sometimes there&#39;s a difference between paths that are set for interactive and non-interactive logins.<br>

&gt;&gt;<br>
&gt;&gt; Can you send all the information listed here:<br>
&gt;&gt;<br>
&gt;&gt;     <a href="http://www.open-mpi.org/community/help/" target="_blank">http://www.open-mpi.org/community/help/</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; On May 16, 2013, at 9:53 AM, Hayato KUNIIE &lt;<a href="mailto:kuni255@oita.email.ne.jp">kuni255@oita.email.ne.jp</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt;&gt; Following is result of mpirun ompi_info on three_nodes.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; three nodes version is same.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Package: Open MPI root@bwhead.clnet Distribution  Open MPI root@bwslv01 Distribution  Open MPI root@bwslv02 Distribution<br>
&gt;&gt;&gt; Open MPI: 1.6.4  1.6.4  1.6.4<br>
&gt;&gt;&gt; Open MPI SVN revision: r28081  r28081  r28081<br>
&gt;&gt;&gt; Open MPI release date: Feb 19, 2013  Feb 19, 2013  Feb 19, 2013<br>
&gt;&gt;&gt; Open RTE: 1.6.4  1.6.4  1.6.4<br>
&gt;&gt;&gt; Open RTE SVN revision: r28081  r28081  r28081<br>
&gt;&gt;&gt; Open RTE release date: Feb 19, 2013  Feb 19, 2013  Feb 19, 2013<br>
&gt;&gt;&gt; OPAL: 1.6.4  1.6.4  1.6.4<br>
&gt;&gt;&gt; OPAL SVN revision: r28081  r28081  r28081<br>
&gt;&gt;&gt; OPAL release date: Feb 19, 2013  Feb 19, 2013  Feb 19, 2013<br>
&gt;&gt;&gt; MPI API: 2.1  2.1  2.1<br>
&gt;&gt;&gt; Ident string: 1.6.4  1.6.4  1.6.4<br>
&gt;&gt;&gt; Prefix: /usr/local  /usr/local  /usr/local<br>
&gt;&gt;&gt; Configured architecture: x86_64-unknown-linux-gnu x86_64-unknown-linux-gnu  x86_64-unknown-linux-gnu<br>
&gt;&gt;&gt; Configure host: bwhead.clnet  bwslv01  bwslv02<br>
&gt;&gt;&gt; Configured by: root  root  root<br>
&gt;&gt;&gt; Configured on: Wed May  8 20:38:14 JST 2013 45 JST 2013 29 JST 2013<br>
&gt;&gt;&gt; Configure host: bwhead.clnet  bwslv01  bwslv02<br>
&gt;&gt;&gt; Built by: root  root  root<br>
&gt;&gt;&gt; Built on: Wed May  8 20:48:44 JST 2013 43 JST 2013 38 JST 2013<br>
&gt;&gt;&gt; Built host: bwhead.clnet  bwslv01  bwslv02<br>
&gt;&gt;&gt; C bindings: yes  yes  yes<br>
&gt;&gt;&gt; C++ bindings: yes  yes  yes<br>
&gt;&gt;&gt; Fortran77 bindings: yes (all)  yes (all)  yes (all)<br>
&gt;&gt;&gt; Fortran90 bindings: yes  yes  yes<br>
&gt;&gt;&gt; Fortran90 bindings size: small  small  small<br>
&gt;&gt;&gt; C compiler: gcc  gcc  gcc<br>
&gt;&gt;&gt; C compiler absolute: /usr/bin/gcc  /usr/bin/gcc  /usr/bin/gcc<br>
&gt;&gt;&gt; C compiler family name: GNU  GNU  GNU<br>
&gt;&gt;&gt; C compiler version: 4.4.7  4.4.7  4.4.7<br>
&gt;&gt;&gt; C++ compiler: g++  g++  g++<br>
&gt;&gt;&gt; C++ compiler absolute: /usr/bin/g++  /usr/bin/g++  /usr/bin/g++<br>
&gt;&gt;&gt; Fortran77 compiler: gfortran  gfortran  gfortran<br>
&gt;&gt;&gt; Fortran77 compiler abs: /usr/bin/gfortran  /usr/bin/gfortran /usr/bin/gfortran<br>
&gt;&gt;&gt; Fortran90 compiler: gfortran  gfortran  gfortran<br>
&gt;&gt;&gt; Fortran90 compiler abs: /usr/bin/gfortran  /usr/bin/gfortran /usr/bin/gfortran<br>
&gt;&gt;&gt; C profiling: yes  yes  yes<br>
&gt;&gt;&gt; C++ profiling: yes  yes  yes<br>
&gt;&gt;&gt; Fortran77 profiling: yes  yes  yes<br>
&gt;&gt;&gt; Fortran90 profiling: yes  yes  yes<br>
&gt;&gt;&gt; C++ exceptions: no  no  no<br>
&gt;&gt;&gt; Thread support: posix (MPI_THREAD_MULTIPLE: no, progress: no) no)  no)<br>
&gt;&gt;&gt; Sparse Groups: no  no  no<br>
&gt;&gt;&gt; Internal debug support: no  no  no<br>
&gt;&gt;&gt; MPI interface warnings: no  no  no<br>
&gt;&gt;&gt; MPI parameter check: runtime  runtime  runtime<br>
&gt;&gt;&gt; Memory profiling support: no  no  no<br>
&gt;&gt;&gt; Memory debugging support: no  no  no<br>
&gt;&gt;&gt; libltdl support: yes  yes  yes<br>
&gt;&gt;&gt; Heterogeneous support: no  no  no<br>
&gt;&gt;&gt; mpirun default --prefix: no  no  no<br>
&gt;&gt;&gt; MPI I/O support: yes  yes  yes<br>
&gt;&gt;&gt; MPI_WTIME support: gettimeofday  gettimeofday  gettimeofday<br>
&gt;&gt;&gt; Symbol vis. support: yes  yes  yes<br>
&gt;&gt;&gt; Host topology support: yes  yes  yes<br>
&gt;&gt;&gt; MPI extensions: affinity example  affinity example  affinity example<br>
&gt;&gt;&gt; FT Checkpoint support: no (checkpoint thread: no)  no)  no)<br>
&gt;&gt;&gt; VampirTrace support: yes  yes  yes<br>
&gt;&gt;&gt; MPI_MAX_PROCESSOR_NAME: 256  256  256<br>
&gt;&gt;&gt; MPI_MAX_ERROR_STRING: 256  256  256<br>
&gt;&gt;&gt; MPI_MAX_OBJECT_NAME: 64  64  64<br>
&gt;&gt;&gt; MPI_MAX_INFO_KEY: 36  36  36<br>
&gt;&gt;&gt; MPI_MAX_INFO_VAL: 256  256  256<br>
&gt;&gt;&gt; MPI_MAX_PORT_NAME: 1024  1024  1024<br>
&gt;&gt;&gt; MPI_MAX_DATAREP_STRING: 128  128  128<br>
&gt;&gt;&gt; Package: Open MPI root@bwslv01 Distribution  execinfo (MCA v2.0, API v2.0, Component v1.6.4)  execinfo (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Open MPI: 1.6.4  linux (MCA v2.0, API v2.0, Component v1.6.4) linux (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Open MPI SVN revision: r28081  hwloc (MCA v2.0, API v2.0, Component v1.6.4)  hwloc (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Open MPI release date: Feb 19, 2013  auto_detect (MCA v2.0, API v2.0, Component v1.6.4)  auto_detect (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Open RTE: 1.6.4  file (MCA v2.0, API v2.0, Component v1.6.4)  file (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Open RTE SVN revision: r28081  mmap (MCA v2.0, API v2.0, Component v1.6.4)  mmap (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Open RTE release date: Feb 19, 2013  posix (MCA v2.0, API v2.0, Component v1.6.4)  posix (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; OPAL: 1.6.4  sysv (MCA v2.0, API v2.0, Component v1.6.4)  sysv (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; OPAL SVN revision: r28081  first_use (MCA v2.0, API v2.0, Component v1.6.4)  first_use (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; OPAL release date: Feb 19, 2013  hwloc (MCA v2.0, API v2.0, Component v1.6.4)  hwloc (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MPI API: 2.1  linux (MCA v2.0, API v2.0, Component v1.6.4)  linux (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Ident string: 1.6.4  env (MCA v2.0, API v2.0, Component v1.6.4) env (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Prefix: /usr/local  config (MCA v2.0, API v2.0, Component v1.6.4) config (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Configured architecture: x86_64-unknown-linux-gnu  linux (MCA v2.0, API v2.0, Component v1.6.4)  linux (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Configure host: bwslv01  hwloc132 (MCA v2.0, API v2.0, Component v1.6.4)  hwloc132 (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Configured by: root  orte (MCA v2.0, API v2.0, Component v1.6.4) orte (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Configured on: Wed May  8 20:56:45 JST 2013  orte (MCA v2.0, API v2.0, Component v1.6.4)  orte (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Configure host: bwslv01  basic (MCA v2.0, API v2.0, Component v1.6.4)  basic (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Built by: root  bucket (MCA v2.0, API v2.0, Component v1.6.4) bucket (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Built on: Wed May  8 21:05:43 JST 2013  basic (MCA v2.0, API v2.0, Component v1.6.4)  basic (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Built host: bwslv01  hierarch (MCA v2.0, API v2.0, Component v1.6.4)  hierarch (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; C bindings: yes  inter (MCA v2.0, API v2.0, Component v1.6.4) inter (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; C++ bindings: yes  self (MCA v2.0, API v2.0, Component v1.6.4) self (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Fortran77 bindings: yes (all)  sm (MCA v2.0, API v2.0, Component v1.6.4)  sm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Fortran90 bindings: yes  sync (MCA v2.0, API v2.0, Component v1.6.4)  sync (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Fortran90 bindings size: small  tuned (MCA v2.0, API v2.0, Component v1.6.4)  tuned (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; C compiler: gcc  romio (MCA v2.0, API v2.0, Component v1.6.4) romio (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; C compiler absolute: /usr/bin/gcc  fake (MCA v2.0, API v2.0, Component v1.6.4)  fake (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; C compiler family name: GNU  rdma (MCA v2.0, API v2.0, Component v1.6.4)  rdma (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; C compiler version: 4.4.7  sm (MCA v2.0, API v2.0, Component v1.6.4)  sm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; C++ compiler: g++  bfo (MCA v2.0, API v2.0, Component v1.6.4)  bfo (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; C++ compiler absolute: /usr/bin/g++  csum (MCA v2.0, API v2.0, Component v1.6.4)  csum (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Fortran77 compiler: gfortran  ob1 (MCA v2.0, API v2.0, Component v1.6.4)  ob1 (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Fortran77 compiler abs: /usr/bin/gfortran  v (MCA v2.0, API v2.0, Component v1.6.4)  v (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Fortran90 compiler: gfortran  r2 (MCA v2.0, API v2.0, Component v1.6.4)  r2 (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Fortran90 compiler abs: /usr/bin/gfortran  vma (MCA v2.0, API v2.0, Component v1.6.4)  vma (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; C profiling: yes  self (MCA v2.0, API v2.0, Component v1.6.4) self (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; C++ profiling: yes  sm (MCA v2.0, API v2.0, Component v1.6.4)  sm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Fortran77 profiling: yes  tcp (MCA v2.0, API v2.0, Component v1.6.4)  tcp (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Fortran90 profiling: yes  unity (MCA v2.0, API v2.0, Component v1.6.4)  unity (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; C++ exceptions: no  pt2pt (MCA v2.0, API v2.0, Component v1.6.4) pt2pt (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Thread support: posix (MPI_THREAD_MULTIPLE: no, progress: no) rdma (MCA v2.0, API v2.0, Component v1.6.4)  rdma (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Sparse Groups: no  hnp (MCA v2.0, API v2.0, Component v1.6.4)  hnp (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Internal debug support: no  orted (MCA v2.0, API v2.0, Component v1.6.4)  orted (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MPI interface warnings: no  tool (MCA v2.0, API v2.0, Component v1.6.4)  tool (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MPI parameter check: runtime  tcp (MCA v2.0, API v2.0, Component v1.6.4)  tcp (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Memory profiling support: no  default (MCA v2.0, API v2.0, Component v1.6.4)  default (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Memory debugging support: no  cm (MCA v2.0, API v2.0, Component v1.6.4)  cm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; libltdl support: yes  loadleveler (MCA v2.0, API v2.0, Component v1.6.4)  loadleveler (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Heterogeneous support: no  slurm (MCA v2.0, API v2.0, Component v1.6.4)  slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; mpirun default --prefix: no  load_balance (MCA v2.0, API v2.0, Component v1.6.4)  load_balance (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MPI I/O support: yes  rank_file (MCA v2.0, API v2.0, Component v1.6.4)  rank_file (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MPI_WTIME support: gettimeofday  resilient (MCA v2.0, API v2.0, Component v1.6.4)  resilient (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Symbol vis. support: yes  round_robin (MCA v2.0, API v2.0, Component v1.6.4)  round_robin (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Host topology support: yes  seq (MCA v2.0, API v2.0, Component v1.6.4)  seq (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MPI extensions: affinity example  topo (MCA v2.0, API v2.0, Component v1.6.4)  topo (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; FT Checkpoint support: no (checkpoint thread: no)  oob (MCA v2.0, API v2.0, Component v1.6.4)  oob (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; VampirTrace support: yes  binomial (MCA v2.0, API v2.0, Component v1.6.4)  binomial (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MPI_MAX_PROCESSOR_NAME: 256  cm (MCA v2.0, API v2.0, Component v1.6.4)  cm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MPI_MAX_ERROR_STRING: 256  direct (MCA v2.0, API v2.0, Component v1.6.4)  direct (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MPI_MAX_OBJECT_NAME: 64  linear (MCA v2.0, API v2.0, Component v1.6.4)  linear (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MPI_MAX_INFO_KEY: 36  radix (MCA v2.0, API v2.0, Component v1.6.4)  radix (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MPI_MAX_INFO_VAL: 256  slave (MCA v2.0, API v2.0, Component v1.6.4)  slave (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MPI_MAX_PORT_NAME: 1024  rsh (MCA v2.0, API v2.0, Component v1.6.4)  rsh (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MPI_MAX_DATAREP_STRING: 128  slurm (MCA v2.0, API v2.0, Component v1.6.4)  slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Package: Open MPI root@bwslv02 Distribution  rsh (MCA v2.0, API v2.0, Component v1.6.4)  rsh (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Open MPI: 1.6.4  default (MCA v2.0, API v2.0, Component v1.6.4) default (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Open MPI SVN revision: r28081  env (MCA v2.0, API v2.0, Component v1.6.4)  env (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Open MPI release date: Feb 19, 2013  hnp (MCA v2.0, API v2.0, Component v1.6.4)  hnp (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Open RTE: 1.6.4  singleton (MCA v2.0, API v2.0, Component v1.6.4) singleton (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Open RTE SVN revision: r28081  slave (MCA v2.0, API v2.0, Component v1.6.4)  slave (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Open RTE release date: Feb 19, 2013  slurm (MCA v2.0, API v2.0, Component v1.6.4)  slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; OPAL: 1.6.4  slurmd (MCA v2.0, API v2.0, Component v1.6.4)  slurmd (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; OPAL SVN revision: r28081  tool (MCA v2.0, API v2.0, Component v1.6.4)  tool (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; OPAL release date: Feb 19, 2013  bad (MCA v2.0, API v2.0, Component v1.6.4)  bad (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MPI API: 2.1  basic (MCA v2.0, API v2.0, Component v1.6.4)  basic (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Ident string: 1.6.4  hier (MCA v2.0, API v2.0, Component v1.6.4) hier (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; Prefix: /usr/local  command (MCA v2.0, API v1.0, Component v1.6.4)  command (MCA v2.0, API v1.0, Component v1.6.4)<br>
&gt;&gt;&gt; Configured architecture: x86_64-unknown-linux-gnu  syslog (MCA v2.0, API v1.0, Component v1.6.4)  syslog (MCA v2.0, API v1.0, Component v1.6.4)<br>
&gt;&gt;&gt; Configure host: bwslv02<br>
&gt;&gt;&gt; Configured by: root<br>
&gt;&gt;&gt; Configured on: Wed May  8 20:56:29 JST 2013<br>
&gt;&gt;&gt; Configure host: bwslv02<br>
&gt;&gt;&gt; Built by: root<br>
&gt;&gt;&gt; Built on: Wed May  8 21:05:38 JST 2013<br>
&gt;&gt;&gt; Built host: bwslv02<br>
&gt;&gt;&gt; C bindings: yes<br>
&gt;&gt;&gt; C++ bindings: yes<br>
&gt;&gt;&gt; Fortran77 bindings: yes (all)<br>
&gt;&gt;&gt; Fortran90 bindings: yes<br>
&gt;&gt;&gt; Fortran90 bindings size: small<br>
&gt;&gt;&gt; C compiler: gcc<br>
&gt;&gt;&gt; C compiler absolute: /usr/bin/gcc<br>
&gt;&gt;&gt; C compiler family name: GNU<br>
&gt;&gt;&gt; C compiler version: 4.4.7<br>
&gt;&gt;&gt; C++ compiler: g++<br>
&gt;&gt;&gt; C++ compiler absolute: /usr/bin/g++<br>
&gt;&gt;&gt; Fortran77 compiler: gfortran<br>
&gt;&gt;&gt; Fortran77 compiler abs: /usr/bin/gfortran<br>
&gt;&gt;&gt; Fortran90 compiler: gfortran<br>
&gt;&gt;&gt; Fortran90 compiler abs: /usr/bin/gfortran<br>
&gt;&gt;&gt; C profiling: yes<br>
&gt;&gt;&gt; C++ profiling: yes<br>
&gt;&gt;&gt; Fortran77 profiling: yes<br>
&gt;&gt;&gt; Fortran90 profiling: yes<br>
&gt;&gt;&gt; C++ exceptions: no<br>
&gt;&gt;&gt; Thread support: posix (MPI_THREAD_MULTIPLE: no, progress: no)<br>
&gt;&gt;&gt; Sparse Groups: no<br>
&gt;&gt;&gt; Internal debug support: no<br>
&gt;&gt;&gt; MPI interface warnings: no<br>
&gt;&gt;&gt; MPI parameter check: runtime<br>
&gt;&gt;&gt; Memory profiling support: no<br>
&gt;&gt;&gt; Memory debugging support: no<br>
&gt;&gt;&gt; libltdl support: yes<br>
&gt;&gt;&gt; Heterogeneous support: no<br>
&gt;&gt;&gt; mpirun default --prefix: no<br>
&gt;&gt;&gt; MPI I/O support: yes<br>
&gt;&gt;&gt; MPI_WTIME support: gettimeofday<br>
&gt;&gt;&gt; Symbol vis. support: yes<br>
&gt;&gt;&gt; Host topology support: yes<br>
&gt;&gt;&gt; MPI extensions: affinity example<br>
&gt;&gt;&gt; FT Checkpoint support: no (checkpoint thread: no)<br>
&gt;&gt;&gt; VampirTrace support: yes<br>
&gt;&gt;&gt; MPI_MAX_PROCESSOR_NAME: 256<br>
&gt;&gt;&gt; MPI_MAX_ERROR_STRING: 256<br>
&gt;&gt;&gt; MPI_MAX_OBJECT_NAME: 64<br>
&gt;&gt;&gt; MPI_MAX_INFO_KEY: 36<br>
&gt;&gt;&gt; MPI_MAX_INFO_VAL: 256<br>
&gt;&gt;&gt; MPI_MAX_PORT_NAME: 1024<br>
&gt;&gt;&gt; MPI_MAX_DATAREP_STRING: 128<br>
&gt;&gt;&gt; MCA backtrace: execinfo (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA memory: linux (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA paffinity: hwloc (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA carto: auto_detect (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA carto: file (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA shmem: mmap (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA shmem: posix (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA shmem: sysv (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA maffinity: first_use (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA maffinity: hwloc (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA timer: linux (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA installdirs: env (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA installdirs: config (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA sysinfo: linux (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA hwloc: hwloc132 (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA dpm: orte (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA pubsub: orte (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA allocator: basic (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA allocator: bucket (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: basic (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: hierarch (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: inter (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: self (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: sm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: sync (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: tuned (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA io: romio (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA mpool: fake (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA mpool: rdma (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA mpool: sm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA pml: bfo (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA pml: csum (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA pml: ob1 (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA pml: v (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA bml: r2 (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rcache: vma (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA btl: self (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA btl: sm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA btl: tcp (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA topo: unity (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA osc: pt2pt (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA osc: rdma (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA iof: hnp (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA iof: orted (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA iof: tool (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA oob: tcp (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA odls: default (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ras: cm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ras: loadleveler (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ras: slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: load_balance (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: rank_file (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: resilient (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: round_robin (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: seq (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: topo (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rml: oob (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: binomial (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: cm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: direct (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: linear (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: radix (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: slave (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA plm: rsh (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA plm: slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA filem: rsh (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA errmgr: default (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: env (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: hnp (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: singleton (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: slave (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: slurmd (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: tool (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA grpcomm: bad (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA grpcomm: basic (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA grpcomm: hier (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA notifier: command (MCA v2.0, API v1.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA notifier: syslog (MCA v2.0, API v1.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA backtrace: execinfo (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA memory: linux (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA paffinity: hwloc (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA carto: auto_detect (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA carto: file (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA shmem: mmap (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA shmem: posix (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA shmem: sysv (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA maffinity: first_use (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA maffinity: hwloc (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA timer: linux (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA installdirs: env (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA installdirs: config (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA sysinfo: linux (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA hwloc: hwloc132 (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA dpm: orte (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA pubsub: orte (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA allocator: basic (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA allocator: bucket (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: basic (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: hierarch (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: inter (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: self (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: sm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: sync (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: tuned (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA io: romio (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA mpool: fake (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA mpool: rdma (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA mpool: sm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA pml: bfo (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA pml: csum (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA pml: ob1 (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA pml: v (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA bml: r2 (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rcache: vma (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA btl: self (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA btl: sm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA btl: tcp (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA topo: unity (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA osc: pt2pt (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA osc: rdma (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA iof: hnp (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA iof: orted (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA iof: tool (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA oob: tcp (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA odls: default (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ras: cm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ras: loadleveler (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ras: slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: load_balance (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: rank_file (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: resilient (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: round_robin (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: seq (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: topo (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rml: oob (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: binomial (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: cm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: direct (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: linear (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: radix (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: slave (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA plm: rsh (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA plm: slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA filem: rsh (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA errmgr: default (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: env (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: hnp (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: singleton (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: slave (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: slurmd (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: tool (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA grpcomm: bad (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA grpcomm: basic (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA grpcomm: hier (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA notifier: command (MCA v2.0, API v1.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA notifier: syslog (MCA v2.0, API v1.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA backtrace: execinfo (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA memory: linux (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA paffinity: hwloc (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA carto: auto_detect (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA carto: file (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA shmem: mmap (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA shmem: posix (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA shmem: sysv (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA maffinity: first_use (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA maffinity: hwloc (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA timer: linux (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA installdirs: env (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA installdirs: config (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA sysinfo: linux (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA hwloc: hwloc132 (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA dpm: orte (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA pubsub: orte (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA allocator: basic (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA allocator: bucket (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: basic (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: hierarch (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: inter (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: self (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: sm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: sync (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA coll: tuned (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA io: romio (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA mpool: fake (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA mpool: rdma (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA mpool: sm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA pml: bfo (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA pml: csum (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA pml: ob1 (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA pml: v (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA bml: r2 (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rcache: vma (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA btl: self (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA btl: sm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA btl: tcp (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA topo: unity (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA osc: pt2pt (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA osc: rdma (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA iof: hnp (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA iof: orted (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA iof: tool (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA oob: tcp (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA odls: default (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ras: cm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ras: loadleveler (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ras: slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: load_balance (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: rank_file (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: resilient (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: round_robin (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: seq (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rmaps: topo (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA rml: oob (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: binomial (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: cm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: direct (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: linear (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: radix (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA routed: slave (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA plm: rsh (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA plm: slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA filem: rsh (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA errmgr: default (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: env (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: hnp (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: singleton (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: slave (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: slurmd (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA ess: tool (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA grpcomm: bad (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA grpcomm: basic (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA grpcomm: hier (MCA v2.0, API v2.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA notifier: command (MCA v2.0, API v1.0, Component v1.6.4)<br>
&gt;&gt;&gt; MCA notifier: syslog (MCA v2.0, API v1.0, Component v1.6.4)<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; (2013/05/16 9:12), Jeff Squyres (jsquyres) wrote:<br>
&gt;&gt;&gt;&gt; I am unable to replicate your error -- 1.6.4 has MPI_REDUCE defined on MPI_SUM properly.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Are you absolutely sure you&#39;re using OMPI 1.6.4 on all nodes?<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Try this:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;     mpirun ... ompi_info<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; (insert whatever hostfile and -np value you&#39;re using for your fortran test) and see what is reported as the OMPI version from other nodes.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; On May 15, 2013, at 7:46 AM, Hayato KUNIIE &lt;<a href="mailto:kuni255@oita.email.ne.jp">kuni255@oita.email.ne.jp</a>&gt; wrote:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; I using Ver, 1.6.4 in all nodes.<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; (2013/05/15 7:10), Jeff Squyres (jsquyres) wrote:<br>
&gt;&gt;&gt;&gt;&gt;&gt; Are you sure that you have exactly the same version of Open MPI on all your nodes?<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; On May 14, 2013, at 11:39 AM, Hayato KUNIIE &lt;<a href="mailto:kuni255@oita.email.ne.jp">kuni255@oita.email.ne.jp</a>&gt; wrote:<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hello I&#39;m kuni255<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; I build bewulf type PC Cluster (Cent OS release 6.4). And I studing<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; about MPI.(Open MPI Ver.1.6.4) I tried following sample which using<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; MPI_REDUCE.<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Then, Error occured.<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; This cluster system consist of one head node and 2 slave nodes.<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; And sharing home directory in head node by NFS. so Open MPI is installed<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; each nodes.<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; When I test this program on only head node, program is run correctly.<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; and output result.<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; But When I test this program on only slave node, same error occured.<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Please tell me, good idea : )<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Error message<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; [bwslv01:30793] *** An error occurred in MPI_Reduce: the reduction<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; operation MPI_SUM is not defined on the MPI_INTEGER datatype<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; [bwslv01:30793] *** on communicator MPI_COMM_WORLD<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; [bwslv01:30793] *** MPI_ERR_OP: invalid reduce operation<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; [bwslv01:30793] *** MPI_ERRORS_ARE_FATAL: your MPI job will now abort<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; mpirun has exited due to process rank 1 with PID 30793 on<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; node bwslv01 exiting improperly. There are two reasons this could occur:<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 1. this process did not call &quot;init&quot; before exiting, but others in<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; the job did. This can cause a job to hang indefinitely while it waits<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; for all processes to call &quot;init&quot;. By rule, if one process calls &quot;init&quot;,<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; then ALL processes must call &quot;init&quot; prior to termination.<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 2. this process called &quot;init&quot;, but exited without calling &quot;finalize&quot;.<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; By rule, all processes that call &quot;init&quot; MUST call &quot;finalize&quot; prior to<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; exiting or it will be considered an &quot;abnormal termination&quot;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; This may have caused other processes in the application to be<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; terminated by signals sent by mpirun (as reported here).<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; [bwhead.clnet:02147] 1 more process has sent help message<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; help-mpi-errors.txt / mpi_errors_are_fatal<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; [bwhead.clnet:02147] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; to see all help / error messages<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Fortran90 source code<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; include &#39;mpif.h&#39;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; parameter(nmax=12)<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; integer n(nmax)<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; call mpi_init(ierr)<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; call mpi_comm_size(MPI_COMM_WORLD, isize, ierr)<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; call mpi_comm_rank(MPI_COMM_WORLD, irank, ierr)<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; ista=irank*(nmax/isize) + 1<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; iend=ista+(nmax/isize-1)<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; isum=0<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; do i=1,nmax<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; n(i) = i<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; isum = isum + n(i)<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; end do<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; call mpi_reduce(isum, itmp, 1, MPI_INTEGER, MPI_SUM,<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; &amp; 0, MPI_COMM_WORLD, ierr)<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; if (irank == 0) then<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; isum=itmp<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; WRITE(*,*) isum<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; endif<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; call mpi_finalize(ierr)<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; end<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;<br>
&gt;<br>
&gt;<br>
&gt; &lt;ompiReport.tar.xz&gt;_______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<span class="HOEnZb"><font color="#888888"><br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></font></span></blockquote></div><br></div>

