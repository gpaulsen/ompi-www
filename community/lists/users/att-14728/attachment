Hi, <div>I&#39;m working with Grzegorz on the mentioned problem. </div><div>If I&#39;m correct on checking the firewall settings, &quot;iptables --list&quot; shows an empty list of rules.</div><div>The second host does not have iptables installed at all.</div>

<div><br></div><div>So what can be a next reason of this problem?</div><div><br></div><div>By the way, how can I enforce mpirun to use specific ethernet interface  for connections, if I have several possible? </div><div>
<br>
</div><div>Cheers,</div><div>Krzysztof </div><div><br></div><div><div class="gmail_quote">2010/11/11 Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span><br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
I&#39;d check the firewall settings.  The stack trace indicates that the one host is trying to connect to the other (Open MPI initiates non-blocking TCP connections that can be polled on later).<br>
<div><div></div><div><br>
<br>
On Nov 10, 2010, at 12:46 PM, David Zhang wrote:<br>
<br>
&gt; Have you double checked your firewall settings, TCP/IP settings, and SSH keys are all setup correctly for all machines including the host?<br>
&gt;<br>
&gt; On Wed, Nov 10, 2010 at 2:57 AM, Grzegorz Maj &lt;<a href="mailto:maju3@wp.pl" target="_blank">maju3@wp.pl</a>&gt; wrote:<br>
&gt; Hi all,<br>
&gt; I&#39;ve got a problem with sending messages from one of my machines. It<br>
&gt; appears during MPI_Send/MPI_Recv and MPI_Bcast. The simplest case I&#39;ve<br>
&gt; found is two processes, rank 0 sending a simple message and rank 1<br>
&gt; receiving this message. I execute these processes using mpirun with<br>
&gt; -np 2.<br>
&gt; - when both processes are executed on the host machine, it works fine;<br>
&gt; - when both processes are executed on client machines (both on the<br>
&gt; same or different machines), it works fine;<br>
&gt; - when sender is executed on one of the client machines and receiver<br>
&gt; on the host machine, it works fine;<br>
&gt; - when sender is executed on the host machine and receiver on client<br>
&gt; machine, it blocks.<br>
&gt;<br>
&gt; This last case is my problem. When adding option &#39;--mca<br>
&gt; btl_base_verbose 30&#39; to mpirun, I get:<br>
&gt;<br>
&gt; ----------<br>
&gt; [host:28186] mca: base: components_open: Looking for btl components<br>
&gt; [host:28186] mca: base: components_open: opening btl components<br>
&gt; [host:28186] mca: base: components_open: found loaded component self<br>
&gt; [host:28186] mca: base: components_open: component self has no register function<br>
&gt; [host:28186] mca: base: components_open: component self open function successful<br>
&gt; [host:28186] mca: base: components_open: found loaded component sm<br>
&gt; [host:28186] mca: base: components_open: component sm has no register function<br>
&gt; [host:28186] mca: base: components_open: component sm open function successful<br>
&gt; [host:28186] mca: base: components_open: found loaded component tcp<br>
&gt; [host:28186] mca: base: components_open: component tcp has no register function<br>
&gt; [host:28186] mca: base: components_open: component tcp open function successful<br>
&gt; [host:28186] select: initializing btl component self<br>
&gt; [host:28186] select: init of component self returned success<br>
&gt; [host:28186] select: initializing btl component sm<br>
&gt; [host:28186] select: init of component sm returned success<br>
&gt; [host:28186] select: initializing btl component tcp<br>
&gt; [host:28186] select: init of component tcp returned success<br>
&gt; [client01:19803] mca: base: components_open: Looking for btl components<br>
&gt; [client01:19803] mca: base: components_open: opening btl components<br>
&gt; [client01:19803] mca: base: components_open: found loaded component self<br>
&gt; [client01:19803] mca: base: components_open: component self has no<br>
&gt; register function<br>
&gt; [client01:19803] mca: base: components_open: component self open<br>
&gt; function successful<br>
&gt; [client01:19803] mca: base: components_open: found loaded component sm<br>
&gt; [client01:19803] mca: base: components_open: component sm has no<br>
&gt; register function<br>
&gt; [client01:19803] mca: base: components_open: component sm open<br>
&gt; function successful<br>
&gt; [client01:19803] mca: base: components_open: found loaded component tcp<br>
&gt; [client01:19803] mca: base: components_open: component tcp has no<br>
&gt; register function<br>
&gt; [client01:19803] mca: base: components_open: component tcp open<br>
&gt; function successful<br>
&gt; [client01:19803] select: initializing btl component self<br>
&gt; [client01:19803] select: init of component self returned success<br>
&gt; [client01:19803] select: initializing btl component sm<br>
&gt; [client01:19803] select: init of component sm returned success<br>
&gt; [client01:19803] select: initializing btl component tcp<br>
&gt; [client01:19803] select: init of component tcp returned success<br>
&gt; 00 of 2 host<br>
&gt; [host:28186] btl: tcp: attempting to connect() to address 10.0.7.97 on<br>
&gt; port 53255<br>
&gt; 01 of 2 client01<br>
&gt; ----------<br>
&gt;<br>
&gt; Where lines &quot;00 of 2 host&quot; and &quot;01 of 2 client01&quot; are just my debug<br>
&gt; saying &quot;mpirank of comm_size hostname&quot;. The last but one line appears<br>
&gt; in call to Send:<br>
&gt; MPI::COMM_WORLD.Send(message, 5, MPI::CHAR, 1, 13);<br>
&gt;<br>
&gt; When executing the sender on host with strace, I get:<br>
&gt;<br>
&gt; ----------<br>
&gt; ...<br>
&gt; connect(10, {sa_family=AF_INET, sin_port=htons(1024),<br>
&gt; sin_addr=inet_addr(&quot;10.0.7.97&quot;)}, 16) = -1 EINPROGRESS (Operation now<br>
&gt; in progress)<br>
&gt; poll([{fd=4, events=POLLIN}, {fd=5, events=POLLIN}, {fd=6,<br>
&gt; events=POLLIN}, {fd=7, events=POLLIN}, {fd=8, events=POLLIN}, {fd=9,<br>
&gt; events=POLLIN}, {fd=10, events=POLLOUT}], 7, 0) = 0 (Timeout)<br>
&gt; poll([{fd=4, events=POLLIN}, {fd=5, events=POLLIN}, {fd=6,<br>
&gt; events=POLLIN}, {fd=7, events=POLLIN}, {fd=8, events=POLLIN}, {fd=9,<br>
&gt; events=POLLIN}, {fd=10, events=POLLOUT}], 7, 0) = 1 ([{fd=10,<br>
&gt; revents=POLLOUT}])<br>
&gt; getsockopt(10, SOL_SOCKET, SO_ERROR, [0], [4]) = 0<br>
&gt; send(10, &quot;D\227\0\1\0\0\0\0&quot;, 8, 0)     = 8<br>
&gt; poll([{fd=4, events=POLLIN}, {fd=5, events=POLLIN}, {fd=6,<br>
&gt; events=POLLIN}, {fd=7, events=POLLIN}, {fd=8, events=POLLIN}, {fd=9,<br>
&gt; events=POLLIN}, {fd=10, events=POLLIN}], 7, 0) = 0 (Timeout)<br>
&gt; poll([{fd=4, events=POLLIN}, {fd=5, events=POLLIN}, {fd=6,<br>
&gt; events=POLLIN}, {fd=7, events=POLLIN}, {fd=8, events=POLLIN}, {fd=9,<br>
&gt; events=POLLIN}, {fd=10, events=POLLIN}], 7, 0) = 1 ([{fd=10,<br>
&gt; revents=POLLIN}])<br>
&gt; recv(10, &quot;&quot;, 8, 0)                      = 0<br>
&gt; close(10)                               = 0<br>
&gt; poll([{fd=4, events=POLLIN}, {fd=5, events=POLLIN}, {fd=6,<br>
&gt; events=POLLIN}, {fd=7, events=POLLIN}, {fd=8, events=POLLIN}, {fd=9,<br>
&gt; events=POLLIN}], 6, 0) = 0 (Timeout)<br>
&gt; poll([{fd=4, events=POLLIN}, {fd=5, events=POLLIN}, {fd=6,<br>
&gt; events=POLLIN}, {fd=7, events=POLLIN}, {fd=8, events=POLLIN}, {fd=9,<br>
&gt; events=POLLIN}], 6, 0) = 0 (Timeout)<br>
&gt; poll([{fd=4, events=POLLIN}, {fd=5, events=POLLIN}, {fd=6,<br>
&gt; events=POLLIN}, {fd=7, events=POLLIN}, {fd=8, events=POLLIN}, {fd=9,<br>
&gt; events=POLLIN}], 6, 0) = 0 (Timeout)<br>
&gt; poll([{fd=4, events=POLLIN}, {fd=5, events=POLLIN}, {fd=6,<br>
&gt; events=POLLIN}, {fd=7, events=POLLIN}, {fd=8, events=POLLIN}, {fd=9,<br>
&gt; events=POLLIN}], 6, 0) = 0 (Timeout)<br>
&gt; poll([{fd=4, events=POLLIN}, {fd=5, events=POLLIN}, {fd=6,<br>
&gt; events=POLLIN}, {fd=7, events=POLLIN}, {fd=8, events=POLLIN}, {fd=9,<br>
&gt; events=POLLIN}], 6, 0) = 0 (Timeout)<br>
&gt; ...<br>
&gt; (forever)<br>
&gt; ...<br>
&gt; ----------<br>
&gt;<br>
&gt; For me it looks like the above connect is responsible for establishing<br>
&gt; connection, but I&#39;m afraid I don&#39;t understand what those calls for<br>
&gt; poll are supposed to do.<br>
&gt;<br>
&gt; Attaching gdb to the sender gives me:<br>
&gt;<br>
&gt; ----------<br>
&gt; (gdb) bt<br>
&gt; #0  0xffffe410 in __kernel_vsyscall ()<br>
&gt; #1  0x0064993b in poll () from /lib/libc.so.6<br>
&gt; #2  0xf7df07b5 in poll_dispatch () from /home/gmaj/openmpi/lib/libopen-pal.so.0<br>
&gt; #3  0xf7def8c3 in opal_event_base_loop () from<br>
&gt; /home/gmaj/openmpi/lib/libopen-pal.so.0<br>
&gt; #4  0xf7defbe7 in opal_event_loop () from<br>
&gt; /home/gmaj/openmpi/lib/libopen-pal.so.0<br>
&gt; #5  0xf7de323b in opal_progress () from /home/gmaj/openmpi/lib/libopen-pal.so.0<br>
&gt; #6  0xf7c51455 in mca_pml_ob1_send () from<br>
&gt; /home/gmaj/openmpi/lib/openmpi/mca_pml_ob1.so<br>
&gt; #7  0xf7ed9c60 in PMPI_Send () from /home/gmaj/openmpi/lib/libmpi.so.0<br>
&gt; #8  0x0804e900 in main ()<br>
&gt; ----------<br>
&gt;<br>
&gt; If anybody knows what may cause this problem or what may I do to find<br>
&gt; the reason, any help is appreciated.<br>
&gt;<br>
&gt; My open-mpi is version 1.4.1.<br>
&gt;<br>
&gt;<br>
&gt; Regards,<br>
&gt; Grzegorz Maj<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; David Zhang<br>
&gt; University of California, San Diego<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
--<br>
</div></div><font color="#888888">Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a><br>
For corporate legal information go to:<br>
<a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
</font><div><div></div><div><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>
</div>

