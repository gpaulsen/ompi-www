<div dir="ltr"><div>No worries :)</div></div><div class="gmail_extra"><br><div class="gmail_quote">2014-11-27 14:20 GMT+01:00 Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span>:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Many thanks!<br>
<br>
Note that it&#39;s a holiday week here in the US -- I&#39;m only on for a short time here this morning; I&#39;ll likely disappear again shortly until next week.  :-)<br>
<div><div class="h5"><br>
<br>
<br>
On Nov 27, 2014, at 8:12 AM, Nick Papior Andersen &lt;<a href="mailto:nickpapior@gmail.com">nickpapior@gmail.com</a>&gt; wrote:<br>
<br>
&gt; Sure, I will make the changes and commit to make them OMPI specific.<br>
&gt;<br>
&gt; I will post forward my problems on the devel list.<br>
&gt;<br>
&gt; I will keep you posted. :)<br>
&gt;<br>
&gt; 2014-11-27 13:58 GMT+01:00 Jeff Squyres (jsquyres) &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;:<br>
&gt; On Nov 26, 2014, at 2:08 PM, Nick Papior Andersen &lt;<a href="mailto:nickpapior@gmail.com">nickpapior@gmail.com</a>&gt; wrote:<br>
&gt;<br>
&gt; &gt; Here is my commit-msg:<br>
&gt; &gt; &quot;<br>
&gt; &gt; We can now split communicators based on hwloc full capabilities up to BOARD.<br>
&gt; &gt; I.e.:<br>
&gt; &gt; HWTHREAD,CORE,L1CACHE,L2CACHE,L3CACHE,SOCKET,NUMA,NODE,BOARD<br>
&gt; &gt; where NODE is the same as SHARED.<br>
&gt; &gt; &quot;<br>
&gt; &gt;<br>
&gt; &gt; Maybe what I did could be useful somehow?<br>
&gt; &gt; I mean to achieve the effect one could do:<br>
&gt; &gt; comm_split_type(MPI_COMM_TYPE_Node,comm)<br>
&gt; &gt; create new group from all nodes not belonging to this group.<br>
&gt; &gt; This can even be more fine tuned if one wishes to create a group of &quot;master&quot; cores on each node.<br>
&gt;<br>
&gt; I will say that there was a lot of debate about this kind of functionality at the MPI Forum.  The problem is that although x86-based clusters are quite common these days, they are not the only kind of machines used for HPC out there, and the exact definitions of these kinds of concepts (hwthread, core, lXcache, socket, numa, ...etc.) can vary between architectures.<br>
&gt;<br>
&gt; Hence, the compromise was to just have MPI_COMM_TYPE_SHARED, where the resulting communicator contains processes that share a single memory space.<br>
&gt;<br>
&gt; That being said, since OMPI uses hwloc for all of its supported architectures, it might be worthwhile to have an OMPI extension for OMPI_COMM_TYPE_&lt;foo&gt; for the various different types.  One could/should only use these new constants if the OPEN_MPI macro is defined and is 1.<br>
&gt;<br>
&gt; And *that* being said, one of the goals of MPI is portability, so anyone using these constants would inherently non-portable.  :-)<br>
&gt;<br>
&gt; &gt; I have not been able to compile it due to my <a href="http://autogen.pl" target="_blank">autogen.pl</a> giving me some errors.<br>
&gt;<br>
&gt; What kind of errors?  (we might want to move this discussion to the devel list...)<br>
&gt;<br>
&gt; &gt;  However, I think it should compile just fine.<br>
&gt; &gt;<br>
&gt; &gt; Do you think it could be useful?<br>
&gt; &gt;<br>
&gt; &gt; If interested you can find my, single commit branch, at: <a href="https://github.com/zerothi/ompi" target="_blank">https://github.com/zerothi/ompi</a><br>
&gt;<br>
&gt; This looks interesting.<br>
&gt;<br>
&gt; Can you file a pull requests against the ompi master, and send something to the devel list about this functionality?<br>
&gt;<br>
&gt; I&#39;d still strongly suggest renaming these constants to the &quot;OMPI_&quot; to differentiate them from standard MPI constants / functionality.<br>
&gt;<br>
&gt; --<br>
&gt; Jeff Squyres<br>
&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/11/25878.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/11/25878.php</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Kind regards Nick<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div>&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/11/25879.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/11/25879.php</a><br>
<span class=""><br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</span>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/11/25880.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/11/25880.php</a><br>
</blockquote></div><br><br clear="all"><br>-- <br><div class="gmail_signature"><div dir="ltr"><div>Kind regards Nick</div></div></div>
</div>

