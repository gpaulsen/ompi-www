Hi.<div>Due that xgrid support is broken at the moment in 1.3, I am trying to install 1.2.9 in a xserve cluster.</div><div><br></div><div>I am using the gcc compilers downloaded from <a href="http://hpc.sourceforge.net/">http://hpc.sourceforge.net/</a>.</div>
<div><br></div><div>To be sure to not mixing compiler I am using the following configure</div><div><br></div><div>./configure --prefix=/opt/openmpi  CC=/usr/local/bin/gcc CXX=/usr/local/bin/g++   2&gt;&amp;1 |tee config.out</div>
<div><br></div><div>The</div><div>$ompi_info</div><div>result in</div><div><br></div><div><div>                Open MPI: 1.2.9</div><div>   Open MPI SVN revision: r20259</div><div>                Open RTE: 1.2.9</div><div>
   Open RTE SVN revision: r20259</div><div>                    OPAL: 1.2.9</div><div>       OPAL SVN revision: r20259</div><div>                  Prefix: /opt/openmpi</div><div> Configured architecture: i386-apple-darwin9.6.0</div>
<div>           Configured by: sofhtest</div><div>           Configured on: Tue Feb 24 18:24:59 CET 2009</div><div>          Configure host: nexus10.nlroc</div><div>                Built by: sofhtest</div><div>                Built on: Tue Feb 24 18:31:38 CET 2009</div>
<div>              Built host: nexus10.nlroc</div><div>              C bindings: yes</div><div>            C++ bindings: yes</div><div>      Fortran77 bindings: yes (single underscore)</div><div>      Fortran90 bindings: yes</div>
<div> Fortran90 bindings size: small</div><div>              C compiler: /usr/local/bin/gcc</div><div>     C compiler absolute: /usr/local/bin/gcc</div><div>            C++ compiler: /usr/local/bin/g++</div><div>   C++ compiler absolute: /usr/local/bin/g++</div>
<div>      Fortran77 compiler: gfortran</div><div>  Fortran77 compiler abs: /usr/local/bin/gfortran</div><div>      Fortran90 compiler: gfortran</div><div>  Fortran90 compiler abs: /usr/local/bin/gfortran</div><div>             C profiling: yes</div>
<div>           C++ profiling: yes</div><div>     Fortran77 profiling: yes</div><div>     Fortran90 profiling: yes</div><div>          C++ exceptions: no</div><div>          Thread support: posix (mpi: no, progress: no)</div>
<div>  Internal debug support: no</div><div>     MPI parameter check: runtime</div><div>Memory profiling support: no</div><div>Memory debugging support: no</div><div>         libltdl support: yes</div><div>   Heterogeneous support: yes</div>
<div> mpirun default --prefix: no</div><div>           MCA backtrace: execinfo (MCA v1.0, API v1.0, Component v1.2.9)</div><div>              MCA memory: darwin (MCA v1.0, API v1.0, Component v1.2.9)</div><div>           MCA maffinity: first_use (MCA v1.0, API v1.0, Component v1.2.9)</div>
<div>               MCA timer: darwin (MCA v1.0, API v1.0, Component v1.2.9)</div><div>         MCA installdirs: env (MCA v1.0, API v1.0, Component v1.2.9)</div><div>         MCA installdirs: config (MCA v1.0, API v1.0, Component v1.2.9)</div>
<div>           MCA allocator: basic (MCA v1.0, API v1.0, Component v1.0)</div><div>           MCA allocator: bucket (MCA v1.0, API v1.0, Component v1.0)</div><div>                MCA coll: basic (MCA v1.0, API v1.0, Component v1.2.9)</div>
<div>                MCA coll: self (MCA v1.0, API v1.0, Component v1.2.9)</div><div>                MCA coll: sm (MCA v1.0, API v1.0, Component v1.2.9)</div><div>                MCA coll: tuned (MCA v1.0, API v1.0, Component v1.2.9)</div>
<div>                  MCA io: romio (MCA v1.0, API v1.0, Component v1.2.9)</div><div>               MCA mpool: rdma (MCA v1.0, API v1.0, Component v1.2.9)</div><div>               MCA mpool: sm (MCA v1.0, API v1.0, Component v1.2.9)</div>
<div>                 MCA pml: cm (MCA v1.0, API v1.0, Component v1.2.9)</div><div>                 MCA pml: ob1 (MCA v1.0, API v1.0, Component v1.2.9)</div><div>                 MCA bml: r2 (MCA v1.0, API v1.0, Component v1.2.9)</div>
<div>              MCA rcache: vma (MCA v1.0, API v1.0, Component v1.2.9)</div><div>                 MCA btl: self (MCA v1.0, API v1.0.1, Component v1.2.9)</div><div>                 MCA btl: sm (MCA v1.0, API v1.0.1, Component v1.2.9)</div>
<div>                 MCA btl: tcp (MCA v1.0, API v1.0.1, Component v1.0)</div><div>                MCA topo: unity (MCA v1.0, API v1.0, Component v1.2.9)</div><div>                 MCA osc: pt2pt (MCA v1.0, API v1.0, Component v1.2.9)</div>
<div>              MCA errmgr: hnp (MCA v1.0, API v1.3, Component v1.2.9)</div><div>              MCA errmgr: orted (MCA v1.0, API v1.3, Component v1.2.9)</div><div>              MCA errmgr: proxy (MCA v1.0, API v1.3, Component v1.2.9)</div>
<div>                 MCA gpr: null (MCA v1.0, API v1.0, Component v1.2.9)</div><div>                 MCA gpr: proxy (MCA v1.0, API v1.0, Component v1.2.9)</div><div>                 MCA gpr: replica (MCA v1.0, API v1.0, Component v1.2.9)</div>
<div>                 MCA iof: proxy (MCA v1.0, API v1.0, Component v1.2.9)</div><div>                 MCA iof: svc (MCA v1.0, API v1.0, Component v1.2.9)</div><div>                  MCA ns: proxy (MCA v1.0, API v2.0, Component v1.2.9)</div>
<div>                  MCA ns: replica (MCA v1.0, API v2.0, Component v1.2.9)</div><div>                 MCA oob: tcp (MCA v1.0, API v1.0, Component v1.0)</div><div>                 MCA ras: dash_host (MCA v1.0, API v1.3, Component v1.2.9)</div>
<div>                 MCA ras: gridengine (MCA v1.0, API v1.3, Component v1.2.9)</div><div>                 MCA ras: localhost (MCA v1.0, API v1.3, Component v1.2.9)</div><div>                 MCA ras: xgrid (MCA v1.0, API v1.3, Component v1.2.9)</div>
<div>                 MCA rds: hostfile (MCA v1.0, API v1.3, Component v1.2.9)</div><div>                 MCA rds: proxy (MCA v1.0, API v1.3, Component v1.2.9)</div><div>                 MCA rds: resfile (MCA v1.0, API v1.3, Component v1.2.9)</div>
<div>               MCA rmaps: round_robin (MCA v1.0, API v1.3, Component v1.2.9)</div><div>                MCA rmgr: proxy (MCA v1.0, API v2.0, Component v1.2.9)</div><div>                MCA rmgr: urm (MCA v1.0, API v2.0, Component v1.2.9)</div>
<div>                 MCA rml: oob (MCA v1.0, API v1.0, Component v1.2.9)</div><div>                 MCA pls: gridengine (MCA v1.0, API v1.3, Component v1.2.9)</div><div>                 MCA pls: proxy (MCA v1.0, API v1.3, Component v1.2.9)</div>
<div>                 MCA pls: rsh (MCA v1.0, API v1.3, Component v1.2.9)</div><div>                 MCA pls: xgrid (MCA v1.0, API v1.3, Component v1.2.9)</div><div>                 MCA sds: env (MCA v1.0, API v1.0, Component v1.2.9)</div>
<div>                 MCA sds: pipe (MCA v1.0, API v1.0, Component v1.2.9)</div><div>                 MCA sds: seed (MCA v1.0, API v1.0, Component v1.2.9)</div><div>                 MCA sds: singleton (MCA v1.0, API v1.0, Component v1.2.9)</div>
<div><br></div><div>The program seems to run correctlly</div><div><br></div><div>but the mpirun finish with</div><div><br></div><div><div>2009-02-24 19:21:52.164 mpirun[22068:10b] *** Terminating app due to uncaught exception &#39;NSInvalidArgumentException&#39;, reason: &#39;*** -[NSKVONotifying_XGConnection&lt;0x216cf0&gt; finalize]: called when collecting not enabled&#39;</div>
<div>2009-02-24 19:21:52.165 mpirun[22068:10b] Stack: (</div><div>    2440954123,</div><div>    2435145275,</div><div>    2440982557,</div><div>    1687435,</div><div>    1679648,</div><div>    503955,</div><div>    379742,</div>
<div>    365359,</div><div>    9986</div><div>)</div><div>Trace/BPT trap</div><div><br></div><div><br></div><div>Any idea what I should look for.</div><div><br></div><div>NOTE:I am using xgrid with kerberos.</div><div><br>
</div><div>Yours</div><div>Ricardo</div><div><br></div><div>PD:I hope I make myself understandable , English is not my primary language </div></div></div>

