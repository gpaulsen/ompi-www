<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">Hi Gus,<div><br></div><div>Interestingly the results for the connectivity_c test... works fine with -np &lt;8. For -np &gt;8 it works some of the time, other times it HANGS. I have got to believe that this is a big clue!! Also, when it hangs, sometimes I get the message "mpirun was unable to cleanly terminate the daemons on the nodes shown below" Note that NO nodes are shown below. &nbsp; Once, I got -np 250 to pass the connectivity test, but I was not able to replicate this reliable, so I'm not sure if it was a fluke, or what. &nbsp;Here is a like to a screenshop of TOP when connectivity_c is hung with -np 14.. I see that 2 processes are only at 50% CPU usage.. Hmmmm &nbsp;</div><div><br></div><div><a href="http://picasaweb.google.com/lh/photo/87zVEucBNFaQ0TieNVZtdw?authkey=Gv1sRgCLKokNOVqo7BYw&amp;feat=directlink" target="_blank">http://picasaweb.google.com/<wbr>lh/photo/<wbr>87zVEucBNFaQ0TieNVZtdw?<wbr>authkey=Gv1sRgCLKokNOVqo7BYw&amp;<wbr>feat=directlink</a></div><div><br></div><div>The other tests, ring_c, hello_c, as well as the cxx versions of these guys with with all values of -np.</div><div><br></div><div>Using -mca mpi-paffinity_alone 1 I get the same behavior.&nbsp;</div><div><br></div><div>I agree that I am should worry about the mismatch between where the libraries are installed versus where I am telling my programs to look for them. Would this type of mismatch cause behavior like what I am seeing, i.e. working with &nbsp;a small number of processors, but failing with larger? &nbsp;It seems like a mismatch would have the same effect regardless of the number of processors used. Maybe I am mistaken. Anyway, to address this, which mpirun gives me /usr/local/bin/mpirun.. so to configure ./configure --with-mpi=/usr/local/bin/mpirun and to run /usr/local/bin/mpirun -np X ... &nbsp;This should&nbsp;</div><div><br></div><div>uname -a gives me: Linux macmanes 2.6.31-16-generic #52-Ubuntu SMP Thu Dec 3 22:07:16 UTC 2006 x86_64 GNU/Linux</div><div><br></div><div>Matt</div><div><br><div><div>On Dec 8, 2009, at 8:50 PM, Gus Correa wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div>Hi Matthew<br><br>Please see comments/answers inline below.<br><br>Matthew MacManes wrote:<br><blockquote type="cite">Hi Gus, Thanks for your ideas.. I have a few questions, and will try to answer yours in hopes of solving this!!<br></blockquote><br>A simple way to test OpenMPI on your system is to run the<br>test programs that come with the OpenMPI source code,<br>hello_c.c, connectivity_c.c, and ring_c.c:<br><a href="http://www.open-mpi.org/">http://www.open-mpi.org/</a><br><br>Get the tarball from the OpenMPI site, gzip and untar it,<br>and look for it in the "examples" directory.<br>Compile it with /your/path/to/openmpi/bin/mpicc hello_c.c<br>Run it with /your/path/to/openmpi/bin/mpiexec -np X a.out<br>using X = 2, 4, 8, 16, 32, 64, ...<br><br>This will tell if your OpenMPI is functional,<br>and if you can run on many Nehalem cores,<br>even with oversubscription perhaps.<br>It will also set the stage for further investigation of your<br>actual programs.<br><br><br><blockquote type="cite">Should I worry about setting things like --num-cores --bind-to-cores? &nbsp;This, I think, gets at your questions about processor affinity.. Am I right? I could not exactly figure out the -mca mpi-paffinity_alone stuff...<br></blockquote><br>I use the simple minded -mca mpi-paffinity_alone 1.<br>This is probably the easiest way to assign a process to a core.<br>There more complex &nbsp;ways in OpenMPI, but I haven't tried.<br>Indeed, -mca mpi-paffinity_alone 1 does improve performance of<br>our programs here.<br>There is a chance that without it the 16 virtual cores of<br>your Nehalem get confused with more than 3 processes<br>(you reported that -np &gt; 3 breaks).<br><br>Did you try adding just -mca mpi-paffinity_alone 1 &nbsp;to<br>your mpiexec command line?<br><br><br><blockquote type="cite">1. Additional load: nope. nothing else, most of the time not even firefox. <br></blockquote><br>Good.<br>Turn off firefox, etc, to make it even better.<br>Ideally, use runlevel 3, no X, like a computer cluster node,<br>but this may not be required.<br><br><blockquote type="cite">2. RAM: no problems apparent when monitoring through TOP. Interesting, I did wonder about oversubscription, so I tried the option --nooversubscription, but this gave me an error mssage.<br></blockquote><br>Oversubscription from your program would only happen if<br>you asked for more processes than available cores, i.e.,<br>-np &gt; 8 (or "virtual" cores, in case of Nehalem hyperthreading,<br>-np &gt; 16).<br>Since you have -np=4 there is no oversubscription,<br>unless you have other external load (e.g. Matlab, etc),<br>but you said you don't.<br><br>Yet another possibility would be if your program is threaded<br>(e.g. using OpenMP along with MPI), but considering what you<br>said about OpenMP I would guess the programs don't use it.<br>For instance, you launch the program with 4 MPI processes,<br>and each process decides to start, say, 8 OpenMP threads.<br>You end up with 32 threads and 8 (real) cores (or 16 hyperthreaded<br>ones on Nehalem).<br><br><br>What else does top say?<br>Any hog processes (memory- or CPU-wise)<br>besides your program processes?<br><br><blockquote type="cite">3. I have not tried other MPI flavors.. Ive been speaking to the authors of the programs, and they are both using openMPI. &nbsp;<br></blockquote><br>I was not trying to convince you to use another MPI.<br>I use MPICH2 also, but OpenMPI reigns here.<br>The idea or trying it with MPICH2 was just to check whether OpenMPI<br>is causing the problem, but I don't think it is.<br><br><blockquote type="cite">4. I don't think that this is a problem, as I'm specifying --with-mpi=/usr/bin/... &nbsp;when I compile the programs. Is there any other way to be sure that this is not a problem?<br></blockquote><br>Hmmm ....<br>I don't know about your Ubuntu (we have CentOS and Fedora on various<br>machines).<br>However, most Linux distributions come with their MPI flavors,<br>and so do compilers, etc.<br>Often times they install these goodies in unexpected places,<br>and this has caused a lot of frustration.<br>There are tons of postings on this list that eventually<br>boiled down to mismatched versions of MPI in unexpected places.<br><br><br>The easy way is to use full path names to compile and to run.<br>Something like this:<br>/my/openmpi/bin/mpicc on your program configuration script),<br><br>and something like this<br>/my/openmpi/bin/mpiexec -np &nbsp;... bla, bla ...<br>when you submit the job.<br><br>You can check your version with "which mpicc", "which mpiexec",<br>and (perhaps using full path names) with<br>"ompi_info", "mpicc --showme", "mpiexec --help".<br><br><br><blockquote type="cite">5. I had not been, and you could see some shuffling when monitoring the load on specific processors. I have tried to use --bind-to-cores to deal with this. I don't understand how to use the -mca options you asked about. 6. I am using Ubuntu 9.10. gcc 4.4.1 and g++ &nbsp;4.4.1<br></blockquote><br>I am afraid I won't be of help, because I don't have Nehalem.<br>However, I read about Nehalem requiring quite recent kernels<br>to get all of its features working right.<br><br>What is the output of "uname -a"?<br>This will tell the kernel version, etc.<br>Other list subscribers may give you a suggestion if you post the<br>information.<br><br><blockquote type="cite">MyBayes is a for bayesian phylogenetics: &nbsp;http://mrbayes.csit.fsu.edu/wiki/index.php/Main_Page ABySS: is a program for assembly of DNA sequence data: http://www.bcgsc.ca/platform/bioinfo/software/abyss<br></blockquote><br>Thanks for the links!<br>I had found the MrBayes link.<br>I eventually found what your ABySS was about, but no links.<br>Amazing that it is about DNA/gene sequencing.<br>Our abyss here is the deep ocean ... :)<br>Abysmal difference!<br><br><blockquote type="cite"><blockquote type="cite">Do the programs mix MPI (message passing) with OpenMP (threads)? <br></blockquote></blockquote><blockquote type="cite">Im honestly not sure what this means..<br></blockquote><br>Some programs mix the two.<br>OpenMP only works in a shared memory environment (e.g. a single<br>computer like yours), whereas MPI can use both shared memory<br>and work across a network (e.g. in a cluster).<br>There are other differences too.<br><br>Unlikely that you have this hybrid type of parallel program,<br>otherwise there would be some reference to OpenMP<br>on the very program configuration files, program documentation, etc.<br>Also, in general the configuration scripts of these hybrid<br>programs can turn on MPI only, or OpenMP only, or both,<br>depending on how you configure.<br><br>Even to compile with OpenMP you would need a proper compiler<br>flag, but that one might be hidden in a Makefile too, making<br>a bit hard to find. "grep -n mp Makefile" may give a clue.<br>Anything on the documentation that mentions threads or OpenMP?<br><br>FYI, here is OpenMP:<br>http://openmp.org/wp/<br><br><blockquote type="cite">Thanks for all your help!<br></blockquote>&gt; Matt<br><br>Well, so far it didn't really help. :(<br><br>But let's hope to find a clue,<br>maybe with a little help of<br>our list subscriber friends.<br><br>Gus Correa<br>---------------------------------------------------------------------<br>Gustavo Correa<br>Lamont-Doherty Earth Observatory - Columbia University<br>Palisades, NY, 10964-8000 - USA<br>---------------------------------------------------------------------<br><br><blockquote type="cite"><blockquote type="cite"> Hi Matthew<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">More guesses/questions than anything else:<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">1) Is there any additional load on this machine?<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">We had problems like that (on different machines) when<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">users start listening to streaming video, doing Matlab calculations,<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">etc, while the MPI programs are running.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">This tends to oversubscribe the cores, and may lead to crashes.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">2) RAM:<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Can you monitor the RAM usage through "top"?<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">(I presume you are on Linux.)<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">It may show unexpected memory leaks, if they exist.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">On "top", type "1" (one) see all cores, type "f" then "j"<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">to see the core number associated to each process.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">3) Do the programs work right with other MPI flavors (e.g. MPICH2)?<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">If not, then it is not OpenMPI's fault.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">4) Any possibility that the MPI versions/flavors of mpicc and<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">mpirun that you are using to compile and launch the program are not the<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">same?<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">5) Are you setting processor affinity on mpiexec?<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">mpiexec -mca mpi_paffinity_alone 1 -np ... bla, bla ...<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Context switching across the cores may also cause trouble, I suppose.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">6) Which Linux are you using (uname -a)?<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">On other mailing lists I read reports that only quite recent kernels<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">support all the Intel Nehalem processor features well.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">I don't have Nehalem, I can't help here,<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">but the information may be useful<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">for other list subscribers to help you.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">***<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">As for the programs, some programs require specific setup,<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">(and even specific compilation) when the number of MPI processes<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">vary.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">It may help if you tell us a link to the program sites.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Baysian statistics is not totally out of our business,<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">but phylogenetic genetic trees is not really my league,<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">hence forgive me any bad guesses, please,<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">but would it need specific compilation or a different<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">set of input parameters to run correctly on a different<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">number of processors?<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Do the programs mix MPI (message passing) with OpenMP (threads)?<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">I found this MrBayes, which seems to do the above:<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">http://mrbayes.csit.fsu.edu/<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">http://mrbayes.csit.fsu.edu/wiki/index.php/Main_Page<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">As for the ABySS, what is it, where can it be found?<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Doesn't look like a deep ocean circulation model, as the name suggest.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">My $0.02<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Gus Correa <br></blockquote></blockquote><blockquote type="cite">------------------------------------------------------------------------<br></blockquote><blockquote type="cite">_______________________________________________<br></blockquote><blockquote type="cite">users mailing list<br></blockquote><blockquote type="cite">users@open-mpi.org<br></blockquote><blockquote type="cite">http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></div></blockquote></div><br><div>
<span class="Apple-style-span" style="font-size: 13px; "><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><div><div>_________________________________<br>Matthew MacManes<br>PhD Candidate<br>University of California- Berkeley<br>Museum of Vertebrate Zoology<br>Phone: 510-495-5833<br>Lab Website:&nbsp;<a href="http://ib.berkeley.edu/labs/lacey">http://ib.berkeley.edu/labs/lacey</a><br>Personal Website: <a href="http://macmanes.com/">http://macmanes.com/</a></div><div><br></div></div><br></div></span><br class="Apple-interchange-newline"><br class="Apple-interchange-newline">
</div>
<br></div></body></html>
