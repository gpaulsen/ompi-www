<html><body>
<p>Jeff:<br>
<br>
I am running on a 2.66 GHz Nehalem node.  On this node, the turbo mode and hyperthreading are enabled.<br>
When I run LINPACK with Intel MPI, I get  82.68 GFlops without much trouble.<br>
<br>
When I ran with OpenMPI (I have  OpenMPI 1.2.8 but my colleague was using 1.3.2). I was using the same MKL libraries both with OpenMPI and<br>
Intel MPI. But with OpenMPI, the best I got so far is 80.22 GFlops and I could never achieve close to what I am getting with Intel MPI.<br>
Here are muy options with OpenMPI:<br>
<br>
mpirun -n 8 --machinefile hf --mca rmaps_rank_file_path rankfile --mca coll_sm_info_num_procs 8 --mca btl self,sm -mca mpi_leave_pinned 1 ./xhpl_ompi<br>
<br>
Here is my rankfile:<br>
<br>
at rankfile<br>
rank 0=i02n05 slot=0<br>
rank 1=i02n05 slot=1<br>
rank 2=i02n05 slot=2<br>
rank 3=i02n05 slot=3<br>
rank 4=i02n05 slot=4<br>
rank 5=i02n05 slot=5<br>
rank 6=i02n05 slot=6<br>
rank 7=i02n05 slot=7<br>
<br>
In this case the physical cores are 0-7 while the additional logical processors with hyperthreading are 8-15.<br>
With &quot;top&quot; command, I could see all the 8 tasks are running on 8 different physical cores. I did not see<br>
2 MPI tasks running on the same physical core. Also, the program is not paging as the problem size<br>
fits in the meory.<br>
<br>
Do you have any ideas how I can improve the performance so that it matches with Intel MPI performance?<br>
Any suggestions will be greatly appreciated.<br>
<br>
Thanks<br>
Swamy Kandadai<br>
<br>
<br>
Dr. Swamy N. Kandadai<br>
IBM Senior Certified Executive IT Specialist<br>
STG WW  Modular Systems Benchmark Center<br>
STG WW HPC and BI CoC Benchmark Center<br>
Phone:( 845) 433 -8429 (8-293) Fax:(845)432-9789<br>
swamy@us.ibm.com<br>
<a href="http://w3.ibm.com/sales/systems/benchmarks">http://w3.ibm.com/sales/systems/benchmarks</a><br>
<br>
<br>
<br>
<br>
</body></html>
