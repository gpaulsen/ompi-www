Dear Eugene<br><br><br>I am sorry that I may not explain the problem clearly last time. The problem is that I tested Ompi with PWscf program on one quadcore node. At the initial several hours, the program went on quite well. When the electronic scf is going to converge, the program started to hang. For example it hangs at the first scf iteration of  bfgs steps =23. I waited another 10 hours for the program to go on, but in vain  <br>
<br>The kernel is 2.6.29.4-167.fc11.i686.PAE<br>The following is the compiler I used to install Ompi. I configured Ompi with options of CC=gcc, FC=ifort.<br><br>******************************<div id=":2a" class="ii gt">********************************************************************************<br>
intel-icc101018-10.1.018-1.i386<br>libgcc-4.4.0-4.i586<br>gcc-4.4.0-4.i586<br>gcc-gfortran-4.4.0-4.i586<br>
gcc-c++-4.4.0-4.i586<br>intel-ifort101018-10.1.018-1.i386<br><br>and the architecture is:<br><br>processor    : 0<br>vendor_id    : GenuineIntel<br>cpu family    : 6<br>model        : 23<br>model name    : Intel(R) Core(TM)2 Quad CPU    Q9550  @ 2.83GHz<br>

stepping    : 10<br>cpu MHz        : 2825.937<br>cache size    : 6144 KB<br>physical id    : 0<br>siblings    : 4<br>core id        : 0<br>cpu cores    : 4<br>apicid        : 0<br>initial apicid    : 0<br>fdiv_bug    : no<br>

hlt_bug        : no<br>f00f_bug    : no<br>coma_bug    : no<br>fpu        : yes<br>fpu_exception    : yes<br>cpuid level    : 13<br>wp        : yes<br>flags   
    : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat
pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe nx lm
constant_tsc arch_perfmon pebs bts pni dtes64 monitor ds_cpl vmx smx
est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm tpr_shadow vnmi
flexpriority<br>
bogomips    : 5651.87<br>clflush size    : 64<br>power management:<br><br>
**************************************************************************************************************</div><br><br><div class="gmail_quote">On Tue, Nov 24, 2009 at 7:27 AM, Eugene Loh <span dir="ltr">&lt;<a href="mailto:Eugene.Loh@sun.com">Eugene.Loh@sun.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">I can&#39;t tell if these problems are related to trac ticket 2043 or not.<br>
<br>
Compiler:  In my experience, trac 2043 depends on GCC 4.4.x.  It isn&#39;t necessarily a GCC bug... perhaps it&#39;s just exposing an OMPI problem.  I&#39;m confused what compiler Jiaye is using, and Vasilis is apparently seeing a problem when using the PGI compiler.   But, maybe other compilers in addition to GCC 4.4.x are exposing the problem.<br>

<br>
Severity:  In my experience, trac 2043 shows up rather dramatically:  within dozens to hundreds of iterations of simple message patterns.  So, a problem that shows up only after hours of execution feels to me to be something different.  But maybe I misunderstand Jiaye&#39;s and Vasili&#39;s cases:  are the programs running well for several hours before the hang occurs?<br>

<br>
Shared memory:  Trac 2043 appears related to shared memory.  Jiaye seems to run on a single node.  Vasilis talks of running on a &quot;cluster&quot; -- so I don&#39;t know if that means over an interconnect or still using sm.<br>

<br>
Anyhow, it&#39;s hard to know which problems are the same or different when we don&#39;t yet really understand what&#39;s going on.<br>
<br>
vasilis gkanis wrote:<br>
<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div class="im">
I also experience a similar problem with the MUMPS solver, when I run it on a cluster. After several hours of running the code does not produce any results, although the command top shows that the program occupies 100% of the CPU.<br>

<br>
The difference here, however, is that the same program runs fine on my PC. The differences between my PC and the cluster are:<br>
1) 32bit vs 64-bit(cluster)<br>
2) intel compiler vs portland compiler(cluster)<br>
<br></div><div class="im">
On Friday 20 November 2009 03:50:17 am Jiaye Li wrote:<br>
 <br>
</div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div class="im">
I installed openmpi-1.3.3 on my single node(cpu) intel 64bit quad-core<br>
machine. The compiler info is:<br>
<br>
***************************************************************************<br>
*********************************** intel-icc101018-10.1.018-1.i386<br>
libgcc-4.4.0-4.i586<br>
gcc-4.4.0-4.i586<br>
gcc-gfortran-4.4.0-4.i586<br>
gcc-c++-4.4.0-4.i586<br>
intel-ifort101018-10.1.018-1.i386<br>
<br></div><div class="im">
***************************************************************************<br>
***********************************<br>
<br>
I compiled PWscf program with openmpi and tested the program. At the<br>
beginning, the execution of  PW went on well, but after about 10 h, when<br>
the program is going to finish. The program hang there, but the cpu time<br>
is still occupied. (100% taken up by the program). There seems to be<br>
something wrong, somewhere. Any ideas? Thank you in advance.<br>
   <br>
</div></blockquote></blockquote>
</blockquote></div><br><br clear="all"><br>-- <br>Sincerely yours<br><br>Jiaye Li<br><br>

