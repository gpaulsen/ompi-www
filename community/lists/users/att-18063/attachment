<html><body><div style="color:#000; background-color:#fff; font-family:times new roman, new york, times, serif;font-size:12pt"><div>I used&nbsp;
"--bind-to-socket --bysocket"&nbsp;&nbsp;all the time. It helps performance. I never oversubscribed node.</div><div>I have Intel westmere CPUs in each node, all cores will be used for application.</div><div><br></div><div>Open MPI version is 1.5.4.</div><div><br></div><div>The way i did to install openmpi:</div><div><span class="apple-style-span"><span style="font-size:
10.0pt;mso-bidi-font-size:8.0pt;font-family:&quot;Arial&quot;,&quot;sans-serif&quot;;mso-fareast-font-family:
宋体;mso-ansi-language:EN-US;mso-fareast-language:EN-US;mso-bidi-language:AR-SA">./autogen.sh</span></span><br></div><div><span class="apple-style-span"><span style="font-size:
10.0pt;mso-bidi-font-size:8.0pt;font-family:&quot;Arial&quot;,&quot;sans-serif&quot;;mso-fareast-font-family:
宋体;mso-ansi-language:EN-US;mso-fareast-language:EN-US;mso-bidi-language:AR-SA">./configure
--prefix=/mpi/openmpi-1.5.4 --with-openib CC=icc CXX=icpc F77=ifort FC=ifort --with-knem=/opt/knem</span></span><br></div><div><br><br></div><div><br></div>  <div style="font-size: 12pt; font-family: 'times new roman', 'new york', times, serif; "> <div style="font-size: 12pt; font-family: 'times new roman', 'new york', times, serif; "> <font size="2" face="Arial"> <hr size="1">  <b><span style="font-weight:bold;">From:</span></b> Eugene Loh &lt;eugene.loh@oracle.com&gt;<br> <b><span style="font-weight: bold;">To:</span></b> Open MPI Users &lt;users@open-mpi.org&gt; <br><b><span style="font-weight: bold;">Cc:</span></b> Eric Feng &lt;hpc_benchmark@yahoo.com&gt; <br> <b><span style="font-weight: bold;">Sent:</span></b> Wednesday, December 28, 2011 1:58 AM<br> <b><span style="font-weight: bold;">Subject:</span></b> Re: [OMPI users] Openmpi performance issue<br> </font> <br>
<meta http-equiv="x-dns-prefetch-control" content="off"><div id="yiv601291872">

  

    
  
  <div>
    If I remember correctly, both Intel MPI and MVAPICH2 bind processes
    by default.&nbsp; OMPI does not.&nbsp; There are many cases where the "bind by
    default" behavior gives better default performance.&nbsp; (There are also
    cases where it can give catastrophically worse performance.)&nbsp;
    Anyhow, it seems possible to me that this accounts for the
    difference you're seeing.<br>
    <br>
    To play with binding in OMPI, you can try adding "--bind-to-socket
    --bysocket" to your mpirun command line, though what to try can
    depend on what version of OMPI you're using as well as details of
    your processor (HyperThreads?), your application, etc.&nbsp; There's a
    FAQ entry at
    http://www.open-mpi.org/faq/?category=tuning#using-paffinity<br>
    <br>
    On 12/27/2011 6:45 AM, Ralph Castain wrote:
    <blockquote type="cite">It depends a lot on the application and how you ran
      it. Can you provide some info? For example, if you oversubscribed
      the node, then we dial down the performance to provide better cpu
      sharing. Another point: we don't bind processes by default while
      other MPIs do. Etc.
      <div><br>
      </div>
      <div>So more info (like the mpirun command line you used, which
        version you used, how OMPI was configured, etc.) would help.</div>
      <div><br>
      </div>
      <div><br>
        <div>
          <div>On Dec 27, 2011, at 6:35 AM, Eric Feng wrote:</div>
          <br class="yiv601291872Apple-interchange-newline">
          <blockquote type="cite">
            <div>
              <div style="color: rgb(0, 0, 0); background-color: rgb(255, 255, 255); font-size: 12pt; font-family: 'times new roman', 'new york', times, serif; ">
                <div><span>Can anyone help me?</span></div>
                <div><span>I got similar performance issue when
                    comparing to mvapich2 which is much faster in each
                    MPI function in real application but similar in IMB
                    benchmark.</span></div>
                <div><br>
                </div>
                <div style="font-size: 12pt; font-family: times, serif; ">
                  <div style="font-size: 12pt; font-family: times, serif; "> <font size="2" face="Arial">
                      <hr size="1"> <b><span style="font-weight:bold;">From:</span></b>
                      Eric Feng &lt;<a rel="nofollow" ymailto="mailto:hpc_benchmark@yahoo.com" target="_blank" href="mailto:hpc_benchmark@yahoo.com">hpc_benchmark@yahoo.com</a>&gt;<br>
                      <b><span style="font-weight:bold;">To:</span></b>
                      "<a rel="nofollow" ymailto="mailto:users@open-mpi.org" target="_blank" href="mailto:users@open-mpi.org">users@open-mpi.org</a>"
                      &lt;<a rel="nofollow" ymailto="mailto:users@open-mpi.org" target="_blank" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;
                      <br>
                      <b><span style="font-weight:bold;">Sent:</span></b>
                      Friday, December 23, 2011 9:12 PM<br>
                      <b><span style="font-weight:bold;">Subject:</span></b>
                      [OMPI users] Openmpi performance issue<br>
                    </font> <br>

                    
                    <div id="yiv601291872">
                      <div>
                        <div style="color: rgb(0, 0, 0); background-color: rgb(255, 255, 255); font-size: 12pt; font-family: times, serif; ">
                          <div>Hello,&nbsp;</div>
                          <div><br>
                          </div>
                          <div>I am running into performance issue with
                            Open MPI, I wish experts here can provide me
                            some help,</div>
                          <div><br>
                          </div>
                          <div>I have one application calls a lot of
                            sendrecv, and isend/irecv, so waitall. When
                            I run Intel MPI, it is around 30% faster
                            than OpenMPI.</div>
                          <div>However if i test sendrecv using IMB,
                            OpenMPI is even faster than Intel MPI, but
                            when run with real application, Open MPI is
                            much slower than Intel MPI in all MPI
                            functions by looking at profiling results.
                            So this is not some function issue, it has a
                            overall drawback somewhere. Can anyone give
                            me some suggestions of where to tune to make
                            it run faster with real application?</div>
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </blockquote>
        </div>
      </div>
    </blockquote>
  </div>

</div><meta http-equiv="x-dns-prefetch-control" content="on"><br><br> </div> </div>  </div></body></html>
