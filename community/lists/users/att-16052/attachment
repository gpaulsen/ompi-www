As one of the error message suggests, you need to add the openmpi library to your LD_LIBRARY_PATH to all your nodes.<br><br><div class="gmail_quote">On Wed, Mar 30, 2011 at 1:24 PM, Nehemiah Dacres <span dir="ltr">&lt;<a href="mailto:dacresni@slu.edu">dacresni@slu.edu</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">I am trying to figure out why my jobs aren&#39;t getting distributed and need some help. I have an install of sun cluster tools on Rockscluster 5.2 (essentially centos4u2). this user&#39;s account has its home dir shared via nfs. I am getting some strange errors. here&#39;s an example run <br>


<br><br>[jian@therock ~]$ /opt/SUNWhpc/HPC8.2.1c/sun/bin/mpirun -np 3 -hostfile list ./job2.sh <br>bash: /opt/SUNWhpc/HPC8.2.1c/sun/bin/orted: No such file or directory<br>--------------------------------------------------------------------------<br>


A daemon (pid 20362) died unexpectedly with status 127 while attempting<br>to launch so we are aborting.<br><br>There may be more information reported by the environment (see above).<br><br>This may be because the daemon was unable to find all the needed shared<br>


libraries on the remote node. You may set your LD_LIBRARY_PATH to have the<br>location of the shared libraries on the remote nodes and this will<br>automatically be forwarded to the remote nodes.<br>--------------------------------------------------------------------------<br>


--------------------------------------------------------------------------<br>mpirun noticed that the job aborted, but has no info as to the process<br>that caused that situation.<br>--------------------------------------------------------------------------<br>


mpirun: clean termination accomplished<br><br>[jian@therock ~]$ /opt/SUNWhpc/HPC8.2.1c/sun/<br>bin/        examples/   instrument/ man/        <br>etc/        include/    lib/        share/      <br>[jian@therock ~]$ /opt/SUNWhpc/HPC8.2.1c/sun/bin/orte<br>


orte-clean  orted       orte-iof    orte-ps     orterun     <br>[jian@therock ~]$ /opt/SUNWhpc/HPC8.2.1c/sun/bin/orted <br>[therock.slu.loc:20365] [[INVALID],INVALID] ORTE_ERROR_LOG: Not found in file runtime/orte_init.c at line 125<br>


--------------------------------------------------------------------------<br>It looks like orte_init failed for some reason; your parallel process is<br>likely to abort.  There are many reasons that a parallel process can<br>


fail during orte_init; some of which are due to configuration or<br>environment problems.  This failure appears to be an internal failure;<br>here&#39;s some additional information (which may only be relevant to an<br>Open MPI developer):<br>


<br>  orte_ess_base_select failed<br>  --&gt; Returned value Not found (-13) instead of ORTE_SUCCESS<br>--------------------------------------------------------------------------<br>[therock.slu.loc:20365] [[INVALID],INVALID] ORTE_ERROR_LOG: Not found in file orted/orted_main.c at line 325<br>


[jian@therock ~]$ <br><font color="#888888"><br clear="all"><br>-- <br>Nehemiah I. Dacres<br>System Administrator <div>Advanced Technology Group Saint Louis University</div><br>
</font><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br><br clear="all"><br>-- <br>David Zhang<br>University of California, San Diego<br>



