<html><head><style type="text/css"><!-- DIV {margin:0px;} --></style></head><body><div style="font-family:verdana,helvetica,sans-serif;font-size:12pt"><div>Joe,<br><br>I don't know why I didn't think about it. It works with the whole path.<br><br>I put the intel env script in user .bash_login file. <br>Do you think I should put the intel env script in the global shell config file like /etc/profile in order for libtool to see icc?<br><br>Thanks for help,<br><br>mike<br></div><div style="font-family: verdana,helvetica,sans-serif; font-size: 12pt;"><br><div style="font-family: arial,helvetica,sans-serif; font-size: 13px;"><font face="Tahoma" size="2"><hr size="1"><b><span style="font-weight: bold;">From:</span></b> "users-request@open-mpi.org" &lt;users-request@open-mpi.org&gt;<br><b><span style="font-weight: bold;">To:</span></b> users@open-mpi.org<br><b><span style="font-weight: bold;">Sent:</span></b> Wednesday, May 27, 2009 9:24:00 PM<br><b><span
 style="font-weight: bold;">Subject:</span></b> users Digest, Vol 1242, Issue 5<br></font><br>Send users mailing list submissions to<br>&nbsp;&nbsp;&nbsp; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><br>To subscribe or unsubscribe via the World Wide Web, visit<br>&nbsp;&nbsp;&nbsp; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>or, via email, send a message with subject or body 'help' to<br>&nbsp;&nbsp;&nbsp; <a ymailto="mailto:users-request@open-mpi.org" href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a><br><br>You can reach the person managing the list at<br>&nbsp;&nbsp;&nbsp; <a ymailto="mailto:users-owner@open-mpi.org" href="mailto:users-owner@open-mpi.org">users-owner@open-mpi.org</a><br><br>When replying, please edit your Subject line so it is more specific<br>than "Re: Contents of users
 digest..."<br><br><br>Today's Topics:<br><br>&nbsp;  1. Re: problem with installing openmpi with intelcompiler<br>&nbsp; &nbsp; &nbsp; onubuntu (Joe Griffin)<br><br><br>----------------------------------------------------------------------<br><br>Message: 1<br>Date: Wed, 27 May 2009 18:23:10 -0700<br>From: "Joe Griffin" &lt;<a ymailto="mailto:joe.griffin@mscsoftware.com" href="mailto:joe.griffin@mscsoftware.com">joe.griffin@mscsoftware.com</a>&gt;<br>Subject: Re: [OMPI users] problem with installing openmpi with<br>&nbsp;&nbsp;&nbsp; intelcompiler&nbsp;&nbsp;&nbsp; onubuntu<br>To: "Open MPI Users" &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;, &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID:<br>&nbsp;&nbsp;&nbsp; &lt;<a ymailto="mailto:1D367926756E9848BABD800E249AA5E04BFF88@NASCMEX01.na.mscsoftware.com"
 href="mailto:1D367926756E9848BABD800E249AA5E04BFF88@NASCMEX01.na.mscsoftware.com">1D367926756E9848BABD800E249AA5E04BFF88@NASCMEX01.na.mscsoftware.com</a>&gt;<br>Content-Type: text/plain; charset="iso-8859-1"<br><br>MK,<br> <br>Hmm.. What if you put CC=/usr/local/intel/Compiler/11.0/083/bin/intel64/icc<br>on the build line.<br> <br>Joe<br> <br><br>________________________________<br><br>From: <a ymailto="mailto:users-bounces@open-mpi.org" href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a> on behalf of Michael Kuklik<br>Sent: Wed 5/27/2009 5:05 PM<br>To: <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subject: Re: [OMPI users] problem with installing openmpi with intelcompiler onubuntu<br><br><br>Joe<br><br>'which icc' returns the path to icc<br>/usr/local/intel/Compiler/11.0/083/bin/intel64/icc<br><br>and I used the env variable script provided by intel.<br>so my shell env is ok and I
 think libtool should inherit my shell environment<br><br>just in case I send you the env printout<br><br>MKLROOT=/usr/local/intel/Compiler/11.0/083/mkl<br>MANPATH=/usr/local/intel/Compiler/11.0/083/man:/usr/local/intel/Compiler/11.0/083/mkl/man/en_US:/usr/local/intel/Compiler/11.0/083/man:/usr/local/intel/Compiler/11.0/083/mkl/man/en_US:/usr/local/man:/usr/local/share/man:/usr/share/man<br>INTEL_LICENSE_FILE=/usr/local/intel/Compiler/11.0/083/licenses:/opt/intel/licenses:/home/mkuklik/intel/licenses:/usr/local/intel/Compiler/11.0/083/licenses:/opt/intel/licenses:/home/mkuklik/intel/licenses<br>IPPROOT=/usr/local/intel/Compiler/11.0/083/ipp/em64t<br>TERM=xterm-color<br>SHELL=/bin/bash<br>XDG_SESSION_COOKIE=d03e782e0b3c90f7ce8380174a15d9d2-1243468120.315267-1057427925<br>SSH_CLIENT=128.151.210.198 54616
 22<br>LIBRARY_PATH=/usr/local/intel/Compiler/11.0/083/ipp/em64t/lib:/usr/local/intel/Compiler/11.0/083/mkl/lib/em64t:/usr/local/intel/Compiler/11.0/083/tbb/em64t/cc4.1.0_libc2.4_kernel2.6.16.21/lib:/usr/local/intel/Compiler/11.0/083/ipp/em64t/lib:/usr/local/intel/Compiler/11.0/083/mkl/lib/em64t:/usr/local/intel/Compiler/11.0/083/tbb/em64t/cc4.1.0_libc2.4_kernel2.6.16.21/lib<br>FPATH=/usr/local/intel/Compiler/11.0/083/mkl/include:/usr/local/intel/Compiler/11.0/083/mkl/include<br>SSH_TTY=/dev/pts/4<br>LC_ALL=C<br>USER=mkuklik<br>LD_LIBRARY_PATH=/usr/local/intel/Compiler/11.0/083/lib/intel64:/usr/local/intel/Compiler/11.0/083/ipp/em64t/sharedlib:/usr/local/intel/Compiler/11.0/083/mkl/lib/em64t:/usr/local/intel/Compiler/11.0/083/tbb/em64t/cc4.1.0_libc2.4_kernel2.6.16.21/lib:/usr/local/intel/Compiler/11.0/083/lib/intel64:/usr/local/intel/Compiler/11.0/083/ipp/em64t/sharedlib:/usr/local/intel/Compiler/11.0/083/mkl/lib/em64t:/usr/local/intel/Compiler/11.0/083
 /tbb/em64t/cc4.1.0_libc2.4_kernel2.6.16.21/lib<br>LIB=/usr/local/intel/Compiler/11.0/083/ipp/em64t/lib:/usr/local/intel/Compiler/11.0/083/ipp/em64t/lib:<br>CPATH=/usr/local/intel/Compiler/11.0/083/ipp/em64t/include:/usr/local/intel/Compiler/11.0/083/mkl/include:/usr/local/intel/Compiler/11.0/083/tbb/include:/usr/local/intel/Compiler/11.0/083/ipp/em64t/include:/usr/local/intel/Compiler/11.0/083/mkl/include:/usr/local/intel/Compiler/11.0/083/tbb/include<br>NLSPATH=/usr/local/intel/Compiler/11.0/083/lib/intel64/locale/%l_%t/%N:/usr/local/intel/Compiler/11.0/083/ipp/em64t/lib/locale/%l_%t/%N:/usr/local/intel/Compiler/11.0/083/mkl/lib/em64t/locale/%l_%t/%N:/usr/local/intel/Compiler/11.0/083/idb/intel64/locale/%l_%t/%N:/usr/local/intel/Compiler/11.0/083/lib/intel64/locale/%l_%t/%N:/usr/local/intel/Compiler/11.0/083/ipp/em64t/lib/locale/%l_%t/%N:/usr/local/intel/Compiler/11.0/083/mkl/lib/em64t/locale/%l_%t/%N:/usr/local/intel/Compiler/11.0/083/idb/intel64/loca
 le/%l_%t/%N<br>MAIL=/var/mail/mkuklik<br>PATH=/usr/local/intel/Compiler/11.0/083/bin/intel64:/usr/local/intel/Compiler/11.0/083/bin/intel64:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games<br>PWD=/home/mkuklik<br>LANG=en_US<br>SHLVL=1<br>HOME=/home/mkuklik<br>DYLD_LIBRARY_PATH=/usr/local/intel/Compiler/11.0/083/tbb/em64t/cc4.1.0_libc2.4_kernel2.6.16.21/lib:/usr/local/intel/Compiler/11.0/083/tbb/em64t/cc4.1.0_libc2.4_kernel2.6.16.21/lib<br>LOGNAME=mkuklik<br>SSH_CONNECTION=128.151.210.198 54616 128.151.210.190 22<br>INCLUDE=/usr/local/intel/Compiler/11.0/083/ipp/em64t/include:/usr/local/intel/Compiler/11.0/083/mkl/include:/usr/local/intel/Compiler/11.0/083/ipp/em64t/include:/usr/local/intel/Compiler/11.0/083/mkl/include<br>_=/usr/bin/env<br><br>Thanks,<br><br>mk<br><br><br>________________________________<br><br><br>----------------------------------------------------------------------<br><br>Message: 1<br>Date: Tue, 26 May 2009
 19:51:48 -0700<br>From: "Joe Griffin" &lt;<a ymailto="mailto:joe.griffin@mscsoftware.com" href="mailto:joe.griffin@mscsoftware.com">joe.griffin@mscsoftware.com</a>&gt;<br>Subject: Re: [OMPI users] problem with installing openmpi with intel<br>&nbsp; &nbsp; compiler onubuntu<br>To: "Open MPI Users" &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID:<br>&nbsp; &nbsp; &lt;<a ymailto="mailto:1D367926756E9848BABD800E249AA5E04BFF84@NASCMEX01.na.mscsoftware.com" href="mailto:1D367926756E9848BABD800E249AA5E04BFF84@NASCMEX01.na.mscsoftware.com">1D367926756E9848BABD800E249AA5E04BFF84@NASCMEX01.na.mscsoftware.com</a>&gt;<br>Content-Type: text/plain; charset="iso-8859-1"<br><br>MK,<br><br>Is "icc" in your path?<br><br>What if you type "which icc"?<br><br>Joe<br><br><br>________________________________<br><br>From: <a ymailto="mailto:users-bounces@open-mpi.org"
 href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a> on behalf of Michael Kuklik<br>Sent: Tue 5/26/2009 7:05 PM<br>To: <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subject: [OMPI users] problem with installing openmpi with intel compiler onubuntu<br><br><br>Hi everybody,<br><br>I try to compile openmpi with intel compiler on ubuntu 9.04.<br>I compiled openmpi on Redhat and os x many times and I could always find a problem. But the error that I'm getting now, gives me no clues where to even search for the problem.<br><br>my config line is a follows:<br>./configure CC=icc CXX=icpc --prefix=/usr/local/intel/openmpi<br><br>Everything configures and compiles OK. But then when I try to install I get this error<br><br>Making install in etc<br>make[2]: Entering directory `/tmp/openmpi-1.3.2/orte/etc'<br>make[3]: Entering directory `/tmp/openmpi-1.3.2/orte/etc'<br>make[3]: Nothing to be done
 for `install-exec-am'.<br>/bin/mkdir -p /usr/local/intel/openmpi/etc<br>******************************* WARNING ************************************<br>*** Not installing new openmpi-default-hostfile over existing file in:<br>***&nbsp; /usr/local/intel/openmpi/etc/openmpi-default-hostfile<br>******************************* WARNING ************************************<br>make[3]: Leaving directory `/tmp/openmpi-1.3.2/orte/etc'<br>make[2]: Leaving directory `/tmp/openmpi-1.3.2/orte/etc'<br>Making install in .<br>make[2]: Entering directory `/tmp/openmpi-1.3.2/orte'<br>make[3]: Entering directory `/tmp/openmpi-1.3.2/orte'<br>test -z "/usr/local/intel/openmpi/lib" || /bin/mkdir -p "/usr/local/intel/openmpi/lib"<br>/bin/bash ../libtool&nbsp; --mode=install /usr/bin/install -c&nbsp; 'libopen-rte.la &lt;<a href="http://libopen-rte.la/" target="_blank">http://libopen-rte.la/</a>&gt; ' '/usr/local/intel/openmpi/lib/libopen-rte.la'<br>libtool: install: warning:
 relinking `libopen-rte.la'<br>libtool: install: (cd /tmp/openmpi-1.3.2/orte; /bin/bash /tmp/openmpi-1.3.2/libtool&nbsp; --tag CC --mode=relink icc -O3 -DNDEBUG -finline-functions -fno-strict-aliasing ................ )<br>libtool: relink: icc -shared&nbsp; runtime/.libs/orte_finalize.o runtime/.libs/orte_init.o runtime/.libs/orte_locks.o runtime/.libs/orte_globals.o runtime/data_type_support/.libs/orte_dt_compare_fns.o runtime/data_type_support/.libs/orte_dt_copy_fns.o runtime/data_type_support/.libs/orte_dt_print_fns.o runtime/data_type_support/.libs/orte_dt_release_fns.o runtime/data_type_support/.libs/orte_dt_size_fns.o runtime/data_type_support/.libs/orte_dt_packing_fns.o runtime/data_type_support/.libs/orte_dt_unpacking_fns.o runtime/.libs/orte_mca_params.o runtime/.libs/orte_wait.o runtime/.libs/orte_cr.o runtime/.libs/..................................... -Wl,libopen-rte.so &lt;<a href="http://libopen-rte.so/"
 target="_blank">http://libopen-rte.so/</a>&gt; .0 -o .libs/libopen-rte.so.0.0.0<br>/tmp/openmpi-1.3.2/libtool: line 7847: icc: command not found<br>libtool: install: error: relink `libopen-rte.la' with the above command before installing it<br>make[3]: *** [install-libLTLIBRARIES] Error 1<br>make[3]: Leaving directory `/tmp/openmpi-1.3.2/orte'<br>make[2]: *** [install-am] Error 2<br>make[2]: Leaving directory `/tmp/openmpi-1.3.2/orte'<br>make[1]: *** [install-recursive] Error 1<br>make[1]: Leaving directory `/tmp/openmpi-1.3.2/orte'<br>make: *** [install-recursive] Error 1<br><br>libtool is the one from ubuntu repository i.e. 2.2.6a-1<br>icc and icpc are the newest ones i.e. 11.083 <br><br>Ouputs of configure make and install are attached.<br><br>Any clues what's wrong?<br><br>Thanks for help<br><br>mk<br><br><br>-------------- next part --------------<br>A non-text attachment was scrubbed...<br>Name: not available<br>Type: application/ms-tnef<br>Size:
 5414 bytes<br>Desc: not available<br>URL: &lt;<a href="http://www.open-mpi.org/MailArchives/users/attachments/20090526/9737163d/attachment.bin" target="_blank">http://www.open-mpi.org/MailArchives/users/attachments/20090526/9737163d/attachment.bin</a>&gt;<br><br>------------------------------<br><br>Message: 2<br>Date: Wed, 27 May 2009 13:09:27 +0300<br>From: vasilis &lt;<a ymailto="mailto:gkanis@ipta.demokritos.gr" href="mailto:gkanis@ipta.demokritos.gr">gkanis@ipta.demokritos.gr</a>&gt;<br>Subject: Re: [OMPI users] "An error occurred in MPI_Recv" with more<br>&nbsp; &nbsp; than 2&nbsp; &nbsp; CPU<br>To: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID: &lt;<a ymailto="mailto:200905271309.27914.gkanis@ipta.demokritos.gr" href="mailto:200905271309.27914.gkanis@ipta.demokritos.gr">200905271309.27914.gkanis@ipta.demokritos.gr</a>&gt;<br>Content-Type: Text/Plain;&nbsp;
 charset="iso-8859-1"<br><br>Thank you Eugene for your suggestion. I used different tags for each variable, <br>and now I do not get this error. <br>The problem now is that I am getting a different solution when I use more than <br>2 CPUs. I checked the matrices and I found that they differ by a very small <br>amount&nbsp; of the order 10^(-10). Actually, I am getting a different solution if I <br>use 4CPUs or 16CPUs!!!<br>Do you have any idea what could cause this behavior?<br><br>Thank you,<br>Vasilis<br><br>On Tuesday 26 of May 2009 7:21:32 pm you wrote:<br>&gt; vasilis wrote:<br>&gt; &gt;Dear openMpi users,<br>&gt; &gt;<br>&gt; &gt;I am trying to develop a code that runs in parallel mode with openMPI<br>&gt; &gt; (1.3.2 version). The code is written in Fortran 90, and I am running on <br>&gt; &gt; a cluster<br>&gt; &gt;<br>&gt; &gt;If I use 2 CPU the program runs fine, but for a larger number of CPUs I<br>&gt; &gt; get the following error:<br>&gt;
 &gt;<br>&gt; &gt;[compute-2-6.local:18491] *** An error occurred in MPI_Recv<br>&gt; &gt;[compute-2-6.local:18491] *** on communicator MPI_COMM_WORLD<br>&gt; &gt;[compute-2-6.local:18491] *** MPI_ERR_TRUNCATE: message truncated<br>&gt; &gt;[compute-2-6.local:18491] *** MPI_ERRORS_ARE_FATAL (your MPI job will now<br>&gt; &gt;abort)<br>&gt; &gt;<br>&gt; &gt;Here is the part of the code that this error refers to:<br>&gt; &gt;if( mumps_par%MYID .eq. 0 ) THEN<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; res=res+res_cpu<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; do iw=1,total_elem_cpu*unique<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; jacob(iw)=jacob(iw)+jacob_cpu(iw)<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; position_col(iw)=position_col(iw)+col_cpu(iw)<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; position_row(iw)=position_row(iw)+row_cpu(iw)<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; end do<br>&gt; &gt;<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; do jw=1,nsize-1<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; call<br>&gt; &gt;MPI_recv(jacob_cpu,total_elem_cpu*unique,MPI_DOUBLE_PRECISION,MPI_ANY_SOUR<br>&gt; &gt;CE,MPI_ANY_TAG,MPI_COMM_WORLD,status1,ierr) call<br>&gt; &gt;MPI_recv(res_cpu,total_unknowns,MPI_DOUBLE_PRECISION,MPI_ANY_SOURCE,MPI_AN<br>&gt; &gt;Y_TAG,MPI_COMM_WORLD,status2,ierr) call<br>&gt; &gt;MPI_recv(row_cpu,total_elem_cpu*unique,MPI_INTEGER,MPI_ANY_SOURCE,MPI_ANY_<br>&gt; &gt;TAG,MPI_COMM_WORLD,status3,ierr) call<br>&gt; &gt;MPI_recv(col_cpu,total_elem_cpu*unique,MPI_INTEGER,MPI_ANY_SOURCE,MPI_ANY_<br>&gt; &gt;TAG,MPI_COMM_WORLD,status4,ierr)<br>&gt; &gt;<br>&gt; &gt;&nbsp;
 res=res+res_cpu<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; do iw=1,total_elem_cpu*unique<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br>&gt; &gt; jacob(status1(MPI_SOURCE)*total_elem_cpu*unique+iw)=&amp;<br>&gt; &gt; jacob(status1(MPI_SOURCE)*total_elem_cpu*unique+iw)+jacob_cpu(iw)<br>&gt; &gt; position_col(status4(MPI_SOURCE)*total_elem_cpu*unique+iw)=&amp;<br>&gt; &gt; position_col(status4(MPI_SOURCE)*total_elem_cpu*unique+iw)+col_cpu(iw)<br>&gt; &gt; position_row(status3(MPI_SOURCE)*total_elem_cpu*unique+iw)=&amp;<br>&gt; &gt; position_row(status3(MPI_SOURCE)*total_elem_cpu*unique+iw)+row_cpu(iw)<br>&gt; &gt; end do<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; end do<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; else<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; call<br>&gt;
 &gt;MPI_Isend(jacob_cpu,total_elem_cpu*unique,MPI_DOUBLE_PRECISION,0,mumps_par<br>&gt; &gt;%MYID,MPI_COMM_WORLD,request1,ierr) call<br>&gt; &gt;MPI_Isend(res_cpu,total_unknowns,MPI_DOUBLE_PRECISION,0,mumps_par%MYID,MPI<br>&gt; &gt;_COMM_WORLD,request2,ierr) call<br>&gt; &gt;MPI_Isend(row_cpu,total_elem_cpu*unique,MPI_INTEGER,0,mumps_par%MYID,MPI_C<br>&gt; &gt;OMM_WORLD,request3,ierr) call<br>&gt; &gt;MPI_Isend(col_cpu,total_elem_cpu*unique,MPI_INTEGER,0,mumps_par%MYID,MPI_C<br>&gt; &gt;OMM_WORLD,request4,ierr) call MPI_Wait(request1, status1, ierr)<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; call MPI_Wait(request2, status2, ierr)<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; call MPI_Wait(request3, status3, ierr)<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; call MPI_Wait(request4, status4, ierr)<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; end if<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt;I am
 also using the MUMPS library<br>&gt; &gt;<br>&gt; &gt;Could someone help to track this error down. Is really annoying to use<br>&gt; &gt; only two processors.<br>&gt; &gt;The cluster has about 8 nodes and each has 4 dual core CPU. I tried to run<br>&gt; &gt; the code on a single node with more than 2 CPU but I got the same error!!<br>&gt;<br>&gt; I think the error message means that the received message was longer<br>&gt; than the receive buffer that was specified.&nbsp; If I look at your code and<br>&gt; try to reason about its correctness, I think of the message-passing<br>&gt; portion as looking like this:<br>&gt;<br>&gt; if( mumps_par%MYID .eq. 0 ) THEN<br>&gt;&nbsp; &nbsp; do jw=1,nsize-1<br>&gt;&nbsp; &nbsp; &nbsp; &nbsp; call<br>&gt; MPI_recv(jacob_cpu,total_elem_cpu*unique,MPI_DOUBLE_PRECISION,MPI_ANY_SOURC<br>&gt;E,MPI_ANY_TAG,MPI_COMM_WORLD,status1,ierr) call MPI_recv( <br>&gt; res_cpu,total_unknowns<br>&gt;
 ,MPI_DOUBLE_PRECISION,MPI_ANY_SOURCE,MPI_ANY_TAG,MPI_COMM_WORLD,status2,ier<br>&gt;r) call MPI_recv(<br>&gt; row_cpu,total_elem_cpu*unique,MPI_INTEGER<br>&gt; ,MPI_ANY_SOURCE,MPI_ANY_TAG,MPI_COMM_WORLD,status3,ierr)<br>&gt;&nbsp; &nbsp; &nbsp; &nbsp; call MPI_recv(<br>&gt; col_cpu,total_elem_cpu*unique,MPI_INTEGER<br>&gt; ,MPI_ANY_SOURCE,MPI_ANY_TAG,MPI_COMM_WORLD,status4,ierr)<br>&gt;&nbsp; &nbsp; end do<br>&gt; else<br>&gt;&nbsp; &nbsp; call<br>&gt; MPI_Send(jacob_cpu,total_elem_cpu*unique,MPI_DOUBLE_PRECISION,0,mumps_par%M<br>&gt;YID,MPI_COMM_WORLD,ierr) call MPI_Send(&nbsp; res_cpu,total_unknowns<br>&gt; ,MPI_DOUBLE_PRECISION,0,mumps_par%MYID,MPI_COMM_WORLD,ierr)<br>&gt;&nbsp; &nbsp; call MPI_Send(&nbsp; row_cpu,total_elem_cpu*unique,MPI_INTEGER<br>&gt; ,0,mumps_par%MYID,MPI_COMM_WORLD,ierr)<br>&gt;&nbsp; &nbsp; call MPI_Send(&nbsp; col_cpu,total_elem_cpu*unique,MPI_INTEGER<br>&gt; ,0,mumps_par%MYID,MPI_COMM_WORLD,ierr)<br>&gt; end
 if<br>&gt;<br>&gt; If you're running on two processes, then the messages you receive are in<br>&gt; the order you expect.&nbsp; If there are more than two processes, however,<br>&gt; certainly messages will start appearing "out of order" and your<br>&gt; indiscriminate use of MPI_ANY_SOURCE and MPI_ANY_TAG will start getting<br>&gt; them mixed up.&nbsp; You won't just get all messages from one rank and then<br>&gt; all from another and then all from another.&nbsp; Rather, the messages from<br>&gt; all these other processes will come interwoven, but you interpret them<br>&gt; in a fixed order.<br>&gt;<br>&gt; Here is what I mean.&nbsp; Let's say you have 3 processes.&nbsp; So, rank 0 will<br>&gt; receive 8 messages:&nbsp; 4 from rank 1and 4 from rank 2.&nbsp; Correspondingly,<br>&gt; rank 1 and rank 2 will each send 4 messages to rank 0.&nbsp; Here is a<br>&gt; possibility for the order in which messages are received:<br>&gt;<br>&gt; jacob_cpu from rank
 1<br>&gt; jacob_cpu from rank 2<br>&gt; res_cpu from rank 1<br>&gt; row_cpu from rank 1<br>&gt; res_cpu from rank 2<br>&gt; row_cpu from rank 2<br>&gt; col_cpu from rank 2<br>&gt; col_cpu from rank 1<br>&gt;<br>&gt; Rank 0, however, is trying to unpack these in the order you prescribed<br>&gt; in your code.&nbsp; Data will get misinterpreted.&nbsp; More to the point here,<br>&gt; you will be trying to receive data into buffers of the wrong size (some<br>&gt; of the time).<br>&gt;<br>&gt; Maybe you should use tags to distinguish between the different types of<br>&gt; messages you're trying to send.<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
 target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><br><br>------------------------------<br><br>Message: 3<br>Date: Wed, 27 May 2009 07:41:08 -0700<br>From: Eugene Loh &lt;<a ymailto="mailto:Eugene.Loh@Sun.COM" href="mailto:Eugene.Loh@Sun.COM">Eugene.Loh@Sun.COM</a>&gt;<br>Subject: Re: [OMPI users] "An error occurred in MPI_Recv" with more<br>&nbsp; &nbsp; than 2&nbsp; &nbsp; CPU<br>To: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID: &lt;<a ymailto="mailto:4A1D5104.7090501@sun.com" href="mailto:4A1D5104.7090501@sun.com">4A1D5104.7090501@sun.com</a>&gt;<br>Content-Type: text/plain; CHARSET=US-ASCII; format=flowed<br><br>vasilis wrote:<br><br>&gt;Thank you Eugene for your suggestion. I used different tags for each variable, <br>&gt;and now I do not get this error. <br>&gt;The problem now is that I am getting a different solution when I use more
 than <br>&gt;2 CPUs. I checked the matrices and I found that they differ by a very small <br>&gt;amount&nbsp; of the order 10^(-10). Actually, I am getting a different solution if I <br>&gt;use 4CPUs or 16CPUs!!!<br>&gt;Do you have any idea what could cause this behavior?<br>&gt;&nbsp; <br>&gt;<br>Sure.<br><br>Rank 0 accumulates all the res_cpu values into a single array, res.&nbsp; It <br>starts with its own res_cpu and then adds all other processes.&nbsp; When <br>np=2, that means the order is prescribed.&nbsp; When np&gt;2, the order is no <br>longer prescribed and some floating-point rounding variations can start <br>to occur.<br><br>If you want results to be more deterministic, you need to fix the order <br>in which res is aggregated.&nbsp; E.g., instead of using MPI_ANY_SOURCE, loop <br>over the peer processes in a specific order.<br><br><br><br>P.S.&nbsp; It seems to me that you could use MPI collective operations to <br>implement what you're
 doing.&nbsp; E.g., something like:<br><br>call MPI_Reduce(res_cpu, res, total_unknown, MPI_DOUBLE_PRECISION, <br>MPI_SUM, 0, MPI_COMM_WORLD, ierr)<br><br>call MPI_Gather(jacob_cpu, total_elem_cpu * unique, MPI_DOUBLE_PRECISION, &amp;<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; jacob&nbsp; &nbsp; , total_elem_cpu * unique, <br>MPI_DOUBLE_PRECISION, 0, MPI_COMM_WORLD, ierr)<br>call MPI_Gather(&nbsp; row_cpu, total_elem_cpu * unique, MPI_INTEGER&nbsp; &nbsp; &nbsp; &nbsp; , &amp;<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; row&nbsp; &nbsp; , total_elem_cpu * unique, MPI_INTEGER&nbsp; &nbsp; &nbsp; &nbsp; <br>, 0, MPI_COMM_WORLD, ierr)<br>call MPI_Gather(&nbsp; col_cpu, total_elem_cpu * unique, MPI_INTEGER&nbsp; &nbsp; &nbsp; &nbsp; , &amp;<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; col&nbsp; &nbsp; , total_elem_cpu * unique, MPI_INTEGER&nbsp; &nbsp; &nbsp; &nbsp; <br>, 0, MPI_COMM_WORLD,
 ierr)<br><br>I think the res part is right.&nbsp; The jacob/row/col parts are not quite <br>right since you don't just want to gather the elements, but add them <br>into particular arrays.&nbsp; Not sure if you really want to allocate a new <br>scratch array to use for this purpose or what.&nbsp; Nor would this solve the <br>res_cpu indeterministic problem you had.&nbsp; I just wanted to make sure you <br>knew about the MPI collective operations as an alternative to your <br>point-to-point implementation.<br><br><br>------------------------------<br><br>Message: 4<br>Date: Wed, 27 May 2009 10:28:42 -0400<br>From: Jeff Squyres &lt;<a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>Subject: Re: [OMPI users] How to use Multiple links with<br>&nbsp; &nbsp; OpenMPI??????????????????<br>To: "Open MPI Users" &lt;<a ymailto="mailto:users@open-mpi.org"
 href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID: &lt;<a ymailto="mailto:8864ED55-66A8-424E-B1B9-249F033816DE@cisco.com" href="mailto:8864ED55-66A8-424E-B1B9-249F033816DE@cisco.com">8864ED55-66A8-424E-B1B9-249F033816DE@cisco.com</a>&gt;<br>Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes<br><br>Open MPI considers hosts differently than network links.<br><br>So you should only list the actual hostname in the hostfile, with&nbsp; <br>slots equal to the number of processors (4 in your case, I think?).<br><br>Once the MPI processes are launched, they each look around on the host&nbsp; <br>that they're running and find network paths to each of their peers.&nbsp; <br>If they are multiple paths between pairs of peers, Open MPI will round- <br>robin stripe messages across each of the links.&nbsp; We don't really have&nbsp; <br>an easy setting for each peer pair only using 1 link.&nbsp; Indeed, since&nbsp;
 <br>connectivity is bidirectional, the traffic patterns become less&nbsp; <br>obvious if you want MPI_COMM_WORLD rank X to only use link Y -- what&nbsp; <br>does that mean to the other 4 MPI processes on the other host (with&nbsp; <br>whom you have assumedly assigned their own individual links as well)?<br><br><br>On May 26, 2009, at 12:24 AM, shan axida wrote:<br><br>&gt; Hi everyone,<br>&gt; I want to ask how to use multiple links (multiple NICs) with OpenMPI.<br>&gt; For example, how can I assign a link to each process, if there are 4&nbsp; <br>&gt; links<br>&gt; and 4 processors on each node in our cluster?<br>&gt; Is this a correct way?<br>&gt; hostfile:<br>&gt; ----------------------<br>&gt; host1-eth0 slots=1<br>&gt; host1-eth1 slots=1<br>&gt; host1-eth2 slots=1<br>&gt; host1-eth3 slots=1<br>&gt; host2-eth0 slots=1<br>&gt; host2-eth1 slots=1<br>&gt; host2-eth2 slots=1<br>&gt; host2-eth3 slots=1<br>&gt; ...&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 ...<br>&gt; ...&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ...<br>&gt; host16-eth0 slots=1<br>&gt; host16-eth1 slots=1<br>&gt; host16-eth2 slots=1<br>&gt; host16-eth3 slots=1<br>&gt; ------------------------<br>&gt;<br>&gt;<br>&gt;<br>&gt;<br>&gt;<br>&gt;<br>&gt;<br>&gt;<br>&gt;<br>&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><br>-- <br>Jeff Squyres<br>Cisco Systems<br><br><br><br>------------------------------<br><br>_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
 target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br>End of users Digest, Vol 1242, Issue 1<br>**************************************<br><br><br>-------------- next part --------------<br>HTML attachment scrubbed and removed<br><br>------------------------------<br><br>_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br>End of users Digest, Vol 1242, Issue 5<br>**************************************<br></div></div></div><br>



      </body></html>
