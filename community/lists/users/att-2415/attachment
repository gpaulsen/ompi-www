<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
<HEAD>
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=us-ascii">
<META NAME="Generator" CONTENT="MS Exchange Server version 6.5.7651.34">
<TITLE>Ompi failing on mx only</TITLE>
</HEAD>
<BODY>
<!-- Converted from text/rtf format -->
<BR>

<P><FONT SIZE=2 FACE="Arial">I was initially using 1.1.2 and moved to 1.2b2 because of a hang on MPI_Bcast() which 1.2b2 reports to fix, and seemed to have done so. My compute nodes are 2 dual core xeons on myrinet with mx. The problem is trying to get ompi running on mx only. My machine file is as follows &#8230;</FONT></P>

<P><FONT SIZE=2 FACE="Arial">node-1 slots=4 max-slots=4</FONT>

<BR><FONT SIZE=2 FACE="Arial">node-2 slots=4 max-slots=4</FONT>

<BR><FONT SIZE=2 FACE="Arial">node-3 slots=4 max-slots=4</FONT>
</P>

<P><FONT SIZE=2 FACE="Arial">'mpirun' with the minimum number of processes in order to get the error ...</FONT>

<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <FONT SIZE=2 FACE="Arial">mpirun --prefix /usr/local/openmpi-1.2b2 -x LD_LIBRARY_PATH --hostfile ./h1-3 -np 2 --mca btl mx,self ./cpi</FONT>
</P>

<P><FONT SIZE=2 FACE="Arial">Results with the following output ...</FONT>
</P>

<P><FONT SIZE=2 FACE="Arial">:~/Projects/ompi/cpi$ mpirun --prefix /usr/local/openmpi-1.2b2 -x LD_LIBRARY_PATH --hostfile ./h1-3 -np 2 --mca btl mx,self ./cpi&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </FONT></P>

<P><FONT SIZE=2 FACE="Arial">--------------------------------------------------------------------------</FONT>

<BR><FONT SIZE=2 FACE="Arial">Process 0.1.0 is unable to reach 0.1.1 for MPI communication.</FONT>

<BR><FONT SIZE=2 FACE="Arial">If you specified the use of a BTL component, you may have</FONT>

<BR><FONT SIZE=2 FACE="Arial">forgotten a component (such as &quot;self&quot;) in the list of </FONT>

<BR><FONT SIZE=2 FACE="Arial">usable components.</FONT>

<BR><FONT SIZE=2 FACE="Arial">--------------------------------------------------------------------------</FONT>

<BR><FONT SIZE=2 FACE="Arial">--------------------------------------------------------------------------</FONT>

<BR><FONT SIZE=2 FACE="Arial">Process 0.1.1 is unable to reach 0.1.0 for MPI communication.</FONT>

<BR><FONT SIZE=2 FACE="Arial">If you specified the use of a BTL component, you may have</FONT>

<BR><FONT SIZE=2 FACE="Arial">forgotten a component (such as &quot;self&quot;) in the list of </FONT>

<BR><FONT SIZE=2 FACE="Arial">usable components.</FONT>

<BR><FONT SIZE=2 FACE="Arial">--------------------------------------------------------------------------</FONT>

<BR><FONT SIZE=2 FACE="Arial">--------------------------------------------------------------------------</FONT>

<BR><FONT SIZE=2 FACE="Arial">It looks like MPI_INIT failed for some reason; your parallel process is</FONT>

<BR><FONT SIZE=2 FACE="Arial">likely to abort.&nbsp; There are many reasons that a parallel process can</FONT>

<BR><FONT SIZE=2 FACE="Arial">fail during MPI_INIT; some of which are due to configuration or environment</FONT>

<BR><FONT SIZE=2 FACE="Arial">problems.&nbsp; This failure appears to be an internal failure; here's some</FONT>

<BR><FONT SIZE=2 FACE="Arial">additional information (which may only be relevant to an Open MPI</FONT>

<BR><FONT SIZE=2 FACE="Arial">developer):</FONT>
</P>

<P><FONT SIZE=2 FACE="Arial">&nbsp; PML add procs failed</FONT>

<BR><FONT SIZE=2 FACE="Arial">&nbsp; --&gt; Returned &quot;Unreachable&quot; (-12) instead of &quot;Success&quot; (0)</FONT>

<BR><FONT SIZE=2 FACE="Arial">--------------------------------------------------------------------------</FONT>

<BR><FONT SIZE=2 FACE="Arial">*** An error occurred in MPI_Init</FONT>

<BR><FONT SIZE=2 FACE="Arial">*** before MPI was initialized</FONT>

<BR><FONT SIZE=2 FACE="Arial">*** MPI_ERRORS_ARE_FATAL (goodbye)</FONT>

<BR><FONT SIZE=2 FACE="Arial">--------------------------------------------------------------------------</FONT>

<BR><FONT SIZE=2 FACE="Arial">It looks like MPI_INIT failed for some reason; your parallel process is</FONT>

<BR><FONT SIZE=2 FACE="Arial">likely to abort.&nbsp; There are many reasons that a parallel process can</FONT>

<BR><FONT SIZE=2 FACE="Arial">fail during MPI_INIT; some of which are due to configuration or environment</FONT>

<BR><FONT SIZE=2 FACE="Arial">problems.&nbsp; This failure appears to be an internal failure; here's some</FONT>

<BR><FONT SIZE=2 FACE="Arial">additional information (which may only be relevant to an Open MPI</FONT>

<BR><FONT SIZE=2 FACE="Arial">developer):</FONT>
</P>

<P><FONT SIZE=2 FACE="Arial">&nbsp; PML add procs failed</FONT>

<BR><FONT SIZE=2 FACE="Arial">&nbsp; --&gt; Returned &quot;Unreachable&quot; (-12) instead of &quot;Success&quot; (0)</FONT>

<BR><FONT SIZE=2 FACE="Arial">--------------------------------------------------------------------------</FONT>

<BR><FONT SIZE=2 FACE="Arial">*** An error occurred in MPI_Init</FONT>

<BR><FONT SIZE=2 FACE="Arial">*** before MPI was initialized</FONT>

<BR><FONT SIZE=2 FACE="Arial">*** MPI_ERRORS_ARE_FATAL (goodbye)</FONT>

<BR><FONT SIZE=2 FACE="Arial">mpirun noticed that job rank 1 with PID 0 on node node-1 exited on signal 1.</FONT>
</P>

<P><FONT SIZE=2 FACE="Arial">---------------- end of output -----------------------</FONT>
</P>

<P><FONT SIZE=2 FACE="Arial">I get that same error w/ the examples included in the ompi-1.2b2 distrib. However, if I change the mca params as such ...</FONT></P>

<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <FONT SIZE=2 FACE="Arial">mpirun --prefix /usr/local/openmpi-1.2b2 -x LD_LIBRARY_PATH --hostfile ./h1-3 -np 5 --mca pml cm ./cpi</FONT>
</P>

<P><FONT SIZE=2 FACE="Arial">Running up to -np 5 works (one of the processes does get put on the 2nd node), but running with -np 6 fails with the following &#8230;</FONT></P>

<P><FONT SIZE=2 FACE="Arial">[node-2:10464] mx_connect fail for node-2:0 with key aaaaffff (error Endpoint closed or not connectable!)</FONT>

<BR><FONT SIZE=2 FACE="Arial">[node-2:10463] mx_connect fail for node-2:0 with key aaaaffff (error Endpoint closed or not connectable!)</FONT>

<BR><FONT SIZE=2 FACE="Arial">--------------------------------------------------------------------------</FONT>

<BR><FONT SIZE=2 FACE="Arial">It looks like MPI_INIT failed for some reason; your parallel process is</FONT>

<BR><FONT SIZE=2 FACE="Arial">likely to abort.&nbsp; There are many reasons that a parallel process can</FONT>

<BR><FONT SIZE=2 FACE="Arial">fail during MPI_INIT; some of which are due to configuration or environment</FONT>

<BR><FONT SIZE=2 FACE="Arial">problems.&nbsp; This failure appears to be an internal failure; here's some</FONT>

<BR><FONT SIZE=2 FACE="Arial">additional information (which may only be relevant to an Open MPI</FONT>

<BR><FONT SIZE=2 FACE="Arial">developer):</FONT>
</P>

<P><FONT SIZE=2 FACE="Arial">&nbsp; PML add procs failed</FONT>

<BR><FONT SIZE=2 FACE="Arial">&nbsp; --&gt; Returned &quot;Error&quot; (-1) instead of &quot;Success&quot; (0)</FONT>

<BR><FONT SIZE=2 FACE="Arial">--------------------------------------------------------------------------</FONT>

<BR><FONT SIZE=2 FACE="Arial">*** An error occurred in MPI_Init</FONT>

<BR><FONT SIZE=2 FACE="Arial">*** before MPI was initialized</FONT>

<BR><FONT SIZE=2 FACE="Arial">*** MPI_ERRORS_ARE_FATAL (goodbye)</FONT>

<BR><FONT SIZE=2 FACE="Arial">--------------------------------------------------------------------------</FONT>

<BR><FONT SIZE=2 FACE="Arial">It looks like MPI_INIT failed for some reason; your parallel process is</FONT>

<BR><FONT SIZE=2 FACE="Arial">likely to abort.&nbsp; There are many reasons that a parallel process can</FONT>

<BR><FONT SIZE=2 FACE="Arial">fail during MPI_INIT; some of which are due to configuration or environment</FONT>

<BR><FONT SIZE=2 FACE="Arial">problems.&nbsp; This failure appears to be an internal failure; here's some</FONT>

<BR><FONT SIZE=2 FACE="Arial">additional information (which may only be relevant to an Open MPI</FONT>

<BR><FONT SIZE=2 FACE="Arial">developer):</FONT>
</P>

<P><FONT SIZE=2 FACE="Arial">&nbsp; PML add procs failed</FONT>

<BR><FONT SIZE=2 FACE="Arial">&nbsp; --&gt; Returned &quot;Error&quot; (-1) instead of &quot;Success&quot; (0)</FONT>

<BR><FONT SIZE=2 FACE="Arial">--------------------------------------------------------------------------</FONT>

<BR><FONT SIZE=2 FACE="Arial">*** An error occurred in MPI_Init</FONT>

<BR><FONT SIZE=2 FACE="Arial">*** before MPI was initialized</FONT>

<BR><FONT SIZE=2 FACE="Arial">*** MPI_ERRORS_ARE_FATAL (goodbye)</FONT>

<BR><FONT SIZE=2 FACE="Arial">mpirun noticed that job rank 0 with PID 0 on node node-1 exited on signal 1.</FONT>

<BR><FONT SIZE=2 FACE="Arial">3 additional processes aborted (not shown)</FONT>
</P>

<P><FONT SIZE=2 FACE="Arial">----------------- end of mpirun output ---------------------</FONT>
</P>

<P><FONT SIZE=2 FACE="Arial">I don't believe there'a anything wrong w/ the hardware as I can ping on mx between this failed node and the master fine. So I tried a different set of 3 nodes and I got the same error, it always fails on the 2nd node of any group of nodes I choose.</FONT></P>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>

</BODY>
</HTML>
