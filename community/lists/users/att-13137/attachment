Hi all,<div><br><div>I had the same problem like Jitsumoto, i.e. OpenMPI 1.4.2 failed to restart and the patch which Fernando gave didn&#39;t work.</div><div>I also tried 1.5 nightly snapshots but it seemed not working well.</div>
<div>For some purpose, I don&#39;t want to use --enable-ft-thread in configure but the same error occurred even --enable-ft-thread is used.</div><div>Here is my configure for OMPI 1.5a1r23135:</div><div><br></div><div><div>
&gt;./configure \</div><div>&gt;--with-ft=cr \</div><div>&gt;--enable-mpi-threads \</div><div>&gt;--with-blcr=/home/nguyen/opt/blcr --with-blcr-libdir=/home/nguyen/opt/blcr/lib \</div><div>&gt;--prefix=/home/nguyen/opt/openmpi_1.5 --enable-mpirun-prefix-by-default \</div>
<div><br></div><div>and errors:</div><div><br></div><div>&gt;$ mpirun -am ft-enable-cr -machinefile ./host ./a.out</div><div><div>&gt;0</div><div>&gt;0</div><div>&gt;1</div><div>&gt;1</div><div>&gt;2</div><div>&gt;2</div>
<div>&gt;3</div><div>&gt;3</div></div><div>&gt;----------------------------------------------------------------------</div><div><div>&gt;mpirun has exited due to process rank 1 with PID 6582 on</div><div>&gt;node rc014 exiting improperly. There are two reasons this could occur:</div>
<div><br></div><div>&gt;1. this process did not call &quot;init&quot; before exiting, but others in</div><div>&gt;the job did. This can cause a job to hang indefinitely while it waits</div><div>&gt;for all processes to call &quot;init&quot;. By rule, if one process calls &quot;init&quot;,</div>
<div>&gt;then ALL processes must call &quot;init&quot; prior to termination.</div><div><br></div><div>&gt;2. this process called &quot;init&quot;, but exited without calling &quot;finalize&quot;.</div><div>&gt;By rule, all processes that call &quot;init&quot; MUST call &quot;finalize&quot; prior to</div>
<div>&gt;exiting or it will be considered an &quot;abnormal termination&quot;</div><div><br></div><div>&gt;This may have caused other processes in the application to be</div><div>&gt;terminated by signals sent by mpirun (as reported here).</div>
<div>&gt;-----------------------------------------------------------------------</div><div><br></div><div>And here is the checkpoint command:</div><div><br></div><div>&gt;$ ompi-checkpoint -s -v --term 10982</div><div><div>
&gt;[rc013.local:11001] [  0.00 /   0.14]                 Requested - ...</div><div>&gt;[rc013.local:11001] [  0.00 /   0.14]                   Pending - ...</div><div>&gt;[rc013.local:11001] [  0.01 /   0.15]                   Running - ...</div>
<div>&gt;[rc013.local:11001] [  7.79 /   7.94]                  Finished - &gt;ompi_global_snapshot_10982.ckpt</div><div>&gt;Snapshot Ref.:   0 ompi_global_snapshot_10982.ckpt</div></div><div><br></div><div>I also took a look inside the checkpoint files and found that the snapshot was taken: ~/tmp/ckpt/ompi_global_snapshot_10982.ckpt/0/opal_snapshot_1.ckpt/ompi_blcr_context.6582</div>
<div><br></div><div>But restarting failed as follows:</div><div>&gt;$ ompi-restart ompi_global_snapshot_10982.ckpt</div><div><div>&gt;--------------------------------------------------------------------------</div><div>&gt;mpirun noticed that process rank 1 with PID 11346 on node rc013.local exited &gt;on signal 11 (Segmentation fault).</div>
<div>&gt;--------------------------------------------------------------------------</div></div><div><br></div><div>Is there any idea about this? Thank you!</div><div><br></div><div>Regards,</div><div>Nguyen Toan</div><div>
<br></div><br><div class="gmail_quote">On Mon, May 24, 2010 at 4:08 PM, Hideyuki Jitsumoto <span dir="ltr">&lt;<a href="mailto:jitumoto@gsic.titech.ac.jp">jitumoto@gsic.titech.ac.jp</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
---------- Forwarded message ----------<br>
From: Fernando Lemos &lt;<a href="mailto:fernandotcl@gmail.com">fernandotcl@gmail.com</a>&gt;<br>
Date: Thu, Apr 15, 2010 at 2:18 AM<br>
Subject: Re: [OMPI users] OpenMPI Checkpoint/Restart is failed<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
<br>
<br>
On Wed, Apr 14, 2010 at 5:25 AM, Hideyuki Jitsumoto<br>
&lt;<a href="mailto:hjitsumoto@gmail.com">hjitsumoto@gmail.com</a>&gt; wrote:<br>
&gt; Fernando,<br>
&gt;<br>
&gt; Thank you for your reply.<br>
&gt; I tried to patch the file you mentioned, but the output did not change.<br>
<br>
I didn&#39;t test the patch, tbh. I&#39;m using 1.5 nightly snapshots, and it<br>
works great.<br>
<br>
&gt;&gt;Are you using a shared file system? You need to use a shared file<br>
&gt; system for checkpointing with 1.4.1:<br>
&gt; What is the shared file system ? do you mean NFS, Lustre and so on ?<br>
&gt; (I&#39;m sorry about my ignorance...)<br>
<br>
Something like NFS, yea.<br>
<br>
&gt; If I use only one node for application, do I need such a shared-file-system ?<br>
<br>
No, for a single node, checkpointing with 1.4.1 should work (it works<br>
for me, at least). If you&#39;re using a single node, then your problem is<br>
probably not related to the bug report I posted.<br>
<br>
<br>
Regards,<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<font color="#888888"><br>
<br>
<br>
--<br>
Sincerely Yours,<br>
Hideyuki Jitsumoto (<a href="mailto:jitumoto@gsic.titech.ac.jp">jitumoto@gsic.titech.ac.jp</a>)<br>
Tokyo Institute of Technology<br>
Global Scientific Information and Computing center (Matsuoka Lab.)<br>
</font></blockquote></div><br></div></div></div>

