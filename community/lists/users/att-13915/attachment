i have good news.<div><br></div><div>after updating to a newer kernel on ubuntu server nodes, sm is not a problem anymore for the nehalem CPUs!!!</div><div>my older kernel, was</div><div><span class="Apple-style-span" style="font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse; ">Linux 2.6.32-22-server #36-Ubuntu SMP Thu Jun 3 20:38:33 UTC 2010 x86_64 GNU/Linux</span></div>

<div><span class="Apple-style-span" style="font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse; "><br></span></div><div><span class="Apple-style-span" style="font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse; ">and i upgraded to</span></div>

<div><div>Linux agua 2.6.32-24-server #39-Ubuntu SMP Wed Jul 28 06:21:40 UTC 2010 x86_64 GNU/Linux</div><div><br></div><div>that solved everything.</div><div>Gus, maybe the problem you had with fedora can be solved in a similar way.</div>

<div><br></div><div>we should keep this for the records.</div><div><br></div><div>regards</div><div>Cristobal</div><div><br></div><div><br></div><div><br></div><div><br></div><div><br><br><div class="gmail_quote">On Wed, Jul 28, 2010 at 6:45 PM, Gus Correa <span dir="ltr">&lt;<a href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;"><div class="im">Cristobal Navarro wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Gus<br>
my kernel for all nodes is this one:<br>
Linux 2.6.32-22-server #36-Ubuntu SMP Thu Jun 3 20:38:33 UTC 2010 x86_64 GNU/Linux<br>
<br>
</blockquote>
<br></div>
Kernel is not my league.<br>
<br>
However, it would be great if somebody clarified<br>
for good these issues with Nehalem/Westmere, HT,<br>
shared memory and what the kernel is doing,<br>
or how to make the kernel do the right thing.<br>
Maybe Intel could tell.<div class="im"><br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
at least for the moment i will use this configuration, at least for deveplopment/testing  of the parallel programs.<br>
lag is minimum :)<br>
<br>
whenever i get another kernel update, i will test again to check if sm works, would be good to know that suddenly another distribution supports nehalem sm.<br>
<br>
best regards and thanks again<br>
Cristobal<br>
ps: guess what are the names of the other 2 nodes lol<br>
</blockquote>
<br></div>
Acatenango (I said that before), and Pacaya.<br>
<br>
Maybe: Santa Maria, Santiaguito, Atitlan, Toliman, San Pedro,<br>
Cerro de Oro ... too many volcanoes, and some are multithreaded ...<br>
You need to buy more nodes!<br><font color="#888888">
<br>
Gus<br>
<br>
</font><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div class="im">
<br>
<br>
<br>
On Wed, Jul 28, 2010 at 5:50 PM, Gus Correa &lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a> &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;&gt; wrote:<br>


<br>
    Hi Cristobal<br>
<br>
    Please, read my answer (way down the message) below.<br>
<br>
    Cristobal Navarro wrote:<br>
<br>
<br>
<br>
        On Wed, Jul 28, 2010 at 3:28 PM, Gus Correa<br>
        &lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a> &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;<br></div><div class="im">
        &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a> &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;&gt;&gt;<br>
        wrote:<br>
<br>
           Hi Cristobal<br>
<br>
           Cristobal Navarro wrote:<br>
<br>
<br>
<br>
               On Wed, Jul 28, 2010 at 11:09 AM, Gus Correa<br>
               &lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a> &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;<br>
        &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a> &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;&gt;<br></div><div><div></div>

<div class="h5">
               &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a><br>
        &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt; &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a><br>
        &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;&gt;&gt;&gt;<br>
               wrote:<br>
<br>
                  Hi Cristobal<br>
<br>
                  In case you are not using full path name for<br>
        mpiexec/mpirun,<br>
                  what does &quot;which mpirun&quot; say?<br>
<br>
<br>
               --&gt; $which mpirun<br>
                    /opt/openmpi-1.4.2<br>
<br>
<br>
                  Often times this is a source of confusion, old<br>
        versions may<br>
                  be first on the PATH.<br>
<br>
                  Gus<br>
<br>
<br>
               openMPI version problem is now gone, i can confirm that the<br>
               version is consistent now :), thanks.<br>
<br>
<br>
           This is good news.<br>
<br>
<br>
               however, i keep getting this kernel crash randomnly when i<br>
               execute with -np higher than 5<br>
               these are Xeons, with Hyperthreading On, is that a problem??<br>
<br>
<br>
           The problem may be with Hyperthreading, maybe not.<br>
           Which Xeons?<br>
<br>
<br>
        --&gt; they are not so old, not so new either<br>
        fcluster@agua:~$ cat /proc/cpuinfo | more<br>
        processor : 0<br>
        vendor_id : GenuineIntel<br>
        cpu family : 6<br>
        model : 26<br>
        model name : Intel(R) Xeon(R) CPU           E5520  @ 2.27GHz<br>
        stepping : 5<br>
        cpu MHz : 1596.000<br>
        cache size : 8192 KB<br>
        physical id : 0<br>
        siblings : 8<br>
        core id : 0<br>
        cpu cores : 4<br>
        apicid : 0<br>
        initial apicid : 0<br>
        fpu : yes<br>
        fpu_exception : yes<br>
        cpuid level : 11<br>
        wp : yes<br>
        flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca<br>
        cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss h<br>
        t tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts<br>
        rep_good xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_<br>
        cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 popcnt<br>
        lahf_lm ida tpr_shadow vnmi flexpriority ept vpid<br>
        bogomips : 4522.21<br>
        clflush size : 64<br>
        cache_alignment : 64<br>
        address sizes : 40 bits physical, 48 bits virtual<br>
        power management:<br>
        ...same for cpu1, 2, 3, ..., 15.<br>
<br>
<br>
    AHA! Nehalems!<br>
<br>
    Here they are E5540, just a different clock speed, I suppose.<br>
<br>
<br>
        information on how the cpu is distributed<br>
        fcluster@agua:~$ lstopo<br>
        System(7992MB)<br>
         Socket#0 + L3(8192KB)<br>
           L2(256KB) + L1(32KB) + Core#0<br>
             P#0<br>
             P#8<br>
           L2(256KB) + L1(32KB) + Core#1<br>
             P#2<br>
             P#10<br>
           L2(256KB) + L1(32KB) + Core#2<br>
             P#4<br>
             P#12<br>
           L2(256KB) + L1(32KB) + Core#3<br>
             P#6<br>
             P#14<br>
         Socket#1 + L3(8192KB)<br>
           L2(256KB) + L1(32KB) + Core#0<br>
             P#1<br>
             P#9<br>
           L2(256KB) + L1(32KB) + Core#1<br>
             P#3<br>
             P#11<br>
           L2(256KB) + L1(32KB) + Core#2<br>
             P#5<br>
             P#13<br>
           L2(256KB) + L1(32KB) + Core#3<br>
             P#7<br>
             P#15<br>
<br>
<br>
<br>
                    If I remember right, the old hyperthreading on old Xeons was<br>
           problematic.<br>
<br>
           OTOH, about 1-2 months ago I had trouble with OpenMPI on a<br>
           relatively new Xeon Nehalem machine with (the new) Hyperthreading<br>
           turned on,<br>
           and Fedora Core 13.<br>
           The machine would hang with the OpenMPI connectivity example.<br>
           I reported this to the list, you may find in the archives.<br>
<br>
<br>
        --i foudn the archives recently about an hour ago, was not sure<br>
        if it was the same problem but i removed HT for testing with<br>
        setting the online flag to 0 on the extra cpus showed with<br>
        lstopo, unfortenately i also crashes, so HT may not be the problem.<br>
<br>
<br>
    It didn&#39;t fix the problem in our Nehalem machine here either,<br>
    although it was FC13, and I don&#39;t know what OS and kernel you&#39;re using.<br>
<br>
<br>
           Apparently other people got everything (OpenMPI with HT on<br>
        Nehalem)<br>
           working in more stable distributions (CentOS, RHEL, etc).<br>
<br>
           That problem was likely to be in the FC13 kernel,<br>
           because even turning off HT I still had the machine hanging.<br>
           Nothing worked with shared memory turned on,<br>
           so I had to switch OpenMPI to use tcp instead,<br>
           which is kind of ridiculous in a standalone machine.<br>
<br>
<br>
        --&gt; very interesting, sm can be the problem<br>
         <br>
<br>
<br>
               im trying to locate the kernel error on logs, but after<br>
               rebooting a crash, the error is not in the kern.log (neither<br>
               kern.log.1).<br>
               all i remember is that it starts with &quot;Kernel BUG...&quot;<br>
               and somepart it mentions a certain CPU X, where that cpu<br>
        can be<br>
               any from 0 to 15 (im testing only in main node).  Someone<br>
        knows<br>
               where the log of kernel error could be?<br>
<br>
<br>
           Have you tried to turn off hyperthreading?<br>
<br>
<br>
        --&gt; yes, tried, same crashes.<br>
                    In any case, depending on the application, it may not help much<br>
           performance to have HT on.<br>
<br>
           A more radical alternative is to try<br>
           -mca btl tcp,self<br>
           in the mpirun command line.<br>
           That is what worked in the case I mentioned above.<br>
<br>
<br>
        wow!, this worked really :),  you pointed out the problem, it<br>
        was shared memory.<br>
<br>
<br>
    Great news!<br>
    That&#39;s exactly the problem we had here.<br>
    Glad that the same solution worked for you.<br>
<br>
    Over a year ago another fellow reported the same problem on Nehalem,<br>
    on the very early days of Nehalem.<br>
    The thread should be in the archives.<br>
    Somebody back then (Ralph, or Jeff, or other?)<br>
    suggested that turning off &quot;sm&quot; might work.<br>
    So, I take no credit for this.<br>
<br>
<br>
        i have 4 nodes, so anyways there will be node comunication, do<br>
        you think i can rely on working with -mca btl tcp,self?? i dont<br>
        mind small lag.<br>
<br>
<br>
    Well, this may be it, short from reinstalling the OS.<br>
<br>
    Some people reported everything works with OpenMPI+HT+sm in CentOS<br>
    and RHEL, see the thread I mentioned in the archives from 1-2 months<br>
    ago.<br>
    I don&#39;t administer that machine, and didn&#39;t have the time to do OS<br>
    reinstall either.<br>
    So I left it with -mca btl tcp,self, and the user/machine owner<br>
    is happy that he can run his programs right,<br>
    and with a performance that he considers good.<br>
<br>
<br>
        i just have one more question, is this a problem of the ubuntu<br>
        server kernel?? from the Nehalem Cpus?? from openMPI (i dont<br>
        think) ??<br>
<br>
<br>
    I don&#39;t have any idea.<br>
    It may be a problem with some kernels, not sure.<br>
    Which kernel do you have?<br>
<br>
    Ours was FC-13, maybe FC-12, I don&#39;t remember exactly.<br>
    Currently that machine has kernel 2.6.33.6-147.fc13.x86_64 #1 SMP.<br>
    However, it may have been a slightly older kernel when I installed<br>
    OpenMPI there.<br>
    It may have been 2.6.33.5-124.fc13.x86_64 or 2.6.32.14-127.fc12.x86_64.<br>
    My colleague here updates the machines with yum,<br>
    so it may have gotten a new kernel since then.<br>
<br>
    Our workhorse machines in the clusters that I take care<br>
    of are AMD Opteron, never had this problem there.<br>
    Maybe the kernels have yet to catch up with Nehalem,<br>
    now Westmere, soon another one.<br>
<br>
<br>
        and on what depends that in the future, sm could be possible on<br>
        the same configuration i have?? kernel update?.<br>
<br>
<br>
    You may want to try CentOS or RHEL, but I can&#39;t guarantee the results.<br>
    Somebody else in the list may have had the direct experience,<br>
    and may speak out.<br>
<br>
    It may be worth the effort anyway.<br>
    After all, intra-node communication should be<br>
    running on shared memory.<br>
    Having to turn it off is outrageous.<br>
<br>
    If you try another OS distribution,<br>
    and if it works, please report the results back to the list:<br>
    OS/distro, kernel, OpenMPI version, HT on or off,<br>
    mca btl sm/tcp/self/etc choices, compilers, etc.<br>
    This type of information is a real time saving for everybody.<br>
<br>
<br>
<br>
        Thanks very much Gus, really!<br>
        Cristobal<br>
<br>
<br>
<br>
    My pleasure.<br>
    Glad that there was a solution, even though not the best.<br>
    Enjoy your cluster with vocano-named nodes!<br>
    Have fun with OpenMPI and PETSc!<br>
<br>
    Gus Correa<br>
    ---------------------------------------------------------------------<br>
    Gustavo Correa<br>
    Lamont-Doherty Earth Observatory - Columbia University<br>
    Palisades, NY, 10964-8000 - USA<br>
    ---------------------------------------------------------------------<br>
<br>
         <br>
           My $0.02<br>
           Gus Correa<br>
<br>
<br>
                  Cristobal Navarro wrote:<br>
<br>
<br>
                      On Tue, Jul 27, 2010 at 7:29 PM, Gus Correa<br>
                      &lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a><br>
        &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt; &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a><br>
        &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;&gt;<br>
               &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a><br>
        &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt; &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a><br>
        &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;&gt;&gt;<br>
                      &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a><br>
        &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;<br>
               &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a><br>
        &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;&gt; &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a><br>
        &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;<br>
               &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a><br>
        &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;&gt;&gt;&gt;&gt;<br>
<br>
                      wrote:<br>
<br>
                         Hi Cristobal<br>
<br>
                         Does it run only on the head node alone?<br>
                         (Fuego? Agua? Acatenango?)<br>
                         Try to put only the head node on the hostfile<br>
        and execute<br>
                      with mpiexec.<br>
<br>
                      --&gt; i will try only with the head node, and post<br>
        results back<br>
                         This may help sort out what is going on.<br>
                         Hopefully it will run on the head node.<br>
<br>
                         Also, do you have Infinband connecting the nodes?<br>
                         The error messages refer to the openib btl (i.e.<br>
               Infiniband),<br>
                         and complains of<br>
<br>
<br>
                      no we are just using normal network 100MBit/s ,<br>
        since i<br>
               am just<br>
                      testing yet.<br>
<br>
<br>
                         &quot;perhaps a missing symbol, or compiled for a<br>
        different<br>
                         version of Open MPI?&quot;.<br>
                         It sounds as a mixup of versions/builds.<br>
<br>
<br>
                      --&gt; i agree, somewhere there must be the remains<br>
        of the older<br>
                      version<br>
<br>
                         Did you configure/build OpenMPI from source, or did<br>
               you install<br>
                         it with apt-get?<br>
                         It may be easier/less confusing to install from<br>
        source.<br>
                         If you did, what configure options did you use?<br>
<br>
<br>
                      --&gt;i installed from source, ./configure<br>
                      --prefix=/opt/openmpi-1.4.2 --with-sge --without-xgid<br>
                      --disable--static<br>
<br>
                         Also, as for the OpenMPI runtime environment,<br>
                         it is not enough to set it on<br>
                         the command line, because it will be effective<br>
        only on the<br>
                      head node.<br>
                         You need to either add them to the PATH and<br>
               LD_LIBRARY_PATH<br>
                         on your .bashrc/.cshrc files (assuming these<br>
        files and<br>
               your home<br>
                         directory are *also* shared with the nodes via<br>
        NFS),<br>
                         or use the --prefix option of mpiexec to point<br>
        to the<br>
               OpenMPI<br>
                      main<br>
                         directory.<br>
<br>
<br>
                      yes, all nodes have their PATH and LD_LIBRARY_PATH<br>
        set up<br>
                      properly inside the login scripts ( .bashrc in my<br>
        case  )<br>
<br>
                         Needless to say, you need to check and ensure<br>
        that the<br>
               OpenMPI<br>
                         directory (and maybe your home directory, and<br>
        your work<br>
                      directory)<br>
                         is (are)<br>
                         really mounted on the nodes.<br>
<br>
<br>
                      --&gt; yes, doublechecked that they are<br>
<br>
                         I hope this helps,<br>
<br>
<br>
                      --&gt; thanks really!<br>
<br>
                         Gus Correa<br>
<br>
                         Update: i just reinstalled openMPI, with the same<br>
               parameters,<br>
                      and it<br>
                         seems that the problem has gone, i couldnt test<br>
               entirely but<br>
                      when i<br>
                         get back to lab ill confirm.<br>
<br>
                      best regards! Cristobal<br>
<br>
<br>
                                    ------------------------------------------------------------------------<br>
<br>
                      _______________________________________________<br>
                      users mailing list<br>
                      <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
        &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;&gt;<br>
               &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
        &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;&gt;&gt;<br>
<br>
<br>
                      <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
                  _______________________________________________<br>
                  users mailing list<br>
                  <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
        &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;&gt;<br>
               &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
        &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;&gt;&gt;<br>
<br>
<br>
                  <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
                      ------------------------------------------------------------------------<br>
<br>
               _______________________________________________<br>
               users mailing list<br>
               <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
        &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;&gt;<br>
               <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
           _______________________________________________<br>
           users mailing list<br>
           <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
        &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;&gt;<br>
           <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
        ------------------------------------------------------------------------<br>
<br>
        _______________________________________________<br>
        users mailing list<br>
        <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
        <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
    _______________________________________________<br>
    users mailing list<br>
    <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
    <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
------------------------------------------------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote><div><div></div><div class="h5">
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div></div>

