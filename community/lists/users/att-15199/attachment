<table cellspacing="0" cellpadding="0" border="0" ><tr><td valign="top" style="font: inherit;"><BLOCKQUOTE style="BORDER-LEFT: rgb(16,16,255) 2px solid; PADDING-LEFT: 5px; MARGIN-LEFT: 5px">
<DIV class=plainMail>I forgot add some other details...</DIV>
<DIV class=plainMail>&nbsp;</DIV>
<DIV class=plainMail>1) I am setting the affinity of each process to a specific core,&nbsp;explicitly in my application (...with OS system call)</DIV>
<DIV class=plainMail>&nbsp;</DIV>
<DIV class=plainMail>2) I enabled the 'use_eager_rdma' with the corresponding buffer limit at 32 KBytes( large enough to cover all my message sizes)</DIV>
<DIV class=plainMail>&nbsp;</DIV>
<DIV class=plainMail>3) I set the btl mtu be 2048 bytes.</DIV>
<DIV class=plainMail>&nbsp;</DIV>
<DIV class=plainMail>My first objective is to figure out if those large jitter on MPI_Test call is from the MPI library / non-optimal setting, or from OS/ related services taking CPU away.</DIV>
<DIV class=plainMail>&nbsp;</DIV>
<DIV class=plainMail>Thanks,</DIV>
<DIV class=plainMail>Sashi</DIV>
<DIV class=plainMail>&nbsp;</DIV>
<DIV class=plainMail>Sashi<BR><BR>Date: Sat, 18 Dec 2010 07:00:36 -0500<BR>From: Tim Prince &lt;<A href="http://us.mc329.mail.yahoo.com/mc/compose?to=n8tm@aol.com" ymailto="mailto:n8tm@aol.com">n8tm@aol.com</A>&gt;<BR>Subject: Re: [OMPI users] Call to MPI_Test has large time-jitter<BR>To: <A href="http://us.mc329.mail.yahoo.com/mc/compose?to=users@open-mpi.org" ymailto="mailto:users@open-mpi.org">users@open-mpi.org</A><BR>Message-ID: &lt;<A href="http://us.mc329.mail.yahoo.com/mc/compose?to=4D0CA264.4040305@aol.com" ymailto="mailto:4D0CA264.4040305@aol.com">4D0CA264.4040305@aol.com</A>&gt;<BR>Content-Type: text/plain; charset="iso-8859-1"; Format="flowed"<BR><BR>On 12/17/2010 6:43 PM, Sashi Balasingam wrote:<BR>&gt; Hi,<BR>&gt; I recently started on an MPI-based, 'real-time', pipelined-processing <BR>&gt; application, and the application fails due to large time-jitter in <BR>&gt; sending and receiving messages. Here are related info -<BR>&gt; 1)
 Platform:<BR>&gt; a) Intel Box: Two Hex-core, Intel Xeon, 2.668 GHz (...total of 12 cores),<BR>&gt; b) OS: SUSE Linux Enterprise Server 11 (x86_64) - Kernel \r (\l)<BR>&gt; c) MPI Rev: (OpenRTE) 1.4, (...Installed OFED package)<BR>&gt; d) HCA: InfiniBand: Mellanox Technologies MT26428 [ConnectX IB QDR, <BR>&gt; PCIe 2.0 5GT/s] (rev a0)<BR>&gt; 2) Application detail<BR>&gt; a) Launching 7 processes, for pipelined processing, where each process <BR>&gt; waits for a message (sizes vary between 1 KBytes to 26 KBytes),<BR>&gt; then process the data, and outputs a message (sizes vary between 1 <BR>&gt; KBytes to 26 KBytes), to next process.<BR>&gt; b) MPI transport functions used : "MPI_Isend", MPI_Irecv, MPI_Test.<BR>&gt;&nbsp; &nbsp; i) For Receiving messages, I first make an MPI_Irecv call, followed <BR>&gt; by a busy-loop on MPI_Test, waiting for message<BR>&gt;&nbsp; &nbsp; ii) For Sending message, there is a busy-loop on MPI_Test to ensure <BR>&gt;
 prior buffer was sent, then use MPI_Isend.<BR>&gt; c) When the job starts, all these 7 process are put in High priority <BR>&gt; mode ( SCHED_FIFO policy, with priority setting of 99).<BR>&gt; The Job entails an input data packet stream (and a series of MPI <BR>&gt; messages), continually at 40 micro-sec rate, for a few minutes.<BR>&gt;<BR>&gt; 3) The Problem:<BR>&gt; Most calls to MPI_Test (...which is non-blocking) takes a few <BR>&gt; micro-sec, but around 10% of the job, it has a large jitter, that vary <BR>&gt; from 1 to 100 odd millisec. This causes<BR>&gt; some of the application input queues to fill-up&nbsp; and cause a failure.<BR>&gt; Any suggestions to look at on the MPI settings or OS config/issues <BR>&gt; will be much appreciated.<BR>&gt;<BR>I didn't see anything there about your -mca affinity settings.&nbsp; Even if <BR>the defaults don't choose optimum mapping, it's way better than allowing <BR>them to float as you would with multiple
 independent jobs running.<BR><BR>-- <BR>Tim Prince<BR><BR>-------------- next part --------------<BR>HTML attachment scrubbed and removed<BR><BR>------------------------------<BR><BR>_______________________________________________<BR>users mailing list<BR><A href="http://us.mc329.mail.yahoo.com/mc/compose?to=users@open-mpi.org" ymailto="mailto:users@open-mpi.org">users@open-mpi.org</A><BR><A href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target=_blank>http://www.open-mpi.org/mailman/listinfo.cgi/users</A><BR><BR>End of users Digest, Vol 1772, Issue 1<BR>**************************************<BR></DIV></BLOCKQUOTE></td></tr></table>
