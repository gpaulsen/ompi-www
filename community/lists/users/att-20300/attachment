<div style="line-height:1.7;color:#000000;font-size:14px;font-family:arial">I have setup a small cluster with 3 nodes, named A, B and C respectively.<div>I tested the ring_c.c program in the examples. For debugging purpose,</div><div>I have added some print statements as follows in the original ring_c.c</div><div><pre>&gt;&gt;&nbsp;&nbsp;60&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf("rank&nbsp;%d,&nbsp;message&nbsp;%d,start===========\n",&nbsp;rank,&nbsp;message);
&gt;&gt;&nbsp;&nbsp;61&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MPI_Send(&amp;message,&nbsp;1,&nbsp;MPI_INT,&nbsp;!&nbsp;next,&nbsp;tag,&nbsp;MPI_COMM_WORLD);
&gt;&gt;&nbsp;&nbsp;62&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf("rank&nbsp;%d,&nbsp;message&nbsp;%d,end-------------\n",&nbsp;rank,&nbsp;message);</pre><pre>I lauched my mpi program as follows:</pre><pre>$mpirun -np 3 --hostfile myhostfile ./ring</pre><pre>the content in myhost file is:</pre><pre>=========</pre><pre>hostA  slots=1</pre><pre>hostB slots=1</pre><pre>hostC slots=1</pre><pre>============</pre><pre>I got the follow output:</pre><pre>==========================</pre><pre>Process 0 sending 10 to 1, tag 201 (3 processes in ring)
Process 0 sent to 1
rank 1, message 10,start===========
rank 1, message 10,end-------------
rank 2, message 10,start===========
Process 0 decremented value: 9
rank 0, message 9,start===========
rank 0, message 9,end-------------
rank 2, message 10,end-------------
rank 1, message 9,start===========</pre><pre>=========================</pre><pre>I assumed there is communication problem between B and C, so I launched the program with 2 processes on B and C.</pre><pre>the output is as follows:</pre><pre>===============</pre><pre>Process 0 sending 10 to 1, tag 201 (2 processes in ring)
Process 0 sent to 1
rank 1, message 10,start===========
rank 1, message 10,end-------------
Process 0 decremented value: 9
rank 0, message 9,start===========
===============</pre></div><div>Again, in the second round of pass, B failed to send message to C.&nbsp;</div><div>I checked firewall config using chkconfig --list iptables on all the nodes. none of them are set as "on".</div><div><br></div><div>Attached is all the information needed, my openmpi version is 1.6.1.</div><div><br></div><div>thanks for your help.</div><div>Richard<br><br><br><div></div><div id="divNeteaseMailCard"></div><br>At 2012-09-25 18:27:15,Richard&nbsp;&lt;codemonkee@163.com&gt; wrote:<br> <blockquote id="isReplyContent" style="PADDING-LEFT: 1ex; MARGIN: 0px 0px 0px 0.8ex; BORDER-LEFT: #ccc 1px solid"><div style="line-height:1.7;color:#000000;font-size:14px;font-family:arial">I used "chkconfig --list iptables ", &nbsp;none of computer is set as "on".<br><br><div>At&nbsp;2012-09-25&nbsp;17:54:53,"Jeff&nbsp;Squyres"&nbsp;&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;&nbsp;wrote:<br><pre>&gt;Hav&nbsp;you&nbsp;disabled&nbsp;firewalls&nbsp;on&nbsp;your&nbsp;nodes&nbsp;(e.g.,&nbsp;iptables)?
&gt;
&gt;On&nbsp;Sep&nbsp;25,&nbsp;2012,&nbsp;at&nbsp;11:08&nbsp;AM,&nbsp;Richard&nbsp;wrote:
&gt;
&gt;&gt;&nbsp;sometimes&nbsp;the&nbsp;following&nbsp;message&nbsp;jumped&nbsp;out&nbsp;when&nbsp;I&nbsp;run&nbsp;the&nbsp;ring&nbsp;program,&nbsp;but&nbsp;not&nbsp;always.
&gt;&gt;&nbsp;I&nbsp;do&nbsp;not&nbsp;know&nbsp;this&nbsp;ip&nbsp;address&nbsp;&nbsp;192.168.122.1,&nbsp;it&nbsp;is&nbsp;not&nbsp;in&nbsp;my&nbsp;list&nbsp;of&nbsp;hosts.
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;[[53402,1],6][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect]&nbsp;connect()&nbsp;to&nbsp;192.168.122.1&nbsp;failed:&nbsp;Connection&nbsp;refused&nbsp;(111
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;At&nbsp;2012-09-25&nbsp;16:53:50,Richard&nbsp;&lt;<a href="mailto:codemonkee@163.com">codemonkee@163.com</a>&gt;&nbsp;wrote:
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;if&nbsp;I&nbsp;tried&nbsp;the&nbsp;ring&nbsp;program,&nbsp;the&nbsp;first&nbsp;round&nbsp;of&nbsp;pass&nbsp;is&nbsp;fine,&nbsp;but&nbsp;the&nbsp;second&nbsp;round&nbsp;is&nbsp;blocked&nbsp;at&nbsp;some&nbsp;node.
&gt;&gt;&nbsp;here&nbsp;is&nbsp;the&nbsp;message&nbsp;printed&nbsp;out
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;Process&nbsp;0&nbsp;sending&nbsp;10&nbsp;to&nbsp;1,&nbsp;tag&nbsp;201&nbsp;(3&nbsp;processes!&nbsp;in&nbsp;ring)
&gt;&gt;&nbsp;Process&nbsp;0&nbsp;sent&nbsp;to&nbsp;1
&gt;&gt;&nbsp;rank&nbsp;1,&nbsp;message&nbsp;10,start===========
&gt;&gt;&nbsp;rank&nbsp;1,&nbsp;message&nbsp;10,end-------------
&gt;&gt;&nbsp;rank&nbsp;2,&nbsp;message&nbsp;10,start===========
&gt;&gt;&nbsp;Process&nbsp;0&nbsp;decremented&nbsp;value:&nbsp;9
&gt;&gt;&nbsp;rank&nbsp;0,&nbsp;message&nbsp;9,start===========
&gt;&gt;&nbsp;rank&nbsp;0,&nbsp;message&nbsp;9,end-------------
&gt;&gt;&nbsp;rank&nbsp;2,&nbsp;message&nbsp;10,end-------------
&gt;&gt;&nbsp;rank&nbsp;1,&nbsp;message&nbsp;9,start===========
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;I&nbsp;have&nbsp;added&nbsp;some&nbsp;printf&nbsp;statements&nbsp;in&nbsp;the&nbsp;ring_c.c&nbsp;as&nbsp;follows:
&gt;&gt;&nbsp;&nbsp;60&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf("rank&nbsp;%d,&nbsp;message&nbsp;%d,start===========\n",&nbsp;rank,&nbsp;message);
&gt;&gt;&nbsp;&nbsp;61&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MPI_Send(&amp;message,&nbsp;1,&nbsp;MPI_INT,&nbsp;!&nbsp;next,&nbsp;tag,&nbsp;MPI_COMM_WORLD);
&gt;&gt;&nbsp;&nbsp;62&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf("rank&nbsp;%d,&nbsp;message&nbsp;%d,end-------------\n",&nbsp;rank,&nbsp;message);
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;At&nbsp;2012-09-25&nbsp;16:30:01,Richard&nbsp;&lt;<a href="mailto:codemonkee@163.com">codemonkee@163.com</a>&gt;&nbsp;wrote:
&gt;&gt;&nbsp;Hi&nbsp;Jody,
&gt;&gt;&nbsp;thanks&nbsp;for&nbsp;your&nbsp;suggestion&nbsp;and&nbsp;you&nbsp;are&nbsp;right.&nbsp;if&nbsp;I&nbsp;use&nbsp;the&nbsp;ring&nbsp;example,&nbsp;the&nbsp;same&nbsp;happened.
&gt;&gt;&nbsp;I&nbsp;have&nbsp;put&nbsp;a&nbsp;printf&nbsp;statement,&nbsp;it&nbsp;seems&nbsp;that&nbsp;all&nbsp;the&nbsp;three&nbsp;processed&nbsp;have&nbsp;reached&nbsp;the&nbsp;line&nbsp;
&gt;&gt;&nbsp;calling&nbsp;"PMPI_Allreduce",&nbsp;any&nbsp;further&nbsp;suggestion?
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;Thanks.
&gt;&gt;&nbsp;Richard
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;Message:&nbsp;12
&gt;&gt;&nbsp;Date:&nbsp;Tue,&nbsp;25&nbsp;Sep&nbsp;2012&nbsp;09:43:09&nbsp;+0200
&gt;&gt;&nbsp;From:&nbsp;jody&nbsp;&lt;
&gt;&gt;&nbsp;<a href="mailto:jody.xha@gmail.com">jody.xha@gmail.com</a>
&gt;&gt;&nbsp;&gt;
&gt;&gt;&nbsp;Subject:&nbsp;Re:&nbsp;[OMPI&nbsp;users]&nbsp;mpi&nbsp;job&nbsp;is&nbsp;blocked
&gt;&gt;&nbsp;To:&nbsp;Open&nbsp;MPI&nbsp;Users&nbsp;&lt;
&gt;&gt;&nbsp;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>
&gt;&gt;&nbsp;&gt;
&gt;&gt;&nbsp;Message-ID:
&gt;&gt;&nbsp;	&lt;
&gt;&gt;&nbsp;<a href="mailto:CAKbzMGfL0tXDYU82HksoHrwh34CbpwbKmrKwC5DcDBT7A7wTxw@mail.gmail.com">CAKbzMGfL0tXDYU82HksoHrwh34CbpwbKmrKwC5DcDBT7A7wTxw@mail.gmail.com</a>
&gt;&gt;&nbsp;&gt;
&gt;&gt;&nbsp;Content-Type:&nbsp;text/plain;&nbsp;charset=ISO-8859-1
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;Hi&nbsp;Richard
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;When&nbsp;a&nbsp;collective&nbsp;call&nbsp;hangs,&nbsp;this&nbsp;usually&nbsp;means&nbsp;that&nbsp;one&nbsp;(or&nbsp;more)
&gt;&gt;&nbsp;processes&nbsp;did&nbsp;not&nbsp;reach&nbsp;this&nbsp;command.
&gt;&gt;&nbsp;Are&nbsp;you&nbsp;sure&nbsp;that&nbsp;all&nbsp;processes&nbsp;reach&nbsp;the&nbsp;allreduce&nbsp;statement?
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;If&nbsp;something&nbsp;like&nbsp;this&nbsp;happens&nbsp;to&nbsp;me,&nbsp;i&nbsp;insert&nbsp;print&nbsp;statements&nbsp;just
&gt;&gt;&nbsp;before&nbsp;the&nbsp;MPI-call&nbsp;so&nbsp;i&nbsp;can&nbsp;see&nbsp;which&nbsp;processes&nbsp;made
&gt;&gt;&nbsp;it&nbsp;to&nbsp;this&nbsp;point&nbsp;and&nbsp;which&nbsp;ones&nbsp;did&nbsp;not.
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;Hope&nbsp;this&nbsp;helps&nbsp;a&nbsp;bit
&gt;&gt;&nbsp;&nbsp;&nbsp;Jody
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;On&nbsp;Tue,&nbsp;Sep&nbsp;25,&nbsp;2012&nbsp;at&nbsp;8:20&nbsp;AM,&nbsp;Richard&nbsp;&lt;
&gt;&gt;&nbsp;<a href="mailto:codemonkee@163.com">codemonkee@163.com</a>
&gt;&gt;&nbsp;&gt;&nbsp;wrote:
&gt;&gt;&nbsp;&gt;&nbsp;I&nbsp;have&nbsp;3&nbsp;computers&nbsp;with&nbsp;the&nbsp;same&nbsp;Linux&nbsp;system.&nbsp;I&nbsp;have&nbsp;setup&nbsp;the&nbsp;mpi&nbsp;cluster
&gt;&gt;&nbsp;&gt;&nbsp;based&nbsp;on&nbsp;ssh&nbsp;connection.
&gt;&gt;&nbsp;&gt;&nbsp;I&nbsp;have&nbsp;tested&nbsp;a&nbsp;very&nbsp;simple&nbsp;mpi&nbsp;program,&nbsp;it&nbsp;works&nbsp;on&nbsp;the&nbsp;cluster.
&gt;&gt;&nbsp;&gt;
&gt;&gt;&nbsp;&gt;&nbsp;To&nbsp;make&nbsp;my&nbsp;story&nbsp;clear,&nbsp;I&nbsp;name&nbsp;the&nbsp;three&nbsp;computer&nbsp;as&nbsp;A,&nbsp;B&nbsp;and&nbsp;C.
&gt;&gt;&nbsp;&gt;
&gt;&gt;&nbsp;&gt;&nbsp;1)&nbsp;If&nbsp;I&nbsp;run&nbsp;the&nbsp;job&nbsp;with&nbsp;2&nbsp;processes&nbsp;on&nbsp;A&nbsp;and&nbsp;B,&nbsp;it&nbsp;works.
&gt;&gt;&nbsp;&gt;&nbsp;2)&nbsp;if&nbsp;I&nbsp;run&nbsp;the&nbsp;job&nbsp;with&nbsp;3&nbsp;processes&nbsp;on&nbsp;A,&nbsp;B&nbsp;and&nbsp;C,&nbsp;it&nbsp;is&nbsp;blocked.
&gt;&gt;&nbsp;&gt;&nbsp;3)&nbsp;if&nbsp;I&nbsp;run&nbsp;the&nbsp;job&nbsp;with&nbsp;2&nbsp;processes&nbsp;on&nbsp;A&nbsp;and&nbsp;C,&nbsp;it&nbsp;works.
&gt;&gt;&nbsp;&gt;&nbsp;4)&nbsp;If&nbsp;I&nbsp;run&nbsp;the&nbsp;job&nbsp;with&nbsp;all&nbsp;the&nbsp;3&nbsp;processes&nbsp;on&nbsp;A,&nbsp;it&nbsp;works.
&gt;&gt;&nbsp;&gt;
&gt;&gt;&nbsp;&gt;&nbsp;Using&nbsp;gdb&nbsp;I&nbsp;found&nbsp;the&nbsp;line&nbsp;at&nbsp;which&nbsp;it&nbsp;is&nbsp;blocked,&nbsp;it&nbsp;is&nbsp;here
&gt;&gt;&nbsp;&gt;
&gt;&gt;&nbsp;&gt;&nbsp;#7&nbsp;&nbsp;0x00002ad8a283043e&nbsp;in&nbsp;PMPI_Allreduce&nbsp;(sendbuf=0x7fff09c7c578,
&gt;&gt;&nbsp;&gt;&nbsp;recvbuf=0x7fff09c7c570,&nbsp;count=1,&nbsp;datatype=0x627180,&nbsp;op=0x627780,
&gt;&gt;&nbsp;&gt;&nbsp;comm=0x627380)
&gt;&gt;&nbsp;&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;at&nbsp;pallreduce.c:105
&gt;&gt;&nbsp;&gt;&nbsp;105&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;err&nbsp;=&nbsp;comm-&gt;c_coll.coll_allreduce(sendbuf,&nbsp;recvbuf,&nbsp;count,
&gt;&gt;&nbsp;&gt;
&gt;&gt;&nbsp;&gt;&nbsp;It&nbsp;seems&nbsp;that&nbsp;there&nbsp;is&nbsp;a&nbsp;communication&nbsp;problem&nbsp;between&nbsp;some&nbsp;computers.&nbsp;But
&gt;&gt;&nbsp;&gt;&nbsp;the&nbsp;above&nbsp;series&nbsp;of&nbsp;test&nbsp;cannot&nbsp;tell&nbsp;me&nbsp;what
&gt;&gt;&nbsp;&gt;&nbsp;exactly&nbsp;it&nbsp;is.&nbsp;Can&nbsp;anyone&nbsp;help&nbsp;me?&nbsp;thanks.
&gt;&gt;&nbsp;&gt;
&gt;&gt;&nbsp;&gt;&nbsp;Richard
&gt;&gt;&nbsp;&gt;
&gt;&gt;&nbsp;&gt;
&gt;&gt;&nbsp;&gt;
&gt;&gt;&nbsp;&gt;
&gt;&gt;&nbsp;&gt;&nbsp;_______________________________________________
&gt;&gt;&nbsp;&gt;&nbsp;users&nbsp;mailing&nbsp;list
&gt;&gt;&nbsp;&gt;&nbsp;
&gt;&gt;&nbsp;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;&gt;&nbsp;http://www.open-mpi.org/mailman/listinfo.cgi/users
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;
&gt;&gt;&nbsp;_______________________________________________
&gt;&gt;&nbsp;users&nbsp;mailing&nbsp;list
&gt;&gt;&nbsp;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>
&gt;&gt;&nbsp;http://www.open-mpi.org/mailman/listinfo.cgi/users
&gt;
&gt;
&gt;--&nbsp;
&gt;Jeff&nbsp;Squyres
&gt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>
&gt;For&nbsp;corporate&nbsp;legal&nbsp;information&nbsp;go&nbsp;to:&nbsp;http://www.cisco.com/web/about/doing_business/legal/cri/
&gt;
&gt;
&gt;_______________________________________________
&gt;users&nbsp;mailing&nbsp;list
&gt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>
&gt;http://www.open-mpi.org/mailman/listinfo.cgi/users
</pre></div></div><br><br><span title="neteasefooter"><span id="netease_mail_footer"></span></span></blockquote></div></div><br><br><span title="neteasefooter"><span id="netease_mail_footer"></span></span>
