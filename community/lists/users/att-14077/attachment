<table cellspacing="0" cellpadding="0" border="0" ><tr><td valign="top" style="font: inherit;">Its a long shot but could it be related to the total data volume ?<br>ie&nbsp; 524288 * 80 = 41943040 bytes active in the cluster<br><br>Can you exceed this 41943040 data volume with a smaller message repeated more often or a larger one less often?<br><br><br>--- On <b>Fri, 20/8/10, Rahul Nabar <i>&lt;rpnabar@gmail.com&gt;</i></b> wrote:<br><blockquote style="border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;"><br>From: Rahul Nabar &lt;rpnabar@gmail.com&gt;<br>Subject: [OMPI users] IMB-MPI broadcast test stalls for large core counts: debug ideas?<br>To: "Open MPI Users" &lt;users@open-mpi.org&gt;<br>Received: Friday, 20 August, 2010, 12:03 PM<br><br><div class="plainMail">My Intel IMB-MPI tests stall, but only in very specific cases:larger<br>packet sizes + large core counts. Only happens for bcast, gather and<br>exchange tests. Only
 for the larger core counts (~256 cores). Other<br>tests like pingpong and sendrecev run fine even with larger core<br>counts.<br><br>e.g. This bcast test hangs consistently at the 524288 bytes packet<br>size when invoked on 256 cores. Same test runs fine on 128 cores.<br><br>NP=256;mpirun&nbsp; -np $NP --host [ 32_HOSTS_8_core_each]&nbsp; -mca btl<br>openib,sm,self&nbsp; &nbsp; /mpitests/imb/src/IMB-MPI1 -npmin $NP&nbsp; bcast<br><br>&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;#bytes #repetitions&nbsp; t_min[usec]&nbsp; t_max[usec]&nbsp; t_avg[usec]<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;1000&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;0.02&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;0.02&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;0.02<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp; &nbsp; 26.94&nbsp; &nbsp; &nbsp; &nbsp; 27.59&nbsp; &nbsp; &nbsp; &nbsp; 27.25<br>&nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; 2&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp; &nbsp; 26.44&nbsp; &nbsp; &nbsp; &nbsp; 27.09&nbsp; &nbsp; &nbsp; &nbsp; 26.77<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 4&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp; &nbsp; 75.98&nbsp; &nbsp; &nbsp; &nbsp; 81.07&nbsp; &nbsp; &nbsp; &nbsp; 76.75<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 8&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp; &nbsp; 28.41&nbsp; &nbsp; &nbsp; &nbsp; 29.06&nbsp; &nbsp; &nbsp; &nbsp; 28.74<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;16&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp; &nbsp; 28.70&nbsp; &nbsp; &nbsp; &nbsp; 29.39&nbsp; &nbsp; &nbsp; &nbsp; 29.03<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;32&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp; &nbsp; 28.48&nbsp; &nbsp; &nbsp; &nbsp; 29.15&nbsp; &nbsp; &nbsp; &nbsp; 28.85<br>&nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp;&nbsp;&nbsp;64&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp; &nbsp; 30.10&nbsp; &nbsp; &nbsp; &nbsp; 30.86&nbsp; &nbsp; &nbsp; &nbsp; 30.48<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 128&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp; &nbsp; 31.62&nbsp; &nbsp; &nbsp; &nbsp; 32.41&nbsp; &nbsp; &nbsp; &nbsp; 32.01<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 256&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp; &nbsp; 31.08&nbsp; &nbsp; &nbsp; &nbsp; 31.72&nbsp; &nbsp; &nbsp; &nbsp; 31.42<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 512&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp; &nbsp; 31.79&nbsp; &nbsp; &nbsp; &nbsp; 32.58&nbsp; &nbsp; &nbsp; &nbsp; 32.13<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;1024&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp; &nbsp; 33.22&nbsp; &nbsp; &nbsp; &nbsp; 34.06&nbsp; &nbsp; &nbsp; &nbsp; 33.65<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;2048&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 130&nbsp; &nbsp; &nbsp; &nbsp; 66.21&nbsp; &nbsp; &nbsp; &nbsp; 67.61&nbsp; &nbsp; &nbsp; &nbsp; 67.21<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;4096&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp; &nbsp; 79.14&nbsp; &nbsp; &nbsp; &nbsp; 80.86&nbsp; &nbsp; &nbsp; &nbsp; 80.37<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;8192&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;103.38&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;105.21&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;104.70<br>&nbsp; &nbsp; &nbsp; &nbsp; 16384&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;160.82&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;163.67&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;162.97<br>&nbsp; &nbsp; &nbsp; &nbsp; 32768&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;516.11&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;541.75&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;533.46<br>&nbsp; &nbsp; &nbsp; &nbsp; 65536&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp;
 1044.09&nbsp; &nbsp; &nbsp; 1063.63&nbsp; &nbsp; &nbsp; 1052.88<br>&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;131072&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp; 1740.09&nbsp; &nbsp; &nbsp; 1750.12&nbsp; &nbsp; &nbsp; 1746.78<br>&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;262144&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 130&nbsp; &nbsp; &nbsp; 3587.23&nbsp; &nbsp; &nbsp; 3598.52&nbsp; &nbsp; &nbsp; 3594.52<br>&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;524288&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;80&nbsp; &nbsp; &nbsp; 4000.99&nbsp; &nbsp; &nbsp; 6669.65&nbsp; &nbsp; &nbsp; 5737.78<br>stalls for at least 5 minutes at this point when I killed the test.<br><br>I did more extensive testing for various combinations of test-type and<br>core counts (see below). I know exactly when the tests fail but I<br>still cannot see a trend from this data. Any points or further debug<br>ideas? I do have padb installed and have collected core dumps if that<br>is going to help? One example
 below:<br><br><a href="http://dl.dropbox.com/u/118481/padb.log.new.new.txt" target="_blank">http://dl.dropbox.com/u/118481/padb.log.new.new.txt</a><br><br>System Details:<br>Intel Nehalem 2.2 GHz<br>10Gig Ethernet Chelsio Cards and Cisco Nexus Switch. Using the OFED drivers.<br>CentOS 5.4<br>Open MPI: 1.4.1 / Open RTE: 1.4.1 / OPAL: 1.4.1<br><br><br>------------------------------------------------------------------<br>bcast:<br>&nbsp; &nbsp; NP256&nbsp; &nbsp; hangs<br>&nbsp; &nbsp; NP128&nbsp; &nbsp; OK<br><br>Note: "bcast" mostly hangs at:<br><br>&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;#bytes #repetitions&nbsp; t_min[usec]&nbsp; t_max[usec]&nbsp; t_avg[usec]<br>&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;524288&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;80&nbsp; &nbsp; &nbsp; 2682.61&nbsp; &nbsp; &nbsp; 4408.94&nbsp; &nbsp; &nbsp; 3880.68<br>------------------------------------------------------------------<br>sendrecv:<br>&nbsp; &nbsp; NP256&nbsp; &nbsp;
 OK<br>------------------------------------------------------------------<br>gather:<br>&nbsp; &nbsp; NP256&nbsp; &nbsp; hangs<br>&nbsp; &nbsp; NP128&nbsp; &nbsp; hangs<br>&nbsp; &nbsp; NP64&nbsp; &nbsp; hangs<br>&nbsp; &nbsp; NP32&nbsp; &nbsp; OK<br><br>Note: "gather" always hangs at the following line of the test:<br>&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;#bytes #repetitions&nbsp; t_min[usec]&nbsp; t_max[usec]&nbsp; t_avg[usec]<br>[snip]<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;4096&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;1000&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;525.80&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;527.69&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;526.79<br>------------------------------------------------------------------<br>exchange:<br>&nbsp; &nbsp; NP256&nbsp; &nbsp; hangs<br>&nbsp; &nbsp; NP128&nbsp; &nbsp; OK<br><br>Note: "exchange" always hangs at:<br><br>#bytes #repetitions&nbsp; t_min[usec]&nbsp; t_max[usec]&nbsp; t_avg[usec]&nbsp;&nbsp;&nbsp;Mbytes/sec<br>8192&nbsp;
 &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;1000&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;109.65&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;110.79&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;110.37&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;282.08<br>------------------------------------------------------------------<br><br>Note: I kept the --host string the same (all 32 servers) and just<br>changed the NPMIN. Just in case this matters for how the procs are<br>mapped out<br>_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="/mc/compose?to=users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></blockquote></td></tr></table><br>



      &nbsp;
