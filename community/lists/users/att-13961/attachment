<table cellspacing="0" cellpadding="0" border="0" ><tr><td valign="top" style="font: inherit;">Sure, but broadcasts are faster - less reliable apparently, but much faster for large clusters.&nbsp; Jeff says that all OpenMPI calls are implemented with point to point B-tree style communications of log N transmissions<br>So I guess that altoall would be N log N<br><br>--- On <b>Wed, 11/8/10, Terry Frankcombe <i>&lt;terry@chem.gu.se&gt;</i></b> wrote:<br><blockquote style="border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;"><br>From: Terry Frankcombe &lt;terry@chem.gu.se&gt;<br>Subject: Re: [OMPI users] MPI_Bcast issue<br>To: "Open MPI Users" &lt;users@open-mpi.org&gt;<br>Received: Wednesday, 11 August, 2010, 1:57 PM<br><br><div class="plainMail">On Tue, 2010-08-10 at 19:09 -0700, Randolph Pullen wrote:<br>&gt; Jeff thanks for the clarification,<br>&gt; What I am trying to do is run N concurrent copies of a 1 to N data<br>&gt;
 movement program to affect an N to N solution.<br><br>I'm no MPI guru, nor do I completely understand what you are doing, but<br>isn't this an allgather (or possibly an alltoall)?<br><br><br><br>_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="/mc/compose?to=users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></blockquote></td></tr></table><br>



      &nbsp;
