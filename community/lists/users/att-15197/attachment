<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <meta content="text/html; charset=ISO-8859-1"
      http-equiv="Content-Type">
    <title></title>
  </head>
  <body text="#000000" bgcolor="#ffffff">
    On 12/17/2010 6:43 PM, Sashi Balasingam wrote:
    <blockquote cite="mid:134954.257.qm@web32903.mail.mud.yahoo.com"
      type="cite">
      <table border="0" cellpadding="0" cellspacing="0">
        <tbody>
          <tr>
            <td style="font: inherit;" valign="top">
              <div>Hi,<br>
                I recently started on an MPI-based, 'real-time',
                pipelined-processing application, and the application
                fails due to large time-jitter in sending and receiving
                messages. Here are related info -</div>
              <div>&nbsp;</div>
              <div>1) Platform:<br>
                a) Intel Box: Two Hex-core, Intel Xeon, 2.668 GHz
                (...total of 12 cores), <br>
                b) OS: SUSE Linux Enterprise Server 11 (x86_64) - Kernel
                \r (\l)<br>
                c) MPI Rev: (OpenRTE) 1.4, (...Installed OFED package)<br>
                d) HCA: InfiniBand: Mellanox Technologies MT26428
                [ConnectX IB QDR, PCIe 2.0 5GT/s] (rev a0)</div>
              <div>&nbsp;</div>
              <div>2) Application detail</div>
              <div>&nbsp;</div>
              <div>a) Launching 7 processes, for pipelined processing,
                where each process waits for a message (sizes vary
                between 1 KBytes to 26 KBytes), <br>
                then process the data, and outputs a message (sizes vary
                between 1 KBytes to 26 KBytes), to next process.</div>
              <div>&nbsp;</div>
              <div>b) MPI transport functions used : "MPI_Isend",
                MPI_Irecv, MPI_Test. <br>
                &nbsp;&nbsp; i) For Receiving messages, I first make an MPI_Irecv
                call, followed by a busy-loop on MPI_Test, waiting for
                message<br>
                &nbsp;&nbsp; ii) For Sending message, there is a busy-loop on
                MPI_Test to ensure prior buffer was sent, then use
                MPI_Isend.</div>
              <div>&nbsp;</div>
              <div>c) When the job starts, all these 7 process are put
                in High priority mode ( SCHED_FIFO policy, with priority
                setting of 99). <br>
                The Job entails an input data packet stream (and a
                series of MPI messages), continually at 40 micro-sec
                rate, for a few minutes.&nbsp;&nbsp;&nbsp; </div>
              <div><br>
                3) The Problem:<br>
                Most calls to MPI_Test (...which is non-blocking) takes
                a few micro-sec, but around 10% of the job, it has a
                large jitter, that vary from 1 to 100 odd millisec. This
                causes<br>
                some of the application input queues to fill-up&nbsp; and
                cause a failure.</div>
              <div>&nbsp;</div>
              <div>Any suggestions to look at on the MPI settings or OS
                config/issues will be much appreciated.</div>
              <div>&nbsp;</div>
            </td>
          </tr>
        </tbody>
      </table>
    </blockquote>
    I didn't see anything there about your -mca affinity settings.&nbsp; Even
    if the defaults don't choose optimum mapping, it's way better than
    allowing them to float as you would with multiple independent jobs
    running.<br>
    <pre class="moz-signature" cols="72">-- 
Tim Prince</pre>
  </body>
</html>

