<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">No idea what is going on here. No MPI call is implemented as a multicast - it all flows over the MPI pt-2-pt system via one of the various algorithms.<div><br></div><div>Best guess I can offer is that there is a race condition in your program that you are tripping when other procs that share the node change the timing.</div><div><br></div><div>How did you configure OMPI when you built it?</div><div><br></div><div><br><div><div>On Aug 8, 2010, at 11:02 PM, Randolph Pullen wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><table cellspacing="0" cellpadding="0" border="0"><tbody><tr><td valign="top" style="font: inherit;">The only MPI calls I am using are these (grep-ed from my code):<br><br>MPI_Abort(MPI_COMM_WORLD, 1);<br>MPI_Barrier(MPI_COMM_WORLD);<br>MPI_Bcast(&amp;bufarray[0].hdr, sizeof(BD_CHDR), MPI_CHAR, 0, MPI_COMM_WORLD);<br>MPI_Comm_rank(MPI_COMM_WORLD,&amp;myid);<br>MPI_Comm_size(MPI_COMM_WORLD,&amp;numprocs); <br>MPI_Finalize();<br>MPI_Init(&amp;argc, &amp;argv);<br>MPI_Irecv(<br>MPI_Isend(<br>MPI_Recv(buff, BUFSIZE, MPI_CHAR, 0, TAG, MPI_COMM_WORLD, &amp;stat);<br>MPI_Send(buff, BUFSIZE, MPI_CHAR, 0, TAG, MPI_COMM_WORLD);<br>MPI_Test(&amp;request, &amp;complete, &amp;status);<br>MPI_Wait(&amp;request, &amp;status);&nbsp; <br><br>The big wait happens on receipt of a bcast call that would otherwise work.<br>Its a bit mysterious really...<br><br>I presume that bcast is implemented with multicast calls but does it use any actual broadcast calls at all?&nbsp; <br>I
 know I'm scraping the edges here looking for something but I just cant get my head around why it should fail where it has.<br><br>--- On <b>Mon, 9/8/10, Ralph Castain <i>&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;</i></b> wrote:<br><blockquote style="border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;"><br>From: Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;<br>Subject: Re: [OMPI users] MPI_Bcast issue<br>To: "Open MPI Users" &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Received: Monday, 9 August, 2010, 1:32 PM<br><br><div id="yiv323045594">Hi Randolph<div><br></div><div>Unless your code is doing a connect/accept between the copies, there is no way they can cross-communicate. As you note, mpirun instances are completely isolated from each other - no process in one instance can possibly receive information from a process in another instance because it lacks all knowledge of it -unless- they wireup into a greater communicator by performing connect/accept calls between
 them.</div><div><br></div><div>I suspect you are inadvertently doing just that - perhaps by doing connect/accept in a tree-like manner, not realizing that the end result is one giant communicator that now links together all the N servers.</div><div><br></div><div>Otherwise, there is no possible way an MPI_Bcast in one mpirun can collide or otherwise communicate with an MPI_Bcast between processes started by another mpirun.</div><div><br></div><div><br></div><div><br><div><div>On Aug 8, 2010, at 7:13 PM, Randolph Pullen wrote:</div><br class="yiv323045594Apple-interchange-newline"><blockquote type="cite"><table style="" border="0" cellpadding="0" cellspacing="0"><tbody><tr><td style="font-family: inherit; font-style: inherit; font-variant: inherit; font-weight: inherit; font-size: inherit; line-height: inherit; font-size-adjust: inherit; font-stretch: inherit; -x-system-font: none;" valign="top">Thanks,&nbsp; although “An intercommunicator cannot be
 used for collective communication.” i.e ,&nbsp; bcast calls., I can see how the MPI_Group_xx calls can be used to produce a useful group and then communicator;&nbsp; - thanks again but this is really the side issue to my main question about MPI_Bcast.<br><br>I seem to have duplicate concurrent processes interfering with each other.&nbsp; This would appear to be a breach of the MPI safety dictum, ie MPI_COMM_WORD is supposed to only include the processes started by a single mpirun command and isolate these processes from other similar groups of processes safely.<br><br>So, it would appear to be a bug.&nbsp; If so this has significant implications for environments such as mine, where it may often occur that the same program is run by different users simultaneously.&nbsp; <br><br>It is really this issue
 that it concerning me, I can rewrite the code but if it can crash when 2 copies run at the same time, I have a much bigger problem.<br><br>My suspicion is that a within the MPI_Bcast handshaking, a syncronising broadcast call may be colliding across the environments.&nbsp; My only evidence is an otherwise working program waits on broadcast reception forever when two or more copies are run at [exactly] the same time.<br><br>Has anyone else seen similar behavior in concurrently running programs that perform lots of broadcasts perhaps?<br><br>Randolph<br><br><br>--- On <b>Sun, 8/8/10, David Zhang <i>&lt;<a rel="nofollow" ymailto="mailto:solarbikedz@gmail.com" target="_blank" href="x-msg://63/mc/compose?to=solarbikedz@gmail.com">solarbikedz@gmail.com</a>&gt;</i></b> wrote:<br><blockquote style="border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;"><br>From: David Zhang &lt;<a rel="nofollow" ymailto="mailto:solarbikedz@gmail.com" target="_blank" href="x-msg://63/mc/compose?to=solarbikedz@gmail.com">solarbikedz@gmail.com</a>&gt;<br>Subject: Re: [OMPI users] MPI_Bcast issue<br>To: "Open MPI Users" &lt;<a rel="nofollow" ymailto="mailto:users@open-mpi.org" target="_blank" href="x-msg://63/mc/compose?to=users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Received: Sunday, 8 August, 2010, 12:34 PM<br><br><div class="yiv323045594plainMail">In particular, intercommunicators<br><br>On 8/7/10, Aurélien Bouteiller &lt;<a rel="nofollow">bouteill@eecs.utk.edu</a>&gt; wrote:<br>&gt; You should consider reading about communicators in MPI.<br>&gt;<br>&gt; Aurelien<br>&gt; --<br>&gt; Aurelien Bouteiller, Ph.D.<br>&gt; Innovative Computing Laboratory, The University of Tennessee.<br>&gt;<br>&gt; Envoyé de mon iPad<br>&gt;<br>&gt; Le Aug 7, 2010 à 1:05, Randolph Pullen &lt;<a rel="nofollow">randolph_pullen@yahoo.com.au</a>&gt; a<br>&gt; écrit :<br>&gt;<br>&gt;&gt; I seem to be having a problem with
 MPI_Bcast.<br>&gt;&gt; My massive I/O intensive data movement program must broadcast from n to n<br>&gt;&gt; nodes. My problem starts because I require 2 processes per node, a sender<br>&gt;&gt; and a receiver and I have implemented
 these using MPI processes rather<br>&gt;&gt; than tackle the complexities of threads on MPI.<br>&gt;&gt;<br>&gt;&gt; Consequently, broadcast and calls like alltoall are not completely<br>&gt;&gt; helpful.&nbsp; The dataset is huge and each node must end up with a complete<br>&gt;&gt; copy built by the large number of contributing broadcasts from the sending<br>&gt;&gt; nodes.&nbsp; Network efficiency and run time are paramount.<br>&gt;&gt;<br>&gt;&gt; As I don’t want to needlessly broadcast all this data to the sending nodes<br>&gt;&gt; and I have a perfectly good MPI program that distributes globally from a<br>&gt;&gt; single node (1 to N), I took the unusual decision to start N copies of<br>&gt;&gt; this program by spawning the MPI system from the PVM system in an effort<br>&gt;&gt; to get my N to N concurrent transfers.<br>&gt;&gt;<br>&gt;&gt; It seems that the broadcasts running on concurrent MPI environments<br>&gt;&gt; collide and cause all but
 the first process to hang waiting for their<br>&gt;&gt; broadcasts.&nbsp; This theory seems to be confirmed by introducing a sleep of<br>&gt;&gt; n-1 seconds before the first MPI_Bcast&nbsp; call on each node, which results<br>&gt;&gt; in the code working perfectly.&nbsp; (total run time 55 seconds, 3 nodes,<br>&gt;&gt; standard TCP stack)<br>&gt;&gt;<br>&gt;&gt; My guess is that unlike PVM, OpenMPI implements broadcasts with broadcasts<br>&gt;&gt; rather than multicasts.&nbsp; Can someone confirm this?&nbsp; Is this a bug?<br>&gt;&gt;<br>&gt;&gt; Is there any multicast or N to N broadcast where sender processes can<br>&gt;&gt; avoid participating when they don’t need to?<br>&gt;&gt;<br>&gt;&gt; Thanks in advance<br>&gt;&gt; Randolph<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt; _______________________________________________<br>&gt;&gt; users mailing list<br>&gt;&gt; <a rel="nofollow">users@open-mpi.org</a><br>&gt;&gt; <a rel="nofollow" target="_blank" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<br><br>-- <br>Sent from my mobile device<br><br>David Zhang<br>University of California, San Diego<br><br>_______________________________________________<br>users mailing list<br><a rel="nofollow">users@open-mpi.org</a><br><a rel="nofollow" target="_blank" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></blockquote></td></tr></tbody></table><br>



      &nbsp;_______________________________________________<br>users mailing list<br><a rel="nofollow" ymailto="mailto:users@open-mpi.org" target="_blank" href="x-msg://63/mc/compose?to=users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote></div><br></div></div><br>-----Inline Attachment Follows-----<br><br><div class="plainMail">_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="x-msg://63/mc/compose?to=users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></blockquote></td></tr></tbody></table><br>



      &nbsp;_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
