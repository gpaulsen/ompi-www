<tt><font size=2>users-bounces@open-mpi.org a écrit sur 19/04/2012 12:42:44
:<br>
<br>
&gt; De : Rohan Deshpande &lt;rohand87@gmail.com&gt;</font></tt>
<br><tt><font size=2>&gt; A : Open MPI Users &lt;users@open-mpi.org&gt;</font></tt>
<br><tt><font size=2>&gt; Date : 19/04/2012 12:44</font></tt>
<br><tt><font size=2>&gt; Objet : Re: [OMPI users] machine exited on signal
11 (Segmentation fault).</font></tt>
<br><tt><font size=2>&gt; Envoyé par : users-bounces@open-mpi.org</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; No I havent tried using valgrind.&nbsp;</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; here is the latest code -&nbsp;</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; #include &quot;mpi.h&quot;</font></tt>
<br><tt><font size=2>&gt; #include &lt;stdio.h&gt;</font></tt>
<br><tt><font size=2>&gt; #include &lt;stdlib.h&gt;</font></tt>
<br><tt><font size=2>&gt; #include &lt;string.h&gt;</font></tt>
<br><tt><font size=2>&gt; #define &nbsp;ARRAYSIZE 200000</font></tt>
<br><tt><font size=2>&gt; #define &nbsp;MASTER 0</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; int &nbsp;*data;</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; int main(int argc, char* argv[])</font></tt>
<br><tt><font size=2>&gt; {</font></tt>
<br><tt><font size=2>&gt; int &nbsp; numtasks, taskid, rc, dest, offset,
i, j, tag1, tag2, source, <br>
&gt; chunksize, namelen;&nbsp;</font></tt>
<br><tt><font size=2>&gt; int mysum, sum;</font></tt>
<br><tt><font size=2>&gt; int update(int myoffset, int chunk, int myid);</font></tt>
<br><tt><font size=2>&gt; char myname[MPI_MAX_PROCESSOR_NAME];</font></tt>
<br><tt><font size=2>&gt; MPI_Status status;</font></tt>
<br><tt><font size=2>&gt; double start, stop, time;</font></tt>
<br><tt><font size=2>&gt; double totaltime;</font></tt>
<br><tt><font size=2>&gt; FILE *fp;</font></tt>
<br><tt><font size=2>&gt; char line[128];</font></tt>
<br><tt><font size=2>&gt; char element;</font></tt>
<br><tt><font size=2>&gt; int n;</font></tt>
<br><tt><font size=2>&gt; int k=0;</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; /***** Initializations *****/</font></tt>
<br><tt><font size=2>&gt; MPI_Init(&amp;argc, &amp;argv);</font></tt>
<br><tt><font size=2>&gt; MPI_Comm_size(MPI_COMM_WORLD, &amp;numtasks);</font></tt>
<br><tt><font size=2>&gt; if (numtasks % 4 != 0) {</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp;printf(&quot;Quitting. Number of
MPI tasks must be divisible by 4.\n&quot;);</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp;MPI_Abort(MPI_COMM_WORLD, rc);</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp;MPI_Finalize();</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp;}</font></tt>
<br><tt><font size=2>&gt; MPI_Comm_rank(MPI_COMM_WORLD,&amp;taskid);&nbsp;</font></tt>
<br><tt><font size=2>&gt; MPI_Get_processor_name(myname, &amp;namelen);</font></tt>
<br><tt><font size=2>&gt; printf (&quot;MPI task %d has started on host
%s...\n&quot;, taskid, myname);</font></tt>
<br><tt><font size=2>&gt; chunksize = (ARRAYSIZE / numtasks);</font></tt>
<br><tt><font size=2>&gt; tag2 = 1;</font></tt>
<br><tt><font size=2>&gt; tag1 = 2;</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; /***** Master task only ******/</font></tt>
<br><tt><font size=2>&gt; if (taskid == MASTER){</font></tt>
<br><tt><font size=2>&gt; &nbsp;&nbsp;</font></tt>
<br><tt><font size=2>&gt; &nbsp;</font></tt>
<br><tt><font size=2>&gt; &nbsp; /* Initialize the array */</font></tt>
<br><tt><font size=2>&gt; &nbsp; data = malloc(ARRAYSIZE * sizeof(int));</font></tt>
<br><tt><font size=2>&gt; &nbsp; if(NULL == data){</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; printf(&quot;Data null&quot;);</font></tt>
<br><tt><font size=2>&gt; &nbsp; }</font></tt>
<br>
<br>
<br><tt><font size=2>==&gt; This array is used in the update subroutine
that is called by all processes</font></tt>
<br><tt><font size=2>So this initialisation should be done in the common
code.</font></tt>
<br>
<br>
<br><tt><font size=2>&gt; &nbsp; sum = 0;</font></tt>
<br><tt><font size=2>&gt; &nbsp; for(i=0; i&lt;ARRAYSIZE; i++) {</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; data[i] = &nbsp;i * 1 ;</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; sum = sum + data[i];</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; }</font></tt>
<br><tt><font size=2>&gt; &nbsp; printf(&quot;Initialized array sum = %d\n&quot;,sum);</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; &nbsp; /* Send each task its portion of the array - master keeps 1st
part */</font></tt>
<br><tt><font size=2>&gt; &nbsp; offset = chunksize;</font></tt>
<br><tt><font size=2>&gt; &nbsp; for (dest=1; dest&lt;numtasks; dest++)
{</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; MPI_Send(&amp;offset, 1, MPI_INT,
dest, tag1, MPI_COMM_WORLD);</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; MPI_Send(&amp;data[offset], chunksize,
MPI_INT, dest, tag2, MPI_COMM_WORLD);</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; printf(&quot;Sent %d elements to
task %d offset= %d\n&quot;,chunksize,dest,offset);</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; offset = offset + chunksize;</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; }</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; &nbsp; /* Master does its part of the work */</font></tt>
<br><tt><font size=2>&gt; &nbsp; offset = 0;</font></tt>
<br><tt><font size=2>&gt; &nbsp; mysum = update(offset, chunksize, taskid);</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; &nbsp; /* Wait to receive results from each task */</font></tt>
<br><tt><font size=2>&gt; &nbsp; for (i=1; i&lt;numtasks; i++) {</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; source = i;</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; MPI_Recv(&amp;offset, 1, MPI_INT,
source, tag1, MPI_COMM_WORLD, &amp;status);</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; MPI_Recv(&amp;data[offset], chunksize,
MPI_INT, source, tag2,</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; &nbsp; MPI_COMM_WORLD, &amp;status);</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; }</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; &nbsp; /* Get final sum and print sample results */ &nbsp;</font></tt>
<br><tt><font size=2>&gt; &nbsp; MPI_Reduce(&amp;mysum, &amp;sum, 1, MPI_INT,
MPI_SUM, MASTER, MPI_COMM_WORLD);</font></tt>
<br><tt><font size=2>&gt; &nbsp;/* printf(&quot;Sample results: \n&quot;);</font></tt>
<br><tt><font size=2>&gt; &nbsp; offset = 0;</font></tt>
<br><tt><font size=2>&gt; &nbsp; for (i=0; i&lt;numtasks; i++) {</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; for (j=0; j&lt;5; j++)&nbsp;</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; &nbsp; printf(&quot; &nbsp;%d&quot;,data[offset+j]);ARRAYSIZE</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; printf(&quot;\n&quot;);</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; offset = offset + chunksize;</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; }*/</font></tt>
<br><tt><font size=2>&gt; &nbsp; printf(&quot;\n*** Final sum= %d ***\n&quot;,sum);</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; &nbsp; } &nbsp;/* end of master section */</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; /***** Non-master tasks only *****/</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; if (taskid &gt; MASTER) {</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; &nbsp; /* Receive my portion of array from the master task */</font></tt>
<br><tt><font size=2>&gt; &nbsp; start= MPI_Wtime();</font></tt>
<br><tt><font size=2>&gt; &nbsp; source = MASTER;</font></tt>
<br><tt><font size=2>&gt; &nbsp; MPI_Recv(&amp;offset, 1, MPI_INT, source,
tag1, MPI_COMM_WORLD, &amp;status);</font></tt>
<br><tt><font size=2>&gt; &nbsp; MPI_Recv(&amp;data[offset], chunksize,
MPI_INT, source, <br>
&gt; tag2,MPI_COMM_WORLD, &amp;status);</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; &nbsp; mysum = update(offset, chunksize, taskid);</font></tt>
<br><tt><font size=2>&gt; &nbsp; stop = MPI_Wtime();</font></tt>
<br><tt><font size=2>&gt; &nbsp; time = stop -start;</font></tt>
<br><tt><font size=2>&gt; &nbsp; printf(&quot;time taken by process %d
to recieve elements and caluclate<br>
&gt; own sum is = %lf seconds \n&quot;, taskid, time);</font></tt>
<br><tt><font size=2>&gt; &nbsp; totaltime = totaltime + time;</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; &nbsp; /* Send my results back to the master task */</font></tt>
<br><tt><font size=2>&gt; &nbsp; dest = MASTER;</font></tt>
<br><tt><font size=2>&gt; &nbsp; MPI_Send(&amp;offset, 1, MPI_INT, dest,
tag1, MPI_COMM_WORLD);</font></tt>
<br><tt><font size=2>&gt; &nbsp; MPI_Send(&amp;data[offset], chunksize,
MPI_INT, MASTER, tag2, MPI_COMM_WORLD);</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; &nbsp; MPI_Reduce(&amp;mysum, &amp;sum, 1, MPI_INT, MPI_SUM, MASTER,
MPI_COMM_WORLD);</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; &nbsp; } /* end of non-master */</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; // printf(&quot;Total time taken for distribution is - &nbsp;%lf &nbsp;seconds&quot;,
totaltime);</font></tt>
<br><tt><font size=2>&gt; MPI_Finalize();</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; } &nbsp; /* end of main */</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; int update(int myoffset, int chunk, int myid) {</font></tt>
<br><tt><font size=2>&gt; &nbsp; int i,j;&nbsp;</font></tt>
<br><tt><font size=2>&gt; &nbsp; int mysum;</font></tt>
<br><tt><font size=2>&gt; &nbsp; int mydata[myoffset+chunk];</font></tt>
<br><tt><font size=2>&gt; &nbsp; /* Perform addition to each of my array
elements and keep my sum */</font></tt>
<br><tt><font size=2>&gt; &nbsp; mysum = 0;</font></tt>
<br><tt><font size=2>&gt; &nbsp;/* &nbsp;printf(&quot;task %d has elements:&quot;,myid);</font></tt>
<br><tt><font size=2>&gt; &nbsp; for(j = myoffset; j&lt;myoffset+chunk;
j++){</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; &nbsp; printf(&quot;\t%d&quot;,
data[j]);</font></tt>
<br><tt><font size=2>&gt; &nbsp; }</font></tt>
<br><tt><font size=2>&gt; &nbsp;printf(&quot;\n&quot;);*/</font></tt>
<br><tt><font size=2>&gt; &nbsp; for(i=myoffset; i &lt; myoffset + chunk;
i++) {</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp;&nbsp;</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; //data[i] = data[i] + i;</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; mysum = mysum + data[i];</font></tt>
<br><tt><font size=2>&gt; &nbsp; &nbsp; }</font></tt>
<br><tt><font size=2>&gt; &nbsp; printf(&quot;Task %d has sum = %d\n&quot;,myid,mysum);</font></tt>
<br><tt><font size=2>&gt; &nbsp; return(mysum);</font></tt>
<br><tt><font size=2>&gt; }</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; On Thu, Apr 19, 2012 at 5:59 PM, Jeffrey Squyres &lt;jsquyres@cisco.com&gt;
wrote:</font></tt>
<br><tt><font size=2>&gt; Send the most recent version of your code.<br>
&gt; <br>
&gt; Have you tried running it through a memory-checking debugger, like
valgrind?</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; <br>
&gt; On Apr 19, 2012, at 4:24 AM, Rohan Deshpande wrote:<br>
&gt; <br>
&gt; &gt; Hi Pascal,<br>
&gt; &gt;<br>
&gt; &gt; The offset is received from the master task. so no need to <br>
&gt; initialize for non master tasks?<br>
&gt; &gt;<br>
&gt; &gt; not sure what u meant exactly.<br>
&gt; &gt;<br>
&gt; &gt; Thanks<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; On Thu, Apr 19, 2012 at 3:36 PM, &lt;pascal.deveze@bull.net&gt;
wrote:<br>
&gt; &gt; I do not see where you initialize the offset on the &quot;Non-master
<br>
&gt; tasks&quot;. This could be the problem.<br>
&gt; &gt;<br>
&gt; &gt; Pascal<br>
&gt; &gt;<br>
&gt; &gt; users-bounces@open-mpi.org a écrit sur 19/04/2012 09:18:31 :<br>
&gt; &gt;<br>
&gt; &gt; &gt; De : Rohan Deshpande &lt;rohand87@gmail.com&gt;<br>
&gt; &gt; &gt; A : Open MPI Users &lt;users@open-mpi.org&gt;<br>
&gt; &gt; &gt; Date : 19/04/2012 09:18<br>
&gt; &gt; &gt; Objet : Re: [OMPI users] machine exited on signal 11 (Segmentation
fault).<br>
&gt; &gt; &gt; Envoyé par : users-bounces@open-mpi.org<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; Hi Jeffy,<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; I checked the SEND RECV buffers and it looks ok to me. The
code I<br>
&gt; &gt; &gt; have sent works only when I statically initialize the array.<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; The code fails everytime I use malloc to initialize the
array.<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; Can you please look at code and let me know what is wrong.<br>
&gt; &gt;<br>
&gt; &gt; &gt; On Wed, Apr 18, 2012 at 8:11 PM, Jeffrey Squyres &lt;jsquyres@cisco.com<br>
&gt; &gt; wrote:<br>
&gt; &gt; &gt; As a guess, you're passing in a bad address.<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; Double check the buffers that you're sending to MPI_SEND/MPI_RECV/etc.<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; On Apr 17, 2012, at 10:43 PM, Rohan Deshpande wrote:<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; After using malloc i am getting following error<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &nbsp;*** Process received signal ***<br>
&gt; &gt; &gt; &gt; &nbsp;Signal: Segmentation fault (11)<br>
&gt; &gt; &gt; &gt; &nbsp;Signal code: Address not mapped (1)<br>
&gt; &gt; &gt; &gt; Failing at address: 0x1312d08<br>
&gt; &gt; &gt; &gt; &nbsp;[ 0] [0x5e840c]<br>
&gt; &gt; &gt; &gt; [ 1] /usr/local/lib/openmpi/mca_btl_tcp.so(+0x5bdb)
[0x119bdb]<br>
&gt; &gt; &gt; &gt; &nbsp;/usr/local/lib/libopen-pal.so.0(+0x19ce0) [0xb2cce0]<br>
&gt; &gt; &gt; &gt; &nbsp;/usr/local/lib/libopen-pal.so.0(opal_event_loop+0x27)
[0xb2cf47]<br>
&gt; &gt; &gt; &gt; &nbsp;/usr/local/lib/libopen-pal.so.0(opal_progress+0xda)
[0xb200ba]<br>
&gt; &gt; &gt; &gt; &nbsp;/usr/local/lib/openmpi/mca_pml_ob1.so(+0x3f75)
[0xa9ef75]<br>
&gt; &gt; &gt; &gt; &nbsp;[ 6] /usr/local/lib/libmpi.so.0(MPI_Recv+0x136)
[0xea7c46]<br>
&gt; &gt; &gt; &gt; &nbsp;[ 7] mpi_array(main+0x501) [0x8048e25]<br>
&gt; &gt; &gt; &gt; [ 8] /lib/libc.so.6(__libc_start_main+0xe6) [0x2fece6]<br>
&gt; &gt; &gt; &gt; &nbsp;[ 9] mpi_array() [0x8048891]<br>
&gt; &gt; &gt; &gt; &nbsp;*** End of error message ***<br>
&gt; &gt; &gt; &gt; [machine4][[3968,1],0][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]<br>
&gt; &gt; &gt; mca_btl_tcp_frag_recv: readv failed: Connection reset by
peer (104)<br>
&gt; &gt; &gt; &gt; <br>
&gt; --------------------------------------------------------------------------<br>
&gt; &gt; &gt; &gt; mpirun noticed that process rank 1 with PID 2936 on
node machine4<br>
&gt; &gt; &gt; exited on signal 11 (Segmentation fault).<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; Can someone help please.<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; Thanks<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; On Tue, Apr 17, 2012 at 6:01 PM, Jeffrey Squyres &lt;jsquyres@cisco.com<br>
&gt; &gt; wrote:<br>
&gt; &gt; &gt; &gt; Try malloc'ing your array instead of creating it statically
on the<br>
&gt; &gt; &gt; stack. &nbsp;Something like:<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; int *data;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; int main(..) {<br>
&gt; &gt; &gt; &gt; {<br>
&gt; &gt; &gt; &gt; &nbsp; &nbsp;data = malloc(ARRAYSIZE * sizeof(int));<br>
&gt; &gt; &gt; &gt; &nbsp; &nbsp;if (NULL == data) {<br>
&gt; &gt; &gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp;perror(&quot;malloc&quot;);<br>
&gt; &gt; &gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp;exit(1);<br>
&gt; &gt; &gt; &gt; &nbsp; &nbsp;}<br>
&gt; &gt; &gt; &gt; &nbsp; &nbsp;// ...<br>
&gt; &gt; &gt; &gt; }<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; On Apr 17, 2012, at 5:05 AM, Rohan Deshpande wrote:<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; Hi,<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; I am trying to distribute large amount of data
using MPI.<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; When I exceed the certain data size the segmentation
fault occurs.<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; Here is my code,<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; #include &quot;mpi.h&quot;<br>
&gt; &gt; &gt; &gt; &gt; #include &lt;stdio.h&gt;<br>
&gt; &gt; &gt; &gt; &gt; #include &lt;stdlib.h&gt;<br>
&gt; &gt; &gt; &gt; &gt; #include &lt;string.h&gt;<br>
&gt; &gt; &gt; &gt; &gt; #define &nbsp;ARRAYSIZE &nbsp; &nbsp;2000000<br>
&gt; &gt; &gt; &gt; &gt; #define &nbsp;MASTER &nbsp; &nbsp; &nbsp; &nbsp;0<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; int &nbsp;data[ARRAYSIZE];<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; int main(int argc, char* argv[])<br>
&gt; &gt; &gt; &gt; &gt; {<br>
&gt; &gt; &gt; &gt; &gt; int &nbsp; numtasks, taskid, rc, dest, offset,
i, j, tag1, tag2,<br>
&gt; &gt; &gt; source, chunksize, namelen;<br>
&gt; &gt; &gt; &gt; &gt; int mysum, sum;<br>
&gt; &gt; &gt; &gt; &gt; int update(int myoffset, int chunk, int myid);<br>
&gt; &gt; &gt; &gt; &gt; char myname[MPI_MAX_PROCESSOR_NAME];<br>
&gt; &gt; &gt; &gt; &gt; MPI_Status status;<br>
&gt; &gt; &gt; &gt; &gt; double start, stop, time;<br>
&gt; &gt; &gt; &gt; &gt; double totaltime;<br>
&gt; &gt; &gt; &gt; &gt; FILE *fp;<br>
&gt; &gt; &gt; &gt; &gt; char line[128];<br>
&gt; &gt; &gt; &gt; &gt; char element;<br>
&gt; &gt; &gt; &gt; &gt; int n;<br>
&gt; &gt; &gt; &gt; &gt; int k=0;<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; /***** Initializations *****/<br>
&gt; &gt; &gt; &gt; &gt; MPI_Init(&amp;argc, &amp;argv);<br>
&gt; &gt; &gt; &gt; &gt; MPI_Comm_size(MPI_COMM_WORLD, &amp;numtasks);<br>
&gt; &gt; &gt; &gt; &gt; MPI_Comm_rank(MPI_COMM_WORLD,&amp;taskid);<br>
&gt; &gt; &gt; &gt; &gt; MPI_Get_processor_name(myname, &amp;namelen);<br>
&gt; &gt; &gt; &gt; &gt; printf (&quot;MPI task %d has started on host
%s...\n&quot;, taskid, myname);<br>
&gt; &gt; &gt; &gt; &gt; chunksize = (ARRAYSIZE / numtasks);<br>
&gt; &gt; &gt; &gt; &gt; tag2 = 1;<br>
&gt; &gt; &gt; &gt; &gt; tag1 = 2;<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; /***** Master task only ******/<br>
&gt; &gt; &gt; &gt; &gt; if (taskid == MASTER){<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp;/* Initialize the array */<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; sum = 0;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; for(i=0; i&lt;ARRAYSIZE; i++) {<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; data[i] = &nbsp;i * 1 ;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; sum = sum + data[i];<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; }<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; printf(&quot;Initialized array sum = %d\n&quot;,sum);<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; /* Send each task its portion of the array
- master keeps <br>
&gt; 1st part */<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; offset = chunksize;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; for (dest=1; dest&lt;numtasks; dest++)
{<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; MPI_Send(&amp;offset, 1, MPI_INT,
dest, tag1, MPI_COMM_WORLD);<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; MPI_Send(&amp;data[offset], chunksize,
MPI_INT, dest, tag2,<br>
&gt; &gt; &gt; MPI_COMM_WORLD);<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; printf(&quot;Sent %d elements to
task %d offset= %d<br>
&gt; &gt; &gt; \n&quot;,chunksize,dest,offset);<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; offset = offset + chunksize;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; }<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; /* Master does its part of the work */<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; offset = 0;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; mysum = update(offset, chunksize, taskid);<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; /* Wait to receive results from each task
*/<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; for (i=1; i&lt;numtasks; i++) {<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; source = i;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; MPI_Recv(&amp;offset, 1, MPI_INT,
source, tag1, <br>
&gt; MPI_COMM_WORLD, &amp;status);<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; MPI_Recv(&amp;data[offset], chunksize,
MPI_INT, source, tag2,<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; &nbsp; MPI_COMM_WORLD, &amp;status);<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; }<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; /* Get final sum and print sample results
*/<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; MPI_Reduce(&amp;mysum, &amp;sum, 1, MPI_INT,
MPI_SUM, MASTER, <br>
&gt; MPI_COMM_WORLD);<br>
&gt; &gt; &gt; &gt; &gt; &nbsp;/* printf(&quot;Sample results: \n&quot;);<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; offset = 0;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; for (i=0; i&lt;numtasks; i++) {<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; for (j=0; j&lt;5; j++)<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; &nbsp; printf(&quot; &nbsp;%d&quot;,data[offset+j]);ARRAYSIZE<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; printf(&quot;\n&quot;);<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; offset = offset + chunksize;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; }*/<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; printf(&quot;\n*** Final sum= %d ***\n&quot;,sum);<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; } &nbsp;/* end of master section */<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; #include &lt;stdlib.h&gt;<br>
&gt; &gt; &gt; &gt; &gt; /***** Non-master tasks only *****/<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; if (taskid &gt; MASTER) {<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; /* Receive my portion of array from the
master task */<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; start= MPI_Wtime();<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; source = MASTER;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; MPI_Recv(&amp;offset, 1, MPI_INT, source,
tag1, <br>
&gt; MPI_COMM_WORLD, &amp;status);<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; MPI_Recv(&amp;data[offset], chunksize,
MPI_INT, source,<br>
&gt; &gt; &gt; tag2,MPI_COMM_WORLD, &amp;status);<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; mysum = update(offset, chunksize, taskid);<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; stop = MPI_Wtime();<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; time = stop -start;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; printf(&quot;time taken by process %d to
recieve elements and<br>
&gt; &gt; &gt; caluclate own sum is = %lf seconds \n&quot;, taskid, time);<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; totaltime = totaltime + time;<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; /* Send my results back to the master task
*/<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; dest = MASTER;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; MPI_Send(&amp;offset, 1, MPI_INT, dest,
tag1, MPI_COMM_WORLD);<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; MPI_Send(&amp;data[offset], chunksize,
MPI_INT, MASTER, tag2,<br>
&gt; &gt; &gt; MPI_COMM_WORLD);<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; MPI_Reduce(&amp;mysum, &amp;sum, 1, MPI_INT,
MPI_SUM, MASTER, <br>
&gt; MPI_COMM_WORLD);<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; } /* end of non-master */<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; // printf(&quot;Total time taken for distribution
is - &nbsp;%lf<br>
&gt; &gt; &gt; &nbsp;seconds&quot;, totaltime);<br>
&gt; &gt; &gt; &gt; &gt; MPI_Finalize();<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; } &nbsp; /* end of main */<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; int update(int myoffset, int chunk, int myid)
{<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; int i,j;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; int mysum;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; int mydata[myoffset+chunk];<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; /* Perform addition to each of my array
elements and keep my sum */<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; mysum = 0;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp;/* &nbsp;printf(&quot;task %d has elements:&quot;,myid);<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; for(j = myoffset; j&lt;myoffset+chunk;
j++){<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; &nbsp; printf(&quot;\t%d&quot;,
data[j]);<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; }<br>
&gt; &gt; &gt; &gt; &gt; &nbsp;printf(&quot;\n&quot;);*/<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; for(i=myoffset; i &lt; myoffset + chunk;
i++) {<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; //data[i] = data[i] + i;<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; mysum = mysum + data[i];<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; &nbsp; }<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; printf(&quot;Task %d has sum = %d\n&quot;,myid,mysum);<br>
&gt; &gt; &gt; &gt; &gt; &nbsp; return(mysum);<br>
&gt; &gt; &gt; &gt; &gt; }<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; When I run it with ARRAYSIZE = 2000000 The program
works fine.<br>
&gt; &gt; &gt; But when I increase the size ARRAYSIZE = 20000000. The program
ends<br>
&gt; &gt; &gt; with segmentation fault.<br>
&gt; &gt; &gt; &gt; &gt; I am running it on a cluster (machine 4 is master,
machine 5,6<br>
&gt; &gt; &gt; are slaves) &nbsp;and np=20<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; MPI task 0 has started on host machine4<br>
&gt; &gt; &gt; &gt; &gt; MPI task 2 has started on host machine4<br>
&gt; &gt; &gt; &gt; &gt; MPI task 3 has started on host machine4<br>
&gt; &gt; &gt; &gt; &gt; MPI task 14 has started on host machine4<br>
&gt; &gt; &gt; &gt; &gt; MPI task 8 has started on host machine6<br>
&gt; &gt; &gt; &gt; &gt; MPI task 10 has started on host machine6<br>
&gt; &gt; &gt; &gt; &gt; MPI task 13 has started on host machine4<br>
&gt; &gt; &gt; &gt; &gt; MPI task 4 has started on host machine5<br>
&gt; &gt; &gt; &gt; &gt; MPI task 6 has started on host machine5<br>
&gt; &gt; &gt; &gt; &gt; MPI task 7 has started on host machine5<br>
&gt; &gt; &gt; &gt; &gt; MPI task 16 has started on host machine5<br>
&gt; &gt; &gt; &gt; &gt; MPI task 11 has started on host machine6<br>
&gt; &gt; &gt; &gt; &gt; MPI task 12 has started on host machine4<br>
&gt; &gt; &gt; &gt; &gt; MPI task 5 has started on hostmachine5<br>
&gt; &gt; &gt; &gt; &gt; MPI task 17 has started on host machine5<br>
&gt; &gt; &gt; &gt; &gt; MPI task 18 has started on host machine5<br>
&gt; &gt; &gt; &gt; &gt; MPI task 15 has started on host machine4<br>
&gt; &gt; &gt; &gt; &gt; MPI task 19 has started on host machine5<br>
&gt; &gt; &gt; &gt; &gt; MPI task 1 has started on host machine4<br>
&gt; &gt; &gt; &gt; &gt; MPI task 9 has started on host machine6<br>
&gt; &gt; &gt; &gt; &gt; Initialized array sum = 542894464<br>
&gt; &gt; &gt; &gt; &gt; Sent 1000000 elements to task 1 offset= 1000000<br>
&gt; &gt; &gt; &gt; &gt; Task 1 has sum = 1055913696<br>
&gt; &gt; &gt; &gt; &gt; time taken by process 1 to recieve elements and
caluclate own<br>
&gt; &gt; &gt; sum is = 0.249345 seconds<br>
&gt; &gt; &gt; &gt; &gt; Sent 1000000 elements to task 2 offset= 2000000<br>
&gt; &gt; &gt; &gt; &gt; Sent 1000000 elements to task 3 offset= 3000000<br>
&gt; &gt; &gt; &gt; &gt; Task 2 has sum = 328533728<br>
&gt; &gt; &gt; &gt; &gt; time taken by process 2 to recieve elements and
caluclate own<br>
&gt; &gt; &gt; sum is = 0.274285 seconds<br>
&gt; &gt; &gt; &gt; &gt; Sent 1000000 elements to task 4 offset= 4000000<br>
&gt; &gt; &gt; &gt; &gt; <br>
&gt; --------------------------------------------------------------------------<br>
&gt; &gt; &gt; &gt; &gt; mpirun noticed that process rank 3 with PID 5695
on node<br>
&gt; &gt; &gt; machine4 exited on signal 11 (Segmentation fault).<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; Any idea what could be wrong here?<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; --<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; Best Regards,<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; ROHAN DESHPANDE<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; _______________________________________________<br>
&gt; &gt; &gt; &gt; &gt; users mailing list<br>
&gt; &gt; &gt; &gt; &gt; users@open-mpi.org<br>
&gt; &gt; &gt; &gt; &gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; --<br>
&gt; &gt; &gt; &gt; Jeff Squyres<br>
&gt; &gt; &gt; &gt; jsquyres@cisco.com<br>
&gt; &gt; &gt; &gt; For corporate legal information go to: </font></tt><a href=http://www.cisco.com/web/><tt><font size=2>http://www.cisco.com/web/</font></tt></a><tt><font size=2><br>
&gt; &gt; &gt; about/doing_business/legal/cri/<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; _______________________________________________<br>
&gt; &gt; &gt; &gt; users mailing list<br>
&gt; &gt; &gt; &gt; users@open-mpi.org<br>
&gt; &gt; &gt; &gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; _______________________________________________<br>
&gt; &gt; &gt; &gt; users mailing list<br>
&gt; &gt; &gt; &gt; users@open-mpi.org<br>
&gt; &gt; &gt; &gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; --<br>
&gt; &gt; &gt; Jeff Squyres<br>
&gt; &gt; &gt; jsquyres@cisco.com<br>
&gt; &gt; &gt; For corporate legal information go to: </font></tt><a href=http://www.cisco.com/web/><tt><font size=2>http://www.cisco.com/web/</font></tt></a><tt><font size=2><br>
&gt; &gt; &gt; about/doing_business/legal/cri/<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; _______________________________________________<br>
&gt; &gt; &gt; users mailing list<br>
&gt; &gt; &gt; users@open-mpi.org<br>
&gt; &gt; &gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; --<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; Best Regards,<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; ROHAN DESHPANDE<br>
&gt; &gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; &gt; _______________________________________________<br>
&gt; &gt; &gt; users mailing list<br>
&gt; &gt; &gt; users@open-mpi.org<br>
&gt; &gt; &gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; users@open-mpi.org<br>
&gt; &gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; --<br>
&gt; &gt;<br>
&gt; &gt; Best Regards,<br>
&gt; &gt;<br>
&gt; &gt; ROHAN DESHPANDE<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; users@open-mpi.org<br>
&gt; &gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
&gt; <br>
&gt; <br>
&gt; --<br>
&gt; Jeff Squyres<br>
&gt; jsquyres@cisco.com<br>
&gt; For corporate legal information go to: </font></tt><a href=http://www.cisco.com/web/><tt><font size=2>http://www.cisco.com/web/</font></tt></a><tt><font size=2><br>
&gt; about/doing_business/legal/cri/<br>
&gt; <br>
&gt; <br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; users@open-mpi.org<br>
&gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a>
<br><tt><font size=2>&gt; <br>
</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; -- </font></tt>
<br><tt><font size=2>&gt; <br>
&gt; Best Regards,</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; ROHAN DESHPANDE &nbsp;</font></tt>
<br><tt><font size=2>&gt; <br>
</font></tt>
<br><tt><font size=2>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; users@open-mpi.org<br>
&gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a>
