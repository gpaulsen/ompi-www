<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta content="text/html;charset=ISO-8859-1" http-equiv="Content-Type">
  <title></title>
</head>
<body bgcolor="#ffffff" text="#000000">
Jeff Squyres wrote:
<blockquote cite="mid:B3E1E7D2-A505-43F6-BED0-9FEB3144A076@cisco.com"
 type="cite">
  <pre wrap="">On Apr 22, 2008, at 9:03 AM, Tomas Ukkonen wrote:
  </pre>
  <blockquote type="cite">
    <pre wrap="">I read from somewhere that OpenMPI supports

some kind of data compression but I couldn't find
any information about it.

Is this true and how it can be used?
    </pre>
  </blockquote>
  <pre wrap="">
Nope, sorry -- not true.

This just came up in a different context, actually.  We added some  
preliminary compression on our startup/mpirun messages and found that  
it really had no effect; any savings that you get in bandwidth (and  
therefore overall wall clock time) are eaten up by the time necessary  
to compress/uncompress the messages.  There were a few more things we  
could have tried, but frankly we had some higher priority items to  
finish for the upcoming v1.3 series.  :-(
  </pre>
</blockquote>
Ok, so I have to do it myself. Not a problem really because <br>
there are only few places where the compression really seems to matter.<br>
<blockquote cite="mid:B3E1E7D2-A505-43F6-BED0-9FEB3144A076@cisco.com"
 type="cite">
  <blockquote type="cite">
    <pre wrap="">Does anyone have any experiences about using it?

Is it possible to use compression in just some
subset of communications (communicator
specific compression settings)?

In our MPI application we are transferring large
amounts of sparse/redundant data that compresses
very well. Also my initial tests showed significant
improvements in performance.
    </pre>
  </blockquote>
  <pre wrap=""><!---->
If your particular data is well-suited for fast compression, you might  
want to compress it before calling MPI_SEND / after calling MPI_RECV.   
Use the MPI_BYTE datatype to send/receive the messages, and then MPI  
won't do anything additional for datatype conversions, etc</pre>
</blockquote>
Yeah, already did something like this. I have a situation where all <br>
the nodes are sending large amounts of redundant data at once. The <br>
combination: "compress --&gt; MPI_SEND --&gt; MPI_RECV --&gt;
decompress" <br>
works of course, but it forces one to allocate large amounts of memory <br>
(or diskspace) for the compressed data. You can do it manually in parts
<br>
of course, but it would be nice if MPI library could do it behind the <br>
scenes. <br>
<br>
Thanks,<br>
<pre class="moz-signature" cols="72">-- 
Tomas Ukkonen

</pre>
</body>
</html>

