<div dir="ltr"><div>Thank you for the fast answer </div><div><br></div>While that resolve my problem with cross ssh authentication   a command as<div><br><div>/opt/openmpi/bin/mpirun  --mca mtl mx --mca pml cm --mca plm_rsh_no_tree_spawn 1 -hostfile hostfile ompi_info<br>
</div></div><div><br></div><div>just hung with no output and although there is a ssh connexion no orte program is initiated in the destination nodes</div><div><br></div><div>and while </div><div><br></div><div>/opt/openmpi/bin/mpirun  -host host18 ompi_info<br>
</div><div><br></div><div>works</div><div><br></div><div>/opt/openmpi/bin/mpirun  --mca plm_rsh_no_tree_spawn 1 -host host18 ompi_info<br></div><div><br></div><div>hangs, is there some condition in the use of this parameter.</div>
<div><br></div><div>Yours truly</div><div><br></div><div>Ricardo </div><div><br></div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Mon, Jul 14, 2014 at 6:35 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">During the 1.7 series and for all follow-on series, OMPI changed to a mode where it launches a daemon on all allocated nodes at the startup of mpirun. This allows us to determine the hardware topology of the nodes and take that into account when mapping. You can override that behavior by either adding --novm to your cmd line (which will impact your mapping/binding options), or by specifying the hosts to use by editing your hostfile, or adding --host host1,host2 to your cmd line<br>

<br>
The rsh launcher defaults to a tree-based pattern, thus requiring that we be able to ssh from one compute node to another. You can change that to a less scalable direct mode by adding<br>
<br>
--mca plm_rsh_no_tree_spawn 1<br>
<br>
to the cmd line<br>
<div><div class="h5"><br>
<br>
On Jul 14, 2014, at 9:21 AM, Ricardo Fernández-Perea &lt;<a href="mailto:rfernandezperea@gmail.com">rfernandezperea@gmail.com</a>&gt; wrote:<br>
<br>
&gt; I&#39;m trying to update to openMPI 1.8.1 thru ssh  and Myrinet<br>
&gt;<br>
&gt; running a command as<br>
&gt;<br>
&gt; /opt/openmpi/bin/mpirun --verbose --mca mtl mx --mca pml cm  -hostfile hostfile -np 16<br>
&gt;<br>
&gt; when the hostfile contain only two nodes as<br>
&gt;<br>
&gt; host1 slots=8 max-slots=8<br>
&gt; host2 slots=8 max-slots=8<br>
&gt;<br>
&gt; it runs perfectly but when the hostfile has a third node as<br>
&gt;<br>
&gt;<br>
&gt; host1 slots=8 max-slots=8<br>
&gt; host2 slots=8 max-slots=8<br>
&gt; host3 slots=8 max-slots=8<br>
&gt;<br>
&gt; it try to establish an ssh connection between  the running hosts1 and host3 that should not run any process  that fails hanging the process without signaling.<br>
&gt;<br>
&gt;<br>
&gt; my ompi_info is as follow<br>
&gt;<br>
&gt;                 Package: Open MPI XXX Distribution<br>
&gt;                 Open MPI: 1.8.1<br>
&gt;   Open MPI repo revision: r31483<br>
&gt;    Open MPI release date: Apr 22, 2014<br>
&gt;                 Open RTE: 1.8.1<br>
&gt;   Open RTE repo revision: r31483<br>
&gt;    Open RTE release date: Apr 22, 2014<br>
&gt;                     OPAL: 1.8.1<br>
&gt;       OPAL repo revision: r31483<br>
&gt;        OPAL release date: Apr 22, 2014<br>
&gt;                  MPI API: 3.0<br>
&gt;             Ident string: 1.8.1<br>
&gt;                   Prefix: /opt/openmpi<br>
&gt;  Configured architecture: x86_64-apple-darwin9.8.0<br>
&gt;           Configure host: XXXX<br>
&gt;            Configured by: XXXX<br>
&gt;            Configured on: Thu Jun 12 10:37:33 CEST 2014<br>
&gt;           Configure host: XXXX<br>
&gt;                 Built by: XXXX<br>
&gt;                 Built on: Thu Jun 12 11:13:16 CEST 2014<br>
&gt;               Built host: XXXX<br>
&gt;               C bindings: yes<br>
&gt;             C++ bindings: yes<br>
&gt;              Fort mpif.h: yes (single underscore)<br>
&gt;             Fort use mpi: yes (full: ignore TKR)<br>
&gt;        Fort use mpi size: deprecated-ompi-info-value<br>
&gt;         Fort use mpi_f08: yes<br>
&gt;  Fort mpi_f08 compliance: The mpi_f08 module is available, but due to<br>
&gt;                           limitations in the ifort compiler, does not support<br>
&gt;                           the following: array subsections, direct passthru<br>
&gt;                           (where possible) to underlying Open MPI&#39;s C<br>
&gt;                           functionality<br>
&gt;   Fort mpi_f08 subarrays: no<br>
&gt;            Java bindings: no<br>
&gt;   Wrapper compiler rpath: unnecessary<br>
&gt;               C compiler: icc<br>
&gt;      C compiler absolute: /opt/intel/Compiler/11.1/080/bin/intel64/icc<br>
&gt;   C compiler family name: INTEL<br>
&gt;       C compiler version: 1110.20091130<br>
&gt;             C++ compiler: icpc<br>
&gt;    C++ compiler absolute: /opt/intel/Compiler/11.1/080/bin/intel64/icpc<br>
&gt;            Fort compiler: ifort<br>
&gt;        Fort compiler abs: /opt/intel/Compiler/11.1/080/bin/intel64/ifort<br>
&gt;          Fort ignore TKR: yes (!DEC$ ATTRIBUTES NO_ARG_CHECK ::)<br>
&gt;    Fort 08 assumed shape: no<br>
&gt;       Fort optional args: yes<br>
&gt;       Fort BIND(C) (all): yes<br>
&gt;       Fort ISO_C_BINDING: yes<br>
&gt;  Fort SUBROUTINE BIND(C): yes<br>
&gt;        Fort TYPE,BIND(C): yes<br>
&gt;  Fort T,BIND(C,name=&quot;a&quot;): yes<br>
&gt;             Fort PRIVATE: yes<br>
&gt;           Fort PROTECTED: yes<br>
&gt;            Fort ABSTRACT: yes<br>
&gt;        Fort ASYNCHRONOUS: yes<br>
&gt;           Fort PROCEDURE: yes<br>
&gt;  Fort f08 using wrappers: yes<br>
&gt;              C profiling: yes<br>
&gt;            C++ profiling: yes<br>
&gt;    Fort mpif.h profiling: yes<br>
&gt;   Fort use mpi profiling: yes<br>
&gt;    Fort use mpi_f08 prof: yes<br>
&gt;           C++ exceptions: no<br>
&gt;           Thread support: posix (MPI_THREAD_MULTIPLE: no, OPAL support: yes,<br>
&gt;                           OMPI progress: no, ORTE progress: yes, Event lib:<br>
&gt;                           yes)<br>
&gt;            Sparse Groups: no<br>
&gt;   Internal debug support: no<br>
&gt;   MPI interface warnings: yes<br>
&gt;      MPI parameter check: runtime<br>
&gt; Memory profiling support: no<br>
&gt; Memory debugging support: no<br>
&gt;          libltdl support: yes<br>
&gt;    Heterogeneous support: no<br>
&gt;  mpirun default --prefix: no<br>
&gt;          MPI I/O support: yes<br>
&gt;        MPI_WTIME support: gettimeofday<br>
&gt;      Symbol vis. support: yes<br>
&gt;    Host topology support: yes<br>
&gt;           MPI extensions:<br>
&gt;    FT Checkpoint support: no (checkpoint thread: no)<br>
&gt;    C/R Enabled Debugging: no<br>
&gt;      VampirTrace support: yes<br>
&gt;   MPI_MAX_PROCESSOR_NAME: 256<br>
&gt;     MPI_MAX_ERROR_STRING: 256<br>
&gt;      MPI_MAX_OBJECT_NAME: 64<br>
&gt;         MPI_MAX_INFO_KEY: 36<br>
&gt;         MPI_MAX_INFO_VAL: 256<br>
&gt;        MPI_MAX_PORT_NAME: 1024<br>
&gt;   MPI_MAX_DATAREP_STRING: 128<br>
&gt;<br>
&gt;<br>
</div></div>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/07/24764.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/07/24764.php</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/07/24765.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/07/24765.php</a><br>
</blockquote></div><br></div>

