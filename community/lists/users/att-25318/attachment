<div dir="ltr"><div>Thanks Jeff,<br></div>It worked. Now latency and bandwidth benchmarks are in performing as expected for both Ethernet and InfiniBand.<br><div class="gmail_extra"><br>--Ansar<br><br></div><div class="gmail_extra"><div class="gmail_quote">On Wed, Sep 10, 2014 at 3:34 PM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Are you inadvertently using the MXM MTL?  That&#39;s an alternate Mellanox transport that may activate itself, even if you&#39;ve disabled the openib BTL.  Try this:<br>
<br>
  mpirun --mca pml ob1 --mca btl ^openib ...<br>
<br>
This forces the use of the ob1 PML (which forces the use of the BTLs, not the MTLs), and then disables the openib BTL.<br>
<div><div class="h5"><br>
<br>
On Sep 9, 2014, at 10:24 AM, Muhammad Ansar Javed &lt;<a href="mailto:muhammad.ansar@seecs.edu.pk">muhammad.ansar@seecs.edu.pk</a>&gt; wrote:<br>
<br>
&gt; Hi,<br>
&gt;<br>
&gt; I am currently conducting some testing on system with Gigabit and InfiniBand interconnects. Both Latency and Bandwidth benchmarks are doing well as expected on InfiniBand interconnects but Ethernet interconnect is achieving very high performance from expectations. Ethernet and InfiniBand both are achieving equivalent performance.<br>
&gt;<br>
&gt; For some reason, it looks like openmpi (v1.8.1) is using the InfiniBand interconnect rather than the Gigabit or TCP communication is being emulated to use InifiniBand interconnect.<br>
&gt;<br>
&gt; Here are Latency and Bandwidth benchmark results.<br>
&gt; #---------------------------------------------------<br>
&gt; # Benchmarking PingPong<br>
&gt; # processes = 2<br>
&gt; # map-by node<br>
&gt; #---------------------------------------------------<br>
&gt;<br>
&gt; Hello, world.  I am 1 on node124<br>
&gt; Hello, world.  I am 0 on node123<br>
&gt; Size Latency (usec) Bandwidth (Mbps)<br>
&gt; 1    1.65    4.62<br>
&gt; 2    1.67    9.16<br>
&gt; 4    1.66    18.43<br>
&gt; 8    1.66    36.74<br>
&gt; 16    1.85    66.00<br>
&gt; 32    1.83    133.28<br>
&gt; 64    1.83    266.36<br>
&gt; 128    1.88    519.10<br>
&gt; 256    1.99    982.29<br>
&gt; 512    2.23    1752.37<br>
&gt; 1024    2.58    3026.98<br>
&gt; 2048    3.32    4710.76<br>
&gt;<br>
&gt; I read some of the FAQs and noted that OpenMPI prefers the faster available interconnect. In an effort to force it to use the gigabit interconnect I ran it as follows<br>
&gt;<br>
&gt; 1. mpirun -np 2 -machinefile machines -map-by node --mca btl tcp --mca btl_tcp_if_include em1 ./latency.ompi<br>
&gt; 2. mpirun -np 2 -machinefile machines -map-by node --mca btl tcp,self,sm --mca btl_tcp_if_include em1 ./latency.ompi<br>
&gt; 3. mpirun -np 2 -machinefile machines -map-by node --mca btl ^openib --mca btl_tcp_if_include em1 ./latency.ompi<br>
&gt; 4. mpirun -np 2 -machinefile machines -map-by node --mca btl ^openib ./latency.ompi<br>
&gt;<br>
&gt; None of them resulted in a significantly different benchmark output.<br>
&gt;<br>
&gt; I am using OpenMPI by loading module on clustered environment and don&#39;t have admin access. It is configured for both TCP and OpenIB (confirmed from ompi_info). After trying all above mentioned methods without success I installed OpenMPI v1.8.2 in my home directory and disable openib with following configuration options<br>
&gt;<br>
&gt; --disable-openib-control-hdr-padding --disable-openib-dynamic-sl --disable-openib-connectx-xrc --disable-openib-udcm --disable-openib-rdmacm  --disable-btl-openib-malloc-alignment  --disable-io-romio --without-openib --without-verbs<br>
&gt;<br>
&gt; Now, openib is not enabled (confirmed from ompi_info script) and there is no &quot;openib.so&quot; file in $prefix/lib/openmpi directory as well. Still, above mentioned mpirun commands are getting the same latency and bandwidth as that of InfiniBand.<br>
&gt;<br>
&gt; I tried mpirun in verbose mode with following command and here is the output<br>
&gt;<br>
&gt; Command:<br>
&gt; mpirun -np 2 -machinefile machines -map-by node --mca btl tcp --mca btl_base_verbose 30 --mca btl_tcp_if_include em1 ./latency.ompi<br>
&gt;<br>
&gt; Output:<br>
&gt; [node123.prv.sciama.cluster:88310] mca: base: components_register: registering btl components<br>
&gt; [node123.prv.sciama.cluster:88310] mca: base: components_register: found loaded component tcp<br>
&gt; [node123.prv.sciama.cluster:88310] mca: base: components_register: component tcp register function successful<br>
&gt; [node123.prv.sciama.cluster:88310] mca: base: components_open: opening btl components<br>
&gt; [node123.prv.sciama.cluster:88310] mca: base: components_open: found loaded component tcp<br>
&gt; [node123.prv.sciama.cluster:88310] mca: base: components_open: component tcp open function successful<br>
&gt; [node124.prv.sciama.cluster:90465] mca: base: components_register: registering btl components<br>
&gt; [node124.prv.sciama.cluster:90465] mca: base: components_register: found loaded component tcp<br>
&gt; [node124.prv.sciama.cluster:90465] mca: base: components_register: component tcp register function successful<br>
&gt; [node124.prv.sciama.cluster:90465] mca: base: components_open: opening btl components<br>
&gt; [node124.prv.sciama.cluster:90465] mca: base: components_open: found loaded component tcp<br>
&gt; [node124.prv.sciama.cluster:90465] mca: base: components_open: component tcp open function successful<br>
&gt; Hello, world.  I am 1 on node124<br>
&gt; Hello, world.  I am 0 on node123<br>
&gt; Size Latency(usec) Bandwidth(Mbps)<br>
&gt; 1    4.18    1.83<br>
&gt; 2    3.66    4.17<br>
&gt; 4    4.08    7.48<br>
&gt; 8    3.12    19.57<br>
&gt; 16    3.83    31.84<br>
&gt; 32    3.40    71.84<br>
&gt; 64    4.10    118.97<br>
&gt; 128    3.89    251.19<br>
&gt; 256    4.22    462.77<br>
&gt; 512    2.95    1325.71<br>
&gt; 1024    2.63    2969.49<br>
&gt; 2048    3.38    4628.29<br>
&gt; [node123.prv.sciama.cluster:88310] mca: base: close: component tcp closed<br>
&gt; [node123.prv.sciama.cluster:88310] mca: base: close: unloading component tcp<br>
&gt; [node124.prv.sciama.cluster:90465] mca: base: close: component tcp closed<br>
&gt; [node124.prv.sciama.cluster:90465] mca: base: close: unloading component tcp<br>
&gt;<br>
&gt; Moreover, same benchmark applications using MPICH are working fine on Ethernet and achieving expected Latency and Bandwidth.<br>
&gt;<br>
&gt; How can this be fixed?<br>
&gt;<br>
&gt; Thanks for help,<br>
&gt;<br>
&gt; --Ansar<br>
&gt;<br>
&gt;<br>
</div></div><span class="">&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</span>&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/09/25297.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/09/25297.php</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<span class=""><br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</span>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/09/25307.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/09/25307.php</a><br>
</blockquote></div><br><br clear="all"><br><br></div></div>

