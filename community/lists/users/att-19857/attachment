Reuti,<br><br>&gt;-nolocal is IMO an option where you want to execute the `mpirun` on your
 local login machine and want the MPI &gt;processes to be allocated 
somewhere in the cluster, in case you don&#39;t have any queuing system 
around to manage &gt;the resources.<br>
<span class="HOEnZb"><font color="#888888"><br>yes, this is exactly my understanding of the -nolocal option. Otherwise, by specifying an &#39;image set&#39; of processors,<br>everything gets &#39;mapped&#39; to some subset of processors in the image set. Again, thanks for your response.<br>
</font></span><br><br><div class="gmail_quote">On Fri, Jul 27, 2012 at 5:15 AM, Reuti <span dir="ltr">&lt;<a href="mailto:reuti@staff.uni-marburg.de" target="_blank">reuti@staff.uni-marburg.de</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Am 27.07.2012 um 03:21 schrieb Ralph Castain:<br>
<div class="im"><br>
&gt; Application processes will *only* be placed on nodes included in the allocation. The -nolocal flag is intended to ensure that no application processes are started on the same node as mpirun in the case where that node is included in the allocation. This happens, for example, with Torque, where mpirun is executed on one of the allocated nodes.<br>

<br>
</div>But the behavior is the same in Torque and SGE. The jobscript is executed on one of the elected exechosts (neither the submit host, nor the qmaster host [unless they are exechosts too]) and so eligible to be used too. In no case there should be -nolocal being used.<br>

<br>
-nolocal is IMO an option where you want to execute the `mpirun` on your local login machine and want the MPI processes to be allocated somewhere in the cluster, in case you don&#39;t have any queuing system around to manage the resources.<br>

<span class="HOEnZb"><font color="#888888"><br>
-- Reuti<br>
</font></span><div class="HOEnZb"><div class="h5"><br>
&gt; I believe SGE doesn&#39;t do that - and so the allocation won&#39;t include the submit host, in which case you don&#39;t need -nolocal.<br>
&gt;<br>
&gt;<br>
&gt; On Jul 26, 2012, at 5:58 PM, Erik Nelson wrote:<br>
&gt;<br>
&gt;&gt; I was under the impression that the -nolocal option keeps processes off the submit<br>
&gt;&gt; host (since there may be hundreds or thousands of jobs submitted at any time,<br>
&gt;&gt; and we don&#39;t want this host to be overloaded).<br>
&gt;&gt;<br>
&gt;&gt; My understanding of what you said in you last email is that, by listing the hosts,  I<br>
&gt;&gt; automatically send all processes (parent and child, or master and slave if you<br>
&gt;&gt; prefer) to the specified list of hosts.<br>
&gt;&gt;<br>
&gt;&gt; Reading your email below, it looks like this was the correct understanding.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; On Thu, Jul 26, 2012 at 5:20 PM, Reuti &lt;<a href="mailto:reuti@staff.uni-marburg.de">reuti@staff.uni-marburg.de</a>&gt; wrote:<br>
&gt;&gt; Am 26.07.2012 um 23:58 schrieb Erik Nelson:<br>
&gt;&gt;<br>
&gt;&gt; &gt; Reuti,<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Thank you. Our queue is backed up, so it will take a little while before I can try this.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; I assume that by specifying the nodes this way, I don&#39;t need (and it would confuse<br>
&gt;&gt; &gt; the system) to add -nolocal. In other words, qsub will try to put the parent node<br>
&gt;&gt; &gt; somewhere in this set.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Is this the idea?<br>
&gt;&gt;<br>
&gt;&gt; Depends what you refer to by &quot;parent node&quot;. I assume you mean the submit host. This is never included in any created selection of SGE unless it&#39;s an execution host too.<br>
&gt;&gt;<br>
&gt;&gt; The master host of the parallel job (i.e. the one where the jobscript with the `mpiexec` is running) will be used as a normal machine from MPI&#39;s point of view.<br>
&gt;&gt;<br>
&gt;&gt; -- Reuti<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; &gt; Erik<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; On Thu, Jul 26, 2012 at 4:48 PM, Reuti &lt;<a href="mailto:reuti@staff.uni-marburg.de">reuti@staff.uni-marburg.de</a>&gt; wrote:<br>
&gt;&gt; &gt; Am 26.07.2012 um 23:33 schrieb Erik Nelson:<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; &gt; I have a purely parallel job that runs ~100 processes. Each process has ~identical<br>
&gt;&gt; &gt; &gt; overhead so the speed of the program is dominated by the slowest processor.<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; For this reason, I would like to restrict the job to a specific set of identical (fast)<br>
&gt;&gt; &gt; &gt; processors on our cluster.<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; I read the FAQ on -hosts and -hostfile, but it is still unclear to me what affect these<br>
&gt;&gt; &gt; &gt; directives will have in a queuing environment.<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; Currently, I submit the job using the &quot;qsub&quot; command in the &quot;sge&quot; environment as :<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt;             qsub -pe mpich 101 jobfile.job<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; where jobfile contains the command<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt;             mpirun -np 101 -nolocal ./executable<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; I would leave -nolocal out here.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; $ qsub -l &quot;h=compute-5-[1-9]|compute-5-1[0-9]|compute-5-2[0-9]|compute-5-3[0-2]&quot; -pe mpich 101 jobfile.job<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; -- Reuti<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; &gt; I would like to restrict the job to nodes compute-5-1 to compute-5-32 on our machine,<br>
&gt;&gt; &gt; &gt; each containing 8 cpu&#39;s (slots). How do I go about this?<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; Thanks, Erik<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; --<br>
&gt;&gt; &gt; &gt; Erik Nelson<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; Howard Hughes Medical Institute<br>
&gt;&gt; &gt; &gt; 6001 Forest Park Blvd., Room ND10.124<br>
&gt;&gt; &gt; &gt; Dallas, Texas 75235-9050<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; p : 214 645 5981<br>
&gt;&gt; &gt; &gt; f : 214 645 5948<br>
&gt;&gt; &gt; &gt; _______________________________________________<br>
&gt;&gt; &gt; &gt; users mailing list<br>
&gt;&gt; &gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; &gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; _______________________________________________<br>
&gt;&gt; &gt; users mailing list<br>
&gt;&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; --<br>
&gt;&gt; &gt; Erik Nelson<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Howard Hughes Medical Institute<br>
&gt;&gt; &gt; 6001 Forest Park Blvd., Room ND10.124<br>
&gt;&gt; &gt; Dallas, Texas 75235-9050<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; p : <a href="tel:214%20645%205981" value="+12146455981">214 645 5981</a><br>
&gt;&gt; &gt; f : <a href="tel:214%20645%205948" value="+12146455948">214 645 5948</a><br>
&gt;&gt; &gt; _______________________________________________<br>
&gt;&gt; &gt; users mailing list<br>
&gt;&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; Erik Nelson<br>
&gt;&gt;<br>
&gt;&gt; Howard Hughes Medical Institute<br>
&gt;&gt; 6001 Forest Park Blvd., Room ND10.124<br>
&gt;&gt; Dallas, Texas 75235-9050<br>
&gt;&gt;<br>
&gt;&gt; p : <a href="tel:214%20645%205981" value="+12146455981">214 645 5981</a><br>
&gt;&gt; f : <a href="tel:214%20645%205948" value="+12146455948">214 645 5948</a><br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br><br clear="all"><br>-- <br>Erik Nelson<br><br>Howard Hughes Medical Institute<br>6001 Forest Park Blvd., Room ND10.124<br>Dallas, Texas 75235-9050<br><br>p : 214 645 5981<br>f : 214 645 5948<br>


