Weird - it works fine for me:<br><br>sjc-vpn5-109:mpi rhc$ mpirun -n 3 ./abort<br>Hello, World, I am 1 of 3<br>--------------------------------------------------------------------------<br>MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD <br>
with errorcode 2.<br><br>NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.<br>You may or may not see output from other processes, depending on<br>exactly when Open MPI kills them.<br>--------------------------------------------------------------------------<br>
--------------------------------------------------------------------------<br>mpirun has exited due to process rank 1 with PID 22980 on<br>node <a href="http://sjc-vpn5-109.cisco.com">sjc-vpn5-109.cisco.com</a> exiting without calling &quot;finalize&quot;. This may<br>
have caused other processes in the application to be<br>terminated by signals sent by mpirun (as reported here).<br>--------------------------------------------------------------------------<br>Hello, World, I am 0 of 3<br>
Hello, World, I am 2 of 3<br><br>I built it with gcc 4.2.1, though - I know we have a problem with shared memory hanging when built with gcc 4.4.x, so I wonder if the issue here is your use of gcc 4.5?<br><br>Can you try running this again with -mca btl ^sm?<br>
<br><br><div class="gmail_quote">On Wed, Jun 2, 2010 at 3:49 AM, Yves Caniou <span dir="ltr">&lt;<a href="mailto:yves.caniou@ens-lyon.fr">yves.caniou@ens-lyon.fr</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">
Dear All,<br>
<br>
As already said on this mailing list, I found that a simple Hello_world<br>
program does not necessarily  end (the program just hangs after the<br>
MPI_Finalize(), and I can printf the MPI_FINALIZED which confirm that the MPI<br>
part of the code has finished, but the exit() or return() never ends).<br>
<br>
So I tried to use MPI_Abort(), and observed two different behaviors<br>
(description of the architecture is given below).<br>
Either it ends with a segfault, or the application doesn&#39;t return to shell,<br>
even if the string &quot;MPI_ABORT was [...] here).&quot; appears on screen (program<br>
just hangs, as with MPI_Finalize()).<br>
<br>
This is annoying since I need several execution in a batch script, since<br>
several submission cost a lot of time in queues. Then, if you have any tips<br>
to bypass the hanging of the application, I take it (even if it means<br>
recompile OpenMPI with specific options of course).<br>
<br>
Thank you!<br>
<br>
.Yves.<br>
<br>
Here is an example of the output produced on screen. Note that errorcode is<br>
the rank of the process which called MPI_Abort().<br>
<br>
############################################<br>
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD<br>
with errorcode 0.<br>
<br>
NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.<br>
You may or may not see output from other processes, depending on<br>
exactly when Open MPI kills them.<br>
--------------------------------------------------------------------------<br>
--------------------------------------------------------------------------<br>
mpiexec has exited due to process rank 0 with PID 18062 on<br>
node ha8000-1 exiting without calling &quot;finalize&quot;. This may<br>
have caused other processes in the application to be<br>
terminated by signals sent by mpiexec (as reported here).<br>
--------------------------------------------------------------------------<br>
[ha8000-1:18060] *** Process received signal ***<br>
[ha8000-1:18060] Signal: Segmentation fault (11)<br>
[ha8000-1:18060] Signal code: Address not mapped (1)<br>
[ha8000-1:18060] Failing at address: 0x2aaaac1bd940<br>
Segmentation fault<br>
############################################<br>
<br>
The architecture is a Quad-Core AMD Opteron(tm) Processor 8356, Ethernet<br>
controller: MYRICOM Inc. Myri-10G Dual-Protocol NIC (10G-PCIE-8A), the<br>
version of OMPI is 1.4.2 and have been compiled with GCC-4.5<br>
$&gt;ompi_info<br>
                 Package: Open MPI p10015@ha8000-1 Distribution<br>
                Open MPI: 1.4.2<br>
   Open MPI SVN revision: r23093<br>
   Open MPI release date: May 04, 2010<br>
                Open RTE: 1.4.2<br>
   Open RTE SVN revision: r23093<br>
   Open RTE release date: May 04, 2010<br>
                    OPAL: 1.4.2<br>
       OPAL SVN revision: r23093<br>
       OPAL release date: May 04, 2010<br>
            Ident string: 1.4.2<br>
                  Prefix: /home/p10015/openmpi<br>
 Configured architecture: x86_64-unknown-linux-gnu<br>
          Configure host: ha8000-1<br>
           Configured by: p10015<br>
           Configured on: Wed May 19 19:01:19 JST 2010<br>
          Configure host: ha8000-1<br>
                Built by: p10015<br>
                Built on: Wed May 19 21:03:33 JST 2010<br>
              Built host: ha8000-1<br>
              C bindings: yes<br>
            C++ bindings: yes<br>
      Fortran77 bindings: yes (all)<br>
      Fortran90 bindings: yes<br>
 Fortran90 bindings size: small<br>
              C<br>
compiler: /home/p10015/gcc/bin/x86_64-unknown-linux-gnu-gcc-4.5.0<br>
     C compiler absolute:<br>
            C++ compiler: /home/p10015/gcc/bin/x86_64-unknown-linux-gnu-g++<br>
   C++ compiler absolute:<br>
      Fortran77 compiler: gfortran<br>
  Fortran77 compiler abs: /usr/bin/gfortran<br>
      Fortran90 compiler: gfortran<br>
  Fortran90 compiler abs: /usr/bin/gfortran<br>
             C profiling: yes<br>
           C++ profiling: yes<br>
     Fortran77 profiling: yes<br>
     Fortran90 profiling: yes<br>
          C++ exceptions: no<br>
          Thread support: posix (mpi: yes, progress: yes)<br>
           Sparse Groups: yes<br>
  Internal debug support: no<br>
     MPI parameter check: runtime<br>
Memory profiling support: no<br>
Memory debugging support: no<br>
         libltdl support: yes<br>
   Heterogeneous support: yes<br>
 mpirun default --prefix: yes<br>
         MPI I/O support: yes<br>
       MPI_WTIME support: gettimeofday<br>
Symbol visibility support: yes<br>
   FT Checkpoint support: no  (checkpoint thread: no)<br>
           MCA backtrace: execinfo (MCA v2.0, API v2.0, Component v1.4.2)<br>
              MCA memory: ptmalloc2 (MCA v2.0, API v2.0, Component v1.4.2)<br>
           MCA paffinity: linux (MCA v2.0, API v2.0, Component v1.4.2)<br>
               MCA carto: auto_detect (MCA v2.0, API v2.0, Component v1.4.2)<br>
               MCA carto: file (MCA v2.0, API v2.0, Component v1.4.2)<br>
           MCA maffinity: first_use (MCA v2.0, API v2.0, Component v1.4.2)<br>
           MCA maffinity: libnuma (MCA v2.0, API v2.0, Component v1.4.2)<br>
               MCA timer: linux (MCA v2.0, API v2.0, Component v1.4.2)<br>
         MCA installdirs: env (MCA v2.0, API v2.0, Component v1.4.2)<br>
         MCA installdirs: config (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA dpm: orte (MCA v2.0, API v2.0, Component v1.4.2)<br>
              MCA pubsub: orte (MCA v2.0, API v2.0, Component v1.4.2)<br>
           MCA allocator: basic (MCA v2.0, API v2.0, Component v1.4.2)<br>
           MCA allocator: bucket (MCA v2.0, API v2.0, Component v1.4.2)<br>
                MCA coll: basic (MCA v2.0, API v2.0, Component v1.4.2)<br>
                MCA coll: hierarch (MCA v2.0, API v2.0, Component v1.4.2)<br>
                MCA coll: inter (MCA v2.0, API v2.0, Component v1.4.2)<br>
                MCA coll: self (MCA v2.0, API v2.0, Component v1.4.2)<br>
                MCA coll: sm (MCA v2.0, API v2.0, Component v1.4.2)<br>
                MCA coll: sync (MCA v2.0, API v2.0, Component v1.4.2)<br>
                MCA coll: tuned (MCA v2.0, API v2.0, Component v1.4.2)<br>
                  MCA io: romio (MCA v2.0, API v2.0, Component v1.4.2)<br>
               MCA mpool: fake (MCA v2.0, API v2.0, Component v1.4.2)<br>
               MCA mpool: rdma (MCA v2.0, API v2.0, Component v1.4.2)<br>
               MCA mpool: sm (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA pml: cm (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA pml: csum (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA pml: ob1 (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA pml: v (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA bml: r2 (MCA v2.0, API v2.0, Component v1.4.2)<br>
              MCA rcache: vma (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA btl: self (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA btl: sm (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA btl: tcp (MCA v2.0, API v2.0, Component v1.4.2)<br>
                MCA topo: unity (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA osc: pt2pt (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA osc: rdma (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA iof: hnp (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA iof: orted (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA iof: tool (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA oob: tcp (MCA v2.0, API v2.0, Component v1.4.2)<br>
                MCA odls: default (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA ras: slurm (MCA v2.0, API v2.0, Component v1.4.2)<br>
               MCA rmaps: load_balance (MCA v2.0, API v2.0, Component v1.4.2)<br>
               MCA rmaps: rank_file (MCA v2.0, API v2.0, Component v1.4.2)<br>
               MCA rmaps: round_robin (MCA v2.0, API v2.0, Component v1.4.2)<br>
               MCA rmaps: seq (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA rml: oob (MCA v2.0, API v2.0, Component v1.4.2)<br>
              MCA routed: binomial (MCA v2.0, API v2.0, Component v1.4.2)<br>
              MCA routed: direct (MCA v2.0, API v2.0, Component v1.4.2)<br>
              MCA routed: linear (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA plm: rsh (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA plm: slurm (MCA v2.0, API v2.0, Component v1.4.2)<br>
               MCA filem: rsh (MCA v2.0, API v2.0, Component v1.4.2)<br>
              MCA errmgr: default (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA ess: env (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA ess: hnp (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA ess: singleton (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA ess: slurm (MCA v2.0, API v2.0, Component v1.4.2)<br>
                 MCA ess: tool (MCA v2.0, API v2.0, Component v1.4.2)<br>
             MCA grpcomm: bad (MCA v2.0, API v2.0, Component v1.4.2)<br>
             MCA grpcomm: basic (MCA v2.0, API v2.0, Component v1.4.2)<br>
<br>
--<br>
Yves Caniou<br>
Associate Professor at Université Lyon 1,<br>
Member of the team project INRIA GRAAL in the LIP ENS-Lyon,<br>
Délégation CNRS in Japan French Laboratory of Informatics (JFLI),<br>
  * in Information Technology Center, The University of Tokyo,<br>
    2-11-16 Yayoi, Bunkyo-ku, Tokyo 113-8658, Japan<br>
    tel: +81-3-5841-0540<br>
  * in National Institute of Informatics<br>
    2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan<br>
    tel: +81-3-4212-2412<br>
<a href="http://graal.ens-lyon.fr/%7Eycaniou/" target="_blank">http://graal.ens-lyon.fr/~ycaniou/</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote></div><br>

