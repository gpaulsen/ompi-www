<html><body>
<p>abc def<br>
<br>
When the parent does a spawn call, it presumably blocks until the child  tasks have called MPI_Init.  The standard allows some flexibility on this but at least after spawn, the spawn side must be able to issue communication calls involving the children and expect them to work.<br>
<br>
What you seem to be missing is that when a parent has spawned a set of children, the parent tasks and child tasks are connected. If you want the children to do an MPI_Finalize and actually finish before the parent calls MPI_Finalize, you must use MPI_Comm_disconnect on the intercommunicator between the spawn side and the children.<br>
<br>
The MPI standard makes MPI_Finalize collective across all currently connected processes so you cannot assume the children will return from MPI_Finalize until the parent process have entered MPI_Finalize.<br>
<br>
MPI_Comm_disconnect makes the parent and children independent so an MPI_Finalize by the children can return and the processes end, even though the parent continues on.<br>
<br>
In your example, perhaps the best approach is to have the children call MPI_Barrier after the file is written and have the parent call MPI_Barrier before the file is read. Have both parent and children call MPI_Comm_disconnect before the parent does another spawn so the children can finalize and go away.<br>
 <br>
<br>
Dick Treumann  -  MPI Team           <br>
IBM Systems &amp; Technology Group<br>
Dept X2ZA / MS P963 -- 2455 South Road -- Poughkeepsie, NY 12601<br>
Tele (845) 433-7846         Fax (845) 433-8363<br>
<br>
<br>
<img width="16" height="16" src="cid:1__=0ABBFC7ADFCDA56F8f9e8a93df938@us.ibm.com" border="0" alt="Inactive hide details for Jeff Squyres ---03/17/2010 12:21:20 PM---On Mar 16, 2010, at 5:12 AM, abc def wrote: &gt; 1. Since Spawn"><font color="#424282">Jeff Squyres ---03/17/2010 12:21:20 PM---On Mar 16, 2010, at 5:12 AM, abc def wrote: &gt; 1. Since Spawn is non-blocking, but I need the parent</font><br>
<br>

<table width="100%" border="0" cellspacing="0" cellpadding="0">
<tr valign="top"><td width="1%"><img width="96" height="1" src="cid:2__=0ABBFC7ADFCDA56F8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<font size="2" color="#5F5F5F">From:</font></td><td width="100%"><img width="1" height="1" src="cid:2__=0ABBFC7ADFCDA56F8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<font size="2">Jeff Squyres &lt;jsquyres@cisco.com&gt;</font></td></tr>

<tr valign="top"><td width="1%"><img width="96" height="1" src="cid:2__=0ABBFC7ADFCDA56F8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<font size="2" color="#5F5F5F">To:</font></td><td width="100%"><img width="1" height="1" src="cid:2__=0ABBFC7ADFCDA56F8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<font size="2">&quot;Open MPI Users&quot; &lt;users@open-mpi.org&gt;</font></td></tr>

<tr valign="top"><td width="1%"><img width="96" height="1" src="cid:2__=0ABBFC7ADFCDA56F8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<font size="2" color="#5F5F5F">Date:</font></td><td width="100%"><img width="1" height="1" src="cid:2__=0ABBFC7ADFCDA56F8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<font size="2">03/17/2010 12:21 PM</font></td></tr>

<tr valign="top"><td width="1%"><img width="96" height="1" src="cid:2__=0ABBFC7ADFCDA56F8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<font size="2" color="#5F5F5F">Subject:</font></td><td width="100%"><img width="1" height="1" src="cid:2__=0ABBFC7ADFCDA56F8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<font size="2">Re: [OMPI users] running externalprogram	on	same	processor	(Fortran)</font></td></tr>

<tr valign="top"><td width="1%"><img width="96" height="1" src="cid:2__=0ABBFC7ADFCDA56F8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<font size="2" color="#5F5F5F">Sent by:</font></td><td width="100%"><img width="1" height="1" src="cid:2__=0ABBFC7ADFCDA56F8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<font size="2">users-bounces@open-mpi.org</font></td></tr>
</table>
<hr width="100%" size="2" align="left" noshade style="color:#8091A5; "><br>
<br>
<br>
<tt>On Mar 16, 2010, at 5:12 AM, abc def wrote:<br>
<br>
&gt; 1. Since Spawn is non-blocking, but I need the parent to wait until the child completes, I am thinking there must be a way to pass a variable from the child to the parent just prior to the FINALIZE command in the child, to signal that the parent can pick up the output files from the child. Am I right in assuming that the message from the child to the parent will go to the correct parent process? The value of &quot;parent&quot; in &quot;CALL MPI_COMM_GET_PARENT(parent, ierr)&quot; is the same in all spawned processes, which is why I ask this question.<br>
<br>
Yes, you can MPI_SEND (etc.) between the parents and children, just like you would expect. &nbsp;Just be aware that the communicator between the parents and children is an *inter*communicator -- so you need to express the source/destination in terms of the &quot;other&quot; group. &nbsp;Check out the MPI spec for a description of intercommunicators.<br>
<br>
&gt; 2. By launching the parent with the &quot;--mca mpi_yield_when_idle 1&quot; option, the child should be able to take CPU power from any blocked parent process, thus avoiding the busy-poll problem mentioned below.<br>
<br>
Somewhat. &nbsp;Note that the parents aren't blocked -- they *are* busy polling, but they call yield() in every pool loop. &nbsp;<br>
<br>
&gt; If each host has 4 processors and I'm running on 2 hosts (ie, 8 processors in total), then I also assume that the spawned child will launch on the same host as the associated parent?<br>
<br>
If you have told Open MPI about 8 process slots and are using all of them, then spawned processes will start overlaying the original process slots -- effectively in the same order.<br>
<br>
-- <br>
Jeff Squyres<br>
jsquyres@cisco.com<br>
For corporate legal information go to:<br>
</tt><tt><a href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a></tt><tt><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
users@open-mpi.org<br>
</tt><tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></tt><tt><br>
</tt><br>
<br>
</body></html>

