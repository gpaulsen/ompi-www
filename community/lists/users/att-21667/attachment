<html><head><meta http-equiv="Content-Type" content="text/html charset=us-ascii"><meta http-equiv="Content-Type" content="text/html charset=iso-8859-1"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">Sounds like something is making the TCP connections unstable. Last time I looked at HVM, they were running something like 64G of memory? If you have more than one proc on a node (as your output would indicate), and you are doing collectives on such large data sizes, it's quite possible you are running out of memory due to the way the collective algorithms work - and perhaps trashing the connection (which would explain it being unreachable until the OS can reset it).<div><br></div><div>You might try running with fewer procs/node to see if that helps.</div><div><br></div><div><br></div><div><div><div>On Apr 4, 2013, at 11:10 AM, Yevgeny Popkov &lt;<a href="mailto:ypopkov@gmail.com">ypopkov@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr"><span style="font-family:arial,sans-serif;font-size:12.727272033691406px">Hi,</span><br><div class="gmail_quote"><div dir="ltr"><div class="gmail_quote"><div dir="ltr"><div style="font-family:arial,sans-serif;font-size:12.727272033691406px">
<br></div>
<div style="font-family:arial,sans-serif;font-size:12.727272033691406px">
I am running some matrix-algebra-based calculations on Amazon EC2 (HVM instances running Ubuntu 11.1 with OpenMPI 1.6.4 and python bindings with mpi4py 1.3). I am using StarCluster to spin up instances so all nodes from a given cluster are in the same placement group (i.e. are on the same 10 Gb network)&nbsp;</div>


<div style="font-family:arial,sans-serif;font-size:12.727272033691406px"><br></div><div style="font-family:arial,sans-serif;font-size:12.727272033691406px">My calculations are CPU-bound and I use just a few collective operations (namely allgatherv, statterv, bcast, and reduce) that exchange a non-trivial amount data (the size of full distributed dense matrix reaches tens of gigabytes - e.g. I use allgatherv on that matrix)&nbsp;</div>


<div style="font-family:arial,sans-serif;font-size:12.727272033691406px"><br></div><div style="font-family:arial,sans-serif;font-size:12.727272033691406px">For smaller matrix sizes everything works fine but once I start increasing the number of parameters in my models and, as a result, increase the number of nodes/workers to keep up I get errors like this one:</div>


<div style="font-family:arial,sans-serif;font-size:12.727272033691406px"><br></div><div style="font-family:arial,sans-serif;font-size:12.727272033691406px"><div>[node005][[18726,1],125][btl_tcp_frag.c:215:mca_btl_tcp_frag_recv] mca_btl_tcp_frag_recv: readv failed: Connection reset by peer (104)</div>


<div>[node008][[18726,1],8][btl_tcp_frag.c:215:mca_btl_tcp_frag_recv] mca_btl_tcp_frag_recv: readv failed: Connection reset by peer (104)</div><div>[node008][[18726,1],108][btl_tcp_frag.c:215:mca_btl_tcp_frag_recv] mca_btl_tcp_frag_recv: readv failed: Connection reset by peer (104)</div>


<div>[node008][[18726,1],28][btl_tcp_frag.c:215:mca_btl_tcp_frag_recv] mca_btl_tcp_frag_recv: readv failed: Connection reset by peer (104)</div><div>[node007][[18726,1],7][btl_tcp_frag.c:215:mca_btl_tcp_frag_recv] mca_btl_tcp_frag_recv: readv failed: Connection reset by peer (104)</div>


<div>[node001][[18726,1],21][btl_tcp_frag.c:215:mca_btl_tcp_frag_recv] mca_btl_tcp_frag_recv: readv failed: Connection reset by peer (104)</div><div><br></div><div>I've also seen other network-related errors such as "unable to find path to host". Whenever I get these errors one or more of the EC2 nodes becomes "unreachable" according EC2 Web UI (even though I can log in to those nodes using internal IP aliases) Such nodes typically recover from being "unreachable" after a few minutes but my MPI job hangs anyway. The culprit is usually allgatherv but I've seen reduce and bcast to cause these errors as well.</div>


<div><br></div><div>I don't get this errors if I run on a single node (but that's way too slow even with 16 workers so I need to run my jobs on at least 20 nodes)</div><div><br></div><div>Any idea how to fix this? May be by adjusting some linux and/or OpenMPI parameters?</div>


<div><br></div><div>Any help would greatly appreciated!</div><div><br></div><div>Thanks,</div><div>Yevgeny</div></div></div></div></div>
</div><br></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote></div><br></div></body></html>
