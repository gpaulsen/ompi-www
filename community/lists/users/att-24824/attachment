<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"><base></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">Try adding -mca oob_base_verbose 10 -mca rml_base_verbose 10 to your cmd line. It looks to me like we are unable to connect back to the node where you are running mpirun for some reason.<div><br></div><div><br><div><div>On Jul 20, 2014, at 9:16 AM, Timur Ismagilov &lt;<a href="mailto:tismagilov@mail.ru">tismagilov@mail.ru</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><p>I have the same problem in openmpi 1.8.1(<span data-mce-style="font-family: verdana, arial, helvetica; text-align: -webkit-right;" style="font-family: verdana, arial, helvetica; text-align: -webkit-right;">Apr 23, 2014</span>).<br>Does the srun command have &nbsp;a --map-by&lt;foo&gt; mpirun parameter, or can i chage it from bash enviroment?</p><p><br><br>-------- Пересылаемое сообщение --------<br>От кого: Timur Ismagilov &lt;<a href="mailto:tismagilov@mail.ru">tismagilov@mail.ru</a>&gt;<br>Кому: Mike Dubman &lt;<a href="mailto:miked@dev.mellanox.co.il">miked@dev.mellanox.co.il</a>&gt;<br>Копия: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Дата: Thu, 17 Jul 2014 16:42:24 +0400<br>Тема: Re[4]: [OMPI users] Salloc and mpirun problem<br><br></p><div class="js-helper js-readmsg-msg"><base target="_self" href="https://e.mail.ru/"><div id="style_14056009440000000880_BODY"><p>With Open MPI&nbsp;1.9a1r32252 (Jul 16, 2014 (nightly snapshot tarball))&nbsp;i got this output&nbsp;(same?):</p><p>$ salloc -N2 --exclusive -p test -J ompi<br>salloc: Granted job allocation 645686<br><br>$LD_PRELOAD=/mnt/data/users/dm2/vol3/semenov/_scratch/mxm/mxm-3.0/lib/libmxm.so &nbsp;mpirun &nbsp;-mca mca_base_env_list 'LD_PRELOAD' &nbsp;--mca plm_base_verbose 10 --debug-daemons -np 1 hello_c<br></p><p>[access1:04312] mca: base: components_register: registering plm components<br>[access1:04312] mca: base: components_register: found loaded component isolated<br>[access1:04312] mca: base: components_register: component isolated has no register or open function<br>[access1:04312] mca: base: components_register: found loaded component rsh<br>[access1:04312] mca: base: components_register: component rsh register function successful<br>[access1:04312] mca: base: components_register: found loaded component slurm<br>[access1:04312] mca: base: components_register: component slurm register function successful<br>[access1:04312] mca: base: components_open: opening plm components<br>[access1:04312] mca: base: components_open: found loaded component isolated<br>[access1:04312] mca: base: components_open: component isolated open function successful<br>[access1:04312] mca: base: components_open: found loaded component rsh<br>[access1:04312] mca: base: components_open: component rsh open function successful<br>[access1:04312] mca: base: components_open: found loaded component slurm<br>[access1:04312] mca: base: components_open: component slurm open function successful<br>[access1:04312] mca:base:select: Auto-selecting plm components<br>[access1:04312] mca:base:select:( plm) Querying component [isolated]<br>[access1:04312] mca:base:select:( plm) Query of component [isolated] set priority to 0<br>[access1:04312] mca:base:select:( plm) Querying component [rsh]<br>[access1:04312] mca:base:select:( plm) Query of component [rsh] set priority to 10<br>[access1:04312] mca:base:select:( plm) Querying component [slurm]<br>[access1:04312] mca:base:select:( plm) Query of component [slurm] set priority to 75<br>[access1:04312] mca:base:select:( plm) Selected component [slurm]<br>[access1:04312] mca: base: close: component isolated closed<br>[access1:04312] mca: base: close: unloading component isolated<br>[access1:04312] mca: base: close: component rsh closed<br>[access1:04312] mca: base: close: unloading component rsh<br>Daemon was launched on node1-128-09 - beginning to initialize<br>Daemon was launched on node1-128-15 - beginning to initialize<br>Daemon [[39207,0],1] checking in as pid 26240 on host node1-128-09<br>[node1-128-09:26240] [[39207,0],1] orted: up and running - waiting for commands!<br>Daemon [[39207,0],2] checking in as pid 30129 on host node1-128-15<br>[node1-128-15:30129] [[39207,0],2] orted: up and running - waiting for commands!<br>srun: error: node1-128-09: task 0: Exited with exit code 1<br>srun: Terminating job step 645686.3<br>srun: error: node1-128-15: task 1: Exited with exit code 1<br>--------------------------------------------------------------------------<br>An ORTE daemon has unexpectedly failed after launch and before<br>communicating back to mpirun. This could be caused by a number<br>of factors, including an inability to create a connection back<br>to mpirun due to a lack of common network interfaces and/or no<br>route found between them. Please check network connectivity<br>(including firewalls and network routing requirements).<br>--------------------------------------------------------------------------<br>[access1:04312] [[39207,0],0] orted_cmd: received halt_vm cmd<br>[access1:04312] mca: base: close: component slurm closed<br>[access1:04312] mca: base: close: unloading component slurm</p><p><br><br>Thu, 17 Jul 2014 11:40:24 +0300 от Mike Dubman &lt;<a href="mailto:miked@dev.mellanox.co.il">miked@dev.mellanox.co.il</a>&gt;:<br></p><blockquote style="border-left-width: 1px; border-left-style: solid; border-left-color: rgb(8, 87, 166); margin: 10px; padding: 0px 0px 0px 10px;"><div dir="ltr">can you use latest ompi-1.8 from svn/git?<div>Ralph - could you please suggest.</div><div>Thx</div></div><div><br><br><div>On Wed, Jul 16, 2014 at 2:48 PM, Timur Ismagilov<span class="Apple-converted-space">&nbsp;</span><span dir="ltr">&lt;<a href="x-msg://e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span><span class="Apple-converted-space">&nbsp;</span>wrote:<br><blockquote style="margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: rgb(204, 204, 204); border-left-style: solid; padding-left: 1ex;"><p>Here it is:<br></p><p><br>$ LD_PRELOAD=/mnt/data/users/dm2/vol3/semenov/_scratch/mxm/mxm-3.0/lib/libmxm.so&nbsp; mpirun&nbsp; -x LD_PRELOAD --mca plm_base_verbose 10 --debug-daemons -np 1 hello_c<br><br>[access1:29064] mca: base: components_register: registering plm components<br>[access1:29064] mca: base: components_register: found loaded component isolated<br>[access1:29064] mca: base: components_register: component isolated has no register or open function<br>[access1:29064] mca: base: components_register: found loaded component rsh<br>[access1:29064] mca: base: components_register: component rsh register function successful<br>[access1:29064] mca: base: components_register: found loaded component slurm<br>[access1:29064] mca: base: components_register: component slurm register function successful<br>[access1:29064] mca: base: components_open: opening plm components<br>[access1:29064] mca: base: components_open: found loaded component isolated<br>[access1:29064] mca: base: components_open: component isolated open function successful<br>[access1:29064] mca: base: components_open: found loaded component rsh<br>[access1:29064] mca: base: components_open: component rsh open function successful<br>[access1:29064] mca: base: components_open: found loaded component slurm<br>[access1:29064] mca: base: components_open: component slurm open function successful<br>[access1:29064] mca:base:select: Auto-selecting plm components<br>[access1:29064] mca:base:select:(&nbsp; plm) Querying component [isolated]<br>[access1:29064] mca:base:select:(&nbsp; plm) Query of component [isolated] set priority to 0<br>[access1:29064] mca:base:select:(&nbsp; plm) Querying component [rsh]<br>[access1:29064] mca:base:select:(&nbsp; plm) Query of component [rsh] set priority to 10<br>[access1:29064] mca:base:select:(&nbsp; plm) Querying component [slurm]<br>[access1:29064] mca:base:select:(&nbsp; plm) Query of component [slurm] set priority to 75<br>[access1:29064] mca:base:select:(&nbsp; plm) Selected component [slurm]<br>[access1:29064] mca: base: close: component isolated closed<br>[access1:29064] mca: base: close: unloading component isolated<br>[access1:29064] mca: base: close: component rsh closed<br>[access1:29064] mca: base: close: unloading component rsh<br>Daemon was launched on node1-128-17 - beginning to initialize<br>Daemon was launched on node1-128-18 - beginning to initialize<br>Daemon [[63607,0],2] checking in as pid 24538 on host node1-128-18<br>[node1-128-18:24538] [[63607,0],2] orted: up and running - waiting for commands!<br>Daemon [[63607,0],1] checking in as pid 17192 on host node1-128-17<br>[node1-128-17:17192] [[63607,0],1] orted: up and running - waiting for commands!<br>srun: error: node1-128-18: task 1: Exited with exit code 1<br>srun: Terminating job step 645191.1<br>srun: error: node1-128-17: task 0: Exited with exit code 1</p><div><br>--------------------------------------------------------------------------<br>An ORTE daemon has unexpectedly failed after launch and before<br>communicating back to mpirun. This could be caused by a number<br>of factors, including an inability to create a connection back<br>to mpirun due to a lack of common network interfaces and/or no<br>route found between them. Please check network connectivity<br>(including firewalls and network routing requirements).<br>--------------------------------------------------------------------------<br></div>[access1:29064] [[63607,0],0] orted_cmd: received halt_vm cmd<br>[access1:29064] mca: base: close: component slurm closed<br>[access1:29064] mca: base: close: unloading component slurm<br><br><br>Wed, 16 Jul 2014 14:20:33 +0300 от Mike Dubman &lt;<a href="x-msg://e.mail.ru/compose/?mailto=mailto%3amiked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt;:<br><div><br class="webkit-block-placeholder"></div><div><blockquote style="border-left-width: 1px; border-left-style: solid; border-left-color: rgb(8, 87, 166); margin: 10px; padding: 0px 0px 0px 10px;"><div dir="ltr">please add following flags to mpirun "--mca plm_base_verbose 10 --debug-daemons" and attach output.<div>Thx</div></div><div><br><br><div>On Wed, Jul 16, 2014 at 11:12 AM, Timur Ismagilov<span class="Apple-converted-space">&nbsp;</span><span dir="ltr">&lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span><span class="Apple-converted-space">&nbsp;</span>wrote:<br><blockquote style="margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: rgb(204, 204, 204); border-left-style: solid; padding-left: 1ex;"><div><p>Hello!<br>I have&nbsp;Open MPI v1.9a1r32142 and slurm 2.5.6.<br><br>I can not use mpirun after salloc:<br><br>$salloc -N2 --exclusive -p test -J ompi<br>$LD_PRELOAD=/mnt/data/users/dm2/vol3/semenov/_scratch/mxm/mxm-3.0/lib/libmxm.so mpirun -np 1 hello_c<br>-----------------------------------------------------------------------------------------------------<br>An ORTE daemon has unexpectedly failed after launch and before<br>communicating back to mpirun. This could be caused by a number<br>of factors, including an inability to create a connection back<br>to mpirun due to a lack of common network interfaces and/or no<br>route found between them. Please check network connectivity<br>(including firewalls and network routing requirements).<br>------------------------------------------------------------------------------------------------------<br>But if i use mpirun in sbutch script it looks correct:<br>$cat ompi_mxm3.0<br>#!/bin/sh<br>LD_PRELOAD=/mnt/data/users/dm2/vol3/semenov/_scratch/mxm/mxm-3.0/lib/libmxm.so&nbsp; mpirun&nbsp; -x LD_PRELOAD -x MXM_SHM_KCOPY_MODE=off --map-by slot:pe=8 "$@"<br><br>$sbatch -N2&nbsp; --exclusive -p test -J ompi&nbsp; ompi_mxm3.0 ./hello_c<br>Submitted batch job 645039<br>$cat slurm-645039.out<span class="Apple-converted-space">&nbsp;</span><br>[warn] Epoll ADD(1) on fd 0 failed.&nbsp; Old events were 0; read change was 1 (add); write change was 0 (none): Operation not permitted<br>[warn] Epoll ADD(4) on fd 1 failed.&nbsp; Old events were 0; read change was 0 (none); write change was 1 (add): Operation not permitted<br>Hello, world, I am 0 of 2, (Open MPI v1.9a1, package: Open MPI semenov@compiler-2 Distribution, ident: 1.9a1r32142, repo rev: r32142, Jul 04, 2014 (nightly snapshot tarball), 146)<br>Hello, world, I am 1 of 2, (Open MPI v1.9a1, package: Open MPI semenov@compiler-2 Distribution, ident: 1.9a1r32142, repo rev: r32142, Jul 04, 2014 (nightly snapshot tarball), 146)<br><br>Regards,<br>Timur</p></div><br>_______________________________________________<br>users mailing list<br><a href="https://e.mail.ru/compose/?mailto=mailto%3ausers@open%2dmpi.org" target="_blank">users@open-mpi.org</a><br>Subscription:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/07/24777.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/07/24777.php</a><br></blockquote></div><br></div></blockquote><br><br><br></div></blockquote></div><br></div></blockquote><br><br><br></div><base target="_self" href="https://e.mail.ru/"></div><br><hr><br><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subscription:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/07/24823.php">http://www.open-mpi.org/community/lists/users/2014/07/24823.php</a></div></blockquote></div><br></div></body></html>
