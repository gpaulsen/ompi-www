<div dir="ltr">what is your command line and setup? (ofed version, distro)<div><br></div><div>This is what was just measured w/ fdr on haswell with v1.8.8 and mxm and UD</div><div><br></div><div><div>+ mpirun -np 2 -bind-to core -display-map -mca rmaps_base_mapping_policy dist:span -x MXM_RDMA_PORTS=mlx5_3:1 -mca rmaps_dist_device mlx5_3:1  -x MXM_TLS=self,shm,ud osu_latency</div><div> Data for JOB [65499,1] offset 0</div><div><br></div><div> ========================   JOB MAP   ========================</div><div><br></div><div> Data for node: clx-orion-001   Num slots: 28   Max slots: 0    Num procs: 1</div><div>        Process OMPI jobid: [65499,1] App: 0 Process rank: 0</div><div><br></div><div> Data for node: clx-orion-002   Num slots: 28   Max slots: 0    Num procs: 1</div><div>        Process OMPI jobid: [65499,1] App: 0 Process rank: 1</div><div><br></div><div> =============================================================</div><div># OSU MPI Latency Test v4.4.1<br></div><div># Size          Latency (us)</div><div>0                       1.18</div><div>1                       1.16</div><div>2                       1.19</div><div>4                       1.20</div><div>8                       1.19</div><div>16                      1.19</div><div>32                      1.21</div><div>64                      1.27</div><div><br></div></div><div><br></div><div>and w/ ob1, openib btl:</div><div><br></div><div>mpirun -np 2 -bind-to core -display-map -mca rmaps_base_mapping_policy dist:span  -mca rmaps_dist_device mlx5_3:1  -mca btl_if_include mlx5_3:1 -mca pml ob1 -mca btl openib,self osu_latency<br></div><div><br></div><div><div># OSU MPI Latency Test v4.4.1</div><div># Size          Latency (us)</div><div>0                       1.13</div><div>1                       1.17</div><div>2                       1.17</div><div>4                       1.17</div><div>8                       1.22</div><div>16                      1.23</div><div>32                      1.25</div><div>64                      1.28</div></div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Tue, Sep 29, 2015 at 6:49 PM, Dave Love <span dir="ltr">&lt;<a href="mailto:d.love@liverpool.ac.uk" target="_blank">d.love@liverpool.ac.uk</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">I&#39;ve just compared IB p2p latency between version 1.6.5 and 1.8.8.  I&#39;m<br>
surprised to find that 1.8 is rather worse, as below.  Assuming that&#39;s<br>
not expected, are there any suggestions for debugging it?<br>
<br>
This is with FDR Mellanox, between two Sandybridge nodes on the same<br>
blade chassis switch.  The results are similar for IMB pingpong and<br>
osu_latency, and reproducible.  I&#39;m running both cases the same way as<br>
far as I can tell (e.g. core binding with 1.6 and not turning it off<br>
with 1.8) just rebuilding the test against between OMPI versions.<br>
<br>
The initial osu_latency figures for 1.6 are:<br>
<br>
  # OSU MPI Latency Test v5.0<br>
  # Size          Latency (us)<br>
  0                       1.16<br>
  1                       1.24<br>
  2                       1.23<br>
  4                       1.23<br>
  8                       1.26<br>
  16                      1.27<br>
  32                      1.30<br>
  64                      1.36<br>
<br>
and for 1.8:<br>
<br>
  # OSU MPI Latency Test v5.0<br>
  # Size          Latency (us)<br>
  0                       1.48<br>
  1                       1.46<br>
  2                       1.42<br>
  4                       1.43<br>
  8                       1.46<br>
  16                      1.47<br>
  32                      1.48<br>
  64                      1.54<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/09/27712.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/09/27712.php</a><br>
</blockquote></div><br><br clear="all"><div><br></div>-- <br><div class="gmail_signature"><div dir="ltr"><br><div>Kind Regards,</div><div><br></div><div>M.</div></div></div>
</div>
