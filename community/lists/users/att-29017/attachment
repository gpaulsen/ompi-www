As far as I understand, the tcp btl is ok<div><br></div>Cheers,<div><br></div><div>Gilles<br><div><br>On Monday, April 25, 2016, dpchoudh . &lt;<a href="mailto:dpchoudh@gmail.com">dpchoudh@gmail.com</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><div><div>Hello Gilles<br><br></div>That idea crossed my mind as well, but I was under the impression that MPI_THREAD_MULTIPLE is not very well supported on OpenMPI? I believe it is not supported on OpenIB, but the original poster seems to be using TCP. Does it work for TCP?<br><br></div>Thanks<br></div>Durga<br></div><div class="gmail_extra"><br clear="all"><div><div><div dir="ltr"><div>1% of the executables have 99% of CPU privilege!<br></div>Userspace code! Unite!! Occupy the kernel!!!<br></div></div></div>
<br><div class="gmail_quote">On Sun, Apr 24, 2016 at 10:48 AM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;gilles.gouaillardet@gmail.com&#39;);" target="_blank">gilles.gouaillardet@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">an other option is to use MPI_THREAD_MULTIPLE, and MPI_Recv() on the master taskÂ in a dedicated thread, and use a unique tag (or MPI_Comm_dup() MPI_COMM_WORLD) to separate the traffic.<div><br></div><div>If this is not the desired design, then the master task has to post MPI_Irecv() and &quot;poll&quot; with MPI_Probe() / MPI_Test() and friends.</div><div>Note it is possible to use non blocking collective (MPI_Ibcast(), MPI_Iscatter() and MPI_Igather()) and &quot;both&quot; collective and the progress statuses</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<br><br>On Sunday, April 24, 2016, dpchoudh . &lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;dpchoudh@gmail.com&#39;);" target="_blank">dpchoudh@gmail.com</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><div><div>Hello<br><br><br></div>I am not sure I am understanding your requirements correctly, but base on what I think it is, how about this: you do an MPI_Send() from all the non-root nodes to the root node and pack all the progress related data into this send. Use a special tag for this message to make it stand out from &#39;regular&#39; sends. The root node does a non-blocking receive on this tag from all the nodes that it is expecting this message from.<br><br></div>Would that work?<br><br></div>Durga<br><div><div><br></div></div></div><div class="gmail_extra"><br clear="all"><div><div><div dir="ltr"><div>1% of the executables have 99% of CPU privilege!<br></div>Userspace code! Unite!! Occupy the kernel!!!<br></div></div></div>
<br><div class="gmail_quote">On Sun, Apr 24, 2016 at 7:05 AM, MM <span dir="ltr">&lt;<a>finjulhich@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Hello,<div><br></div><div>With a miniature case of 3 linux quadcore boxes, linked via 1Gbit Ethernet, I have a UI that runs on 1 of the 3 boxes, and that is the root of the communicator.</div><div>I have a 1-second-running function on up to 10 parameters, my parameter space fits in the memory of the root, the space of it is N ~~ 1 million.<br><br>I use broadcast/scatter/gather to collect the value of my function on each of the 1million points, dividing 1million by the number of nodes (11: rootnode has 1 core/thread assigned to the UI, 1 core/thread for its evaluation of the function on its own part of the parameter space and 2 other cores run non-root nodes, and the 2 other boxes all run non-root nodes)</div><div><br></div><div>the rootnode does:</div><div>1. broadcast needed data</div><div>2. scatter param space</div><div>3. evaluate function locally</div><div>4. gather results from this and all other nodes</div><div><br></div><div>How would I go about having the non-root nodes send a sort of progress status to the root node? that&#39;s used for plotting results on the root box as soon as they are available?</div><div><br></div><div>Rds,</div><div><br></div><div><br></div><div><br></div><div><br></div></div>
<br>_______________________________________________<br>
users mailing list<br>
<a>users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/04/29013.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/29013.php</a><br></blockquote></div><br></div>
</blockquote></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;users@open-mpi.org&#39;);" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/04/29015.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/29015.php</a><br></blockquote></div><br></div>
</blockquote></div></div>

