<div dir="ltr">On Sun, Feb 7, 2016 at 8:29 AM, Jeff Squyres (jsquyres) &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br>&gt;<br>&gt; On Feb 4, 2016, at 9:46 PM, Brian Taylor &lt;<a href="mailto:spam.brian.taylor@gmail.com">spam.brian.taylor@gmail.com</a>&gt; wrote:<br>&gt; &gt;<br>&gt; &gt; Thanks for the explanation, Jeff.  I&#39;m not surprised to hear that using a Fortran type from C in this manner is potentially buggy and not portable.  However, assuming that the C and Fortran types are interoperable, is there any guarantee that the call to MPI_Reduce in the program above will execute successfully?<br>&gt;<br>&gt; If the representations of the C and Fortran datatypes are the same, then yes, it should work.<br>&gt;<br>&gt; &gt; If OpenMPI 1.10.2 is built with Fortran support, the program above runs and gives the expected output.  If OpenMPI 1.10.2 is built without Fortran support, the program exits with the following error:<br>&gt; &gt;<br>&gt; &gt; taylor@host $ mpirun -np 1 ./bug<br>&gt; &gt; [host:49234] *** An error occurred in MPI_Reduce: the reduction operation MPI_MAXLOC is not defined on the MPI_2DBLPREC datatype<br>&gt; &gt; [host:49234] *** reported by process [3133079553,0]<br>&gt; &gt; [host:49234] *** on communicator MPI_COMM_WORLD<br>&gt; &gt; [host:49234] *** MPI_ERR_OP: invalid reduce operation<br>&gt; &gt; [host:49234] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>&gt; &gt; [host:49234] ***    and potentially your MPI job)<br>&gt;<br>&gt; I suppose that error message is a bit misleading -- it&#39;s not available because we disabled *all* Fortran support, including support for Fortran data types.  So yes, MPI_MAXLOC is not defined for MPI2DBLPREC, but *because* there&#39;s no Fortran support *at all*.<br>&gt;<br>&gt; &gt; It seems that the MPI_MAXLOC operator for MPI_2DOUBLE_PRECISION is not available if OpenMPI is built without Fortran support; thus, the call to MPI_Reduce fails.  Is this the expected behavior?  Is the MPI_MAXLOC operator for MPI_2DOUBLE_PRECISION required to be available from C for compliance with the MPI standard, or is its availability from C in OpenMPI (when built with Fortran support) an implementation-dependent &quot;extension&quot;?<br>&gt;<br>&gt; If there&#39;s no Fortran compiler, Open MPI can&#39;t possibly know what the Fortran representation of the Fortran datatypes.<br><br>You might think that&#39;s the case - it certainly makes sense to me! - but the following program suggests that OpenMPI &quot;knows&quot; more than you think.  Consider the following program:<br><br>#include &lt;stdio.h&gt;<br>#include &lt;mpi.h&gt;<br><br>int main(int argc, char **argv)<br>{<br>  int rank, size;<br><br>  MPI_Init(&amp;argc, &amp;argv);<br>  MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);<br><br>  MPI_Type_size(MPI_2DOUBLE_PRECISION, &amp;size);<br>  if (rank == 0) printf(&quot;MPI_2DOUBLE_PRECISION is %d bytes\n&quot;, size);<br><br>  MPI_Finalize();<br><br>  return 0;<br>}<br><br>When compiled and linked with OpenMPI 1.10.2 built without Fortran support, the program runs successfully:<br><br>taylor@host $ mpicc main.c -o bug<br>taylor@host $ mpirun -np 1 ./bug<br>MPI_2DOUBLE_PRECISION is 16 bytes<br><br>It certainly appears that MPI_2DOUBLE_PRECISION is a valid type - at least it has a sensible size.  I would expect that when OpenMPI is built with Fortran support, it would either not define the Fortran types or set them to MPI_TYPE_NULL.  The former would prevent C code that uses Fortran types from compiling when there is no Fortran support, while the latter would hopefully lead to more sensible run time error messages.<br><br>&gt;<br>&gt; BTW: is there a reason you don&#39;t want to just use the C datatypes?  The fundamental output of the index is an integer value -- casting it to a float of some flavor doesn&#39;t fundamentally change its value.<div><br></div><div>The code in question is not mine.  I have suggested to the developers that they should use the correct C types.  The reason I became aware of this issue is that one of my colleagues compiled and ran this code on a system where OpenMPI was built without Fortran support and the code started failing with errors from MPI which were not seen on other systems.</div><div><br>&gt;<br>&gt; --<br>&gt; Jeff Squyres<br>&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>&gt; For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>&gt;</div><div><br></div><div>Thanks,</div><div>Brian<br><br></div></div>

