<HTML><HEAD>
<META content="text/html; charset=utf-8" http-equiv=Content-Type>
<META name=GENERATOR content="MSHTML 8.00.6001.18812"></HEAD>
<BODY style="MARGIN: 4px 4px 1px; FONT: 10pt Tahoma; WORD-WRAP: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space">
<DIV>Thanks thats it!</DIV>
<DIV>&nbsp;</DIV>
<DIV>Would have been straigth forward, but there is a lot of things to consider by setting up a cluster the first time - a lot to oversee.</DIV>
<DIV>&nbsp;</DIV>
<DIV>Anyway thanks for your help.<BR><BR>&gt;&gt;&gt; Ralph Castain &lt;rhc@open-mpi.org&gt; 18.11.2009 15:57 &gt;&gt;&gt;<BR>Bingo! This is why we ask for info on how you configure OMPI :-)</DIV>
<DIV><BR></DIV>
<DIV>You need to rebuild OMPI with --enable-heterogeneous. Because there is additional overhead associated with running hetero configurations, and so few people do so, it is disabled by default.</DIV>
<DIV><BR></DIV>
<DIV><BR></DIV>
<DIV>
<DIV>
<DIV>On Nov 18, 2009, at 2:55 AM, Laurin Müller wrote:</DIV><BR class=Apple-interchange-newline>
<BLOCKQUOTE type="cite">
<DIV style="MARGIN: 4px 4px 1px; FONT: 10pt Tahoma">
<DIV>Now i have the same openmpi versions. 1.3.2</DIV>
<DIV>&nbsp;</DIV>
<DIV>recalulated on both nodes and it works again on each node seperatly:</DIV>
<DIV>&nbsp;</DIV>
<DIV>node1:</DIV>
<DIV>cluster@bioclust:/mnt/projects/PS3Cluster/Benchmark$ mpirun --version<BR>mpirun (Open MPI) 1.3.2</DIV>
<DIV><A href="mailto:1.3.2cluster@bioclust:/mnt/projects/PS3Cluster/Benchmark$">cluster@bioclust:/mnt/projects/PS3Cluster/Benchmark$</A> mpirun --hostfile /etc/openmpi/openmpi-default-hostfile -np 4 /mnt/projects/PS3Cluster/Benchmark/pi<BR>Input number of intervals:<BR>20<BR>1: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.798498008827023<BR>2: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.773339953424083<BR>3: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.747089984650041<BR>0: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.822248040052981<BR>pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.141175986954128<BR></DIV>
<DIV>node2 (PS3):</DIV>
<DIV><A href="mailto:root@kasimir:/mnt/projects/PS3Cluster/Benchmark">root@kasimir:/mnt/projects/PS3Cluster/Benchmark</A># mpirun --version<BR>mpirun (Open MPI) 1.3.2</DIV>
<DIV>[...]</DIV>
<DIV><A href="mailto:root@kasimir:/mnt/projects/PS3Cluster/Benchmark">root@kasimir:/mnt/projects/PS3Cluster/Benchmark</A># mpirun -np 2 pi<BR>Input number of intervals:<BR>20<BR>0: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.595587993477064<BR>1: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.545587993477064<BR>pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.141175986954128<BR></DIV>
<DIV>BUT when i start it on node1 with more than 16 processes and hostfile. i get this errors:</DIV>
<DIV><A href="mailto:cluster@bioclust:/mnt/projects/PS3Cluster/Benchmark$">cluster@bioclust:/mnt/projects/PS3Cluster/Benchmark$</A> mpirun --hostfile /etc/openmpi/openmpi-default-hostfile -np 17 /mnt/projects/PS3Cluster/Benchmark/pi<BR>--------------------------------------------------------------------------<BR>This installation of Open MPI was configured without support for<BR>heterogeneous architectures, but at least one node in the allocation<BR>was detected to have a different architecture. The detected node was:</DIV>
<DIV>&nbsp;</DIV>
<DIV>Node: bioclust</DIV>
<DIV>&nbsp;</DIV>
<DIV>In order to operate in a heterogeneous environment, please reconfigure<BR>Open MPI with --enable-heterogeneous.<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>It looks like MPI_INIT failed for some reason; your parallel process is<BR>likely to abort.&nbsp; There are many reasons that a parallel process can<BR>fail during MPI_INIT; some of which are due to configuration or environment<BR>problems.&nbsp; This failure appears to be an internal failure; here's some<BR>additional information (which may only be relevant to an Open MPI<BR>developer):</DIV>
<DIV>&nbsp;</DIV>
<DIV>&nbsp; ompi_proc_set_arch failed<BR>&nbsp; --&gt; Returned "Not supported" (-8) instead of "Success" (0)<BR>--------------------------------------------------------------------------<BR>*** An error occurred in MPI_Init<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[bioclust:1239] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<BR>*** An error occurred in MPI_Init<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[bioclust:1240] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<BR>*** An error occurred in MPI_Init<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[bioclust:1241] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<BR>*** An error occurred in MPI_Init<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[bioclust:1242] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<BR>*** An error occurred in MPI_Init<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[bioclust:1244] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<BR>*** An error occurred in MPI_Init<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[bioclust:1245] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<BR>*** An error occurred in MPI_Init<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[bioclust:1246] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<BR>*** An error occurred in MPI_Init<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[bioclust:1247] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<BR>*** An error occurred in MPI_Init<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[bioclust:1248] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<BR>*** An error occurred in MPI_Init<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[bioclust:1250] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<BR>*** An error occurred in MPI_Init<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[bioclust:1251] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<BR>*** An error occurred in MPI_Init<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[bioclust:1238] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<BR>*** An error occurred in MPI_Init<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[kasimir:12678] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<BR>*** An error occurred in MPI_Init<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[bioclust:1243] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<BR>*** An error occurred in MPI_Init<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[bioclust:1249] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<BR>*** An error occurred in MPI_Init<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[bioclust:1252] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<BR>*** An error occurred in MPI_Init<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[bioclust:1253] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<BR>--------------------------------------------------------------------------<BR>mpirun has exited due to process rank 16 with PID 12678 on<BR>node 10.4.1.23 exiting without calling "finalize". This may<BR>have caused other processes in the application to be<BR>terminated by signals sent by mpirun (as reported here).<BR>--------------------------------------------------------------------------<BR>[bioclust:01236] 16 more processes have sent help message help-mpi-runtime / heterogeneous-support-unavailable<BR>[bioclust:01236] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages<BR>[bioclust:01236] 16 more processes have sent help message help-mpi-runtime / mpi_init:startup:internal-failure<BR></DIV>
<DIV>&nbsp;</DIV>
<DIV>&nbsp;</DIV>
<DIV>&nbsp;</DIV>
<DIV><BR><BR>&gt;&gt;&gt; Lenny Verkhovsky &lt;<A href="mailto:lenny.verkhovsky@gmail.com">lenny.verkhovsky@gmail.com</A>&gt; 17.11.2009 16:52 &gt;&gt;&gt;<BR></DIV>
<DIV dir=ltr>
<DIV>I noticed that you also have different versions of OMPI. You have 1.3.2 on node1 and 1.3 on node2.</DIV>
<DIV>can you try to put same versions of OMPI on both nodes.</DIV>
<DIV>can you also try running np 16 on node1 when you try running separately.</DIV>
<DIV></DIV>
<DIV>Lenny.</DIV><BR>
<DIV class=gmail_quote>On Tue, Nov 17, 2009 at 5:45 PM, Laurin Müller <SPAN dir=ltr>&lt;<A href="mailto:laurin.mueller@umit.at">laurin.mueller@umit.at</A>&gt;</SPAN> wrote:<BR>
<BLOCKQUOTE style="BORDER-LEFT: #ccc 1px solid; MARGIN: 0px 0px 0px 0.8ex; PADDING-LEFT: 1ex" class=gmail_quote>
<DIV style="FONT-FAMILY: Tahoma, sans-serif; FONT-SIZE: 13px"><BR><BR>&gt;&gt;&gt; Ralph Castain 11/17/09 4:04 PM &gt;&gt;&gt; 
<DIV class=im><BR>&gt;Your cmd line is telling OMPI to run 17 processes. Since your hostfile indicates that only 16 of them are to &gt;run on 10.4.23.107 (which I assume is your PS3 node?), 1 process is going to be run on 10.4.1.23 (I assume &gt;this is node1?).<BR></DIV>node1 has 16 Cores (4 x AMD Quad Core Processors)<BR><BR>node2 is the ps3 with two processors (slots) 
<DIV class=im><BR>
<DIV><BR></DIV>
<DIV>&gt;I would guess that the executable is compiled to run on the PS3 given your specified path, so I would &gt;expect it to bomb on node1 - which is exactly what appears to be happening.</DIV></DIV>
<DIV>the executable is compiled on each node separately and lies at each node in the same directory 
<DIV class=im><BR>/mnt/projects/PS3Cluster/Benchmark/pi<BR></DIV>on each node different directories are mounted. so there exists a separate executable file compiled at each node.<BR><BR>in the end i want to ran R on this cluster with Rmpi - as i get a similar problem there i rist wanted to try with an c programm.<BR><BR>with r happens the same thing it works when i start it on each node but if i want to start more than 16 processes on node one in exits.<BR><BR></DIV>
<DIV>
<DIV class=h5>
<DIV><BR>
<DIV>
<DIV>On Nov 17, 2009, at 1:59 AM, Laurin Müller wrote:</DIV><BR>
<BLOCKQUOTE type="cite">
<DIV style="MARGIN: 4px 4px 1px; FONT: 10pt Tahoma; font-size-adjust: none; font-stretch: normal">
<DIV>Hi,</DIV>
<DIV></DIV>
<DIV>i want to build a cluster with openmpi.</DIV>
<DIV></DIV>
<DIV>2 nodes:</DIV>
<DIV>node 1: 4 x Amd Quad Core, ubuntu 9.04, openmpi 1.3.2</DIV>
<DIV>node 2: Sony PS3, ubuntu 9.04, openmpi 1.3</DIV>
<DIV></DIV>
<DIV>both can connect with ssh to each other and to itself without passwd.</DIV>
<DIV></DIV>
<DIV>I can run the sample proramm pi.c on both nodes seperatly (see below). But if i try to start it on node1 with --hostfile option to use node 2 "remote" i got this error:</DIV>
<DIV></DIV>
<DIV><A href="mailto:cluster@bioclust:%7E$" target=_blank>cluster@bioclust:~$</A> mpirun --hostfile /etc/openmpi/openmpi-default-hostfile -np 17 /mnt/projects/PS3Cluster/Benchmark/pi<BR>--------------------------------------------------------------------------<BR>mpirun noticed that the job aborted, but has no info as to the process<BR>that caused that situation.<BR>--------------------------------------------------------------------------<BR></DIV>
<DIV>my hostfile:</DIV>
<DIV><A href="mailto:cluster@bioclust:%7E$" target=_blank>cluster@bioclust:~$</A> cat /etc/openmpi/openmpi-default-hostfile</DIV>
<DIV>10.4.23.107 slots=16<BR>10.4.1.23 slots=2<BR></DIV>
<DIV>i can see with top that the processors of node2 begin to work shortly, then it apports on node1.</DIV>
<DIV></DIV>
<DIV>I use this sample/test program:</DIV>
<DIV>#include &lt;stdio.h&gt;<BR>#include &lt;stdlib.h&gt;</DIV>
<DIV>#include "mpi.h"</DIV>
<DIV>int main(int argc, char *argv[])<BR>{<BR>int i, n;<BR>double h, pi, x;</DIV>
<DIV>int me, nprocs;<BR>double piece;</DIV>
<DIV>/* --------------------------------------------------- */</DIV>
<DIV>MPI_Init (&amp;argc, &amp;argv);</DIV>
<DIV>MPI_Comm_size (MPI_COMM_WORLD, &amp;nprocs);<BR>MPI_Comm_rank (MPI_COMM_WORLD, &amp;me);</DIV>
<DIV>/* --------------------------------------------------- */</DIV>
<DIV>if (me == 0)<BR>{<BR>printf("%s", "Input number of intervals:\n");<BR>scanf ("%d", &amp;n);<BR>}</DIV>
<DIV>/* --------------------------------------------------- */</DIV>
<DIV>MPI_Bcast (&amp;n, 1, MPI_INT, 0, MPI_COMM_WORLD);</DIV>
<DIV>/* --------------------------------------------------- */</DIV>
<DIV>h = 1. / (double) n;</DIV>
<DIV>piece = 0.;</DIV>
<DIV>for (i=me+1; i &lt;= n; i+=nprocs)<BR>{<BR>x = (i-1)*h;</DIV>
<DIV>piece = piece + ( 4/(1+(x)*(x)) + 4/(1+(x+h)*(x+h))) / 2 * h;<BR>}</DIV>
<DIV>printf("%d: pi = %25.15f\n", me, piece);</DIV>
<DIV>/* --------------------------------------------------- */</DIV>
<DIV>MPI_Reduce (&amp;piece, &amp;pi, 1, MPI_DOUBLE,<BR>MPI_SUM, 0, MPI_COMM_WORLD);</DIV>
<DIV>/* --------------------------------------------------- */</DIV>
<DIV>if (me == 0)<BR>{<BR>printf("pi = %25.15f\n", pi);<BR>}</DIV>
<DIV>/* --------------------------------------------------- */</DIV>
<DIV>MPI_Finalize();</DIV>
<DIV>return 0;<BR>}<BR></DIV>
<DIV>it works on each node.</DIV>
<DIV>node1:</DIV>
<DIV><A href="mailto:cluster@bioclust:%7E$" target=_blank>cluster@bioclust:~$</A> mpirun -np 4 /mnt/projects/PS3Cluster/Benchmark/piInput number of intervals:<BR>20<BR>0: pi = 0.822248040052981<BR>2: pi = 0.773339953424083<BR>3: pi = 0.747089984650041<BR>1: pi = 0.798498008827023<BR>pi = 3.141175986954128</DIV>
<DIV></DIV>
<DIV>node2:</DIV>
<DIV><A href="mailto:cluster@kasimir:%7E$" target=_blank>cluster@kasimir:~$</A> mpirun -np 2 /mnt/projects/PS3Cluster/Benchmark/pi<BR>Input number of intervals:<BR>5<BR>1: pi = 1.267463056905495<BR>0: pi = 1.867463056905495<BR>pi = 3.134926113810990<BR><A href="mailto:cluster@kasimir:%7E$" target=_blank>cluster@kasimir:~$</A> </DIV>
<DIV></DIV>
<DIV>Thx in advance,</DIV>
<DIV>Laurin<BR></DIV>
<DIV><BR></DIV>
<DIV></DIV></DIV>_______________________________________________<BR>users mailing list<BR><A href="mailto:users@open-mpi.org" target=_blank>users@open-mpi.org</A><BR><A href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target=_blank>http://www.open-mpi.org/mailman/listinfo.cgi/users</A></BLOCKQUOTE></DIV><BR></DIV></DIV></DIV></DIV><BR>_______________________________________________<BR>users mailing list<BR><A href="mailto:users@open-mpi.org">users@open-mpi.org</A><BR><A href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target=_blank>http://www.open-mpi.org/mailman/listinfo.cgi/users</A><BR></BLOCKQUOTE></DIV><BR></DIV></DIV>_______________________________________________<BR>users mailing list<BR><A href="mailto:users@open-mpi.org">users@open-mpi.org</A><BR>http://www.open-mpi.org/mailman/listinfo.cgi/users</BLOCKQUOTE></DIV><BR></DIV></BODY></HTML>

