<html><head><meta http-equiv="Content-Type" content="text/html charset=us-ascii"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">On&nbsp;&nbsp;<a href="https://github.com/open-mpi/ompi/pull/1385" class="">https://github.com/open-mpi/ompi/pull/1385</a>&nbsp;Gilles indicated he would update the patch and commit it on Monday<div class=""><br class=""><div class=""><br class=""><div style=""><blockquote type="cite" class=""><div class="">On Feb 20, 2016, at 12:48 AM, Siegmar Gross &lt;<a href="mailto:Siegmar.Gross@informatik.hs-fulda.de" class="">Siegmar.Gross@informatik.hs-fulda.de</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><div class="">Hi Gilles,<br class=""><br class="">do you know, when fixes for the problems will be ready? They still exist<br class="">in the current version.<br class=""><br class=""><br class="">tyr spawn 136 ompi_info | grep -e "Open MPI repo revision" -e "C compiler absolute"<br class=""> &nbsp;Open MPI repo revision: v2.x-dev-1108-gaaf15d9<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;C compiler absolute: /usr/local/gcc-5.1.0/bin/gcc<br class=""><br class=""><br class="">tyr spawn 137 mpiexec -np 1 --host tyr,sunpc1 spawn_multiple_master<br class=""><br class="">Parent process 0 running on <a href="http://tyr.informatik.hs-fulda.de" class="">tyr.informatik.hs-fulda.de</a><br class=""> &nbsp;I create 3 slave processes.<br class=""><br class="">[<a href="http://tyr.informatik.hs-fulda.de" class="">tyr.informatik.hs-fulda.de</a>:23580] PMIX ERROR: UNPACK-PAST-END in file ../../../../../../openmpi-v2.x-dev-1108-gaaf15d9/opal/mca/pmix/pmix112/pmix/src/server/pmix_server_ops.c at line 829<br class="">[<a href="http://tyr.informatik.hs-fulda.de" class="">tyr.informatik.hs-fulda.de</a>:23580] PMIX ERROR: UNPACK-PAST-END in file ../../../../../../openmpi-v2.x-dev-1108-gaaf15d9/opal/mca/pmix/pmix112/pmix/src/server/pmix_server.c at line 2176<br class="">[tyr:23587] *** An error occurred in MPI_Comm_spawn_multiple<br class="">[tyr:23587] *** reported by process [4198105089,0]<br class="">[tyr:23587] *** on communicator MPI_COMM_WORLD<br class="">[tyr:23587] *** MPI_ERR_SPAWN: could not spawn processes<br class="">[tyr:23587] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br class="">[tyr:23587] *** &nbsp;&nbsp;&nbsp;and potentially your MPI job)<br class="">tyr spawn 138<br class=""><br class=""><br class=""><br class=""><br class="">tyr spawn 115 ompi_info | grep -e "Open MPI repo revision" -e "C compiler absolute"<br class=""> &nbsp;Open MPI repo revision: v2.x-dev-1108-gaaf15d9<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;C compiler absolute: /opt/solstudio12.4/bin/cc<br class=""><br class=""><br class="">tyr spawn 116 mpiexec -np 1 --host tyr,sunpc1 spawn_multiple_master<br class="">[<a href="http://tyr.informatik.hs-fulda.de" class="">tyr.informatik.hs-fulda.de</a>:28715] [[54797,0],0] ORTE_ERROR_LOG: Not found in file ../../../../../openmpi-v2.x-dev-1108-gaaf15d9/orte/mca/ess/hnp/ess_hnp_module.c at line 638<br class="">--------------------------------------------------------------------------<br class="">It looks like orte_init failed for some reason; your parallel process is<br class="">likely to abort. &nbsp;There are many reasons that a parallel process can<br class="">fail during orte_init; some of which are due to configuration or<br class="">environment problems. &nbsp;This failure appears to be an internal failure;<br class="">here's some additional information (which may only be relevant to an<br class="">Open MPI developer):<br class=""><br class=""> &nbsp;opal_pmix_base_select failed<br class=""> &nbsp;--&gt; Returned value Not found (-13) instead of ORTE_SUCCESS<br class="">--------------------------------------------------------------------------<br class="">tyr spawn 117<br class=""><br class=""><br class=""><br class="">Kind regards<br class=""><br class="">Siegmar<br class=""><br class=""><br class=""><br class="">On 01/15/16 08:03, Gilles Gouaillardet wrote:<br class=""><blockquote type="cite" class="">Siegmar,<br class=""><br class="">the fix is now being discussed at <a href="https://github.com/open-mpi/ompi/pull/1285" class="">https://github.com/open-mpi/ompi/pull/1285</a><br class=""><br class="">the other error your reported (MPI_Comm_spawn hanging on an heterogeneous cluster) is<br class="">being discussed at <a href="https://github.com/open-mpi/ompi/pull/1292" class="">https://github.com/open-mpi/ompi/pull/1292</a><br class=""><br class=""><br class="">Cheers,<br class=""><br class="">Gilles<br class=""><br class="">On 1/14/2016 11:06 PM, Siegmar Gross wrote:<br class=""><blockquote type="cite" class="">Hi,<br class=""><br class="">I've successfully built openmpi-v2.x-dev-958-g7e94425 on my machine<br class="">(SUSE Linux Enterprise Server 12.0 x86_64) with gcc-5.2.0 and<br class="">Sun C 5.13. Unfortunately I get a runtime error for all programs<br class="">if I use the Sun compiler. Most of my small works es expected with<br class="">the GNU compiler. I used the following command to build the package<br class="">for cc.<br class=""><br class=""><br class="">mkdir openmpi-v2.x-dev-958-g7e94425-${SYSTEM_ENV}.${MACHINE_ENV}.64_cc<br class="">cd openmpi-v2.x-dev-958-g7e94425-${SYSTEM_ENV}.${MACHINE_ENV}.64_cc<br class=""><br class="">../openmpi-v2.x-dev-958-g7e94425/configure \<br class=""> &nbsp;--prefix=/usr/local/openmpi-2.0.0_64_cc \<br class=""> &nbsp;--libdir=/usr/local/openmpi-2.0.0_64_cc/lib64 \<br class=""> &nbsp;--with-jdk-bindir=/usr/local/jdk1.8.0_66/bin \<br class=""> &nbsp;--with-jdk-headers=/usr/local/jdk1.8.0_66/include \<br class=""> &nbsp;JAVA_HOME=/usr/local/jdk1.8.0_66 \<br class=""> &nbsp;LDFLAGS="-m64" CC="cc" CXX="CC" FC="f95" \<br class=""> &nbsp;CFLAGS="-m64 -z noexecstack" CXXFLAGS="-m64 -library=stlport4" FCFLAGS="-m64" \<br class=""> &nbsp;CPP="cpp" CXXCPP="cpp" \<br class=""> &nbsp;--enable-mpi-cxx \<br class=""> &nbsp;--enable-cxx-exceptions \<br class=""> &nbsp;--enable-mpi-java \<br class=""> &nbsp;--enable-heterogeneous \<br class=""> &nbsp;--enable-mpi-thread-multiple \<br class=""> &nbsp;--with-hwloc=internal \<br class=""> &nbsp;--without-verbs \<br class=""> &nbsp;--with-wrapper-cflags="-m64" \<br class=""> &nbsp;--with-wrapper-cxxflags="-m64 -library=stlport4" \<br class=""> &nbsp;--with-wrapper-fcflags="-m64" \<br class=""> &nbsp;--with-wrapper-ldflags="" \<br class=""> &nbsp;--enable-debug \<br class=""> &nbsp;|&amp; tee log.configure.$SYSTEM_ENV.$MACHINE_ENV.64_cc<br class=""><br class="">make |&amp; tee log.make.$SYSTEM_ENV.$MACHINE_ENV.64_cc<br class=""><br class=""><br class=""><br class="">loki hello_1 120 ompi_info | egrep -e "Open MPI repo revision:" -e "C compiler absolute:"<br class=""> &nbsp;Open MPI repo revision: v2.x-dev-958-g7e94425<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;C compiler absolute: /opt/solstudio12.4/bin/cc<br class=""><br class=""><br class="">loki hello_1 120 mpiexec -np 3 --host loki --slot-list 0:0-5,1:0-5 hello_1_mpi<br class="">mpiexec: symbol lookup error: /usr/local/openmpi-2.0.0_64_cc/lib64/libpmix.so.2: undefined symbol: __builtin_clz<br class="">loki hello_1 121<br class=""><br class=""><br class=""><br class="">I get the following error spawning a process and a different one<br class="">spawning multiple processes.<br class=""><br class=""><br class="">loki spawn 137 mpiexec -np 1 --host loki --slot-list 0:0-5,1:0-5 spawn_master<br class=""><br class="">Parent process 0 running on loki<br class=""> &nbsp;I create 4 slave processes<br class=""><br class="">[loki:24531] [[49263,0],0] ORTE_ERROR_LOG: Not found in file<br class="">../../openmpi-v2.x-dev-958-g7e94425/orte/orted/pmix/pmix_server_fence.c at line 186<br class="">[loki:24531] [[49263,0],0] ORTE_ERROR_LOG: Not found in file<br class="">../../openmpi-v2.x-dev-958-g7e94425/orte/orted/pmix/pmix_server_fence.c at line 186<br class="">[loki:24531] [[49263,0],0] ORTE_ERROR_LOG: Not found in file<br class="">../../openmpi-v2.x-dev-958-g7e94425/orte/orted/pmix/pmix_server_fence.c at line 186<br class="">--------------------------------------------------------------------------<br class="">It looks like MPI_INIT failed for some reason; your parallel process is<br class="">likely to abort. &nbsp;There are many reasons that a parallel process can<br class="">fail during MPI_INIT; some of which are due to configuration or environment<br class="">problems. &nbsp;This failure appears to be an internal failure; here's some<br class="">additional information (which may only be relevant to an Open MPI<br class="">developer):<br class=""><br class=""> &nbsp;ompi_proc_complete_init failed<br class=""> &nbsp;--&gt; Returned "Not found" (-13) instead of "Success" (0)<br class="">--------------------------------------------------------------------------<br class="">*** An error occurred in MPI_Init<br class="">*** on a NULL communicator<br class="">*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br class="">*** &nbsp;&nbsp;&nbsp;and potentially your MPI job)<br class="">--------------------------------------------------------------------------<br class="">...<br class=""><br class=""><br class=""><br class=""><br class="">loki spawn 138 mpiexec -np 1 --host loki --slot-list 0:0-5,1:0-5 spawn_multiple_master<br class=""><br class="">Parent process 0 running on loki<br class=""> &nbsp;I create 3 slave processes.<br class=""><br class="">[loki:24717] PMIX ERROR: UNPACK-PAST-END in file<br class="">../../../../../../openmpi-v2.x-dev-958-g7e94425/opal/mca/pmix/pmix112/pmix/src/server/pmix_server_ops.c at line 829<br class="">[loki:24717] PMIX ERROR: UNPACK-PAST-END in file<br class="">../../../../../../openmpi-v2.x-dev-958-g7e94425/opal/mca/pmix/pmix112/pmix/src/server/pmix_server.c at line 2176<br class="">[loki:24721] *** An error occurred in MPI_Comm_spawn_multiple<br class="">[loki:24721] *** reported by process [4281401345,0]<br class="">[loki:24721] *** on communicator MPI_COMM_WORLD<br class="">[loki:24721] *** MPI_ERR_SPAWN: could not spawn processes<br class="">[loki:24721] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br class="">[loki:24721] *** &nbsp;&nbsp;&nbsp;and potentially your MPI job)<br class="">loki spawn 139<br class=""><br class=""><br class=""><br class="">Everything works as expected for the following program.<br class=""><br class="">loki spawn 139 mpiexec -np 1 --host loki --slot-list 0:0-5,1:0-5 spawn_intra_comm<br class="">Parent process 0: I create 2 slave processes<br class=""><br class="">Parent process 0 running on loki<br class=""> &nbsp;&nbsp;&nbsp;MPI_COMM_WORLD ntasks: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1<br class=""> &nbsp;&nbsp;&nbsp;COMM_CHILD_PROCESSES ntasks_local: &nbsp;1<br class=""> &nbsp;&nbsp;&nbsp;COMM_CHILD_PROCESSES ntasks_remote: 1<br class=""> &nbsp;&nbsp;&nbsp;COMM_ALL_PROCESSES ntasks: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2<br class=""> &nbsp;&nbsp;&nbsp;mytid in COMM_ALL_PROCESSES: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0<br class=""><br class="">Child process 0 running on loki<br class=""> &nbsp;&nbsp;&nbsp;MPI_COMM_WORLD ntasks: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1<br class=""> &nbsp;&nbsp;&nbsp;COMM_ALL_PROCESSES ntasks: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2<br class=""> &nbsp;&nbsp;&nbsp;mytid in COMM_ALL_PROCESSES: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1<br class="">loki spawn 140<br class=""><br class=""><br class=""><br class="">I would be grateful if somebody can fix the problem. Please let me<br class="">know if you need anything else. Thank you very much for any help in<br class="">advance.<br class=""><br class=""><br class="">Best regards<br class=""><br class="">Siegmar<br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2016/01/28273.php<br class=""><br class=""></blockquote><br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2016/01/28283.php<br class=""></blockquote>_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2016/02/28559.php<br class=""></div></div></blockquote></div><br class=""></div></div></body></html>
