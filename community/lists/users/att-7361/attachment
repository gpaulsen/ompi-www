<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
</head>
<body bgcolor="#ffffff" text="#000000">
<pre wrap="">Hi all,

I have "inherited" a small cluster with a head node and four compute
nodes which I have to administer.  The nodes are connected via infiniband (OFED), but the head is not. 
I am a complete novice to the infiniband stuff and here is my problem:

The infiniband configuration seems to be OK. The usual tests suggested in the OFED install guide give 
the expected output, e.g.

</pre>
<p><big><big><tt>ibv_devinfo on the nodes:</tt></big></big><br>
</p>
<tt><br>
<small>************************* oscar_cluster *************************<br>
--------- n01---------<br>
hca_id: mthca0<br>
fw_ver: 1.2.0<br>
node_guid: 0002:c902:0025:930c<br>
sys_image_guid: 0002:c902:0025:930f<br>
vendor_id: 0x02c9<br>
vendor_part_id: 25204<br>
hw_ver: 0xA0<br>
board_id: MT_03B0140001<br>
phys_port_cnt: 1<br>
port: 1<br>
state: PORT_ACTIVE (4)<br>
max_mtu: 2048 (4)<br>
active_mtu: 2048 (4)<br>
sm_lid: 2<br>
port_lid: 1<br>
port_lmc: 0x00<br>
</small></tt><br>
etc. for the other nodes.<br>
<br>
<big><big>sminfo on the nodes:</big></big><br>
<br>
*<small>************************ oscar_cluster *************************<br>
--------- n01---------<br>
sminfo: sm lid 2 sm guid 0x2c90200259201, activity count 6881 priority
0 state 3 SMINFO_MASTER<br>
--------- n02---------<br>
sminfo: sm lid 2 sm guid 0x2c90200259201, activity count 6882 priority
0 state 3 SMINFO_MASTER<br>
--------- n03---------<br>
sminfo: sm lid 2 sm guid 0x2c90200259201, activity count 6883 priority
0 state 3 SMINFO_MASTER<br>
--------- n04---------<br>
sminfo: sm lid 2 sm guid 0x2c90200259201, activity count 6884 priority
0 state 3 SMINFO_MASTER<br>
<br>
</small><br>
<br>
However, when I directly start a mpi job (without using a scheduler)
via:<br>
<br>
<small>/usr/mpi/gcc4/openmpi-1.2.2-1/bin/mpirun -np 4 -hostfile
/home/sysgen/infiniband-mpi-test/machine/usr/mpi/gcc4/openmpi-1.2.2-1/tests/IMB-2.3/IMB-MPI1<br>
<br>
<big>I get the error message:<br>
<br>
</big></small>
<pre wrap=""><small>0,1,0]: uDAPL on host n01 was unable to find any NICs.
Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
[0,1,2]: uDAPL on host n01 was unable to find any NICs.
Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
[0,1,3]: uDAPL on host n02 was unable to find any NICs.
Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
[0,1,1]: uDAPL on host n02 was unable to find any NICs.
Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------</small></pre>
<small><big>MPI with normal GB Etherrnet and IP networking just works
fine, but the infinband doesn't. The MPI libs I am using<br>
for the test are definitely compiled with IB support and the tests have
been run successfully on<br>
the cluster before. <br>
<br>
Any suggestions what is going wrong here?<br>
<br>
Best regards and thanks for any help!<br>
<br>
Michael<br>
</big><br>
<br>
<br>
</small><br>
</body>
</html>

