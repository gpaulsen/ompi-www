<html><head><meta http-equiv="Content-Type" content="text/html charset=windows-1252"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">Hi there,<div><br></div><div>I am doing benchmarks on a GPU cluster with two CPU sockets and 4 K80 GPUs each node. Two K80 are connected with CPU socket 0, another two with socket 1. An IB ConnectX-3 (FDR) is also under socket 1. We are using Linux’s OFED, so I know there is no way to do GPU RDMA inter-node communication. I can do intra-node IPC for MPI_Send and MPI_Receive with two K80 (4 GPUs in total) which are connected under same socket (PCI-e switch). So I thought I could do intra-node MPI_Reduce with IPC support in openmpi 1.8.5.</div><div><br></div><div>The benchmark I was using is osu-micro-benchmarks-4.4.1, and I got the same results when I use two GPU under the same socket or different socket. The result was the same even I used two GPUs in different nodes.&nbsp;</div><div><br></div><div>Does MPI_Reduce use IPC for intra-node? Should I have to install Mellanox OFED stack to support GPU RDMA reduction on GPUs even they are under with the same PCI-e switch?</div><div><br></div><div>Thanks,</div><div><br></div><div><div>Fei Mao<br>High Performance Computing Technical Consultant&nbsp;</div><div>SHARCNET |&nbsp;<a href="http://www.sharcnet.ca/">http://www.sharcnet.ca</a><br>Compute/Calcul Canada |&nbsp;<a href="http://www.computecanada.ca">http://www.computecanada.ca</a></div></div></body></html>
