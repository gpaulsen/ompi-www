Thanks for your suggestion Gus, we need a way of debugging what is going on. I am pretty sure the problem lies with our cluster configuration. I know MPI simply relies on the underlying network. However, we can ping and ssh to all nodes (and in between and pair as well) so it is currently a mystery why MPI doesn&#39;t communicate across nodes on our cluster.<br>
Two further questions for the group<br><ol><li>I would love to run the test program connectivity.c, but cannot find it anywhere. Can anyone help please?</li><li>After having left the job hanging over night we got the message [node5][[9454,1],1][../../../../../../ompi/mca/btl/tcp/btl_tcp_frag.c:216:mca_btl_tcp_frag_recv] mca_btl_tcp_frag_recv: readv failed: Connection timed out (110). Does anyone know what this means?</li>
</ol><br>Cheers and thanks<br>Ole<br>PS - I don&#39;t see how separate buffers would help. Recall that the test program I use works fine on other installations and indeed when run on one the cores of one Node.<br><br><br>
<br><br>Message: 11<br>
Date: Mon, 19 Sep 2011 10:37:02 -0400<br>
From: Gus Correa &lt;<a href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt;<br>
Subject: Re: [OMPI users] RE :  MPI hangs on multiple nodes<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Message-ID: &lt;<a href="mailto:4E77538E.3070007@ldeo.columbia.edu">4E77538E.3070007@ldeo.columbia.edu</a>&gt;<br>
Content-Type: text/plain; charset=iso-8859-1; format=flowed<br>
<br>
Hi Ole<br>
<br>
You could try the examples/connectivity.c program in the<br>
OpenMPI source tree, to test if everything is alright.<br>
It also hints how to solve the buffer re-use issue<br>
that Sebastien [rightfully] pointed out [i.e., declare separate<br>
buffers for MPI_Send and MPI_Recv].<br>
<br>
Gus Correa

