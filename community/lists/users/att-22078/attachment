<html><head><meta http-equiv="Content-Type" content="text/html charset=iso-8859-1"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">Couple of things stand out. You should remove the following configure options:<div><br></div><div>--enable-mpi-thread-multiple</div><div>--with-threads</div><div>--enable-heterogeneous</div><div><br></div><div>Thread multiple is not ready yet in OMPI (and openib doesn't support threaded operations anyway), and the support for hetero systems really isn't working. Not saying that's the sole source of the problem, but it may well be contributing if you are trying to run a multi-threaded app and it exposes alternative code paths that may not be fully debugged.</div><div><br></div><div><br><div><div>On Jun 11, 2013, at 7:40 AM, Jesús Escudero Sahuquillo &lt;<a href="mailto:jescudero@dsi.uclm.es">jescudero@dsi.uclm.es</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite">
  
    <meta content="text/html; charset=ISO-8859-1" http-equiv="Content-Type">
  
  <div bgcolor="#FFFFFF" text="#000000">
    <div class="moz-cite-prefix">In fact, I also have tried to configure
      the OpenMPI with this:<br>
      <br>
      ./configure --with-sge --with-openib --enable-mpi-thread-multiple
      --with-threads --with-hwloc --enable-heterogeneous --disable-vt
      --enable-openib-dynamic-sl --prefix=/home/jescudero/opt/openmpi<br>
      <br>
      And the problem is still present<br>
      <br>
      El 11/06/13 15:32, Mike Dubman escribió:<br>
    </div>
    <blockquote cite="mid:CAAO1KyYdO4ijgrP9TZeGNSvD8U34WXHTz7twp2ZUh=V44hb=CA@mail.gmail.com" type="cite">
      <div dir="ltr"><span style="font-family:arial,sans-serif;font-size:13px">--mca
          btl_openib_ib_path_record_</span><span style="font-family:arial,sans-serif;font-size:13px">serv</span><span style="font-family:arial,sans-serif;font-size:13px">ice_level
          1 flag controls openib btl, you need to remove&nbsp;</span><span style="font-family:arial,sans-serif;font-size:13px">&nbsp;--mca mtl
          mxm&nbsp; from command line.</span><br>
        <div><span style="font-family:arial,sans-serif;font-size:13px"><br>
          </span></div>
        <div style=""><span style="font-family:arial,sans-serif;font-size:13px">Have you
            compiled OpenMPI with rhel6.4 inbox ofed driver? AFAIK, the
            MOFED 2.x does not have XRC and you mentioned "</span><span style="font-family:arial,sans-serif;font-size:13px">--enable-openib-connectx-xrc"
            flag in configure.</span></div>
      </div>
      <div class="gmail_extra"><br>
        <br>
        <div class="gmail_quote">On Tue, Jun 11, 2013 at 3:02 PM, Jesús
          Escudero Sahuquillo <span dir="ltr">&lt;<a moz-do-not-send="true" href="mailto:jescudero@dsi.uclm.es" target="_blank">jescudero@dsi.uclm.es</a>&gt;</span>
          wrote:<br>
          <blockquote class="gmail_quote" style="margin:0 0 0
            .8ex;border-left:1px #ccc solid;padding-left:1ex">I have a
            16-node Mellanox cluster built with Mellanox ConnectX3
            cards. Recently I have updated the MLNX_OFED to the 2.0.5
            version. The reason of this e-mail to the OpenMPI users list
            is that I am not able to run MPI applications using the
            service levels (SLs) feature of the OpenMPI driver.<br>
            <br>
            Currently, the nodes have the Red-Hat 6.4 with the kernel
            2.6.32-358.el6.x86_64. I have compiled OpenMPI 1.6.4 with:<br>
            <br>
            &nbsp;./configure --with-sge --with-openib=/usr
            --enable-openib-connectx-xrc --enable-mpi-thread-multiple
            --with-threads --with-hwloc --enable-heterogeneous
            --with-fca=/opt/mellanox/fca --with-mxm-libdir=/opt/mellanox/mxm/lib
            --with-mxm=/opt/mellanox/mxm --prefix=/home/jescudero/opt/openmpi<br>
            <br>
            I have modified the OpenSM code (which is based on 3.3.15)
            in order to include a special routing algorithm based on
            "ftree". Apparently all is correct with the OpenSM since it
            returns the SLs when I execute the command "saquery
            --src-to-dst slid:dlid". Anyway, I have also tried to run
            the OpenSM with the DFSSSP algorithm.<br>
            <br>
            However, when I try to run MPI applications (i.e. HPCC, OSU
            or even alltoall.c -included in the OpenMPI sources-) I
            experience some errors if the "btl_openib_path_record_info"
            is set to "1", otherwise (i.e. if the
            btl_openib_path_record_info is not enabled) the application
            execution ends correctly. I run the MPI application with the
            next command:<br>
            <br>
            mpirun -display-allocation -display-map -np 8 -machinefile
            maquinas.aux --mca btl openib,self,sm --mca mtl mxm --mca
            btl_openib_ib_path_record_service_level 1 --mca
            btl_openib_cpc_include oob hpcc<br>
            <br>
            I obtain the next trace:<br>
            <br>
            [nodo20.XXXXX][[31227,1],6][connect/btl_openib_connect_sl.c:239:get_pathrecord_info]
            error posting receive on QP [0x16db] errno says: Success [0]<br>
            [nodo15.XXXXX][[31227,1],4][connect/btl_openib_connect_sl.c:239:get_pathrecord_info]
            error posting receive on QP [0x1749] errno says: Success [0]<br>
            [nodo17.XXXXX][[31227,1],5][connect/btl_openib_connect_sl.c:239:get_pathrecord_info]
            error posting receive on QP [0x1783] errno says: Success [0]<br>
            [nodo21.XXXXX][[31227,1],7][connect/btl_openib_connect_sl.c:239:get_pathrecord_info]
            error posting receive on QP [0x1838] errno says: Success [0]<br>
            [nodo21.XXXXX][[31227,1],7][connect/btl_openib_connect_oob.c:885:rml_recv_cb]
            endpoint connect error: -1<br>
            [nodo17.XXXXX][[31227,1],5][connect/btl_openib_connect_oob.c:885:rml_recv_cb]
            endpoint connect error: -1<br>
            [nodo15.XXXXX][[31227,1],4][connect/btl_openib_connect_oob.c:885:rml_recv_cb]
            endpoint connect error: -1<br>
            [nodo20.XXXXX][[31227,1],6][connect/btl_openib_connect_oob.c:885:rml_recv_cb]
            endpoint connect error: -1<br>
            <br>
            Does anyone know what I am doing wrong?<br>
            <br>
            All the best,<br>
            <br>
            <br>
            <br>
            <br>
            <br>
            <br>
            _______________________________________________<br>
            users mailing list<br>
            <a moz-do-not-send="true" href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
            <a moz-do-not-send="true" href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
          </blockquote>
        </div>
        <br>
      </div>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></pre>
    </blockquote>
    <br>
  </div>

_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
