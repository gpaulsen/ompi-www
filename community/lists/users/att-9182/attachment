I finally have opportunity to run the imb-3.2 benchmark over myrinet I am running in a cluster of 16 node Xservers connected with myrinet 15 of them are 8core ones and the last one is a 4 cores one. Having a limit of 124 process  <div>
<br></div><div>I have run the test with the bynode option so from the 2 to the 16 process test is always running 1 process by node.</div><div><br></div><div>the following test  pingpong, pingping, sendrecv, exchange presents a strong drop in performance with the 64k packet size.</div>
<div><br></div><div>any idea where I should look for the cause.</div><div><br></div><div>Ricardo <br><br><div class="gmail_quote">On Fri, Mar 20, 2009 at 7:32 PM, Ricardo Fernández-Perea <span dir="ltr">&lt;<a href="mailto:rfernandezperea@gmail.com">rfernandezperea@gmail.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">It is the F-2M but I think for inter-node communication should be equivalents.<div><br></div><div>I have not run and MPI pingpong benchmark yet. </div>
<div><br></div><div>The truth is I have a 10 days travel coming next week and I thought I can take some optimization   &quot;light reading&quot; with me.</div>
<div><br></div><div>so I know what I must look for  when I came back.</div><div><br><font color="#888888"><div>Ricardo</div></font><div><div></div><div class="h5"><div><br><br><div class="gmail_quote">On Fri, Mar 20, 2009 at 5:10 PM, Scott Atchley <span dir="ltr">&lt;<a href="mailto:atchley@myri.com" target="_blank">atchley@myri.com</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div>On Mar 20, 2009, at 11:33 AM, Ricardo Fernández-Perea wrote:<br>
<br>
</div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div>
This are the results initially<br>
Running 1000 iterations.<br>
   Length   Latency(us)    Bandwidth(MB/s)<br>
        0       2.738          0.000<br>
        1       2.718          0.368<br>
        2       2.707          0.739<br></div>
&lt;snip&gt;<div><br>
  1048576    4392.217        238.735<br>
  2097152    8705.028        240.913<br>
  4194304   17359.166        241.619<br>
<br>
with  export MX_RCACHE=1<br>
<br>
Running 1000 iterations.<br>
   Length   Latency(us)    Bandwidth(MB/s)<br>
        0       2.731          0.000<br>
        1       2.705          0.370<br>
        2       2.719          0.736<br></div>
&lt;snip&gt;<div><br>
  1048576    4265.846        245.807<br>
  2097152    8491.122        246.982<br>
  4194304   16953.997        247.393<br>
</div></blockquote>
<br>
Ricardo,<br>
<br>
I am assuming that these are PCI-X NICs. Given the latency and bandwidth, are these &quot;D&quot; model NICs (see the top of the mx_info output)? If so, that looks about as good as you can expect.<br>
<br>
Have you run Intel MPI Benchmark (IMB) or another MPI pingpong type benchmark?<br><font color="#888888">
<br>
Scott</font><div><div></div><div><br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div></div></div></div>
</blockquote></div><br></div>

