<html>
<head>
</head>
<body class='hmmessage'><div dir='ltr'>


<style><!--
.hmmessage P
{
margin:0px;
padding:0px
}
body.hmmessage
{
font-size: 10pt;
font-family:Tahoma
}
--></style>
<div dir="ltr">Dear all,<br><br>I am using openmpi 1.6 on linux. I have a question on MPI_Reduce_scatter.<br><br>I try to see how large the data can push through MPI_Reduce_scatter using the<br>following code.<br><br>size = (long) 1024*1024*1024*4;<br>for(k=1;k&lt;=16;++k) {<br>&nbsp;&nbsp;&nbsp; bufsize = k*size/16;<br>&nbsp;&nbsp;&nbsp; for(i=0;i&lt;nproc;++i)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; recvCount[i] = bufsize/nproc;<br>&nbsp;&nbsp;&nbsp; for (i=0;i&lt;bufsize;++i)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sbuf[i] = myid+1;<br>&nbsp;&nbsp;&nbsp; printf("buffer size: %ld recvCount[0]:%d\n",bufsize,recvCount[0]);<br><br>&nbsp;&nbsp;&nbsp; MPI_Reduce_scatter(sbuf,rbuf,recvCount,MPI_LONG,<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MPI_SUM,MPI_COMM_WORLD);<br>&nbsp;&nbsp;&nbsp; for(i=0;i&lt;bufsize/nproc;++i) {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (rbuf[i] != nproc/2*(nproc+1)) {<br>&nbsp;&nbsp;&nbsp; printf("failed in %d",myid);<br>&nbsp;&nbsp;&nbsp; break;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp; printf("done\n");<br>&nbsp; }<br>&nbsp; <br>&nbsp; ierr = MPI_Finalize();<br><br><br>I used 4 processes and found that if 4 processes are in the same machine. It can<br>go through size = MAX_INT. However, if 4 processes are in 4 different machines,<br>it hangs at size=&nbsp; 1073741824.<br><br><br>#0&nbsp; 0x000000337f6d3fc3 in __epoll_wait_nocancel () from /lib64/libc.so.6<br>#1&nbsp; 0x00002b1e9c45d4eb in epoll_dispatch (base=0xd08e940, arg=0xd08e800,<br>&nbsp;&nbsp;&nbsp; tv=&lt;value optimized out&gt;) at epoll.c:215<br>#2&nbsp; 0x00002b1e9c45f98a in opal_event_base_loop (base=0xd08e940,<br>&nbsp;&nbsp;&nbsp; flags=&lt;value optimized out&gt;) at event.c:838<br>#3&nbsp; 0x00002b1e9c485809 in opal_progress () at runtime/opal_progress.c:189<br>#4&nbsp; 0x00002b1e9c3ccf05 in opal_condition_wait (req_ptr=0x7fffc4519fb0,<br>&nbsp;&nbsp;&nbsp; status=0x0) at ../opal/threads/condition.h:99<br>#5&nbsp; ompi_request_wait_completion (req_ptr=0x7fffc4519fb0, status=0x0)<br>&nbsp;&nbsp;&nbsp; at ../ompi/request/request.h:377<br>#6&nbsp; ompi_request_default_wait (req_ptr=0x7fffc4519fb0, status=0x0)<br>&nbsp;&nbsp;&nbsp; at request/req_wait.c:38<br>#7&nbsp; 0x00002b1ea0d60dda in ompi_coll_tuned_reduce_scatter_intra_ring (<br>&nbsp;&nbsp;&nbsp; sbuf=0x7fffc4519fb0, rbuf=0x2b1ea1384010, rcounts=0xd458e30,<br>&nbsp;&nbsp;&nbsp; dtype=0x601fa0, op=0x601790, comm=0x601390, module=0xd458a10)<br>&nbsp;&nbsp;&nbsp; at coll_tuned_reduce_scatter.c:584<br>#8&nbsp; 0x00002b1ea0b4cd8c in mca_coll_sync_reduce_scatter (sbuf=0x2b26a1385010,<br>&nbsp;&nbsp;&nbsp; rbuf=0x2b1ea1384010, rcounts=&lt;value optimized out&gt;,<br>&nbsp;&nbsp;&nbsp; dtype=&lt;value optimized out&gt;, op=&lt;value optimized out&gt;, comm=0x601390,<br>&nbsp;&nbsp;&nbsp; module=0xd458820) at coll_sync_reduce_scatter.c:46<br>#9&nbsp; 0x00002b1e9c3e7e51 in PMPI_Reduce_scatter (sendbuf=0x2b26a1385010,<br>&nbsp;&nbsp;&nbsp; recvbuf=0x2b1ea1384010, recvcounts=0xd458e30,<br>&nbsp;&nbsp;&nbsp; datatype=&lt;value optimized out&gt;, op=0x601790, comm=0x601390)<br>---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---<br>&nbsp;&nbsp;&nbsp; at preduce_scatter.c:129<br>#10 0x0000000000400ddb in main (argc=1, argv=0x7fffc451a998)<br>&nbsp;&nbsp;&nbsp; at test_reduce_scatter.c:50<br><br>Does openmpi 1.6 uses different mechanisms in reduce_scatter when communicate<br>within a machine and inter-machines?<br><br>What is the limit of size of buffer to use reduce_scatter?<br><br>Thanks for your attention.<br><br>Regards,<br><br>William<br><br></div>
 		 	   		  </div></body>
</html>
