Hello Pasha,<br><br>          As the error was not repeating frequently, I didn&#39;t look into the issue from a long time. But now I started to diagnose it:<br><br>Initially I tested with ibv_rc_pingpong (Master node to all compute nodes using a for loop). Its working for each of the nodes.<br>
<br>The files generated out of the command &quot;ibdiagnet -v -r -o .&quot; are attached herewith. The ibcheckerrors shows following error message:<br><br># ibcheckerrors <br>#warn: counter RcvSwRelayErrors = 408   (threshold 100)<br>
Error check on lid 2 (MT47396 Infiniscale-III Mellanox Technologies) port all:  FAILED <br>#warn: counter RcvSwRelayErrors = 179   (threshold 100)<br>Error check on lid 2 (MT47396 Infiniscale-III Mellanox Technologies) port 7:  FAILED <br>
# Checked Switch: nodeguid 0x000b8cffff00551b with failure<br><br>## Summary: 25 nodes checked, 0 bad nodes found<br>##          48 ports checked, 1 ports have errors beyond threshold<br><br>Are these messages helpful to  find the issue with node-0-2? Can you please help us to diagnose further?<br>
<br>Thanks,<br>Sangamesh<br><br><br><div class="gmail_quote">On Mon, Sep 21, 2009 at 1:36 PM, Pavel Shamis (Pasha) <span dir="ltr">&lt;<a href="mailto:pashash@gmail.com">pashash@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Sangamesh,<br>
<br>
The ib tunings that you added to your command line only delay the problem but doesn&#39;t resolve it.<br>
The node-0-2.local gets asynchronous event &quot;IBV_EVENT_PORT_ERROR&quot; and as result<br>
the processes fails to deliver packets to some remote hosts and as result you see bunch of IB errors.<br>
<br>
The IBV_EVENT_PORT_ERROR error means that the IB port gone from ACTIVE state do DOWN state.<br>
Or in other words you have problem with your IB networks that cause all these networks errors.<br>
Source cause of such issue maybe some bad cable or some problematic port on switch.<br>
<br>
For the IB network debug I propose you use Ibdiaget, it is open source IB network diagnostic tool :<br>
<a href="http://linux.die.net/man/1/ibdiagnet" target="_blank">http://linux.die.net/man/1/ibdiagnet</a><br>
The tool is part of OFED distribution.<br>
<br>
Pasha.<br>
<br>
<br>
Sangamesh B wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div class="im">
Dear all,<br>
      The CPMD application which is compiled with OpenMPI-1.3 (Intel 10.1 compilers) on CentOS-4.5, fails only, when a specific node i.e. node-0-2 is involved. But runs well on other nodes.<br>
       Initially job failed after 5-10 mins (on node-0-2 + some other nodes). After googling error, I added options &quot;-mca btl_openib_ib_min_rnr_timer 25 -mca btl_openib_ib_timeout 20&quot; to mpirun command in the SGE script:<br>

 $ cat cpmdrun.sh<br>
#!/bin/bash<br>
#$ -N cpmd-acw<br>
#$ -S /bin/bash<br>
#$ -cwd<br>
#$ -e err.$JOB_ID.$JOB_NAME<br>
#$ -o out.$JOB_ID.$JOB_NAME<br>
#$ -pe ib 32<br>
unset SGE_ROOT  PP_LIBRARY=/home/user1/cpmdrun/wac/prod/PP<br>
CPMD=/opt/apps/cpmd/3.11/ompi/SOURCE/cpmd311-ompi-mkl.x<br>
MPIRUN=/opt/mpi/openmpi/1.3/intel/bin/mpirun<br></div>
$MPIRUN -np $NSLOTS -hostfile $TMPDIR/machines -mca btl_openib_ib_min_rnr_timer 25 -mca btl_openib_ib_timeout 20 $CPMD <a href="http://wac_md26.in" target="_blank">wac_md26.in</a> &lt;<a href="http://wac_md26.in" target="_blank">http://wac_md26.in</a>&gt;  $PP_LIBRARY &gt; wac_md26.out<div>
<div></div><div class="h5"><br>
After adding these options, job executed for 24+ hours then failed with the same error as earlier. The error is:<br>
 $ cat err.6186.cpmd-acw<br>
--------------------------------------------------------------------------<br>
The OpenFabrics stack has reported a network error event.  Open MPI<br>
will try to continue, but your job may end up failing.<br>
  Local host:        node-0-2.local<br>
  MPI process PID:   11840<br>
  Error number:      10 (IBV_EVENT_PORT_ERR)<br>
This error may indicate connectivity problems within the fabric;<br>
please contact your system administrator.<br>
--------------------------------------------------------------------------<br>
[node-0-2.local:11836] 7 more processes have sent help message help-mpi-btl-openib.txt / of error event<br>
[node-0-2.local:11836] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>
[node-0-2.local:11836] 1 more process has sent help message help-mpi-btl-openib.txt / of error event<br>
[node-0-2.local:11836] 7 more processes have sent help message help-mpi-btl-openib.txt / of error event<br>
[node-0-2.local:11836] 1 more process has sent help message help-mpi-btl-openib.txt / of error event<br>
[node-0-2.local:11836] 7 more processes have sent help message help-mpi-btl-openib.txt / of error event<br>
[node-0-2.local:11836] 1 more process has sent help message help-mpi-btl-openib.txt / of error event<br>
[node-0-2.local:11836] 7 more processes have sent help message help-mpi-btl-openib.txt / of error event<br>
[node-0-2.local:11836] 1 more process has sent help message help-mpi-btl-openib.txt / of error event<br>
[node-0-2.local:11836] 7 more processes have sent help message help-mpi-btl-openib.txt / of error event<br>
[node-0-2.local:11836] 1 more process has sent help message help-mpi-btl-openib.txt / of error event<br>
[node-0-2.local:11836] 15 more processes have sent help message help-mpi-btl-openib.txt / of error event<br>
[node-0-2.local:11836] 16 more processes have sent help message help-mpi-btl-openib.txt / of error event<br>
[node-0-2.local:11836] 16 more processes have sent help message help-mpi-btl-openib.txt / of error event<br>
[[718,1],20][btl_openib_component.c:2902:handle_wc] from node-0-22.local to: node-0-2 --------------------------------------------------------------------------<br>
The InfiniBand retry count between two MPI processes has been<br>
exceeded.  &quot;Retry count&quot; is defined in the InfiniBand spec 1.2<br>
(section 12.7.38):<br>
    The total number of times that the sender wishes the receiver to<br>
    retry timeout, packet sequence, etc. errors before posting a<br>
    completion error.<br>
This error typically means that there is something awry within the<br>
InfiniBand fabric itself.  You should note the hosts on which this<br>
error has occurred; it has been observed that rebooting or removing a<br>
particular host from the job can sometimes resolve this issue.<br>
Two MCA parameters can be used to control Open MPI&#39;s behavior with<br>
respect to the retry count:<br>
* btl_openib_ib_retry_count - The number of times the sender will<br>
  attempt to retry (defaulted to 7, the maximum value).<br>
* btl_openib_ib_timeout - The local ACK timeout parameter (defaulted<br>
  to 10).  The actual timeout value used is calculated as:<br>
     4.096 microseconds * (2^btl_openib_ib_timeout)<br>
  See the InfiniBand spec 1.2 (section 12.7.34) for more details.<br>
Below is some information about the host that raised the error and the<br>
peer to which it was connected:<br>
  Local host:   node-0-22.local<br>
  Local device: mthca0<br>
  Peer host:    node-0-2<br>
You may need to consult with your system administrator to get this<br>
problem fixed.<br>
--------------------------------------------------------------------------<br>
error polling LP CQ with status RETRY EXCEEDED ERROR status number 12 for wr_id 66384128 opcode 128 qp_idx 3<br>
--------------------------------------------------------------------------<br>
mpirun has exited due to process rank 20 with PID 10425 on<br>
node ibc22 exiting without calling &quot;finalize&quot;. This may<br>
have caused other processes in the application to be<br>
terminated by signals sent by mpirun (as reported here).<br>
--------------------------------------------------------------------------<br>
rm: cannot remove `/tmp/6186.1.iblong.q/rsh&#39;: No such file or directory<br>
 The openibd service is running fine:<br>
 $ service openibd status<br>
  HCA driver loaded<br>
Configured devices:<br>
ib0<br>
Currently active devices:<br>
ib0<br>
The following OFED modules are loaded:<br>
  rdma_ucm<br>
  ib_sdp<br>
  rdma_cm<br>
  ib_addr<br>
  ib_ipoib<br>
  mlx4_core<br>
  mlx4_ib<br>
  ib_mthca<br>
  ib_uverbs<br>
  ib_umad<br>
  ib_ucm<br>
  ib_sa<br>
  ib_cm<br>
  ib_mad<br>
  ib_core<br>
But still the job is failing after hours of running, that to for a particular node. What&#39;s the wrong with node-0-2? How can it be resolved?<br>
 Thanks,<br>
Sangamesh<br></div></div>
------------------------------------------------------------------------<div class="im"><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></blockquote><div><div></div><div class="h5">
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

