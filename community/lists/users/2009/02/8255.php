<?
$subject_val = "Re: [OMPI users] TCP instead of openIB doesn't work";
include("../../include/msg-header.inc");
?>
<!-- received="Fri Feb 27 17:46:09 2009" -->
<!-- isoreceived="20090227224609" -->
<!-- sent="Fri, 27 Feb 2009 23:46:03 +0100" -->
<!-- isosent="20090227224603" -->
<!-- name="Vittorio Giovara" -->
<!-- email="vitto.giova_at_[hidden]" -->
<!-- subject="Re: [OMPI users] TCP instead of openIB doesn't work" -->
<!-- id="4de51c660902271446v44959ab8hf9feda9947635ae7_at_mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="4de51c660902270700u24438df9p5b9cc65da90d0598_at_mail.gmail.com" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>Subject:</strong> Re: [OMPI users] TCP instead of openIB doesn't work<br>
<strong>From:</strong> Vittorio Giovara (<em>vitto.giova_at_[hidden]</em>)<br>
<strong>Date:</strong> 2009-02-27 17:46:03
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8256.php">Jeff Squyres: "Re: [OMPI users] Latest SVN failures"</a>
<li><strong>Previous message:</strong> <a href="8254.php">Jeff Squyres: "Re: [OMPI users] openib RETRY EXCEEDED ERROR"</a>
<li><strong>In reply to:</strong> <a href="8241.php">Vittorio Giovara: "[OMPI users] TCP instead of openIB doesn't work"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8257.php">Jeff Squyres: "Re: [OMPI users] TCP instead of openIB doesn't work"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
Hello, i ve corrected the syntax and added the flag you suggested, but
<br>
unfortunately the result doen't change.
<br>
<p>randori ~ # mpirun --display-map --mca btl tcp,self  -np 2 -host
<br>
randori,tatami graph
<br>
[randori:22322]  Map for job: 1    Generated by mapping mode: byslot
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Starting vpid: 0    Vpid range: 2    Num app_contexts: 1
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data for app_context: index 0    app: graph
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Num procs: 2
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Argv[0]: graph
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[0]: OMPI_MCA_btl=tcp,self
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[1]: OMPI_MCA_rmaps_base_display_map=1
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[2]:
<br>
OMPI_MCA_orte_precondition_transports=d45d47f6e1ed0e0b-691fd7f24609dec3
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[3]: OMPI_MCA_rds=proxy
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[4]: OMPI_MCA_ras=proxy
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[5]: OMPI_MCA_rmaps=proxy
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[6]: OMPI_MCA_pls=proxy
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[7]: OMPI_MCA_rmgr=proxy
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Working dir: /root (user: 0)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Num maps: 1
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data for app_context_map: Type: 1    Data: randori,tatami
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Num elements in nodes list: 2
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mapped node:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cell: 0    Nodename: randori    Launch id: -1    Username: NULL
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Daemon name:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data type: ORTE_PROCESS_NAME    Data Value: NULL
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Oversubscribed: False    Num elements in procs list: 1
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mapped proc:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Proc Name:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data type: ORTE_PROCESS_NAME    Data Value: [0,1,0]
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Proc Rank: 0    Proc PID: 0    App_context index: 0
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mapped node:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cell: 0    Nodename: tatami    Launch id: -1    Username: NULL
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Daemon name:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data type: ORTE_PROCESS_NAME    Data Value: NULL
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Oversubscribed: False    Num elements in procs list: 1
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mapped proc:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Proc Name:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data type: ORTE_PROCESS_NAME    Data Value: [0,1,1]
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Proc Rank: 1    Proc PID: 0    App_context index: 0
<br>
Master thread reporting
<br>
matrix size 33554432 kB, time is in [us]
<br>
<p>(and then it just hangs)
<br>
<p>Vittorio
<br>
<p>On Fri, Feb 27, 2009 at 6:00 PM, &lt;users-request_at_[hidden]&gt; wrote:
<br>
<p><span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Date: Fri, 27 Feb 2009 08:22:17 -0700
</span><br>
<span class="quotelev1">&gt; From: Ralph Castain &lt;rhc_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Subject: Re: [OMPI users] TCP instead of openIB doesn't work
</span><br>
<span class="quotelev1">&gt; To: Open MPI Users &lt;users_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Message-ID: &lt;E3C4683C-1F97-4558-AB68-006E39A8334B_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; I'm not entirely sure what is causing the problem here, but one thing
</span><br>
<span class="quotelev1">&gt; does stand out. You have specified two -host options for the same
</span><br>
<span class="quotelev1">&gt; application - this is not our normal syntax. The usual way of
</span><br>
<span class="quotelev1">&gt; specifying this would be:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; mpirun  --mca btl tcp,self  -np 2 -host randori,tatami hostname
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; I'm not entirely sure what OMPI does when it gets two separate -host
</span><br>
<span class="quotelev1">&gt; arguments - could be equivalent to the above syntax, but could also
</span><br>
<span class="quotelev1">&gt; cause some unusual behavior.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Could you retry your job with the revised syntax? Also, could you add
</span><br>
<span class="quotelev1">&gt; --display-map to your mpirun cmd line? This will tell us where OMPI
</span><br>
<span class="quotelev1">&gt; thinks the procs are going, and a little info about how it interpreted
</span><br>
<span class="quotelev1">&gt; your cmd line.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Thanks
</span><br>
<span class="quotelev1">&gt; Ralph
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; On Feb 27, 2009, at 8:00 AM, Vittorio Giovara wrote:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev2">&gt; &gt; Hello, i'm posting here another problem of my installation
</span><br>
<span class="quotelev2">&gt; &gt; I wanted to benchmark the differences between tcp and openib transport
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; if i run a simple non mpi application i get
</span><br>
<span class="quotelev2">&gt; &gt; randori ~ # mpirun  --mca btl tcp,self  -np 2 -host randori -host
</span><br>
<span class="quotelev2">&gt; &gt; tatami hostname
</span><br>
<span class="quotelev2">&gt; &gt; randori
</span><br>
<span class="quotelev2">&gt; &gt; tatami
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; but as soon as i switch to my benchmark program i have
</span><br>
<span class="quotelev2">&gt; &gt; mpirun  --mca btl tcp,self  -np 2 -host randori -host tatami graph
</span><br>
<span class="quotelev2">&gt; &gt; Master thread reporting
</span><br>
<span class="quotelev2">&gt; &gt; matrix size 33554432 kB, time is in [us]
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; and instead of starting the send/receive functions it just hangs
</span><br>
<span class="quotelev2">&gt; &gt; there; i also checked the transmitted packets with wireshark but
</span><br>
<span class="quotelev2">&gt; &gt; after the handshake no more packets are exchanged
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; I read in the archives that there were some problems in this area
</span><br>
<span class="quotelev2">&gt; &gt; and so i tried what was suggested in previous emails
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; mpirun --mca btl ^openib  -np 2 -host randori -host tatami graph
</span><br>
<span class="quotelev2">&gt; &gt; mpirun --mca pml ob1  --mca btl tcp,self  -np 2 -host randori -host
</span><br>
<span class="quotelev2">&gt; &gt; tatami graph
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; gives exactly the same output as before (no mpisend/receive)
</span><br>
<span class="quotelev2">&gt; &gt; while the next commands gives something more interesting
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; mpirun --mca pml cm  --mca btl tcp,self  -np 2 -host randori -host
</span><br>
<span class="quotelev2">&gt; &gt; tatami graph
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev1">&gt; --------------------------------------------------------------------------
</span><br>
<span class="quotelev2">&gt; &gt; No available pml components were found!
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; This means that there are no components of this type installed on your
</span><br>
<span class="quotelev2">&gt; &gt; system or all the components reported that they could not be used.
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; This is a fatal error; your MPI process is likely to abort.  Check the
</span><br>
<span class="quotelev2">&gt; &gt; output of the &quot;ompi_info&quot; command and ensure that components of this
</span><br>
<span class="quotelev2">&gt; &gt; type are available on your system.  You may also wish to check the
</span><br>
<span class="quotelev2">&gt; &gt; value of the &quot;component_path&quot; MCA parameter and ensure that it has at
</span><br>
<span class="quotelev2">&gt; &gt; least one directory that contains valid MCA components.
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev1">&gt; --------------------------------------------------------------------------
</span><br>
<span class="quotelev2">&gt; &gt; [tatami:06619] PML cm cannot be selected
</span><br>
<span class="quotelev2">&gt; &gt; mpirun noticed that job rank 0 with PID 6710 on node randori exited
</span><br>
<span class="quotelev2">&gt; &gt; on signal 15 (Terminated).
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; which is not possible as if i do ompi_info --param all there is the
</span><br>
<span class="quotelev2">&gt; &gt; CM pml component
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;                  MCA pml: cm (MCA v1.0, API v1.0, Component v1.2.8)
</span><br>
<span class="quotelev2">&gt; &gt;                  MCA pml: ob1 (MCA v1.0, API v1.0, Component v1.2.8)
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; my test program is quite simple, just a couple of MPI_Send and
</span><br>
<span class="quotelev2">&gt; &gt; MPI_Recv (just after the signature)
</span><br>
<span class="quotelev2">&gt; &gt; do you have any ideas that might help me?
</span><br>
<span class="quotelev2">&gt; &gt; thanks a lot
</span><br>
<span class="quotelev2">&gt; &gt; Vittorio
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; ========================
</span><br>
<span class="quotelev2">&gt; &gt; #include &quot;mpi.h&quot;
</span><br>
<span class="quotelev2">&gt; &gt; #include &lt;stdio.h&gt;
</span><br>
<span class="quotelev2">&gt; &gt; #include &lt;stdlib.h&gt;
</span><br>
<span class="quotelev2">&gt; &gt; #include &lt;string.h&gt;
</span><br>
<span class="quotelev2">&gt; &gt; #include &lt;math.h&gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; #define M_COL 4096
</span><br>
<span class="quotelev2">&gt; &gt; #define M_ROW 524288
</span><br>
<span class="quotelev2">&gt; &gt; #define NUM_MSG 25
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; unsigned long int  gigamatrix[M_ROW][M_COL];
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; int main (int argc, char *argv[]) {
</span><br>
<span class="quotelev2">&gt; &gt;     int numtasks, rank, dest, source, rc, tmp, count, tag=1;
</span><br>
<span class="quotelev2">&gt; &gt;     unsigned long int  exp, exchanged;
</span><br>
<span class="quotelev2">&gt; &gt;     unsigned long int i, j, e;
</span><br>
<span class="quotelev2">&gt; &gt;     unsigned long matsize;
</span><br>
<span class="quotelev2">&gt; &gt;     MPI_Status Stat;
</span><br>
<span class="quotelev2">&gt; &gt;     struct timeval timing_start, timing_end;
</span><br>
<span class="quotelev2">&gt; &gt;     double inittime = 0;
</span><br>
<span class="quotelev2">&gt; &gt;     long int totaltime = 0;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;     MPI_Init (&amp;argc, &amp;argv);
</span><br>
<span class="quotelev2">&gt; &gt;     MPI_Comm_size (MPI_COMM_WORLD, &amp;numtasks);
</span><br>
<span class="quotelev2">&gt; &gt;     MPI_Comm_rank (MPI_COMM_WORLD, &amp;rank);
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;     if (rank == 0) {
</span><br>
<span class="quotelev2">&gt; &gt;         fprintf (stderr, &quot;Master thread reporting\n&quot;, numtasks - 1);
</span><br>
<span class="quotelev2">&gt; &gt;         matsize = (long) M_COL * M_ROW / 64;
</span><br>
<span class="quotelev2">&gt; &gt;         fprintf (stderr, &quot;matrix size %d kB, time is in [us]\n&quot;,
</span><br>
<span class="quotelev2">&gt; &gt; matsize);
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;         source = 1;
</span><br>
<span class="quotelev2">&gt; &gt;         dest = 1;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;         /*warm up phase*/
</span><br>
<span class="quotelev2">&gt; &gt;         rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);
</span><br>
<span class="quotelev2">&gt; &gt;         rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag,
</span><br>
<span class="quotelev2">&gt; &gt; MPI_COMM_WORLD, &amp;Stat);
</span><br>
<span class="quotelev2">&gt; &gt;         rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);
</span><br>
<span class="quotelev2">&gt; &gt;         rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);
</span><br>
<span class="quotelev2">&gt; &gt;         rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag,
</span><br>
<span class="quotelev2">&gt; &gt; MPI_COMM_WORLD, &amp;Stat);
</span><br>
<span class="quotelev2">&gt; &gt;         rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;         for (e = 0; e &lt; NUM_MSG; e++) {
</span><br>
<span class="quotelev2">&gt; &gt;             exp = pow (2, e);
</span><br>
<span class="quotelev2">&gt; &gt;             exchanged = 64 * exp;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;             /*timing of ops*/
</span><br>
<span class="quotelev2">&gt; &gt;             gettimeofday (&amp;timing_start, NULL);
</span><br>
<span class="quotelev2">&gt; &gt;             rc = MPI_Send (&amp;gigamatrix[0], exchanged,
</span><br>
<span class="quotelev2">&gt; &gt; MPI_UNSIGNED_LONG, dest, tag, MPI_COMM_WORLD);
</span><br>
<span class="quotelev2">&gt; &gt;             rc = MPI_Recv (&amp;gigamatrix[0], exchanged,
</span><br>
<span class="quotelev2">&gt; &gt; MPI_UNSIGNED_LONG, source, tag, MPI_COMM_WORLD, &amp;Stat);
</span><br>
<span class="quotelev2">&gt; &gt;             gettimeofday (&amp;timing_end, NULL);
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;             totaltime = (timing_end.tv_sec - timing_start.tv_sec) *
</span><br>
<span class="quotelev2">&gt; &gt; 1000000 + (timing_end.tv_usec - timing_start.tv_usec);
</span><br>
<span class="quotelev2">&gt; &gt;             memset (&amp;timing_start, 0, sizeof(struct timeval));
</span><br>
<span class="quotelev2">&gt; &gt;             memset (&amp;timing_end, 0, sizeof(struct timeval));
</span><br>
<span class="quotelev2">&gt; &gt;             fprintf (stdout, &quot;%d kB\t%d\n&quot;, exp, totaltime);
</span><br>
<span class="quotelev2">&gt; &gt;         }
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;         fprintf(stderr, &quot;task complete\n&quot;);
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;     } else {
</span><br>
<span class="quotelev2">&gt; &gt;         if (rank &gt;= 1) {
</span><br>
<span class="quotelev2">&gt; &gt;             dest = 0;
</span><br>
<span class="quotelev2">&gt; &gt;             source = 0;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;             rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag,
</span><br>
<span class="quotelev2">&gt; &gt; MPI_COMM_WORLD, &amp;Stat);
</span><br>
<span class="quotelev2">&gt; &gt;             rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag,
</span><br>
<span class="quotelev2">&gt; &gt; MPI_COMM_WORLD);
</span><br>
<span class="quotelev2">&gt; &gt;             rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag,
</span><br>
<span class="quotelev2">&gt; &gt; MPI_COMM_WORLD, &amp;Stat);
</span><br>
<span class="quotelev2">&gt; &gt;             rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag,
</span><br>
<span class="quotelev2">&gt; &gt; MPI_COMM_WORLD, &amp;Stat);
</span><br>
<span class="quotelev2">&gt; &gt;             rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag,
</span><br>
<span class="quotelev2">&gt; &gt; MPI_COMM_WORLD);
</span><br>
<span class="quotelev2">&gt; &gt;             rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag,
</span><br>
<span class="quotelev2">&gt; &gt; MPI_COMM_WORLD, &amp;Stat);
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;             for (e = 0; e &lt; NUM_MSG; e++) {
</span><br>
<span class="quotelev2">&gt; &gt;                 exp = pow (2, e);
</span><br>
<span class="quotelev2">&gt; &gt;                 exchanged = 64 * exp;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;                 rc = MPI_Recv (&amp;gigamatrix[0], (unsigned)
</span><br>
<span class="quotelev2">&gt; &gt; exchanged, MPI_UNSIGNED_LONG, source, tag, MPI_COMM_WORLD, &amp;Stat);
</span><br>
<span class="quotelev2">&gt; &gt;                 rc = MPI_Send (&amp;gigamatrix[0], (unsigned)
</span><br>
<span class="quotelev2">&gt; &gt; exchanged, MPI_UNSIGNED_LONG, dest, tag, MPI_COMM_WORLD);
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;             }
</span><br>
<span class="quotelev2">&gt; &gt;         }
</span><br>
<span class="quotelev2">&gt; &gt;     }
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;     MPI_Finalize ();
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;     return 0;
</span><br>
<span class="quotelev2">&gt; &gt; }
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; _______________________________________________
</span><br>
<span class="quotelev2">&gt; &gt; users mailing list
</span><br>
<span class="quotelev2">&gt; &gt; users_at_[hidden]
</span><br>
<span class="quotelev2">&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1">&gt;
</span><br>
<p><hr>
<ul>
<li>text/html attachment: <a href="http://www.open-mpi.org/community/lists/users/att-8255/attachment">attachment</a>
</ul>
<!-- attachment="attachment" -->
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="8256.php">Jeff Squyres: "Re: [OMPI users] Latest SVN failures"</a>
<li><strong>Previous message:</strong> <a href="8254.php">Jeff Squyres: "Re: [OMPI users] openib RETRY EXCEEDED ERROR"</a>
<li><strong>In reply to:</strong> <a href="8241.php">Vittorio Giovara: "[OMPI users] TCP instead of openIB doesn't work"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="8257.php">Jeff Squyres: "Re: [OMPI users] TCP instead of openIB doesn't work"</a>
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
