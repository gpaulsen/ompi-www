<?
$subject_val = "Re: [OMPI users] Help tracing casue of readv errors";
include("../../include/msg-header.inc");
?>
<!-- received="Wed Nov 25 06:36:08 2009" -->
<!-- isoreceived="20091125113608" -->
<!-- sent="Wed, 25 Nov 2009 12:36:01 +0100" -->
<!-- isosent="20091125113601" -->
<!-- name="Atle Rudshaug" -->
<!-- email="atle_at_[hidden]" -->
<!-- subject="Re: [OMPI users] Help tracing casue of readv errors" -->
<!-- id="4B0D16A1.5070401_at_numericalrocks.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="F6F95316010549478F4D65CED2554929061BD6A6_at_exchange-be5.lancs.local" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>Subject:</strong> Re: [OMPI users] Help tracing casue of readv errors<br>
<strong>From:</strong> Atle Rudshaug (<em>atle_at_[hidden]</em>)<br>
<strong>Date:</strong> 2009-11-25 06:36:01
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11281.php">Ashley Pittman: "Re: [OMPI users] Help tracing casue of readv errors"</a>
<li><strong>Previous message:</strong> <a href="11279.php">Vivek Satpute: "[OMPI users] OpenMPI without IPoIB"</a>
<li><strong>In reply to:</strong> <a href="http://www.open-mpi.org/community/lists/users/2009/09/10767.php">Pacey, Mike: "[OMPI users] Help tracing casue of readv errors"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11281.php">Ashley Pittman: "Re: [OMPI users] Help tracing casue of readv errors"</a>
<li><strong>Reply:</strong> <a href="11281.php">Ashley Pittman: "Re: [OMPI users] Help tracing casue of readv errors"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
Pacey, Mike wrote:
<br>
<span class="quotelev1">&gt; One my users recently reported random hangs of his OpenMPI application.
</span><br>
<span class="quotelev1">&gt; I've run some tests using multiple 2-node 16-core runs of the IMB
</span><br>
<span class="quotelev1">&gt; benchmark and can occasionally replicate the problem. Looking through
</span><br>
<span class="quotelev1">&gt; the mail archive, a previous occurrence of this error seems to been
</span><br>
<span class="quotelev1">&gt; suspect code, but as it's IMB failing here, I suspect the problem lies
</span><br>
<span class="quotelev1">&gt; elsewhere. The full set of errors generated by a failed run are:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; [lancs2-015][[37376,1],2][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
</span><br>
<span class="quotelev1">&gt; mca_btl_tcp_frag_recv: readv failed: Conne
</span><br>
<span class="quotelev1">&gt; ction reset by peer (104)
</span><br>
<span class="quotelev1">&gt; [lancs2-015][[37376,1],6][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
</span><br>
<span class="quotelev1">&gt; mca_btl_tcp_frag_recv: readv failed: Conne
</span><br>
<span class="quotelev1">&gt; ction reset by peer (104)
</span><br>
<span class="quotelev1">&gt; [lancs2-015][[37376,1],8][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
</span><br>
<span class="quotelev1">&gt; [lancs2-015][[37376,1],14][btl_tcp_frag.c:
</span><br>
<span class="quotelev1">&gt; 216:mca_btl_tcp_frag_recv] mca_btl_tcp_frag_recv: readv failed:
</span><br>
<span class="quotelev1">&gt; Connection reset by peer (104)
</span><br>
<span class="quotelev1">&gt; mca_btl_tcp_frag_recv: readv failed: Connection reset by peer (104)
</span><br>
<span class="quotelev1">&gt; [lancs2-015][[37376,1],14][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
</span><br>
<span class="quotelev1">&gt; mca_btl_tcp_frag_recv: readv failed: Conn
</span><br>
<span class="quotelev1">&gt; ection reset by peer (104)
</span><br>
<span class="quotelev1">&gt; [lancs2-015][[37376,1],4][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
</span><br>
<span class="quotelev1">&gt; mca_btl_tcp_frag_recv: readv failed: Conne
</span><br>
<span class="quotelev1">&gt; ction reset by peer (104)
</span><br>
<span class="quotelev1">&gt; [lancs2-015][[37376,1],4][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
</span><br>
<span class="quotelev1">&gt; mca_btl_tcp_frag_recv: readv failed: Conne
</span><br>
<span class="quotelev1">&gt; ction reset by peer (104)
</span><br>
<span class="quotelev1">&gt; [lancs2-015][[37376,1],2][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
</span><br>
<span class="quotelev1">&gt; mca_btl_tcp_frag_recv: readv failed: Conne
</span><br>
<span class="quotelev1">&gt; ction reset by peer (104)
</span><br>
<span class="quotelev1">&gt; [lancs2-015][[37376,1],6][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
</span><br>
<span class="quotelev1">&gt; mca_btl_tcp_frag_recv: readv failed: Conne
</span><br>
<span class="quotelev1">&gt; ction reset by peer (104)
</span><br>
<span class="quotelev1">&gt; [lancs2-015][[37376,1],0][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
</span><br>
<span class="quotelev1">&gt; mca_btl_tcp_frag_recv: readv failed: Conne
</span><br>
<span class="quotelev1">&gt; ction reset by peer (104)
</span><br>
<span class="quotelev1">&gt; [lancs2-015][[37376,1],12][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
</span><br>
<span class="quotelev1">&gt; mca_btl_tcp_frag_recv: readv failed: Conn
</span><br>
<span class="quotelev1">&gt; ection reset by peer (104)
</span><br>
<span class="quotelev1">&gt; [lancs2-015][[37376,1],4][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
</span><br>
<span class="quotelev1">&gt; mca_btl_tcp_frag_recv: readv failed: Conne
</span><br>
<span class="quotelev1">&gt; ction reset by peer (104)
</span><br>
<span class="quotelev1">&gt; [lancs2-015][[37376,1],12][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
</span><br>
<span class="quotelev1">&gt; mca_btl_tcp_frag_recv: readv failed: Conn
</span><br>
<span class="quotelev1">&gt; ection reset by peer (104)
</span><br>
<span class="quotelev1">&gt; [lancs2-015][[37376,1],2][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
</span><br>
<span class="quotelev1">&gt; mca_btl_tcp_frag_recv: readv failed: Conne
</span><br>
<span class="quotelev1">&gt; ction reset by peer (104)
</span><br>
<span class="quotelev1">&gt; [lancs2-015][[37376,1],10][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
</span><br>
<span class="quotelev1">&gt; mca_btl_tcp_frag_recv: readv failed: Conn
</span><br>
<span class="quotelev1">&gt; ection reset by peer (104)
</span><br>
<span class="quotelev1">&gt; [lancs2-015][[37376,1],8][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
</span><br>
<span class="quotelev1">&gt; mca_btl_tcp_frag_recv: readv failed: Conne
</span><br>
<span class="quotelev1">&gt; ction reset by peer (104)
</span><br>
<span class="quotelev1">&gt; [lancs2-015][[37376,1],6][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
</span><br>
<span class="quotelev1">&gt; mca_btl_tcp_frag_recv: readv failed: Conne
</span><br>
<span class="quotelev1">&gt; ction reset by peer (104)
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; I'm used to OpenMPI terminating cleanly, but that's not happening in
</span><br>
<span class="quotelev1">&gt; this case. All the OpenMPI processes on one node terminate, while the
</span><br>
<span class="quotelev1">&gt; processes on the other simply spin with 100% CPU utilisation. I've run
</span><br>
<span class="quotelev1">&gt; this 2-node test a number of times, and I'm not seeing any pattern (ie,
</span><br>
<span class="quotelev1">&gt; I can't pin it down to a single node - a subsequent run using the two
</span><br>
<span class="quotelev1">&gt; nodes involved above ran fine).
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Can anyone provide any pointers in tracking down this problem? System
</span><br>
<span class="quotelev1">&gt; details as follows:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; -	OpenMPI 1.3.3, compiled with gcc version 4.1.2 20080704 (Red Hat
</span><br>
<span class="quotelev1">&gt; 4.1.2-44), using only the -prefix and -with-sge options.
</span><br>
<span class="quotelev1">&gt; -	OS is Scientific Linux SL release 5.3
</span><br>
<span class="quotelev1">&gt; -	CPUs are 2.3GHz Opteron 2356
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Regards,
</span><br>
<span class="quotelev1">&gt; Mike.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; -----
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Dr Mike Pacey,                         Email: M.Pacey_at_[hidden]
</span><br>
<span class="quotelev1">&gt; High Performance Systems Support,      Phone: 01524 593543
</span><br>
<span class="quotelev1">&gt; Information Systems Services,            Fax: 01524 594459
</span><br>
<span class="quotelev1">&gt; Lancaster University,
</span><br>
<span class="quotelev1">&gt; Lancaster LA1 4YW
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; _______________________________________________
</span><br>
<span class="quotelev1">&gt; users mailing list
</span><br>
<span class="quotelev1">&gt; users_at_[hidden]
</span><br>
<span class="quotelev1">&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;   
</span><br>
I got a similar error when using non-blocking communication on large 
<br>
datasets. I could not figure out why this was happening, since it seemed 
<br>
sort of random. I eventually bypassed the problem by switching to 
<br>
blocking communication, which felt kind of sad...If anyone knows if this 
<br>
is a bug in OpenMPI or connected to hardware somehow, please share.
<br>
<p>- Atle
<br>
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="11281.php">Ashley Pittman: "Re: [OMPI users] Help tracing casue of readv errors"</a>
<li><strong>Previous message:</strong> <a href="11279.php">Vivek Satpute: "[OMPI users] OpenMPI without IPoIB"</a>
<li><strong>In reply to:</strong> <a href="http://www.open-mpi.org/community/lists/users/2009/09/10767.php">Pacey, Mike: "[OMPI users] Help tracing casue of readv errors"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="11281.php">Ashley Pittman: "Re: [OMPI users] Help tracing casue of readv errors"</a>
<li><strong>Reply:</strong> <a href="11281.php">Ashley Pittman: "Re: [OMPI users] Help tracing casue of readv errors"</a>
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
