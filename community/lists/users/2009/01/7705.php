<?
$subject_val = "Re: [OMPI users] slow MPI_BCast for messages size from 24K bytes to 800K bytes. (fwd)";
include("../../include/msg-header.inc");
?>
<!-- received="Wed Jan 14 14:39:01 2009" -->
<!-- isoreceived="20090114193901" -->
<!-- sent="Wed, 14 Jan 2009 11:38:55 -0800 (PST)" -->
<!-- isosent="20090114193855" -->
<!-- name="kmuriki_at_[hidden]" -->
<!-- email="kmuriki_at_[hidden]" -->
<!-- subject="Re: [OMPI users] slow MPI_BCast for messages size from 24K bytes to 800K bytes. (fwd)" -->
<!-- id="Pine.LNX.4.64.0901141128570.1227_at_kmuriki.lbl.gov" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="572711CD-CBC2-4F8C-A189-B992BE7AF4F2_at_cisco.com" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>Subject:</strong> Re: [OMPI users] slow MPI_BCast for messages size from 24K bytes to 800K bytes. (fwd)<br>
<strong>From:</strong> <a href="mailto:kmuriki_at_[hidden]?Subject=Re:%20[OMPI%20users]%20slow%20MPI_BCast%20for%20messages%20size%20from%2024K%20bytes%20to%20800K%20bytes.%20(fwd)"><em>kmuriki_at_[hidden]</em></a><br>
<strong>Date:</strong> 2009-01-14 14:38:55
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7706.php">David Robertson: "[OMPI users] MPI_COMM_WORLD not set in Fortran 90"</a>
<li><strong>Previous message:</strong> <a href="7704.php">Jeff Squyres: "Re: [OMPI users] mpirun (signal 15 Termination)"</a>
<li><strong>In reply to:</strong> <a href="7697.php">Jeff Squyres: "Re: [OMPI users] slow MPI_BCast for messages size from 24K bytes to 800K bytes. (fwd)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
Hi Jeff,
<br>
<p>Here is the code with a warmup broadcast of 10K real values and
<br>
actual broadcast of 100K real*8 values(different buffers):
<br>
<p>[kmuriki_at_n0000 pub]$ more testbcast.f90
<br>
program em3d
<br>
implicit real*8 (a-h,o-z)
<br>
include 'mpif.h'
<br>
! em3d_inv main driver
<br>
!  INITIALIZE MPI AND DETERMINE BOTH INDIVIDUAL PROCESSOR #
<br>
!   AND THE TOTAL NUMBER OF PROCESSORS
<br>
!
<br>
integer:: Proc
<br>
real*8, allocatable:: dbuf(:)
<br>
real warmup(10000)
<br>
<p>call MPI_INIT(ierror)
<br>
call MPI_COMM_RANK(MPI_COMM_WORLD,Proc,IERROR)
<br>
call MPI_COMM_SIZE(MPI_COMM_WORLD,Num_Proc,IERROR)
<br>
<p>ndat=100000
<br>
<p>!print*,'bcasting to no of tasks',num_proc
<br>
allocate(dbuf(ndat))
<br>
do i=1,ndat
<br>
&nbsp;&nbsp;&nbsp;dbuf(i)=dble(i)
<br>
enddo
<br>
<p>do i=1,10000
<br>
&nbsp;&nbsp;&nbsp;warmup(i)=(i)
<br>
enddo
<br>
<p>!print*, 'Making warmup BCAST',proc
<br>
call MPI_BCAST(warmup,10000, &amp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MPI_REAL,0,MPI_COMM_WORLD,ierror)
<br>
<p>t1=MPI_WTIME()
<br>
call MPI_BCAST(dbuf,ndat, &amp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,ierror)
<br>
!print*, 'Done with call to broadcast',proc
<br>
t2=MPI_WTIME()
<br>
write(*,*)'time for bcast',t2-t1
<br>
<p>deallocate(dbuf)
<br>
call MPI_FINALIZE(IERROR)
<br>
end program em3d
<br>
[kmuriki_at_n0000 pub]$ !mpif90
<br>
mpif90 -o testbcast testbcast.f90
<br>
testbcast.f90(20): (col. 1) remark: LOOP WAS VECTORIZED.
<br>
testbcast.f90(24): (col. 1) remark: LOOP WAS VECTORIZED.
<br>
/global/software/centos-5.x86_64/modules/intel/fce/10.1.018/lib/libimf.so: 
<br>
warning: warning: feupdateenv is not implemented and will always fail
<br>
[kmuriki_at_n0000 pub]$ !mpirun
<br>
mpirun -v -display-map -mca btl openib,self -mca mpi_leave_pinned 1 
<br>
-hostfile ./hostfile.geophys -np 4 ./testbcast
<br>
[n0000.scs00:12909]  Map for job: 1     Generated by mapping mode: byslot
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Starting vpid: 0        Vpid range: 4   Num app_contexts: 1
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data for app_context: index 0   app: ./testbcast
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Num procs: 4
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Argv[0]: ./testbcast
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[0]: OMPI_MCA_btl=openib,self
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[1]: OMPI_MCA_mpi_leave_pinned=1
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[2]: OMPI_MCA_rmaps_base_display_map=1
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[3]: OMPI_MCA_rds_hostfile_path=./hostfile.geophys
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[4]: 
<br>
OMPI_MCA_orte_precondition_transports=1e4532db63da3056-33551606203d9c19
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[5]: OMPI_MCA_rds=proxy
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[6]: OMPI_MCA_ras=proxy
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[7]: OMPI_MCA_rmaps=proxy
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[8]: OMPI_MCA_pls=proxy
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Env[9]: OMPI_MCA_rmgr=proxy
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Working dir: 
<br>
/global/home/users/kmuriki/sample_executables/pub (user: 0)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Num maps: 0
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Num elements in nodes list: 2
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mapped node:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cell: 0 Nodename: n0015.geophys Launch id: -1   Username: 
<br>
NULL
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Daemon name:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data type: ORTE_PROCESS_NAME    Data Value: NULL
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Oversubscribed: False   Num elements in procs list: 2
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mapped proc:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Proc Name:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data type: ORTE_PROCESS_NAME    Data Value: 
<br>
[0,1,0]
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Proc Rank: 0    Proc PID: 0     App_context index: 
<br>
0
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mapped proc:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Proc Name:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data type: ORTE_PROCESS_NAME    Data Value: 
<br>
[0,1,1]
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Proc Rank: 1    Proc PID: 0     App_context index: 
<br>
0
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mapped node:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cell: 0 Nodename: n0016.geophys Launch id: -1   Username: 
<br>
NULL
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Daemon name:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data type: ORTE_PROCESS_NAME    Data Value: NULL
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Oversubscribed: False   Num elements in procs list: 2
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mapped proc:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Proc Name:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data type: ORTE_PROCESS_NAME    Data Value: 
<br>
[0,1,2]
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Proc Rank: 2    Proc PID: 0     App_context index: 
<br>
0
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mapped proc:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Proc Name:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data type: ORTE_PROCESS_NAME    Data Value: 
<br>
[0,1,3]
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Proc Rank: 3    Proc PID: 0     App_context index: 
<br>
0
<br>
&nbsp;&nbsp;time for bcast  5.556106567382812E-003
<br>
&nbsp;&nbsp;time for bcast  5.569934844970703E-003
<br>
&nbsp;&nbsp;time for bcast  2.491402626037598E-002
<br>
&nbsp;&nbsp;time for bcast  2.490019798278809E-002
<br>
[kmuriki_at_n0000 pub]$
<br>
<p>If I reduce the warmup size from 10K to 1K, below is the output:
<br>
<p>&nbsp;&nbsp;time for bcast  2.994060516357422E-003
<br>
&nbsp;&nbsp;time for bcast  2.840995788574219E-003
<br>
&nbsp;&nbsp;time for bcast   52.0005199909210
<br>
&nbsp;&nbsp;time for bcast   52.0438468456268
<br>
<p>May be when I tried 1K size warmup, as the size is small it just 
<br>
used copy in/copy out semantics and the RDMA buffers are not setup
<br>
so the actual bcast was slow ! and when I used 10K size warmup it
<br>
did setup RDMA buffers and hence the actual bcast was quick !.
<br>
<p>Is it possible to see more diagnostic output from mpirun command
<br>
with any addiitional option to see if its doing copyin/copyout etc... ?
<br>
Like with Myrinet mpirun '-v' option gives a lot of diagnostic output.
<br>
<p>Finally below are the numbers with IB and gige when I try to run
<br>
Bcast in IMB, which looks good:
<br>
<p>[kmuriki_at_n0000 runIMB]$ mpirun -v -np 4 --mca btl openib,self -hostfile 
<br>
../pub/hostfile.geophys 
<br>
/global/home/groups/scs/tests/IMB/IMB_3.1/src/IMB-MPI1 -npmin 4 bcast
<br>
#---------------------------------------------------
<br>
#    Intel (R) MPI Benchmark Suite V3.1, MPI-1 part
<br>
#---------------------------------------------------
<br>
# Date                  : Wed Jan 14 11:37:54 2009
<br>
# Machine               : x86_64
<br>
# System                : Linux
<br>
# Release               : 2.6.18-92.1.18.el5
<br>
# Version               : #1 SMP Wed Nov 12 09:19:49 EST 2008
<br>
# MPI Version           : 2.0
<br>
# MPI Thread Environment: MPI_THREAD_SINGLE
<br>
<p><p><p># Calling sequence was:
<br>
<p># /global/home/groups/scs/tests/IMB/IMB_3.1/src/IMB-MPI1 -npmin 4 bcast
<br>
<p># Minimum message length in bytes:   0
<br>
# Maximum message length in bytes:   4194304
<br>
#
<br>
# MPI_Datatype                   :   MPI_BYTE
<br>
# MPI_Datatype for reductions    :   MPI_FLOAT
<br>
# MPI_Op                         :   MPI_SUM
<br>
#
<br>
#
<br>
<p># List of Benchmarks to run:
<br>
<p># Bcast
<br>
<p>#----------------------------------------------------------------
<br>
# Benchmarking Bcast
<br>
# #processes = 4
<br>
#----------------------------------------------------------------
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0         1000         0.05         0.05         0.05
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1         1000        11.73        11.75        11.75
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2         1000        10.38        10.40        10.39
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4         1000        10.26        10.28        10.27
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8         1000        10.43        10.45        10.44
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16         1000        10.26        10.28        10.27
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;32         1000        10.46        10.48        10.47
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;64         1000        10.47        10.49        10.48
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;128         1000        10.41        10.43        10.42
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;256         1000        11.13        11.15        11.14
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;512         1000        11.30        11.31        11.31
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1024         1000        14.45        14.47        14.47
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2048         1000        26.03        26.05        26.04
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4096         1000        44.00        44.04        44.02
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8192         1000        72.21        72.28        72.26
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16384         1000       135.48       135.60       135.56
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;32768         1000       297.64       297.71       297.67
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;65536          640       579.20       579.37       579.28
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;131072          320      1174.31      1174.81      1174.57
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;262144          160      2484.21      2486.33      2485.28
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;524288           80      2686.47      2695.13      2690.80
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1048576           40      5706.35      5740.59      5723.47
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2097152           20     10705.90     10761.65     10742.98
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4194304           10     21567.58     21678.50     21641.65
<br>
[kmuriki_at_n0000 runIMB]$ mpirun -v -np 4 --mca btl tcp,self -hostfile 
<br>
../pub/hostfile.geophys 
<br>
/global/home/groups/scs/tests/IMB/IMB_3.1/src/IMB-MPI1 -npmin 4 bcast
<br>
#---------------------------------------------------
<br>
#    Intel (R) MPI Benchmark Suite V3.1, MPI-1 part
<br>
#---------------------------------------------------
<br>
# Date                  : Wed Jan 14 11:38:01 2009
<br>
# Machine               : x86_64
<br>
# System                : Linux
<br>
# Release               : 2.6.18-92.1.18.el5
<br>
# Version               : #1 SMP Wed Nov 12 09:19:49 EST 2008
<br>
# MPI Version           : 2.0
<br>
# MPI Thread Environment: MPI_THREAD_SINGLE
<br>
<p><p><p># Calling sequence was:
<br>
<p># /global/home/groups/scs/tests/IMB/IMB_3.1/src/IMB-MPI1 -npmin 4 bcast
<br>
<p># Minimum message length in bytes:   0
<br>
# Maximum message length in bytes:   4194304
<br>
#
<br>
# MPI_Datatype                   :   MPI_BYTE
<br>
# MPI_Datatype for reductions    :   MPI_FLOAT
<br>
# MPI_Op                         :   MPI_SUM
<br>
#
<br>
#
<br>
<p># List of Benchmarks to run:
<br>
<p># Bcast
<br>
<p>#----------------------------------------------------------------
<br>
# Benchmarking Bcast
<br>
# #processes = 4
<br>
#----------------------------------------------------------------
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0         1000         0.05         0.06         0.05
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1         1000        51.23        51.31        51.25
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2         1000        49.98        50.08        50.01
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4         1000        49.93        50.08        49.97
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8         1000        51.23        51.39        51.27
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16         1000        49.92        50.04        49.96
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;32         1000        49.88        50.02        49.93
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;64         1000        49.94        50.07        49.99
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;128         1000        50.03        50.19        50.08
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;256         1000        53.46        53.62        53.53
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;512         1000        62.36        62.52        62.41
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1024         1000        74.82        75.05        74.89
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2048         1000       190.87       191.09       190.98
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4096         1000       215.01       215.29       215.20
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8192         1000       285.16       285.41       285.28
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16384         1000       426.49       426.79       426.64
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;32768         1000       680.94       681.29       681.16
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;65536          640      1148.72      1149.69      1149.34
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;131072          320      2511.92      2512.13      2512.03
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;262144          160      4716.58      4717.14      4716.86
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;524288           80      8010.99      8016.05      8013.21
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1048576           40     16657.90     16676.32     16667.73
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2097152           20     27720.20     27916.86     27825.34
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4194304           10     54355.69     54781.70     54585.30
<br>
[kmuriki_at_n0000 runIMB]$
<br>
<p>thanks,
<br>
Krishna.
<br>
<p>On Wed, 14 Jan 2009, Jeff Squyres wrote:
<br>
<p><span class="quotelev1">&gt; On Jan 13, 2009, at 3:32 PM, kmuriki_at_[hidden] wrote:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; With IB, there's also the issue of registered memory.  Open MPI v1.2.x 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; defaults to copy in/copy out semantics (with pre-registered memory) until 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; the message reaches a certain size, and then it uses a pipelined 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; register/RDMA protocol.  However, even with copy in/out semantics of small 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; messages, the resulting broadcast should still be much faster than over 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; gige.
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Are you using the same buffer for the warmup bcast as the actual bcast? 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; You might try using &quot;--mca mpi_leave_pinned 1&quot; to see if that helps as 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; well (will likely only help with large messages).
</span><br>
<span class="quotelev2">&gt;&gt; 
</span><br>
<span class="quotelev2">&gt;&gt; I'm using different buffers for warmup and actual bcast. I tried the 
</span><br>
<span class="quotelev2">&gt;&gt; mpi_leave_pinned 1, but did not see any difference in behaviour.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; In this case, you likely won't see much of a difference -- mpi_leave_pinned 
</span><br>
<span class="quotelev1">&gt; will generally only be a boost for long messages that use the same buffers 
</span><br>
<span class="quotelev1">&gt; repeatedly.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev2">&gt;&gt; May be when ever the openmpi defaults to copy in/copy out semantics on my
</span><br>
<span class="quotelev2">&gt;&gt; cluster its performing very slow (than gige) but not when it uses RDMA.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; That would be quite surprising.  I still think there's some kind of startup 
</span><br>
<span class="quotelev1">&gt; overhead going on here.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Surprisingly just doing two consecutive 80K byte MPI_BCASTs
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; performs very quick (forget about warmup and actual broadcast).
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; wheres as a single 80K broadcast is slow. Not sure if I'm missing
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; anything!.
</span><br>
<span class="quotelev3">&gt;&gt;&gt; There's also the startup time and synchronization issues.  Remember that 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; although MPI_BCAST does not provide any synchronization guarantees, it 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; could well be that the 1st bcast effectively synchronizes the processes 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; and the 2nd one therefore runs much faster (because individual processes 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; won't need to spend much time blocking waiting for messages because 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; they're effectively operating in lock step after the first bcast).
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Benchmarking is a very tricky business; it can be extremely difficult to 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; precisely measure exactly what you want to measure.
</span><br>
<span class="quotelev2">&gt;&gt; 
</span><br>
<span class="quotelev2">&gt;&gt; My main effort here is not to benchmark my cluster but to resolve a
</span><br>
<span class="quotelev2">&gt;&gt; user problem, where in he complained that his bcasts are running very slow. 
</span><br>
<span class="quotelev2">&gt;&gt; I tried to recreate the situation with a simple fortran program
</span><br>
<span class="quotelev2">&gt;&gt; which just performs a bcast of size similar in his code. It also performed
</span><br>
<span class="quotelev2">&gt;&gt; very slow (than gige) then I started increasing and decreasing the sizes
</span><br>
<span class="quotelev2">&gt;&gt; of bcast to observe that it performs slow only in the range 8K bytes
</span><br>
<span class="quotelev2">&gt;&gt; to 100K bytes.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Can you send your modified test program (with a warmup send)?
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; What happens if you run a benchmark like the broadcast section of IMB on TCP 
</span><br>
<span class="quotelev1">&gt; and IB?
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; -- 
</span><br>
<span class="quotelev1">&gt; Jeff Squyres
</span><br>
<span class="quotelev1">&gt; Cisco Systems
</span><br>
<span class="quotelev1">&gt;
</span><br>
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7706.php">David Robertson: "[OMPI users] MPI_COMM_WORLD not set in Fortran 90"</a>
<li><strong>Previous message:</strong> <a href="7704.php">Jeff Squyres: "Re: [OMPI users] mpirun (signal 15 Termination)"</a>
<li><strong>In reply to:</strong> <a href="7697.php">Jeff Squyres: "Re: [OMPI users] slow MPI_BCast for messages size from 24K bytes to 800K bytes. (fwd)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
