<?
$subject_val = "Re: [OMPI users] slow MPI_BCast for messages size from 24K bytes to	800K bytes.";
include("../../include/msg-header.inc");
?>
<!-- received="Sun Jan 11 20:40:06 2009" -->
<!-- isoreceived="20090112014006" -->
<!-- sent="Sun, 11 Jan 2009 17:40:00 -0800" -->
<!-- isosent="20090112014000" -->
<!-- name="David Prendergast" -->
<!-- email="dgprendergast_at_[hidden]" -->
<!-- subject="Re: [OMPI users] slow MPI_BCast for messages size from 24K bytes to	800K bytes." -->
<!-- id="496A9F70.8070607_at_lbl.gov" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="Pine.LNX.4.64.0901091746520.29471_at_kmuriki.lbl.gov" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>Subject:</strong> Re: [OMPI users] slow MPI_BCast for messages size from 24K bytes to	800K bytes.<br>
<strong>From:</strong> David Prendergast (<em>dgprendergast_at_[hidden]</em>)<br>
<strong>Date:</strong> 2009-01-11 20:40:00
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7676.php">Thomas Ropars: "Re: [OMPI users] Error message when using MPI_Type_struct()"</a>
<li><strong>Previous message:</strong> <a href="7674.php">Hana Milani: "[OMPI users] problem with xfmpi_sane"</a>
<li><strong>In reply to:</strong> <a href="7673.php">kmuriki_at_[hidden]: "[OMPI users] slow MPI_BCast for messages size from 24K bytes to 800K bytes."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7680.php">Jeff Squyres: "Re: [OMPI users] slow MPI_BCast for messages size from 24K bytes to 800K bytes."</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
Hey Krishna,
<br>
Is this part of the reason that our users are seeing a significant 
<br>
slowdown when they go beyond using 2 nodes with espresso? You should try 
<br>
that as an example. It's surprising that using more than 2 nodes can 
<br>
lead to a slower wall time for calculations than using 2 alone.
<br>
David
<br>
<p>KMuriki_at_[hidden] wrote:
<br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Hello there,
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; We have a DDR IB cluster with Open MPI ver 1.2.8.
</span><br>
<span class="quotelev1">&gt; I'm testing on two nodes with two processors each and both
</span><br>
<span class="quotelev1">&gt; the nodes are adjacent (2 hops distant) on the same leaf
</span><br>
<span class="quotelev1">&gt; of the tree interconnect.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; I observe that when I try to MPI_BCAST among the four MPI
</span><br>
<span class="quotelev1">&gt; tasks it takes a lot of time with IB network (more than
</span><br>
<span class="quotelev1">&gt; the GiGE network) when the payload sizes range from 24K bytes
</span><br>
<span class="quotelev1">&gt; to 800K bytes.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; For payloads below 8K bytes and above 200K bytes the performance
</span><br>
<span class="quotelev1">&gt; is acceptable.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Any suggestions on how I debug this and locate the source of
</span><br>
<span class="quotelev1">&gt; the problem ? (More info below) Please let me know if you need
</span><br>
<span class="quotelev1">&gt; any more information from my side.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; thanks for your time,
</span><br>
<span class="quotelev1">&gt; Krishna Muriki,
</span><br>
<span class="quotelev1">&gt; HPC User Services,
</span><br>
<span class="quotelev1">&gt; Scientific Cluster Support,
</span><br>
<span class="quotelev1">&gt; Lawrence Berkeley National Laboratory.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; I) Payload size 8M bytes over IB:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; [kmuriki_at_n0005 pub]$ mpirun -v -display-map --mca btl openib,self -np 
</span><br>
<span class="quotelev1">&gt; 4 -hostfile hostfile.lr ./testbcast.8000000
</span><br>
<span class="quotelev1">&gt; [n0005.scs00:13902]  Map for job: 1     Generated by mapping mode: byslot
</span><br>
<span class="quotelev1">&gt;         Starting vpid: 0        Vpid range: 4   Num app_contexts: 1
</span><br>
<span class="quotelev1">&gt;         Data for app_context: index 0   app: ./testbcast.8000000
</span><br>
<span class="quotelev1">&gt;                 Num procs: 4
</span><br>
<span class="quotelev1">&gt;                 Argv[0]: ./testbcast.8000000
</span><br>
<span class="quotelev1">&gt;                 Env[0]: OMPI_MCA_btl=openib,self
</span><br>
<span class="quotelev1">&gt;                 Env[1]: OMPI_MCA_rmaps_base_display_map=1
</span><br>
<span class="quotelev1">&gt;                 Env[2]: OMPI_MCA_rds_hostfile_path=hostfile.lr
</span><br>
<span class="quotelev1">&gt;                 Env[3]: 
</span><br>
<span class="quotelev1">&gt; OMPI_MCA_orte_precondition_transports=1405b3b501aa4086-00dbc7151c7348e1
</span><br>
<span class="quotelev1">&gt;                 Env[4]: OMPI_MCA_rds=proxy
</span><br>
<span class="quotelev1">&gt;                 Env[5]: OMPI_MCA_ras=proxy
</span><br>
<span class="quotelev1">&gt;                 Env[6]: OMPI_MCA_rmaps=proxy
</span><br>
<span class="quotelev1">&gt;                 Env[7]: OMPI_MCA_pls=proxy
</span><br>
<span class="quotelev1">&gt;                 Env[8]: OMPI_MCA_rmgr=proxy
</span><br>
<span class="quotelev1">&gt;                 Working dir: 
</span><br>
<span class="quotelev1">&gt; /global/home/users/kmuriki/sample_executables/pub (user: 0)
</span><br>
<span class="quotelev1">&gt;                 Num maps: 0
</span><br>
<span class="quotelev1">&gt;         Num elements in nodes list: 2
</span><br>
<span class="quotelev1">&gt;         Mapped node:
</span><br>
<span class="quotelev1">&gt;                 Cell: 0 Nodename: n0172.lr      Launch id: -1   
</span><br>
<span class="quotelev1">&gt; Username: NULL
</span><br>
<span class="quotelev1">&gt;                 Daemon name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: NULL
</span><br>
<span class="quotelev1">&gt;                 Oversubscribed: False   Num elements in procs list: 2
</span><br>
<span class="quotelev1">&gt;                 Mapped proc:
</span><br>
<span class="quotelev1">&gt;                         Proc Name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: 
</span><br>
<span class="quotelev1">&gt; [0,1,0]
</span><br>
<span class="quotelev1">&gt;                         Proc Rank: 0    Proc PID: 0     App_context 
</span><br>
<span class="quotelev1">&gt; index: 0
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;                 Mapped proc:
</span><br>
<span class="quotelev1">&gt;                         Proc Name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: 
</span><br>
<span class="quotelev1">&gt; [0,1,1]
</span><br>
<span class="quotelev1">&gt;                         Proc Rank: 1    Proc PID: 0     App_context 
</span><br>
<span class="quotelev1">&gt; index: 0
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;         Mapped node:
</span><br>
<span class="quotelev1">&gt;                 Cell: 0 Nodename: n0173.lr      Launch id: -1   
</span><br>
<span class="quotelev1">&gt; Username: NULL
</span><br>
<span class="quotelev1">&gt;                 Daemon name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: NULL
</span><br>
<span class="quotelev1">&gt;                 Oversubscribed: False   Num elements in procs list: 2
</span><br>
<span class="quotelev1">&gt;                 Mapped proc:
</span><br>
<span class="quotelev1">&gt;                         Proc Name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: 
</span><br>
<span class="quotelev1">&gt; [0,1,2]
</span><br>
<span class="quotelev1">&gt;                         Proc Rank: 2    Proc PID: 0     App_context 
</span><br>
<span class="quotelev1">&gt; index: 0
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;                 Mapped proc:
</span><br>
<span class="quotelev1">&gt;                         Proc Name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: 
</span><br>
<span class="quotelev1">&gt; [0,1,3]
</span><br>
<span class="quotelev1">&gt;                         Proc Rank: 3    Proc PID: 0     App_context 
</span><br>
<span class="quotelev1">&gt; index: 0
</span><br>
<span class="quotelev1">&gt;  About to call broadcast           3
</span><br>
<span class="quotelev1">&gt;  About to call broadcast           1
</span><br>
<span class="quotelev1">&gt;  About to call broadcast           2
</span><br>
<span class="quotelev1">&gt;  About to call broadcast           0
</span><br>
<span class="quotelev1">&gt;  Done with call to broadcast           2
</span><br>
<span class="quotelev1">&gt;  time for bcast  0.133496046066284
</span><br>
<span class="quotelev1">&gt;  Done with call to broadcast           3
</span><br>
<span class="quotelev1">&gt;  time for bcast  0.148098945617676
</span><br>
<span class="quotelev1">&gt;  Done with call to broadcast           0
</span><br>
<span class="quotelev1">&gt;  time for bcast  0.113168954849243
</span><br>
<span class="quotelev1">&gt;  Done with call to broadcast           1
</span><br>
<span class="quotelev1">&gt;  time for bcast  0.145189046859741
</span><br>
<span class="quotelev1">&gt; [kmuriki_at_n0005 pub]$
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; II) Payload size 80K bytes using GiGE Network:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; [kmuriki_at_n0005 pub]$ mpirun -v -display-map --mca btl tcp,self -np 4 
</span><br>
<span class="quotelev1">&gt; -hostfile hostfile.lr ./testbcast.80000
</span><br>
<span class="quotelev1">&gt; [n0005.scs00:13928]  Map for job: 1     Generated by mapping mode: byslot
</span><br>
<span class="quotelev1">&gt;         Starting vpid: 0        Vpid range: 4   Num app_contexts: 1
</span><br>
<span class="quotelev1">&gt;         Data for app_context: index 0   app: ./testbcast.80000
</span><br>
<span class="quotelev1">&gt;                 Num procs: 4
</span><br>
<span class="quotelev1">&gt;                 Argv[0]: ./testbcast.80000
</span><br>
<span class="quotelev1">&gt;                 Env[0]: OMPI_MCA_btl=tcp,self
</span><br>
<span class="quotelev1">&gt;                 Env[1]: OMPI_MCA_rmaps_base_display_map=1
</span><br>
<span class="quotelev1">&gt;                 Env[2]: OMPI_MCA_rds_hostfile_path=hostfile.lr
</span><br>
<span class="quotelev1">&gt;                 Env[3]: 
</span><br>
<span class="quotelev1">&gt; OMPI_MCA_orte_precondition_transports=305b93d4acc82685-12bbf20d2e6d250b
</span><br>
<span class="quotelev1">&gt;                 Env[4]: OMPI_MCA_rds=proxy
</span><br>
<span class="quotelev1">&gt;                 Env[5]: OMPI_MCA_ras=proxy
</span><br>
<span class="quotelev1">&gt;                 Env[6]: OMPI_MCA_rmaps=proxy
</span><br>
<span class="quotelev1">&gt;                 Env[7]: OMPI_MCA_pls=proxy
</span><br>
<span class="quotelev1">&gt;                 Env[8]: OMPI_MCA_rmgr=proxy
</span><br>
<span class="quotelev1">&gt;                 Working dir: 
</span><br>
<span class="quotelev1">&gt; /global/home/users/kmuriki/sample_executables/pub (user: 0)
</span><br>
<span class="quotelev1">&gt;                 Num maps: 0
</span><br>
<span class="quotelev1">&gt;         Num elements in nodes list: 2
</span><br>
<span class="quotelev1">&gt;         Mapped node:
</span><br>
<span class="quotelev1">&gt;                 Cell: 0 Nodename: n0172.lr      Launch id: -1   
</span><br>
<span class="quotelev1">&gt; Username: NULL
</span><br>
<span class="quotelev1">&gt;                 Daemon name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: NULL
</span><br>
<span class="quotelev1">&gt;                 Oversubscribed: False   Num elements in procs list: 2
</span><br>
<span class="quotelev1">&gt;                 Mapped proc:
</span><br>
<span class="quotelev1">&gt;                         Proc Name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: 
</span><br>
<span class="quotelev1">&gt; [0,1,0]
</span><br>
<span class="quotelev1">&gt;                         Proc Rank: 0    Proc PID: 0     App_context 
</span><br>
<span class="quotelev1">&gt; index: 0
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;                 Mapped proc:
</span><br>
<span class="quotelev1">&gt;                         Proc Name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: 
</span><br>
<span class="quotelev1">&gt; [0,1,1]
</span><br>
<span class="quotelev1">&gt;                         Proc Rank: 1    Proc PID: 0     App_context 
</span><br>
<span class="quotelev1">&gt; index: 0
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;         Mapped node:
</span><br>
<span class="quotelev1">&gt;                 Cell: 0 Nodename: n0173.lr      Launch id: -1   
</span><br>
<span class="quotelev1">&gt; Username: NULL
</span><br>
<span class="quotelev1">&gt;                 Daemon name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: NULL
</span><br>
<span class="quotelev1">&gt;                 Oversubscribed: False   Num elements in procs list: 2
</span><br>
<span class="quotelev1">&gt;                 Mapped proc:
</span><br>
<span class="quotelev1">&gt;                         Proc Name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: 
</span><br>
<span class="quotelev1">&gt; [0,1,2]
</span><br>
<span class="quotelev1">&gt;                         Proc Rank: 2    Proc PID: 0     App_context 
</span><br>
<span class="quotelev1">&gt; index: 0
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;                 Mapped proc:
</span><br>
<span class="quotelev1">&gt;                         Proc Name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: 
</span><br>
<span class="quotelev1">&gt; [0,1,3]
</span><br>
<span class="quotelev1">&gt;                         Proc Rank: 3    Proc PID: 0     App_context 
</span><br>
<span class="quotelev1">&gt; index: 0
</span><br>
<span class="quotelev1">&gt;  About to call broadcast           0
</span><br>
<span class="quotelev1">&gt;  About to call broadcast           2
</span><br>
<span class="quotelev1">&gt;  About to call broadcast           1
</span><br>
<span class="quotelev1">&gt;  Done with call to broadcast           2
</span><br>
<span class="quotelev1">&gt;  time for bcast  7.137393951416016E-002
</span><br>
<span class="quotelev1">&gt;  About to call broadcast           3
</span><br>
<span class="quotelev1">&gt;  Done with call to broadcast           3
</span><br>
<span class="quotelev1">&gt;  time for bcast  1.110005378723145E-002
</span><br>
<span class="quotelev1">&gt;  Done with call to broadcast           0
</span><br>
<span class="quotelev1">&gt;  time for bcast  7.121706008911133E-002
</span><br>
<span class="quotelev1">&gt;  Done with call to broadcast           1
</span><br>
<span class="quotelev1">&gt;  time for bcast  3.379988670349121E-002
</span><br>
<span class="quotelev1">&gt; [kmuriki_at_n0005 pub]$
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; III) Payload size 80K bytes using IB Network:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; [kmuriki_at_n0005 pub]$ mpirun -v -display-map --mca btl openib,self -np 
</span><br>
<span class="quotelev1">&gt; 4 -hostfile hostfile.lr ./testbcast.80000
</span><br>
<span class="quotelev1">&gt; [n0005.scs00:13941]  Map for job: 1     Generated by mapping mode: byslot
</span><br>
<span class="quotelev1">&gt;         Starting vpid: 0        Vpid range: 4   Num app_contexts: 1
</span><br>
<span class="quotelev1">&gt;         Data for app_context: index 0   app: ./testbcast.80000
</span><br>
<span class="quotelev1">&gt;                 Num procs: 4
</span><br>
<span class="quotelev1">&gt;                 Argv[0]: ./testbcast.80000
</span><br>
<span class="quotelev1">&gt;                 Env[0]: OMPI_MCA_btl=openib,self
</span><br>
<span class="quotelev1">&gt;                 Env[1]: OMPI_MCA_rmaps_base_display_map=1
</span><br>
<span class="quotelev1">&gt;                 Env[2]: OMPI_MCA_rds_hostfile_path=hostfile.lr
</span><br>
<span class="quotelev1">&gt;                 Env[3]: 
</span><br>
<span class="quotelev1">&gt; OMPI_MCA_orte_precondition_transports=4cdb5ae2babe9010-709842ac574605f9
</span><br>
<span class="quotelev1">&gt;                 Env[4]: OMPI_MCA_rds=proxy
</span><br>
<span class="quotelev1">&gt;                 Env[5]: OMPI_MCA_ras=proxy
</span><br>
<span class="quotelev1">&gt;                 Env[6]: OMPI_MCA_rmaps=proxy
</span><br>
<span class="quotelev1">&gt;                 Env[7]: OMPI_MCA_pls=proxy
</span><br>
<span class="quotelev1">&gt;                 Env[8]: OMPI_MCA_rmgr=proxy
</span><br>
<span class="quotelev1">&gt;                 Working dir: 
</span><br>
<span class="quotelev1">&gt; /global/home/users/kmuriki/sample_executables/pub (user: 0)
</span><br>
<span class="quotelev1">&gt;                 Num maps: 0
</span><br>
<span class="quotelev1">&gt;         Num elements in nodes list: 2
</span><br>
<span class="quotelev1">&gt;         Mapped node:
</span><br>
<span class="quotelev1">&gt;                 Cell: 0 Nodename: n0172.lr      Launch id: -1   
</span><br>
<span class="quotelev1">&gt; Username: NULL
</span><br>
<span class="quotelev1">&gt;                 Daemon name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: NULL
</span><br>
<span class="quotelev1">&gt;                 Oversubscribed: False   Num elements in procs list: 2
</span><br>
<span class="quotelev1">&gt;                 Mapped proc:
</span><br>
<span class="quotelev1">&gt;                         Proc Name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: 
</span><br>
<span class="quotelev1">&gt; [0,1,0]
</span><br>
<span class="quotelev1">&gt;                         Proc Rank: 0    Proc PID: 0     App_context 
</span><br>
<span class="quotelev1">&gt; index: 0
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;                 Mapped proc:
</span><br>
<span class="quotelev1">&gt;                         Proc Name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: 
</span><br>
<span class="quotelev1">&gt; [0,1,1]
</span><br>
<span class="quotelev1">&gt;                         Proc Rank: 1    Proc PID: 0     App_context 
</span><br>
<span class="quotelev1">&gt; index: 0
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;         Mapped node:
</span><br>
<span class="quotelev1">&gt;                 Cell: 0 Nodename: n0173.lr      Launch id: -1   
</span><br>
<span class="quotelev1">&gt; Username: NULL
</span><br>
<span class="quotelev1">&gt;                 Daemon name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: NULL
</span><br>
<span class="quotelev1">&gt;                 Oversubscribed: False   Num elements in procs list: 2
</span><br>
<span class="quotelev1">&gt;                 Mapped proc:
</span><br>
<span class="quotelev1">&gt;                         Proc Name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: 
</span><br>
<span class="quotelev1">&gt; [0,1,2]
</span><br>
<span class="quotelev1">&gt;                         Proc Rank: 2    Proc PID: 0     App_context 
</span><br>
<span class="quotelev1">&gt; index: 0
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;                 Mapped proc:
</span><br>
<span class="quotelev1">&gt;                         Proc Name:
</span><br>
<span class="quotelev1">&gt;                         Data type: ORTE_PROCESS_NAME    Data Value: 
</span><br>
<span class="quotelev1">&gt; [0,1,3]
</span><br>
<span class="quotelev1">&gt;                         Proc Rank: 3    Proc PID: 0     App_context 
</span><br>
<span class="quotelev1">&gt; index: 0
</span><br>
<span class="quotelev1">&gt;  About to call broadcast           0
</span><br>
<span class="quotelev1">&gt;  About to call broadcast           3
</span><br>
<span class="quotelev1">&gt;  About to call broadcast           1
</span><br>
<span class="quotelev1">&gt;  Done with call to broadcast           1
</span><br>
<span class="quotelev1">&gt;  time for bcast  2.550005912780762E-002
</span><br>
<span class="quotelev1">&gt;  About to call broadcast           2
</span><br>
<span class="quotelev1">&gt;  Done with call to broadcast           2
</span><br>
<span class="quotelev1">&gt;  time for bcast  2.154898643493652E-002
</span><br>
<span class="quotelev1">&gt;  Done with call to broadcast           3
</span><br>
<span class="quotelev1">&gt;  Done with call to broadcast           0
</span><br>
<span class="quotelev1">&gt;  time for bcast   38.1956140995026
</span><br>
<span class="quotelev1">&gt;  time for bcast   38.2115209102631
</span><br>
<span class="quotelev1">&gt; [kmuriki_at_n0005 pub]$
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Finally here is the fortran code I'm playing with and I'm modifying the
</span><br>
<span class="quotelev1">&gt; payload size by changing the value of the variable 'ndat':
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; [kmuriki_at_n0005 pub]$ more testbcast.f90
</span><br>
<span class="quotelev1">&gt; program em3d
</span><br>
<span class="quotelev1">&gt; implicit real*8 (a-h,o-z)
</span><br>
<span class="quotelev1">&gt; include 'mpif.h'
</span><br>
<span class="quotelev1">&gt; ! em3d_inv main driver
</span><br>
<span class="quotelev1">&gt; !  INITIALIZE MPI AND DETERMINE BOTH INDIVIDUAL PROCESSOR #
</span><br>
<span class="quotelev1">&gt; !   AND THE TOTAL NUMBER OF PROCESSORS
</span><br>
<span class="quotelev1">&gt; !
</span><br>
<span class="quotelev1">&gt; integer:: Proc
</span><br>
<span class="quotelev1">&gt; real*8, allocatable:: dbuf(:)
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; call MPI_INIT(ierror)
</span><br>
<span class="quotelev1">&gt; call MPI_COMM_RANK(MPI_COMM_WORLD,Proc,IERROR)
</span><br>
<span class="quotelev1">&gt; call MPI_COMM_SIZE(MPI_COMM_WORLD,Num_Proc,IERROR)
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ndat=1000000
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; !print*,'bcasting to no of tasks',num_proc
</span><br>
<span class="quotelev1">&gt; allocate(dbuf(ndat))
</span><br>
<span class="quotelev1">&gt; do i=1,ndat
</span><br>
<span class="quotelev1">&gt;   dbuf(i)=dble(i)
</span><br>
<span class="quotelev1">&gt; enddo
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; print*, 'About to call broadcast',proc
</span><br>
<span class="quotelev1">&gt; t1=MPI_WTIME()
</span><br>
<span class="quotelev1">&gt; call MPI_BCAST(dbuf,ndat, &amp;
</span><br>
<span class="quotelev1">&gt;      MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,ierror)
</span><br>
<span class="quotelev1">&gt; print*, 'Done with call to broadcast',proc
</span><br>
<span class="quotelev1">&gt; t2=MPI_WTIME()
</span><br>
<span class="quotelev1">&gt; write(*,*)'time for bcast',t2-t1
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; deallocate(dbuf)
</span><br>
<span class="quotelev1">&gt; call MPI_FINALIZE(IERROR)
</span><br>
<span class="quotelev1">&gt; end program em3d
</span><br>
<span class="quotelev1">&gt; [kmuriki_at_n0005 pub]$
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; _______________________________________________
</span><br>
<span class="quotelev1">&gt; users mailing list
</span><br>
<span class="quotelev1">&gt; users_at_[hidden]
</span><br>
<span class="quotelev1">&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<p><pre>
-- 
OoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOo
  David Prendergast
  Lawrence Berkeley National Laboratory
  Molecular Foundry                                phone: (510) 486-4948
  1 Cyclotron Rd., MS 67-3207                      fax:   (510) 486-7424
  Berkeley, CA 94720                        email: dgprendergast_at_[hidden]
  USA             web: <a href="http://nanotheory.lbl.gov/people/prendergast.html">http://nanotheory.lbl.gov/people/prendergast.html</a>
OoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOoOo
</pre>
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="7676.php">Thomas Ropars: "Re: [OMPI users] Error message when using MPI_Type_struct()"</a>
<li><strong>Previous message:</strong> <a href="7674.php">Hana Milani: "[OMPI users] problem with xfmpi_sane"</a>
<li><strong>In reply to:</strong> <a href="7673.php">kmuriki_at_[hidden]: "[OMPI users] slow MPI_BCast for messages size from 24K bytes to 800K bytes."</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="7680.php">Jeff Squyres: "Re: [OMPI users] slow MPI_BCast for messages size from 24K bytes to 800K bytes."</a>
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
