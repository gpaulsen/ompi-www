<?
$subject_val = "[OMPI users] Help tracing casue of readv errors";
include("../../include/msg-header.inc");
?>
<!-- received="Fri Sep 25 11:38:59 2009" -->
<!-- isoreceived="20090925153859" -->
<!-- sent="Fri, 25 Sep 2009 16:38:53 +0100" -->
<!-- isosent="20090925153853" -->
<!-- name="Pacey, Mike" -->
<!-- email="m.pacey_at_[hidden]" -->
<!-- subject="[OMPI users] Help tracing casue of readv errors" -->
<!-- id="F6F95316010549478F4D65CED2554929061BD6A6_at_exchange-be5.lancs.local" -->
<!-- charset="us-ascii" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>Subject:</strong> [OMPI users] Help tracing casue of readv errors<br>
<strong>From:</strong> Pacey, Mike (<em>m.pacey_at_[hidden]</em>)<br>
<strong>Date:</strong> 2009-09-25 11:38:53
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10768.php">Charles Wright: "[OMPI users] [btl_openib_component.c:1373:btl_openib_component_progress] error polling HP CQ with -2 errno says Success"</a>
<li><strong>Previous message:</strong> <a href="10766.php">Thomas Ropars: "[OMPI users] segfault on finalize"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="http://www.open-mpi.org/community/lists/users/2009/11/11280.php">Atle Rudshaug: "Re: [OMPI users] Help tracing casue of readv errors"</a>
<li><strong>Reply:</strong> <a href="http://www.open-mpi.org/community/lists/users/2009/11/11280.php">Atle Rudshaug: "Re: [OMPI users] Help tracing casue of readv errors"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
One my users recently reported random hangs of his OpenMPI application.
<br>
I've run some tests using multiple 2-node 16-core runs of the IMB
<br>
benchmark and can occasionally replicate the problem. Looking through
<br>
the mail archive, a previous occurrence of this error seems to been
<br>
suspect code, but as it's IMB failing here, I suspect the problem lies
<br>
elsewhere. The full set of errors generated by a failed run are:
<br>
<p>[lancs2-015][[37376,1],2][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
<br>
mca_btl_tcp_frag_recv: readv failed: Conne
<br>
ction reset by peer (104)
<br>
[lancs2-015][[37376,1],6][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
<br>
mca_btl_tcp_frag_recv: readv failed: Conne
<br>
ction reset by peer (104)
<br>
[lancs2-015][[37376,1],8][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
<br>
[lancs2-015][[37376,1],14][btl_tcp_frag.c:
<br>
216:mca_btl_tcp_frag_recv] mca_btl_tcp_frag_recv: readv failed:
<br>
Connection reset by peer (104)
<br>
mca_btl_tcp_frag_recv: readv failed: Connection reset by peer (104)
<br>
[lancs2-015][[37376,1],14][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
<br>
mca_btl_tcp_frag_recv: readv failed: Conn
<br>
ection reset by peer (104)
<br>
[lancs2-015][[37376,1],4][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
<br>
mca_btl_tcp_frag_recv: readv failed: Conne
<br>
ction reset by peer (104)
<br>
[lancs2-015][[37376,1],4][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
<br>
mca_btl_tcp_frag_recv: readv failed: Conne
<br>
ction reset by peer (104)
<br>
[lancs2-015][[37376,1],2][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
<br>
mca_btl_tcp_frag_recv: readv failed: Conne
<br>
ction reset by peer (104)
<br>
[lancs2-015][[37376,1],6][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
<br>
mca_btl_tcp_frag_recv: readv failed: Conne
<br>
ction reset by peer (104)
<br>
[lancs2-015][[37376,1],0][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
<br>
mca_btl_tcp_frag_recv: readv failed: Conne
<br>
ction reset by peer (104)
<br>
[lancs2-015][[37376,1],12][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
<br>
mca_btl_tcp_frag_recv: readv failed: Conn
<br>
ection reset by peer (104)
<br>
[lancs2-015][[37376,1],4][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
<br>
mca_btl_tcp_frag_recv: readv failed: Conne
<br>
ction reset by peer (104)
<br>
[lancs2-015][[37376,1],12][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
<br>
mca_btl_tcp_frag_recv: readv failed: Conn
<br>
ection reset by peer (104)
<br>
[lancs2-015][[37376,1],2][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
<br>
mca_btl_tcp_frag_recv: readv failed: Conne
<br>
ction reset by peer (104)
<br>
[lancs2-015][[37376,1],10][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
<br>
mca_btl_tcp_frag_recv: readv failed: Conn
<br>
ection reset by peer (104)
<br>
[lancs2-015][[37376,1],8][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
<br>
mca_btl_tcp_frag_recv: readv failed: Conne
<br>
ction reset by peer (104)
<br>
[lancs2-015][[37376,1],6][btl_tcp_frag.c:216:mca_btl_tcp_frag_recv]
<br>
mca_btl_tcp_frag_recv: readv failed: Conne
<br>
ction reset by peer (104)
<br>
<p>I'm used to OpenMPI terminating cleanly, but that's not happening in
<br>
this case. All the OpenMPI processes on one node terminate, while the
<br>
processes on the other simply spin with 100% CPU utilisation. I've run
<br>
this 2-node test a number of times, and I'm not seeing any pattern (ie,
<br>
I can't pin it down to a single node - a subsequent run using the two
<br>
nodes involved above ran fine).
<br>
<p>Can anyone provide any pointers in tracking down this problem? System
<br>
details as follows:
<br>
<p>-	OpenMPI 1.3.3, compiled with gcc version 4.1.2 20080704 (Red Hat
<br>
4.1.2-44), using only the -prefix and -with-sge options.
<br>
-	OS is Scientific Linux SL release 5.3
<br>
-	CPUs are 2.3GHz Opteron 2356
<br>
<p>Regards,
<br>
Mike.
<br>
<p>-----
<br>
<p>Dr Mike Pacey,                         Email: M.Pacey_at_[hidden]
<br>
High Performance Systems Support,      Phone: 01524 593543
<br>
Information Systems Services,            Fax: 01524 594459
<br>
Lancaster University,
<br>
Lancaster LA1 4YW
<br>
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="10768.php">Charles Wright: "[OMPI users] [btl_openib_component.c:1373:btl_openib_component_progress] error polling HP CQ with -2 errno says Success"</a>
<li><strong>Previous message:</strong> <a href="10766.php">Thomas Ropars: "[OMPI users] segfault on finalize"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="http://www.open-mpi.org/community/lists/users/2009/11/11280.php">Atle Rudshaug: "Re: [OMPI users] Help tracing casue of readv errors"</a>
<li><strong>Reply:</strong> <a href="http://www.open-mpi.org/community/lists/users/2009/11/11280.php">Atle Rudshaug: "Re: [OMPI users] Help tracing casue of readv errors"</a>
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
