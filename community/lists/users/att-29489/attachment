<div dir="ltr">Hi Rizwan,<div><br></div><div>If you need to rewrite your fork system calls, you may want to check out mpi&#39;s spawn functionality. I recently found out about it and it&#39;s really useful if you haven&#39;t heard of it already. I am using it through python&#39;s mpi4py and it seems to be working well.</div><div><br></div><div>Best,</div><div>Jason</div></div><div class="gmail_extra"><br clear="all"><div><div class="gmail_signature" data-smartmail="gmail_signature"><div dir="ltr">Jason Maldonis<div>Research Assistant of Professor Paul Voyles</div><div>Materials Science Grad Student<br></div><div>University of Wisconsin, Madison<br>1509 University Ave, Rm M142<br>Madison, WI 53706<br></div><div><a href="mailto:maldonis@wisc.edu" target="_blank">maldonis@wisc.edu</a></div><div>608-295-5532</div></div></div></div>
<br><div class="gmail_quote">On Mon, Jun 20, 2016 at 8:38 AM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles.gouaillardet@gmail.com" target="_blank">gilles.gouaillardet@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">There is no guarantee that will work on a multiple mode job.<div>tcp should be fine, infiniband might not work.</div><div><br></div><div>the best way to be on the safe side is you rewrite your MPI app so it does not invoke the fork system call. this is generally invoked directly, or via the &quot;system&quot; subroutine.</div><div class="HOEnZb"><div class="h5"><div><br>Cheers,</div><div><br></div><div>Gilles</div><div><br>On Monday, June 20, 2016, Ahmed Rizwan &lt;<a href="mailto:rizwan.ahmed@aalto.fi" target="_blank">rizwan.ahmed@aalto.fi</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">




<div>
<div style="direction:ltr;font-family:Tahoma;color:#000000;font-size:10pt">Hi Gilles,<br>
<br>
Thanks for the support. :)<br>
<br>
This is a test which I am running on a single node, but I am intending to run calculations on multiple nodes. You mean it wouldn&#39;t work on multiple nodes? If I run on multiple nodes, how can I avoid these errors then? I would just test it for multiple nodes.
<br>
<br>
Regards,<br>
Rizwan<br>
<div style="font-family:Times New Roman;color:#000000;font-size:16px">
<hr>
<div style="direction:ltr"><font face="Tahoma" size="2" color="#000000"><b>From:</b> users [<a>users-bounces@open-mpi.org</a>] on behalf of Gilles Gouaillardet [<a>gilles.gouaillardet@gmail.com</a>]<br>
<b>Sent:</b> Monday, June 20, 2016 3:10 PM<br>
<b>To:</b> Open MPI Users<br>
<b>Subject:</b> [OMPI users] memory cg &#39;(null)&#39;<br>
</font><br>
</div>
<div></div>
<div>There are two points here
<div>1. slurm(stepd) is unable to put the processes in the (null) cgroup.</div>
<div>   at first glance, this looks more of a slurm jus configuration</div>
<div>2. the MPI process forking. though this has a much better support than in the past, that might not always work, especially with fast interconnects. since you are running on a single node, you should be fine. simply</div>
<div>export OMPI_MCA_mpi_warn_on_fork=0</div>
<div>before invoking srun, in order to silence this message.</div>
<div><br>
</div>
<div>Cheers,</div>
<div><br>
</div>
<div>Gilles</div>
<div><br>
On Monday, June 20, 2016, Ahmed Rizwan &lt;<a href="http://UrlBlockedError.aspx" target="_blank">rizwan.ahmed@aalto.fi</a>&gt; wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div>
<div style="direction:ltr;font-family:Tahoma;color:#000000;font-size:10pt">Dear MPI users,<br>
<br>
I am getting the errors below while submitting/executing following script, <br>
<br>
#!/bin/sh<br>
#SBATCH -p short<br>
#SBATCH -J layers<br>
#SBATCH -n 12<br>
#SBATCH -N 1 <br>
#SBATCH -t 01:30:00<br>
#SBATCH --mem-per-cpu=2500<br>
#SBATCH --exclusive<br>
#SBATCH --mail-type=END<br>
#SBATCH --mail-user=<a>rizwan.ahmed@aalto.fi</a><br>
#SBATCH -o output_%j.out<br>
#SBATCH -e errors_%j.err<br>
<br>
srun --mpi=pmi2 gpaw-python layers.py<br>
<br>
--------------------------------------------------------------------------<br>
slurmstepd: error: task/cgroup: unable to add task[pid=126453] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=80379] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124258] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124259] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124261] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124266] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124264] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124262] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124260] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124265] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124263] to memory cg &#39;(null)&#39;<br>
--------------------------------------------------------------------------<br>
An MPI process has executed an operation involving a call to the<br>
&quot;fork()&quot; system call to create a child process.  Open MPI is currently<br>
operating in a condition that could result in memory corruption or<br>
other system errors; your MPI job may hang, crash, or produce silent<br>
data corruption.  The use of fork() (or system() or other calls that<br>
create child processes) is strongly discouraged.  <br>
<br>
The process that invoked fork was:<br>
<br>
  Local host:          pe38 (PID 80379)<br>
  MPI_COMM_WORLD rank: 1<br>
<br>
If you are *absolutely sure* that your application will successfully<br>
and correctly survive a call to fork(), you may disable this warning<br>
by setting the mpi_warn_on_fork MCA parameter to 0.<br>
--------------------------------------------------------------------------<br>
<br>
Is this error fatal or should it be ignored? Thanks<br>
Regards,<br>
Rizwan<br>
</div>
</div>
</blockquote>
</div>
</div>
</div>
</div>
</div>

</blockquote></div>
</div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/06/29488.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/06/29488.php</a><br></blockquote></div><br></div>

