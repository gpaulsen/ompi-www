<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <meta content="text/html; charset=ISO-8859-1"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#ffffff" text="#000000">
    On 11/23/2011 2:02 PM, Paul Kapinos wrote:
    <blockquote cite="mid:4ECD4345.2030806@rz.rwth-aachen.de"
      type="cite">Hello Ralph, hello all,
      <br>
      <br>
      Two news, as usual a good and a bad one.
      <br>
      <br>
      The good: we believe to find out *why* it hangs
      <br>
      <br>
      The bad: it seem for me, this is a bug or at least undocumented
      feature of Open MPI /1.5.x.
      <br>
      <br>
      In detail:
      <br>
      As said, we see mystery hang-ups if starting on some nodes using
      some permutation of hostnames. Usually removing "some bad" nodes
      helps, sometimes a permutation of node names in the hostfile is
      enough(!). The behaviour is reproducible.
      <br>
      <br>
      The machines have at least 2 networks:
      <br>
      <br>
      *eth0* is used for installation, monitoring, ... - this ethernet
      is very slim
      <br>
      <br>
      *ib0* - is the "IP over IB" interface and is used for everything:
      the file systems, ssh and so on. The hostnames are bound to the
      ib0 network; our idea was not to use eth0 for MPI at all.
      <br>
      <br>
      all machines are available from any over ib0 (are in one network).
      <br>
      <br>
      But on eth0 there are at least two different networks; especially
      the computer linuxbsc025 is in different network than the others
      and is not reachable from other nodes over eth0! (but reachable
      over ib0. The name used in the hostfile is resolved to the IP of
      ib0 ).
      <br>
      <br>
      So I believe that Open MPI /1.5.x tries to communicate over eth0
      and cannot do it, and hangs. The /1.4.3 does not hang, so this
      issue is 1.5.x-specific (seen in 1.5.3 and 1.5.4). A bug?
      <br>
      <br>
      I also tried to disable the eth0 completely:
      <br>
      <br>
      $ mpiexec -mca btl_tcp_if_exclude eth0,lo&nbsp; -mca btl_tcp_if_include
      ib0 ...
      <br>
      <br>
    </blockquote>
    I believe if you give "-mca btl_tcp_if_include ib0" you do not need
    to specify the exclude parameter.<br>
    <blockquote cite="mid:4ECD4345.2030806@rz.rwth-aachen.de"
      type="cite">...but this does not help. All right, the above
      command should disable the usage of eth0 for MPI communication
      itself, but it hangs just before the MPI is started, isn't it?
      (because one process lacks, the MPI_INIT cannot be passed)
      <br>
      <br>
    </blockquote>
    By "just before the MPI is started" do you mean while orte is
    launching the processes.<br>
    I wonder if you need to specify "-mca oob_tcp_if_include ib0" also
    but I think that may depend on which oob you are using.<br>
    <blockquote cite="mid:4ECD4345.2030806@rz.rwth-aachen.de"
      type="cite">Now a question: is there a way to forbid the mpiexec
      to use some interfaces at all?
      <br>
      <br>
      Best wishes,
      <br>
      <br>
      Paul Kapinos
      <br>
      <br>
      P.S. Of course we know about the good idea to bring all nodes into
      the same net on eth0, but at this point it is impossible due of
      technical reason[s]...
      <br>
      <br>
      P.S.2 I'm not sure that the issue is really rooted in the above
      mentioned misconfiguration of eth0, but I have no better idea at
      this point...
      <br>
      <br>
      <br>
      <blockquote type="cite">
        <blockquote type="cite">The map seem to be correctly build, also
          the output if the daemons seem to be the same (see
          helloworld.txt)
          <br>
        </blockquote>
        <br>
        Unfortunately, it appears that OMPI was not built with
        --enable-debug as there is no debug info in the output. Without
        a debug installation of OMPI, the ability to determine the
        problem is pretty limited.
        <br>
      </blockquote>
      <br>
      well, this will be the next option we will activate. We also have
      another issue here, on (not) using uDAPL..
      <br>
      <br>
      <br>
      <blockquote type="cite">
        <br>
        <br>
        <blockquote type="cite">
          <blockquote type="cite">You should also try putting that long
            list of nodes in a hostfile - see if that makes a
            difference.
            <br>
            It will process the nodes thru a different code path, so if
            there is some problem in --host,
            <br>
            this will tell us.
            <br>
          </blockquote>
          No, with the host file instead of host list on command line
          the behaviour is the same.
          <br>
          <br>
          But, I just found out that the 1.4.3 does *not* hang on this
          constellation. The next thing I will try will be the
          installation of 1.5.4 :o)
          <br>
          <br>
          Best,
          <br>
          <br>
          Paul
          <br>
          <br>
          P.S. started:
          <br>
          <br>
          $ /opt/MPI/openmpi-1.5.3/linux/intel/bin/mpiexec --hostfile
          hostfile-mini -mca odls_base_verbose 5
          --leave-session-attached --display-map&nbsp; helloworld 2&gt;&amp;1
          | tee helloworld.txt
          <br>
          <br>
          <br>
          <br>
          <blockquote type="cite">On Nov 21, 2011, at 9:33 AM, Paul
            Kapinos wrote:
            <br>
            <blockquote type="cite">Hello Open MPI volks,
              <br>
              <br>
              We use OpenMPI 1.5.3 on our pretty new 1800+ nodes
              InfiniBand cluster, and we have some strange hangups if
              starting OpenMPI processes.
              <br>
              <br>
              The nodes are named linuxbsc001,linuxbsc002,... (with some
              lacuna due of&nbsp; offline nodes). Each node is accessible
              from each other over SSH (without password), also MPI
              programs between any two nodes are checked to run.
              <br>
              <br>
              <br>
              So long, I tried to start some bigger number of processes,
              one process per node:
              <br>
              $ mpiexec -np NN&nbsp; --host linuxbsc001,linuxbsc002,...
              MPI_FastTest.exe
              <br>
              <br>
              Now the problem: there are some constellations of names in
              the host list on which mpiexec reproducible hangs forever;
              and more surprising: other *permutation* of the *same*
              node names may run without any errors!
              <br>
              <br>
              Example: the command in laueft.txt runs OK, the command in
              haengt.txt hangs. Note: the only difference is that the
              node linuxbsc025 is put on the end of the host list.
              Amazed, too?
              <br>
              <br>
              Looking on the particular nodes during the above mpiexec
              hangs, we found the orted daemons started on *each* node
              and the binary on all but one node (orted.txt,
              MPI_FastTest.txt).
              <br>
              Again amazing that the node with no user process started
              (leading to hangup in MPI_Init of all processes and thus
              to hangup, I believe) was always the same, linuxbsc005,
              which is NOT the permuted item linuxbsc025...
              <br>
              <br>
              This behaviour is reproducible. The hang-on only occure if
              the started application is a MPI application ("hostname"
              does not hang).
              <br>
              <br>
              <br>
              Any Idea what is gonna on?
              <br>
              <br>
              <br>
              Best,
              <br>
              <br>
              Paul Kapinos
              <br>
              <br>
              <br>
              P.S: no alias names used, all names are real ones
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              --&nbsp;<br>
              Dipl.-Inform. Paul Kapinos&nbsp;&nbsp; -&nbsp;&nbsp; High Performance
              Computing,
              <br>
              RWTH Aachen University, Center for Computing and
              Communication
              <br>
              Seffenter Weg 23,&nbsp; D 52074&nbsp; Aachen (Germany)
              <br>
              Tel: +49 241/80-24915
              <br>
              linuxbsc001: STDOUT: 24323 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc002: STDOUT:&nbsp; 2142 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc003: STDOUT: 69266 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc004: STDOUT: 58899 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc006: STDOUT: 68255 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc007: STDOUT: 62026 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc008: STDOUT: 54221 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc009: STDOUT: 55482 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc010: STDOUT: 59380 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc011: STDOUT: 58312 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc014: STDOUT: 56013 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc016: STDOUT: 58563 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc017: STDOUT: 54693 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc018: STDOUT: 54187 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc020: STDOUT: 55811 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc021: STDOUT: 54982 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc022: STDOUT: 50032 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc023: STDOUT: 54044 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc024: STDOUT: 51247 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc025: STDOUT: 18575 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc027: STDOUT: 48969 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc028: STDOUT: 52397 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc029: STDOUT: 52780 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc030: STDOUT: 47537 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc031: STDOUT: 54609 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              linuxbsc032: STDOUT: 52833 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SLl&nbsp;&nbsp;&nbsp; 0:00
              MPI_FastTest.exe
              <br>
              $ timex /opt/MPI/openmpi-1.5.3/linux/intel/bin/mpiexec -np
              27&nbsp; --host
              linuxbsc001,linuxbsc002,linuxbsc003,linuxbsc004,linuxbsc005,linuxbsc006,linuxbsc007,linuxbsc008,linuxbsc009,linuxbsc010,linuxbsc011,linuxbsc014,linuxbsc016,linuxbsc017,linuxbsc018,linuxbsc020,linuxbsc021,linuxbsc022,linuxbsc023,linuxbsc024,linuxbsc025,linuxbsc027,linuxbsc028,linuxbsc029,linuxbsc030,linuxbsc031,linuxbsc032
              MPI_FastTest.exe
              <br>
              $ timex /opt/MPI/openmpi-1.5.3/linux/intel/bin/mpiexec -np
              27&nbsp; --host
              linuxbsc001,linuxbsc002,linuxbsc003,linuxbsc004,linuxbsc005,linuxbsc006,linuxbsc007,linuxbsc008,linuxbsc009,linuxbsc010,linuxbsc011,linuxbsc014,linuxbsc016,linuxbsc017,linuxbsc018,linuxbsc020,linuxbsc021,linuxbsc022,linuxbsc023,linuxbsc024,linuxbsc027,linuxbsc028,linuxbsc029,linuxbsc030,linuxbsc031,linuxbsc032,linuxbsc025
              MPI_FastTest.exe
              <br>
              linuxbsc001: STDOUT: 24322 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 1 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc002: STDOUT:&nbsp; 2141 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 2 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc003: STDOUT: 69265 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 3 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc004: STDOUT: 58898 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 4 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc005: STDOUT: 65642 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 5 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc006: STDOUT: 68254 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 6 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc007: STDOUT: 62025 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 7 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc008: STDOUT: 54220 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 8 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc009: STDOUT: 55481 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 9 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc010: STDOUT: 59379 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 10 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc011: STDOUT: 58311 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 11 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc014: STDOUT: 56012 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 12 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc016: STDOUT: 58562 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 13 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc017: STDOUT: 54692 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 14 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc018: STDOUT: 54186 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 15 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc020: STDOUT: 55810 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 16 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc021: STDOUT: 54981 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 17 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc022: STDOUT: 50031 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 18 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc023: STDOUT: 54043 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 19 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc024: STDOUT: 51246 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 20 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc025: STDOUT: 18574 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 21 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc027: STDOUT: 48968 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 22 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc028: STDOUT: 52396 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 23 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc029: STDOUT: 52779 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 24 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc030: STDOUT: 47536 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 25 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc031: STDOUT: 54608 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 26 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              linuxbsc032: STDOUT: 52832 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ss&nbsp;&nbsp;&nbsp;&nbsp; 0:00
              /opt/MPI/openmpi-1.5.3/linux/intel/bin/orted --daemonize
              -mca ess env -mca orte_ess_jobid 751435776 -mca
              orte_ess_vpid 27 -mca orte_ess_num_procs 28 --hnp-uri
              751435776.0;tcp://134.61.194.2:33210 -mca plm rsh
              <br>
              _______________________________________________
              <br>
              users mailing list
              <br>
              <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
              <br>
              <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
              <br>
            </blockquote>
            _______________________________________________
            <br>
            users mailing list
            <br>
            <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
            <br>
            <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
            <br>
          </blockquote>
          <br>
          --&nbsp;<br>
          Dipl.-Inform. Paul Kapinos&nbsp;&nbsp; -&nbsp;&nbsp; High Performance Computing,
          <br>
          RWTH Aachen University, Center for Computing and Communication
          <br>
          Seffenter Weg 23,&nbsp; D 52074&nbsp; Aachen (Germany)
          <br>
          Tel: +49 241/80-24915
          <br>
          linuxbsc005 slots=1
          <br>
          linuxbsc006 slots=1
          <br>
          linuxbsc007 slots=1
          <br>
          linuxbsc008 slots=1
          <br>
          linuxbsc009 slots=1
          <br>
          linuxbsc010 slots=1
          <br>
          linuxbsc011 slots=1
          <br>
          linuxbsc014 slots=1
          <br>
          linuxbsc016 slots=1
          <br>
          linuxbsc017 slots=1
          <br>
          linuxbsc018 slots=1
          <br>
          linuxbsc020 slots=1
          <br>
          linuxbsc021 slots=1
          <br>
          linuxbsc022 slots=1
          <br>
          linuxbsc023 slots=1
          <br>
          linuxbsc024 slots=1
          <br>
          linuxbsc025 slots=1[linuxc2.rz.RWTH-Aachen.DE:22229]
          mca:base:select:( odls) Querying component [default]
          <br>
          [linuxc2.rz.RWTH-Aachen.DE:22229] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxc2.rz.RWTH-Aachen.DE:22229] mca:base:select:( odls)
          Selected component [default]
          <br>
          <br>
          ========================&nbsp;&nbsp; JOB MAP&nbsp;&nbsp; ========================
          <br>
          <br>
          Data for node: linuxbsc005&nbsp;&nbsp;&nbsp; Num procs: 1
          <br>
          &nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [87,1] Process rank: 0
          <br>
          <br>
          Data for node: linuxbsc006&nbsp;&nbsp;&nbsp; Num procs: 1
          <br>
          &nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [87,1] Process rank: 1
          <br>
          <br>
          Data for node: linuxbsc007&nbsp;&nbsp;&nbsp; Num procs: 1
          <br>
          &nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [87,1] Process rank: 2
          <br>
          <br>
          Data for node: linuxbsc008&nbsp;&nbsp;&nbsp; Num procs: 1
          <br>
          &nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [87,1] Process rank: 3
          <br>
          <br>
          Data for node: linuxbsc009&nbsp;&nbsp;&nbsp; Num procs: 1
          <br>
          &nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [87,1] Process rank: 4
          <br>
          <br>
          Data for node: linuxbsc010&nbsp;&nbsp;&nbsp; Num procs: 1
          <br>
          &nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [87,1] Process rank: 5
          <br>
          <br>
          Data for node: linuxbsc011&nbsp;&nbsp;&nbsp; Num procs: 1
          <br>
          &nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [87,1] Process rank: 6
          <br>
          <br>
          Data for node: linuxbsc014&nbsp;&nbsp;&nbsp; Num procs: 1
          <br>
          &nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [87,1] Process rank: 7
          <br>
          <br>
          Data for node: linuxbsc016&nbsp;&nbsp;&nbsp; Num procs: 1
          <br>
          &nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [87,1] Process rank: 8
          <br>
          <br>
          Data for node: linuxbsc017&nbsp;&nbsp;&nbsp; Num procs: 1
          <br>
          &nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [87,1] Process rank: 9
          <br>
          <br>
          Data for node: linuxbsc018&nbsp;&nbsp;&nbsp; Num procs: 1
          <br>
          &nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [87,1] Process rank: 10
          <br>
          <br>
          Data for node: linuxbsc020&nbsp;&nbsp;&nbsp; Num procs: 1
          <br>
          &nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [87,1] Process rank: 11
          <br>
          <br>
          Data for node: linuxbsc021&nbsp;&nbsp;&nbsp; Num procs: 1
          <br>
          &nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [87,1] Process rank: 12
          <br>
          <br>
          Data for node: linuxbsc022&nbsp;&nbsp;&nbsp; Num procs: 1
          <br>
          &nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [87,1] Process rank: 13
          <br>
          <br>
          Data for node: linuxbsc023&nbsp;&nbsp;&nbsp; Num procs: 1
          <br>
          &nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [87,1] Process rank: 14
          <br>
          <br>
          Data for node: linuxbsc024&nbsp;&nbsp;&nbsp; Num procs: 1
          <br>
          &nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [87,1] Process rank: 15
          <br>
          <br>
          Data for node: linuxbsc025&nbsp;&nbsp;&nbsp; Num procs: 1
          <br>
          &nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [87,1] Process rank: 16
          <br>
          <br>
          =============================================================
          <br>
          [linuxbsc007.rz.RWTH-Aachen.DE:07574] mca:base:select:( odls)
          Querying component [default]
          <br>
          [linuxbsc007.rz.RWTH-Aachen.DE:07574] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxbsc007.rz.RWTH-Aachen.DE:07574] mca:base:select:( odls)
          Selected component [default]
          <br>
          [linuxbsc016.rz.RWTH-Aachen.DE:03146] mca:base:select:( odls)
          Querying component [default]
          <br>
          [linuxbsc016.rz.RWTH-Aachen.DE:03146] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxbsc016.rz.RWTH-Aachen.DE:03146] mca:base:select:( odls)
          Selected component [default]
          <br>
          [linuxbsc005.rz.RWTH-Aachen.DE:22051] mca:base:select:( odls)
          Querying component [default]
          <br>
          [linuxbsc005.rz.RWTH-Aachen.DE:22051] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxbsc005.rz.RWTH-Aachen.DE:22051] mca:base:select:( odls)
          Selected component [default]
          <br>
          [linuxbsc011.rz.RWTH-Aachen.DE:07131] mca:base:select:( odls)
          Querying component [default]
          <br>
          [linuxbsc011.rz.RWTH-Aachen.DE:07131] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxbsc011.rz.RWTH-Aachen.DE:07131] mca:base:select:( odls)
          Selected component [default]
          <br>
          [linuxbsc025.rz.RWTH-Aachen.DE:43153] mca:base:select:( odls)
          Querying component [default]
          <br>
          [linuxbsc025.rz.RWTH-Aachen.DE:43153] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxbsc025.rz.RWTH-Aachen.DE:43153] mca:base:select:( odls)
          Selected component [default]
          <br>
          [linuxbsc017.rz.RWTH-Aachen.DE:05044] mca:base:select:( odls)
          Querying component [default]
          <br>
          [linuxbsc017.rz.RWTH-Aachen.DE:05044] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxbsc017.rz.RWTH-Aachen.DE:05044] mca:base:select:( odls)
          Selected component [default]
          <br>
          [linuxbsc018.rz.RWTH-Aachen.DE:01840] mca:base:select:( odls)
          Querying component [default]
          <br>
          [linuxbsc018.rz.RWTH-Aachen.DE:01840] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxbsc018.rz.RWTH-Aachen.DE:01840] mca:base:select:( odls)
          Selected component [default]
          <br>
          [linuxbsc024.rz.RWTH-Aachen.DE:79549] mca:base:select:( odls)
          Querying component [default]
          <br>
          [linuxbsc024.rz.RWTH-Aachen.DE:79549] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxbsc024.rz.RWTH-Aachen.DE:79549] mca:base:select:( odls)
          Selected component [default]
          <br>
          [linuxbsc022.rz.RWTH-Aachen.DE:73501] mca:base:select:( odls)
          Querying component [default]
          <br>
          [linuxbsc022.rz.RWTH-Aachen.DE:73501] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxbsc022.rz.RWTH-Aachen.DE:73501] mca:base:select:( odls)
          Selected component [default]
          <br>
          [linuxbsc023.rz.RWTH-Aachen.DE:03364] mca:base:select:( odls)
          Querying component [default]
          <br>
          [linuxbsc023.rz.RWTH-Aachen.DE:03364] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxbsc023.rz.RWTH-Aachen.DE:03364] mca:base:select:( odls)
          Selected component [default]
          <br>
          [linuxbsc006.rz.RWTH-Aachen.DE:16811] mca:base:select:( odls)
          Querying component [default]
          <br>
          [linuxbsc006.rz.RWTH-Aachen.DE:16811] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxbsc006.rz.RWTH-Aachen.DE:16811] mca:base:select:( odls)
          Selected component [default]
          <br>
          [linuxbsc014.rz.RWTH-Aachen.DE:10206] mca:base:select:( odls)
          Querying component [default]
          <br>
          [linuxbsc014.rz.RWTH-Aachen.DE:10206] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxbsc014.rz.RWTH-Aachen.DE:10206] mca:base:select:( odls)
          Selected component [default]
          <br>
          [linuxbsc008.rz.RWTH-Aachen.DE:00858] mca:base:select:( odls)
          Querying component [default]
          <br>
          [linuxbsc008.rz.RWTH-Aachen.DE:00858] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxbsc008.rz.RWTH-Aachen.DE:00858] mca:base:select:( odls)
          Selected component [default]
          <br>
          [linuxbsc010.rz.RWTH-Aachen.DE:09727] mca:base:select:( odls)
          Querying component [default]
          <br>
          [linuxbsc010.rz.RWTH-Aachen.DE:09727] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxbsc010.rz.RWTH-Aachen.DE:09727] mca:base:select:( odls)
          Selected component [default]
          <br>
          [linuxbsc020.rz.RWTH-Aachen.DE:06680] mca:base:select:( odls)
          Querying component [default]
          <br>
          [linuxbsc020.rz.RWTH-Aachen.DE:06680] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxbsc020.rz.RWTH-Aachen.DE:06680] mca:base:select:( odls)
          Selected component [default]
          <br>
          [linuxbsc009.rz.RWTH-Aachen.DE:05145] mca:base:select:( odls)
          Querying component [default]
          <br>
          [linuxbsc009.rz.RWTH-Aachen.DE:05145] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxbsc009.rz.RWTH-Aachen.DE:05145] mca:base:select:( odls)
          Selected component [default]
          <br>
          [linuxbsc021.rz.RWTH-Aachen.DE:01405] mca:base:select:( odls)
          Querying component [default]
          <br>
          [linuxbsc021.rz.RWTH-Aachen.DE:01405] mca:base:select:( odls)
          Query of component [default] set priority to 1
          <br>
          [linuxbsc021.rz.RWTH-Aachen.DE:01405] mca:base:select:( odls)
          Selected component [default]
          <br>
          _______________________________________________
          <br>
          users mailing list
          <br>
          <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
          <br>
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
          <br>
        </blockquote>
        <br>
        <br>
        _______________________________________________
        <br>
        users mailing list
        <br>
        <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
        <br>
        <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
        <br>
        <br>
      </blockquote>
      <br>
      <br>
      <pre wrap="">
<fieldset class="mimeAttachmentHeader"></fieldset>
_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></pre>
    </blockquote>
    <br>
    <div class="moz-signature">-- <br>
      <meta content="text/html; charset=ISO-8859-1"
        http-equiv="Content-Type">
      <div class="moz-signature">
        <meta http-equiv="content-type" content="text/html;
          charset=ISO-8859-1">
        <title></title>
        <img moz-do-not-send="false"
          src="cid:part1.02020005.04090802@oracle.com" alt="Oracle"><br>
        <div class="moz-signature">
          <div class="moz-signature">
            <div class="moz-signature">
              <div class="moz-signature">Terry D. Dontje | Principal
                Software Engineer<br>
                <div class="moz-signature"><font size="2"
                    color="#666666" face="Verdana">Developer
                    Tools
                    Engineering | +1.781.442.2631<br>
                  </font>
                  <font size="2" color="#ff0000" face="Verdana">Oracle
                  </font><font size="2" color="#666666" face="Verdana"><b>
                      - Performance
                      Technologies</b></font><br>
                  <font size="2" color="#666666" face="Verdana">
                    95 Network Drive, Burlington, MA 01803<br>
                    Email <a moz-do-not-send="true"
                      href="mailto:terry.dontje@oracle.com">terry.dontje@oracle.com</a><br>
                  </font><br>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <br>
      <br>
    </div>
  </body>
</html>

