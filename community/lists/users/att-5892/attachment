<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">
Brad just curious.<div><br></div><div>Did you tweak any other values for starting and running a job on such a large system?  You say unmodified,  but OpenMPI lets you tweak many values at runtime.  </div><div><br></div><div>I would be curious to expand what I know from what you discovered.<br><div> <span class="Apple-style-span" style="border-collapse: separate; color: rgb(0, 0, 0); font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0; "><div><br class="Apple-interchange-newline">Brock Palen</div><div>www.umich.edu/~brockp</div><div>Center for Advanced Computing</div><div><a href="mailto:brockp@umich.edu">brockp@umich.edu</a></div><div>(734)936-1985</div><br class="Apple-interchange-newline"></span><br class="Apple-interchange-newline"> </div><br><div><html>On Jun 16, 2008, at 10:12 PM, Brad Benton wrote:</html><br class="Apple-interchange-newline"><blockquote type="cite">Greetings Open MPI users; we thought you'd be interested in the  <br>following announcement...<br><br>A new supercomputer, powered by Open MPI, has broken the petaflop  <br>barrier to become the world's fastest supercomputer.  The  <br> "Roadrunner" system was jointly developed by Los Alamos National  <br>Laboratories and IBM.  Roadrunner's design uses a cluster of AMD  <br>dual-core processors coupled with computational accelerators based  <br> on the IBM Cell Broadband Engine.  The cluster consists of 3,060  <br>nodes, each of which has 2 dual-core AMD processors and associated  <br>Cell accelerators.  The AMD nodes are connected with 4x DDR  <br>InfiniBand links.<br> <br>Open MPI was used as the communications library for the 12,240  <br>processes comprising the Linpack run which broke the Petaflop  <br>barrier at 1.026 Petaflop/s.  The version of Open MPI used in the  <br>run-for-record was a pre-release version of the upcoming 1.3  <br> release.  Enhancements in this release include modifications for  <br>efficient, scalable process launch.  As such, Open MPI was run  <br>unmodified from a snapshot of the pre-1.3 source base (meaning:  <br>there are no Roadrunner-specific enhancements that are unportable to  <br> other environments -- all Open MPI users benefit from the  <br>scalability and performance improvements contributed by the  <br>Roadrunner project).<br><br>--Brad Benton<br>Open MPI/Roadrunner Team<br><div style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; ">_______________________________________________</div><div style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; ">users mailing list</div><div style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; "><a href="mailto:users@open-mpi.org">users@open-mpi.org</a></div><div style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; "><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div> </blockquote></div><br></div></body></html>
