Many many thanks.<br><br clear="all">Best,<br>Kishore Kumar Pusukuri<br><a href="http://www.cs.ucr.edu/~kishore">http://www.cs.ucr.edu/~kishore</a><br><br>
<br><br><div class="gmail_quote">On 28 April 2010 18:00, Martin Siegert <span dir="ltr">&lt;<a href="mailto:siegert@sfu.ca">siegert@sfu.ca</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">
Yes, I am quite sure that you need at least 16GB to run SPEC MPIM2007.<br>
See the FAQ at <a href="http://www.spec.org/mpi2007/docs/faq.html#MemoryMedium" target="_blank">http://www.spec.org/mpi2007/docs/faq.html#MemoryMedium</a><br>
Furthermore, the benchmark is designed to run on at least 16p.<br>
<div class="im"><br>
Cheers,<br>
Martin<br>
<br>
--<br>
Martin Siegert<br>
Head, Research Computing<br>
WestGrid Site Lead<br>
IT Services                                phone: 778 782-4691<br>
Simon Fraser University                    fax:   778 782-4242<br>
Burnaby, British Columbia                  email: <a href="mailto:siegert@sfu.ca">siegert@sfu.ca</a><br>
Canada  V5A 1S6<br>
<br>
</div><div class="im">On Wed, Apr 28, 2010 at 05:47:52PM -0700, kishore kumar wrote:<br>
&gt;<br>
&gt;    Oh..... Thank you for the information.<br>
&gt;    The machine has 6GM of RAM and I am creating 4 processes (for 4 cores).<br>
&gt;    Are you sure that it is because of lack of resources or some problem<br>
&gt;    with the network settings (I want to run the programs only on my<br>
&gt;    server)?<br>
&gt;    Is there anyway to do this (I need to run only 4 processes for my<br>
&gt;    project)?<br>
&gt;    Thank you.<br>
&gt;    Best,<br>
&gt;    Kishore Kumar Pusukuri<br>
</div><div class="im">&gt;    [1]<a href="http://www.cs.ucr.edu/%7Ekishore" target="_blank">http://www.cs.ucr.edu/~kishore</a><br>
&gt;<br>
</div><div class="im">&gt;    On 28 April 2010 17:18, Martin Siegert &lt;[2]<a href="mailto:siegert@sfu.ca">siegert@sfu.ca</a>&gt; wrote:<br>
&gt;<br>
&gt;      How much memory is available on that quad core machine?<br>
&gt;      The minimum requirements for MPIM2007 are:<br>
&gt;      16GB of memory for the whole system or 1GB of memory per rank,<br>
&gt;      whichever<br>
&gt;      is larger.<br>
&gt;      For MPIL2007 you need to use at least 64 processes and a minimum of<br>
&gt;      128GB<br>
&gt;      (2GB/process) is required.<br>
&gt;      Cheers,<br>
&gt;      Martin<br>
&gt;      --<br>
&gt;      Martin Siegert<br>
&gt;      Head, Research Computing<br>
&gt;      WestGrid Site Lead<br>
&gt;      IT Services                                phone: 778 782-4691<br>
&gt;      Simon Fraser University                    fax:   778 782-4242<br>
</div>&gt;      Burnaby, British Columbia                  email: [3]<a href="mailto:siegert@sfu.ca">siegert@sfu.ca</a><br>
<div class="im">&gt;      Canada  V5A 1S6<br>
&gt;<br>
&gt;    On Wed, Apr 28, 2010 at 05:32:12AM -0500, Jeff Squyres (jsquyres)<br>
&gt;    wrote:<br>
&gt;    &gt;<br>
&gt;    &gt;    I don&#39;t know much about specmpi, but it seems like it is choosing<br>
&gt;    to<br>
&gt;    &gt;    abort. Maybe the &quot;no room for lattice&quot; has some meaning...?<br>
&gt;    &gt;    -jms<br>
&gt;    &gt;    Sent from my PDA. No type good.<br>
&gt;    &gt;<br>
&gt;    _______________________________________________________________________<br>
&gt;    &gt;<br>
</div>&gt;    &gt;    From: [4]<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a><br>
&gt;    &lt;[5]<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>&gt;<br>
&gt;    &gt;    To: [6]<a href="mailto:users@open-mpi.org">users@open-mpi.org</a> &lt;[7]<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
<div><div></div><div class="h5">&gt;    &gt;    Sent: Wed Apr 28 01:47:01 2010<br>
&gt;    &gt;    Subject: [OMPI users] MPI_ABORT was invoked on rank 0 in<br>
&gt;    &gt;    communicatorMPI_COMM_WORLD with errorcode 0.<br>
&gt;    &gt;<br>
&gt;    &gt;    Hi,<br>
&gt;    &gt;    I am trying to run SPEC MPI 2007 workload on a quad-core machine.<br>
&gt;    &gt;    However getting this error message. I also tried to use hostfile<br>
&gt;    option<br>
&gt;    &gt;    by specifying localhost slots=4, but still getting the following<br>
&gt;    error.<br>
&gt;    &gt;    Please help me.<br>
&gt;<br>
&gt;      &gt;    $mpirunÂ  --mca btl tcp,sm,self -np 4 su3imp_base.solaris<br>
&gt;<br>
&gt;    &gt;    SU3 with improved KS action<br>
&gt;    &gt;    Microcanonical simulation with refreshing<br>
&gt;    &gt;    MIMD version 6<br>
&gt;    &gt;    Machine =<br>
&gt;    &gt;    R algorithm<br>
&gt;<br>
&gt;      &gt;    type 0 for no promptsÂ  or 1 for prompts<br>
&gt;<br>
&gt;    &gt;    nflavors 2<br>
&gt;    &gt;    nx 30<br>
&gt;    &gt;    ny 30<br>
&gt;    &gt;    nz 56<br>
&gt;    &gt;    nt 84<br>
&gt;    &gt;    iseed 1234<br>
&gt;    &gt;    LAYOUT = Hypercubes, options = EVENFIRST,<br>
&gt;    &gt;    NODE 0: no room for lattice<br>
&gt;    &gt;    termination: Tue Apr 27 23:41:44 2010<br>
&gt;    &gt;    Termination: node 0, status = 1<br>
&gt;    &gt;<br>
&gt;    -----------------------------------------------------------------------<br>
&gt;    &gt;    ---<br>
&gt;    &gt;    MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD<br>
&gt;    &gt;    with errorcode 0.<br>
&gt;    &gt;    NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI<br>
&gt;    processes.<br>
&gt;    &gt;    You may or may not see output from other processes, depending on<br>
&gt;    &gt;    exactly when Open MPI kills them.<br>
&gt;    &gt;<br>
&gt;    -----------------------------------------------------------------------<br>
&gt;    &gt;    ---<br>
&gt;    &gt;<br>
&gt;    -----------------------------------------------------------------------<br>
&gt;    &gt;    ---<br>
&gt;    &gt;    mpirun has exited due to process rank 0 with PID 17239 on<br>
&gt;    &gt;    node cache-aware exiting without calling &quot;finalize&quot;. This may<br>
&gt;    &gt;    have caused other processes in the application to be<br>
&gt;    &gt;    terminated by signals sent by mpirun (as reported here).<br>
&gt;    &gt;    Best,<br>
&gt;    &gt;    Kishore Kumar Pusukuri<br>
&gt;<br>
</div></div>&gt;      &gt;    [1][8]<a href="http://www.cs.ucr.edu/%7Ekishore" target="_blank">http://www.cs.ucr.edu/~kishore</a><br>
&gt;      &gt;<br>
&gt;      &gt; References<br>
&gt;      &gt;<br>
&gt;      &gt;    1. [9]<a href="http://www.cs.ucr.edu/%7Ekishore" target="_blank">http://www.cs.ucr.edu/~kishore</a><br>
&gt;<br>
&gt;    &gt; _______________________________________________<br>
&gt;    &gt; users mailing list<br>
&gt;    &gt; [10]<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;    &gt; [11]<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;    _______________________________________________<br>
&gt;    users mailing list<br>
&gt;    [12]<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;    [13]<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<div class="im">&gt;<br>
&gt; References<br>
&gt;<br>
&gt;    1. <a href="http://www.cs.ucr.edu/%7Ekishore" target="_blank">http://www.cs.ucr.edu/~kishore</a><br>
</div>&gt;    2. mailto:<a href="mailto:siegert@sfu.ca">siegert@sfu.ca</a><br>
&gt;    3. mailto:<a href="mailto:siegert@sfu.ca">siegert@sfu.ca</a><br>
&gt;    4. mailto:<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a><br>
&gt;    5. mailto:<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a><br>
&gt;    6. mailto:<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;    7. mailto:<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;    8. <a href="http://www.cs.ucr.edu/%7Ekishore" target="_blank">http://www.cs.ucr.edu/%7Ekishore</a><br>
&gt;    9. <a href="http://www.cs.ucr.edu/%7Ekishore" target="_blank">http://www.cs.ucr.edu/%7Ekishore</a><br>
&gt;   10. mailto:<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;   11. <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;   12. mailto:<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;   13. <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<div class="im"><br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
</div>--<br>
<div class="im">Martin Siegert<br>
Head, Research Computing<br>
WestGrid Site Lead<br>
IT Services                                phone: 778 782-4691<br>
Simon Fraser University                    fax:   778 782-4242<br>
Burnaby, British Columbia                  email: <a href="mailto:siegert@sfu.ca">siegert@sfu.ca</a><br>
Canada  V5A 1S6<br>
</div><div><div></div><div class="h5">_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

