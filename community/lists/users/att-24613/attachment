<div dir="ltr"><div class="gmail_extra"><div class="gmail_quote">On Mon, Jun 9, 2014 at 3:31 PM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div class="">On Jun 9, 2014, at 5:41 PM, Vineet Rawat &lt;<a href="mailto:vineetrawat0@gmail.com">vineetrawat0@gmail.com</a>&gt; wrote:<br>

<br>
</div><div class="">&gt; We&#39;ve deployed OpenMPI on a small cluster but get a SEGV in orted. Debug information is very limited as the cluster is at a remote customer site. They have a network card with which I&#39;m not familiar (Cisco Systems Inc VIC P81E PCIe Ethernet NIC) and it seems capable of using the usNIC BTL.<br>

<br>
</div>Unfortunately, this is the 1st generation Cisco VIC -- our usNIC BTL is only enabled starting with the 2nd generation Cisco VIC (the 12xx series, not the Pxxx series).<br>
<br>
So runs over this Ethernet NIC should be using just plain ol&#39; TCP.<br></blockquote><div><br></div><div>OK, that should be fine here.</div><div><br></div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">

<div class=""><br>
&gt; I&#39;m suspicious that it might be at the root of the problem. They&#39;re also bonding the 2 ports.<br>
<br>
</div>FWIW, it&#39;s not necessary to bond the interfaces for Open MPI -- meaning that Open MPI will automatically stripe large messages across multiple IP interfaces, etc.  So if they&#39;re bonding for the purposes of MPI bandwidth, you can tell them to turn off the bonding.<br>
</blockquote><div><br></div><div>They said they&#39;re doing it for resilience, not bandwidth.</div><div> </div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">

<br>
Also note that, by default, Open MPI&#39;s TCP MPI transport will aggressively use *all* IP interfaces that it finds.  So in your case, it will likely use bond0, eth0, *and* eth1.  Meaning: OMPI can effectively oversubscribe the network coming out of each VIC.  You might want to set a system-wide default MCA parameter to have OMPI not use the bond0 interface.  For example, add this line to $prefix/etc/mca-params.conf:<br>

<br>
btl_tcp_if_include = eth0,eth1<br>
<br>
This will have OMPI *only* use eth0 and eth1 -- it&#39;ll ignore lo and bond0.<br></blockquote><div><br></div><div>OK, will do.</div><div> </div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">

<div class=""><br>
&gt; However, we&#39;re also doing a few unusual things which could be causing problems. Firstly, we built OpenMPI (I tried 1.6.4 and 1.8.1) without the ibverbs or usnic BTLs. Then, we only ship what (we think) we need: otrerun, orted, libmpi, libmpi_cxx, libopen-rte and libopen-pal. Could there be a dependency on some other binary executable or dlopen&#39;ed library? We also use a special plm_rsh_agent but we&#39;ve used this approach for some time without issue.<br>

<br>
</div>All that sounds fine.<br>
<br>
Open MPI 1.8.1 is preferred; the 1.6.x series is pretty old at this point.  If there&#39;s a bug in 1.8.1, it&#39;s a whole lot easier for us to fix it in the 1.8.x series.<br></blockquote><div><br></div><div>Yes, we&#39;ve been deploying 1.6.4 for a while and are wary of change. We only went to 1.8.1 to see if it changed anything related to this issue. I completely understand that any fixes, if needed, are likely to go in the latest version.</div>
<div> </div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">
<div class=""><br>
&gt; I tried a few different MCA settings, the most restrictive of which led to the failure of this command:<br>
&gt;<br>
&gt; orted --debug --debug-daemons -mca ess env -mca orte_ess_jobid 1925054464 -mca orte_ess_vpid 1 -mca orte_ess_num_procs 2 -mca orte_hnp_uri \&quot;1925054464.0;tcp://10.xxx.xxx.xxx:40547\&quot; --tree-spawn --mca orte_base_help_aggregate 1 --mca plm_rsh_agent yyy --mca btl_tcp_port_min_v4 2000 --mca btl_tcp_port_range_v4 100 --mca btl tcp,self --mca btl_tcp_if_include bond0 --mca orte_create_session_dirs 0 --mca plm_rsh_assume_same_shell 0 -mca plm rsh -mca orte_debug_daemons 1 -mca orte_debug 1 -mca orte_tag_output 1<br>

&gt;<br>
&gt; It seems that the host is set up such that the core file is generated and immediately removed (&quot;ulimit -c&quot; is unlimited) but the abrt daemon is doing something weird.<br>
<br>
</div>As Ralph mentioned, can you verify that the correct version MPI libraries are being picked up on the remote servers?  E.g., is LD_LIBRARY_PATH being set properly in the shell startup files on the remote servers (e.g., to find the 1.8.1 shared libraries)?<br>

<br>
Also make sure that you install each version of Open MPI into a &quot;clean&quot; directory -- don&#39;t install OMPI 1.6.x into /foo and then install OMPI 1.8.x info /foo, too.  The two versions are incompatible with each other, and have conflicting/not-wholly-overlapping libraries.  Meaning: if you install OMPI 1.6.x into /foo, you should either &quot;rm -rf /foo&quot; before you install OMPI 1.8.x into /foo, or just install OMPI 1.8.x into /bar.<br>
</blockquote><div><br></div><div>The installations are entirely separate. The LD_LIBRARY_PATH is set up by our own launch wrapper and I&#39;m confident it&#39;s correct.<br></div><div><br></div><div>Vineet</div><div><br></div>
<blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">
<br>
Make sense? </blockquote><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">
<span class=""><font color="#888888"><br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
</font></span><div class=""><div class="h5"><br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div></div>

