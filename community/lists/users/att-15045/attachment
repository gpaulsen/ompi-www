Dear <meta http-equiv="content-type" content="text/html; charset=utf-8">Ralph,<div><br></div><div>Thank you for your reply. I did check the ld_library_path and recompile with the new version and it worked perfectly.</div><meta http-equiv="content-type" content="text/html; charset=utf-8"><div>
Thank you again.</div><div><br></div><div>Best Regards,</div><div>Toan<br><br><div class="gmail_quote">On Thu, Dec 9, 2010 at 12:30 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">That could mean you didn&#39;t recompile the code using the new version of OMPI. The 1.4 and 1.5 series are not binary compatible - you have to recompile your code.<br>

<br>
If you did recompile, you may be getting version confusion on the backend nodes - you should check your ld_library_path and ensure it is pointing to the 1.5 series install.<br>
<div><div></div><div class="h5"><br>
On Dec 8, 2010, at 8:02 AM, Nguyen Toan wrote:<br>
<br>
&gt; Dear all,<br>
&gt;<br>
&gt; I am having a problem while running mpirun in OpenMPI 1.5 version. I compiled OpenMPI 1.5 with BLCR 0.8.2 and OFED 1.4.1 as follows:<br>
&gt;<br>
&gt; ./configure \<br>
&gt; --with-ft=cr \<br>
&gt; --enable-mpi-threads \<br>
&gt; --with-blcr=/home/nguyen/opt/blcr \<br>
&gt; --with-blcr-libdir=/home/nguyen/opt/blcr/lib \<br>
&gt; --prefix=/home/nguyen/opt/openmpi-1.5 \<br>
&gt; --with-openib \<br>
&gt; --enable-mpirun-prefix-by-default<br>
&gt;<br>
&gt; For programs under &quot;openmpi-1.5/examples&quot; folder, mpirun tests were successful. But mpirun aborted immediately when running a program in MPI CUDA code, which was tested successfully with OpenMPI 1.4.3. Below is the error message.<br>

&gt;<br>
&gt; Can anyone give me an idea about this error?<br>
&gt; Thank you.<br>
&gt;<br>
&gt; Best Regards,<br>
&gt; Toan<br>
&gt; ----------------------<br>
&gt;<br>
&gt;<br>
&gt; [rc002.local:17727] [[56831,1],1] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file util/nidmap.c at line 371<br>
&gt; --------------------------------------------------------------------------<br>
&gt; It looks like orte_init failed for some reason; your parallel process is<br>
&gt; likely to abort.  There are many reasons that a parallel process can<br>
&gt; fail during orte_init; some of which are due to configuration or<br>
&gt; environment problems.  This failure appears to be an internal failure;<br>
&gt; here&#39;s some additional information (which may only be relevant to an<br>
&gt; Open MPI developer):<br>
&gt;<br>
&gt;   orte_ess_base_build_nidmap failed<br>
&gt;   --&gt; Returned value Data unpack would read past end of buffer (-26) instead of ORTE_SUCCESS<br>
&gt; --------------------------------------------------------------------------<br>
&gt; [rc002.local:17727] [[56831,1],1] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file base/ess_base_nidmap.c at line 62<br>
&gt; [rc002.local:17727] [[56831,1],1] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file ess_env_module.c at line 173<br>
&gt; --------------------------------------------------------------------------<br>
&gt; It looks like orte_init failed for some reason; your parallel process is<br>
&gt; likely to abort.  There are many reasons that a parallel process can<br>
&gt; fail during orte_init; some of which are due to configuration or<br>
&gt; environment problems.  This failure appears to be an internal failure;<br>
&gt; here&#39;s some additional information (which may only be relevant to an<br>
&gt; Open MPI developer):<br>
&gt;<br>
&gt;   orte_ess_set_name failed<br>
&gt;   --&gt; Returned value Data unpack would read past end of buffer (-26) instead of ORTE_SUCCESS<br>
&gt; --------------------------------------------------------------------------<br>
&gt; [rc002.local:17727] [[56831,1],1] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file runtime/orte_init.c at line 132<br>
&gt; --------------------------------------------------------------------------<br>
&gt; It looks like MPI_INIT failed for some reason; your parallel process is<br>
&gt; likely to abort.  There are many reasons that a parallel process can<br>
&gt; fail during MPI_INIT; some of which are due to configuration or environment<br>
&gt; problems.  This failure appears to be an internal failure; here&#39;s some<br>
&gt; additional information (which may only be relevant to an Open MPI<br>
&gt; developer):<br>
&gt;<br>
&gt;   ompi_mpi_init: orte_init failed<br>
&gt;   --&gt; Returned &quot;Data unpack would read past end of buffer&quot; (-26) instead of &quot;Success&quot; (0)<br>
&gt; --------------------------------------------------------------------------<br>
&gt; *** An error occurred in MPI_Init<br>
&gt; *** before MPI was initialized<br>
&gt; *** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>
&gt; [rc002.local:17727] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>
&gt; --------------------------------------------------------------------------<br>
&gt; mpirun has exited due to process rank 1 with PID 17727 on<br>
&gt; node rc002 exiting improperly. There are two reasons this could occur:<br>
&gt;<br>
&gt; 1. this process did not call &quot;init&quot; before exiting, but others in<br>
&gt; the job did. This can cause a job to hang indefinitely while it waits<br>
&gt; for all processes to call &quot;init&quot;. By rule, if one process calls &quot;init&quot;,<br>
&gt; then ALL processes must call &quot;init&quot; prior to termination.<br>
&gt;<br>
&gt; 2. this process called &quot;init&quot;, but exited without calling &quot;finalize&quot;.<br>
&gt; By rule, all processes that call &quot;init&quot; MUST call &quot;finalize&quot; prior to<br>
&gt; exiting or it will be considered an &quot;abnormal termination&quot;<br>
&gt;<br>
&gt; This may have caused other processes in the application to be<br>
&gt; terminated by signals sent by mpirun (as reported here).<br>
&gt; --------------------------------------------------------------------------<br>
&gt;<br>
</div></div>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br></div>

