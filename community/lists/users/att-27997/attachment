<div dir="ltr">Dear Abe,<div><br></div><div>Open MPI provides a simple way to validate your code against the eager problem, by forcing the library to use a 0 size eager (basically all messages are then matched). First, identify the networks used by your application and then set both btl_&lt;network&gt;_eager_limit and btl_&lt;network&gt;_rndv_eager_limit to 0 (via the MCA parameters or in the configuration file).</div><div><br></div><div>  George.</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Wed, Nov 4, 2015 at 7:30 PM, ABE Hiroshi <span dir="ltr">&lt;<a href="mailto:habe36@gmail.com" target="_blank">habe36@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word"><div>Dear Dr. Bosilca and Dr. Gouaillardet,</div><div><br></div><div>Thank you for your kind mail. I believe I could configure the problem.</div><div><br></div><div>As is described in Dr. Boslica’s mail, this should be the eager problem. In order to avoid that we should take one of the methods which are suggested in Dr. Gouaillardet’s mail.</div><div><br></div><div>Also I suppose to try MPICH but our code should work on both of the most popular MPI implementations.</div><div><br></div><div>Again, thank you very much for your kind helps.</div><br><div><div>2015/11/05 0:36、George Bosilca &lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank">bosilca@icl.utk.edu</a>&gt; のメール：</div><span class=""><br><blockquote type="cite"><div dir="ltr">A reproducer without the receiver part limited usability. <div><br></div><div>1) Have you checked that your code doesn&#39;t suffer from the eager problem? It might happen that if your message size is under the eager limit, your perception is that the code works when in fact your message is just on the unexpected queue on the receiver, and will potentially never be matched. On the opposite, when the length of the message is larger than the eager size (which is network dependent), the code will stall obviously in MPI_Wait as the send is never matched. The latter is the expected and defined behavior based on the MPI standard.<div><br></div><div>2) In order to rule this out add a lock around your sends to make sure that 1) a sequential version of the code is valid; and 2) that we are not facing some consistent thread interleaving issues. If this step successfully complete, then we can start looking deeper in the OMPI internals for a bug.<br><div><br></div><div>  George.</div><div><br></div></div></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Wed, Nov 4, 2015 at 12:34 AM, ABE Hiroshi <span dir="ltr">&lt;<a href="mailto:habe36@gmail.com" target="_blank">habe36@gmail.com</a>&gt;</span> wrote:<br></div></div></blockquote></span>[snip]</div><div><br></div><blockquote type="cite">Abe-san,<div><br></div><div>you can be blocking on one side, and non blocking on the other side.</div><div>for example, one task can do MPI_Send, and the other MPI_Irecv and MPI_Wait.</div><div><br></div><div>in order to avoid deadlock, your program should do</div><div>1. master MPI_Isend and start the workers</div><div>2. worker receive and process messages (in there is one recv per thread, you can do MPI_Recv e.g. blocking recv)</div><div>3. master MPI_Wait the request used in MPI_Isend</div><div>4. do simulation</div><div>I do not know if some kind of synchronization is required between master and workers.</div><div>the key point is you MPI_Wait after the workers have been started.</div><div><br></div><div>I do not know all the details of your program, but you might avoid using threads :</div><div>1. MPI_Isend</div><div>2. several MPI_Irecv</div><div>3. MPI_Waitall (or a loop with MPI_Waitany/MPI_Waitsome)</div><div>4. do simulation</div><div><br></div><div>if you really want threads, an other option is to start the worker after MPI_Waitany/MPI_Waitsome</div><div><br></div><div>once again, I do not know your full program, so I can just guess.</div>you might also want to try an other MPI flavor (such as mpich), since your program could be correct and the deadlock might be open MPI specific.</blockquote><span class="HOEnZb"><font color="#888888"><div><br></div><div>
<span style="border-collapse:separate;border-spacing:0px"><div><div><div><div><div>ABE Hiroshi</div><div> Three Wells, JAPAN</div><div> <a href="http://www.3wells-computing.com/" target="_blank">http://www.3wells-computing.com/</a></div></div><br></div><br></div><br></div><br></span><br>

</div>


<br></font></span></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/11/27996.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/11/27996.php</a><br></blockquote></div><br></div>

