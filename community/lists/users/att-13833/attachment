yes, <div><br></div><div>somehow after the second install, the installlation is consistent.</div><div><br></div><div>im only running into an issue, might be mpi im not sure.</div><div>these nodes, each one have 8 phisical procesors (2xIntel Xeon quad core), and 16 virtual ones, btw i have ubuntu server 64bit 10.04 instaled on these nodes.</div>

<div><br></div><div>the problem seems to be whenever y try to use over 8 proceses (make use of the virtual ones), i get a horrible error saying about a kernel error and a certain cpu that crashed, the error hags there for about a minute, then it switches to another cpu and shows the same error. i have no other option to press power off button.</div>

<div><br></div><div>ill try to copy the error, and post it.</div><div><br></div><div><br></div><meta http-equiv="content-type" content="text/html; charset=utf-8"><div><br><div class="gmail_quote">On Wed, Jul 28, 2010 at 7:39 AM, Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">This issue is usually caused by installing one version of Open MPI over an older version:<br>
<br>
    <a href="http://www.open-mpi.org/faq/?category=building#install-overwrite" target="_blank">http://www.open-mpi.org/faq/?category=building#install-overwrite</a><br>
<div><div></div><div class="h5"><br>
<br>
On Jul 27, 2010, at 10:35 PM, Cristobal Navarro wrote:<br>
<br>
&gt;<br>
&gt; On Tue, Jul 27, 2010 at 7:29 PM, Gus Correa &lt;<a href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt; wrote:<br>
&gt; Hi Cristobal<br>
&gt;<br>
&gt; Does it run only on the head node alone?<br>
&gt; (Fuego? Agua? Acatenango?)<br>
&gt; Try to put only the head node on the hostfile and execute with mpiexec.<br>
&gt; --&gt; i will try only with the head node, and post results back<br>
&gt; This may help sort out what is going on.<br>
&gt; Hopefully it will run on the head node.<br>
&gt;<br>
&gt; Also, do you have Infinband connecting the nodes?<br>
&gt; The error messages refer to the openib btl (i.e. Infiniband),<br>
&gt; and complains of<br>
&gt;<br>
&gt; no we are just using normal network 100MBit/s , since i am just testing yet.<br>
&gt;<br>
&gt; &quot;perhaps a missing symbol, or compiled for a different<br>
&gt; version of Open MPI?&quot;.<br>
&gt; It sounds as a mixup of versions/builds.<br>
&gt;<br>
&gt; --&gt; i agree, somewhere there must be the remains of the older version<br>
&gt;<br>
&gt; Did you configure/build OpenMPI from source, or did you install<br>
&gt; it with apt-get?<br>
&gt; It may be easier/less confusing to install from source.<br>
&gt; If you did, what configure options did you use?<br>
&gt;<br>
&gt; --&gt;i installed from source,<br>
&gt; ./configure --prefix=/opt/openmpi-1.4.2 --with-sge --without-xgid --disable--static<br>
&gt;<br>
&gt; Also, as for the OpenMPI runtime environment,<br>
&gt; it is not enough to set it on<br>
&gt; the command line, because it will be effective only on the head node.<br>
&gt; You need to either add them to the PATH and LD_LIBRARY_PATH<br>
&gt; on your .bashrc/.cshrc files (assuming these files and your home directory are *also* shared with the nodes via NFS),<br>
&gt; or use the --prefix option of mpiexec to point to the OpenMPI main directory.<br>
&gt;<br>
&gt; yes, all nodes have their PATH and LD_LIBRARY_PATH set up properly inside the login scripts ( .bashrc in my case  )<br>
&gt;<br>
&gt; Needless to say, you need to check and ensure that the OpenMPI directory (and maybe your home directory, and your work directory) is (are)<br>
&gt; really mounted on the nodes.<br>
&gt;<br>
&gt; --&gt; yes, doublechecked that they are<br>
&gt;<br>
&gt; I hope this helps,<br>
&gt;<br>
&gt; --&gt; thanks really!<br>
&gt;<br>
&gt; Gus Correa<br>
&gt;<br>
&gt; Update: i just reinstalled openMPI, with the same parameters, and it seems that the problem has gone, i couldnt test entirely but when i get back to lab ill confirm.<br>
&gt;<br>
&gt; best regards!<br>
&gt; Cristobal<br>
&gt;<br>
</div></div><div class="im">&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
</div><font color="#888888">--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to:<br>
<a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
</font><div><div></div><div class="h5"><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div>

