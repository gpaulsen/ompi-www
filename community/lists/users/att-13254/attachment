<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">Ok,<div><br></div><div>1. type 'which mdrun' to see where the mdrun executable resides.</div><div>2. type ldd 'which mdrun' to find out against which mpi library it is linked</div><div>3. type which mpirun (or which mpiexec, whatever you use) to verify that</div><div>this is the right mpi launcher for your mdrun.</div><div>4. If the MPI's do not match, either use the right mpiexec or recompile</div><div>gromacs with the current mpi.</div><div><br></div><div>Carsten</div><div><br></div><div><br><div><div>On Jun 8, 2010, at 5:50 PM, lauren wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><span class="Apple-style-span" style="border-collapse: separate; font-family: Helvetica; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; "><div><div style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; font-family: 'times new roman', 'new york', times, serif; font-size: 12pt; "><div style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; ">I saw<span class="Apple-converted-space">&nbsp;</span><br></div><div style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; font-family: 'times new roman', 'new york', times, serif; font-size: 12pt; ">Host: &lt;somename&gt; pid: &lt;somepid&gt; nodeid: 0 nnodes: 1<br><br>really it`s running in 1 node<br>and All of you really undestood my problem, thanks<br><br>But how can I fix it.<br>How can I run 1 job in 4 nodes...?<br>I really need help,<span class="Apple-converted-space">&nbsp;</span><br>I took a look in my files and erase all the errors and the implementations seem corect.<br>From the beginning, please.<br>`case all tutorials only explain the same thing that look right.<br>And thanks very much for this help!<br><br><br>&nbsp;<br><div style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; font-family: arial, helvetica, sans-serif; font-size: 13px; "><font face="Tahoma" size="2"><hr size="1"><b><span style="font-weight: bold; ">De:</span></b><span class="Apple-converted-space">&nbsp;</span>Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br><b><span style="font-weight: bold; ">Para:</span></b><span class="Apple-converted-space">&nbsp;</span>Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br><b><span style="font-weight: bold; ">Enviadas:</span></b><span class="Apple-converted-space">&nbsp;</span>Terça-feira, 8 de Junho de 2010 10:30:03<br><b><span style="font-weight: bold; ">Assunto:</span></b><span class="Apple-converted-space">&nbsp;</span>Re: [OMPI users] Res: Gromacs run in parallel<br></font><br>No, I'm sorry -- I wasn't clear.&nbsp; What I meant was, that if you run:<br><br>&nbsp; mpirun -np 4 my_mpi_application<br><br>1. If you see a single, 4-process MPI job (regardless of how many nodes/servers it's spread across), then all is good.&nbsp; This is what you want.<br><br>2. But if you're seeing 4 independent 1-process MPI jobs (again, regardless of how many nodes/servers they are spread across), it's possible that you compiled your application with MPI implementation X and then used the "mpirun" from MPI implementation Y.&nbsp;<span class="Apple-converted-space">&nbsp;</span><br><br>You will need X==Y to make it work properly -- i.e., to see case #1, above.&nbsp; I mention this because your first post mentioned that you're seeing the same job run 4 times.&nbsp; This implied to me that you are running into case #2.&nbsp; If I misunderstood your problem, then ignore me and forgive the noise.<br><br><br><br>On Jun 8, 2010, at 9:20 AM, Carsten Kutzner wrote:<br><br>&gt; On Jun 8, 2010, at 3:06 PM, Jeff Squyres wrote:<br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; I know nothing about Gromacs, but you might want to ensure that your Gromacs was compiled with Open MPI.&nbsp; A common symptom of "mpirun -np 4 my_mpi_application" running 4 1-process MPI jobs (instead of 1 4-process MPI job) is that you compiled my_mpi_application with one MPI implementation, but then used the mpirun from a different MPI implementation.<br>&gt; &gt;<br>&gt; Hi,<br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; this can be checked by looking at the Gromacs output file md.log. The second line should<br>&gt; read something like<br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; Host: &lt;somename&gt; pid: &lt;somepid&gt; nodeid: 0 nnodes: 4<br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; Lauren, you will want to ensure that nnodes is 4 in your case, and not 1.<br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; You can also easily test that without any input file by typing<br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; mpirun -np 4 mdrun -h<br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; and then should see<br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; NNODES=4, MYRANK=1, HOSTNAME=&lt;...&gt;<br>&gt; NNODES=4, MYRANK=2, HOSTNAME=&lt;...&gt;<br>&gt; NNODES=4, MYRANK=3, HOSTNAME=&lt;...&gt;<br>&gt; NNODES=4, MYRANK=4, HOSTNAME=&lt;...&gt;<br>&gt; ...<br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; Carsten<br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt;<br>&gt; &gt; On Jun 8, 2010, at 8:59 AM, lauren wrote:<br>&gt; &gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; The version of Gromacs is 4.0.7.<br>&gt; &gt;&gt; This is the first time that I using Gromacs, then excuse me if I'm nonsense.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Wich part of md.log output&nbsp; should I post?<br>&gt; &gt;&gt; after or before the input description?<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; thanks for all,<br>&gt; &gt;&gt; and sorry<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; De: Carsten Kutzner &lt;<a ymailto="mailto:ckutzne@gwdg.de" href="mailto:ckutzne@gwdg.de">ckutzne@gwdg.de</a>&gt;<br>&gt; &gt;&gt; Para: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>&gt; &gt;&gt; Enviadas: Domingo, 6 de Junho de 2010 9:51:26<br>&gt; &gt;&gt; Assunto: Re: [OMPI users] Gromacs run in parallel<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Hi,<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; which version of Gromacs is this? Could you post the first lines of<br>&gt; &gt;&gt; the md.log output file?<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Carsten<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; On Jun 5, 2010, at 10:23 PM, lauren wrote:<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;&gt; sorry my english..<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; I want to know how can I run&nbsp; Gromancs in parallel!<br>&gt; &gt;&gt;&gt; Because when I used<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; mdrun &amp;<br>&gt; &gt;&gt;&gt; mpiexec -np 4 mdrun_mpi -v -deffnm em<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; to run the minimization in 4 cores &gt; all cores make the same job, again!<br>&gt; &gt;&gt;&gt; They don't run together.<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt;&gt;&gt; I want all in parallel make the job faster.<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; what could be wrong?<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; thank's a lot!<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; _______________________________________________<br>&gt; &gt;&gt;&gt; users mailing list<br>&gt; &gt;&gt;&gt;<span class="Apple-converted-space">&nbsp;</span><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt;&gt;&gt;<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; _______________________________________________<br>&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt;<span class="Apple-converted-space">&nbsp;</span><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt;&gt;<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt; --<br>&gt; &gt; Jeff Squyres<br>&gt; &gt;<span class="Apple-converted-space">&nbsp;</span><a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>&gt; &gt; For corporate legal information go to:<br>&gt; &gt;<span class="Apple-converted-space">&nbsp;</span><a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt; _______________________________________________<br>&gt; &gt; users mailing list<br>&gt; &gt;<span class="Apple-converted-space">&nbsp;</span><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt;<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; --<br>&gt; Dr. Carsten Kutzner<br>&gt; Max Planck Institute for Biophysical Chemistry<br>&gt; Theoretical and Computational Biophysics<br>&gt; Am Fassberg 11, 37077 Goettingen, Germany<br>&gt; Tel. +49-551-2012313, Fax: +49-551-2012302<br>&gt;<span class="Apple-converted-space">&nbsp;</span><a href="http://www.mpibpc.mpg.de/home/grubmueller/ihp/ckutzne" target="_blank">http://www.mpibpc.mpg.de/home/grubmueller/ihp/ckutzne</a><br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt;<span class="Apple-converted-space">&nbsp;</span><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<span class="Apple-converted-space">&nbsp;</span><br><br><br>--<span class="Apple-converted-space">&nbsp;</span><br>Jeff Squyres<br><a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>For corporate legal information go to:<br><a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br><br><br>_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div></div><br>&nbsp;_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></span></blockquote></div><br><div>
<span class="Apple-style-span" style="border-collapse: separate; color: rgb(0, 0, 0); font-family: Helvetica; font-size: 14px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0; "><span class="Apple-style-span" style="border-collapse: separate; color: rgb(0, 0, 0); font-family: Helvetica; font-size: 14px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; "><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><div><br class="Apple-interchange-newline">--</div><div>Dr.&nbsp;Carsten Kutzner</div><div>Max Planck Institute for Biophysical Chemistry</div><div>Theoretical and Computational Biophysics</div><div>Am Fassberg 11,&nbsp;37077 Goettingen, Germany</div><div>Tel. +49-551-2012313, Fax: +49-551-2012302</div><div><a href="http://www.mpibpc.mpg.de/home/grubmueller/ihp/ckutzne">http://www.mpibpc.mpg.de/home/grubmueller/ihp/ckutzne</a></div><div><br></div></div><br class="Apple-interchange-newline"></span><br class="Apple-interchange-newline"></span>
</div>
<br></div></body></html>
