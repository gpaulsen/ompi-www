<br><font color="#000099"><font size="4"><font face="garamond,serif">Dear Madam/Sir,<br><br></font></font></font><br><font color="#000099"><font size="4"><font face="garamond,serif">I have a serial Fortran code (f90), dealing with matrix 
diagonalizing subroutines, and recently got its parallel version to be 
faster in some unfeasible parts via the serial program. <br>I have been using the following commands for initializing MPI in the code<br>




---------------<br>    call MPI_INIT(ierr)<br>    call MPI_COMM_SIZE(MPI_COMM_WORLD, p, ierr)<br>    call MPI_COMM_RANK(MPI_COMM_WORLD, my_rank, ierr)<br><br>CPU requirement &gt;&gt; <font color="#000099"><font size="4"><font face="garamond,serif">pmem=1500mb,nodes=5:ppn=8 &lt;&lt;</font></font></font></font></font></font><br>











<font color="#000099"><font size="4"><font face="garamond,serif">-------------------<br>Everything looks OK when matrix dimensions are less than 1000x1000. When I increase 
the matrix dimensions to some larger values the parallel code gets the 
following error<br>



------------------<br></font></font></font><font color="#000099"><font size="4"><font face="garamond,serif"><font color="#000099"><font size="4"><font face="garamond,serif">mpirun noticed that process rank 6 with PID 1566 on node node1082 exited on signal 11 (Segmentation fault)</font></font></font>
<br>------------------<br>There is no such error with the serial version
 even for larger matrix dimensions than 2400x2400. I then thought it 
might be raised by the number of nodes and memory space I&#39;m requiring. 
Then changed it as follows<br>


</font></font></font><br><font color="#000099"><font size="4"><font face="garamond,serif"><font color="#000099"><font size="4"><font face="garamond,serif">pmem=10gb,nodes=20:ppn=2<br><br>which is more or less similar to what I&#39;m using for serial jobs (</font></font></font></font></font></font><font color="#000099"><font size="4"><font face="garamond,serif"><font color="#000099"><font size="4"><font face="garamond,serif"><font color="#000099"><font size="4"><font face="garamond,serif">mem=10gb,nodes=1:ppn=1)</font></font></font>.
 But the problem persists still. Is there any limitation on MPI 
subroutines for transferring data size or the issue would be raised by 
some cause else? <br>


<br>Best of Regards,<br>Mohammad</font></font></font></font></font></font><br>

