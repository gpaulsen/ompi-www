<div dir="ltr"><div><div><div><div><br></div>Hi George,<br><br></div>Sorry for the delay in writing to you.<br><br></div>Your latest suggestion has worked admirably well.<br><br></div>Thanks a lot for your help.<br><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Sun, Apr 26, 2015 at 9:32 PM, George Bosilca <span dir="ltr">&lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank">bosilca@icl.utk.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><span style="font-size:12.8000001907349px">With the arguments I sent you the error about connection refused should have disappeared. Let&#39;s try to force all traffic over the first TCP interface eth3. Try the following flags to your mpirun: </span></div><div><span style="font-size:12.8000001907349px"><br></span></div><div><span style="font-size:12.8000001907349px">--mca pml ob1 --mca btl tcp,sm,self --mca btl_tcp_if_include eth3</span></div><div><span style="font-size:12.8000001907349px"><br></span></div><div><span style="font-size:12.8000001907349px">  George.</span></div><div><span style="font-size:12.8000001907349px"><br></span></div></div><div class="gmail_extra"><br><div class="gmail_quote"><div><div class="h5">On Sun, Apr 26, 2015 at 8:04 AM, Manumachu Reddy <span dir="ltr">&lt;<a href="mailto:manumachu.reddy@gmail.com" target="_blank">manumachu.reddy@gmail.com</a>&gt;</span> wrote:<br></div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div class="h5"><div dir="ltr"><br><div>Hi George,</div><div><br></div><div>I am afraid the suggestion to use bcl_tcp_if_exclude has not applied. I executed the following command:</div><div><br></div><div><b>shell$ mpirun <span style="font-size:12.8000001907349px">--mca btl_tcp_if_exclude mic0,mic1 -app appfile</span></b></div><div><span style="font-size:12.8000001907349px">&lt;the same output and hang&gt;</span></div><div><span style="font-size:12.8000001907349px"><br></span></div><div><span style="font-size:12.8000001907349px">Please let me know if there are options to mpirun (apart from -v) to get verbose output to understand what is happening.</span></div><div><span style="font-size:12.8000001907349px"><br></span></div></div><div class="gmail_extra"><br><div class="gmail_quote"><div><div>On Fri, Apr 24, 2015 at 5:59 PM, George Bosilca <span dir="ltr">&lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank">bosilca@icl.utk.edu</a>&gt;</span> wrote:<br></div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div><div dir="auto"><div>Manumachu,</div><div><br></div><div>Both nodes have the same IP for their Phi (mic0 and mic1). This is OK as long as they don&#39;t try to connect to each other using these addresses. A simple fix is to prevent OMPI from using the supposedly local mic0 and mic1 IP. Add --mca btl_tcp_if_exclude mic0,mic1 to your mpirun command and things should start working better.</div><div><br></div><div>George.<br><br><br></div><div><div><div><br>On Apr 24, 2015, at 03:32, Manumachu Reddy &lt;<a href="mailto:manumachu.reddy@gmail.com" target="_blank">manumachu.reddy@gmail.com</a>&gt; wrote:<br><br></div><blockquote type="cite"><div><div dir="ltr"><br>Dear OpenMPI Users,<br><br>I request your help to resolve a hang in my OpenMPI application.<br><br>My OpenMPI application hangs in MPI_Comm_split() operation. The code for this simple application is at the end of this email. Broadcast works fine.<br><br>My experimental setup comprises of two RHEL6.4 Linux nodes. Each node has 2 mic cards. Please note that although there are mic cards, I do not use mic cards in my OpenMPI application.<br><br>I have tested with two OpenMPI versions (1.6.5, 1.8.4). I see the hang in both the versions. OpenMPI is installed using the following command:<br><br>./configure --prefix=/home/manumachu/OpenMPI/openmpi-1.8.4/OPENMPI_INSTALL_ICC CC=&quot;icc -fPIC&quot; CXX=&quot;icpc -fPIC&quot;<br><br>I have made sure I have turned off the firewall using the following commands:<br><br>sudo service iptables save<br>sudo service iptables stop<br>sudo chkconfig iptables off<br><br>I made sure the mic cards are online and healthy. I am able to login to the mic cards.<br><br>I use an appfile to launch 2 processes on each node.<br><br>I have also attached the &quot;ifconfig&quot; list for each node. Could this problem be related to multiple network interfaces (from the application output also shown at the end of the email)?<br><br>Please let me know if you need further information and I look forward to your suggestions.<br><br>Best Regards<br>Manumachu<br><br><u><b>Application</b></u><br><br>#include &lt;stdio.h&gt;<br>#include &lt;mpi.h&gt;<br><br>int main(int argc, char** argv)<br>{<br>    int me, hostnamelen;<br>    char hostname[MPI_MAX_PROCESSOR_NAME];<br><br>    MPI_Init(&amp;argc, &amp;argv);<br><br>    MPI_Get_processor_name(hostname, &amp;hostnamelen);<br><br>    MPI_Comm_rank(MPI_COMM_WORLD, &amp;me);<br>    printf(&quot;Hostname %s: Me is %d.\n&quot;, hostname, me);<br><br>    int a;<br>    MPI_Bcast(&amp;a, 1, MPI_INT, 0, MPI_COMM_WORLD);<br><br>    printf(&quot;Hostname %s: Me %d broadcasted.\n&quot;, hostname, me);<br><br>    MPI_Comm intraNodeComm;<br>    int rc = MPI_Comm_split(<br>                MPI_COMM_WORLD,<br>                me,<br>                me,<br>                &amp;intraNodeComm<br>    );<br><br>    if (rc != MPI_SUCCESS)<br>    {<br>       printf(&quot;MAIN: Problems MPI_Comm_split...Exiting...\n&quot;);<br>       return -1;<br>    }<br><br>    printf(&quot;Hostname %s: Me %d after comm split.\n&quot;, hostname, me);<br>    MPI_Comm_free(&amp;intraNodeComm);<br>    MPI_Finalize();<br><br>    return 0;<br>}<br><br><u><b>Application output</b></u><br><br>Hostname server5: Me is 0.<br>Hostname server5: Me is 1.<br>Hostname server5: Me 1 broadcasted.<br>Hostname server5: Me 0 broadcasted.<br>[server5][[50702,1],0][btl_tcp_endpoint.c:655:mca_btl_tcp_endpoint_complete_connect] connect() to 172.31.1.254 failed: Connection refused (111)<br>[server5][[50702,1],1][btl_tcp_endpoint.c:655:mca_btl_tcp_endpoint_complete_connect] connect() to 172.31.1.254 failed: Connection refused (111)<br>Hostname server2: Me is 2.<br>Hostname server2: Me 2 broadcasted.<br>Hostname server2: Me is 3.<br>Hostname server2: Me 3 broadcasted.<br><br><u><b>server2 ifconfig</b></u><br><br>eth0      Link encap:Ethernet...<br>          UP BROADCAST MULTICAST  MTU:1500  Metric:1<br>          ...<br>eth1      Link encap:Ethernet...<br>          UP BROADCAST MULTICAST  MTU:1500  Metric:1<br>          ...<br>eth2      Link encap:Ethernet...<br>          UP BROADCAST MULTICAST  MTU:1500  Metric:1<br>          ...<br>eth3      Link encap:Ethernet...<br>          inet addr:172.17.27.17  Bcast:172.17.27.255  Mask:255.255.255.0<br>          inet6 addr: fe80::921b:eff:fe42:a5ba/64 Scope:Link<br>          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1<br>          ...<br>lo        Link encap:Local Loopback  <br>          inet addr:127.0.0.1  Mask:255.0.0.0<br>          UP LOOPBACK RUNNING  MTU:65536  Metric:1<br>          ...<br><br>mic0      Link encap:Ethernet<br>          inet addr:172.31.1.254  Bcast:172.31.1.255  Mask:255.255.255.0<br>          ...<br><br>mic1      Link encap:Ethernet...<br>          inet addr:172.31.2.254  Bcast:172.31.2.255  Mask:255.255.255.0<br>          ...<br><br><u><b>server5 ifconfig</b></u><br>eth0      Link encap:Ethernet...<br>          UP BROADCAST MULTICAST  MTU:1500  Metric:1<br>          ...<br><br>eth1      Link encap:Ethernet...<br>          UP BROADCAST MULTICAST  MTU:1500  Metric:1<br>          ...<br><br>eth2      Link encap:Ethernet...<br>          UP BROADCAST MULTICAST  MTU:1500  Metric:1<br>          ...<br><br>eth3      Link encap:Ethernet...<br>          inet addr:172.17.27.14  Bcast:172.17.27.255  Mask:255.255.255.0<br>          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1<br>          ...<br><br>lo        Link encap:Local Loopback  <br>          inet addr:127.0.0.1  Mask:255.0.0.0<br>          ...<br><br>mic0      Link encap:Ethernet...<br>          inet addr:172.31.1.254  Bcast:172.31.1.255  Mask:255.255.255.0<br>          UP BROADCAST RUNNING  MTU:64512  Metric:1<br>          ...<br><br>mic1      Link encap:Ethernet...<br>          inet addr:172.31.2.254  Bcast:172.31.2.255  Mask:255.255.255.0<br>          ...<br><br></div>
</div></blockquote></div></div><blockquote type="cite"><div><span>_______________________________________________</span><br><span>users mailing list</span><br><span><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a></span><br><span>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></span><br><span>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/04/26780.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/04/26780.php</a></span></div></blockquote></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/04/26784.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/04/26784.php</a><span><font color="#888888"><br></font></span></blockquote></div><span><font color="#888888"><br><br clear="all"><div><br></div>-- <br><div><div dir="ltr">Best Regards<br>Ravi<br></div></div>
</font></span></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/04/26788.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/04/26788.php</a><br></blockquote></div><br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/04/26791.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/04/26791.php</a><br></blockquote></div><br><br clear="all"><br>-- <br><div class="gmail_signature"><div dir="ltr">Best Regards<br>Ravi<br></div></div>
</div>

