<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <meta content="text/html; charset=ISO-8859-1"
      http-equiv="Content-Type">
    <title></title>
  </head>
  <body bgcolor="#ffffff" text="#000000">
    MPI can get through your firewall, right?<br>
    <br>
    Damien<br>
    <br>
    On 20/05/2011 12:53 PM, Jason Mackay wrote:
    <blockquote cite="mid:BAY149-w7EF09BB6CAD4D22280350EC710@phx.gbl"
      type="cite">
      <style><!--
.hmmessage P
{
margin:0px;
padding:0px
}
body.hmmessage
{
font-size: 10pt;
font-family:Tahoma
}
--></style>
      I have verified that&nbsp;disabling UAC does not fix the problem.
      xhlp.exe starts, threads spin up on both machines, CPU usage is at
      80-90% but no progress is ever made.<br>
      &nbsp;<br>
      &gt;From this state, Ctrl-break&nbsp;on the&nbsp;head node&nbsp;yields the
      following output:<br>
      <br>
      [REMOTEMACHINE:02032] [[20816,1],0]-[[20816,0],0]
      mca_oob_tcp_msg_recv: readv failed: Unknown error (108)<br>
      [REMOTEMACHINE:05064] [[20816,1],1]-[[20816,0],0]
      mca_oob_tcp_msg_recv: readv failed: Unknown error (108)<br>
      [REMOTEMACHINE:05420] [[20816,1],2]-[[20816,0],0]
      mca_oob_tcp_msg_recv: readv failed: Unknown error (108)<br>
      [REMOTEMACHINE:03852] [[20816,1],3]-[[20816,0],0]
      mca_oob_tcp_msg_recv: readv failed: Unknown error (108)<br>
      [REMOTEMACHINE:05436] [[20816,1],4]-[[20816,0],0]
      mca_oob_tcp_msg_recv: readv failed: Unknown error (108)<br>
      [REMOTEMACHINE:04416] [[20816,1],5]-[[20816,0],0]
      mca_oob_tcp_msg_recv: readv failed: Unknown error (108)<br>
      [REMOTEMACHINE:02032] [[20816,1],0] routed:binomial: Connection to
      lifeline [[20816,0],0] lost<br>
      [REMOTEMACHINE:05064] [[20816,1],1] routed:binomial: Connection to
      lifeline [[20816,0],0] lost<br>
      [REMOTEMACHINE:05420] [[20816,1],2] routed:binomial: Connection to
      lifeline [[20816,0],0] lost<br>
      [REMOTEMACHINE:03852] [[20816,1],3] routed:binomial: Connection to
      lifeline [[20816,0],0] lost<br>
      [REMOTEMACHINE:05436] [[20816,1],4] routed:binomial: Connection to
      lifeline [[20816,0],0] lost<br>
      [REMOTEMACHINE:04416] [[20816,1],5] routed:binomial: Connection to
      lifeline [[20816,0],0] lost<br>
      &nbsp;<br>
      &nbsp;<br>
      &nbsp;<br>
      &gt; From: <a class="moz-txt-link-abbreviated" href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a><br>
      &gt; Subject: users Digest, Vol 1911, Issue 1<br>
      &gt; To: <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; Date: Fri, 20 May 2011 08:14:13 -0400<br>
      &gt; <br>
      &gt; Send users mailing list submissions to<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; <br>
      &gt; To subscribe or unsubscribe via the World Wide Web, visit<br>
      &gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      &gt; or, via email, send a message with subject or body 'help' to<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a><br>
      &gt; <br>
      &gt; You can reach the person managing the list at<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:users-owner@open-mpi.org">users-owner@open-mpi.org</a><br>
      &gt; <br>
      &gt; When replying, please edit your Subject line so it is more
      specific<br>
      &gt; than "Re: Contents of users digest..."<br>
      &gt; <br>
      &gt; <br>
      &gt; Today's Topics:<br>
      &gt; <br>
      &gt; 1. Re: Error: Entry Point Not Found (Zhangping Wei)<br>
      &gt; 2. Re: Problem with MPI_Request, MPI_Isend/recv and<br>
      &gt; MPI_Wait/Test (George Bosilca)<br>
      &gt; 3. Re: v1.5.3-x64 does not work on Windows 7 workgroup (Jeff
      Squyres)<br>
      &gt; 4. Re: Error: Entry Point Not Found (Jeff Squyres)<br>
      &gt; 5. Re: openmpi (1.2.8 or above) and Intel composer XE 2011
      (aka<br>
      &gt; 12.0) (Jeff Squyres)<br>
      &gt; 6. Re: Openib with &gt; 32 cores per node (Jeff Squyres)<br>
      &gt; 7. Re: MPI_COMM_DUP freeze with OpenMPI 1.4.1 (Jeff Squyres)<br>
      &gt; 8. Re: Trouble with MPI-IO (Jeff Squyres)<br>
      &gt; 9. Re: Trouble with MPI-IO (Tom Rosmond)<br>
      &gt; 10. Re: Problem with MPI_Request, MPI_Isend/recv and<br>
      &gt; MPI_Wait/Test (David B?ttner)<br>
      &gt; 11. Re: Trouble with MPI-IO (Jeff Squyres)<br>
      &gt; 12. Re: MPI_Alltoallv function crashes when np &gt; 100 (Jeff
      Squyres)<br>
      &gt; 13. Re: MPI_ERR_TRUNCATE with MPI_Allreduce() error, but only<br>
      &gt; sometimes... (Jeff Squyres)<br>
      &gt; 14. Re: Trouble with MPI-IO (Jeff Squyres)<br>
      &gt; <br>
      &gt; <br>
      &gt;
      ----------------------------------------------------------------------<br>
      &gt; <br>
      &gt; Message: 1<br>
      &gt; Date: Thu, 19 May 2011 09:13:53 -0700 (PDT)<br>
      &gt; From: Zhangping Wei <a class="moz-txt-link-rfc2396E" href="mailto:zhangping_wei@yahoo.com">&lt;zhangping_wei@yahoo.com&gt;</a><br>
      &gt; Subject: Re: [OMPI users] Error: Entry Point Not Found<br>
      &gt; To: <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; Message-ID:
      <a class="moz-txt-link-rfc2396E" href="mailto:101342.7961.qm@web111818.mail.gq1.yahoo.com">&lt;101342.7961.qm@web111818.mail.gq1.yahoo.com&gt;</a><br>
      &gt; Content-Type: text/plain; charset="gb2312"<br>
      &gt; <br>
      &gt; Dear Paul,<br>
      &gt; <br>
      &gt; I checked the way 'mpirun -np N &lt;cmd&gt;' you mentioned,
      but it was the same <br>
      &gt; problem.<br>
      &gt; <br>
      &gt; I guess it may related to the system I used, because I have
      used it correctly in <br>
      &gt; another XP 32 bit system.<br>
      &gt; <br>
      &gt; I look forward to more advice.Thanks.<br>
      &gt; <br>
      &gt; Zhangping <br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; ________________________________<br>
      &gt; ???????? <a class="moz-txt-link-rfc2396E" href="mailto:users-request@open-mpi.org">"users-request@open-mpi.org"</a>
      <a class="moz-txt-link-rfc2396E" href="mailto:users-request@open-mpi.org">&lt;users-request@open-mpi.org&gt;</a><br>
      &gt; ???????? <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; ?????????? 2011/5/19 (????) 11:00:02 ????<br>
      &gt; ?? ???? users Digest, Vol 1910, Issue 2<br>
      &gt; <br>
      &gt; Send users mailing list submissions to<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; <br>
      &gt; To subscribe or unsubscribe via the World Wide Web, visit<br>
      &gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      &gt; or, via email, send a message with subject or body 'help' to<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a><br>
      &gt; <br>
      &gt; You can reach the person managing the list at<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:users-owner@open-mpi.org">users-owner@open-mpi.org</a><br>
      &gt; <br>
      &gt; When replying, please edit your Subject line so it is more
      specific<br>
      &gt; than "Re: Contents of users digest..."<br>
      &gt; <br>
      &gt; <br>
      &gt; Today's Topics:<br>
      &gt; <br>
      &gt; 1. Re: Error: Entry Point Not Found (Paul van der Walt)<br>
      &gt; 2. Re: Openib with &gt; 32 cores per node (Robert Horton)<br>
      &gt; 3. Re: Openib with &gt; 32 cores per node (Samuel K.
      Gutierrez)<br>
      &gt; <br>
      &gt; <br>
      &gt;
      ----------------------------------------------------------------------<br>
      &gt; <br>
      &gt; Message: 1<br>
      &gt; Date: Thu, 19 May 2011 16:14:02 +0100<br>
      &gt; From: Paul van der Walt <a class="moz-txt-link-rfc2396E" href="mailto:paul@denknerd.nl">&lt;paul@denknerd.nl&gt;</a><br>
      &gt; Subject: Re: [OMPI users] Error: Entry Point Not Found<br>
      &gt; To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a><br>
      &gt; Message-ID:
      <a class="moz-txt-link-rfc2396E" href="mailto:BANLkTinjZ0CNtchQJCZYhfGSnR51jPuP7w@mail.gmail.com">&lt;BANLkTinjZ0CNtchQJCZYhfGSnR51jPuP7w@mail.gmail.com&gt;</a><br>
      &gt; Content-Type: text/plain; charset=UTF-8<br>
      &gt; <br>
      &gt; Hi,<br>
      &gt; <br>
      &gt; On 19 May 2011 15:54, Zhangping Wei
      <a class="moz-txt-link-rfc2396E" href="mailto:zhangping_wei@yahoo.com">&lt;zhangping_wei@yahoo.com&gt;</a> wrote:<br>
      &gt; &gt; 4, I use command window to run it in this way: ?mpirun
      ?n 4 ?**.exe ?,then I<br>
      &gt; <br>
      &gt; Probably not the problem, but shouldn't that be 'mpirun -np N
      &lt;cmd&gt;' ?<br>
      &gt; <br>
      &gt; Paul<br>
      &gt; <br>
      &gt; -- <br>
      &gt; O&lt; ascii ribbon campaign - stop html mail -
      <a class="moz-txt-link-abbreviated" href="http://www.asciiribbon.org">www.asciiribbon.org</a><br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; ------------------------------<br>
      &gt; <br>
      &gt; Message: 2<br>
      &gt; Date: Thu, 19 May 2011 16:37:56 +0100<br>
      &gt; From: Robert Horton <a class="moz-txt-link-rfc2396E" href="mailto:r.horton@qmul.ac.uk">&lt;r.horton@qmul.ac.uk&gt;</a><br>
      &gt; Subject: Re: [OMPI users] Openib with &gt; 32 cores per node<br>
      &gt; To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a><br>
      &gt; Message-ID: &lt;1305819476.9663.148.camel@moelwyn&gt;<br>
      &gt; Content-Type: text/plain; charset="UTF-8"<br>
      &gt; <br>
      &gt; On Thu, 2011-05-19 at 08:27 -0600, Samuel K. Gutierrez wrote:<br>
      &gt; &gt; Hi,<br>
      &gt; &gt; <br>
      &gt; &gt; Try the following QP parameters that only use shared
      receive queues.<br>
      &gt; &gt; <br>
      &gt; &gt; -mca btl_openib_receive_queues
      S,12288,128,64,32:S,65536,128,64,32<br>
      &gt; &gt; <br>
      &gt; <br>
      &gt; Thanks for that. If I run the job over 2 x 48 cores it now
      works and the<br>
      &gt; performance seems reasonable (I need to do some more tuning)
      but when I<br>
      &gt; go up to 4 x 48 cores I'm getting the same problem:<br>
      &gt; <br>
      &gt;
[compute-1-7.local][[14383,1],86][../../../../../ompi/mca/btl/openib/connect/btl_openib_connect_oob.c:464:qp_create_one]<br>
      &gt; error creating qp errno says Cannot allocate memory<br>
      &gt; [compute-1-7.local:18106] *** An error occurred in MPI_Isend<br>
      &gt; [compute-1-7.local:18106] *** on communicator MPI_COMM_WORLD<br>
      &gt; [compute-1-7.local:18106] *** MPI_ERR_OTHER: known error not
      in list<br>
      &gt; [compute-1-7.local:18106] *** MPI_ERRORS_ARE_FATAL (your MPI
      job will now abort)<br>
      &gt; <br>
      &gt; Any thoughts?<br>
      &gt; <br>
      &gt; Thanks,<br>
      &gt; Rob<br>
      &gt; -- <br>
      &gt; Robert Horton<br>
      &gt; System Administrator (Research Support) - School of
      Mathematical Sciences<br>
      &gt; Queen Mary, University of London<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:r.horton@qmul.ac.uk">r.horton@qmul.ac.uk</a> - +44 (0) 20 7882 7345<br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; ------------------------------<br>
      &gt; <br>
      &gt; Message: 3<br>
      &gt; Date: Thu, 19 May 2011 09:59:13 -0600<br>
      &gt; From: "Samuel K. Gutierrez" <a class="moz-txt-link-rfc2396E" href="mailto:samuel@lanl.gov">&lt;samuel@lanl.gov&gt;</a><br>
      &gt; Subject: Re: [OMPI users] Openib with &gt; 32 cores per node<br>
      &gt; To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a><br>
      &gt; Message-ID:
      <a class="moz-txt-link-rfc2396E" href="mailto:B3E83138-9AF0-48C0-871C-DBBB2E712E12@lanl.gov">&lt;B3E83138-9AF0-48C0-871C-DBBB2E712E12@lanl.gov&gt;</a><br>
      &gt; Content-Type: text/plain; charset=us-ascii<br>
      &gt; <br>
      &gt; Hi,<br>
      &gt; <br>
      &gt; On May 19, 2011, at 9:37 AM, Robert Horton wrote<br>
      &gt; <br>
      &gt; &gt; On Thu, 2011-05-19 at 08:27 -0600, Samuel K. Gutierrez
      wrote:<br>
      &gt; &gt;&gt; Hi,<br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt; Try the following QP parameters that only use shared
      receive queues.<br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt; -mca btl_openib_receive_queues
      S,12288,128,64,32:S,65536,128,64,32<br>
      &gt; &gt;&gt; <br>
      &gt; &gt; <br>
      &gt; &gt; Thanks for that. If I run the job over 2 x 48 cores it
      now works and the<br>
      &gt; &gt; performance seems reasonable (I need to do some more
      tuning) but when I<br>
      &gt; &gt; go up to 4 x 48 cores I'm getting the same problem:<br>
      &gt; &gt; <br>
      &gt;
&gt;[compute-1-7.local][[14383,1],86][../../../../../ompi/mca/btl/openib/connect/btl_openib_connect_oob.c:464:qp_create_one]<br>
      &gt; &gt;] error creating qp errno says Cannot allocate memory<br>
      &gt; &gt; [compute-1-7.local:18106] *** An error occurred in
      MPI_Isend<br>
      &gt; &gt; [compute-1-7.local:18106] *** on communicator
      MPI_COMM_WORLD<br>
      &gt; &gt; [compute-1-7.local:18106] *** MPI_ERR_OTHER: known error
      not in list<br>
      &gt; &gt; [compute-1-7.local:18106] *** MPI_ERRORS_ARE_FATAL (your
      MPI job will now <br>
      &gt; &gt;abort)<br>
      &gt; &gt; <br>
      &gt; &gt; Any thoughts?<br>
      &gt; <br>
      &gt; How much memory does each node have? Does this happen at
      startup?<br>
      &gt; <br>
      &gt; Try adding:<br>
      &gt; <br>
      &gt; -mca btl_openib_cpc_include rdmacm<br>
      &gt; <br>
      &gt; I'm not sure if your version of OFED supports this feature,
      but maybe using XRC <br>
      &gt; may help. I **think** other tweaks are needed to get this
      going, but I'm not <br>
      &gt; familiar with the details.<br>
      &gt; <br>
      &gt; Hope that helps,<br>
      &gt; <br>
      &gt; Samuel K. Gutierrez<br>
      &gt; Los Alamos National Laboratory<br>
      &gt; <br>
      &gt; <br>
      &gt; &gt; <br>
      &gt; &gt; Thanks,<br>
      &gt; &gt; Rob<br>
      &gt; &gt; -- <br>
      &gt; &gt; Robert Horton<br>
      &gt; &gt; System Administrator (Research Support) - School of
      Mathematical Sciences<br>
      &gt; &gt; Queen Mary, University of London<br>
      &gt; &gt; <a class="moz-txt-link-abbreviated" href="mailto:r.horton@qmul.ac.uk">r.horton@qmul.ac.uk</a> - +44 (0) 20 7882 7345<br>
      &gt; &gt; <br>
      &gt; &gt; _______________________________________________<br>
      &gt; &gt; users mailing list<br>
      &gt; &gt; <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; &gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; ------------------------------<br>
      &gt; <br>
      &gt; _______________________________________________<br>
      &gt; users mailing list<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      &gt; <br>
      &gt; End of users Digest, Vol 1910, Issue 2<br>
      &gt; **************************************<br>
      &gt; -------------- next part --------------<br>
      &gt; HTML attachment scrubbed and removed<br>
      &gt; <br>
      &gt; ------------------------------<br>
      &gt; <br>
      &gt; Message: 2<br>
      &gt; Date: Thu, 19 May 2011 08:48:03 -0800<br>
      &gt; From: George Bosilca <a class="moz-txt-link-rfc2396E" href="mailto:bosilca@eecs.utk.edu">&lt;bosilca@eecs.utk.edu&gt;</a><br>
      &gt; Subject: Re: [OMPI users] Problem with MPI_Request,
      MPI_Isend/recv and<br>
      &gt; MPI_Wait/Test<br>
      &gt; To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a><br>
      &gt; Message-ID:
      <a class="moz-txt-link-rfc2396E" href="mailto:FCAC66F9-FDB5-48BB-A800-263D8A4F9337@eecs.utk.edu">&lt;FCAC66F9-FDB5-48BB-A800-263D8A4F9337@eecs.utk.edu&gt;</a><br>
      &gt; Content-Type: text/plain; charset=iso-8859-1<br>
      &gt; <br>
      &gt; David,<br>
      &gt; <br>
      &gt; I do not see any mechanism for protecting the accesses to the
      requests to a single thread? What is the thread model you're
      using?<br>
      &gt; <br>
      &gt; &gt;From an implementation perspective, your code is correct
      only if you initialize the MPI library with MPI_THREAD_MULTIPLE
      and if the library accepts. Otherwise, there is an assumption that
      the application is single threaded, or that the MPI behavior is
      implementation dependent. Please read the MPI standard regarding
      to MPI_Init_thread for more details.<br>
      &gt; <br>
      &gt; Regards,<br>
      &gt; george.<br>
      &gt; <br>
      &gt; On May 19, 2011, at 02:34 , David B?ttner wrote:<br>
      &gt; <br>
      &gt; &gt; Hello,<br>
      &gt; &gt; <br>
      &gt; &gt; I am working on a hybrid MPI (OpenMPI 1.4.3) and Pthread
      code. I am using MPI_Isend and MPI_Irecv for communication and
      MPI_Test/MPI_Wait to check if it is done. I do this repeatedly in
      the outer loop of my code. The MPI_Test is used in the inner loop
      to check if some function can be called which depends on the
      received data.<br>
      &gt; &gt; The program regularly crashed (only when not using
      printf...) and after debugging it I figured out the following
      problem:<br>
      &gt; &gt; <br>
      &gt; &gt; In MPI_Isend I have an invalid read of memory. I fixed
      the problem with not re-using a<br>
      &gt; &gt; <br>
      &gt; &gt; MPI_Request req_s, req_r;<br>
      &gt; &gt; <br>
      &gt; &gt; but by using<br>
      &gt; &gt; <br>
      &gt; &gt; MPI_Request* req_s;<br>
      &gt; &gt; MPI_Request* req_r<br>
      &gt; &gt; <br>
      &gt; &gt; and re-allocating them before the MPI_Isend/recv.<br>
      &gt; &gt; <br>
      &gt; &gt; The documentation says, that in MPI_Wait and MPI_Test
      (if successful) the request-objects are deallocated and set to
      MPI_REQUEST_NULL.<br>
      &gt; &gt; It also says, that in MPI_Isend and MPI_Irecv, it
      allocates the Objects and associates it with the request object.<br>
      &gt; &gt; <br>
      &gt; &gt; As I understand this, this either means I can use a
      pointer to MPI_Request which I don't have to initialize for this
      (it doesn't work but crashes), or that I can use a MPI_Request
      pointer which I have initialized with malloc(sizeof(MPI_REQUEST))
      (or passing the address of a MPI_Request req), which is set and
      unset in the functions. But this version crashes, too.<br>
      &gt; &gt; What works is using a pointer, which I allocate before
      the MPI_Isend/recv and which I free after MPI_Wait in every
      iteration. In other words: It only uses if I don't reuse any kind
      of MPI_Request. Only if I recreate one every time.<br>
      &gt; &gt; <br>
      &gt; &gt; Is this, what is should be like? I believe that a reuse
      of the memory would be a lot more efficient (less calls to
      malloc...). Am I missing something here? Or am I doing something
      wrong?<br>
      &gt; &gt; <br>
      &gt; &gt; <br>
      &gt; &gt; Let me provide some more detailed information about my
      problem:<br>
      &gt; &gt; <br>
      &gt; &gt; I am running the program on a 30 node infiniband
      cluster. Each node has 4 single core Opteron CPUs. I am running 1
      MPI Rank per node and 4 threads per rank (-&gt; one thread per
      core).<br>
      &gt; &gt; I am compiling with mpicc of OpenMPI using gcc below.<br>
      &gt; &gt; Some pseudo-code of the program can be found at the end
      of this e-mail.<br>
      &gt; &gt; <br>
      &gt; &gt; I was able to reproduce the problem using different
      amount of nodes and even using one node only. The problem does not
      arise when I put printf-debugging information into the code. This
      pointed me into the direction that I have some memory problem,
      where some write accesses some memory it is not supposed to.<br>
      &gt; &gt; I ran the tests using valgrind with --leak-check=full
      and --show-reachable=yes, which pointed me either to MPI_Isend or
      MPI_Wait depending on whether I had the threads spin in a loop for
      MPI_Test to return success or used MPI_Wait respectively.<br>
      &gt; &gt; <br>
      &gt; &gt; I would appreciate your help with this. Am I missing
      something important here? Is there a way to re-use the request in
      the different iterations other than I thought it should work?<br>
      &gt; &gt; Or is there a way to re-initialize the allocated memory
      before the MPI_Isend/recv so that I at least don't have to call
      free and malloc each time?<br>
      &gt; &gt; <br>
      &gt; &gt; Thank you very much for your help!<br>
      &gt; &gt; Kind regards,<br>
      &gt; &gt; David B?ttner<br>
      &gt; &gt; <br>
      &gt; &gt; _____________________<br>
      &gt; &gt; Pseudo-Code of program:<br>
      &gt; &gt; <br>
      &gt; &gt; MPI_Request* req_s;<br>
      &gt; &gt; MPI_Request* req_w;<br>
      &gt; &gt; OUTER-LOOP<br>
      &gt; &gt; if(0 == threadid)<br>
      &gt; &gt; {<br>
      &gt; &gt; req_s = malloc(sizeof(MPI_Request));<br>
      &gt; &gt; req_r = malloc(sizeof(MPI_Request));<br>
      &gt; &gt; MPI_Isend(..., req_s)<br>
      &gt; &gt; MPI_Irecv(..., req_r)<br>
      &gt; &gt; }<br>
      &gt; &gt; pthread_barrier<br>
      &gt; &gt; INNER-LOOP (while NOT_DONE or RET)<br>
      &gt; &gt; if(TRYLOCK &amp;&amp; NOT_DONE)<br>
      &gt; &gt; {<br>
      &gt; &gt; if(MPI_TEST(req_r))<br>
      &gt; &gt; {<br>
      &gt; &gt; Call_Function_A;<br>
      &gt; &gt; NOT_DONE = 0;<br>
      &gt; &gt; }<br>
      &gt; &gt; <br>
      &gt; &gt; }<br>
      &gt; &gt; RET = Call_Function_B;<br>
      &gt; &gt; }<br>
      &gt; &gt; pthread_barrier_wait<br>
      &gt; &gt; if(0 == threadid)<br>
      &gt; &gt; {<br>
      &gt; &gt; MPI_WAIT(req_s)<br>
      &gt; &gt; MPI_WAIT(req_r)<br>
      &gt; &gt; free(req_s);<br>
      &gt; &gt; free(req_r);<br>
      &gt; &gt; }<br>
      &gt; &gt; _____________<br>
      &gt; &gt; <br>
      &gt; &gt; <br>
      &gt; &gt; -- <br>
      &gt; &gt; David B?ttner, Informatik, Technische Universit?t
      M?nchen<br>
      &gt; &gt; TUM I-10 - FMI 01.06.059 - Tel. 089 / 289-17676<br>
      &gt; &gt; <br>
      &gt; &gt; _______________________________________________<br>
      &gt; &gt; users mailing list<br>
      &gt; &gt; <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; &gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      &gt; <br>
      &gt; "To preserve the freedom of the human mind then and freedom
      of the press, every spirit should be ready to devote itself to
      martyrdom; for as long as we may think as we will, and speak as we
      think, the condition of man will proceed in improvement."<br>
      &gt; -- Thomas Jefferson, 1799<br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; ------------------------------<br>
      &gt; <br>
      &gt; Message: 3<br>
      &gt; Date: Thu, 19 May 2011 21:22:48 -0400<br>
      &gt; From: Jeff Squyres <a class="moz-txt-link-rfc2396E" href="mailto:jsquyres@cisco.com">&lt;jsquyres@cisco.com&gt;</a><br>
      &gt; Subject: Re: [OMPI users] v1.5.3-x64 does not work on Windows
      7<br>
      &gt; workgroup<br>
      &gt; To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a><br>
      &gt; Message-ID:
      <a class="moz-txt-link-rfc2396E" href="mailto:278274F0-BF00-4498-950F-9779E0083C5A@cisco.com">&lt;278274F0-BF00-4498-950F-9779E0083C5A@cisco.com&gt;</a><br>
      &gt; Content-Type: text/plain; charset=us-ascii<br>
      &gt; <br>
      &gt; Unfortunately, our Windows guy (Shiqing) is off getting
      married and will be out for a little while. :-(<br>
      &gt; <br>
      &gt; All that I can cite is the README.WINDOWS.txt file in the
      top-level directory. I'm afraid that I don't know much else about
      Windows. :-(<br>
      &gt; <br>
      &gt; <br>
      &gt; On May 18, 2011, at 8:17 PM, Jason Mackay wrote:<br>
      &gt; <br>
      &gt; &gt; Hi all,<br>
      &gt; &gt; <br>
      &gt; &gt; My thanks to all those involved for putting together
      this Windows binary release of OpenMPI! I am hoping to use it in a
      small Windows based OpenMPI cluster at home.<br>
      &gt; &gt; <br>
      &gt; &gt; Unfortunately my experience so far has not exactly been
      trouble free. It seems that, due to the fact that this release is
      using WMI, there are a number of settings that must be configured
      on the machines in order to get this to work. These settings are
      not documented in the distribution at all. I have been
      experimenting with it for over a week on and off and as soon as I
      solve one problem, another one arises.<br>
      &gt; &gt; <br>
      &gt; &gt; Currently, after much searching, reading, and tinkering
      with DCOM settings etc..., I can remotely start processes on all
      my machines using mpirun but those processes cannot access network
      shares (e.g. for binary distribution) and HPL (which works on any
      one node) does not seem to work if I run it across multiple nodes,
      also indicating a network issue (CPU sits at 100% in all processes
      with no network traffic and never terminates). To eliminate
      premission issues that may be caused by UAC I tried the same setup
      on two domain machines using an administrative account to launch
      and the behavior was the same. I have read that WMI processes
      cannot access network resources and I am at a loss for a solution
      to this newest of problems. If anyone knows how to make this work
      I would appreciate the help. I assume that someone has gotten this
      working and has the answers.<br>
      &gt; &gt; <br>
      &gt; &gt; I have searched the mailing list archives and I found
      other users with similar problems but no clear guidance on the
      threads. Some threads make references to Microsoft KB articles but
      do not explicitly tell the user what needs to be done, leaving
      each new user to rediscover the tricks on their own. One thread
      made it appear that testing had only been done on Windows XP.
      Needless to say, security has changed dramatically in Windows
      since XP!<br>
      &gt; &gt; <br>
      &gt; &gt; I would like to see OpenMPI for Windows be usable by a
      newcomer without all of this pain.<br>
      &gt; &gt; <br>
      &gt; &gt; What would be fantastic would be:<br>
      &gt; &gt; 1) a step-by-step procedure for how to get OpenMPI 1.5
      working on Windows<br>
      &gt; &gt; a) preferably in a bare Windows 7 workgroup environment
      with nothing else (i.e. no Microsoft Cluster Compute Pack, no
      domain etc...)<br>
      &gt; &gt; 2) inclusion of these steps in the binary distribution<br>
      &gt; &gt; 3) bonus points for a script which accomplishes these
      things automatically<br>
      &gt; &gt; <br>
      &gt; &gt; If someone can help with (1), I would happily volunteer
      my time to work on (3).<br>
      &gt; &gt; <br>
      &gt; &gt; Regards,<br>
      &gt; &gt; Jason<br>
      &gt; &gt; _______________________________________________<br>
      &gt; &gt; users mailing list<br>
      &gt; &gt; <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; &gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      &gt; <br>
      &gt; <br>
      &gt; -- <br>
      &gt; Jeff Squyres<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
      &gt; For corporate legal information go to:<br>
      &gt; <a class="moz-txt-link-freetext" href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; ------------------------------<br>
      &gt; <br>
      &gt; Message: 4<br>
      &gt; Date: Thu, 19 May 2011 21:26:43 -0400<br>
      &gt; From: Jeff Squyres <a class="moz-txt-link-rfc2396E" href="mailto:jsquyres@cisco.com">&lt;jsquyres@cisco.com&gt;</a><br>
      &gt; Subject: Re: [OMPI users] Error: Entry Point Not Found<br>
      &gt; To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a><br>
      &gt; Message-ID:
      <a class="moz-txt-link-rfc2396E" href="mailto:F830EC35-FC9B-4801-B2A3-50F54D2152A4@cisco.com">&lt;F830EC35-FC9B-4801-B2A3-50F54D2152A4@cisco.com&gt;</a><br>
      &gt; Content-Type: text/plain; charset=windows-1252<br>
      &gt; <br>
      &gt; On May 19, 2011, at 10:54 AM, Zhangping Wei wrote:<br>
      &gt; <br>
      &gt; &gt; 4, I use command window to run it in this way: ?mpirun
      ?n 4 **.exe ?,then I met the error: ?entry point not found: the
      procedure entry point inet_pton could not be located in the
      dynamic link library WS2_32.dll?<br>
      &gt; <br>
      &gt; Unfortunately our Windows developer/maintainer is out for a
      little while (he's getting married); he pretty much did the
      Windows stuff by himself, so none of the rest of us know much
      about it. :(<br>
      &gt; <br>
      &gt; inet_pton is a standard function call relating to IP
      addresses that we use in the internals of OMPI; I'm not sure why
      it wouldn't be found on Windows XP (Shiqing did cite that the OMPI
      Windows port should work on Windows XP). <br>
      &gt; <br>
      &gt; This post seems to imply that inet_ntop is only available on
      Vista and above:<br>
      &gt; <br>
      &gt;
<a class="moz-txt-link-freetext" href="http://social.msdn.microsoft.com/Forums/en-US/vcgeneral/thread/e40465f2-41b7-4243-ad33-15ae9366f4e6/">http://social.msdn.microsoft.com/Forums/en-US/vcgeneral/thread/e40465f2-41b7-4243-ad33-15ae9366f4e6/</a><br>
      &gt; <br>
      &gt; So perhaps Shiqing needs to put in some kind of portability
      workaround for OMPI, and the current binaries won't actually work
      for XP...?<br>
      &gt; <br>
      &gt; I can't say that for sure because I really know very little
      about Windows; we'll unfortunately have to wait until he returns
      to get a definitive answer. :-(<br>
      &gt; <br>
      &gt; -- <br>
      &gt; Jeff Squyres<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
      &gt; For corporate legal information go to:<br>
      &gt; <a class="moz-txt-link-freetext" href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; ------------------------------<br>
      &gt; <br>
      &gt; Message: 5<br>
      &gt; Date: Thu, 19 May 2011 21:37:49 -0400<br>
      &gt; From: Jeff Squyres <a class="moz-txt-link-rfc2396E" href="mailto:jsquyres@cisco.com">&lt;jsquyres@cisco.com&gt;</a><br>
      &gt; Subject: Re: [OMPI users] openmpi (1.2.8 or above) and Intel
      composer<br>
      &gt; XE 2011 (aka 12.0)<br>
      &gt; To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a><br>
      &gt; Cc: Giovanni Bracco <a class="moz-txt-link-rfc2396E" href="mailto:giovanni.bracco@enea.it">&lt;giovanni.bracco@enea.it&gt;</a>, Agostino
      Funel<br>
      &gt; <a class="moz-txt-link-rfc2396E" href="mailto:agostino.funel@enea.it">&lt;agostino.funel@enea.it&gt;</a>, Fiorenzo Ambrosino<br>
      &gt; <a class="moz-txt-link-rfc2396E" href="mailto:fiorenzo.ambrosino@enea.it">&lt;fiorenzo.ambrosino@enea.it&gt;</a>, Guido Guarnieri<br>
      &gt; <a class="moz-txt-link-rfc2396E" href="mailto:guido.guarnieri@enea.it">&lt;guido.guarnieri@enea.it&gt;</a>, Roberto Ciavarella<br>
      &gt; <a class="moz-txt-link-rfc2396E" href="mailto:roberto.ciavarella@enea.it">&lt;roberto.ciavarella@enea.it&gt;</a>, Salvatore Podda<br>
      &gt; <a class="moz-txt-link-rfc2396E" href="mailto:salvatore.podda@enea.it">&lt;salvatore.podda@enea.it&gt;</a>, Giovanni Ponti
      <a class="moz-txt-link-rfc2396E" href="mailto:giovanni.ponti@enea.it">&lt;giovanni.ponti@enea.it&gt;</a><br>
      &gt; Message-ID:
      <a class="moz-txt-link-rfc2396E" href="mailto:45362608-B8B0-4ADE-9959-B35C5690A6F3@cisco.com">&lt;45362608-B8B0-4ADE-9959-B35C5690A6F3@cisco.com&gt;</a><br>
      &gt; Content-Type: text/plain; charset=us-ascii<br>
      &gt; <br>
      &gt; Sorry for the late reply.<br>
      &gt; <br>
      &gt; Other users have seen something similar but we have never
      been able to reproduce it. Is this only when using IB? If you use
      "mpirun --mca btl_openib_cpc_if_include rdmacm", does the problem
      go away?<br>
      &gt; <br>
      &gt; <br>
      &gt; On May 11, 2011, at 6:00 PM, Marcus R. Epperson wrote:<br>
      &gt; <br>
      &gt; &gt; I've seen the same thing when I build openmpi 1.4.3 with
      Intel 12, but only when I have -O2 or -O3 in CFLAGS. If I drop it
      down to -O1 then the collectives hangs go away. I don't know what,
      if anything, the higher optimization buys you when compiling
      openmpi, so I'm not sure if that's an acceptable workaround or
      not.<br>
      &gt; &gt; <br>
      &gt; &gt; My system is similar to yours - Intel X5570 with QDR
      Mellanox IB running RHEL 5, Slurm, and these openmpi btls:
      openib,sm,self. I'm using IMB 3.2.2 with a single iteration of
      Barrier to reproduce the hang, and it happens 100% of the time for
      me when I invoke it like this:<br>
      &gt; &gt; <br>
      &gt; &gt; # salloc -N 9 orterun -n 65 ./IMB-MPI1 -npmin 64 -iter 1
      barrier<br>
      &gt; &gt; <br>
      &gt; &gt; The hang happens on the first Barrier (64 ranks) and
      each of the participating ranks have this backtrace:<br>
      &gt; &gt; <br>
      &gt; &gt; __poll (...)<br>
      &gt; &gt; poll_dispatch () from [instdir]/lib/libopen-pal.so.0<br>
      &gt; &gt; opal_event_loop () from [instdir]/lib/libopen-pal.so.0<br>
      &gt; &gt; opal_progress () from [instdir]/lib/libopen-pal.so.0<br>
      &gt; &gt; ompi_request_default_wait_all () from
      [instdir]/lib/libmpi.so.0<br>
      &gt; &gt; ompi_coll_tuned_sendrecv_actual () from
      [instdir]/lib/libmpi.so.0<br>
      &gt; &gt; ompi_coll_tuned_barrier_intra_recursivedoubling () from
      [instdir]/lib/libmpi.so.0<br>
      &gt; &gt; ompi_coll_tuned_barrier_intra_dec_fixed () from
      [instdir]/lib/libmpi.so.0<br>
      &gt; &gt; PMPI_Barrier () from [instdir]/lib/libmpi.so.0<br>
      &gt; &gt; IMB_barrier ()<br>
      &gt; &gt; IMB_init_buffers_iter ()<br>
      &gt; &gt; main ()<br>
      &gt; &gt; <br>
      &gt; &gt; The one non-participating rank has this backtrace:<br>
      &gt; &gt; <br>
      &gt; &gt; __poll (...)<br>
      &gt; &gt; poll_dispatch () from [instdir]/lib/libopen-pal.so.0<br>
      &gt; &gt; opal_event_loop () from [instdir]/lib/libopen-pal.so.0<br>
      &gt; &gt; opal_progress () from [instdir]/lib/libopen-pal.so.0<br>
      &gt; &gt; ompi_request_default_wait_all () from
      [instdir]/lib/libmpi.so.0<br>
      &gt; &gt; ompi_coll_tuned_sendrecv_actual () from
      [instdir]/lib/libmpi.so.0<br>
      &gt; &gt; ompi_coll_tuned_barrier_intra_bruck () from
      [instdir]/lib/libmpi.so.0<br>
      &gt; &gt; ompi_coll_tuned_barrier_intra_dec_fixed () from
      [instdir]/lib/libmpi.so.0<br>
      &gt; &gt; PMPI_Barrier () from [instdir]/lib/libmpi.so.0<br>
      &gt; &gt; main ()<br>
      &gt; &gt; <br>
      &gt; &gt; If I use more nodes I can get it to hang with 1ppn, so
      that seems to rule out the sm btl (or interactions with it) as a
      culprit at least.<br>
      &gt; &gt; <br>
      &gt; &gt; I can't reproduce this with openmpi 1.5.3,
      interestingly.<br>
      &gt; &gt; <br>
      &gt; &gt; -Marcus<br>
      &gt; &gt; <br>
      &gt; &gt; <br>
      &gt; &gt; On 05/10/2011 03:37 AM, Salvatore Podda wrote:<br>
      &gt; &gt;&gt; Dear all,<br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt; we succeed in building several version of openmpi
      from 1.2.8 to 1.4.3 <br>
      &gt; &gt;&gt; with Intel composer XE 2011 (aka 12.0).<br>
      &gt; &gt;&gt; However we found a threshold in the number of cores
      (depending from the <br>
      &gt; &gt;&gt; application: IMB, xhpl or user applications<br>
      &gt; &gt;&gt; and form the number of required cores) above which
      the application hangs <br>
      &gt; &gt;&gt; (sort of deadlocks).<br>
      &gt; &gt;&gt; The building of openmpi with 'gcc' and 'pgi' does
      not show the same limits.<br>
      &gt; &gt;&gt; There are any known incompatibilities of openmpi
      with this version of <br>
      &gt; &gt;&gt; intel compiilers?<br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt; The characteristics of our computational
      infrastructure are:<br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt; Intel processors E7330, E5345, E5530 e E5620<br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt; CentOS 5.3, CentOS 5.5.<br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt; Intel composer XE 2011<br>
      &gt; &gt;&gt; gcc 4.1.2<br>
      &gt; &gt;&gt; pgi 10.2-1<br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt; Regards<br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt; Salvatore Podda<br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt; ENEA UTICT-HPC<br>
      &gt; &gt;&gt; Department for Computer Science Development and ICT<br>
      &gt; &gt;&gt; Facilities Laboratory for Science and High
      Performace Computing<br>
      &gt; &gt;&gt; C.R. Frascati<br>
      &gt; &gt;&gt; Via E. Fermi, 45<br>
      &gt; &gt;&gt; PoBox 65<br>
      &gt; &gt;&gt; 00044 Frascati (Rome)<br>
      &gt; &gt;&gt; Italy<br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt; Tel: +39 06 9400 5342<br>
      &gt; &gt;&gt; Fax: +39 06 9400 5551<br>
      &gt; &gt;&gt; Fax: +39 06 9400 5735<br>
      &gt; &gt;&gt; E-mail: <a class="moz-txt-link-abbreviated" href="mailto:salvatore.podda@enea.it">salvatore.podda@enea.it</a><br>
      &gt; &gt;&gt; Home Page: <a class="moz-txt-link-abbreviated" href="http://www.cresco.enea.it">www.cresco.enea.it</a><br>
      &gt; &gt;&gt; _______________________________________________<br>
      &gt; &gt;&gt; users mailing list<br>
      &gt; &gt;&gt; <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; &gt;&gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      &gt; &gt;&gt; <br>
      &gt; &gt; <br>
      &gt; &gt; _______________________________________________<br>
      &gt; &gt; users mailing list<br>
      &gt; &gt; <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; &gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      &gt; <br>
      &gt; <br>
      &gt; -- <br>
      &gt; Jeff Squyres<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
      &gt; For corporate legal information go to:<br>
      &gt; <a class="moz-txt-link-freetext" href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; ------------------------------<br>
      &gt; <br>
      &gt; Message: 6<br>
      &gt; Date: Thu, 19 May 2011 22:01:00 -0400<br>
      &gt; From: Jeff Squyres <a class="moz-txt-link-rfc2396E" href="mailto:jsquyres@cisco.com">&lt;jsquyres@cisco.com&gt;</a><br>
      &gt; Subject: Re: [OMPI users] Openib with &gt; 32 cores per node<br>
      &gt; To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a><br>
      &gt; Message-ID:
      <a class="moz-txt-link-rfc2396E" href="mailto:C18C4827-D305-484A-9DAE-290902D40DB3@cisco.com">&lt;C18C4827-D305-484A-9DAE-290902D40DB3@cisco.com&gt;</a><br>
      &gt; Content-Type: text/plain; charset=us-ascii<br>
      &gt; <br>
      &gt; What Sam is alluding to is that the OpenFabrics driver code
      in OMPI is sucking up oodles of memory for each IB connection that
      you're using. The receive_queues param that he sent tells OMPI to
      use all shared receive queues (instead of defaulting to one
      per-peer receive queue and the rest shared receive queues -- the
      per-peer RQ sucks up all the memory when you multiple it by N
      peers).<br>
      &gt; <br>
      &gt; <br>
      &gt; On May 19, 2011, at 11:59 AM, Samuel K. Gutierrez wrote:<br>
      &gt; <br>
      &gt; &gt; Hi,<br>
      &gt; &gt; <br>
      &gt; &gt; On May 19, 2011, at 9:37 AM, Robert Horton wrote<br>
      &gt; &gt; <br>
      &gt; &gt;&gt; On Thu, 2011-05-19 at 08:27 -0600, Samuel K.
      Gutierrez wrote:<br>
      &gt; &gt;&gt;&gt; Hi,<br>
      &gt; &gt;&gt;&gt; <br>
      &gt; &gt;&gt;&gt; Try the following QP parameters that only use
      shared receive queues.<br>
      &gt; &gt;&gt;&gt; <br>
      &gt; &gt;&gt;&gt; -mca btl_openib_receive_queues
      S,12288,128,64,32:S,65536,128,64,32<br>
      &gt; &gt;&gt;&gt; <br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt; Thanks for that. If I run the job over 2 x 48 cores
      it now works and the<br>
      &gt; &gt;&gt; performance seems reasonable (I need to do some more
      tuning) but when I<br>
      &gt; &gt;&gt; go up to 4 x 48 cores I'm getting the same problem:<br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt;
      [compute-1-7.local][[14383,1],86][../../../../../ompi/mca/btl/openib/connect/btl_openib_connect_oob.c:464:qp_create_one]
      error creating qp errno says Cannot allocate memory<br>
      &gt; &gt;&gt; [compute-1-7.local:18106] *** An error occurred in
      MPI_Isend<br>
      &gt; &gt;&gt; [compute-1-7.local:18106] *** on communicator
      MPI_COMM_WORLD<br>
      &gt; &gt;&gt; [compute-1-7.local:18106] *** MPI_ERR_OTHER: known
      error not in list<br>
      &gt; &gt;&gt; [compute-1-7.local:18106] *** MPI_ERRORS_ARE_FATAL
      (your MPI job will now abort)<br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt; Any thoughts?<br>
      &gt; &gt; <br>
      &gt; &gt; How much memory does each node have? Does this happen at
      startup?<br>
      &gt; &gt; <br>
      &gt; &gt; Try adding:<br>
      &gt; &gt; <br>
      &gt; &gt; -mca btl_openib_cpc_include rdmacm<br>
      &gt; &gt; <br>
      &gt; &gt; I'm not sure if your version of OFED supports this
      feature, but maybe using XRC may help. I **think** other tweaks
      are needed to get this going, but I'm not familiar with the
      details.<br>
      &gt; &gt; <br>
      &gt; &gt; Hope that helps,<br>
      &gt; &gt; <br>
      &gt; &gt; Samuel K. Gutierrez<br>
      &gt; &gt; Los Alamos National Laboratory<br>
      &gt; &gt; <br>
      &gt; &gt; <br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt; Thanks,<br>
      &gt; &gt;&gt; Rob<br>
      &gt; &gt;&gt; -- <br>
      &gt; &gt;&gt; Robert Horton<br>
      &gt; &gt;&gt; System Administrator (Research Support) - School of
      Mathematical Sciences<br>
      &gt; &gt;&gt; Queen Mary, University of London<br>
      &gt; &gt;&gt; <a class="moz-txt-link-abbreviated" href="mailto:r.horton@qmul.ac.uk">r.horton@qmul.ac.uk</a> - +44 (0) 20 7882 7345<br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt; _______________________________________________<br>
      &gt; &gt;&gt; users mailing list<br>
      &gt; &gt;&gt; <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; &gt;&gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      &gt; &gt; <br>
      &gt; &gt; <br>
      &gt; &gt; <br>
      &gt; &gt; <br>
      &gt; &gt; _______________________________________________<br>
      &gt; &gt; users mailing list<br>
      &gt; &gt; <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; &gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      &gt; <br>
      &gt; <br>
      &gt; -- <br>
      &gt; Jeff Squyres<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
      &gt; For corporate legal information go to:<br>
      &gt; <a class="moz-txt-link-freetext" href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; ------------------------------<br>
      &gt; <br>
      &gt; Message: 7<br>
      &gt; Date: Thu, 19 May 2011 22:04:46 -0400<br>
      &gt; From: Jeff Squyres <a class="moz-txt-link-rfc2396E" href="mailto:jsquyres@cisco.com">&lt;jsquyres@cisco.com&gt;</a><br>
      &gt; Subject: Re: [OMPI users] MPI_COMM_DUP freeze with OpenMPI
      1.4.1<br>
      &gt; To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a><br>
      &gt; Message-ID:
      <a class="moz-txt-link-rfc2396E" href="mailto:0DCF20B8-CA5C-4746-8187-A2DFF39B15DD@cisco.com">&lt;0DCF20B8-CA5C-4746-8187-A2DFF39B15DD@cisco.com&gt;</a><br>
      &gt; Content-Type: text/plain; charset=us-ascii<br>
      &gt; <br>
      &gt; On May 13, 2011, at 8:31 AM,
      <a class="moz-txt-link-abbreviated" href="mailto:francoise.roch@obs.ujf-grenoble.fr">francoise.roch@obs.ujf-grenoble.fr</a> wrote:<br>
      &gt; <br>
      &gt; &gt; Here is the MUMPS portion of code (in zmumps_part1.F
      file) where the slaves call MPI_COMM_DUP , id%PAR and MASTER are
      initialized to 0 before :<br>
      &gt; &gt; <br>
      &gt; &gt; CALL MPI_COMM_SIZE(id%COMM, id%NPROCS, IERR )<br>
      &gt; <br>
      &gt; I re-indented so that I could read it better:<br>
      &gt; <br>
      &gt; CALL MPI_COMM_SIZE(id%COMM, id%NPROCS, IERR )<br>
      &gt; IF ( id%PAR .eq. 0 ) THEN<br>
      &gt; IF ( id%MYID .eq. MASTER ) THEN<br>
      &gt; color = MPI_UNDEFINED<br>
      &gt; ELSE<br>
      &gt; color = 0<br>
      &gt; END IF<br>
      &gt; CALL MPI_COMM_SPLIT( id%COMM, color, 0,<br>
      &gt; &amp; id%COMM_NODES, IERR )<br>
      &gt; id%NSLAVES = id%NPROCS - 1<br>
      &gt; ELSE<br>
      &gt; CALL MPI_COMM_DUP( id%COMM, id%COMM_NODES, IERR )<br>
      &gt; id%NSLAVES = id%NPROCS<br>
      &gt; END IF<br>
      &gt; <br>
      &gt; IF (id%PAR .ne. 0 .or. id%MYID .NE. MASTER) THEN<br>
      &gt; CALL MPI_COMM_DUP( id%COMM_NODES, id%COMM_LOAD, IERR<br>
      &gt; ENDIF<br>
      &gt; <br>
      &gt; That doesn't look right -- both MPI_COMM_SPLIT and
      MPI_COMM_DUP are collective, meaning that all processes in the
      communicator must call them. In the first case, only some
      processes are calling MPI_COMM_SPLIT. Is there some other logic
      that forces the rest of the processes to call MPI_COMM_SPLIT, too?<br>
      &gt; <br>
      &gt; -- <br>
      &gt; Jeff Squyres<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
      &gt; For corporate legal information go to:<br>
      &gt; <a class="moz-txt-link-freetext" href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; ------------------------------<br>
      &gt; <br>
      &gt; Message: 8<br>
      &gt; Date: Thu, 19 May 2011 22:30:03 -0400<br>
      &gt; From: Jeff Squyres <a class="moz-txt-link-rfc2396E" href="mailto:jsquyres@cisco.com">&lt;jsquyres@cisco.com&gt;</a><br>
      &gt; Subject: Re: [OMPI users] Trouble with MPI-IO<br>
      &gt; To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a><br>
      &gt; Message-ID:
      <a class="moz-txt-link-rfc2396E" href="mailto:EEFB638F-72F1-4208-8EA2-4F25F610C47B@cisco.com">&lt;EEFB638F-72F1-4208-8EA2-4F25F610C47B@cisco.com&gt;</a><br>
      &gt; Content-Type: text/plain; charset=us-ascii<br>
      &gt; <br>
      &gt; Props for that testio script. I think you win the award for
      "most easy to reproduce test case." :-)<br>
      &gt; <br>
      &gt; I notice that some of the lines went over 72 columns, so I
      renamed the file x.f90 and changed all the comments from "c" to
      "!" and joined the two &amp;-split lines. The error about implicit
      type for lenr went away, but then when I enabled better type
      checking by using "use mpi" instead of "include 'mpif.h'", I got
      the following:<br>
      &gt; <br>
      &gt; x.f90:99.77:<br>
      &gt; <br>
      &gt; call
      mpi_type_indexed(lenij,ijlena,ijdisp,mpi_real,ij_vector_type,ierr)<br>
      &gt; 1 <br>
      &gt; Error: There is no specific subroutine for the generic
      'mpi_type_indexed' at (1)<br>
      &gt; <br>
      &gt; I looked at our mpi F90 module and see the following:<br>
      &gt; <br>
      &gt; interface MPI_Type_indexed<br>
      &gt; subroutine MPI_Type_indexed(count, array_of_blocklengths,
      array_of_displacements, oldtype, newtype, ierr)<br>
      &gt; integer, intent(in) :: count<br>
      &gt; integer, dimension(*), intent(in) :: array_of_blocklengths<br>
      &gt; integer, dimension(*), intent(in) :: array_of_displacements<br>
      &gt; integer, intent(in) :: oldtype<br>
      &gt; integer, intent(out) :: newtype<br>
      &gt; integer, intent(out) :: ierr<br>
      &gt; end subroutine MPI_Type_indexed<br>
      &gt; end interface<br>
      &gt; <br>
      &gt; I don't quite grok the syntax of the "allocatable" type
      ijdisp, so that might be the problem here...?<br>
      &gt; <br>
      &gt; Regardless, I'm not entirely sure if the problem is the
      &gt;72 character lines, but then when that is gone, I'm not sure
      how the allocatable stuff fits in... (I'm not enough of a Fortran
      programmer to know)<br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; On May 10, 2011, at 7:14 PM, Tom Rosmond wrote:<br>
      &gt; <br>
      &gt; &gt; I would appreciate someone with experience with MPI-IO
      look at the<br>
      &gt; &gt; simple fortran program gzipped and attached to this
      note. It is<br>
      &gt; &gt; imbedded in a script so that all that is necessary to
      run it is do:<br>
      &gt; &gt; 'testio' from the command line. The program generates a
      small 2-D input<br>
      &gt; &gt; array, sets up an MPI-IO environment, and write a 2-D
      output array<br>
      &gt; &gt; twice, with the only difference being the displacement
      arrays used to<br>
      &gt; &gt; construct the indexed datatype. For the first write,
      simple<br>
      &gt; &gt; monotonically increasing displacements are used, for the
      second the<br>
      &gt; &gt; displacements are 'shuffled' in one dimension. They are
      printed during<br>
      &gt; &gt; the run.<br>
      &gt; &gt; <br>
      &gt; &gt; For the first case the file is written properly, but for
      the second the<br>
      &gt; &gt; program hangs on MPI_FILE_WRITE_AT_ALL and must be
      aborted manually.<br>
      &gt; &gt; Although the program is compiled as an mpi program, I am
      running on a<br>
      &gt; &gt; single processor, which makes the problem more puzzling.<br>
      &gt; &gt; <br>
      &gt; &gt; The program should be relatively self-explanatory, but
      if more<br>
      &gt; &gt; information is needed, please ask. I am on an 8 core
      Xeon based Dell<br>
      &gt; &gt; workstation running Scientific Linux 5.5, Intel fortran
      12.0.3, and<br>
      &gt; &gt; OpenMPI 1.5.3. I have also attached output from
      'ompi_info'.<br>
      &gt; &gt; <br>
      &gt; &gt; T. Rosmond<br>
      &gt; &gt; <br>
      &gt; &gt; <br>
      &gt; &gt;
&lt;testio.gz&gt;&lt;info_ompi.gz&gt;_______________________________________________<br>
      &gt; &gt; users mailing list<br>
      &gt; &gt; <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; &gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      &gt; <br>
      &gt; <br>
      &gt; -- <br>
      &gt; Jeff Squyres<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
      &gt; For corporate legal information go to:<br>
      &gt; <a class="moz-txt-link-freetext" href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; ------------------------------<br>
      &gt; <br>
      &gt; Message: 9<br>
      &gt; Date: Thu, 19 May 2011 20:24:25 -0700<br>
      &gt; From: Tom Rosmond <a class="moz-txt-link-rfc2396E" href="mailto:rosmond@reachone.com">&lt;rosmond@reachone.com&gt;</a><br>
      &gt; Subject: Re: [OMPI users] Trouble with MPI-IO<br>
      &gt; To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a><br>
      &gt; Message-ID:
      <a class="moz-txt-link-rfc2396E" href="mailto:1305861865.4284.104.camel@cedar.reachone.com">&lt;1305861865.4284.104.camel@cedar.reachone.com&gt;</a><br>
      &gt; Content-Type: text/plain<br>
      &gt; <br>
      &gt; Thanks for looking at my problem. Sounds like you did
      reproduce my<br>
      &gt; problem. I have added some comments below<br>
      &gt; <br>
      &gt; On Thu, 2011-05-19 at 22:30 -0400, Jeff Squyres wrote:<br>
      &gt; &gt; Props for that testio script. I think you win the award
      for "most easy to reproduce test case." :-)<br>
      &gt; &gt; <br>
      &gt; &gt; I notice that some of the lines went over 72 columns, so
      I renamed the file x.f90 and changed all the comments from "c" to
      "!" and joined the two &amp;-split lines. The error about implicit
      type for lenr went away, but then when I enabled better type
      checking by using "use mpi" instead of "include 'mpif.h'", I got
      the following:<br>
      &gt; <br>
      &gt; What fortran compiler did you use?<br>
      &gt; <br>
      &gt; In the original script my Intel compile used the -132 option,
      <br>
      &gt; allowing up to that many columns per line. I still think in<br>
      &gt; F77 fortran much of the time, and use 'c' for comments out<br>
      &gt; of habit. The change to '!' doesn't make any difference.<br>
      &gt; <br>
      &gt; <br>
      &gt; &gt; x.f90:99.77:<br>
      &gt; &gt; <br>
      &gt; &gt; call
      mpi_type_indexed(lenij,ijlena,ijdisp,mpi_real,ij_vector_type,ierr)<br>
      &gt; &gt; 1 <br>
      &gt; &gt; Error: There is no specific subroutine for the generic
      'mpi_type_indexed' at (1)<br>
      &gt; <br>
      &gt; Hmmm, very strange, since I am looking right at the MPI
      standard<br>
      &gt; documents with that routine documented. I too get this
      compile failure<br>
      &gt; when I switch to 'use mpi'. Could that be a problem with the
      Open MPI<br>
      &gt; fortran libraries???<br>
      &gt; &gt; <br>
      &gt; &gt; I looked at our mpi F90 module and see the following:<br>
      &gt; &gt; <br>
      &gt; &gt; interface MPI_Type_indexed<br>
      &gt; &gt; subroutine MPI_Type_indexed(count,
      array_of_blocklengths, array_of_displacements, oldtype, newtype,
      ierr)<br>
      &gt; &gt; integer, intent(in) :: count<br>
      &gt; &gt; integer, dimension(*), intent(in) ::
      array_of_blocklengths<br>
      &gt; &gt; integer, dimension(*), intent(in) ::
      array_of_displacements<br>
      &gt; &gt; integer, intent(in) :: oldtype<br>
      &gt; &gt; integer, intent(out) :: newtype<br>
      &gt; &gt; integer, intent(out) :: ierr<br>
      &gt; &gt; end subroutine MPI_Type_indexed<br>
      &gt; &gt; end interface<br>
      &gt; &gt; <br>
      &gt; &gt; I don't quite grok the syntax of the "allocatable" type
      ijdisp, so that might be the problem here...?<br>
      &gt; <br>
      &gt; Just a standard F90 'allocatable' statement. I've written
      thousands<br>
      &gt; just like it.<br>
      &gt; &gt; <br>
      &gt; &gt; Regardless, I'm not entirely sure if the problem is the
      &gt;72 character lines, but then when that is gone, I'm not sure
      how the allocatable stuff fits in... (I'm not enough of a Fortran
      programmer to know)<br>
      &gt; &gt; <br>
      &gt; Anyone else out that who can comment????<br>
      &gt; <br>
      &gt; <br>
      &gt; T. Rosmond<br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; &gt; <br>
      &gt; &gt; On May 10, 2011, at 7:14 PM, Tom Rosmond wrote:<br>
      &gt; &gt; <br>
      &gt; &gt; &gt; I would appreciate someone with experience with
      MPI-IO look at the<br>
      &gt; &gt; &gt; simple fortran program gzipped and attached to this
      note. It is<br>
      &gt; &gt; &gt; imbedded in a script so that all that is necessary
      to run it is do:<br>
      &gt; &gt; &gt; 'testio' from the command line. The program
      generates a small 2-D input<br>
      &gt; &gt; &gt; array, sets up an MPI-IO environment, and write a
      2-D output array<br>
      &gt; &gt; &gt; twice, with the only difference being the
      displacement arrays used to<br>
      &gt; &gt; &gt; construct the indexed datatype. For the first
      write, simple<br>
      &gt; &gt; &gt; monotonically increasing displacements are used,
      for the second the<br>
      &gt; &gt; &gt; displacements are 'shuffled' in one dimension. They
      are printed during<br>
      &gt; &gt; &gt; the run.<br>
      &gt; &gt; &gt; <br>
      &gt; &gt; &gt; For the first case the file is written properly,
      but for the second the<br>
      &gt; &gt; &gt; program hangs on MPI_FILE_WRITE_AT_ALL and must be
      aborted manually.<br>
      &gt; &gt; &gt; Although the program is compiled as an mpi program,
      I am running on a<br>
      &gt; &gt; &gt; single processor, which makes the problem more
      puzzling.<br>
      &gt; &gt; &gt; <br>
      &gt; &gt; &gt; The program should be relatively self-explanatory,
      but if more<br>
      &gt; &gt; &gt; information is needed, please ask. I am on an 8
      core Xeon based Dell<br>
      &gt; &gt; &gt; workstation running Scientific Linux 5.5, Intel
      fortran 12.0.3, and<br>
      &gt; &gt; &gt; OpenMPI 1.5.3. I have also attached output from
      'ompi_info'.<br>
      &gt; &gt; &gt; <br>
      &gt; &gt; &gt; T. Rosmond<br>
      &gt; &gt; &gt; <br>
      &gt; &gt; &gt; <br>
      &gt; &gt; &gt;
&lt;testio.gz&gt;&lt;info_ompi.gz&gt;_______________________________________________<br>
      &gt; &gt; &gt; users mailing list<br>
      &gt; &gt; &gt; <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; &gt; &gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      &gt; &gt; <br>
      &gt; &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; ------------------------------<br>
      &gt; <br>
      &gt; Message: 10<br>
      &gt; Date: Fri, 20 May 2011 09:25:14 +0200<br>
      &gt; From: David B?ttner <a class="moz-txt-link-rfc2396E" href="mailto:david.buettner@in.tum.de">&lt;david.buettner@in.tum.de&gt;</a><br>
      &gt; Subject: Re: [OMPI users] Problem with MPI_Request,
      MPI_Isend/recv and<br>
      &gt; MPI_Wait/Test<br>
      &gt; To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a><br>
      &gt; Message-ID: <a class="moz-txt-link-rfc2396E" href="mailto:4DD6175A.1080403@in.tum.de">&lt;4DD6175A.1080403@in.tum.de&gt;</a><br>
      &gt; Content-Type: text/plain; charset=ISO-8859-1; format=flowed<br>
      &gt; <br>
      &gt; Hello,<br>
      &gt; <br>
      &gt; thanks for the quick answer. I am sorry that I forgot to
      mention this: I <br>
      &gt; did compile OpenMPI with MPI_THREAD_MULTIPLE support and test
      if <br>
      &gt; required == provided after the MPI_Thread_init call.<br>
      &gt; <br>
      &gt; &gt; I do not see any mechanism for protecting the accesses
      to the requests to a single thread? What is the thread model
      you're using?<br>
      &gt; &gt;<br>
      &gt; Again I am sorry that this was not clear: In the pseudo code
      below I <br>
      &gt; wanted to indicate the access-protection I do by thread-id
      dependent <br>
      &gt; calls if(0 == thread-id) and by using the trylock(...) (using
      <br>
      &gt; pthread-mutexes). In the code all accesses concerning one
      MPI_Request <br>
      &gt; (which are pthread-global-pointers in my case) are protected
      and called <br>
      &gt; in sequential order, i.e. MPI_Isend/recv is returns before
      any thread is <br>
      &gt; allowed to call the corresponding MPI_Test and no-one can
      call MPI_Test <br>
      &gt; any more when a thread is allowed to call MPI_Wait.<br>
      &gt; I did this in the same manner before with other MPI
      implementations, but <br>
      &gt; also on the same machine with the same (untouched) OpenMPI <br>
      &gt; implementation, also using pthreads and MPI in combination,
      but I used<br>
      &gt; <br>
      &gt; MPI_Request req;<br>
      &gt; <br>
      &gt; instead of<br>
      &gt; <br>
      &gt; MPI_Request* req;<br>
      &gt; (and later)<br>
      &gt; req = (MPI_Request*)malloc(sizeof(MPI_Request));<br>
      &gt; <br>
      &gt; <br>
      &gt; In my recent (problem) code, I also tried not using pointers,
      but got <br>
      &gt; the same problem. Also, as I described in the first mail, I
      tried <br>
      &gt; everything concerning the memory allocation of the
      MPI_Request objects.<br>
      &gt; I tried not calling malloc. This I guessed wouldn't work, but
      the <br>
      &gt; OpenMPI documentation says this:<br>
      &gt; <br>
      &gt; " Nonblocking calls allocate a communication request object
      and <br>
      &gt; associate it with the request handle the argument request). "
      <br>
      &gt; [<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/doc/v1.4/man3/MPI_Isend.3.php">http://www.open-mpi.org/doc/v1.4/man3/MPI_Isend.3.php</a>] and<br>
      &gt; <br>
      &gt; " [...] if the communication object was created by a
      nonblocking send or <br>
      &gt; receive, then it is deallocated and the request handle is set
      to <br>
      &gt; MPI_REQUEST_NULL." <br>
      &gt; [<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/doc/v1.4/man3/MPI_Test.3.php">http://www.open-mpi.org/doc/v1.4/man3/MPI_Test.3.php</a>] and
      (in slightly <br>
      &gt; different words)
      [<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/doc/v1.4/man3/MPI_Wait.3.php">http://www.open-mpi.org/doc/v1.4/man3/MPI_Wait.3.php</a>]<br>
      &gt; <br>
      &gt; So I thought that it might do some kind of optimized memory
      stuff <br>
      &gt; internally.<br>
      &gt; <br>
      &gt; I also tried allocating req (for each used MPI_Request) once
      before the <br>
      &gt; first use and deallocation after the last use (which I
      thought was the <br>
      &gt; way it was supposed to work), but that crashes also.<br>
      &gt; <br>
      &gt; I tried replacing the pointers through global variables<br>
      &gt; <br>
      &gt; MPI_Request req;<br>
      &gt; <br>
      &gt; which didn't do the job...<br>
      &gt; <br>
      &gt; The only thing that seems to work is what I mentioned below:
      Allocate <br>
      &gt; every time I am going to need it in the MPI_Isend/recv, use
      it in <br>
      &gt; MPI_Test/Wait and after that deallocate it by hand each time.<br>
      &gt; I don't think that this is supposed to be like this since I
      have to do a <br>
      &gt; call to malloc and free so often (for multiple MPI_Request
      objects in <br>
      &gt; each iteration) that it will most likely limit performance...<br>
      &gt; <br>
      &gt; Anyway I still have the same problem and am still unclear on
      what kind <br>
      &gt; of memory allocation I should be doing for the MPI_Requests.
      Is there <br>
      &gt; anything else (besides MPI_THREAD_MULTIPLE support, thread
      access <br>
      &gt; control, sequential order of MPI_Isend/recv, MPI_Test and
      MPI_Wait for <br>
      &gt; one MPI_Request object) I need to take care of? If not, what
      could I do <br>
      &gt; to find the source of my problem?<br>
      &gt; <br>
      &gt; Thanks again for any kind of help!<br>
      &gt; <br>
      &gt; Kind regards,<br>
      &gt; David<br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; &gt; &gt; From an implementation perspective, your code is
      correct only if you initialize the MPI library with
      MPI_THREAD_MULTIPLE and if the library accepts. Otherwise, there
      is an assumption that the application is single threaded, or that
      the MPI behavior is implementation dependent. Please read the MPI
      standard regarding to MPI_Init_thread for more details.<br>
      &gt; &gt;<br>
      &gt; &gt; Regards,<br>
      &gt; &gt; george.<br>
      &gt; &gt;<br>
      &gt; &gt; On May 19, 2011, at 02:34 , David B?ttner wrote:<br>
      &gt; &gt;<br>
      &gt; &gt;&gt; Hello,<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; I am working on a hybrid MPI (OpenMPI 1.4.3) and
      Pthread code. I am using MPI_Isend and MPI_Irecv for communication
      and MPI_Test/MPI_Wait to check if it is done. I do this repeatedly
      in the outer loop of my code. The MPI_Test is used in the inner
      loop to check if some function can be called which depends on the
      received data.<br>
      &gt; &gt;&gt; The program regularly crashed (only when not using
      printf...) and after debugging it I figured out the following
      problem:<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; In MPI_Isend I have an invalid read of memory. I
      fixed the problem with not re-using a<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; MPI_Request req_s, req_r;<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; but by using<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; MPI_Request* req_s;<br>
      &gt; &gt;&gt; MPI_Request* req_r<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; and re-allocating them before the MPI_Isend/recv.<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; The documentation says, that in MPI_Wait and
      MPI_Test (if successful) the request-objects are deallocated and
      set to MPI_REQUEST_NULL.<br>
      &gt; &gt;&gt; It also says, that in MPI_Isend and MPI_Irecv, it
      allocates the Objects and associates it with the request object.<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; As I understand this, this either means I can use a
      pointer to MPI_Request which I don't have to initialize for this
      (it doesn't work but crashes), or that I can use a MPI_Request
      pointer which I have initialized with malloc(sizeof(MPI_REQUEST))
      (or passing the address of a MPI_Request req), which is set and
      unset in the functions. But this version crashes, too.<br>
      &gt; &gt;&gt; What works is using a pointer, which I allocate
      before the MPI_Isend/recv and which I free after MPI_Wait in every
      iteration. In other words: It only uses if I don't reuse any kind
      of MPI_Request. Only if I recreate one every time.<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; Is this, what is should be like? I believe that a
      reuse of the memory would be a lot more efficient (less calls to
      malloc...). Am I missing something here? Or am I doing something
      wrong?<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; Let me provide some more detailed information about
      my problem:<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; I am running the program on a 30 node infiniband
      cluster. Each node has 4 single core Opteron CPUs. I am running 1
      MPI Rank per node and 4 threads per rank (-&gt; one thread per
      core).<br>
      &gt; &gt;&gt; I am compiling with mpicc of OpenMPI using gcc
      below.<br>
      &gt; &gt;&gt; Some pseudo-code of the program can be found at the
      end of this e-mail.<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; I was able to reproduce the problem using different
      amount of nodes and even using one node only. The problem does not
      arise when I put printf-debugging information into the code. This
      pointed me into the direction that I have some memory problem,
      where some write accesses some memory it is not supposed to.<br>
      &gt; &gt;&gt; I ran the tests using valgrind with
      --leak-check=full and --show-reachable=yes, which pointed me
      either to MPI_Isend or MPI_Wait depending on whether I had the
      threads spin in a loop for MPI_Test to return success or used
      MPI_Wait respectively.<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; I would appreciate your help with this. Am I missing
      something important here? Is there a way to re-use the request in
      the different iterations other than I thought it should work?<br>
      &gt; &gt;&gt; Or is there a way to re-initialize the allocated
      memory before the MPI_Isend/recv so that I at least don't have to
      call free and malloc each time?<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; Thank you very much for your help!<br>
      &gt; &gt;&gt; Kind regards,<br>
      &gt; &gt;&gt; David B?ttner<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; _____________________<br>
      &gt; &gt;&gt; Pseudo-Code of program:<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; MPI_Request* req_s;<br>
      &gt; &gt;&gt; MPI_Request* req_w;<br>
      &gt; &gt;&gt; OUTER-LOOP<br>
      &gt; &gt;&gt; if(0 == threadid)<br>
      &gt; &gt;&gt; {<br>
      &gt; &gt;&gt; req_s = malloc(sizeof(MPI_Request));<br>
      &gt; &gt;&gt; req_r = malloc(sizeof(MPI_Request));<br>
      &gt; &gt;&gt; MPI_Isend(..., req_s)<br>
      &gt; &gt;&gt; MPI_Irecv(..., req_r)<br>
      &gt; &gt;&gt; }<br>
      &gt; &gt;&gt; pthread_barrier<br>
      &gt; &gt;&gt; INNER-LOOP (while NOT_DONE or RET)<br>
      &gt; &gt;&gt; if(TRYLOCK&amp;&amp; NOT_DONE)<br>
      &gt; &gt;&gt; {<br>
      &gt; &gt;&gt; if(MPI_TEST(req_r))<br>
      &gt; &gt;&gt; {<br>
      &gt; &gt;&gt; Call_Function_A;<br>
      &gt; &gt;&gt; NOT_DONE = 0;<br>
      &gt; &gt;&gt; }<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; }<br>
      &gt; &gt;&gt; RET = Call_Function_B;<br>
      &gt; &gt;&gt; }<br>
      &gt; &gt;&gt; pthread_barrier_wait<br>
      &gt; &gt;&gt; if(0 == threadid)<br>
      &gt; &gt;&gt; {<br>
      &gt; &gt;&gt; MPI_WAIT(req_s)<br>
      &gt; &gt;&gt; MPI_WAIT(req_r)<br>
      &gt; &gt;&gt; free(req_s);<br>
      &gt; &gt;&gt; free(req_r);<br>
      &gt; &gt;&gt; }<br>
      &gt; &gt;&gt; _____________<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; -- <br>
      &gt; &gt;&gt; David B?ttner, Informatik, Technische Universit?t
      M?nchen<br>
      &gt; &gt;&gt; TUM I-10 - FMI 01.06.059 - Tel. 089 / 289-17676<br>
      &gt; &gt;&gt;<br>
      &gt; &gt;&gt; _______________________________________________<br>
      &gt; &gt;&gt; users mailing list<br>
      &gt; &gt;&gt; <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; &gt;&gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      &gt; &gt; "To preserve the freedom of the human mind then and
      freedom of the press, every spirit should be ready to devote
      itself to martyrdom; for as long as we may think as we will, and
      speak as we think, the condition of man will proceed in
      improvement."<br>
      &gt; &gt; -- Thomas Jefferson, 1799<br>
      &gt; &gt;<br>
      &gt; &gt;<br>
      &gt; &gt; _______________________________________________<br>
      &gt; &gt; users mailing list<br>
      &gt; &gt; <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; &gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      &gt; <br>
      &gt; -- <br>
      &gt; David B?ttner, Informatik, Technische Universit?t M?nchen<br>
      &gt; TUM I-10 - FMI 01.06.059 - Tel. 089 / 289-17676<br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; ------------------------------<br>
      &gt; <br>
      &gt; Message: 11<br>
      &gt; Date: Fri, 20 May 2011 06:23:21 -0400<br>
      &gt; From: Jeff Squyres <a class="moz-txt-link-rfc2396E" href="mailto:jsquyres@cisco.com">&lt;jsquyres@cisco.com&gt;</a><br>
      &gt; Subject: Re: [OMPI users] Trouble with MPI-IO<br>
      &gt; To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a><br>
      &gt; Message-ID:
      <a class="moz-txt-link-rfc2396E" href="mailto:A5B121E9-E664-49D0-AE54-2CFE527129D2@cisco.com">&lt;A5B121E9-E664-49D0-AE54-2CFE527129D2@cisco.com&gt;</a><br>
      &gt; Content-Type: text/plain; charset=us-ascii<br>
      &gt; <br>
      &gt; On May 19, 2011, at 11:24 PM, Tom Rosmond wrote:<br>
      &gt; <br>
      &gt; &gt; What fortran compiler did you use?<br>
      &gt; <br>
      &gt; gfortran.<br>
      &gt; <br>
      &gt; &gt; In the original script my Intel compile used the -132
      option, <br>
      &gt; &gt; allowing up to that many columns per line. <br>
      &gt; <br>
      &gt; Gotcha.<br>
      &gt; <br>
      &gt; &gt;&gt; x.f90:99.77:<br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt; call
      mpi_type_indexed(lenij,ijlena,ijdisp,mpi_real,ij_vector_type,ierr)<br>
      &gt; &gt;&gt; 1 <br>
      &gt; &gt;&gt; Error: There is no specific subroutine for the
      generic 'mpi_type_indexed' at (1)<br>
      &gt; &gt; <br>
      &gt; &gt; Hmmm, very strange, since I am looking right at the MPI
      standard<br>
      &gt; &gt; documents with that routine documented. I too get this
      compile failure<br>
      &gt; &gt; when I switch to 'use mpi'. Could that be a problem with
      the Open MPI<br>
      &gt; &gt; fortran libraries???<br>
      &gt; <br>
      &gt; I think that that error is telling us that there's a
      compile-time mismatch -- that the signature of what you've passed
      doesn't match the signature of OMPI's MPI_Type_indexed subroutine.<br>
      &gt; <br>
      &gt; &gt;&gt; I looked at our mpi F90 module and see the
      following:<br>
      &gt; &gt;&gt; <br>
      &gt; &gt;&gt; interface MPI_Type_indexed<br>
      &gt; &gt;&gt; subroutine MPI_Type_indexed(count,
      array_of_blocklengths, array_of_displacements, oldtype, newtype,
      ierr)<br>
      &gt; &gt;&gt; integer, intent(in) :: count<br>
      &gt; &gt;&gt; integer, dimension(*), intent(in) ::
      array_of_blocklengths<br>
      &gt; &gt;&gt; integer, dimension(*), intent(in) ::
      array_of_displacements<br>
      &gt; &gt;&gt; integer, intent(in) :: oldtype<br>
      &gt; &gt;&gt; integer, intent(out) :: newtype<br>
      &gt; &gt;&gt; integer, intent(out) :: ierr<br>
      &gt; &gt;&gt; end subroutine MPI_Type_indexed<br>
      &gt; &gt;&gt; end interface<br>
      &gt; <br>
      &gt; Shouldn't ijlena and ijdisp be 1D arrays, not 2D arrays?<br>
      &gt; <br>
      &gt; -- <br>
      &gt; Jeff Squyres<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
      &gt; For corporate legal information go to:<br>
      &gt; <a class="moz-txt-link-freetext" href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; ------------------------------<br>
      &gt; <br>
      &gt; Message: 12<br>
      &gt; Date: Fri, 20 May 2011 07:26:19 -0400<br>
      &gt; From: Jeff Squyres <a class="moz-txt-link-rfc2396E" href="mailto:jsquyres@cisco.com">&lt;jsquyres@cisco.com&gt;</a><br>
      &gt; Subject: Re: [OMPI users] MPI_Alltoallv function crashes when
      np &gt; 100<br>
      &gt; To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a><br>
      &gt; Message-ID:
      <a class="moz-txt-link-rfc2396E" href="mailto:F9F71854-B9DD-459F-999D-8A8AEF8D6006@cisco.com">&lt;F9F71854-B9DD-459F-999D-8A8AEF8D6006@cisco.com&gt;</a><br>
      &gt; Content-Type: text/plain; charset=GB2312<br>
      &gt; <br>
      &gt; I missed this email in my INBOX, sorry.<br>
      &gt; <br>
      &gt; Can you be more specific about what exact error is occurring?
      You just say that the application crashes...? Please send all the
      information listed here:<br>
      &gt; <br>
      &gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/help/">http://www.open-mpi.org/community/help/</a><br>
      &gt; <br>
      &gt; <br>
      &gt; On Apr 26, 2011, at 10:51 PM, ?????? wrote:<br>
      &gt; <br>
      &gt; &gt; It seems that the const variable SOMAXCONN who used by
      listen() system call causes this problem. Can anybody help me
      resolve this question?<br>
      &gt; &gt; <br>
      &gt; &gt; 2011/4/25 ?????? <a class="moz-txt-link-rfc2396E" href="mailto:xjun.meng@gmail.com">&lt;xjun.meng@gmail.com&gt;</a><br>
      &gt; &gt; Dear all,<br>
      &gt; &gt; <br>
      &gt; &gt; As I mentioned, when I mpiruned an application with the
      parameter "np = 150(or bigger)", the application who used the
      MPI_Alltoallv function would carsh. The problem would recur no
      matter how many nodes we used. <br>
      &gt; &gt; <br>
      &gt; &gt; The edition of OpenMPI: 1.4.1 or 1.4.3<br>
      &gt; &gt; The OS: linux redhat 2.6.32<br>
      &gt; &gt; <br>
      &gt; &gt; BTW, my nodes had enough memory to run the application,
      and the MPI_Alltoall function worked well at my environment.<br>
      &gt; &gt; Did anybody meet the same problem? Thanks.<br>
      &gt; &gt; <br>
      &gt; &gt; <br>
      &gt; &gt; Best Regards<br>
      &gt; &gt; <br>
      &gt; &gt; <br>
      &gt; &gt; <br>
      &gt; &gt; <br>
      &gt; &gt; _______________________________________________<br>
      &gt; &gt; users mailing list<br>
      &gt; &gt; <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; &gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      &gt; <br>
      &gt; <br>
      &gt; -- <br>
      &gt; Jeff Squyres<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
      &gt; For corporate legal information go to:<br>
      &gt; <a class="moz-txt-link-freetext" href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; ------------------------------<br>
      &gt; <br>
      &gt; Message: 13<br>
      &gt; Date: Fri, 20 May 2011 07:28:28 -0400<br>
      &gt; From: Jeff Squyres <a class="moz-txt-link-rfc2396E" href="mailto:jsquyres@cisco.com">&lt;jsquyres@cisco.com&gt;</a><br>
      &gt; Subject: Re: [OMPI users] MPI_ERR_TRUNCATE with
      MPI_Allreduce() error,<br>
      &gt; but only sometimes...<br>
      &gt; To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a><br>
      &gt; Message-ID:
      <a class="moz-txt-link-rfc2396E" href="mailto:CAEF632E-757B-49EE-B545-5CCCBC712247@cisco.com">&lt;CAEF632E-757B-49EE-B545-5CCCBC712247@cisco.com&gt;</a><br>
      &gt; Content-Type: text/plain; charset=us-ascii<br>
      &gt; <br>
      &gt; Sorry for the super-late reply. :-\<br>
      &gt; <br>
      &gt; Yes, ERR_TRUNCATE means that the receiver didn't have a large
      enough buffer.<br>
      &gt; <br>
      &gt; Have you tried upgrading to a newer version of Open MPI?
      1.4.3 is the current stable release (I have a very dim and not
      guaranteed to be correct recollection that we fixed something in
      the internals of collectives somewhere with regards to
      ERR_TRUNCATE...?).<br>
      &gt; <br>
      &gt; <br>
      &gt; On Apr 25, 2011, at 4:44 PM, Wei Hao wrote:<br>
      &gt; <br>
      &gt; &gt; Hi:<br>
      &gt; &gt; <br>
      &gt; &gt; I'm running openmpi 1.2.8. I'm working on a project
      where one part involves communicating an integer, representing the
      number of data points I'm keeping track of, to all the processors.
      The line is simple:<br>
      &gt; &gt; <br>
      &gt; &gt;
      MPI_Allreduce(&amp;np,&amp;geo_N,1,MPI_INT,MPI_MAX,MPI_COMM_WORLD);<br>
      &gt; &gt; <br>
      &gt; &gt; where np and geo_N are integers, np is the result of a
      local calculation, and geo_N has been declared on all the
      processors. geo_N is nondecreasing. This line works the first time
      I call it (geo_N goes from 0 to some other integer), but if I call
      it later in the program, I get the following error:<br>
      &gt; &gt; <br>
      &gt; &gt; <br>
      &gt; &gt; [woodhen-039:26189] *** An error occurred in
      MPI_Allreduce<br>
      &gt; &gt; [woodhen-039:26189] *** on communicator MPI_COMM_WORLD<br>
      &gt; &gt; [woodhen-039:26189] *** MPI_ERR_TRUNCATE: message
      truncated<br>
      &gt; &gt; [woodhen-039:26189] *** MPI_ERRORS_ARE_FATAL (goodbye)<br>
      &gt; &gt; <br>
      &gt; &gt; <br>
      &gt; &gt; As I understand it, MPI_ERR_TRUNCATE means that the
      output buffer is too small, but I'm not sure where I've made a
      mistake. It's particularly frustrating because it seems to work
      fine the first time. Does anyone have any thoughts?<br>
      &gt; &gt; <br>
      &gt; &gt; Thanks<br>
      &gt; &gt; Wei<br>
      &gt; &gt; _______________________________________________<br>
      &gt; &gt; users mailing list<br>
      &gt; &gt; <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; &gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      &gt; <br>
      &gt; <br>
      &gt; -- <br>
      &gt; Jeff Squyres<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
      &gt; For corporate legal information go to:<br>
      &gt; <a class="moz-txt-link-freetext" href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; <br>
      &gt; ------------------------------<br>
      &gt; <br>
      &gt; Message: 14<br>
      &gt; Date: Fri, 20 May 2011 08:14:07 -0400<br>
      &gt; From: Jeff Squyres <a class="moz-txt-link-rfc2396E" href="mailto:jsquyres@cisco.com">&lt;jsquyres@cisco.com&gt;</a><br>
      &gt; Subject: Re: [OMPI users] Trouble with MPI-IO<br>
      &gt; To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a><br>
      &gt; Message-ID:
      <a class="moz-txt-link-rfc2396E" href="mailto:42DB03B3-9CF4-4ACB-AA20-B857E5F76087@cisco.com">&lt;42DB03B3-9CF4-4ACB-AA20-B857E5F76087@cisco.com&gt;</a><br>
      &gt; Content-Type: text/plain; charset="us-ascii"<br>
      &gt; <br>
      &gt; On May 20, 2011, at 6:23 AM, Jeff Squyres wrote:<br>
      &gt; <br>
      &gt; &gt; Shouldn't ijlena and ijdisp be 1D arrays, not 2D arrays?<br>
      &gt; <br>
      &gt; Ok, if I convert ijlena and ijdisp to 1D arrays, I don't get
      the compile error (even though they're allocatable -- so allocate
      was a red herring, sorry). That's all that "use mpi" is
      complaining about -- that the function signatures didn't match.<br>
      &gt; <br>
      &gt; use mpi is your friend -- even if you don't use F90
      constructs much. Compile-time checking is Very Good Thing (you
      were effectively "getting lucky" by passing in the 2D arrays, I
      think).<br>
      &gt; <br>
      &gt; Attached is my final version. And with this version, I see
      the hang when running it with the "T" parameter.<br>
      &gt; <br>
      &gt; That being said, I'm not an expert on the MPI IO stuff --
      your code *looks* right to me, but I could be missing something
      subtle in the interpretation of MPI_FILE_SET_VIEW. I tried running
      your code with MPICH 1.3.2p1 and it also hung.<br>
      &gt; <br>
      &gt; Rob (ROMIO guy) -- can you comment this code? Is it correct?<br>
      &gt; <br>
      &gt; -- <br>
      &gt; Jeff Squyres<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
      &gt; For corporate legal information go to:<br>
      &gt; <a class="moz-txt-link-freetext" href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
      &gt; -------------- next part --------------<br>
      &gt; A non-text attachment was scrubbed...<br>
      &gt; Name: x.f90<br>
      &gt; Type: application/octet-stream<br>
      &gt; Size: 3820 bytes<br>
      &gt; Desc: not available<br>
      &gt; URL:
<a class="moz-txt-link-rfc2396E" href="http://www.open-mpi.org/MailArchives/users/attachments/20110520/53a5461b/attachment.obj">&lt;http://www.open-mpi.org/MailArchives/users/attachments/20110520/53a5461b/attachment.obj&gt;</a><br>
      &gt; <br>
      &gt; ------------------------------<br>
      &gt; <br>
      &gt; _______________________________________________<br>
      &gt; users mailing list<br>
      &gt; <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
      &gt; <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      &gt; <br>
      &gt; End of users Digest, Vol 1911, Issue 1<br>
      &gt; **************************************<br>
      <pre wrap="">
<fieldset class="mimeAttachmentHeader"></fieldset>
_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></pre>
    </blockquote>
  </body>
</html>

