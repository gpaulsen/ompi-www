Hi Ryan,<br><br>Thanks for the confirmation that this is an issue with mixing<br>different transports.&nbsp; I will followup by trying this with the current <br>trunk.&nbsp; If things work okay there, then the best approach might be<br>
for you to move forward to that, if possible.&nbsp; It&#39;s at a fairly stable<br>point now in anticipation for branching for the upcoming 1.3 release.<br>However, if it&#39;s still an issue on the trunk, then I will file a defect<br>
on it &amp; we&#39;ll get it resolved for the 1.3 release.<br><br>--Brad<br><br><br><div class="gmail_quote">On Thu, May 15, 2008 at 3:30 PM, Ryan Buckley ; 21426 &lt;<a href="mailto:rbuckley@mc.com">rbuckley@mc.com</a>&gt; wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">Hello Brad,<br>
<br>
I removed the openib specifier from the btl lists in the appfile and the<br>
application ran fine over ethernet. &nbsp;And yes, to confirm, if I attempt<br>
to mix systems with IB and systems without IB, the application<br>
hangs.<br>
<br>
Thanks,<br>
<br>
Ryan<br>
<br>
<br>
<br>
Hello Ryan,<br>
<br>
<br>
I have been running a similar heterogeneous setup in my lab; i.e., a mix<br>
of<br>
ppc64 and x86_64 systems connected by ethernet and InfiniBand. In trying<br>
to<br>
replicate your problem, what I see is that it is not an issue of<br>
processor<br>
heterogeneity, but rather an issue with heterogeneous transports. Can<br>
you<br>
remove the openib specifier from the btl lists in the appfile and try<br>
again? I.e., force all inter-system communications over ethernet? For<br>
me,<br>
that works. But, if I mix systems with IB with systems without IB, I,<br>
too,<br>
see a hang...even if the processor architectures are the same. If you<br>
could<br>
confirm that your case is the same, then we can make sure we&#39;re only<br>
chasing<br>
one problem and not two.<br>
<br>
<br>
Thanks,<br>
--Brad<br>
<br>
<br>
Brad Benton<br>
IBM<br>
<br>
<br>
On Thu, May 1, 2008 at 11:02 AM, Ryan Buckley ; 21426<br>
&lt;rbuckley_at_[hidden]&gt;<br>
wrote:<br>
<br>
<br>
&gt; Hello,<br>
&gt;<br>
&gt; I am trying to run a simple Hello World MPI application in a<br>
&gt; heterogeneous environment. The machines include 1 x86 machine with a<br>
&gt; standard 1Gb ethernet connection and 2 ppc machines with standard 1Gb<br>
&gt; ethernet as well as a 10Gb ethernet (Infiniband) switch between the<br>
two.<br>
&gt; The Hello World program is the same hello_c.c that is included in the<br>
&gt; examples directory of the Open MPI installation.<br>
&gt;<br>
&gt; The goal is that I would like to run heterogeneous applications<br>
between<br>
&gt; the three aforementioned machines in the following manner:<br>
&gt;<br>
&gt; The x86 machine will use tcp to communicate to the 2 ppc machines,<br>
&gt; while the ppc machines will communicate with one another via the<br>
10GbE.<br>
&gt;<br>
&gt; x86 &lt;--tcp--&gt; ppc_1<br>
&gt; x86 &lt;--tcp--&gt; ppc_2<br>
&gt; ppc1 &lt;--openib--&gt; ppc_2<br>
&gt;<br>
&gt; I am currently using a machfile set up as follows,<br>
&gt;<br>
&gt; # cat machfile<br>
&gt; &lt;ppc_host_1&gt;<br>
&gt; &lt;ppc_host_2&gt;<br>
&gt; &lt;x86_host&gt;<br>
&gt;<br>
&gt; In addition I am using an appfile set up as follows,<br>
&gt;<br>
&gt; # cat appfile<br>
&gt; -np 1 --hostfile machfile --host &lt;ppc_host_1&gt; --mca btl<br>
&gt; sm,self,tcp,openib /path/to/ppc/openmpi-1.2.5/examples/hello<br>
&gt; -np 1 --hostfile machfile --host &lt;ppc_host_2&gt; --mca btl<br>
&gt; sm,self,tcp,openib /path/to/ppc/openmpi-1.2.5/examples/hello<br>
&gt; -np 1 --hostfile machfile --host &lt;x86_host&gt; --mca btl<br>
&gt; sm,self,tcp /path/to/x86/openmpi-1.2.5/examples/hello<br>
&gt;<br>
&gt; I am running on the command line via<br>
&gt;<br>
&gt; # mpirun --app appfile<br>
&gt;<br>
&gt; I&#39;ve also attached the output from &#39;ompi_info --all&#39; from all<br>
machines.<br>
&gt;<br>
&gt; Any suggestions would be much appreciated.<br>
&gt;<br>
&gt; Thanks,<br>
&gt;<br>
&gt; Ryan<br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; users_at_[hidden]<br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br>

