Dear All,<br /><br />&nbsp;&nbsp;&nbsp; Could anyone tell me the important tuning parameters in openmpi with IB interconnect? I tried setting eager_rdma, min_rdma_size, mpi_leave_pinned parameters from the mpirun command line on 38 nodes cluster (38*2 processors) but in vain. I found simple mpirun with no mca parameters performing better. I conducted test on P2P send/receive with data size of 8MB.<br /><br />&nbsp;&nbsp;&nbsp; Similarly i patched HPL linpack code with libnbc(non blocking collectives) and found no performance benefits. I went through its patch and found that, its probably not overlapping computation with communication.<br /><br />Any help in this direction would be appreciated.<br />-Neeraj<br />
<br><Table border=0 Width=644 Height=57 cellspacing=0 cellpadding=0 style='font-family:Verdana;font-size:11px;line-height:15px;'><TR><td><a href='http://adworks.rediff.com/cgi-bin/AdWorks/click.cgi/www.rediff.com/signature-home.htm/1050715198@Middle5/1473272_1466559/1473095/1?PARTNER=3&OAS_QUERY=null target=new '><img src =http://imadworks.rediff.com/cgi-bin/AdWorks/adimage.cgi/1473272_1466559/creative_1473095.gif  alt='Love'  border=0></a></td></TR></Table>
