<div dir="ltr"><div class="gmail_default" style="font-family:tahoma,sans-serif">Hello,</div><div class="gmail_default" style="font-family:tahoma,sans-serif"><br></div><div class="gmail_default" style="font-family:tahoma,sans-serif">
I added the &quot;self&quot; e.g </div><div class="gmail_default" style="font-family:tahoma,sans-serif"><br></div><div class="gmail_default"><div class="gmail_default"><font face="tahoma, sans-serif" color="#ff0000">hsaeed@karp:~/Task4_mpi/scatterv$ mpirun -np 8 --mca btl ^openib --mca btl_tcp_if_exclude sm,self,lo,br0,br1,ib0,br2 --host karp,wirth ./scatterv</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif"><br></font></div><div class="gmail_default"><font face="tahoma, sans-serif">Enter passphrase for key &#39;/home/hsaeed/.ssh/id_rsa&#39;:</font></div><div class="gmail_default">
<font face="tahoma, sans-serif">--------------------------------------------------------------------------</font></div><div class="gmail_default"><font face="tahoma, sans-serif"><br></font></div><div class="gmail_default">
<font face="tahoma, sans-serif" color="#ff0000">ERROR::</font></div><div class="gmail_default"><font face="tahoma, sans-serif"><br></font></div><div class="gmail_default"><font face="tahoma, sans-serif">At least one pair of MPI processes are unable to reach each other for</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">MPI communications.  This means that no Open MPI device has indicated</font></div><div class="gmail_default"><font face="tahoma, sans-serif">that it can be used to communicate between these processes.  This is</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">an error; Open MPI requires that all MPI processes be able to reach</font></div><div class="gmail_default"><font face="tahoma, sans-serif">each other.  This error can sometimes be the result of forgetting to</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">specify the &quot;self&quot; BTL.</font></div><div class="gmail_default"><font face="tahoma, sans-serif"><br></font></div><div class="gmail_default"><font face="tahoma, sans-serif">  Process 1 ([[15751,1],7]) is on host: wirth</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">  Process 2 ([[15751,1],0]) is on host: karp</font></div><div class="gmail_default"><font face="tahoma, sans-serif">  BTLs attempted: self sm</font></div><div class="gmail_default">
<font face="tahoma, sans-serif"><br></font></div><div class="gmail_default"><font face="tahoma, sans-serif">Your MPI job is now going to abort; sorry.</font></div><div class="gmail_default"><font face="tahoma, sans-serif">--------------------------------------------------------------------------</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">--------------------------------------------------------------------------</font></div><div class="gmail_default"><font face="tahoma, sans-serif">MPI_INIT has failed because at least one MPI process is unreachable</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">from another.  This *usually* means that an underlying communication</font></div><div class="gmail_default"><font face="tahoma, sans-serif">plugin -- such as a BTL or an MTL -- has either not loaded or not</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">allowed itself to be used.  Your MPI job will now abort.</font></div><div class="gmail_default"><font face="tahoma, sans-serif"><br></font></div><div class="gmail_default">
<font face="tahoma, sans-serif">You may wish to try to narrow down the problem;</font></div><div class="gmail_default"><font face="tahoma, sans-serif"><br></font></div><div class="gmail_default"><font face="tahoma, sans-serif"> * Check the output of ompi_info to see which BTL/MTL plugins are</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">   available.</font></div><div class="gmail_default"><font face="tahoma, sans-serif"> * Run your application with MPI_THREAD_SINGLE.</font></div><div class="gmail_default">
<font face="tahoma, sans-serif"> * Set the MCA parameter btl_base_verbose to 100 (or mtl_base_verbose,</font></div><div class="gmail_default"><font face="tahoma, sans-serif">   if using MTL-based communications) to see exactly which</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">   communication plugins were considered and/or discarded.</font></div><div class="gmail_default"><font face="tahoma, sans-serif">--------------------------------------------------------------------------</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">[wirth:40329] *** An error occurred in MPI_Init</font></div><div class="gmail_default"><font face="tahoma, sans-serif">[wirth:40329] *** on a NULL communicator</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">[wirth:40329] *** Unknown error</font></div><div class="gmail_default"><font face="tahoma, sans-serif">[wirth:40329] *** MPI_ERRORS_ARE_FATAL: your MPI job will now abort</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">--------------------------------------------------------------------------</font></div><div class="gmail_default"><font face="tahoma, sans-serif">An MPI process is aborting at a time when it cannot guarantee that all</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">of its peer processes in the job will be killed properly.  You should</font></div><div class="gmail_default"><font face="tahoma, sans-serif">double check that everything has shut down cleanly.</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif"><br></font></div><div class="gmail_default"><font face="tahoma, sans-serif">  Reason:     Before MPI_INIT completed</font></div><div class="gmail_default"><font face="tahoma, sans-serif">  Local host: wirth</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">  PID:        40329</font></div><div class="gmail_default"><font face="tahoma, sans-serif">--------------------------------------------------------------------------</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">--------------------------------------------------------------------------</font></div><div class="gmail_default"><font face="tahoma, sans-serif">mpirun has exited due to process rank 7 with PID 40329 on</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">node wirth exiting improperly. There are two reasons this could occur:</font></div><div class="gmail_default"><font face="tahoma, sans-serif"><br></font></div><div class="gmail_default">
<font face="tahoma, sans-serif">1. this process did not call &quot;init&quot; before exiting, but others in</font></div><div class="gmail_default"><font face="tahoma, sans-serif">the job did. This can cause a job to hang indefinitely while it waits</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">for all processes to call &quot;init&quot;. By rule, if one process calls &quot;init&quot;,</font></div><div class="gmail_default"><font face="tahoma, sans-serif">then ALL processes must call &quot;init&quot; prior to termination.</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif"><br></font></div><div class="gmail_default"><font face="tahoma, sans-serif">2. this process called &quot;init&quot;, but exited without calling &quot;finalize&quot;.</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">By rule, all processes that call &quot;init&quot; MUST call &quot;finalize&quot; prior to</font></div><div class="gmail_default"><font face="tahoma, sans-serif">exiting or it will be considered an &quot;abnormal termination&quot;</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif"><br></font></div><div class="gmail_default"><font face="tahoma, sans-serif">This may have caused other processes in the application to be</font></div><div class="gmail_default">
<font face="tahoma, sans-serif">terminated by signals sent by mpirun (as reported here).</font></div><div class="gmail_default"><font face="tahoma, sans-serif">--------------------------------------------------------------------------</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">[karp:29513] 1 more process has sent help message help-mca-bml-r2.txt / unreachable proc</font></div><div class="gmail_default"><font face="tahoma, sans-serif">[karp:29513] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">[karp:29513] 1 more process has sent help message help-mpi-runtime / mpi_init:startup:pml-add-procs-fail</font></div><div class="gmail_default"><font face="tahoma, sans-serif">[karp:29513] 1 more process has sent help message help-mpi-errors.txt / mpi_errors_are_fatal unknown handle</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">[karp:29513] 1 more process has sent help message help-mpi-runtime.txt / ompi mpi abort:cannot guarantee all killed</font></div><div class="gmail_default"><font face="tahoma, sans-serif"><br>
</font></div><div class="gmail_default"><font face="tahoma, sans-serif" color="#0000ff">I tried every combination for btl_tcp_if_include or exclude...</font></div><div class="gmail_default"><font face="tahoma, sans-serif" color="#0000ff"><br>
</font></div><div class="gmail_default"><font face="tahoma, sans-serif" color="#0000ff">I cant figure out what is wrong.</font></div><div class="gmail_default"><font face="tahoma, sans-serif" color="#0000ff">I can easily talk with the remote pc using netcat.</font></div>
<div class="gmail_default"><font face="tahoma, sans-serif">I am sure i am very near to the solution but..</font></div><div class="gmail_default"><font face="tahoma, sans-serif"><br></font></div><div class="gmail_default">
<font face="tahoma, sans-serif">regards.</font></div><div style="font-family:tahoma,sans-serif"><br></div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Mon, Mar 24, 2014 at 3:25 PM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">If you you use btl_tcp_if_exclude, you also need to exclude the loopback interface.  Loopback is excluded by the default value of btl_tcp_if_exclude, but if you overwrite that value, then you need to *also* include the loopback interface in the new value.<br>

<div class="HOEnZb"><div class="h5"><br>
<br>
<br>
On Mar 24, 2014, at 4:57 AM, Hamid Saeed &lt;<a href="mailto:e.hamidsaeed@gmail.com">e.hamidsaeed@gmail.com</a>&gt; wrote:<br>
<br>
&gt; Hello,<br>
&gt; Still i am facing problems.<br>
&gt; I checked there is no firewall which is acting as a barrier for the mpi communication.<br>
&gt;<br>
&gt; even i used the execution line like<br>
&gt; hsaeed@karp:~/Task4_mpi/scatterv$ mpiexec -n 2 --mca btl_tcp_if_exclude br2 -host wirth,karp ./a.out<br>
&gt;<br>
&gt; Now the output hangup without displaying any error.<br>
&gt;<br>
&gt; Used &quot;..exclude bt2&quot; because the failed connection was from bt2 as you can see in the &quot;ifconfig&quot; output mentioned above.<br>
&gt;<br>
&gt; I know there is something wrong but i am almost unable to figure it out.<br>
&gt;<br>
&gt; I need some more kind suggestions.<br>
&gt;<br>
&gt; regards.<br>
&gt;<br>
&gt;<br>
&gt; On Fri, Mar 21, 2014 at 6:05 PM, Jeff Squyres (jsquyres) &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br>
&gt; Do you have any firewalling enabled on these machines?  If so, you&#39;ll want to either disable it, or allow random TCP connections between any of the cluster nodes.<br>
&gt;<br>
&gt;<br>
&gt; On Mar 21, 2014, at 10:24 AM, Hamid Saeed &lt;<a href="mailto:e.hamidsaeed@gmail.com">e.hamidsaeed@gmail.com</a>&gt; wrote:<br>
&gt;<br>
&gt; &gt; /sbin/ifconfig<br>
&gt; &gt;<br>
&gt; &gt; hsaeed@karp:~$ /sbin/ifconfig<br>
&gt; &gt; br0       Link encap:Ethernet  HWaddr 00:25:90:59:c9:ba<br>
&gt; &gt;           inet addr:134.106.3.231  Bcast:134.106.3.255  Mask:255.255.255.0<br>
&gt; &gt;           inet6 addr: fe80::225:90ff:fe59:c9ba/64 Scope:Link<br>
&gt; &gt;           UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1<br>
&gt; &gt;           RX packets:49080961 errors:0 dropped:50263 overruns:0 frame:0<br>
&gt; &gt;           TX packets:43279252 errors:0 dropped:0 overruns:0 carrier:0<br>
&gt; &gt;           collisions:0 txqueuelen:0<br>
&gt; &gt;           RX bytes:41348407558 (38.5 GiB)  TX bytes:80505842745 (74.9 GiB)<br>
&gt; &gt;<br>
&gt; &gt; br1       Link encap:Ethernet  HWaddr 00:25:90:59:c9:bb<br>
&gt; &gt;           inet addr:134.106.53.231  Bcast:134.106.53.255  Mask:255.255.255.0<br>
&gt; &gt;           inet6 addr: fe80::225:90ff:fe59:c9bb/64 Scope:Link<br>
&gt; &gt;           UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1<br>
&gt; &gt;           RX packets:41573060 errors:0 dropped:50261 overruns:0 frame:0<br>
&gt; &gt;           TX packets:1693509 errors:0 dropped:0 overruns:0 carrier:0<br>
&gt; &gt;           collisions:0 txqueuelen:0<br>
&gt; &gt;           RX bytes:6177072160 (5.7 GiB)  TX bytes:230617435 (219.9 MiB)<br>
&gt; &gt;<br>
&gt; &gt; br2       Link encap:Ethernet  HWaddr 00:c0:0a:ec:02:e7<br>
&gt; &gt;           inet addr:10.231.2.231  Bcast:10.231.2.255  Mask:255.255.255.0<br>
&gt; &gt;           UP BROADCAST MULTICAST  MTU:1500  Metric:1<br>
&gt; &gt;           RX packets:0 errors:0 dropped:0 overruns:0 frame:0<br>
&gt; &gt;           TX packets:0 errors:0 dropped:0 overruns:0 carrier:0<br>
&gt; &gt;           collisions:0 txqueuelen:0<br>
&gt; &gt;           RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)<br>
&gt; &gt;<br>
&gt; &gt; eth0      Link encap:Ethernet  HWaddr 00:25:90:59:c9:ba<br>
&gt; &gt;           UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1<br>
&gt; &gt;           RX packets:69108377 errors:0 dropped:0 overruns:0 frame:0<br>
&gt; &gt;           TX packets:86459066 errors:0 dropped:0 overruns:0 carrier:0<br>
&gt; &gt;           collisions:0 txqueuelen:1000<br>
&gt; &gt;           RX bytes:43533091399 (40.5 GiB)  TX bytes:83359370885 (77.6 GiB)<br>
&gt; &gt;           Memory:dfe60000-dfe80000<br>
&gt; &gt;<br>
&gt; &gt; eth1      Link encap:Ethernet  HWaddr 00:25:90:59:c9:bb<br>
&gt; &gt;           UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1<br>
&gt; &gt;           RX packets:43531546 errors:0 dropped:0 overruns:0 frame:0<br>
&gt; &gt;           TX packets:1716151 errors:0 dropped:0 overruns:0 carrier:0<br>
&gt; &gt;           collisions:0 txqueuelen:1000<br>
&gt; &gt;           RX bytes:7201915977 (6.7 GiB)  TX bytes:232026383 (221.2 MiB)<br>
&gt; &gt;           Memory:dfee0000-dff00000<br>
&gt; &gt;<br>
&gt; &gt; lo        Link encap:Local Loopback<br>
&gt; &gt;           inet addr:127.0.0.1  Mask:255.0.0.0<br>
&gt; &gt;           inet6 addr: ::1/128 Scope:Host<br>
&gt; &gt;           UP LOOPBACK RUNNING  MTU:16436  Metric:1<br>
&gt; &gt;           RX packets:10890707 errors:0 dropped:0 overruns:0 frame:0<br>
&gt; &gt;           TX packets:10890707 errors:0 dropped:0 overruns:0 carrier:0<br>
&gt; &gt;           collisions:0 txqueuelen:0<br>
&gt; &gt;           RX bytes:36194379576 (33.7 GiB)  TX bytes:36194379576 (33.7 GiB)<br>
&gt; &gt;<br>
&gt; &gt; tap0      Link encap:Ethernet  HWaddr 00:c0:0a:ec:02:e7<br>
&gt; &gt;           UP BROADCAST MULTICAST  MTU:1500  Metric:1<br>
&gt; &gt;           RX packets:0 errors:0 dropped:0 overruns:0 frame:0<br>
&gt; &gt;           TX packets:0 errors:0 dropped:0 overruns:0 carrier:0<br>
&gt; &gt;           collisions:0 txqueuelen:500<br>
&gt; &gt;           RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)<br>
&gt; &gt;<br>
&gt; &gt; When i execute the following line<br>
&gt; &gt;<br>
&gt; &gt; hsaeed@karp:~/Task4_mpi/scatterv$ mpiexec -n 2 -host wirth,karp ./a.out<br>
&gt; &gt;<br>
&gt; &gt; i receive Error<br>
&gt; &gt;<br>
&gt; &gt; [wirth][[59430,1],0][btl_tcp_endpoint.c:655:mca_btl_tcp_endpoint_complete_connect] connect() to 10.231.2.231 failed: Connection refused (111)<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; NOTE: Karp and wirth are two machines on ssh cluster.<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; On Fri, Mar 21, 2014 at 3:13 PM, Jeff Squyres (jsquyres) &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br>
&gt; &gt; On Mar 21, 2014, at 10:09 AM, Hamid Saeed &lt;<a href="mailto:e.hamidsaeed@gmail.com">e.hamidsaeed@gmail.com</a>&gt; wrote:<br>
&gt; &gt;<br>
&gt; &gt; &gt; &gt; I think i have a tcp connection. As for as i know my cluster is not configured for Infiniband (IB).<br>
&gt; &gt;<br>
&gt; &gt; Ok.<br>
&gt; &gt;<br>
&gt; &gt; &gt; &gt; but even for tcp connections.<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; mpirun -n 2 -host master,node001 --mca btl tcp,sm,self ./helloworldmpi<br>
&gt; &gt; &gt; &gt; mpirun -n 2 -host master,node001 ./helloworldmpi<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; These line are not working they output<br>
&gt; &gt; &gt; &gt; Error like<br>
&gt; &gt; &gt; &gt; [btl_tcp_endpoint.c:655:mca_btl_tcp_endpoint_complete_connect] connect() to xx.xxx.x.xxx failed: Connection refused (111)<br>
&gt; &gt;<br>
&gt; &gt; What are the IP addresses reported by connect()?  (i.e., the address you X&#39;ed out)<br>
&gt; &gt;<br>
&gt; &gt; Send the output from ifconfig on each of your servers.  Note that some Linux distributions do not put ifconfig in the default PATH of normal users; look for it in/sbin/ifconfig or /usr/sbin/ifconfig.<br>
&gt; &gt;<br>
&gt; &gt; --<br>
&gt; &gt; Jeff Squyres<br>
&gt; &gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; &gt; For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; --<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; Hamid Saeed<br>
&gt; &gt; CoSynth GmbH &amp; Co. KG<br>
&gt; &gt; Escherweg 2 - 26121 Oldenburg - Germany<br>
&gt; &gt; Tel <a href="tel:%2B49%20441%209722%20738" value="+494419722738">+49 441 9722 738</a> | Fax -278<br>
&gt; &gt; <a href="http://www.cosynth.com" target="_blank">http://www.cosynth.com</a><br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Jeff Squyres<br>
&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; _______________________________________________<br>
&gt; Hamid Saeed<br>
&gt; CoSynth GmbH &amp; Co. KG<br>
&gt; Escherweg 2 - 26121 Oldenburg - Germany<br>
&gt; Tel <a href="tel:%2B49%20441%209722%20738" value="+494419722738">+49 441 9722 738</a> | Fax -278<br>
&gt; <a href="http://www.cosynth.com" target="_blank">http://www.cosynth.com</a><br>
&gt; _______________________________________________<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br><br clear="all"><div><br></div>-- <br><div dir="ltr"><div dir="ltr"><p style="color:rgb(34,34,34);margin:0cm 0cm 0.0001pt;font-size:11pt;font-family:Calibri,sans-serif"><span lang="EN-US">_______________________________________________<u></u><u></u></span></p>
<p style="color:rgb(34,34,34);margin:0cm 0cm 0.0001pt"><span lang="EN-US"><font face="tahoma, sans-serif">Hamid Saeed</font></span></p></div></div>
</div></div>

