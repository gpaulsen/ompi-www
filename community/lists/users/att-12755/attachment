Hi Ralph,<div>I&#39;m Krzysztof and I&#39;m working with Grzegorz Maj on this our small project/experiment. </div><div><br></div><div>We definitely would like to give your patch a try. But could you please explain your solution a little more?</div>

<div>You still would like to start one mpirun per mpi grid, and then have processes started by us to join the MPI comm? </div><div>It is a good solution of course.</div><div>But it would be especially preferable to have one daemon running persistently on our &quot;entry&quot; machine that can handle several mpi grid starts. Can your patch help us this way too?</div>

<div><br></div><div>Thanks for your help!</div><div>Krzysztof<br><br><div class="gmail_quote">On 24 April 2010 03:51, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">In thinking about this, my proposed solution won&#39;t entirely fix the problem - you&#39;ll still wind up with all those daemons. I believe I can resolve that one as well, but it would require a patch.<br>


<br>
Would you like me to send you something you could try? Might take a couple of iterations to get it right...<br>
<div><div></div><div class="h5"><br>
On Apr 23, 2010, at 12:12 PM, Ralph Castain wrote:<br>
<br>
&gt; Hmmm....I -think- this will work, but I cannot guarantee it:<br>
&gt;<br>
&gt; 1. launch one process (can just be a spinner) using mpirun that includes the following option:<br>
&gt;<br>
&gt; mpirun -report-uri file<br>
&gt;<br>
&gt; where file is some filename that mpirun can create and insert its contact info into it. This can be a relative or absolute path. This process must remain alive throughout your application - doesn&#39;t matter what it does. It&#39;s purpose is solely to keep mpirun alive.<br>


&gt;<br>
&gt; 2. set OMPI_MCA_dpm_orte_server=FILE:file in your environment, where &quot;file&quot; is the filename given above. This will tell your processes how to find mpirun, which is acting as a meeting place to handle the connect/accept operations<br>


&gt;<br>
&gt; Now run your processes, and have them connect/accept to each other.<br>
&gt;<br>
&gt; The reason I cannot guarantee this will work is that these processes will all have the same rank &amp;&amp; name since they all start as singletons. Hence, connect/accept is likely to fail.<br>
&gt;<br>
&gt; But it -might- work, so you might want to give it a try.<br>
&gt;<br>
&gt; On Apr 23, 2010, at 8:10 AM, Grzegorz Maj wrote:<br>
&gt;<br>
&gt;&gt; To be more precise: by &#39;server process&#39; I mean some process that I<br>
&gt;&gt; could run once on my system and it could help in creating those<br>
&gt;&gt; groups.<br>
&gt;&gt; My typical scenario is:<br>
&gt;&gt; 1. run N separate processes, each without mpirun<br>
&gt;&gt; 2. connect them into MPI group<br>
&gt;&gt; 3. do some job<br>
&gt;&gt; 4. exit all N processes<br>
&gt;&gt; 5. goto 1<br>
&gt;&gt;<br>
&gt;&gt; 2010/4/23 Grzegorz Maj &lt;<a href="mailto:maju3@wp.pl">maju3@wp.pl</a>&gt;:<br>
&gt;&gt;&gt; Thank you Ralph for your explanation.<br>
&gt;&gt;&gt; And, apart from that descriptors&#39; issue, is there any other way to<br>
&gt;&gt;&gt; solve my problem, i.e. to run separately a number of processes,<br>
&gt;&gt;&gt; without mpirun and then to collect them into an MPI intracomm group?<br>
&gt;&gt;&gt; If I for example would need to run some &#39;server process&#39; (even using<br>
&gt;&gt;&gt; mpirun) for this task, that&#39;s OK. Any ideas?<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Thanks,<br>
&gt;&gt;&gt; Grzegorz Maj<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; 2010/4/18 Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;:<br>
&gt;&gt;&gt;&gt; Okay, but here is the problem. If you don&#39;t use mpirun, and are not operating in an environment we support for &quot;direct&quot; launch (i.e., starting processes outside of mpirun), then every one of those processes thinks it is a singleton - yes?<br>


&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; What you may not realize is that each singleton immediately fork/exec&#39;s an orted daemon that is configured to behave just like mpirun. This is required in order to support MPI-2 operations such as MPI_Comm_spawn, MPI_Comm_connect/accept, etc.<br>


&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; So if you launch 64 processes that think they are singletons, then you have 64 copies of orted running as well. This eats up a lot of file descriptors, which is probably why you are hitting this 65 process limit - your system is probably running out of file descriptors. You might check you system limits and see if you can get them revised upward.<br>


&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; On Apr 17, 2010, at 4:24 PM, Grzegorz Maj wrote:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; Yes, I know. The problem is that I need to use some special way for<br>
&gt;&gt;&gt;&gt;&gt; running my processes provided by the environment in which I&#39;m working<br>
&gt;&gt;&gt;&gt;&gt; and unfortunately I can&#39;t use mpirun.<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; 2010/4/18 Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;:<br>
&gt;&gt;&gt;&gt;&gt;&gt; Guess I don&#39;t understand why you can&#39;t use mpirun - all it does is start things, provide a means to forward io, etc. It mainly sits there quietly without using any cpu unless required to support the job.<br>


&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; Sounds like it would solve your problem. Otherwise, I know of no way to get all these processes into comm_world.<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; On Apr 17, 2010, at 2:27 PM, Grzegorz Maj wrote:<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hi,<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; I&#39;d like to dynamically create a group of processes communicating via<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; MPI. Those processes need to be run without mpirun and create<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; intracommunicator after the startup. Any ideas how to do this<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; efficiently?<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; I came up with a solution in which the processes are connecting one by<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; one using MPI_Comm_connect, but unfortunately all the processes that<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; are already in the group need to call MPI_Comm_accept. This means that<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; when the n-th process wants to connect I need to collect all the n-1<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; processes on the MPI_Comm_accept call. After I run about 40 processes<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; every subsequent call takes more and more time, which I&#39;d like to<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; avoid.<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Another problem in this solution is that when I try to connect 66-th<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; process the root of the existing group segfaults on MPI_Comm_accept.<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Maybe it&#39;s my bug, but it&#39;s weird as everything works fine for at most<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 65 processes. Is there any limitation I don&#39;t know about?<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; My last question is about MPI_COMM_WORLD. When I run my processes<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; without mpirun their MPI_COMM_WORLD is the same as MPI_COMM_SELF. Is<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; there any way to change MPI_COMM_WORLD and set it to the<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; intracommunicator that I&#39;ve created?<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Thanks,<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Grzegorz Maj<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div>

