<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">pe=N tells us to map N cores (we call them “processing elements” because they could be HTs if you —use-hwthreads-as-cpus) to each process. So we will bind each process to N cores.<div class=""><br class=""></div><div class="">So if you want 16 procs, each with two processing elements assigned to them (which is a good choice if you are using 2 threads/process), then you would use:</div><div class=""><br class=""></div><div class="">mpirun -map-by core:pe=2 -np 16</div><div class=""><br class=""></div><div class="">If you add -report-bindings, you’ll see each process bound to two cores, with the procs tightly packed on each node until that node’s cores are fully utilized. We do handle the unlikely event that you asked for a non-integer multiple of cores - i.e., if you have 32 cores on a node, and you ask for pe=6, we will wind up leaving two cores idle.</div><div class=""><br class=""></div><div class="">HTH</div><div class="">Ralph</div><div class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Mar 25, 2016, at 11:11 AM, Ronald Cohen &lt;<a href="mailto:recohen3@gmail.com" class="">recohen3@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">or is it mpirun -map-by core:pe=8 -n 16 ?</span><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">---</span><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">Ron Cohen</span><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><a href="mailto:recohen3@gmail.com" style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class="">recohen3@gmail.com</a><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">skypename: ronaldcohen</span><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">twitter: @recohen3</span><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">On Fri, Mar 25, 2016 at 2:10 PM, Ronald Cohen &lt;</span><a href="mailto:recohen3@gmail.com" style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class="">recohen3@gmail.com</a><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">&gt; wrote:</span><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><blockquote type="cite" style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class="">Thank you--I looked on the man page and it is not clear to me what<br class="">pe=2 does. Is that the number of threads? So if I want 16 mpi procs<br class="">with 2 threads is it for 32 cores (two nodes)<br class=""><br class="">mpirun -map-by core:pe=2 -n 16<br class=""><br class="">?<br class=""><br class="">Sorry if I mangled this.<br class=""><br class=""><br class="">Ron<br class=""><br class="">---<br class="">Ron Cohen<br class=""><a href="mailto:recohen3@gmail.com" class="">recohen3@gmail.com</a><br class="">skypename: ronaldcohen<br class="">twitter: @recohen3<br class=""><br class=""><br class="">On Fri, Mar 25, 2016 at 2:03 PM, Ralph Castain &lt;rhc@open-mpi.org&gt; wrote:<br class=""><blockquote type="cite" class="">Okay, what I would suggest is that you use the following cmd line:<br class=""><br class="">mpirun -map-by core:pe=2 (or 8 or whatever number you want)<br class=""><br class="">This should give you the best performance as it will tight-pack the procs and assign them to the correct number of cores. See if that helps<br class=""><br class=""><blockquote type="cite" class="">On Mar 25, 2016, at 10:38 AM, Ronald Cohen &lt;recohen3@gmail.com&gt; wrote:<br class=""><br class="">1.10.2<br class=""><br class="">Ron<br class=""><br class="">---<br class="">Ron Cohen<br class="">recohen3@gmail.com<br class="">skypename: ronaldcohen<br class="">twitter: @recohen3<br class=""><br class=""><br class="">On Fri, Mar 25, 2016 at 1:30 PM, Ralph Castain &lt;rhc@open-mpi.org&gt; wrote:<br class=""><blockquote type="cite" class="">Hmmm…what version of OMPI are you using?<br class=""><br class=""><br class="">On Mar 25, 2016, at 10:27 AM, Ronald Cohen &lt;recohen3@gmail.com&gt; wrote:<br class=""><br class="">--report-bindings didn't report anything<br class="">---<br class="">Ron Cohen<br class="">recohen3@gmail.com<br class="">skypename: ronaldcohen<br class="">twitter: @recohen3<br class=""><br class=""><br class="">On Fri, Mar 25, 2016 at 1:24 PM, Ronald Cohen &lt;recohen3@gmail.com&gt; wrote:<br class=""><br class="">—display-allocation an<br class="">didn't seem to give useful information:<br class=""><br class="">====================== &nbsp;&nbsp;ALLOCATED NODES &nbsp;&nbsp;======================<br class="">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n005: slots=16 max_slots=0 slots_inuse=0 state=UP<br class="">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n008.cluster.com: slots=16 max_slots=0 slots_inuse=0 state=UP<br class="">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n007.cluster.com: slots=16 max_slots=0 slots_inuse=0 state=UP<br class="">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n006.cluster.com: slots=16 max_slots=0 slots_inuse=0 state=UP<br class="">=================================================================<br class=""><br class="">for<br class="">mpirun -display-allocation &nbsp;--map-by ppr:8:node -n 32<br class=""><br class="">Ron<br class=""><br class="">---<br class="">Ron Cohen<br class="">recohen3@gmail.com<br class="">skypename: ronaldcohen<br class="">twitter: @recohen3<br class=""><br class=""><br class="">On Fri, Mar 25, 2016 at 1:17 PM, Ronald Cohen &lt;recohen3@gmail.com&gt; wrote:<br class=""><br class="">Actually there was the same number of procs per node in each case. I<br class="">verified this by logging into the nodes while they were running--in<br class="">both cases 4 per node .<br class=""><br class="">Ron<br class=""><br class="">---<br class="">Ron Cohen<br class="">recohen3@gmail.com<br class="">skypename: ronaldcohen<br class="">twitter: @recohen3<br class=""><br class=""><br class="">On Fri, Mar 25, 2016 at 1:14 PM, Ralph Castain &lt;rhc@open-mpi.org&gt; wrote:<br class=""><br class=""><br class="">On Mar 25, 2016, at 9:59 AM, Ronald Cohen &lt;recohen3@gmail.com&gt; wrote:<br class=""><br class="">It is very strange but my program runs slower with any of these<br class="">choices than if IO simply use:<br class=""><br class="">mpirun &nbsp;-n 16<br class="">with<br class="">#PBS -l<br class="">nodes=n013.cluster.com:ppn=4+n014.cluster.com:ppn=4+n015.cluster.com:ppn=4+n016.cluster.com:ppn=4<br class="">for example.<br class=""><br class=""><br class="">This command will tightly pack as many procs as possible on a node - note<br class="">that we may well not see the PBS directives regarding number of ppn. Add<br class="">—display-allocation and let’s see how many slots we think were assigned on<br class="">each node<br class=""><br class=""><br class="">The timing for the latter is 165 seconds, and for<br class="">#PBS -l nodes=4:ppn=16,pmem=1gb<br class="">mpirun &nbsp;--map-by ppr:4:node -n 16<br class="">it is 368 seconds.<br class=""><br class=""><br class="">It will typically be faster if you pack more procs/node as they can use<br class="">shared memory for communication.<br class=""><br class=""><br class="">Ron<br class=""><br class="">---<br class="">Ron Cohen<br class="">recohen3@gmail.com<br class="">skypename: ronaldcohen<br class="">twitter: @recohen3<br class=""><br class=""><br class="">On Fri, Mar 25, 2016 at 12:43 PM, Ralph Castain &lt;rhc@open-mpi.org&gt; wrote:<br class=""><br class=""><br class="">On Mar 25, 2016, at 9:40 AM, Ronald Cohen &lt;recohen3@gmail.com&gt; wrote:<br class=""><br class="">Thank you! I will try it!<br class=""><br class=""><br class="">What would<br class="">-cpus-per-proc &nbsp;4 -n 16<br class="">do?<br class=""><br class=""><br class="">This would bind each process to 4 cores, filling each node with procs until<br class="">the cores on that node were exhausted, to a total of 16 processes within the<br class="">allocation.<br class=""><br class=""><br class="">Ron<br class="">---<br class="">Ron Cohen<br class="">recohen3@gmail.com<br class="">skypename: ronaldcohen<br class="">twitter: @recohen3<br class=""><br class=""><br class="">On Fri, Mar 25, 2016 at 12:38 PM, Ralph Castain &lt;rhc@open-mpi.org&gt; wrote:<br class=""><br class="">Add -rank-by node to your cmd line. You’ll still get 4 procs/node, but they<br class="">will be ranked by node instead of consecutively within a node.<br class=""><br class=""><br class=""><br class="">On Mar 25, 2016, at 9:30 AM, Ronald Cohen &lt;recohen3@gmail.com&gt; wrote:<br class=""><br class="">I am using<br class=""><br class="">mpirun &nbsp;--map-by ppr:4:node -n 16<br class=""><br class="">and this loads the processes in round robin fashion. This seems to be<br class="">twice as slow for my code as loading them node by node, 4 processes<br class="">per node.<br class=""><br class="">How can I not load them round robin, but node by node?<br class=""><br class="">Thanks!<br class=""><br class="">Ron<br class=""><br class=""><br class="">---<br class="">Ron Cohen<br class="">recohen3@gmail.com<br class="">skypename: ronaldcohen<br class="">twitter: @recohen3<br class=""><br class="">---<br class="">Ronald Cohen<br class="">Geophysical Laboratory<br class="">Carnegie Institution<br class="">5251 Broad Branch Rd., N.W.<br class="">Washington, D.C. 20015<br class="">_______________________________________________<br class="">users mailing list<br class="">users@open-mpi.org<br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post:<br class="">http://www.open-mpi.org/community/lists/users/2016/03/28828.php<br class=""><br class=""><br class="">_______________________________________________<br class="">users mailing list<br class="">users@open-mpi.org<br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post:<br class="">http://www.open-mpi.org/community/lists/users/2016/03/28829.php<br class=""><br class="">_______________________________________________<br class="">users mailing list<br class="">users@open-mpi.org<br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post:<br class="">http://www.open-mpi.org/community/lists/users/2016/03/28830.php<br class=""><br class=""><br class="">_______________________________________________<br class="">users mailing list<br class="">users@open-mpi.org<br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post:<br class="">http://www.open-mpi.org/community/lists/users/2016/03/28831.php<br class=""><br class="">_______________________________________________<br class="">users mailing list<br class="">users@open-mpi.org<br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post:<br class="">http://www.open-mpi.org/community/lists/users/2016/03/28832.php<br class=""><br class=""><br class="">_______________________________________________<br class="">users mailing list<br class="">users@open-mpi.org<br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post:<br class="">http://www.open-mpi.org/community/lists/users/2016/03/28833.php<br class=""><br class="">_______________________________________________<br class="">users mailing list<br class="">users@open-mpi.org<br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post:<br class="">http://www.open-mpi.org/community/lists/users/2016/03/28837.php<br class=""><br class=""><br class=""><br class="">_______________________________________________<br class="">users mailing list<br class="">users@open-mpi.org<br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post:<br class="">http://www.open-mpi.org/community/lists/users/2016/03/28840.php<br class=""></blockquote>_______________________________________________<br class="">users mailing list<br class="">users@open-mpi.org<br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2016/03/28843.php<br class=""></blockquote><br class="">_______________________________________________<br class="">users mailing list<br class="">users@open-mpi.org<br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2016/03/28844.php<br class=""></blockquote></blockquote><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">_______________________________________________</span><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">users mailing list</span><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a></span><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></span><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">Link to this post:<span class="Apple-converted-space">&nbsp;</span></span><a href="http://www.open-mpi.org/community/lists/users/2016/03/28846.php" style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class="">http://www.open-mpi.org/community/lists/users/2016/03/28846.php</a></div></blockquote></div><br class=""></div></body></html>
