<html><head><meta http-equiv="Content-Type" content="text/html charset=windows-1252"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">FWIW: I don’t think this actually has anything to do with the #procs you are trying to run. Instead, I expect it has to do with confusion over how many cores it can bind across. When you tell it to use-hwthread-cpus, you are asking us to map processes to hwthreads, and not cores. I don’t know which nodes are which, but it could be that we are getting incorrect info somewhere.<div class=""><br class=""></div><div class="">Given that you are limiting the number of procs to the number of cores, is there some reason why you are asking us to use-hwthread-cpus? Why not just leave it at the default core level?</div><div class=""><br class=""></div><div class="">I also suspect that you would have no problems if you —bind-to none - does that in fact work?</div><div class=""><br class=""></div><div class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Jun 18, 2015, at 4:54 PM, Lane, William &lt;<a href="mailto:William.Lane@cshs.org" class="">William.Lane@cshs.org</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><div style="font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; direction: ltr; font-family: Tahoma; font-size: 10pt;" class=""><div style="direction: ltr; font-family: Tahoma; font-size: 10pt;" class="">I'm having a strange problem w/OpenMPI 1.8.6. If I run<br class="">my OpenMPI test code (compiled against OpenMPI 1.8.6<br class="">libraries) on &lt; 131 slots I get no issues. Anything over 131<br class="">errors out:<br class=""><br class="">mpirun -np 132 -report-bindings --prefix /hpc/apps/mpi/openmpi/1.8.6/ --hostfile hostfile-single --mca btl_tcp_if_include eth0 --hetero-nodes --use-hwthread-cpus /hpc/home/lanew/mpi/openmpi/ProcessColors3<br class=""><br class="">The hostfile has the number of slots restricted<br class="">to the number of cores, while the max-slots includes<br class="">the hyperthreading cores (e.g. csclprd3-0-0 slots=6<span class="Apple-converted-space">&nbsp;</span><br class="">max-slots=12).<br class=""><br class="">The nodes are a mix of IBM x3550 nodes some<br class="">are Sandybridges and others are older Xeons.<br class=""><br class="">I would like to add that the submit node from<br class="">which I am launching mpirun has the open files<br class="">soft limit (ulimit -a) set to 1024, while the hard limit<br class="">(ulimit -Ha) is set to 4096. I know open file limits<br class="">were an issue w/an older version of OpenMPI. The<br class="">compute nodes all have their hard open files limit<br class="">and soft open files limits set to 4096.<br class=""><br class="">Here's the output (csclprd3-0-13 is the last node<br class="">listed in the hostfile hostfile-single):<br class=""><br class="">[csclprd3-0-13:28765] Signal: Bus error (7)<br class="">[csclprd3-0-13:28765] Signal code: Non-existant physical address (2)<br class="">[csclprd3-0-13:28765] Failing at address: 0x7f30002a8980<br class="">[csclprd3-0-13:28766] *** Process received signal ***<br class="">[csclprd3-0-13:28766] Signal: Bus error (7)<br class="">[csclprd3-0-13:28766] Signal code: Non-existant physical address (2)<br class="">[csclprd3-0-13:28766] Failing at address: 0x7fe137662880<br class="">[csclprd3-0-13:28768] *** Process received signal ***<br class="">[csclprd3-0-13:28768] Signal: Bus error (7)<br class="">[csclprd3-0-13:28768] Signal code: Non-existant physical address (2)<br class="">[csclprd3-0-13:28768] Failing at address: 0x7f9b40228a80<br class="">[csclprd3-0-13:28770] *** Process received signal ***<br class="">[csclprd3-0-13:28770] Signal: Bus error (7)<br class="">[csclprd3-0-13:28770] Signal code: Non-existant physical address (2)<br class="">[csclprd3-0-13:28770] Failing at address: 0x7f0de7f2bb00<br class="">[csclprd3-0-13:28767] *** Process received signal ***<br class="">[csclprd3-0-13:28767] Signal: Bus error (7)<br class="">[csclprd3-0-13:28767] Signal code: Non-existant physical address (2)<br class="">[csclprd3-0-13:28767] Failing at address: 0x7f9b6c2e8980<br class="">[csclprd3-0-13:28764] *** Process received signal ***<br class="">[csclprd3-0-13:28764] Signal: Bus error (7)<br class="">[csclprd3-0-13:28764] Signal code: Non-existant physical address (2)<br class="">[csclprd3-0-13:28765] Signal: Bus error (7)<br class="">[csclprd3-0-13:28765] Signal code: Non-existant physical address (2)<br class="">[csclprd3-0-13:28765] Failing at address: 0x7f30002a8980<br class="">[csclprd3-0-13:28766] *** Process received signal ***<br class="">[csclprd3-0-13:28766] Signal: Bus error (7)<br class="">[csclprd3-0-13:28766] Signal code: Non-existant physical address (2)<br class="">[csclprd3-0-13:28766] Failing at address: 0x7fe137662880<br class="">[csclprd3-0-13:28768] *** Process received signal ***<br class="">[csclprd3-0-13:28768] Signal: Bus error (7)<br class="">[csclprd3-0-13:28768] Signal code: Non-existant physical address (2)<br class="">[csclprd3-0-13:28768] Failing at address: 0x7f9b40228a80<br class="">[csclprd3-0-13:28770] *** Process received signal ***<br class="">[csclprd3-0-13:28770] Signal: Bus error (7)<br class="">[csclprd3-0-13:28770] Signal code: Non-existant physical address (2)<br class="">[csclprd3-0-13:28770] Failing at address: 0x7f0de7f2bb00<br class="">[csclprd3-0-13:28767] *** Process received signal ***<br class="">[csclprd3-0-13:28767] Signal: Bus error (7)<br class="">[csclprd3-0-13:28767] Signal code: Non-existant physical address (2)<br class="">[csclprd3-0-13:28767] Failing at address: 0x7f9b6c2e8980<br class="">[csclprd3-0-13:28764] *** Process received signal ***<br class="">[csclprd3-0-13:28764] Signal: Bus error (7)<br class="">[csclprd3-0-13:28764] Signal code: Non-existant physical address (2)<br class="">[csclprd3-0-13:28768] [ 3] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(ompi_free_list_resize_mt+0x40)[0x7f9b513ad110]<br class="">[csclprd3-0-13:28768] [ 4] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(ompi_free_list_grow+0x219)[0x7f0df77b6009]<br class="">[csclprd3-0-13:28770] [ 3] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(ompi_free_list_resize_mt+0x40)[0x7f0df77b6110]<br class="">[csclprd3-0-13:28770] [ 4] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(+0xc568e)[0x7f9b5141d68e]<br class="">[csclprd3-0-13:28768] [ 5] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(mca_pml_ob1_add_procs+0xd5)[0x7f9b514f1715]<br class="">[csclprd3-0-13:28768] [ 6] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(+0xc568e)[0x7f30115ea68e]<br class="">[csclprd3-0-13:28765] [ 5] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(mca_pml_ob1_add_procs+0xd5)[0x7f30116be715]<br class="">[csclprd3-0-13:28765] [ 6] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(+0xc568e)[0x7f9b7bb3b68e]<br class="">[csclprd3-0-13:28767] [ 5] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(mca_pml_ob1_add_procs+0xd5)[0x7f9b7bc0f715]<br class="">[csclprd3-0-13:28767] [ 6] [csclprd3-0-13:28764] [ 4] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(+0xc568e)[0x7fa946bb768e]<br class="">[csclprd3-0-13:28764] [ 5] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(+0xc568e)[0x7fe146d4068e]<br class="">[csclprd3-0-13:28766] [ 5] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(+0xc568e)[0x7f0df782668e]<br class="">[csclprd3-0-13:28770] [ 5] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(mca_pml_ob1_add_procs+0xd5)[0x7f0df78fa715]<br class="">[csclprd3-0-13:28770] [ 6] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(ompi_mpi_init+0x8d6)[0x7f0df77d0ad6]<br class="">[csclprd3-0-13:28770] [ 7] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(mca_pml_ob1_add_procs+0xd5)[0x7fe146e14715]<br class="">[csclprd3-0-13:28766] [ 6] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(ompi_mpi_init+0x8d6)[0x7fe146ceaad6]<br class="">[csclprd3-0-13:28766] [ 7] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(ompi_mpi_init+0x8d6)[0x7f9b513c7ad6]<br class="">[csclprd3-0-13:28768] [ 7] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(MPI_Init+0x170)[0x7f9b513e6c60]<br class="">[csclprd3-0-13:28768] [ 8] /hpc/home/lanew/mpi/openmpi/ProcessColors3[0x400ad0]<br class="">[csclprd3-0-13:28768] [ 9] /lib64/libc.so.6(__libc_start_main+0xfd)[0x7f9b50dc7cdd]<br class="">[csclprd3-0-13:28768] [10] /hpc/home/lanew/mpi/openmpi/ProcessColors3[0x400999]<br class="">[csclprd3-0-13:28768] *** End of error message ***<br class="">/hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(ompi_mpi_init+0x8d6)[0x7f3011594ad6]<br class="">[csclprd3-0-13:28765] [ 7] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(MPI_Init+0x170)[0x7f30115b3c60]<br class="">[csclprd3-0-13:28765] [ 8] /hpc/home/lanew/mpi/openmpi/ProcessColors3[0x400ad0]<br class="">[csclprd3-0-13:28765] [ 9] /lib64/libc.so.6(__libc_start_main+0xfd)[0x7f3010f94cdd]<br class="">[csclprd3-0-13:28765] [10] /hpc/home/lanew/mpi/openmpi/ProcessColors3[0x400999]<br class="">[csclprd3-0-13:28765] *** End of error message ***<br class="">/hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(ompi_mpi_init+0x8d6)[0x7f9b7bae5ad6]<br class="">[csclprd3-0-13:28767] [ 7] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(MPI_Init+0x170)[0x7f9b7bb04c60]<br class="">[csclprd3-0-13:28767] [ 8] /hpc/home/lanew/mpi/openmpi/ProcessColors3[0x400ad0]<br class="">[csclprd3-0-13:28767] [ 9] /lib64/libc.so.6(__libc_start_main+0xfd)[0x7f9b7b4e5cdd]<br class="">[csclprd3-0-13:28767] [10] /hpc/home/lanew/mpi/openmpi/ProcessColors3[0x400999]<br class="">[csclprd3-0-13:28767] *** End of error message ***<br class="">/hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(mca_pml_ob1_add_procs+0xd5)[0x7fa946c8b715]<br class="">[csclprd3-0-13:28764] [ 6] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(ompi_mpi_init+0x8d6)[0x7fa946b61ad6]<br class="">[csclprd3-0-13:28764] [ 7] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(MPI_Init+0x170)[0x7f0df77efc60]<br class="">[csclprd3-0-13:28770] [ 8] /hpc/home/lanew/mpi/openmpi/ProcessColors3[0x400ad0]<br class="">[csclprd3-0-13:28770] [ 9] /lib64/libc.so.6(__libc_start_main+0xfd)[0x7f0df71d0cdd]<br class="">[csclprd3-0-13:28770] [10] /hpc/home/lanew/mpi/openmpi/ProcessColors3[0x400999]<br class="">[csclprd3-0-13:28770] *** End of error message ***<br class="">/hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(MPI_Init+0x170)[0x7fe146d09c60]<br class="">[csclprd3-0-13:28766] [ 8] /hpc/home/lanew/mpi/openmpi/ProcessColors3[0x400ad0]<br class="">[csclprd3-0-13:28766] [ 9] /lib64/libc.so.6(__libc_start_main+0xfd)[0x7fe1466eacdd]<br class="">[csclprd3-0-13:28767] *** End of error message ***<br class="">/hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(mca_pml_ob1_add_procs+0xd5)[0x7fa946c8b715]<br class="">[csclprd3-0-13:28764] [ 6] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(ompi_mpi_init+0x8d6)[0x7fa946b61ad6]<br class="">[csclprd3-0-13:28764] [ 7] /hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(MPI_Init+0x170)[0x7f0df77efc60]<br class="">[csclprd3-0-13:28770] [ 8] /hpc/home/lanew/mpi/openmpi/ProcessColors3[0x400ad0]<br class="">[csclprd3-0-13:28770] [ 9] /lib64/libc.so.6(__libc_start_main+0xfd)[0x7f0df71d0cdd]<br class="">[csclprd3-0-13:28770] [10] /hpc/home/lanew/mpi/openmpi/ProcessColors3[0x400999]<br class="">[csclprd3-0-13:28770] *** End of error message ***<br class="">/hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(MPI_Init+0x170)[0x7fe146d09c60]<br class="">[csclprd3-0-13:28766] [ 8] /hpc/home/lanew/mpi/openmpi/ProcessColors3[0x400ad0]<br class="">[csclprd3-0-13:28766] [ 9] /lib64/libc.so.6(__libc_start_main+0xfd)[0x7fe1466eacdd]<br class="">[csclprd3-0-13:28766] [10] /hpc/home/lanew/mpi/openmpi/ProcessColors3[0x400999]<br class="">[csclprd3-0-13:28766] *** End of error message ***<br class="">/hpc/apps/mpi/openmpi/1.8.6/lib/libmpi.so.1(MPI_Init+0x170)[0x7fa946b80c60]<br class="">[csclprd3-0-13:28764] [ 8] /hpc/home/lanew/mpi/openmpi/ProcessColors3[0x400ad0]<br class="">[csclprd3-0-13:28764] [ 9] /lib64/libc.so.6(__libc_start_main+0xfd)[0x7fa946561cdd]<br class="">[csclprd3-0-13:28764] [10] /hpc/home/lanew/mpi/openmpi/ProcessColors3[0x400999]<br class="">[csclprd3-0-13:28764] *** End of error message ***<br class="">--------------------------------------------------------------------------<br class="">mpirun noticed that process rank 126 with PID 0 on node csclprd3-0-13 exited on signal 7 (Bus error).<br class=""><br class="">Could a lack of the necessary NUMA libraries or the wrong version of NUMA<br class="">libraries be contributing to this?<br class=""></div></div><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">IMPORTANT WARNING: This message is intended for the use of the person or entity to which it is addressed and may contain information that is privileged and confidential, the disclosure of which is governed by applicable law. If the reader of this message is not the intended recipient, or the employee or agent responsible for delivering it to the intended recipient, you are hereby notified that any dissemination, distribution or copying of this information is strictly prohibited. Thank you for your cooperation. _______________________________________________</span><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">users mailing list</span><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><a href="mailto:users@open-mpi.org" style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class="">users@open-mpi.org</a><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">Subscription:<span class="Apple-converted-space">&nbsp;</span></span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">Link to this post:<span class="Apple-converted-space">&nbsp;</span></span><a href="http://www.open-mpi.org/community/lists/users/2015/06/27159.php" style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class="">http://www.open-mpi.org/community/lists/users/2015/06/27159.php</a></div></blockquote></div><br class=""></div></body></html>
