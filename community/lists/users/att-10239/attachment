<font size=2 face="sans-serif">Hi Terry,</font>
<br>
<br><font size=2 face="sans-serif">&nbsp; &nbsp; &nbsp; &nbsp; I
feel hierarchical collectives are slower compare to tuned one. I had done
some benchmark in the past specific to collectives, and this is what i
feel based on my observation.</font>
<br>
<br><font size=2 face="sans-serif">Regards<br>
<br>
Neeraj Chourasia (MTS)<br>
Computational Research Laboratories Ltd.<br>
(A wholly Owned Subsidiary of TATA SONS Ltd)<br>
B-101, ICC Trade Towers, Senapati Bapat Road<br>
Pune 411016 (Mah) INDIA<br>
(O) +91-20-6620 9863 &nbsp;(Fax) +91-20-6620 9862<br>
M: +91.9225520634<br>
</font>
<br>
<br>
<br>
<table width=100%>
<tr valign=top>
<td width=40%><font size=1 face="sans-serif"><b>Terry Dontje &lt;Terry.Dontje@sun.com&gt;</b>
</font>
<br><font size=1 face="sans-serif">Sent by: users-bounces@open-mpi.org</font>
<p><font size=1 face="sans-serif">08/07/2009 04:35 PM</font>
<table border>
<tr valign=top>
<td bgcolor=white>
<div align=center><font size=1 face="sans-serif">Please respond to<br>
Open MPI Users &lt;users@open-mpi.org&gt;</font></div></table>
<br>
<td width=59%>
<table width=100%>
<tr valign=top>
<td>
<div align=right><font size=1 face="sans-serif">To</font></div>
<td><font size=1 face="sans-serif">users@open-mpi.org</font>
<tr valign=top>
<td>
<div align=right><font size=1 face="sans-serif">cc</font></div>
<td>
<tr valign=top>
<td>
<div align=right><font size=1 face="sans-serif">Subject</font></div>
<td><font size=1 face="sans-serif">Re: [OMPI users] Performance question
about OpenMPI and MVAPICH2 &nbsp; &nbsp; &nbsp; &nbsp;on &nbsp;
&nbsp; &nbsp; &nbsp;IB</font></table>
<br>
<table>
<tr valign=top>
<td>
<td></table>
<br></table>
<br>
<br>
<br><tt><font size=2>Craig,<br>
<br>
Did your affinity script bind the processes per socket or linearly to cores.
&nbsp;If the former you'll want to look at using rankfiles and place the
ranks based on sockets. &nbsp;TWe have found this especially useful if
you are not running fully subscribed on your machines.<br>
<br>
Also, if you think the main issue is collectives performance you may want
to try using the hierarchical and SM collectives. &nbsp;However, be forewarned
we are right now trying to pound out some errors with these modules. &nbsp;To
enable them you add the following parameters &quot;--mca coll_hierarch_priority
100 --mca coll_sm_priority 100&quot;. &nbsp;We would be very interested
in any results you get (failures, improvements, non-improvements).<br>
<br>
thanks,<br>
<br>
--td<br>
<br>
&gt; Message: 4<br>
&gt; Date: Thu, 06 Aug 2009 17:03:08 -0600<br>
&gt; From: Craig Tierney &lt;Craig.Tierney@noaa.gov&gt;<br>
&gt; Subject: Re: [OMPI users] Performance question about OpenMPI and<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MVAPICH2
on &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
IB<br>
&gt; To: Open MPI Users &lt;users@open-mpi.org&gt;<br>
&gt; Message-ID: &lt;4A7B612C.8070501@noaa.gov&gt;<br>
&gt; Content-Type: text/plain; charset=ISO-8859-1<br>
&gt;<br>
&gt; A followup....<br>
&gt;<br>
&gt; Part of problem was affinity. &nbsp;I had written a script to do processor<br>
&gt; and memory affinity (which works fine with MVAPICH2). &nbsp;It is
an<br>
&gt; idea that I got from TACC. &nbsp;However, the script didn't seem to<br>
&gt; work correctly with OpenMPI (or I still have bugs).<br>
&gt;<br>
&gt; Setting --mca mpi_paffinity_alone 1 made things better. &nbsp;However,<br>
&gt; the performance is still not as good:<br>
&gt;<br>
&gt; Cores &nbsp; Mvapich2 &nbsp; &nbsp;Openmpi<br>
&gt; ---------------------------<br>
&gt; &nbsp; &nbsp;8 &nbsp; &nbsp; &nbsp;17.3 &nbsp; &nbsp; &nbsp; &nbsp;17.3<br>
&gt; &nbsp; 16 &nbsp; &nbsp; &nbsp;31.7 &nbsp; &nbsp; &nbsp; &nbsp;31.5<br>
&gt; &nbsp; 32 &nbsp; &nbsp; &nbsp;62.9 &nbsp; &nbsp; &nbsp; &nbsp;62.8<br>
&gt; &nbsp; 64 &nbsp; &nbsp; 110.8 &nbsp; &nbsp; &nbsp; 108.0<br>
&gt; &nbsp;128 &nbsp; &nbsp; 219.2 &nbsp; &nbsp; &nbsp; 201.4<br>
&gt; &nbsp;256 &nbsp; &nbsp; 384.5 &nbsp; &nbsp; &nbsp; 342.7<br>
&gt; &nbsp;512 &nbsp; &nbsp; 687.2 &nbsp; &nbsp; &nbsp; 537.6<br>
&gt;<br>
&gt; The performance number is GFlops (so larger is better).<br>
&gt;<br>
&gt; The first few numbers show that the executable is the right<br>
&gt; speed. &nbsp;I verified that IB is being used by using OMB and<br>
&gt; checking latency and bandwidth. &nbsp;Those numbers are what I<br>
&gt; expect (3GB/s, 1.5mu/s for QDR).<br>
&gt;<br>
&gt; However, the Openmpi version is not scaling as well. &nbsp;Any<br>
&gt; ideas on why that might be the case?<br>
&gt;<br>
&gt; Thanks,<br>
&gt; Craig<br>
<br>
_______________________________________________<br>
users mailing list<br>
users@open-mpi.org<br>
</font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
</font></tt>
<br><p>=====-----=====-----=====



Notice: The information contained in this e-mail
message and/or attachments to it may contain 
confidential or privileged information. If you are 
not the intended recipient, any dissemination, use, 
review, distribution, printing or copying of the 
information contained in this e-mail message 
and/or attachments to it are strictly prohibited. If 
you have received this communication in error, 
please notify us by reply e-mail or telephone and 
immediately and permanently delete the message 
and any attachments. 

Internet communications cannot be guaranteed to be timely,
secure, error or virus-free. The sender does not accept liability
for any errors or omissions.Thank you

=====-----=====-----=====</p>

