<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">I'm a tad confused - this trace would appear to indicate that mpirun is failing, yes? Not your application?<div><br></div><div>The reason it works for local procs is that tm_init isn't called for that case - mpirun just fork/exec's the procs directly. When remote nodes are required, mpirun must connect to Torque. This is done with a call to:</div><div><br></div><div><div style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; font: normal normal normal 11px/normal Menlo; ">&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;ret = tm_init(<span style="color: #b70ea3">NULL</span>, &amp;tm_root);</div><div><font class="Apple-style-span" face="Menlo" size="3"><span class="Apple-style-span" style="font-size: 11px;"><br></span></font></div><div><font class="Apple-style-span" size="3"><span class="Apple-style-span" style="font-size: 12px;">My guess is that something changed in PBS Pro 10.2 to that API. Can you check the tm header file and see? I have no access to PBS any more, so I'll have to rely on your eyes to see a diff.</span></font></div><div><font class="Apple-style-span" size="3"><span class="Apple-style-span" style="font-size: 12px;"><br></span></font></div><div><font class="Apple-style-span" size="3"><span class="Apple-style-span" style="font-size: 12px;">Thanks</span></font></div><div><font class="Apple-style-span" size="3"><span class="Apple-style-span" style="font-size: 12px;">Ralph</span></font></div><div><font class="Apple-style-span" face="Menlo" size="3"><span class="Apple-style-span" style="font-size: 11px;"><br></span></font></div><div><div>On Feb 12, 2010, at 8:50 AM, Repsher, Stephen J wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div>Hello,<br><br>I'm having problems running Open MPI jobs under PBS Pro 10.2. &nbsp;I've configured and built OpenMPI 1.4.1 with the Intel 11.1 compiler on Linux and with --with-tm support and the build runs fine. &nbsp;I've also built with static libraries per the FAQ suggestion since libpbs is static. &nbsp;However, my test application keep failing with a segmentation fault, but ONLY when trying to select more than 1 node. &nbsp;Running on a single node withing PBS works fine. &nbsp;Also, running outside of PBS vis ssh runs fine as well, even across multiple nodes. &nbsp;OpenIB support is also enabled, but that doesn't seem to affect the error because I've also tried running with the --mca btl tcp,self flag and it still doesn't work. &nbsp;Here is the error I'm getting:<br><br>[n34:26892] *** Process received signal ***<br>[n34:26892] Signal: Segmentation fault (11)<br>[n34:26892] Signal code: Address not mapped (1)<br>[n34:26892] Failing at address: 0x3f<br>[n34:26892] [ 0] /lib64/libpthread.so.0 [0x7fc0309d6a90]<br>[n34:26892] [ 1] /part0/apps/MPI/intel/openmpi-1.4.1/bin/pbs_mpirun(discui_+0x84) [0x476a50]<br>[n34:26892] [ 2] /part0/apps/MPI/intel/openmpi-1.4.1/bin/pbs_mpirun(diswsi+0xc3) [0x474063]<br>[n34:26892] [ 3] /part0/apps/MPI/intel/openmpi-1.4.1/bin/pbs_mpirun [0x471d0c]<br>[n34:26892] [ 4] /part0/apps/MPI/intel/openmpi-1.4.1/bin/pbs_mpirun(tm_init+0x1fe) [0x471ff8]<br>[n34:26892] [ 5] /part0/apps/MPI/intel/openmpi-1.4.1/bin/pbs_mpirun [0x43f580]<br>[n34:26892] [ 6] /part0/apps/MPI/intel/openmpi-1.4.1/bin/pbs_mpirun [0x413921]<br>[n34:26892] [ 7] /part0/apps/MPI/intel/openmpi-1.4.1/bin/pbs_mpirun [0x412b78]<br>[n34:26892] [ 8] /lib64/libc.so.6(__libc_start_main+0xe6) [0x7fc03068d586]<br>[n34:26892] [ 9] /part0/apps/MPI/intel/openmpi-1.4.1/bin/pbs_mpirun [0x412ac9]<br>[n34:26892] *** End of error message ***<br>Segmentation fault<br><br>(NOTE: pbs_mpirun = orterun, mpirun, etc.)<br><br>Has anyone else seen errors like this within PBS?<br><br>============================================<br>Steve Repsher<br>Boeing Defense, Space, &amp; Security - Rotorcraft<br>Aerodynamics/CFD<br>Phone: (610) 591-1510<br>Fax: (610) 591-6263<br><a href="mailto:stephen.j.repsher@boeing.com">stephen.j.repsher@boeing.com</a><br><br><br><br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></div></blockquote></div><br></div></body></html>
