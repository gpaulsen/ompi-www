<html><body><div style="color:#000; background-color:#fff; font-family:times new roman, new york, times, serif;font-size:12pt"><div><span style="font-size: 12pt; ">No RoCE, Just native IB with TCP over the top.</span><br></div><div style="font-family: 'times new roman', 'new york', times, serif; font-size: 12pt; "><div style="font-family: 'times new roman', 'new york', times, serif; font-size: 12pt; "><div id="yiv1569473536"><div style="color: rgb(0, 0, 0); background-color: rgb(255, 255, 255); font-family: 'times new roman', 'new york', times, serif; font-size: 12pt; "><div style="color: rgb(0, 0, 0); font-size: 15.833333015441895px; font-family: 'times new roman', 'new york', times, serif; background-color: transparent; font-style: normal; "><span><br></span></div><div style="color: rgb(0, 0, 0); font-size: 15.833333015441895px; font-family: 'times new roman', 'new york', times, serif; background-color: transparent; font-style: normal; "><span>No I
 haven't used 1.6 I was trying to stick with the standards on the mellanox disk.</span></div><div style="color: rgb(0, 0, 0); font-size: 15.833333015441895px; font-family: 'times new roman', 'new york', times, serif; background-color: transparent; font-style: normal; "><span>Is there a known problem with 1.4.3 ?</span></div><div style="color: rgb(0, 0, 0); font-size: 15.833333015441895px; font-family: 'times new roman', 'new york', times, serif; background-color: transparent; font-style: normal; "><span><br></span></div>  <div style="font-family: 'times new roman', 'new york', times, serif; font-size: 12pt; "> <div style="font-family: 'times new roman', 'new york', times, serif; font-size: 12pt; "> <div dir="ltr"> <font size="2" face="Arial"> <hr size="1">  <b><span style="font-weight:bold;">From:</span></b> Yevgeny Kliteynik &lt;kliteyn@dev.mellanox.co.il&gt;<br> <b><span style="font-weight:bold;">To:</span></b> Randolph Pullen
 &lt;randolph_pullen@yahoo.com.au&gt;; Open MPI Users &lt;users@open-mpi.org&gt; <br> <b><span style="font-weight:bold;">Sent:</span></b> Sunday, 2 September 2012 10:54 PM<br> <b><span style="font-weight:bold;">Subject:</span></b> Re: [OMPI users] Infiniband performance Problem and stalling<br> </font> </div> <br>Randolph,<br><br>Some clarification on the setup:<br><br>"Melanox III HCA 10G
 cards" - are those ConnectX 3 cards configured to Ethernet?<br>That is, when you're using openib BTL, you mean RoCE, right?<br><br>Also, have you had a chance to try some newer OMPI release?<br>Any 1.6.x would do.<br><br><br>-- YK<br><br>On 8/31/2012 10:53 AM, Randolph Pullen wrote:<br>&gt; (reposted with consolidatedinformation)<br>&gt; I have a test rig comprising 2 i7 systems 8GB RAM with Melanox III HCA 10G cards<br>&gt; running Centos 5.7 Kernel 2.6.18-274<br>&gt; Open MPI 1.4.3<br>&gt; MLNX_OFED_LINUX-1.5.3-1.0.0.2 (OFED-1.5.3-1.0.0.2):<br>&gt; On a Cisco 24 pt switch<br>&gt; Normal performance is:<br>&gt; $ mpirun --mca btl openib,self -n 2 -hostfile mpi.hosts PingPong<br>&gt; results in:<br>&gt; Max rate = 958.388867 MB/sec Min latency = 4.529953 usec<br>&gt; and:<br>&gt; $ mpirun --mca btl tcp,self -n 2 -hostfile mpi.hosts PingPong<br>&gt; Max rate = 653.547293 MB/sec Min latency = 19.550323 usec<br>&gt; NetPipeMPI results show a max of 7.4
 Gb/s at 8388605 bytes which seems fine.<br>&gt; log_num_mtt =20 and log_mtts_per_seg params =2<br>&gt; My application exchanges about a gig of data between the processes with 2 sender and 2 consumer processes on each node with 1 additional controller process on the starting node.<br>&gt; The program splits the data into 64K blocks and uses non blocking sends and receives with busy/sleep loops to monitor progress until completion.<br>&gt; Each process owns a single buffer for these 64K blocks.<br>&gt; My problem is I see better performance under IPoIB then I do on native IB (RDMA_CM).<br>&gt; My understanding is that IPoIB is limited to about 1G/s so I am at a loss to know why it is faster.<br>&gt; These 2 configurations are equivelant (about 8-10 seconds per cycle)<br>&gt; mpirun --mca btl_openib_flags 2 --mca mpi_leave_pinned 1 --mca btl tcp,self -H vh2,vh1 -np 9 --bycore prog<br>&gt; mpirun --mca btl_openib_flags 3 --mca mpi_leave_pinned 1 --mca btl
 tcp,self -H vh2,vh1 -np 9 --bycore prog<br>&gt; And this one produces similar run times but seems to degrade with repeated cycles:<br>&gt; mpirun --mca btl_openib_eager_limit 64 --mca mpi_leave_pinned 1 --mca btl openib,self -H vh2,vh1 -np 9 --bycore prog<br>&gt; Other btl_openib_flags settings result in much lower performance.<br>&gt; Changing the first of the above configs to use openIB results in a 21 second run time at best. Sometimes it takes up to 5 minutes.<br>&gt; In all cases, OpenIB runs in twice the time it takes TCP,except if I push the small message max to 64K and force short messages. Then the openib times are the same as TCP and no faster.<br>&gt; With openib:<br>&gt; - Repeated cycles during a single run seem to slow down with each cycle<br>&gt; (usually by about 10 seconds).<br>&gt; - On occasions it seems to stall indefinitely, waiting on a single receive.<br>&gt; I'm still at a loss as to why. I canâ€™t find any errors logged during
 the runs.<br>&gt; Any ideas appreciated.<br>&gt; Thanks in advance,<br>&gt; Randolph<br>&gt; <br>&gt; <br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a rel="nofollow" ymailto="mailto:users@open-mpi.org" target="_blank" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a rel="nofollow" target="_blank" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><br><br> </div> </div>  </div></div><br><br> </div> </div>  </div></body></html>
