Siegmar,<div><br></div><div>please add this to your CFLAGS for the time being.</div><div><br></div><div>configure tries to detect which flags must be added for C99 support, and it seems</div><div>the test is not working for Solaris 10 and Oracle compilers.</div><div>this is no more a widely used environment, and I am not sure I can find the time to fix this</div><div>in a near future.</div><div><br></div><div><br>regarding the runtime issue, can you please describe your 4 hosts (os, endianness and bitness)</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles</div><div><br>On Wednesday, April 27, 2016, Siegmar Gross &lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;siegmar.gross@informatik.hs-fulda.de&#39;);" target="_blank">siegmar.gross@informatik.hs-fulda.de</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi Gilles,<br>
<br>
adding &quot;-std=c99&quot; to CFLAGS solves the problem with the missing library.<br>
Shall I add it permanently to my configure command or will you add it,<br>
so that I will not run into problems if you need the C11 standard later?<br>
<br>
&quot;spawn_multiple_master&quot; breaks with the same error that I reported<br>
yesterday for my gcc-version of Open MPI. Hopefully you can solve the<br>
problem as well.<br>
<br>
<br>
Kind regards and thank you very much for your help<br>
<br>
Siegmar<br>
<br>
<br>
Am 27.04.2016 um 08:05 schrieb Gilles Gouaillardet:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Siegmar,<br>
<br>
<br>
here is the error :<br>
<br>
configure:17969: cc -o conftest -m64 -D_REENTRANT -g  -g<br>
-I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1<br>
-I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1-SunOS.sparc.64_cc<br>
-I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1/opal/include<br>
-I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1-SunOS.sparc.64_cc/opal/include<br>
-D_REENTRANT<br>
-I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1/opal/mca/hwloc/hwloc1112/hwloc/include<br>
-I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1-SunOS.sparc.64_cc/opal/mca/hwloc/hwloc1112/hwloc/include<br>
-I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1/opal/mca/event/libevent2022/libevent<br>
-I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1/opal/mca/event/libevent2022/libevent/include<br>
-I/export2/src/openmpi-2.0.0/openmpi-v2.x-dev-1290-gbd0e4e1-SunOS.sparc.64_cc/opal/mca/event/libevent2022/libevent/include<br>
-m64 conftest.c  &gt;&amp;5<br>
&quot;/usr/include/stdbool.h&quot;, line 42: #error: &quot;Use of &lt;stdbool.h&gt; is valid only<br>
in a c99 compilation environment.&quot;<br>
<br>
<br>
i cannot reproduce this on solaris 11 with oracle studio 5.3 compiler, and i<br>
do not have solaris 10 yet.<br>
<br>
could you please re-configure with &#39;-std=c99&#39; appended to your CFLAGS and see<br>
if it helps ?<br>
<br>
<br>
Cheers,<br>
<br>
<br>
Gilles<br>
<br>
<br>
On 4/26/2016 7:57 PM, Siegmar Gross wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Hi Gilles and Ralph,<br>
<br>
I was able to sort out my mess. In my last email I compared the<br>
files from &quot;SunOS_sparc/openmpi-2.0.0_64_gcc/lib64/openmpi&quot; from<br>
the attachment of my email to Ralph with the files from<br>
&quot;SunOS_sparc/openmpi-2.0.0_64_cc/lib64/openmpi&quot; from my current<br>
file system. That&#39;s the reason while I have had different<br>
timestamps. The other problem was that Ralph didn&#39;t recognize<br>
that &quot;mca_pmix_pmix112.so&quot; wasn&#39;t built on Solaris with the<br>
Sun C compiler. I&#39;ve removed most of the files from the attachment<br>
of my email so that it is easier to see the relevant files. Below<br>
I try to give you more information that may be relevant to track<br>
down the problem. I still get an error running one of my small<br>
test programs, when I use my gcc-version of Open MPI.<br>
&quot;mca_pmix_pmix112.so&quot; is a 64 bits library.<br>
<br>
Linux_x86_64/openmpi-2.0.0_64_cc/lib64/openmpi:<br>
...<br>
-rwxr-xr-x 1 root root  261327 Apr 19 16:46 mca_plm_slurm.so<br>
-rwxr-xr-x 1 root root    1002 Apr 19 16:45 <a href="http://mca_pmix_pmix112.la" target="_blank">mca_pmix_pmix112.la</a><br>
-rwxr-xr-x 1 root root 3906526 Apr 19 16:45 mca_pmix_pmix112.so<br>
-rwxr-xr-x 1 root root     966 Apr 19 16:51 <a href="http://mca_pml_cm.la" target="_blank">mca_pml_cm.la</a><br>
-rwxr-xr-x 1 root root 1574265 Apr 19 16:51 mca_pml_cm.so<br>
...<br>
<br>
Linux_x86_64/openmpi-2.0.0_64_gcc/lib64/openmpi:<br>
...<br>
-rwxr-xr-x 1 root root   70371 Apr 19 16:43 mca_plm_slurm.so<br>
-rwxr-xr-x 1 root root    1008 Apr 19 16:42 <a href="http://mca_pmix_pmix112.la" target="_blank">mca_pmix_pmix112.la</a><br>
-rwxr-xr-x 1 root root 1029005 Apr 19 16:42 mca_pmix_pmix112.so<br>
-rwxr-xr-x 1 root root     972 Apr 19 16:46 <a href="http://mca_pml_cm.la" target="_blank">mca_pml_cm.la</a><br>
-rwxr-xr-x 1 root root  284858 Apr 19 16:46 mca_pml_cm.so<br>
...<br>
<br>
SunOS_sparc/openmpi-2.0.0_64_cc/lib64/openmpi:<br>
...<br>
-rwxr-xr-x 1 root root  319816 Apr 19 19:58 mca_plm_rsh.so<br>
-rwxr-xr-x 1 root root     970 Apr 19 20:00 <a href="http://mca_pml_cm.la" target="_blank">mca_pml_cm.la</a><br>
-rwxr-xr-x 1 root root 1507440 Apr 19 20:00 mca_pml_cm.so<br>
...<br>
<br>
SunOS_sparc/openmpi-2.0.0_64_gcc/lib64/openmpi:<br>
...<br>
-rwxr-xr-x 1 root root  153280 Apr 19 19:49 mca_plm_rsh.so<br>
-rwxr-xr-x 1 root root    1007 Apr 19 19:47 <a href="http://mca_pmix_pmix112.la" target="_blank">mca_pmix_pmix112.la</a><br>
-rwxr-xr-x 1 root root 1400512 Apr 19 19:47 mca_pmix_pmix112.so<br>
-rwxr-xr-x 1 root root     971 Apr 19 19:52 <a href="http://mca_pml_cm.la" target="_blank">mca_pml_cm.la</a><br>
-rwxr-xr-x 1 root root  342440 Apr 19 19:52 mca_pml_cm.so<br>
...<br>
<br>
SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/openmpi:<br>
...<br>
-rwxr-xr-x 1 root root  300096 Apr 19 17:18 mca_plm_rsh.so<br>
-rwxr-xr-x 1 root root     970 Apr 19 17:23 <a href="http://mca_pml_cm.la" target="_blank">mca_pml_cm.la</a><br>
-rwxr-xr-x 1 root root 1458816 Apr 19 17:23 mca_pml_cm.so<br>
...<br>
<br>
SunOS_x86_64/openmpi-2.0.0_64_gcc/lib64/openmpi:<br>
...<br>
-rwxr-xr-x 1 root root  133096 Apr 19 17:42 mca_plm_rsh.so<br>
-rwxr-xr-x 1 root root    1007 Apr 19 17:41 <a href="http://mca_pmix_pmix112.la" target="_blank">mca_pmix_pmix112.la</a><br>
-rwxr-xr-x 1 root root 1320240 Apr 19 17:41 mca_pmix_pmix112.so<br>
-rwxr-xr-x 1 root root     971 Apr 19 17:46 <a href="http://mca_pml_cm.la" target="_blank">mca_pml_cm.la</a><br>
-rwxr-xr-x 1 root root  419848 Apr 19 17:46 mca_pml_cm.so<br>
...<br>
<br>
<br>
Yesterday I&#39;ve installed openmpi-v2.x-dev-1290-gbd0e4e1 so that we<br>
have a current version for the investigation of the problem. Once<br>
more mca_pmix_pmix112.so isn&#39;t available on Solaris if I use the<br>
Sun C compiler.<br>
<br>
&quot;config.log&quot; for gcc-5.1.0 shows the following.<br>
<br>
...<br>
configure:127799: /bin/bash<br>
&#39;../../../../../../openmpi-v2.x-dev-1290-gbd0e4e1/opal/mca/pmix/pmix112/<br>
pmix/configure&#39; succeeded for opal/mca/pmix/pmix112/pmix<br>
configure:127916: checking if MCA component pmix:pmix112 can compile<br>
configure:127918: result: yes<br>
configure:5637: --- MCA component pmix:external (m4 configuration macro)<br>
configure:128523: checking for MCA component pmix:external compile mode<br>
configure:128529: result: dso<br>
configure:129054: checking if MCA component pmix:external can compile<br>
configure:129056: result: no<br>
...<br>
config.status:3897: creating opal/mca/pmix/Makefile<br>
config.status:3897: creating opal/mca/pmix/s1/Makefile<br>
config.status:3897: creating opal/mca/pmix/cray/Makefile<br>
config.status:3897: creating opal/mca/pmix/s2/Makefile<br>
config.status:3897: creating opal/mca/pmix/pmix112/Makefile<br>
config.status:3897: creating opal/mca/pmix/external/Makefile<br>
...<br>
MCA_BUILD_opal_pmix_cray_DSO_FALSE=&#39;#&#39;<br>
MCA_BUILD_opal_pmix_cray_DSO_TRUE=&#39;&#39;<br>
MCA_BUILD_opal_pmix_external_DSO_FALSE=&#39;#&#39;<br>
MCA_BUILD_opal_pmix_external_DSO_TRUE=&#39;&#39;<br>
MCA_BUILD_opal_pmix_pmix112_DSO_FALSE=&#39;#&#39;<br>
MCA_BUILD_opal_pmix_pmix112_DSO_TRUE=&#39;&#39;<br>
MCA_BUILD_opal_pmix_s1_DSO_FALSE=&#39;#&#39;<br>
MCA_BUILD_opal_pmix_s1_DSO_TRUE=&#39;&#39;<br>
MCA_BUILD_opal_pmix_s2_DSO_FALSE=&#39;#&#39;<br>
MCA_BUILD_opal_pmix_s2_DSO_TRUE=&#39;&#39;<br>
...<br>
MCA_opal_FRAMEWORKS=&#39;common  allocator backtrace btl dl event hwloc if<br>
installdirs memchecker memcpy memory mpool pmix pstat rcache sec shmem timer&#39;<br>
MCA_opal_FRAMEWORKS_SUBDIRS=&#39;mca/common  mca/allocator mca/backtrace mca/btl<br>
mca/dl mca/event mca/hwloc mca/if mca/installdirs mca/memchecker mca/memcpy<br>
mca/memory mca/mpool mca/pmix mca/pstat mca/rcache mca/sec mca/shmem mca/timer&#39;<br>
MCA_opal_FRAMEWORK_COMPONENT_ALL_SUBDIRS=&#39;$(MCA_opal_common_ALL_SUBDIRS)<br>
$(MCA_opal_allocator_ALL_SUBDIRS) $(MCA_opal_backtrace_ALL_SUBDIRS)<br>
$(MCA_opal_btl_ALL_SUBDIRS) $(MCA_opal_dl_ALL_SUBDIRS)<br>
$(MCA_opal_event_ALL_SUBDIRS) $(MCA_opal_hwloc_ALL_SUBDIRS)<br>
$(MCA_opal_if_ALL_SUBDIRS) $(MCA_opal_installdirs_ALL_SUBDIRS)<br>
$(MCA_opal_memchecker_ALL_SUBDIRS) $(MCA_opal_memcpy_ALL_SUBDIRS)<br>
$(MCA_opal_memory_ALL_SUBDIRS) $(MCA_opal_mpool_ALL_SUBDIRS)<br>
$(MCA_opal_pmix_ALL_SUBDIRS) $(MCA_opal_pstat_ALL_SUBDIRS)<br>
$(MCA_opal_rcache_ALL_SUBDIRS) $(MCA_opal_sec_ALL_SUBDIRS)<br>
$(MCA_opal_shmem_ALL_SUBDIRS) $(MCA_opal_timer_ALL_SUBDIRS)&#39;<br>
MCA_opal_FRAMEWORK_COMPONENT_DSO_SUBDIRS=&#39;$(MCA_opal_common_DSO_SUBDIRS)<br>
$(MCA_opal_allocator_DSO_SUBDIRS) $(MCA_opal_backtrace_DSO_SUBDIRS)<br>
$(MCA_opal_btl_DSO_SUBDIRS) $(MCA_opal_dl_DSO_SUBDIRS)<br>
$(MCA_opal_event_DSO_SUBDIRS) $(MCA_opal_hwloc_DSO_SUBDIRS)<br>
$(MCA_opal_if_DSO_SUBDIRS) $(MCA_opal_installdirs_DSO_SUBDIRS)<br>
$(MCA_opal_memchecker_DSO_SUBDIRS) $(MCA_opal_memcpy_DSO_SUBDIRS)<br>
$(MCA_opal_memory_DSO_SUBDIRS) $(MCA_opal_mpool_DSO_SUBDIRS)<br>
$(MCA_opal_pmix_DSO_SUBDIRS) $(MCA_opal_pstat_DSO_SUBDIRS)<br>
$(MCA_opal_rcache_DSO_SUBDIRS) $(MCA_opal_sec_DSO_SUBDIRS)<br>
$(MCA_opal_shmem_DSO_SUBDIRS) $(MCA_opal_timer_DSO_SUBDIRS)&#39;<br>
MCA_opal_FRAMEWORK_COMPONENT_STATIC_SUBDIRS=&#39;$(MCA_opal_common_STATIC_SUBDIRS)<br>
 $(MCA_opal_allocator_STATIC_SUBDIRS) $(MCA_opal_backtrace_STATIC_SUBDIRS)<br>
$(MCA_opal_btl_STATIC_SUBDIRS) $(MCA_opal_dl_STATIC_SUBDIRS)<br>
$(MCA_opal_event_STATIC_SUBDIRS) $(MCA_opal_hwloc_STATIC_SUBDIRS)<br>
$(MCA_opal_if_STATIC_SUBDIRS) $(MCA_opal_installdirs_STATIC_SUBDIRS)<br>
$(MCA_opal_memchecker_STATIC_SUBDIRS) $(MCA_opal_memcpy_STATIC_SUBDIRS)<br>
$(MCA_opal_memory_STATIC_SUBDIRS) $(MCA_opal_mpool_STATIC_SUBDIRS)<br>
$(MCA_opal_pmix_STATIC_SUBDIRS) $(MCA_opal_pstat_STATIC_SUBDIRS)<br>
$(MCA_opal_rcache_STATIC_SUBDIRS) $(MCA_opal_sec_STATIC_SUBDIRS)<br>
$(MCA_opal_shmem_STATIC_SUBDIRS) $(MCA_opal_timer_STATIC_SUBDIRS)&#39;<br>
MCA_opal_FRAMEWORK_LIBS=&#39; $(MCA_opal_common_STATIC_LTLIBS)<br>
mca/allocator/<a href="http://libmca_allocator.la" target="_blank">libmca_allocator.la</a> $(MCA_opal_allocator_STATIC_LTLIBS)<br>
mca/backtrace/<a href="http://libmca_backtrace.la" target="_blank">libmca_backtrace.la</a> $(MCA_opal_backtrace_STATIC_LTLIBS)<br>
mca/btl/<a href="http://libmca_btl.la" target="_blank">libmca_btl.la</a> $(MCA_opal_btl_STATIC_LTLIBS) mca/dl/<a href="http://libmca_dl.la" target="_blank">libmca_dl.la</a><br>
$(MCA_opal_dl_STATIC_LTLIBS) mca/event/<a href="http://libmca_event.la" target="_blank">libmca_event.la</a><br>
$(MCA_opal_event_STATIC_LTLIBS) mca/hwloc/<a href="http://libmca_hwloc.la" target="_blank">libmca_hwloc.la</a><br>
$(MCA_opal_hwloc_STATIC_LTLIBS) mca/if/<a href="http://libmca_if.la" target="_blank">libmca_if.la</a><br>
$(MCA_opal_if_STATIC_LTLIBS) mca/installdirs/<a href="http://libmca_installdirs.la" target="_blank">libmca_installdirs.la</a><br>
$(MCA_opal_installdirs_STATIC_LTLIBS) mca/memchecker/<a href="http://libmca_memchecker.la" target="_blank">libmca_memchecker.la</a><br>
$(MCA_opal_memchecker_STATIC_LTLIBS) mca/memcpy/<a href="http://libmca_memcpy.la" target="_blank">libmca_memcpy.la</a><br>
$(MCA_opal_memcpy_STATIC_LTLIBS) mca/memory/<a href="http://libmca_memory.la" target="_blank">libmca_memory.la</a><br>
$(MCA_opal_memory_STATIC_LTLIBS) mca/mpool/<a href="http://libmca_mpool.la" target="_blank">libmca_mpool.la</a><br>
$(MCA_opal_mpool_STATIC_LTLIBS) mca/pmix/<a href="http://libmca_pmix.la" target="_blank">libmca_pmix.la</a><br>
$(MCA_opal_pmix_STATIC_LTLIBS) mca/pstat/<a href="http://libmca_pstat.la" target="_blank">libmca_pstat.la</a><br>
$(MCA_opal_pstat_STATIC_LTLIBS) mca/rcache/<a href="http://libmca_rcache.la" target="_blank">libmca_rcache.la</a><br>
$(MCA_opal_rcache_STATIC_LTLIBS) mca/sec/<a href="http://libmca_sec.la" target="_blank">libmca_sec.la</a><br>
$(MCA_opal_sec_STATIC_LTLIBS) mca/shmem/<a href="http://libmca_shmem.la" target="_blank">libmca_shmem.la</a><br>
$(MCA_opal_shmem_STATIC_LTLIBS) mca/timer/<a href="http://libmca_timer.la" target="_blank">libmca_timer.la</a><br>
$(MCA_opal_timer_STATIC_LTLIBS)&#39;<br>
...<br>
MCA_opal_pmix_ALL_COMPONENTS=&#39; s1 cray s2 pmix112 external&#39;<br>
MCA_opal_pmix_ALL_SUBDIRS=&#39; mca/pmix/s1 mca/pmix/cray mca/pmix/s2<br>
mca/pmix/pmix112 mca/pmix/external&#39;<br>
MCA_opal_pmix_DSO_COMPONENTS=&#39; pmix112&#39;<br>
MCA_opal_pmix_DSO_SUBDIRS=&#39; mca/pmix/pmix112&#39;<br>
MCA_opal_pmix_STATIC_COMPONENTS=&#39;&#39;<br>
MCA_opal_pmix_STATIC_LTLIBS=&#39;&#39;<br>
MCA_opal_pmix_STATIC_SUBDIRS=&#39;&#39;<br>
...<br>
opal_pmix_ext_CPPFLAGS=&#39;&#39;<br>
opal_pmix_ext_LDFLAGS=&#39;&#39;<br>
opal_pmix_ext_LIBS=&#39;&#39;<br>
opal_pmix_pmix112_CPPFLAGS=&#39;-I$(OPAL_TOP_BUILDDIR)/opal/mca/pmix/pmix112/pmix/include/pmix<br>
-I$(OPAL_TOP_BUILDDIR)/opal/mca/pmix/pmix112/pmix/include<br>
-I$(OPAL_TOP_BUILDDIR)/opal/mca/pmix/pmix112/pmix<br>
-I$(OPAL_TOP_SRCDIR)/opal/mca/pmix/pmix112/pmix&#39;<br>
opal_pmix_pmix112_LIBS=&#39;$(OPAL_TOP_BUILDDIR)/opal/mca/pmix/pmix112/pmix/<a href="http://libpmix.la" target="_blank">libpmix.la</a>&#39;<br>
<br>
...<br>
<br>
<br>
<br>
&quot;config.log&quot; for Sun C 5.13 shows the following.<br>
<br>
...<br>
configure:127803: /bin/bash<br>
&#39;../../../../../../openmpi-v2.x-dev-1290-gbd0e4e1/opal/mca/pmix/pmix112/<br>
pmix/configure&#39; *failed* for opal/mca/pmix/pmix112/pmix<br>
configure:128379: checking if MCA component pmix:pmix112 can compile<br>
configure:128381: result: no<br>
configure:5637: --- MCA component pmix:external (m4 configuration macro)<br>
configure:128523: checking for MCA component pmix:external compile mode<br>
configure:128529: result: dso<br>
configure:129054: checking if MCA component pmix:external can compile<br>
configure:129056: result: no<br>
...<br>
config.status:3887: creating opal/mca/pmix/Makefile<br>
config.status:3887: creating opal/mca/pmix/s1/Makefile<br>
config.status:3887: creating opal/mca/pmix/cray/Makefile<br>
config.status:3887: creating opal/mca/pmix/s2/Makefile<br>
config.status:3887: creating opal/mca/pmix/pmix112/Makefile<br>
config.status:3887: creating opal/mca/pmix/external/Makefile<br>
...<br>
MCA_BUILD_opal_pmix_cray_DSO_FALSE=&#39;#&#39;<br>
MCA_BUILD_opal_pmix_cray_DSO_TRUE=&#39;&#39;<br>
MCA_BUILD_opal_pmix_external_DSO_FALSE=&#39;#&#39;<br>
MCA_BUILD_opal_pmix_external_DSO_TRUE=&#39;&#39;<br>
MCA_BUILD_opal_pmix_pmix112_DSO_FALSE=&#39;#&#39;<br>
MCA_BUILD_opal_pmix_pmix112_DSO_TRUE=&#39;&#39;<br>
MCA_BUILD_opal_pmix_s1_DSO_FALSE=&#39;#&#39;<br>
MCA_BUILD_opal_pmix_s1_DSO_TRUE=&#39;&#39;<br>
MCA_BUILD_opal_pmix_s2_DSO_FALSE=&#39;#&#39;<br>
MCA_BUILD_opal_pmix_s2_DSO_TRUE=&#39;&#39;<br>
...<br>
MCA_opal_FRAMEWORKS=&#39;common  allocator backtrace btl dl event hwloc if<br>
installdirs memchecker memcpy memory mpool pmix pstat rcache sec shmem timer&#39;<br>
MCA_opal_FRAMEWORKS_SUBDIRS=&#39;mca/common  mca/allocator mca/backtrace mca/btl<br>
mca/dl mca/event mca/hwloc mca/if mca/installdirs mca/memchecker mca/memcpy<br>
mca/memory mca/mpool mca/pmix mca/pstat mca/rcache mca/sec mca/shmem mca/timer&#39;<br>
MCA_opal_FRAMEWORK_COMPONENT_ALL_SUBDIRS=&#39;$(MCA_opal_common_ALL_SUBDIRS)<br>
$(MCA_opal_allocator_ALL_SUBDIRS) $(MCA_opal_backtrace_ALL_SUBDIRS)<br>
$(MCA_opal_btl_ALL_SUBDIRS) $(MCA_opal_dl_ALL_SUBDIRS)<br>
$(MCA_opal_event_ALL_SUBDIRS) $(MCA_opal_hwloc_ALL_SUBDIRS)<br>
$(MCA_opal_if_ALL_SUBDIRS) $(MCA_opal_installdirs_ALL_SUBDIRS)<br>
$(MCA_opal_memchecker_ALL_SUBDIRS) $(MCA_opal_memcpy_ALL_SUBDIRS)<br>
$(MCA_opal_memory_ALL_SUBDIRS) $(MCA_opal_mpool_ALL_SUBDIRS)<br>
$(MCA_opal_pmix_ALL_SUBDIRS) $(MCA_opal_pstat_ALL_SUBDIRS)<br>
$(MCA_opal_rcache_ALL_SUBDIRS) $(MCA_opal_sec_ALL_SUBDIRS)<br>
$(MCA_opal_shmem_ALL_SUBDIRS) $(MCA_opal_timer_ALL_SUBDIRS)&#39;<br>
MCA_opal_FRAMEWORK_COMPONENT_DSO_SUBDIRS=&#39;$(MCA_opal_common_DSO_SUBDIRS)<br>
$(MCA_opal_allocator_DSO_SUBDIRS) $(MCA_opal_backtrace_DSO_SUBDIRS)<br>
$(MCA_opal_btl_DSO_SUBDIRS) $(MCA_opal_dl_DSO_SUBDIRS)<br>
$(MCA_opal_event_DSO_SUBDIRS) $(MCA_opal_hwloc_DSO_SUBDIRS)<br>
$(MCA_opal_if_DSO_SUBDIRS) $(MCA_opal_installdirs_DSO_SUBDIRS)<br>
$(MCA_opal_memchecker_DSO_SUBDIRS) $(MCA_opal_memcpy_DSO_SUBDIRS)<br>
$(MCA_opal_memory_DSO_SUBDIRS) $(MCA_opal_mpool_DSO_SUBDIRS)<br>
$(MCA_opal_pmix_DSO_SUBDIRS) $(MCA_opal_pstat_DSO_SUBDIRS)<br>
$(MCA_opal_rcache_DSO_SUBDIRS) $(MCA_opal_sec_DSO_SUBDIRS)<br>
$(MCA_opal_shmem_DSO_SUBDIRS) $(MCA_opal_timer_DSO_SUBDIRS)&#39;<br>
MCA_opal_FRAMEWORK_COMPONENT_STATIC_SUBDIRS=&#39;$(MCA_opal_common_STATIC_SUBDIRS)<br>
 $(MCA_opal_allocator_STATIC_SUBDIRS) $(MCA_opal_backtrace_STATIC_SUBDIRS)<br>
$(MCA_opal_btl_STATIC_SUBDIRS) $(MCA_opal_dl_STATIC_SUBDIRS)<br>
$(MCA_opal_event_STATIC_SUBDIRS) $(MCA_opal_hwloc_STATIC_SUBDIRS)<br>
$(MCA_opal_if_STATIC_SUBDIRS) $(MCA_opal_installdirs_STATIC_SUBDIRS)<br>
$(MCA_opal_memchecker_STATIC_SUBDIRS) $(MCA_opal_memcpy_STATIC_SUBDIRS)<br>
$(MCA_opal_memory_STATIC_SUBDIRS) $(MCA_opal_mpool_STATIC_SUBDIRS)<br>
$(MCA_opal_pmix_STATIC_SUBDIRS) $(MCA_opal_pstat_STATIC_SUBDIRS)<br>
$(MCA_opal_rcache_STATIC_SUBDIRS) $(MCA_opal_sec_STATIC_SUBDIRS)<br>
$(MCA_opal_shmem_STATIC_SUBDIRS) $(MCA_opal_timer_STATIC_SUBDIRS)&#39;<br>
MCA_opal_FRAMEWORK_LIBS=&#39; $(MCA_opal_common_STATIC_LTLIBS)<br>
mca/allocator/<a href="http://libmca_allocator.la" target="_blank">libmca_allocator.la</a> $(MCA_opal_allocator_STATIC_LTLIBS)<br>
mca/backtrace/<a href="http://libmca_backtrace.la" target="_blank">libmca_backtrace.la</a> $(MCA_opal_backtrace_STATIC_LTLIBS)<br>
mca/btl/<a href="http://libmca_btl.la" target="_blank">libmca_btl.la</a> $(MCA_opal_btl_STATIC_LTLIBS) mca/dl/<a href="http://libmca_dl.la" target="_blank">libmca_dl.la</a><br>
$(MCA_opal_dl_STATIC_LTLIBS) mca/event/<a href="http://libmca_event.la" target="_blank">libmca_event.la</a><br>
$(MCA_opal_event_STATIC_LTLIBS) mca/hwloc/<a href="http://libmca_hwloc.la" target="_blank">libmca_hwloc.la</a><br>
$(MCA_opal_hwloc_STATIC_LTLIBS) mca/if/<a href="http://libmca_if.la" target="_blank">libmca_if.la</a><br>
$(MCA_opal_if_STATIC_LTLIBS) mca/installdirs/<a href="http://libmca_installdirs.la" target="_blank">libmca_installdirs.la</a><br>
$(MCA_opal_installdirs_STATIC_LTLIBS) mca/memchecker/<a href="http://libmca_memchecker.la" target="_blank">libmca_memchecker.la</a><br>
$(MCA_opal_memchecker_STATIC_LTLIBS) mca/memcpy/<a href="http://libmca_memcpy.la" target="_blank">libmca_memcpy.la</a><br>
$(MCA_opal_memcpy_STATIC_LTLIBS) mca/memory/<a href="http://libmca_memory.la" target="_blank">libmca_memory.la</a><br>
$(MCA_opal_memory_STATIC_LTLIBS) mca/mpool/<a href="http://libmca_mpool.la" target="_blank">libmca_mpool.la</a><br>
$(MCA_opal_mpool_STATIC_LTLIBS) mca/pmix/<a href="http://libmca_pmix.la" target="_blank">libmca_pmix.la</a><br>
$(MCA_opal_pmix_STATIC_LTLIBS) mca/pstat/<a href="http://libmca_pstat.la" target="_blank">libmca_pstat.la</a><br>
$(MCA_opal_pstat_STATIC_LTLIBS) mca/rcache/<a href="http://libmca_rcache.la" target="_blank">libmca_rcache.la</a><br>
$(MCA_opal_rcache_STATIC_LTLIBS) mca/sec/<a href="http://libmca_sec.la" target="_blank">libmca_sec.la</a><br>
$(MCA_opal_sec_STATIC_LTLIBS) mca/shmem/<a href="http://libmca_shmem.la" target="_blank">libmca_shmem.la</a><br>
$(MCA_opal_shmem_STATIC_LTLIBS) mca/timer/<a href="http://libmca_timer.la" target="_blank">libmca_timer.la</a><br>
$(MCA_opal_timer_STATIC_LTLIBS)&#39;<br>
...<br>
MCA_opal_pmix_ALL_COMPONENTS=&#39; s1 cray s2 pmix112 external&#39;<br>
MCA_opal_pmix_ALL_SUBDIRS=&#39; mca/pmix/s1 mca/pmix/cray mca/pmix/s2<br>
mca/pmix/pmix112 mca/pmix/external&#39;<br>
MCA_opal_pmix_DSO_COMPONENTS=&#39;&#39;<br>
MCA_opal_pmix_DSO_SUBDIRS=&#39;&#39;<br>
MCA_opal_pmix_STATIC_COMPONENTS=&#39;&#39;<br>
MCA_opal_pmix_STATIC_LTLIBS=&#39;&#39;<br>
MCA_opal_pmix_STATIC_SUBDIRS=&#39;&#39;<br>
...<br>
opal_pmix_ext_CPPFLAGS=&#39;&#39;<br>
opal_pmix_ext_LDFLAGS=&#39;&#39;<br>
opal_pmix_ext_LIBS=&#39;&#39;<br>
opal_pmix_pmix112_CPPFLAGS=&#39;&#39;<br>
opal_pmix_pmix112_LIBS=&#39;&#39;<br>
...<br>
<br>
<br>
<br>
<br>
I&#39;ve attached the config.log files for pmix.<br>
<br>
tyr openmpi-2.0.0 142 tar zvft pmix_config.log.tar.gz<br>
-rw-r--r-- root/root    136291 2016-04-25 08:05:34<br>
openmpi-v2.x-dev-1290-gbd0e4e1-SunOS.sparc.64_cc/opal/mca/pmix/pmix112/pmix/config.log<br>
-rw-r--r-- root/root    528808 2016-04-25 08:07:54<br>
openmpi-v2.x-dev-1290-gbd0e4e1-SunOS.sparc.64_gcc/opal/mca/pmix/pmix112/pmix/config.log<br>
tyr openmpi-2.0.0 143<br>
<br>
<br>
<br>
I&#39;ve also attached the output for the broken execution of<br>
&quot;spawn_multiple_master&quot; for my gcc-version of Open MPI.<br>
&quot;spawn_master&quot; works as expected with my gcc-version of Open MPI.<br>
<br>
Hopefully you can fix the problem.<br>
<br>
<br>
Kind regards and thank you very much for your help<br>
<br>
Siegmar<br>
<br>
<br>
<br>
Am 23.04.2016 um 21:34 schrieb Siegmar Gross:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Hi Gilles,<br>
<br>
I don&#39;t know what happened, but the files are not available now<br>
and they were definitely available when I answered the email from<br>
Ralph. The files also have a different timestamp now. This is an<br>
extract from my email to Ralph for Solaris Sparc.<br>
<br>
-rwxr-xr-x 1 root root     977 Apr 19 19:49 <a href="http://mca_plm_rsh.la" target="_blank">mca_plm_rsh.la</a><br>
-rwxr-xr-x 1 root root  153280 Apr 19 19:49 mca_plm_rsh.so<br>
-rwxr-xr-x 1 root root    1007 Apr 19 19:47 <a href="http://mca_pmix_pmix112.la" target="_blank">mca_pmix_pmix112.la</a><br>
-rwxr-xr-x 1 root root 1400512 Apr 19 19:47 mca_pmix_pmix112.so<br>
-rwxr-xr-x 1 root root     971 Apr 19 19:52 <a href="http://mca_pml_cm.la" target="_blank">mca_pml_cm.la</a><br>
-rwxr-xr-x 1 root root  342440 Apr 19 19:52 mca_pml_cm.so<br>
<br>
Now I have the following output for these files.<br>
<br>
-rwxr-xr-x 1 root root     976 Apr 19 19:58 <a href="http://mca_plm_rsh.la" target="_blank">mca_plm_rsh.la</a><br>
-rwxr-xr-x 1 root root  319816 Apr 19 19:58 mca_plm_rsh.so<br>
-rwxr-xr-x 1 root root     970 Apr 19 20:00 <a href="http://mca_pml_cm.la" target="_blank">mca_pml_cm.la</a><br>
-rwxr-xr-x 1 root root 1507440 Apr 19 20:00 mca_pml_cm.so<br>
<br>
I&#39;ll try to find out what happened next week when I&#39;m back in<br>
my office.<br>
<br>
<br>
Kind regards<br>
<br>
Siegmar<br>
<br>
<br>
<br>
<br>
<br>
Am 23.04.16 um 02:12 schrieb Gilles Gouaillardet:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Siegmar,<br>
<br>
I will try to reproduce this on my solaris11 x86_64 vm<br>
<br>
In the mean time, can you please double check mca_pmix_pmix_pmix112.so<br>
is a 64 bits library ?<br>
(E.g, confirm &quot;-m64&quot; was correctly passed to pmix)<br>
<br>
Cheers,<br>
<br>
Gilles<br>
<br>
On Friday, April 22, 2016, Siegmar Gross<br>
&lt;<a>siegmar.gross@informatik.hs-fulda.de</a><br>
&lt;mailto:<a>siegmar.gross@informatik.hs-fulda.de</a>&gt;&gt; wrote:<br>
<br>
    Hi Ralph,<br>
<br>
    I&#39;ve already used &quot;-enable-debug&quot;. &quot;SYSTEM_ENV&quot; is &quot;SunOS&quot; or<br>
    &quot;Linux&quot; and &quot;MACHINE_ENV&quot; is &quot;sparc&quot; or &quot;x86_84&quot;.<br>
<br>
    mkdir openmpi-v2.x-dev-1280-gc110ae8-${SYSTEM_ENV}.${MACHINE_ENV}.64_gcc<br>
    cd openmpi-v2.x-dev-1280-gc110ae8-${SYSTEM_ENV}.${MACHINE_ENV}.64_gcc<br>
<br>
    ../openmpi-v2.x-dev-1280-gc110ae8/configure \<br>
      --prefix=/usr/local/openmpi-2.0.0_64_gcc \<br>
      --libdir=/usr/local/openmpi-2.0.0_64_gcc/lib64 \<br>
      --with-jdk-bindir=/usr/local/jdk1.8.0/bin \<br>
      --with-jdk-headers=/usr/local/jdk1.8.0/include \<br>
      JAVA_HOME=/usr/local/jdk1.8.0 \<br>
      LDFLAGS=&quot;-m64&quot; CC=&quot;gcc&quot; CXX=&quot;g++&quot; FC=&quot;gfortran&quot; \<br>
      CFLAGS=&quot;-m64&quot; CXXFLAGS=&quot;-m64&quot; FCFLAGS=&quot;-m64&quot; \<br>
      CPP=&quot;cpp&quot; CXXCPP=&quot;cpp&quot; \<br>
      --enable-mpi-cxx \<br>
      --enable-cxx-exceptions \<br>
      --enable-mpi-java \<br>
      --enable-heterogeneous \<br>
      --enable-mpi-thread-multiple \<br>
      --with-hwloc=internal \<br>
      --without-verbs \<br>
      --with-wrapper-cflags=&quot;-std=c11 -m64&quot; \<br>
      --with-wrapper-cxxflags=&quot;-m64&quot; \<br>
      --with-wrapper-fcflags=&quot;-m64&quot; \<br>
      --enable-debug \<br>
      |&amp; tee log.configure.$SYSTEM_ENV.$MACHINE_ENV.64_gcc<br>
<br>
<br>
    mkdir openmpi-v2.x-dev-1280-gc110ae8-${SYSTEM_ENV}.${MACHINE_ENV}.64_cc<br>
    cd openmpi-v2.x-dev-1280-gc110ae8-${SYSTEM_ENV}.${MACHINE_ENV}.64_cc<br>
<br>
    ../openmpi-v2.x-dev-1280-gc110ae8/configure \<br>
      --prefix=/usr/local/openmpi-2.0.0_64_cc \<br>
      --libdir=/usr/local/openmpi-2.0.0_64_cc/lib64 \<br>
      --with-jdk-bindir=/usr/local/jdk1.8.0/bin \<br>
      --with-jdk-headers=/usr/local/jdk1.8.0/include \<br>
      JAVA_HOME=/usr/local/jdk1.8.0 \<br>
      LDFLAGS=&quot;-m64&quot; CC=&quot;cc&quot; CXX=&quot;CC&quot; FC=&quot;f95&quot; \<br>
      CFLAGS=&quot;-m64&quot; CXXFLAGS=&quot;-m64 -library=stlport4&quot; FCFLAGS=&quot;-m64&quot; \<br>
      CPP=&quot;cpp&quot; CXXCPP=&quot;cpp&quot; \<br>
      --enable-mpi-cxx \<br>
      --enable-cxx-exceptions \<br>
      --enable-mpi-java \<br>
      --enable-heterogeneous \<br>
      --enable-mpi-thread-multiple \<br>
      --with-hwloc=internal \<br>
      --without-verbs \<br>
      --with-wrapper-cflags=&quot;-m64&quot; \<br>
      --with-wrapper-cxxflags=&quot;-m64 -library=stlport4&quot; \<br>
      --with-wrapper-fcflags=&quot;-m64&quot; \<br>
      --with-wrapper-ldflags=&quot;&quot; \<br>
      --enable-debug \<br>
      |&amp; tee log.configure.$SYSTEM_ENV.$MACHINE_ENV.64_cc<br>
<br>
<br>
    Kind regards<br>
<br>
    Siegmar<br>
<br>
    Am 21.04.2016 um 18:18 schrieb Ralph Castain:<br>
<br>
        Can you please rebuild OMPI with -enable-debug in the configure<br>
        cmd? It will let us see more error output<br>
<br>
<br>
            On Apr 21, 2016, at 8:52 AM, Siegmar Gross<br>
            &lt;<a>siegmar.gross@informatik.hs-fulda.de</a>&gt; wrote:<br>
<br>
            Hi Ralph,<br>
<br>
            I don&#39;t see any additional information.<br>
<br>
            tyr hello_1 108 mpiexec -np 4 --host<br>
            tyr,sunpc1,linpc1,ruester -mca<br>
            mca_base_component_show_load_errors 1 hello_1_mpi<br>
            [<a href="http://tyr.informatik.hs-fulda.de:06211" target="_blank">tyr.informatik.hs-fulda.de:06211</a><br>
            &lt;<a href="http://tyr.informatik.hs-fulda.de:06211" target="_blank">http://tyr.informatik.hs-fulda.de:06211</a>&gt;] [[48741,0],0]<br>
            ORTE_ERROR_LOG: Not found in file<br>
<br>
../../../../../openmpi-v2.x-dev-1280-gc110ae8/orte/mca/ess/hnp/ess_hnp_module.c<br>
<br>
            at line 638<br>
<br>
--------------------------------------------------------------------------<br>
            It looks like orte_init failed for some reason; your<br>
            parallel process is<br>
            likely to abort.  There are many reasons that a parallel<br>
            process can<br>
            fail during orte_init; some of which are due to configuration or<br>
            environment problems.  This failure appears to be an<br>
            internal failure;<br>
            here&#39;s some additional information (which may only be<br>
            relevant to an<br>
            Open MPI developer):<br>
<br>
             opal_pmix_base_select failed<br>
             --&gt; Returned value Not found (-13) instead of ORTE_SUCCESS<br>
<br>
--------------------------------------------------------------------------<br>
<br>
<br>
            tyr hello_1 109 mpiexec -np 4 --host<br>
            tyr,sunpc1,linpc1,ruester -mca<br>
            mca_base_component_show_load_errors 1 -mca pmix_base_verbose<br>
            10 -mca pmix_server_verbose 5 hello_1_mpi<br>
            [<a href="http://tyr.informatik.hs-fulda.de:06212" target="_blank">tyr.informatik.hs-fulda.de:06212</a><br>
            &lt;<a href="http://tyr.informatik.hs-fulda.de:06212" target="_blank">http://tyr.informatik.hs-fulda.de:06212</a>&gt;] mca: base:<br>
            components_register: registering framework pmix components<br>
            [<a href="http://tyr.informatik.hs-fulda.de:06212" target="_blank">tyr.informatik.hs-fulda.de:06212</a><br>
            &lt;<a href="http://tyr.informatik.hs-fulda.de:06212" target="_blank">http://tyr.informatik.hs-fulda.de:06212</a>&gt;] mca: base:<br>
            components_open: opening pmix components<br>
            [<a href="http://tyr.informatik.hs-fulda.de:06212" target="_blank">tyr.informatik.hs-fulda.de:06212</a><br>
            &lt;<a href="http://tyr.informatik.hs-fulda.de:06212" target="_blank">http://tyr.informatik.hs-fulda.de:06212</a>&gt;] mca:base:select:<br>
            Auto-selecting pmix components<br>
            [<a href="http://tyr.informatik.hs-fulda.de:06212" target="_blank">tyr.informatik.hs-fulda.de:06212</a><br>
            &lt;<a href="http://tyr.informatik.hs-fulda.de:06212" target="_blank">http://tyr.informatik.hs-fulda.de:06212</a>&gt;] mca:base:select:(<br>
            pmix) No component selected!<br>
            [<a href="http://tyr.informatik.hs-fulda.de:06212" target="_blank">tyr.informatik.hs-fulda.de:06212</a><br>
            &lt;<a href="http://tyr.informatik.hs-fulda.de:06212" target="_blank">http://tyr.informatik.hs-fulda.de:06212</a>&gt;] [[48738,0],0]<br>
            ORTE_ERROR_LOG: Not found in file<br>
<br>
../../../../../openmpi-v2.x-dev-1280-gc110ae8/orte/mca/ess/hnp/ess_hnp_module.c<br>
<br>
            at line 638<br>
<br>
--------------------------------------------------------------------------<br>
            It looks like orte_init failed for some reason; your<br>
            parallel process is<br>
            likely to abort.  There are many reasons that a parallel<br>
            process can<br>
            fail during orte_init; some of which are due to configuration or<br>
            environment problems.  This failure appears to be an<br>
            internal failure;<br>
            here&#39;s some additional information (which may only be<br>
            relevant to an<br>
            Open MPI developer):<br>
<br>
             opal_pmix_base_select failed<br>
             --&gt; Returned value Not found (-13) instead of ORTE_SUCCESS<br>
<br>
--------------------------------------------------------------------------<br>
            tyr hello_1 110<br>
<br>
<br>
            Kind regards<br>
<br>
            Siegmar<br>
<br>
<br>
            Am 21.04.2016 um 17:24 schrieb Ralph Castain:<br>
<br>
                Hmmm…it looks like you built the right components, but<br>
                they are not being picked up. Can you run your mpiexec<br>
                command again, adding “-mca<br>
                mca_base_component_show_load_errors 1” to the cmd line?<br>
<br>
<br>
                    On Apr 21, 2016, at 8:16 AM, Siegmar Gross<br>
                    &lt;<a>siegmar.gross@informatik.hs-fulda.de</a>&gt; wrote:<br>
<br>
                    Hi Ralph,<br>
<br>
                    I have attached ompi_info output for both compilers<br>
                    from my<br>
                    sparc machine and the listings for both compilers<br>
                    from the<br>
                    &lt;prefix&gt;/lib/openmpi directories. Hopefully that<br>
                    helps to<br>
                    find the problem.<br>
<br>
                    hermes tmp 3 tar zvft openmpi-2.x_info.tar.gz<br>
                    -rw-r--r-- root/root     10969 2016-04-21 17:06<br>
                    ompi_info_SunOS_sparc_cc.txt<br>
                    -rw-r--r-- root/root     11044 2016-04-21 17:06<br>
                    ompi_info_SunOS_sparc_gcc.txt<br>
                    -rw-r--r-- root/root     71252 2016-04-21 17:02<br>
                    lib64_openmpi.txt<br>
                    hermes tmp 4<br>
<br>
<br>
                    Kind regards and thank you very much once more for<br>
                    your help<br>
<br>
                    Siegmar<br>
<br>
<br>
                    Am 21.04.2016 um 15:54 schrieb Ralph Castain:<br>
<br>
                        Odd - it would appear that none of the pmix<br>
                        components built? Can you send<br>
                        along the output from ompi_info? Or just send a<br>
                        listing of the files in the<br>
                        &lt;prefix&gt;/lib/openmpi directory?<br>
<br>
<br>
                            On Apr 21, 2016, at 1:27 AM, Siegmar Gross<br>
                            &lt;<a>Siegmar.Gross@informatik.hs-fulda.de</a><br>
                            &lt;mailto:<a>Siegmar.Gross@informatik.hs-fulda.de</a>&gt;&gt;<br>
                            wrote:<br>
<br>
                            Hi Ralph,<br>
<br>
                            Am 21.04.2016 um 00:18 schrieb Ralph Castain:<br>
<br>
                                Could you please rerun these test and<br>
                                add “-mca pmix_base_verbose 10<br>
                                -mca pmix_server_verbose 5” to your cmd<br>
                                line? I need to see why the<br>
                                pmix components failed.<br>
<br>
<br>
<br>
                            tyr spawn 111 mpiexec -np 1 --host<br>
                            tyr,sunpc1,linpc1,ruester -mca<br>
                            pmix_base_verbose 10 -mca<br>
                            pmix_server_verbose 5 spawn_multiple_master<br>
                            [<a href="http://tyr.informatik.hs-fulda.de" target="_blank">tyr.informatik.hs-fulda.de</a><br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;<br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de/" target="_blank">http://tyr.informatik.hs-fulda.de/</a>&gt;:26652] mca:<br>
                            base: components_register: registering<br>
                            framework pmix components<br>
                            [<a href="http://tyr.informatik.hs-fulda.de" target="_blank">tyr.informatik.hs-fulda.de</a><br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;<br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de/" target="_blank">http://tyr.informatik.hs-fulda.de/</a>&gt;:26652] mca:<br>
                            base: components_open: opening pmix components<br>
                            [<a href="http://tyr.informatik.hs-fulda.de" target="_blank">tyr.informatik.hs-fulda.de</a><br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;<br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de/" target="_blank">http://tyr.informatik.hs-fulda.de/</a>&gt;:26652]<br>
                            mca:base:select: Auto-selecting pmix components<br>
                            [<a href="http://tyr.informatik.hs-fulda.de" target="_blank">tyr.informatik.hs-fulda.de</a><br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;<br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de/" target="_blank">http://tyr.informatik.hs-fulda.de/</a>&gt;:26652]<br>
                            mca:base:select:( pmix) No component selected!<br>
                            [<a href="http://tyr.informatik.hs-fulda.de" target="_blank">tyr.informatik.hs-fulda.de</a><br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;<br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de/" target="_blank">http://tyr.informatik.hs-fulda.de/</a>&gt;:26652]<br>
                            [[52794,0],0] ORTE_ERROR_LOG: Not found in file<br>
<br>
../../../../../openmpi-v2.x-dev-1280-gc110ae8/orte/mca/ess/hnp/ess_hnp_module.c<br>
<br>
                            at line 638<br>
<br>
--------------------------------------------------------------------------<br>
                            It looks like orte_init failed for some<br>
                            reason; your parallel process is<br>
                            likely to abort.  There are many reasons<br>
                            that a parallel process can<br>
                            fail during orte_init; some of which are due<br>
                            to configuration or<br>
                            environment problems.  This failure appears<br>
                            to be an internal failure;<br>
                            here&#39;s some additional information (which<br>
                            may only be relevant to an<br>
                            Open MPI developer):<br>
<br>
                            opal_pmix_base_select failed<br>
                            --&gt; Returned value Not found (-13) instead<br>
                            of ORTE_SUCCESS<br>
<br>
--------------------------------------------------------------------------<br>
                            tyr spawn 112<br>
<br>
<br>
<br>
<br>
                            tyr hello_1 116 mpiexec -np 1 --host<br>
                            tyr,sunpc1,linpc1,ruester -mca<br>
                            pmix_base_verbose 10 -mca<br>
                            pmix_server_verbose 5 hello_1_mpi<br>
                            [<a href="http://tyr.informatik.hs-fulda.de" target="_blank">tyr.informatik.hs-fulda.de</a><br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;<br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de/" target="_blank">http://tyr.informatik.hs-fulda.de/</a>&gt;:27261] mca:<br>
                            base: components_register: registering<br>
                            framework pmix components<br>
                            [<a href="http://tyr.informatik.hs-fulda.de" target="_blank">tyr.informatik.hs-fulda.de</a><br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;<br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de/" target="_blank">http://tyr.informatik.hs-fulda.de/</a>&gt;:27261] mca:<br>
                            base: components_open: opening pmix components<br>
                            [<a href="http://tyr.informatik.hs-fulda.de" target="_blank">tyr.informatik.hs-fulda.de</a><br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;<br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de/" target="_blank">http://tyr.informatik.hs-fulda.de/</a>&gt;:27261]<br>
                            mca:base:select: Auto-selecting pmix components<br>
                            [<a href="http://tyr.informatik.hs-fulda.de" target="_blank">tyr.informatik.hs-fulda.de</a><br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;<br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de/" target="_blank">http://tyr.informatik.hs-fulda.de/</a>&gt;:27261]<br>
                            mca:base:select:( pmix) No component selected!<br>
                            [<a href="http://tyr.informatik.hs-fulda.de" target="_blank">tyr.informatik.hs-fulda.de</a><br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;<br>
                            &lt;<a href="http://tyr.informatik.hs-fulda.de/" target="_blank">http://tyr.informatik.hs-fulda.de/</a>&gt;:27261]<br>
                            [[52315,0],0] ORTE_ERROR_LOG: Not found in file<br>
<br>
../../../../../openmpi-v2.x-dev-1280-gc110ae8/orte/mca/ess/hnp/ess_hnp_module.c<br>
<br>
                            at line 638<br>
<br>
--------------------------------------------------------------------------<br>
                            It looks like orte_init failed for some<br>
                            reason; your parallel process is<br>
                            likely to abort.  There are many reasons<br>
                            that a parallel process can<br>
                            fail during orte_init; some of which are due<br>
                            to configuration or<br>
                            environment problems.  This failure appears<br>
                            to be an internal failure;<br>
                            here&#39;s some additional information (which<br>
                            may only be relevant to an<br>
                            Open MPI developer):<br>
<br>
                            opal_pmix_base_select failed<br>
                            --&gt; Returned value Not found (-13) instead<br>
                            of ORTE_SUCCESS<br>
<br>
--------------------------------------------------------------------------<br>
                            tyr hello_1 117<br>
<br>
<br>
<br>
                            Thank you very much for your help.<br>
<br>
<br>
                            Kind regards<br>
<br>
                            Siegmar<br>
<br>
<br>
<br>
<br>
                                Thanks<br>
                                Ralph<br>
<br>
                                    On Apr 20, 2016, at 10:12 AM,<br>
                                    Siegmar Gross<br>
                                    &lt;<a>Siegmar.Gross@informatik.hs-fulda.de</a><br>
<br>
&lt;mailto:<a>Siegmar.Gross@informatik.hs-fulda.de</a>&gt;&gt;<br>
                                    wrote:<br>
<br>
                                    Hi,<br>
<br>
                                    I have built<br>
                                    openmpi-v2.x-dev-1280-gc110ae8 on my<br>
                                    machines<br>
                                    (Solaris 10 Sparc, Solaris 10<br>
                                    x86_64, and openSUSE Linux<br>
                                    12.1 x86_64) with gcc-5.1.0 and Sun<br>
                                    C 5.13. Unfortunately I get<br>
                                    runtime errors for some programs.<br>
<br>
<br>
                                    Sun C 5.13:<br>
                                    ===========<br>
<br>
                                    For all my test programs I get the<br>
                                    same error on Solaris Sparc and<br>
                                    Solaris x86_64, while the programs<br>
                                    work fine on Linux.<br>
<br>
                                    tyr hello_1 115 mpiexec -np 2<br>
                                    hello_1_mpi<br>
                                    [<a href="http://tyr.informatik.hs-fulda.de" target="_blank">tyr.informatik.hs-fulda.de</a><br>
                                    &lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;<br>
<br>
&lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;:22373]<br>
                                    [[61763,0],0] ORTE_ERROR_LOG: Not<br>
                                    found in file<br>
<br>
../../../../../openmpi-v2.x-dev-1280-gc110ae8/orte/mca/ess/hnp/ess_hnp_module.c<br>
<br>
                                    at line 638<br>
<br>
--------------------------------------------------------------------------<br>
                                    It looks like orte_init failed for<br>
                                    some reason; your parallel process is<br>
                                    likely to abort.  There are many<br>
                                    reasons that a parallel process can<br>
                                    fail during orte_init; some of which<br>
                                    are due to configuration or<br>
                                    environment problems.  This failure<br>
                                    appears to be an internal failure;<br>
                                    here&#39;s some additional information<br>
                                    (which may only be relevant to an<br>
                                    Open MPI developer):<br>
<br>
                                    opal_pmix_base_select failed<br>
                                    --&gt; Returned value Not found (-13)<br>
                                    instead of ORTE_SUCCESS<br>
<br>
--------------------------------------------------------------------------<br>
                                    tyr hello_1 116<br>
<br>
<br>
<br>
<br>
                                    GCC-5.1.0:<br>
                                    ==========<br>
<br>
                                    tyr spawn 121 mpiexec -np 1 --host<br>
                                    tyr,sunpc1,linpc1,ruester<br>
                                    spawn_multiple_master<br>
<br>
                                    Parent process 0 running on<br>
                                    <a href="http://tyr.informatik.hs-fulda.de" target="_blank">tyr.informatik.hs-fulda.de</a><br>
                                    &lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;<br>
                                    &lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;<br>
                                    I create 3 slave processes.<br>
<br>
                                    [<a href="http://tyr.informatik.hs-fulda.de" target="_blank">tyr.informatik.hs-fulda.de</a><br>
                                    &lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;<br>
<br>
&lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;:25366]<br>
                                    PMIX ERROR: UNPACK-PAST-END in file<br>
<br>
../../../../../../openmpi-v2.x-dev-1280-gc110ae8/opal/mca/pmix/pmix112/pmix/src/server/pmix_server_ops.c<br>
<br>
<br>
                                    at line 829<br>
                                    [<a href="http://tyr.informatik.hs-fulda.de" target="_blank">tyr.informatik.hs-fulda.de</a><br>
                                    &lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;<br>
<br>
&lt;<a href="http://tyr.informatik.hs-fulda.de" target="_blank">http://tyr.informatik.hs-fulda.de</a>&gt;:25366]<br>
                                    PMIX ERROR: UNPACK-PAST-END in file<br>
<br>
../../../../../../openmpi-v2.x-dev-1280-gc110ae8/opal/mca/pmix/pmix112/pmix/src/server/pmix_server.c<br>
<br>
<br>
                                    at line 2176<br>
                                    [tyr:25377] *** An error occurred in<br>
                                    MPI_Comm_spawn_multiple<br>
                                    [tyr:25377] *** reported by process<br>
                                    [3308257281,0]<br>
                                    [tyr:25377] *** on communicator<br>
                                    MPI_COMM_WORLD<br>
                                    [tyr:25377] *** MPI_ERR_SPAWN: could<br>
                                    not spawn processes<br>
                                    [tyr:25377] *** MPI_ERRORS_ARE_FATAL<br>
                                    (processes in this communicator will<br>
                                    now abort,<br>
                                    [tyr:25377] ***    and potentially<br>
                                    your MPI job)<br>
                                    tyr spawn 122<br>
<br>
<br>
                                    I would be grateful if somebody can<br>
                                    fix the problems. Thank you very<br>
                                    much for any help in advance.<br>
<br>
<br>
                                    Kind regards<br>
<br>
                                    Siegmar<br>
<br>
&lt;hello_1_mpi.c&gt;&lt;spawn_multiple_master.c&gt;_______________________________________________<br>
<br>
<br>
                                    users mailing list<br>
                                    <a>users@open-mpi.org</a><br>
&lt;mailto:<a>users@open-mpi.org</a>&gt;<br>
                                    Subscription:<br>
<br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                                    Link to this post:<br>
<br>
<a href="http://www.open-mpi.org/community/lists/users/2016/04/28983.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/28983.php</a><br>
<br>
<br>
<br>
_______________________________________________<br>
                                users mailing list<br>
                                <a>users@open-mpi.org</a><br>
&lt;mailto:<a>users@open-mpi.org</a>&gt;<br>
                                Subscription:<br>
<br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                                Link to this<br>
                                post:<br>
<br>
<a href="http://www.open-mpi.org/community/lists/users/2016/04/28986.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/28986.php</a><br>
<br>
<br>
                            _______________________________________________<br>
                            users mailing list<br>
                            <a>users@open-mpi.org</a> &lt;mailto:<a>users@open-mpi.org</a>&gt;<br>
                            Subscription:<br>
<br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                            Link to this<br>
                            post:<br>
<br>
<a href="http://www.open-mpi.org/community/lists/users/2016/04/28987.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/28987.php</a><br>
<br>
<br>
<br>
<br>
                        _______________________________________________<br>
                        users mailing list<br>
                        <a>users@open-mpi.org</a><br>
                        Subscription:<br>
                        <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                        Link to this post:<br>
<br>
<a href="http://www.open-mpi.org/community/lists/users/2016/04/28988.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/28988.php</a><br>
<br>
<br>
&lt;openmpi-2.x_info.tar.gz&gt;_______________________________________________<br>
                    users mailing list<br>
                    <a>users@open-mpi.org</a><br>
                    Subscription:<br>
                    <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                    Link to this post:<br>
<br>
<a href="http://www.open-mpi.org/community/lists/users/2016/04/28989.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/28989.php</a><br>
<br>
<br>
                _______________________________________________<br>
                users mailing list<br>
                <a>users@open-mpi.org</a><br>
                Subscription:<br>
                <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                Link to this post:<br>
<br>
<a href="http://www.open-mpi.org/community/lists/users/2016/04/28990.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/28990.php</a><br>
<br>
<br>
            _______________________________________________<br>
            users mailing list<br>
            <a>users@open-mpi.org</a><br>
            Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
            Link to this post:<br>
            <a href="http://www.open-mpi.org/community/lists/users/2016/04/28991.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/28991.php</a><br>
<br>
<br>
        _______________________________________________<br>
        users mailing list<br>
        <a>users@open-mpi.org</a><br>
        Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
        Link to this post:<br>
        <a href="http://www.open-mpi.org/community/lists/users/2016/04/28992.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/28992.php</a><br>
<br>
    _______________________________________________<br>
    users mailing list<br>
    <a>users@open-mpi.org</a><br>
    Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
    Link to this post:<br>
    <a href="http://www.open-mpi.org/community/lists/users/2016/04/28993.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/28993.php</a><br>
<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a>users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post:<br>
<a href="http://www.open-mpi.org/community/lists/users/2016/04/28999.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/28999.php</a><br>
<br>
</blockquote>
_______________________________________________<br>
users mailing list<br>
<a>users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post:<br>
<a href="http://www.open-mpi.org/community/lists/users/2016/04/29009.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/29009.php</a><br>
</blockquote>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a>users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/04/29033.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/29033.php</a><br>
</blockquote>
<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a>users@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/04/29038.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/29038.php</a><br>
<br>
</blockquote>
<br>
-- <br>
<br>
########################################################################<br>
#                                                                      #<br>
# Hochschule Fulda          University of Applied Sciences             #<br>
# FB Angewandte Informatik  Department of Applied Computer Science     #<br>
#                                                                      #<br>
# Prof. Dr. Siegmar Gross   Tel.: +49 (0)661 9640 - 333                #<br>
#                           Fax:  +49 (0)661 9640 - 349                #<br>
# Leipziger Str. 123        WWW:  <a href="http://www.hs-fulda.de/~gross" target="_blank">http://www.hs-fulda.de/~gross</a>        #<br>
# D-36037 Fulda             Mail: <a>Siegmar.Gross@informatik.hs-fulda.de</a> #<br>
#                                                                      #<br>
#                                                                      #<br>
# IT-Sicherheit: <a href="http://www.hs-fulda.de/it-sicherheit" target="_blank">http://www.hs-fulda.de/it-sicherheit</a>                  #<br>
#                                                                      #<br>
########################################################################<br>
<br>
</blockquote></div>

