Thanks, that was actually a lot of help, I had very little understanding of the bynode and byslot thingy, thanks<br><br>
<div><span class="gmail_quote">On 6/5/08, <b class="gmail_sendername">Jeff Squyres</b> &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:</span>
<blockquote class="gmail_quote" style="PADDING-LEFT: 1ex; MARGIN: 0px 0px 0px 0.8ex; BORDER-LEFT: #ccc 1px solid">On May 23, 2008, at 9:07 PM, Cally K wrote:<br><br>&gt; Hi, I have a question about --bynode and --byslot that i would like<br>
&gt; to clarify<br>&gt;<br>&gt; Say, for example, I have a hostfile<br>&gt;<br>&gt; #Hostfile<br>&gt;<br>&gt; __________________________<br>&gt; node0<br>&gt; node1 slots=2 max_slots=2<br>&gt; node2 slots=2 max_slots=2<br>
&gt; node3 slots=4 max_slots=4<br>&gt; ___________________________<br>&gt;<br>&gt; There are 4 nodes and 9 slots, how do I run my mpirun, for now I use<br>&gt;<br>&gt; a) mpirun -np --bynode 4 ./abcd<br><br>I assume you mean &quot;... -np 4 --bynode ...&quot;<br>
<br>&gt; I know that the slot thingy is for SMPs, and I have tried running<br>&gt; mpirun -np --byslot 9 ./abcd<br>&gt;<br>&gt; and I noticed that its longer when I do --byslot when compared to --<br>&gt; bynode<br><br>According to your text, you&#39;re running 9 processes when using --byslot<br>
and 4 when using --bynode.&nbsp;&nbsp;Is that a typo?&nbsp;&nbsp;I&#39;ll assume that it is --<br>that you meant to use 9 in both cases.<br><br>&gt; and I just read the faq that said, by defauly the byslot option is<br>&gt; used, so I dun have to use it rite,,,<br>
<br>I&#39;m not sure what your question is.&nbsp;&nbsp;The actual performance may depend<br>on your application and what its communication and computation<br>patterns are.&nbsp;&nbsp;It gets more difficult to model when you have a<br>heterogeneous setup (like it looks like you have, per your hostfile).<br>
<br>Let&#39;s take your example of 9 processes.<br><br>- With --bynode, the MPI_COMM_WORLD ranks will be laid out as follows<br>(MCRW = &quot;MPI_COMM_WORLD rank&quot;)<br><br>node0: MCWR 0<br>node1: MCWR 1, MCWR 4<br>node2: MCWR 2, MCWR 5<br>
node3: MCRW 3, MCRW 6, MCWR 7, MCWR 8<br><br>- With --byslot, it&#39;ll look like this:<br><br>node0: MCWR 0<br>node1: MCWR 1, MCWR 2<br>node2: MCWR 3, MCWR 4<br>node3: MCRW 5, MCRW 6, MCWR 7, MCWR 8<br><br>In short, OMPI is doing round-robin placement of your processes; the<br>
only difference is in which dimension is traversed first: by node or<br>by slot.<br><br>As to why there&#39;s such a performance difference, it could depend on a<br>lot of things: the difference in computational speed and/or RAM on<br>
your 4 nodes, the changing communication patterns between the two<br>(shared memory is usually used for on-node communication, which is<br>usually faster than most networks), etc.&nbsp;&nbsp;It really depends on what<br>your application is *doing*.<br>
<br>Sorry I can&#39;t be of more help...<br><br>--<br>Jeff Squyres<br>Cisco Systems<br><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

