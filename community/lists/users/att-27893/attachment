<div dir="ltr">







<p class="">Hello,</p>
<p class=""><span class="">I am currently trying to get MPI working using docker containers across networked virtual machines. The problem I have is that I don&#39;t seem to get any output from a dockerized MPI program (a tutorial matrix multiply program), but inspecting the nodes (docker containers) each process sits at 98% cpu usage whilst mpirun does not seem to get any output, and nothing complains or finishes. Running the same application on my laptop (with a local mpirun call) the running time is less than a second. </span></p>
<p class=""><span class="">Running a “hello world” program which just printed its “rank&quot; seems to work fine with this docker setup, with mpirun getting the stdout printf calls. However, that example never had any inter-node communications. </span></p>
<p class="">Much of the system setup we have is based on the idea that environment variables are what passes information from mpirun to the node processes (under ssh), but I have not had any exposure to MPI prior to this, so perhaps this is wrong.<br><span class=""></span></p>
<p class="">A (relatively) quick description of the system;<br><span class=""></span></p>
<p class=""><span class=""> - We have 3 virtual machines that are interconnected on an infiniband network (however, in the hope of it being simpler, we currently use the tcp/ip layer not infiniband)</span></p>
<p class=""><span class=""> - There is no resource manager (e.g. Slurm) running, everything is over ssh.</span></p>
<p class=""><span class=""> - Each vm is running Centos 7, has openmpi 1.6.4 installed, and loads the mpi environment module up via .bashrc.</span></p>
<p class=""><span class=""> - We have docker installed on each vm.</span></p>
<p class=""><span class=""> - We have a container that is based on ubuntu 14:04, has openmpi version 1.6.5 installed, and runs an mpi-based matrix_multiply program on startup. Ompi_info output is:</span></p>
<p class=""><span class="">   &gt; ompi_info -v ompi full --parsable</span></p>
<p class=""><span class="">     package:Open MPI buildd@allspice Distribution</span></p>
<p class=""><span class="">     ompi:version:full:1.6.5</span></p>
<p class=""><span class="">     ompi:version:svn:r28673</span></p>
<p class=""><span class="">     ompi:version:release_date:Jun 26, 2013</span></p>
<p class=""><span class="">     orte:version:full:1.6.5</span></p>
<p class=""><span class="">     orte:version:svn:r28673</span></p>
<p class=""><span class="">     orte:version:release_date:Jun 26, 2013</span></p>
<p class=""><span class="">     opal:version:full:1.6.5</span></p>
<p class=""><span class="">     opal:version:svn:r28673</span></p>
<p class=""><span class="">     opal:version:release_date:Jun 26, 2013</span></p>
<p class=""><span class="">     mpi-api:version:full:2.1</span></p>
<p class=""><span class="">     ident:1.6.5</span></p>
<p class=""><span class=""> </span></p>
<p class=""><span class=""> - To start the mpi program(s), on one of the vms we call mpirun with:</span></p>
<p class="">     mpirun --mca btl_tcp_port_min_v4 7950 --mca btl_tcp_port_range_v4 50 --mca btl tcp,self --mca mpi_base_verbose 30 -H 10.3.2.41,10.3.2.42,10.3.2.43 /root/docker-machine-script.sh<br><span class=""></span></p>
<p class=""> - This then runs our shell script on the vm, which collects environment variables, filters out the vm mpi variables, and starts the docker container with those env variables. Our shell script is:<br><span class=""></span></p>
<blockquote style="margin:0 0 0 40px;border:none;padding:0px"><p class="">#!/bin/bash</p></blockquote><p class=""><span class=""></span></p>
<blockquote style="margin:0 0 0 40px;border:none;padding:0px"><p class=""><span class="">env | grep MPI | grep -v &#39;MPI_INCLUDE\|MPI_PYTHON\|MPI_LIB\|MPI_BIN\|MPI_COMPILER\|MPI_SYSCONFIG\|MPI_SUFFIX\|MPI_MAN\|MPI_HOME\|MPI_FORTRAN_MOD_DIR&#39; &gt; /root/env.txt</span></p></blockquote>
<blockquote style="margin:0 0 0 40px;border:none;padding:0px"><p class=""><span class="">docker run --privileged --env-file /root/env.txt -p 7950-8000:7950-8000 mrmagooey/mpi-mm </span></p></blockquote>
<p class=""><span class=""></span><br></p>
<p class=""><span class=""> - Finally, the container runs the mpi application (the matrix multiply) using the environment variables.</span></p>
<p class=""><span class=""> - Each container is capable of ssh’ing into the other containers without password. </span></p>
<p class="">It is a rather complicated setup, but the real application that we will eventually run is a pain to compile from scratch and so docker is a very appealing solution. <br><span class=""></span></p>
<p class="">Attached is the &quot;ompi_info —all” and “ifconfig” called on one of the host vm’s (as per the forum support instructions). Also attached is the result of /usr/bin/printenv on one of the containers.</p>
<p class=""><span class="">Thank you,</span></p>
<p class=""><span class="">Peter</span></p><p class=""><span class=""></span></p><p class=""><span class=""></span></p><p class=""><span class=""></span></p><p class=""><span class=""></span></p><p class=""><span class=""></span></p>
<p class=""><span class="">    </span></p></div>

