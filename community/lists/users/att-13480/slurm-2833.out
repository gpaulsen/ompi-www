
hostname
node02

env | grep SLURM
SLURM_JOBID=2833
SLURM_JOB_NUM_NODES=1
SLURM_TASKS_PER_NODE=1
SLURM_TOPOLOGY_ADDR_PATTERN=node
SLURM_PRIO_PROCESS=0
SLURM_JOB_CPUS_PER_NODE=2
SLURM_JOB_NAME=submit.sh
SLURM_PROCID=0
SLURM_CPUS_ON_NODE=2
SLURM_NODELIST=node02
SLURM_NNODES=1
SLURM_JOB_ID=2833
SLURMD_NODENAME=node02
SLURM_JOB_NODELIST=node02
SLURM_GTIDS=0
SLURM_CPUS_PER_TASK=2
SLURM_CHECKPOINT_IMAGE_DIR=/home/droundy/qe-test
SLURM_LOCALID=0
SLURM_TASK_PID=15024
SLURM_TOPOLOGY_ADDR=node02
SLURM_NODEID=0
SLURM_SUBMIT_DIR=/home/droundy/qe-test

# clear SLURM environment variables 
#for i in `env | awk -F= '/SLURM/ {print $1}' | grep SLURM`; do
#  echo unsetting $i
#  unset $i
#done

mpirun --display-devel-map pw.x < pw.in
--------------------------------------------------------------------------
All nodes which are allocated for this job are already filled.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A daemon (pid unknown) died unexpectedly on signal 1  while attempting to
launch so we are aborting.

There may be more information reported by the environment (see above).

This may be because the daemon was unable to find all the needed shared
libraries on the remote node. You may set your LD_LIBRARY_PATH to have the
location of the shared libraries on the remote nodes and this will
automatically be forwarded to the remote nodes.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that the job aborted, but has no info as to the process
that caused that situation.
--------------------------------------------------------------------------
mpirun: clean termination accomplished


echo hello world, it is now done
hello world, it is now done
