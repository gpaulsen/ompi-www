<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">I’m afraid I’m not familiar with JupyterHub at all, or Salt. All you really need is:<div class=""><br class=""></div><div class="">* a scheduler that understands the need to start all the procs at the same time - i.e., as a block</div><div class=""><br class=""></div><div class="">* wireup support for the MPI procs themselves</div><div class=""><br class=""></div><div class="">If JupyterHub can do the first, then you could just have it launch the set of ORTE daemons by creating a hostfile with the IP addresses of the Docket containers and using the “orte-dvm” command, and then use “mpiexec” to start the application against those daemons. The daemons would provide the wireup support. This could then be streamlined later by adding a plugin to ORTE to get the “names” of the Docker containers without putting them in a hostfile.</div><div class=""><br class=""></div><div class="">HTH</div><div class="">Ralph</div><div class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Jun 2, 2016, at 3:58 PM, Rob Nagler &lt;<a href="mailto:openmpi-wooxi@q33.us" class="">openmpi-wooxi@q33.us</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><div dir="ltr" class="">We would like to use MPI on Docker with arbitrarily configured clusters (e.g. created with StarCluster or bare metal). What I'm curious about is if there is a queue manager that understands Docker, file systems, MPI, and OpenAuth. JupyterHub does a lot of this, but it doesn't interface with MPI. Ideally, we'd like users to be able to queue up jobs directly from JupyterHub.<div class=""><br class=""></div><div class="">Currently, we can configure and initiate an MPI-compatible Docker cluster running on a VPC using Salt. What's missing is the ability to manage a queue of these clusters. Here's a list of requirements:</div><div class=""><br class=""></div><div class=""><ul style="padding-left:2em;margin-top:0px;margin-bottom:16px;color:rgb(51,51,51);font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';line-height:25.6px" class=""><li class="">JupyterHub users do not have Unix user ids</li><li class="">Containers must be started as a non-root guest user (--user)</li><li class="">JupyterHub user's data directory is mounted in container</li><li class="">Data is shared via NFS or other cluster file system</li><li class="">sshd runs in container for MPI as guest user</li><li class="">Results have to be reported back to GitHub user</li><li class="">MPI network must be visible (--net=host)</li><li class="">Queue manager must be compatible with the above</li><li class="">JupyterHub user is not allowed to interact with Docker directly</li><li style="" class="">Docker images are user selectable (from an approved list)</li><li style="" class="">Jupyter and MPI containers started from same image</li></ul>Know of a system which supports this?</div><div class=""><br class=""></div><div class="">Our code and config are open source, and your feedback would be greatly appreciated.</div><div class=""><br class=""></div><div class="">Salt configuration: <a href="https://github.com/radiasoft/salt-conf" class="">https://github.com/radiasoft/salt-conf</a><br class=""></div><div class="">Container builders:&nbsp;<a href="https://github.com/radiasoft/containers/tree/master/radiasoft" class="">https://github.com/radiasoft/containers/tree/master/radiasoft</a></div><div class="">Early phase wiki:&nbsp;<a href="https://github.com/radiasoft/devops/wiki/DockerMPI" class="">https://github.com/radiasoft/devops/wiki/DockerMPI</a></div><div class=""><br class=""></div><div class="">Thanks,<br class=""></div><div class="">Rob</div><div class=""><br class=""></div></div>
_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: https://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2016/06/29355.php</div></blockquote></div><br class=""></div></body></html>
