<html>
  <head>
    <meta content="text/html; charset=ISO-8859-1"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    <div class="moz-cite-prefix">I just tried Gromacs with two nodes. It
      crashes, but with a different error. I get<br>
      [gpu-k20-13:142156] *** Process received signal ***<br>
      [gpu-k20-13:142156] Signal: Segmentation fault (11)<br>
      [gpu-k20-13:142156] Signal code: Address not mapped (1)<br>
      [gpu-k20-13:142156] Failing at address: 0x8<br>
      [gpu-k20-13:142156] [ 0]
      /lib64/libpthread.so.0(+0xf710)[0x2ac5d070c710]<br>
      [gpu-k20-13:142156] [ 1]
      /usr/lib64/nvidia/libcuda.so.1(+0x263acf)[0x2ac5ddfbcacf]<br>
      [gpu-k20-13:142156] [ 2]
      /usr/lib64/nvidia/libcuda.so.1(+0x229a83)[0x2ac5ddf82a83]<br>
      [gpu-k20-13:142156] [ 3]
      /usr/lib64/nvidia/libcuda.so.1(+0x15b2da)[0x2ac5ddeb42da]<br>
      [gpu-k20-13:142156] [ 4]
      /usr/lib64/nvidia/libcuda.so.1(cuInit+0x43)[0x2ac5ddea0933]<br>
      [gpu-k20-13:142156] [ 5]
/software-gpu/cuda/6.0.37/lib64/libcudart.so.6.0(+0x15965)[0x2ac5d0930965]<br>
      [gpu-k20-13:142156] [ 6]
/software-gpu/cuda/6.0.37/lib64/libcudart.so.6.0(+0x15a0a)[0x2ac5d0930a0a]<br>
      [gpu-k20-13:142156] [ 7]
/software-gpu/cuda/6.0.37/lib64/libcudart.so.6.0(+0x15a3b)[0x2ac5d0930a3b]<br>
      [gpu-k20-13:142156] [ 8]
/software-gpu/cuda/6.0.37/lib64/libcudart.so.6.0(cudaDriverGetVersion+0x4a)[0x2ac5d094602a]<br>
      [gpu-k20-13:142156] [ 9]
/software-gpu/apps/gromacs/4.6.5_gcc/lib/libgmxmpi.so.8(gmx_print_version_info_gpu+0x55)[0x2ac5cf9a90b5]<br>
      [gpu-k20-13:142156] [10]
/software-gpu/apps/gromacs/4.6.5_gcc/lib/libgmxmpi.so.8(gmx_log_open+0x17e)[0x2ac5cf54b9be]<br>
      [gpu-k20-13:142156] [11] mdrunmpi(cmain+0x1cdb)[0x43b4bb]<br>
      [gpu-k20-13:142156] [12]
      /lib64/libc.so.6(__libc_start_main+0xfd)[0x2ac5d1534d1d]<br>
      [gpu-k20-13:142156] [13] mdrunmpi[0x407be1]<br>
      [gpu-k20-13:142156] *** End of error message ***<br>
--------------------------------------------------------------------------<br>
      mpiexec noticed that process rank 0 with PID 142156 on node
      gpu-k20-13 exited on signal 11 (Segmentation fault).<br>
--------------------------------------------------------------------------<br>
      <br>
      <br>
      <br>
      We do not have MPI_THREAD_MULTIPLE enabled in our build, so
      Charm++ cannot be using this level of threading. The configure
      line for OpenMPI was<br>
      <meta http-equiv="content-type" content="text/html;
        charset=ISO-8859-1">
      ./configure --prefix=$PREFIX \<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --with-threads --with-verbs=yes --enable-shared
      --enable-static \<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --with-io-romio-flags="--with-file-system=nfs+lustre" \<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --without-loadleveler --without-slurm --with-tm \<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --with-cuda=$(dirname $(dirname $(which nvcc)))<br>
      <br>
      Maxime<br>
      <br>
      <meta charset="utf-8">
      <br>
      Le 2014-08-14 14:20, Joshua Ladd a &eacute;crit&nbsp;:<br>
    </div>
    <blockquote
cite="mid:CAG4F6z-RpVNQWd=UQ_xp8L1e-LECY_M5XxwCdki1OJ3GRORgJg@mail.gmail.com"
      type="cite">
      <div dir="ltr">
        <div>What about between nodes? Since this is coming from the
          OpenIB BTL, would be good to check this. <br>
          <br>
          Do you know what the MPI thread level is set to when used with
          the Charm++ runtime? Is it MPI_THREAD_MULTIPLE? The OpenIB BTL
          is not thread safe.<br>
          <br>
        </div>
        Josh <br>
      </div>
      <div class="gmail_extra"><br>
        <br>
        <div class="gmail_quote">On Thu, Aug 14, 2014 at 2:17 PM, Maxime
          Boissonneault <span dir="ltr">&lt;<a moz-do-not-send="true"
              href="mailto:maxime.boissonneault@calculquebec.ca"
              target="_blank">maxime.boissonneault@calculquebec.ca</a>&gt;</span>
          wrote:<br>
          <blockquote class="gmail_quote" style="margin:0 0 0
            .8ex;border-left:1px #ccc solid;padding-left:1ex">
            <div bgcolor="#FFFFFF" text="#000000">
              <div>Hi,<br>
                I ran gromacs successfully with OpenMPI 1.8.1 and Cuda
                6.0.37 on a single node, with 8 ranks and multiple
                OpenMP threads.<br>
                <br>
                Maxime<br>
                <br>
                <br>
                Le 2014-08-14 14:15, Joshua Ladd a &eacute;crit&nbsp;:<br>
              </div>
              <blockquote type="cite">
                <div>
                  <div class="h5">
                    <div dir="ltr">
                      <div>
                        <div>Hi, Maxime<br>
                          <br>
                        </div>
                        Just curious, are you able to run a vanilla MPI
                        program? Can you try one one of the example
                        programs in the "examples" subdirectory. Looks
                        like a threading issue to me.<br>
                        <br>
                      </div>
                      <div>Thanks,<br>
                        <br>
                      </div>
                      Josh<br>
                      <div>&nbsp;<br>
                      </div>
                    </div>
                    <br>
                    <fieldset></fieldset>
                    <br>
                  </div>
                </div>
                <pre><div class="">_______________________________________________
users mailing list
<a moz-do-not-send="true" href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>
Subscription: <a moz-do-not-send="true" href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div>
Link to this post: <a moz-do-not-send="true" href="http://www.open-mpi.org/community/lists/users/2014/08/25023.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25023.php</a></pre>
              </blockquote>
              <br>
              <br>
            </div>
            <br>
            _______________________________________________<br>
            users mailing list<br>
            <a moz-do-not-send="true" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
            Subscription: <a moz-do-not-send="true"
              href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
              target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
            Link to this post: <a moz-do-not-send="true"
              href="http://www.open-mpi.org/community/lists/users/2014/08/25024.php"
              target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25024.php</a><br>
          </blockquote>
        </div>
        <br>
      </div>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2014/08/25025.php">http://www.open-mpi.org/community/lists/users/2014/08/25025.php</a></pre>
    </blockquote>
    <br>
    <br>
    <pre class="moz-signature" cols="72">-- 
---------------------------------
Maxime Boissonneault
Analyste de calcul - Calcul Qu&eacute;bec, Universit&eacute; Laval
Ph. D. en physique</pre>
  </body>
</html>

