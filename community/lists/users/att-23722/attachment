<html>
  <head>
    <meta content="text/html; charset=ISO-8859-1"
      http-equiv="Content-Type">
  </head>
  <body text="#000000" bgcolor="#FFFFFF">
    <div class="moz-cite-prefix">OK, the problem is that node14's BIOS
      reports invalid NUMA info. It properly detects 2 sockets with
      16-cores each. But it reports 2 NUMA nodes total, instead of 2 per
      socket (4 total). And hwloc warns because the cores contained in
      these NUMA nodes are incompatible with sockets:<br>
      socket0 contains 0-15<br>
      socket1 contains 16-23<br>
      NUMA node0 contains 0-7+16-23<br>
      NUMA node1 contains 8-15+24-31<br>
      <br>
      <br>
      Interestingly the BIOS is more recent on the broken node:<br>
      <pre>--- node15/sys/class/dmi/id/bios_date	2014-02-27 22:26:57.000000000 +0100
+++ node14/sys/class/dmi/id/bios_date	2014-02-28 01:16:16.000000000 +0100
@@ -1 +1 @@
-08/31/2012
+11/25/2013
--- node15/sys/class/dmi/id/bios_version	2014-02-27 22:26:57.000000000 +0100
+++ node14/sys/class/dmi/id/bios_version	2014-02-28 01:16:16.000000000 +0100
@@ -1 +1 @@
-3.0       
+3.5       

</pre>
      Unfortunately, 3.5 is the latest BIOS available for this machine
      on supermicro's website, and 3.0 isn't available online afaict.
      You should contact supermicro to report the bug and ask them to
      provide the old 3.0 BIOS in the meantime.<br>
      <br>
      You could try running "lstopo foo.xml" on node15, make foo.xml
      available on node14, and pass HWLOC_XMLFILE=/path/to/foo.xml in
      your environment. May need some tweaks, but could work.<br>
      <br>
      Brice<br>
      <br>
      <br>
      <br>
      Le 28/02/2014 20:55, Gus Correa a &eacute;crit&nbsp;:<br>
    </div>
    <blockquote cite="mid:5310E9C0.8070604@ldeo.columbia.edu"
      type="cite">Hi Brice and Ralph
      <br>
      <br>
      Many thanks for helping out with this!
      <br>
      <br>
      Yes, you are right about node15 being OK.
      <br>
      Node15 was a red herring, as along with node14 it was part of
      <br>
      the same job that failed.
      <br>
      However, after a closer look, I noticed that failure reported
      <br>
      by hwloc was indeed in node14.
      <br>
      <br>
      I attach both diagnostic files generated by hwloc-gather-topology
      on
      <br>
      node14.
      <br>
      <br>
      I will open the node and see if there is anything unusual with the
      <br>
      hardware, and perhaps reinstall the OS, as Ralph suggested.
      <br>
      It is awkward that the other node that had the motherboard
      replaced
      <br>
      passes the hwloc-gather-topology test.
      <br>
      After motherboard replacement I renistalled the OS on both,
      <br>
      but it doesn't hurt to do it again.
      <br>
      <br>
      Gus Correa
      <br>
      <br>
      <br>
      <br>
      <br>
      On 02/28/2014 03:26 AM, Brice Goglin wrote:
      <br>
      <blockquote type="cite">Hello Gus,
        <br>
        I'll need the tarball generated by gather-topology on node14 to
        debug
        <br>
        this. node15 doesn't have any issue.
        <br>
        We've seen issues on AMD machines because of buggy BIOS
        reporting
        <br>
        incompatible Socket and NUMA info. If node14 doesn't have the
        same BIOS
        <br>
        version as other nodes, that could explain things.
        <br>
        Brice
        <br>
        <br>
        <br>
        <br>
        <br>
        Le 28/02/2014 01:39, Gus Correa a &eacute;crit :
        <br>
        <blockquote type="cite">Thank you, Ralph!
          <br>
          <br>
          I did a bit more of homework, and found out that all jobs that
          had
          <br>
          the hwloc error involved one specific node (node14).
          <br>
          <br>
          The "report bindings" output in those jobs' stderr show
          <br>
          that node14 systematically failed to bind the processes to the
          cores,
          <br>
          while other nodes on the same jobs didn't fail.
          <br>
          Interestingly, the jobs continued to run, although they
          <br>
          eventually failed, but much later.
          <br>
          So, the hwloc error doesn't seem to stop the job on its
          tracks.
          <br>
          As a matter of policy, should it perhaps shutdown the job
          instead?
          <br>
          <br>
          In addition, when I try the hwloc-gather-topology diagnostic
          on node14
          <br>
          I get the same error, a bit more verbose (see below).
          <br>
          So, now my guess is that this may be a hardware problem on
          that node.
          <br>
          <br>
          I replaced two nodes' motherboards last week, including
          node14's,
          <br>
          and something may have gone wrong on that one.
          <br>
          The other node that had the motherboard replaced
          <br>
          doesn't show the hwloc-gather-topology error, though.
          <br>
          <br>
          Does the error message below (Socket P#0 ...)
          <br>
          suggest anything that I should be looking for on the hardware
          side?
          <br>
          (Thermal compound on the heatsink, memory modules, etc)
          <br>
          <br>
          Thank you,
          <br>
          Gus Correa
          <br>
          <br>
          <br>
          <br>
          [root@node14 ~]# /usr/bin/hwloc-gather-topology /tmp/$(uname
          -n)
          <br>
          Hierarchy gathered in /tmp/node14.tar.bz2 and kept in
          <br>
          /tmp/tmp.D46Sdhcnru/node14/
          <br>
****************************************************************************
          <br>
          <br>
          * Hwloc has encountered what looks like an error from the
          operating
          <br>
          system.
          <br>
          *
          <br>
          * object (Socket P#0 cpuset 0x0000ffff) intersection without
          inclusion!
          <br>
          * Error occurred in topology.c line 718
          <br>
          *
          <br>
          * Please report this error message to the hwloc user's mailing
          list,
          <br>
          * along with the output from the hwloc-gather-topology.sh
          script.
          <br>
****************************************************************************
          <br>
          <br>
          Expected topology output stored in /tmp/node14.output
          <br>
          <br>
          <br>
          On 02/27/2014 06:39 PM, Ralph Castain wrote:
          <br>
          <blockquote type="cite">The hwloc in 1.6.5 is very old
            (v1.3.2), so it's possible it is having
            <br>
          </blockquote>
          trouble with those data/instruction cache breakdowns.
          <br>
          I don't know why it wouldn't have shown up before,
          <br>
          however, as this looks to be happening when we first try to
          <br>
          assemble the topology. To check that, what happens if you just
          run
          <br>
          "mpiexec hostname" on the local node?
          <br>
          <blockquote type="cite">
            <br>
            <br>
            On Feb 27, 2014, at 3:04 PM, Gus
            Correa<a class="moz-txt-link-rfc2396E" href="mailto:gus@ldeo.columbia.edu">&lt;gus@ldeo.columbia.edu&gt;</a>&nbsp;&nbsp; wrote:
            <br>
            <br>
            <blockquote type="cite">Dear OMPI pros
              <br>
              <br>
              This seems to be a question in the nowhere land between
              OMPI and hwloc.
              <br>
              However, it appeared as an OMPI error, hence it may be OK
              to ask the
              <br>
              question in this list.
              <br>
              <br>
              ***
              <br>
              <br>
              A user here got this error (or warning?) message today:
              <br>
              <br>
              + mpiexec -np 64 $HOME/echam-aiv_ldeo_6.1.00p1/bin/echam6
              <br>
****************************************************************************
              <br>
              <br>
              * Hwloc has encountered what looks like an error from the
              operating
              <br>
              system.
              <br>
              *
              <br>
              * object intersection without inclusion!
              <br>
              * Error occurred in topology.c line 594
              <br>
              *
              <br>
              * Please report this error message to the hwloc user's
              mailing list,
              <br>
              * along with the output from the hwloc-gather-topology.sh
              script.
              <br>
****************************************************************************
              <br>
              <br>
              <br>
              Additional info:
              <br>
              <br>
              1) We have OMPI 1.6.5. This user is using the one built
              <br>
              with Intel compilers 2011.13.367.
              <br>
              <br>
              2) I set these MCA parameters in
              $OMPI/etc/openmpi-mca-params.conf
              <br>
              (includes binding to core):
              <br>
              <br>
              btl = ^tcp
              <br>
              orte_tag_output = 1
              <br>
              rmaps_base_schedule_policy = core
              <br>
              orte_process_binding = core
              <br>
              orte_report_bindings = 1
              <br>
              opal_paffinity_alone = 1
              <br>
              <br>
              <br>
              3) The machines have dual-socket 16-core AMD Opteron 6376
              (Abu-Dhabi),
              <br>
              which have one FPU for each pair of cores, a hierarchy of
              caches
              <br>
              serving
              <br>
              sub-groups of cores, etc.
              <br>
              The OS is&nbsp; Linux CentOS 6.4 with stock CentOS OFED.
              <br>
              Interconnect is Infiniband QDR (Mellanox HW).
              <br>
              <br>
              4) We have Torque 4.2.5, built with cpuset support.
              <br>
              OMPI is built with Torque (tm) support.
              <br>
              <br>
              5) In case it helps, I attach the output of
              <br>
              hwloc-gather-topology, which I ran on the node that threw
              the error,
              <br>
              although not immediately after the job failure.
              <br>
              I used the hwloc-gather-topology script that comes with
              <br>
              the hwloc (version 1.5) provided by CentOS.
              <br>
              As far as I can tell the hwloc nuts and bits built into
              OMPI
              <br>
              do not include the hwloc-gather-topology script (although
              it may be
              <br>
              a newer hwloc version. 1.8 perhaps?).
              <br>
              Hopefully the mail servers won't chop off the attachments.
              <br>
              <br>
              6) I am a bit surprised by this error message, because I
              haven't
              <br>
              seen it before, although we have used OMPI 1.6.5 in
              <br>
              this machine with several other programs without problems.
              <br>
              Alas, it happened now.
              <br>
              <br>
              **
              <br>
              <br>
              - Is this a known hwloc problem in this processor
              architecture?
              <br>
              <br>
              - Is this a known issue in this combination of HW and SW?
              <br>
              <br>
              - Would not binding the MPI processes (to core or socket),
              perhaps
              <br>
              help?
              <br>
              <br>
              - Any workarounds or suggestions?
              <br>
              <br>
              **
              <br>
              <br>
              Thank you,
              <br>
              Gus Correa
              <br>
&lt;node15.output&gt;&lt;node15.tar.bz2&gt;_______________________________________________
              <br>
              <br>
              users mailing list
              <br>
              <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
              <br>
              <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
              <br>
            </blockquote>
            <br>
            _______________________________________________
            <br>
            users mailing list
            <br>
            <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
            <br>
            <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
            <br>
          </blockquote>
          <br>
          _______________________________________________
          <br>
          users mailing list
          <br>
          <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
          <br>
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
          <br>
        </blockquote>
        <br>
      </blockquote>
      <br>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></pre>
    </blockquote>
    <br>
  </body>
</html>

