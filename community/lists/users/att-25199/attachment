<html xmlns:v="urn:schemas-microsoft-com:vml" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:w="urn:schemas-microsoft-com:office:word" xmlns:m="http://schemas.microsoft.com/office/2004/12/omml" xmlns="http://www.w3.org/TR/REC-html40">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="Generator" content="Microsoft Word 14 (filtered medium)">
<style><!--
/* Font Definitions */
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:Tahoma;
	panose-1:2 11 6 4 3 5 4 4 2 4;}
/* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin:0in;
	margin-bottom:.0001pt;
	font-size:12.0pt;
	font-family:"Times New Roman","serif";}
a:link, span.MsoHyperlink
	{mso-style-priority:99;
	color:blue;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{mso-style-priority:99;
	color:purple;
	text-decoration:underline;}
span.apple-tab-span
	{mso-style-name:apple-tab-span;}
span.apple-converted-space
	{mso-style-name:apple-converted-space;}
span.EmailStyle19
	{mso-style-type:personal-reply;
	font-family:"Calibri","sans-serif";
	color:#1F497D;}
.MsoChpDefault
	{mso-style-type:export-only;
	font-size:10.0pt;}
@page WordSection1
	{size:8.5in 11.0in;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
	{page:WordSection1;}
--></style><!--[if gte mso 9]><xml>
<o:shapedefaults v:ext="edit" spidmax="1026" />
</xml><![endif]--><!--[if gte mso 9]><xml>
<o:shapelayout v:ext="edit">
<o:idmap v:ext="edit" data="1" />
</o:shapelayout></xml><![endif]-->
</head>
<body lang="EN-US" link="blue" vlink="purple">
<div class="WordSection1">
<p class="MsoNormal"><span style="font-size:9.0pt;font-family:&quot;Courier New&quot;">I am able to run all 15 of my jobs simultaneously; 16 MPI processes per job; mapping by socket and binding to socket. On a given socket, 6 MPI processes from 6 separate mpiruns share
 the 6 cores, or at least I assume they are sharing. The load for all CPUs and all processes is 100%. I understand that I am loading the system to its limits, but is what I am doing OK? My jobs are running, and the only problem seems to be that some jobs are
 hanging at random times. This is a new cluster I am shaking down, and I am guessing that the message passing traffic is causing packet losses. I am working with the vendor to sort this out, but I am curious whether or not I am using OpenMPI appropriately.
<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:9.0pt;font-family:&quot;Courier New&quot;"><o:p>&nbsp;</o:p></span></p>
<p class="MsoNormal"><span style="font-size:9.0pt;font-family:&quot;Courier New&quot;">#PBS -l nodes=8:ppn=2<br>
mpirun --report-bindings --bind-to socket --map-by socket:PE=1 -np 16<span class="apple-converted-space">&nbsp;</span>&lt;executable file name&gt;<br>
<br>
The bindings are:<br>
<br>
[burn004:07244] MCW rank 0 bound to socket 0[core 0[hwt 0]], socket<span class="apple-converted-space">&nbsp;</span>0[core 1[hwt 0]], socket 0[core 2[hwt 0]], socket 0[core 3[hwt 0]],<span class="apple-converted-space">&nbsp;</span>socket 0[core 4[hwt 0]], socket 0[core
 5[hwt 0]]:<span class="apple-converted-space">&nbsp;&nbsp; </span>[B/B/B/B/B/B][./././././.]
<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:9.0pt;font-family:&quot;Courier New&quot;">[burn004:07244] MCW rank 1 bound to socket<span class="apple-converted-space">&nbsp;</span>1[core 6[hwt 0]], socket 1[core 7[hwt 0]], socket 1[core 8[hwt 0]], socket 1[core 9[hwt 0]], socket
 1[core 10[hwt 0]], socket 1[core 11[hwt 0]]: [./././././.][B/B/B/B/B/B] <o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:9.0pt;font-family:&quot;Courier New&quot;">[burn008:07256] MCW rank 3 bound to socket 1[core 6[hwt 0]], socket 1[core 7[hwt 0]], socket 1[core 8[hwt 0]], socket 1[core 9[hwt 0]], socket 1[core 10[hwt 0]], socket 1[core 11[hwt
 0]]: [./././././.][B/B/B/B/B/B] <o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:9.0pt;font-family:&quot;Courier New&quot;">[burn008:07256] MCW rank 2 bound to socket 0[core 0[hwt 0]], socket 0[core 1[hwt 0]], socket 0[core 2[hwt 0]], socket 0[core 3[hwt 0]], socket 0[core 4[hwt 0]], socket 0[core 5[hwt
 0]]: &nbsp;&nbsp;[B/B/B/B/B/B][./././././.] and so on.<br>
<br>
</span><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D"><o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D"><o:p>&nbsp;</o:p></span></p>
<div>
<div style="border:none;border-top:solid #B5C4DF 1.0pt;padding:3.0pt 0in 0in 0in">
<p class="MsoNormal"><b><span style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;">From:</span></b><span style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;"> users [mailto:users-bounces@open-mpi.org]
<b>On Behalf Of </b>Ralph Castain<br>
<b>Sent:</b> Friday, August 29, 2014 3:26 PM<br>
<b>To:</b> Open MPI Users<br>
<b>Subject:</b> Re: [OMPI users] How does binding option affect network traffic?<o:p></o:p></span></p>
</div>
</div>
<p class="MsoNormal"><o:p>&nbsp;</o:p></p>
<p class="MsoNormal"><o:p>&nbsp;</o:p></p>
<div>
<div>
<p class="MsoNormal">On Aug 29, 2014, at 10:51 AM, McGrattan, Kevin B. Dr. &lt;<a href="mailto:kevin.mcgrattan@nist.gov">kevin.mcgrattan@nist.gov</a>&gt; wrote:<o:p></o:p></p>
</div>
<p class="MsoNormal"><br>
<br>
<o:p></o:p></p>
<div>
<p class="MsoNormal"><span style="font-size:9.0pt">Thanks for the tip. I understand how using the --cpuset option would help me in the example I described. However, suppose I have multiple users submitting MPI jobs of various sizes? I wouldn't know a priori
 which cores were in use and which weren't. I always assumed that this is what these various schedulers did. Is there a way to map-by socket but not allow a single core to be used by more than one process. At first glance, I thought that --map-by socket and
 --bind-to core would do this. Would one of these &quot;NOOVERSUBSCRIBE&quot; options help?<o:p></o:p></span></p>
</div>
<div>
<p class="MsoNormal"><o:p>&nbsp;</o:p></p>
</div>
<p class="MsoNormal">I'm afraid not - the issue here is that the mpirun's don't know about each other. What you'd need to do is have your scheduler assign cores for our use - we'll pick that up and stay inside that envelope. The exact scheduler command depends
 on the scheduler, of course, but the scheduler would then have the more global picture and could keep things separated.<o:p></o:p></p>
</div>
<div>
<p class="MsoNormal"><br>
<br>
<o:p></o:p></p>
<div>
<p class="MsoNormal"><span style="font-size:9.0pt"><br>
Also, in my test case, I have exactly the right amount of cores (240) to run 15 jobs using 16 MPI processes. I am shaking down a new cluster we just bought. This is an extreme case, but not atypical of the way we use our clusters.<o:p></o:p></span></p>
</div>
<div>
<p class="MsoNormal"><o:p>&nbsp;</o:p></p>
</div>
<p class="MsoNormal">Well, you do, but not exactly the way you showed you were trying to use this. If you try to run as you described, with 2ppn for each mpirun and 12 cores/node, you can run a maximum of 6 mpirun's at a time across a given set of nodes. So
 you'd need to stage your allocations correctly to make it work.<o:p></o:p></p>
</div>
<div>
<p class="MsoNormal"><o:p>&nbsp;</o:p></p>
</div>
<div>
<p class="MsoNormal"><o:p>&nbsp;</o:p></p>
</div>
<div>
<p class="MsoNormal"><br>
<br>
<o:p></o:p></p>
<div>
<p class="MsoNormal"><span style="font-size:9.0pt"><br>
------------------------------<br>
<br>
Date: Thu, 28 Aug 2014 13:27:12 -0700<br>
From: Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Subject: Re: [OMPI users] How does binding option affect network<br>
<span class="apple-tab-span">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>traffic?<br>
Message-ID: &lt;<a href="mailto:637CAEF5-BBB3-46C2-9387-DECDF8CBDDD6@open-mpi.org">637CAEF5-BBB3-46C2-9387-DECDF8CBDDD6@open-mpi.org</a>&gt;<br>
Content-Type: text/plain; charset=&quot;windows-1252&quot;<br>
<br>
<br>
On Aug 28, 2014, at 11:50 AM, McGrattan, Kevin B. Dr. &lt;<a href="mailto:kevin.mcgrattan@nist.gov">kevin.mcgrattan@nist.gov</a>&gt; wrote:<br>
<br>
<br>
<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:9.0pt">My institute recently purchased a linux cluster with 20 nodes; 2 sockets per node; 6 cores per socket. OpenMPI v 1.8.1 is installed. I want to run 15 jobs. Each job requires 16 MPI processes. &nbsp;For each job,
 I want to use two cores on each node, mapping by socket. If I use these options:<br>
<br>
#PBS -l nodes=8:ppn=2<br>
mpirun --report-bindings --bind-to core --map-by socket:PE=1 -np 16<span class="apple-converted-space">&nbsp;</span><br>
&lt;executable file name&gt;<br>
<br>
The reported bindings are:<br>
<br>
[burn001:09186] MCW rank 0 bound to socket 0[core 0[hwt 0]]:<span class="apple-converted-space">&nbsp;</span><br>
[B/././././.][./././././.] [burn001:09186] MCW rank 1 bound to socket<span class="apple-converted-space">&nbsp;</span><br>
1[core 6[hwt 0]]: [./././././.][B/././././.] [burn004:07113] MCW rank<span class="apple-converted-space">&nbsp;</span><br>
6 bound to socket 0[core 0[hwt 0]]: [B/././././.][./././././.]<span class="apple-converted-space">&nbsp;</span><br>
[burn004:07113] MCW rank 7 bound to socket 1[core 6[hwt 0]]: [./././././.][B/././././.] and so on?<br>
<br>
These bindings appear to be OK, but when I do a ?top ?H? on each node, I see that all 15 jobs use core 0 and core 6 on each node. This means, I believe, that I am only using 1/6 or my resources.<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:9.0pt"><br>
That is correct. The problem is that each mpirun execution has no idea what the others are doing, or even that they exist. Thus, they will each independently bind to core zero and core 6, as you observe. You can get around this by submitting each with a separate
 --cpuset argument telling it which cpus it is allowed to use - something like this (note that there is no value to having pe=1 as that is automatically what happens with bind-to core):<br>
<br>
mpirun --cpuset 0,6 --bind-to core &nbsp;....<br>
mpirun --cpuset 1,7 --bind-to core &nbsp;...<br>
<br>
etc. You specified only two procs/node with your PBS request, so we'll only map two on each node. This command line tells the first mpirun to only use cores 0 and 6, and to bind each proc to one of those cores. The second uses only cores 1 and 7, and thus is
 separated from the first command.<br>
<br>
However, you should note that you can't run 15 jobs at the same time in the manner you describe without overloading some cores as you only have 12 cores/node. This will create a poor-performance situation.<br>
<br>
<br>
<br>
<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:9.0pt">I want to use 100%. So I try this:<br>
<br>
#PBS -l nodes=8:ppn=2<br>
mpirun --report-bindings --bind-to socket --map-by socket:PE=1 -np 16<span class="apple-converted-space">&nbsp;</span><br>
&lt;executable file name&gt;<br>
<br>
Now it appears that I am getting 100% usage of all cores on all nodes. The bindings are:<br>
<br>
[burn004:07244] MCW rank 0 bound to socket 0[core 0[hwt 0]], socket<span class="apple-converted-space">&nbsp;</span><br>
0[core 1[hwt 0]], socket 0[core 2[hwt 0]], socket 0[core 3[hwt 0]],<span class="apple-converted-space">&nbsp;</span><br>
socket 0[core 4[hwt 0]], socket 0[core 5[hwt 0]]:<span class="apple-converted-space">&nbsp;</span><br>
[B/B/B/B/B/B][./././././.] [burn004:07244] MCW rank 1 bound to socket<span class="apple-converted-space">&nbsp;</span><br>
1[core 6[hwt 0]], socket 1[core 7[hwt 0]], socket 1[core 8[hwt 0]], socket 1[core 9[hwt 0]], socket 1[core 10[hwt 0]], socket 1[core 11[hwt 0]]: [./././././.][B/B/B/B/B/B] [burn008:07256] MCW rank 3 bound to socket 1[core 6[hwt 0]], socket 1[core 7[hwt 0]],
 socket 1[core 8[hwt 0]], socket 1[core 9[hwt 0]], socket 1[core 10[hwt 0]], socket 1[core 11[hwt 0]]: [./././././.][B/B/B/B/B/B] [burn008:07256] MCW rank 2 bound to socket 0[core 0[hwt 0]], socket 0[core 1[hwt 0]], socket 0[core 2[hwt 0]], socket 0[core 3[hwt
 0]], socket 0[core 4[hwt 0]], socket 0[core 5[hwt 0]]: [B/B/B/B/B/B][./././././.] and so on?<br>
<br>
The problem now is that some of my jobs are hanging. They all start running fine, and produce output. But at some point I lose about 4 out of 15 jobs due to hanging. I suspect that an MPI message is passed and not received. The number of jobs that hang and
 the time when they hang varies from test to test. We have run these cases successfully on our old cluster dozens of times ? they are part of our benchmark suite.<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:9.0pt"><br>
Did you have more cores on your old cluster? I suspect the problem here is resource exhaustion, especially if you are using Infiniband as you are overloading some of the cores, as mentioned above.<br>
<br>
<br>
<o:p></o:p></span></p>
<p class="MsoNormal" style="margin-bottom:12.0pt"><span style="font-size:9.0pt"><br>
When I run these jobs using a map by core strategy (that is, the MPI processes are just mapped by core, and each job only uses 16 cores on two nodes), I do not see as much hanging. It still occurs, but less often. This leads me to suspect that there is something
 about the increased network traffic due to the map-by-socket approach that is the cause of the problem. But I do not know what to do about it. I think that the map-by-socket approach is the right one, but I do not know if I have my OpenMPI options just right.<br>
<br>
Can you tell me what OpenMPI options to use, and can you tell me how I might debug the hanging issue.<br>
<br>
<br>
<br>
Kevin McGrattan<br>
National Institute of Standards and Technology<br>
100 Bureau Drive, Mail Stop 8664<br>
Gaithersburg, Maryland 20899<br>
<br>
301 975 2712<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:9.0pt">_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post:<span class="apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/08/25195.php">http://www.open-mpi.org/community/lists/users/2014/08/25195.php</a><o:p></o:p></span></p>
</div>
</div>
<p class="MsoNormal"><o:p>&nbsp;</o:p></p>
</div>
</body>
</html>

