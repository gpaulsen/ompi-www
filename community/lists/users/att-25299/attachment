<div dir="ltr">Thanks George,<div>I am selecting Ethernet device (em1) in mpirun script.</div><div><br></div><div>Here is ifconfig output:</div><div><div>em1       Link encap:Ethernet  HWaddr E0:DB:55:FD:38:46</div><div>          inet addr:10.30.10.121  Bcast:10.30.255.255  Mask:255.255.0.0</div><div>          inet6 addr: fe80::e2db:55ff:fefd:3846/64 Scope:Link</div><div>          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</div><div>          RX packets:1537270190 errors:0 dropped:0 overruns:0 frame:0</div><div>          TX packets:136123598 errors:0 dropped:0 overruns:0 carrier:0</div><div>          collisions:0 txqueuelen:1000</div><div>          RX bytes:309333740659 (288.0 GiB)  TX bytes:143480101212 (133.6 GiB)</div><div>          Memory:91820000-91840000</div><div><br></div><div>Ifconfig uses the ioctl access method to get the full address information, which limits hardware addresses to 8 bytes.</div><div>Because Infiniband address has 20 bytes, only the first 8 bytes are displayed correctly.</div><div>Ifconfig is obsolete! For replacement check ip.</div><div>ib0       Link encap:InfiniBand  HWaddr 80:00:00:03:FE:80:00:00:00:00:00:00:00:00:00:00:00:00:00:00</div><div>          inet addr:10.32.10.121  Bcast:10.32.255.255  Mask:255.255.0.0</div><div>          inet6 addr: fe80::211:7500:70:6ab4/64 Scope:Link</div><div>          UP BROADCAST RUNNING MULTICAST  MTU:2044  Metric:1</div><div>          RX packets:33621 errors:0 dropped:0 overruns:0 frame:0</div><div>          TX packets:365 errors:0 dropped:5 overruns:0 carrier:0</div><div>          collisions:0 txqueuelen:256</div><div>          RX bytes:1882728 (1.7 MiB)  TX bytes:21920 (21.4 KiB)</div><div><br></div><div>lo        Link encap:Local Loopback</div><div>          inet addr:127.0.0.1  Mask:255.0.0.0</div><div>          inet6 addr: ::1/128 Scope:Host</div><div>          UP LOOPBACK RUNNING  MTU:16436  Metric:1</div><div>          RX packets:66889 errors:0 dropped:0 overruns:0 frame:0</div><div>          TX packets:66889 errors:0 dropped:0 overruns:0 carrier:0</div><div>          collisions:0 txqueuelen:0</div><div>          RX bytes:19005445 (18.1 MiB)  TX bytes:19005445 (18.1 MiB)</div></div><div><br></div><div><br></div><div><br></div><div> <br><div class="gmail_extra"><br><div class="gmail_quote"><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><br>
Date: Wed, 10 Sep 2014 00:06:51 +0900<br>
From: George Bosilca &lt;<a href="mailto:bosilca@icl.utk.edu">bosilca@icl.utk.edu</a>&gt;<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Subject: Re: [OMPI users] Forcing OpenMPI to use Ethernet interconnect<br>
        instead of InfiniBand<br>
<br>
Look at your ifconfig output and select the Ethernet device (instead of the<br>
IPoIB one). Traditionally the name lack any fanciness, most distributions<br>
using eth0 as a default.<br>
<br>
  George.<br>
<br>
<br>
On Tue, Sep 9, 2014 at 11:24 PM, Muhammad Ansar Javed &lt;<br>
<a href="mailto:muhammad.ansar@seecs.edu.pk">muhammad.ansar@seecs.edu.pk</a>&gt; wrote:<br>
<br>
&gt; Hi,<br>
&gt;<br>
&gt; I am currently conducting some testing on system with Gigabit and<br>
&gt; InfiniBand interconnects. Both Latency and Bandwidth benchmarks are doing<br>
&gt; well as expected on InfiniBand interconnects but Ethernet interconnect is<br>
&gt; achieving very high performance from expectations. Ethernet and InfiniBand<br>
&gt; both are achieving equivalent performance.<br>
&gt;<br>
&gt; For some reason, it looks like openmpi (v1.8.1) is using the InfiniBand<br>
&gt; interconnect rather than the Gigabit or TCP communication is being emulated<br>
&gt; to use InifiniBand interconnect.<br>
&gt;<br>
&gt; Here are Latency and Bandwidth benchmark results.<br>
&gt; #---------------------------------------------------<br>
&gt; # Benchmarking PingPong<br>
&gt; # processes = 2<br>
&gt; # map-by node<br>
&gt; #---------------------------------------------------<br>
&gt;<br>
&gt; Hello, world.  I am 1 on node124<br>
&gt; Hello, world.  I am 0 on node123<br>
&gt; Size Latency (usec) Bandwidth (Mbps)<br>
&gt; 1    1.65    4.62<br>
&gt; 2    1.67    9.16<br>
&gt; 4    1.66    18.43<br>
&gt; 8    1.66    36.74<br>
&gt; 16    1.85    66.00<br>
&gt; 32    1.83    133.28<br>
&gt; 64    1.83    266.36<br>
&gt; 128    1.88    519.10<br>
&gt; 256    1.99    982.29<br>
&gt; 512    2.23    1752.37<br>
&gt; 1024    2.58    3026.98<br>
&gt; 2048    3.32    4710.76<br>
&gt;<br>
&gt; I read some of the FAQs and noted that OpenMPI prefers the faster<br>
&gt; available interconnect. In an effort to force it to use the gigabit<br>
&gt; interconnect I ran it as follows<br>
&gt;<br>
&gt; 1. mpirun -np 2 -machinefile machines -map-by node --mca btl tcp --mca<br>
&gt; btl_tcp_if_include em1 ./latency.ompi<br>
&gt; 2. mpirun -np 2 -machinefile machines -map-by node --mca btl tcp,self,sm<br>
&gt; --mca btl_tcp_if_include em1 ./latency.ompi<br>
&gt; 3. mpirun -np 2 -machinefile machines -map-by node --mca btl ^openib --mca<br>
&gt; btl_tcp_if_include em1 ./latency.ompi<br>
&gt; 4. mpirun -np 2 -machinefile machines -map-by node --mca btl ^openib<br>
&gt; ./latency.ompi<br>
&gt;<br>
&gt; None of them resulted in a significantly different benchmark output.<br>
&gt;<br>
&gt; I am using OpenMPI by loading module on clustered environment and don&#39;t<br>
&gt; have admin access. It is configured for both TCP and OpenIB (confirmed from<br>
&gt; ompi_info). After trying all above mentioned methods without success I<br>
&gt; installed OpenMPI v1.8.2 in my home directory and disable openib with<br>
&gt; following configuration options<br>
&gt;<br>
&gt; --disable-openib-control-hdr-padding --disable-openib-dynamic-sl<br>
&gt; --disable-openib-connectx-xrc --disable-openib-udcm<br>
&gt; --disable-openib-rdmacm  --disable-btl-openib-malloc-alignment<br>
&gt; --disable-io-romio --without-openib --without-verbs<br>
&gt;<br>
&gt; Now, openib is not enabled (confirmed from ompi_info script) and there is<br>
&gt; no &quot;openib.so&quot; file in $prefix/lib/openmpi directory as well. Still, above<br>
&gt; mentioned mpirun commands are getting the same latency and bandwidth as<br>
&gt; that of InfiniBand.<br>
&gt;<br>
&gt; I tried mpirun in verbose mode with following command and here is the<br>
&gt; output<br>
&gt;<br>
&gt; Command:<br>
&gt; mpirun -np 2 -machinefile machines -map-by node --mca btl tcp --mca<br>
&gt; btl_base_verbose 30 --mca btl_tcp_if_include em1 ./latency.ompi<br>
&gt;<br>
&gt; Output:<br>
&gt; [node123.prv.sciama.cluster:88310] mca: base: components_register:<br>
&gt; registering btl components<br>
&gt; [node123.prv.sciama.cluster:88310] mca: base: components_register: found<br>
&gt; loaded component tcp<br>
&gt; [node123.prv.sciama.cluster:88310] mca: base: components_register:<br>
&gt; component tcp register function successful<br>
&gt; [node123.prv.sciama.cluster:88310] mca: base: components_open: opening btl<br>
&gt; components<br>
&gt; [node123.prv.sciama.cluster:88310] mca: base: components_open: found<br>
&gt; loaded component tcp<br>
&gt; [node123.prv.sciama.cluster:88310] mca: base: components_open: component<br>
&gt; tcp open function successful<br>
&gt; [node124.prv.sciama.cluster:90465] mca: base: components_register:<br>
&gt; registering btl components<br>
&gt; [node124.prv.sciama.cluster:90465] mca: base: components_register: found<br>
&gt; loaded component tcp<br>
&gt; [node124.prv.sciama.cluster:90465] mca: base: components_register:<br>
&gt; component tcp register function successful<br>
&gt; [node124.prv.sciama.cluster:90465] mca: base: components_open: opening btl<br>
&gt; components<br>
&gt; [node124.prv.sciama.cluster:90465] mca: base: components_open: found<br>
&gt; loaded component tcp<br>
&gt; [node124.prv.sciama.cluster:90465] mca: base: components_open: component<br>
&gt; tcp open function successful<br>
&gt; Hello, world.  I am 1 on node124<br>
&gt; Hello, world.  I am 0 on node123<br>
&gt; Size Latency(usec) Bandwidth(Mbps)<br>
&gt; 1    4.18    1.83<br>
&gt; 2    3.66    4.17<br>
&gt; 4    4.08    7.48<br>
&gt; 8    3.12    19.57<br>
&gt; 16    3.83    31.84<br>
&gt; 32    3.40    71.84<br>
&gt; 64    4.10    118.97<br>
&gt; 128    3.89    251.19<br>
&gt; 256    4.22    462.77<br>
&gt; 512    2.95    1325.71<br>
&gt; 1024    2.63    2969.49<br>
&gt; 2048    3.38    4628.29<br>
&gt; [node123.prv.sciama.cluster:88310] mca: base: close: component tcp closed<br>
&gt; [node123.prv.sciama.cluster:88310] mca: base: close: unloading component<br>
&gt; tcp<br>
&gt; [node124.prv.sciama.cluster:90465] mca: base: close: component tcp closed<br>
&gt; [node124.prv.sciama.cluster:90465] mca: base: close: unloading component<br>
&gt; tcp<br>
&gt;<br>
&gt; Moreover, same benchmark applications using MPICH are working fine on<br>
&gt; Ethernet and achieving expected Latency and Bandwidth.<br>
&gt;<br>
&gt; How can this be fixed?<br>
&gt;<br>
&gt; Thanks for help,<br>
&gt;<br>
&gt; --Ansar<br></blockquote><div> </div></div><br><br clear="all"><div><br></div>-- <br><div dir="ltr">Regards<br><br>Ansar Javed<br>HPC Lab<br>SEECS NUST <br>Contact: +92 334 438 9394<br><div>Skype: ansar.javed.859</div><div>Email: <a href="mailto:muhammad.ansar@seecs.edu.pk" target="_blank">muhammad.ansar@seecs.edu.pk</a><br></div></div>
</div></div></div>

