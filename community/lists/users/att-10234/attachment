<font size=2 face="sans-serif">Hi Craig,</font>
<br>
<br><font size=2 face="sans-serif">&nbsp; &nbsp; &nbsp; &nbsp; How
was the nodefile selected for execution? Whether it was provided by scheduler
say LSF/SGE/PBS or you manually gave it?</font>
<br><font size=2 face="sans-serif">With WRF, we observed giving sequential
nodes (Blades which are in the same order as in enclosure) gave us some
performance benefit.</font>
<br>
<br><font size=2 face="sans-serif">Regards<br>
<br>
Neeraj Chourasia (MTS)<br>
Computational Research Laboratories Ltd.<br>
(A wholly Owned Subsidiary of TATA SONS Ltd)<br>
B-101, ICC Trade Towers, Senapati Bapat Road<br>
Pune 411016 (Mah) INDIA<br>
(O) +91-20-6620 9863 &nbsp;(Fax) +91-20-6620 9862<br>
M: +91.9225520634<br>
</font>
<br>
<br>
<br>
<table width=100%>
<tr valign=top>
<td width=40%><font size=1 face="sans-serif"><b>Craig Tierney &lt;Craig.Tierney@noaa.gov&gt;</b>
</font>
<br><font size=1 face="sans-serif">Sent by: users-bounces@open-mpi.org</font>
<p><font size=1 face="sans-serif">08/07/2009 04:43 AM</font>
<table border>
<tr valign=top>
<td bgcolor=white>
<div align=center><font size=1 face="sans-serif">Please respond to<br>
Open MPI Users &lt;users@open-mpi.org&gt;</font></div></table>
<br>
<td width=59%>
<table width=100%>
<tr valign=top>
<td>
<div align=right><font size=1 face="sans-serif">To</font></div>
<td><font size=1 face="sans-serif">Open MPI Users &lt;users@open-mpi.org&gt;</font>
<tr valign=top>
<td>
<div align=right><font size=1 face="sans-serif">cc</font></div>
<td>
<tr valign=top>
<td>
<div align=right><font size=1 face="sans-serif">Subject</font></div>
<td><font size=1 face="sans-serif">Re: [OMPI users] Performance question
about OpenMPI and MVAPICH2 on &nbsp; &nbsp; &nbsp; &nbsp;IB</font></table>
<br>
<table>
<tr valign=top>
<td>
<td></table>
<br></table>
<br>
<br>
<br><tt><font size=2>Gus Correa wrote:<br>
&gt; Hi Craig, list<br>
&gt; <br>
&gt; I suppose WRF uses MPI collective calls (MPI_Reduce,<br>
&gt; MPI_Bcast, MPI_Alltoall etc),<br>
&gt; just like the climate models we run here do.<br>
&gt; A recursive grep on the source code will tell.<br>
&gt; <br>
<br>
I will check this out. &nbsp;I am not the WRF expert, but<br>
I was under the impression that most weather models are<br>
nearest neighbor communications, not collectives.<br>
<br>
<br>
&gt; If that is the case, you may need to tune the collectives dynamically.<br>
&gt; We are experimenting with tuned collectives here also.<br>
&gt; <br>
&gt; Specifically, we had a scaling problem with the MITgcm<br>
&gt; (also running on an IB cluster)<br>
&gt; that is probably due to collectives.<br>
&gt; Similar problems were reported on this list before,<br>
&gt; with computational chemistry software.<br>
&gt; See these threads:<br>
&gt; </font></tt><a href="http://www.open-mpi.org/community/lists/users/2009/07/10045.php"><tt><font size=2>http://www.open-mpi.org/community/lists/users/2009/07/10045.php</font></tt></a><tt><font size=2><br>
&gt; </font></tt><a href="http://www.open-mpi.org/community/lists/users/2009/05/9419.php"><tt><font size=2>http://www.open-mpi.org/community/lists/users/2009/05/9419.php</font></tt></a><tt><font size=2><br>
&gt; <br>
&gt; If WRF outputs timing information, particularly the time spent on
MPI<br>
&gt; routines, you may also want to compare how the OpenMPI and<br>
&gt; MVAPICH versions fare w.r.t. MPI collectives.<br>
&gt; <br>
&gt; I hope this helps.<br>
&gt; <br>
<br>
I will look into this. &nbsp;Thanks for the ideas.<br>
<br>
Craig<br>
<br>
<br>
<br>
&gt; Gus Correa<br>
&gt; ---------------------------------------------------------------------<br>
&gt; Gustavo Correa<br>
&gt; Lamont-Doherty Earth Observatory - Columbia University<br>
&gt; Palisades, NY, 10964-8000 - USA<br>
&gt; ---------------------------------------------------------------------<br>
&gt; <br>
&gt; <br>
&gt; <br>
&gt; Craig Tierney wrote:<br>
&gt;&gt; I am running openmpi-1.3.3 on my cluster which is using<br>
&gt;&gt; OFED-1.4.1 for Infiniband support. &nbsp;I am comparing performance<br>
&gt;&gt; between this version of OpenMPI and Mvapich2, and seeing a<br>
&gt;&gt; very large difference in performance.<br>
&gt;&gt;<br>
&gt;&gt; The code I am testing is WRF v3.0.1. &nbsp;I am running the<br>
&gt;&gt; 12km benchmark.<br>
&gt;&gt;<br>
&gt;&gt; The two builds are the exact same codes and configuration<br>
&gt;&gt; files. &nbsp;All I did different was use modules to switch versions<br>
&gt;&gt; of MPI, and recompiled the code.<br>
&gt;&gt;<br>
&gt;&gt; Performance:<br>
&gt;&gt;<br>
&gt;&gt; Cores &nbsp; Mvapich2 &nbsp; &nbsp;Openmpi<br>
&gt;&gt; ---------------------------<br>
&gt;&gt; &nbsp; &nbsp;8 &nbsp; &nbsp; &nbsp;17.3 &nbsp; &nbsp; &nbsp; &nbsp;13.9<br>
&gt;&gt; &nbsp; 16 &nbsp; &nbsp; &nbsp;31.7 &nbsp; &nbsp; &nbsp; &nbsp;25.9<br>
&gt;&gt; &nbsp; 32 &nbsp; &nbsp; &nbsp;62.9 &nbsp; &nbsp; &nbsp; &nbsp;51.6<br>
&gt;&gt; &nbsp; 64 &nbsp; &nbsp; 110.8 &nbsp; &nbsp; &nbsp; &nbsp;92.8<br>
&gt;&gt; &nbsp;128 &nbsp; &nbsp; 219.2 &nbsp; &nbsp; &nbsp; 189.4<br>
&gt;&gt; &nbsp;256 &nbsp; &nbsp; 384.5 &nbsp; &nbsp; &nbsp; 317.8<br>
&gt;&gt; &nbsp;512 &nbsp; &nbsp; 687.2 &nbsp; &nbsp; &nbsp; 516.7<br>
&gt;&gt;<br>
&gt;&gt; The performance number is GFlops (so larger is better).<br>
&gt;&gt;<br>
&gt;&gt; I am calling openmpi as:<br>
&gt;&gt;<br>
&gt;&gt; /opt/openmpi/1.3.3-intel/bin/mpirun &nbsp;--mca plm_rsh_disable_qrsh
1<br>
&gt;&gt; --mca btl openib,sm,self \<br>
&gt;&gt; -machinefile /tmp/6026489.1.qntest.q/machines -x LD_LIBRARY_PATH
-np<br>
&gt;&gt; $NSLOTS /home/ctierney/bin/noaa_affinity ./wrf.exe<br>
&gt;&gt;<br>
&gt;&gt; So,<br>
&gt;&gt;<br>
&gt;&gt; Is this expected? &nbsp;Are some common sense optimizations to
use?<br>
&gt;&gt; Is there a way to verify that I am really using the IB? &nbsp;When<br>
&gt;&gt; I try:<br>
&gt;&gt;<br>
&gt;&gt; -mca bta ^tcp,openib,sm,self<br>
&gt;&gt;<br>
&gt;&gt; I get the errors:<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt;<br>
&gt;&gt; No available btl components were found!<br>
&gt;&gt;<br>
&gt;&gt; This means that there are no components of this type installed
on your<br>
&gt;&gt; system or all the components reported that they could not be used.<br>
&gt;&gt;<br>
&gt;&gt; This is a fatal error; your MPI process is likely to abort. &nbsp;Check
the<br>
&gt;&gt; output of the &quot;ompi_info&quot; command and ensure that components
of this<br>
&gt;&gt; type are available on your system. &nbsp;You may also wish to
check the<br>
&gt;&gt; value of the &quot;component_path&quot; MCA parameter and ensure
that it has at<br>
&gt;&gt; least one directory that contains valid MCA components.<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; But ompi_info is telling me that I have openib support:<br>
&gt;&gt;<br>
&gt;&gt; &nbsp; &nbsp;MCA btl: openib (MCA v2.0, API v2.0, Component v1.3.3)<br>
&gt;&gt;<br>
&gt;&gt; Note, I did rebuild OFED and put it in a different directory<br>
&gt;&gt; and did not rebuild OpenMPI. &nbsp;However, since ompi_info isn't<br>
&gt;&gt; complaining and the libraries are available, I am thinking that<br>
&gt;&gt; is isn't a problem. &nbsp;I could be wrong.<br>
&gt;&gt;<br>
&gt;&gt; Thanks,<br>
&gt;&gt; Craig<br>
&gt; <br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; users@open-mpi.org<br>
&gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
&gt; <br>
<br>
<br>
-- <br>
Craig Tierney (craig.tierney@noaa.gov)<br>
_______________________________________________<br>
users mailing list<br>
users@open-mpi.org<br>
</font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
</font></tt>
<br><p>=====-----=====-----=====



Notice: The information contained in this e-mail
message and/or attachments to it may contain 
confidential or privileged information. If you are 
not the intended recipient, any dissemination, use, 
review, distribution, printing or copying of the 
information contained in this e-mail message 
and/or attachments to it are strictly prohibited. If 
you have received this communication in error, 
please notify us by reply e-mail or telephone and 
immediately and permanently delete the message 
and any attachments. 

Internet communications cannot be guaranteed to be timely,
secure, error or virus-free. The sender does not accept liability
for any errors or omissions.Thank you

=====-----=====-----=====</p>

