<br>Here is what I meant: the results of 500 procs in fact shows it with 272-304(&lt;500) real cores, the program&#39;s running time is good, which is almost five times 100 procs&#39; time. So it can be handled very well. Therefore I guess OpenMPI or Rocks OS does make use of hyperthreading to do the job. But with 600 procs, the running time is more than double of that of 500 procs. I don&#39;t know why. This is my problem.   <br>
<br>BTW, how to use -bind-to-core? I added it as mpirun&#39;s options. It always gives me error &quot; the executable &#39;bind-to-core&#39; can&#39;t be found. Isn&#39;t it like:<br>mpirun --mca btl_tcp_if_include eth0 -np 600  -bind-to-core scatttest<br>
<br>Thank you very much.<br><br>Linbao<br><br><div class="gmail_quote">On Mon, Oct 4, 2010 at 4:42 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<div style="word-wrap: break-word;"><br><div><div class="im"><div>On Oct 4, 2010, at 1:48 PM, Storm Zhang wrote:</div><br><blockquote type="cite"><span style="font-family: arial,sans-serif; font-size: 13px; border-collapse: collapse;">Thanks a lot, Ralgh. As I said, I also tried to use SGE(also showing 1024 available for parallel tasks) which only assign 34-38 compute nodes which only has 272-304 real cores for 500 procs running. The running time is consistent with 100 procs and not a lot fluctuations due to the number of machines&#39; changing.</span></blockquote>
<div><br></div></div>Afraid I don&#39;t understand your statement. If you have 500 procs running on &lt; 500 cores, then the performance relative to a high-performance job (#procs &lt;= #cores) will be worse. We deliberately dial down the performance when oversubscribed to ensure that procs &quot;play nice&quot; in situations where the node is oversubscribed.</div>
<div><div class="im"><br><blockquote type="cite"><span style="font-family: arial,sans-serif; font-size: 13px; border-collapse: collapse;"> So I guess it is not related to hyperthreading. Correct me if I&#39;m wrong.</span></blockquote>
<div><br></div></div>Has nothing to do with hyperthreading - OMPI has no knowledge of hyperthreads at this time.</div><div><div class="im"><br><blockquote type="cite"><div>
<span style="font-family: arial,sans-serif; font-size: 13px; border-collapse: collapse;"><br></span></div><div><span style="font-family: arial,sans-serif; font-size: 13px; border-collapse: collapse;">BTW, how to bind the proc to the core? I tried --bind-to-core or -bind-to-core but neither works. Is it for OpenMP, not for OpenMPI? </span></div>
</blockquote><div><br></div></div>Those should work. You might try --report-bindings to see what OMPI thought it did.</div><div><div></div><div class="h5"><div><br><blockquote type="cite"><div><div>
<span style="font-family: arial,sans-serif; font-size: 13px; border-collapse: collapse;"><div><br></div><div>Linbao</div></span><br><br><div class="gmail_quote">On Mon, Oct 4, 2010 at 12:27 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div style="word-wrap: break-word;">Some of what you are seeing is the natural result of context switching....some thoughts regarding the results:<div>

<br></div><div>1. You didn&#39;t bind your procs to cores when running with #procs &lt; #cores, so you&#39;re performance in those scenarios will also be less than max. </div><div><br></div><div>2. Once the number of procs exceeds the number of cores, you guarantee a lot of context switching, so performance will definitely take a hit.<div>

<br></div><div>3. Sometime in the not-too-distant-future, OMPI will (hopefully) become hyperthread aware. For now, we don&#39;t see them as separate processing units. So as far as OMPI is concerned, you only have 512 computing units to work with, not 1024.</div>

<div><br></div><div>Bottom line is that you are running oversubscribed, so OMPI turns down your performance so that the machine doesn&#39;t hemorrhage as it context switches.</div><div><div></div><div><div><br>
</div><div><br><div><div>On Oct 4, 2010, at 11:06 AM, Doug Reeder wrote:</div><br><blockquote type="cite"><div style="word-wrap: break-word;">In my experience hyperthreading can&#39;t really deliver two cores worth of processing simultaneously for processes expecting sole use of a core. Since you really have 512 cores I&#39;m not surprised that you see a performance hit when requesting &gt; 512 compute units. We should really get input from a hyperthreading expert, preferably form intel.<div>

<br></div><div>Doug Reeder<br><div><div>On Oct 4, 2010, at 9:53 AM, Storm Zhang wrote:</div><br><blockquote type="cite"><span style="font-family: arial,sans-serif; font-size: 13px; border-collapse: collapse;"><div><span style="font-family: arial,sans-serif; font-size: 13px; border-collapse: collapse;">We have 64 compute nodes which are dual qual-core and hyperthreaded CPUs. So we have 1024 compute units shown in the ROCKS 5.3 system. I&#39;m trying to scatter an array from the master node to the compute nodes using mpiCC and mpirun using C++. </span></div>


<div><span style="font-family: arial,sans-serif; font-size: 13px; border-collapse: collapse;"><br></span></div><div><span style="font-family: arial,sans-serif; font-size: 13px; border-collapse: collapse;">Here is my test:</span></div>


<div><span style="font-family: arial,sans-serif; font-size: 13px; border-collapse: collapse;"><br></span></div><div><span style="font-family: arial,sans-serif; font-size: 13px; border-collapse: collapse;">The array size is 18KB * Number of compute nodes and is scattered to the compute nodes 5000 times repeatly. </span></div>


<div><span style="font-family: arial,sans-serif; font-size: 13px; border-collapse: collapse;"><br></span></div><div><span style="font-family: arial,sans-serif; font-size: 13px; border-collapse: collapse;">The average running time(seconds):</span></div>


<div><span style="font-family: arial,sans-serif; font-size: 13px; border-collapse: collapse;"><br></span></div><div><span style="font-family: arial,sans-serif; font-size: 13px; border-collapse: collapse;">100 nodes: 170,</span></div>


<div>400 nodes: 690,</div><div>500 nodes: 855,</div><div>600 nodes: 2550,</div><div>700 nodes: 2720,</div><div>800 nodes: 2900,</div><div><br></div><div>There is a big jump of running time from 500 nodes to 600 nodes. <span style="font-size: small;">Don&#39;t know what&#39;s the problem. </span></div>


<div>Tried both in OMPI 1.3.2 and OMPI 1.4.2. Running time is a little faster for all the tests in 1.4.2 but the jump still exists. </div><div>Tried using either Bcast function or simply Send/Recv which give very close results. </div>


<div>Tried both in running it directly or using SGE and got the same results.</div><div><span style="font-size: small;"><br></span></div><div><span style="font-size: small;"><span style="font-size: 13px;"><div>
<span style="font-size: small;">The code and ompi_info are attached to this email. The direct running command is :</span></div><div><span style="font-size: small;"><div>
/opt/openmpi/bin/mpirun --mca btl_tcp_if_include eth0 --machinefile ../machines -np 600 scatttest</div><div><br></div><div>The ifconfig of head node for eth0 is:</div><div><div>eth0      Link encap:Ethernet  HWaddr 00:26:B9:56:8B:44  </div>


<div>          inet addr:192.168.1.1  Bcast:192.168.1.255  Mask:255.255.255.0</div><div>          inet6 addr: fe80::226:b9ff:fe56:8b44/64 Scope:Link</div><div>          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</div>


<div>          RX packets:1096060373 errors:0 dropped:2512622 overruns:0 frame:0</div><div>          TX packets:513387679 errors:0 dropped:0 overruns:0 carrier:0</div><div>          collisions:0 txqueuelen:1000 </div><div>


          RX bytes:832328807459 (775.1 GiB)  TX bytes:250824621959 (233.5 GiB)</div><div>          Interrupt:106 Memory:d6000000-d6012800 </div></div><div><br></div><div>A typical ifconfig of a compute node is:</div><div>


<div>eth0      Link encap:Ethernet  HWaddr 00:21:9B:9A:15:AC  </div><div>          inet addr:192.168.1.253  Bcast:192.168.1.255  Mask:255.255.255.0</div><div>          inet6 addr: fe80::221:9bff:fe9a:15ac/64 Scope:Link</div>


<div>          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</div><div>          RX packets:362716422 errors:0 dropped:0 overruns:0 frame:0</div><div>          TX packets:349967746 errors:0 dropped:0 overruns:0 carrier:0</div>


<div>          collisions:0 txqueuelen:1000 </div><div>          RX bytes:139699954685 (130.1 GiB)  TX bytes:338207741480 (314.9 GiB)</div><div>          Interrupt:82 Memory:d6000000-d6012800 </div></div><div><br></div></span></div>


</span></span></div><div><span style="font-size: small;"><br></span></div><div><span style="font-size: small;">Does anyone help me out of this? It bothers me a lot.</span></div>
<div><span style="font-size: small;"><br></span></div><div><span style="font-size: small;">Thank you very much.</span></div><div><span style="font-size: small;"><br>
</span></div><div><span style="font-size: small;">Linbao</span></div></span>
<span>&lt;scatttest.cpp&gt;</span><span>&lt;ompi_info&gt;</span>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>

</div><br></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>

</div><br></div></div></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

