Assuming you aren&#39;t oversubscribing your nodes, set mpi_paffinity_alone=1.<br><br>BTW: did you set that mpi_show_mca_params option to ensure the app is actually seeing these params?<br><br><br><div class="gmail_quote">
On Tue, Jun 23, 2009 at 12:35 PM, Jim Kress <span dir="ltr">&lt;<a href="mailto:jimkress_58@kressworks.org">jimkress_58@kressworks.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
I assume you a referring to the openmpi-mca-params.conf file<br>
<br>
As I indicated previously, my first run was with the line<br>
<br>
btl=self,openib<br>
<br>
As the only entry in the openmpi-mca-params.conf file.  This my default<br>
setting and was what I used, and it worked well, for v 1.2.8<br>
<br>
Then I tried<br>
<br>
btl=self,openib<br>
mpi_yield_when_idle=0<br>
<br>
As the only entries in the openmpi-mca-params.conf file.  No difference in<br>
the results.<br>
<br>
Then I tried<br>
<br>
btl=self,openib<br>
mpi_yield_when_idle=0<br>
<br>
As the only entries in the openmpi-mca-params.conf file and also set the<br>
environment variable OMPI_MCA_mpi_leave_pinned=0<br>
No difference in the results.<br>
<br>
What else can I provide?<br>
<br>
By the way, did you read the message where I retracted my assumption about<br>
MPI traffic being forced over Ethernet?<br>
<font color="#888888"><br>
Jim<br>
</font><div><div></div><div class="h5"><br>
&gt; -----Original Message-----<br>
&gt; From: <a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a><br>
&gt; [mailto:<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>] On Behalf Of Pavel Shamis (Pasha)<br>
&gt; Sent: Tuesday, June 23, 2009 7:24 AM<br>
&gt; To: Open MPI Users<br>
&gt; Subject: Re: [OMPI users] 50% performance reduction due to<br>
&gt; OpenMPI v 1.3.2 forcing all MPI traffic over Ethernet instead<br>
&gt; of using Infiniband<br>
&gt;<br>
&gt; Jim,<br>
&gt; Can you please share with us you mca conf file.<br>
&gt;<br>
&gt; Pasha.<br>
&gt; Jim Kress ORG wrote:<br>
&gt; &gt; For the app I am using, ORCA (a Quantum Chemistry program), when it<br>
&gt; &gt; was compiled using openMPI 1.2.8 and run under 1.2.8 with the<br>
&gt; &gt; following in the openmpi-mca-params.conf file:<br>
&gt; &gt;<br>
&gt; &gt; btl=self,openib<br>
&gt; &gt;<br>
&gt; &gt; the app ran fine with no traffic over my Ethernet network and all<br>
&gt; &gt; traffic over my Infiniband network.<br>
&gt; &gt;<br>
&gt; &gt; However, now that ORCA has been recompiled with openMPI<br>
&gt; v1.3.2 and run<br>
&gt; &gt; under 1.3.2 (using the same openmpi-mca-params.conf file), the<br>
&gt; &gt; performance has been reduced by 50% and all the MPI traffic<br>
&gt; is going<br>
&gt; &gt; over the Ethernet network.<br>
&gt; &gt;<br>
&gt; &gt; As a matter of fact, the openMPI v1.3.2 performance now<br>
&gt; looks exactly<br>
&gt; &gt; like the performance I get if I use MPICH 1.2.7.<br>
&gt; &gt;<br>
&gt; &gt; Anyone have any ideas:<br>
&gt; &gt;<br>
&gt; &gt; 1) How could this have happened?<br>
&gt; &gt;<br>
&gt; &gt; 2) How can I fix it?<br>
&gt; &gt;<br>
&gt; &gt; a 50% reduction in performance is just not acceptable.  Ideas/<br>
&gt; &gt; suggestions would be appreciated.<br>
&gt; &gt;<br>
&gt; &gt; Jim<br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

