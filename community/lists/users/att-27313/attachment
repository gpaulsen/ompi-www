<div dir="ltr">You say that you can run the code OK &#39;by hand&#39; with an mpirun.<div><br></div><div>Are you assuming somehow that the Gridengine jobs will inherit your environment variables, paths etc?</div><div>If I remember correctly, you should submit wiht the  -V  option to pass over environment settings.</div><div>Even better, make sure that the job script itself sets all the paths and variables.</div><div>Have you looked at using the &#39;modules&#39; environment?</div><div><br></div><div>Also submit a job script and put the &#39;env&#39; command in as the first command.</div><div>Look at your output closely.</div><div><br></div><div><br></div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On 23 July 2015 at 15:45,  <span dir="ltr">&lt;<a href="mailto:m.delorme@surrey.ac.uk" target="_blank">m.delorme@surrey.ac.uk</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">




<div dir="ltr">
<div style="font-size:12pt;color:#000000;background-color:#ffffff;font-family:Calibri,Arial,Helvetica,sans-serif">
<p>Hello, </p>
<p><br>
</p>
<p>I have been working on this problem for the last week, browsing the help and the mailing list with no success.</p>
<p>While trying to run MPI programs using SGE, I end up with seg faults every time.</p>
<p><br>
</p>
<p>A bit of information on the system :</p>
<p><br>
</p>
<p>I am working on a 14 nodes cluster. Every node is an Intel Xeon, each composed of 2 sockets with 10 cores each (so 20 cores per node). The nodes are Infiniband connected. The job scheduler is Grid Engine as mentioned before.</p>
<p>Since I don&#39;t have the hand on the cluster administration, and the &quot;default&quot; installation of openMPI is an old one, I compiled and installed myself Open-MPI 1.8.6 and prepended paths (general and library) to ensure the use of my version of mpi.</p>
<p><br>
</p>
<p>Open MPI has been configured with the flags --with-sge, and grepping grid engine in ompi_info returns something that looks correct :</p>
<p><br>
</p>
<p>MCA ras: gridengine (MCA v2.0, API v2.0, Component v1.8.6)<br>
</p>
<p><br>
</p>
<p><br>
</p>
<p>Now when running a simple script, displaying the hostname, on two slots binded on one single node, I get the following message :</p>
<p><br>
</p>
<p>[galaxy1:44361] Error: unknown option &quot;--hnp-topo-sig&quot;</p>
<p>Segmentation fault</p>
<p>--------------------------------------------------------------------------</p>
<p>ORTE was unable to reliably start one or more daemons.</p>
<p>This usually is caused by:</p>
<p><br>
</p>
<p>* not finding the required libraries and/or binaries on</p>
<p>  one or more nodes. Please check your PATH and LD_LIBRARY_PATH</p>
<p>  settings, or configure OMPI with --enable-orterun-prefix-by-default</p>
<p><br>
</p>
<p>* lack of authority to execute on one or more specified nodes.</p>
<p>  Please verify your allocation and authorities.</p>
<p><br>
</p>
<p>* the inability to write startup files into /tmp (--tmpdir/orte_tmpdir_base).</p>
<p>  Please check with your sys admin to determine the correct location to use.</p>
<p><br>
</p>
<p>*  compilation of the orted with dynamic libraries when static are required</p>
<p>  (e.g., on Cray). Please check your configure cmd line and consider using</p>
<p>  one of the contrib/platform definitions for your system type.</p>
<p><br>
</p>
<p>* an inability to create a connection back to mpirun due to a</p>
<p>  lack of common network interfaces and/or no route found between</p>
<p>  them. Please check network connectivity (including firewalls</p>
<p>  and network routing requirements).</p>
<p>--------------------------------------------------------------------------</p>
<div><br>
</div>
<div><br>
</div>
<div>When I connect to the specific host crashing and try to run the program by hand with mpirun, the whole thing executes without problem.</div>
<div>I made sure the libraries and path are right, that I have the rights on the node, that /tmp is accessible. I don&#39;t think the fourth point of the list is the problem, as for the last one, I suppose that if I can access the node by sshing it, SGE shouldn&#39;t
 have a problem with it as well ...</div>
<div><br>
</div>
<div>My guess is then a problem from SGE or the integration of OpenMPI with SGE ... </div>
<div><br>
</div>
<div>I googled with no real success the &quot;hnp-topo-sig&quot;, and only got to a stackoverflow page indicating that the problem should be nodes running a different version of OpenMPI. </div>
<div>I know that there is an old OpenMPI version by default on the nodes, but shouldn&#39;t prepending the paths and exporting the environment (using the -V flag in the script) be sufficient to ensure the right version of openMPI is used ?</div>
<div><br>
</div>
<div>A bit of additional information, </div>
<div><br>
</div>
<div>qconf -se orte :</div>
<div><br>
</div>
<div>
<div>pe_name            orte</div>
<div>slots              2000</div>
<div>user_lists         NONE</div>
<div>xuser_lists        NONE</div>
<div>start_proc_args    /bin/true</div>
<div>stop_proc_args     /bin/true</div>
<div>allocation_rule    $fill_up</div>
<div>control_slaves     TRUE</div>
<div>job_is_first_task  FALSE</div>
<div>urgency_slots      min</div>
<div>accounting_summary FALSE</div>
<div>qsort_args         NONE</div>
</div>
<div><br>
</div>
<div><br>
</div>
<div>You will find attached the compressed log of ompi_info -a --parsable</div>
<div><br>
</div>
<div><br>
</div>
<div><br>
</div>
<div>Thank you very much in advance for any suggestion, </div>
<div><br>
</div>
<div><br>
</div>
<div>MD</div>
<div><br>
</div>
<div><br>
</div>
</div>
</div>

<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/07/27312.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/07/27312.php</a><br></blockquote></div><br></div>

