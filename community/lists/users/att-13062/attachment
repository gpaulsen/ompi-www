Hello,<div><br></div><div>I&#39;ve been having some troubles with OpenMPI 1.4.X and slurm recently.  I seem to be able to run jobs this way ok:</div><div>---</div><div><div>sh-3.1$ mpirun -np 2 mpi_hello</div><div>Hello, I am node cut1n7 with rank 0</div>

<div>Hello, I am node cut1n8 with rank 1</div></div><div>--</div><div><br></div><div>However if I try and use the -npernode option I get:</div><div>---</div><div><div>sh-3.1$ mpirun -npernode 1 mpi_hello</div><div>[cut1n7:16368] *** Process received signal ***</div>

<div>[cut1n7:16368] Signal: Segmentation fault (11)</div><div>[cut1n7:16368] Signal code: Address not mapped (1)</div><div>[cut1n7:16368] Failing at address: 0x50</div><div>[cut1n7:16368] [ 0] /lib64/libpthread.so.0 [0x37bda0de80]</div>

<div>[cut1n7:16368] [ 1] /apps/mpi/openmpi/1.4.2-gcc-4.1.2-may.12.10/lib/libopen-rte.so.0(orte_util_encode_pidmap+0xdb) [0x2b73eb84df8b]</div><div>[cut1n7:16368] [ 2] /apps/mpi/openmpi/1.4.2-gcc-4.1.2-may.12.10/lib/libopen-rte.so.0(orte_odls_base_default_get_add_procs_data+0x655) [0x2b73eb8592f5]</div>

<div>[cut1n7:16368] [ 3] /apps/mpi/openmpi/1.4.2-gcc-4.1.2-may.12.10/lib/libopen-rte.so.0(orte_plm_base_launch_apps+0x10b) [0x2b73eb86031b]</div><div>[cut1n7:16368] [ 4] /apps/mpi/openmpi/1.4.2-gcc-4.1.2-may.12.10/lib/openmpi/mca_plm_slurm.so [0x2b73ec709ecf]</div>

<div>[cut1n7:16368] [ 5] mpirun [0x40335a]</div><div>[cut1n7:16368] [ 6] mpirun [0x4029f3]</div><div>[cut1n7:16368] [ 7] /lib64/libc.so.6(__libc_start_main+0xf4) [0x37bce1d8b4]</div><div>[cut1n7:16368] [ 8] mpirun [0x402929]</div>

<div>[cut1n7:16368] *** End of error message ***</div><div>Segmentation fault</div></div><div>---</div><div><br></div><div>This is ompi 1.4.2, gcc 4.1.1 and slurm 2.0.9 ... I&#39;m sure it&#39;s a rather silly detail on my end, but figure I should start this thread for any insights and feedback I can help provide to resolve this.</div>

<div><br></div><div>Thanks,</div><div>-cdm</div>

