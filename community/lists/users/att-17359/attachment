<html><head><base href="x-msg://153/"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">Hmmm....perhaps you didn't notice the mpi_preconnect_all option? It does precisely what you described - it pushes zero-byte messages around a ring to force all the connections open at MPI_Init.<div><br></div><div><br><div><div>On Sep 20, 2011, at 3:06 PM, Henderson, Brent wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><span class="Apple-style-span" style="border-collapse: separate; font-family: Helvetica; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; "><div lang="EN-US" link="blue" vlink="purple"><div class="WordSection1" style="page: WordSection1; "><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; ">I recently had access to a 200+ node Magny Cours (24 ranks/host) 10G Linux cluster.&nbsp; I was able to use OpenMPI v1.5.4 with hello world, IMB and HPCC, but there were a couple of issues along the way. &nbsp;After setting some system tunables up a little bit on all of the nodes a hello_world program worked just fine – it appears that the TCP connections between most or all of the ranks are deferred until they are actually used so the easy test ran reasonably quickly.&nbsp; I then moved to IMB.&nbsp;<o:p></o:p></div><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; "><o:p>&nbsp;</o:p></div><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; ">I typically don’t care about the small rank counts, so I add the –npmin 99999 option to just run the ‘big’ number of ranks.&nbsp; This ended with an abort after MPI_Init(), but before running any tests.&nbsp; Lots (possibly all) of ranks emitted messages that looked like:<o:p></o:p></div><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; "><o:p>&nbsp;</o:p></div><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; ">&nbsp;&nbsp;&nbsp; ‘[n112][[13200,1],1858][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 172.23.4.1 failed: Connection timed out (110)’<o:p></o:p></div><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; "><o:p>&nbsp;</o:p></div><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; ">Where n112 is one of the nodes in the job, and 172.23.4.1 is the first node in the job.&nbsp; One of the first things that IMB does before running a test is create a communicator for each specific rank count it is testing.&nbsp; Apparently this collective operation causes a large number of connections to be made.&nbsp; The abort messages (one example shown above) all show the connect failure to a single node, so it would appear that a very large number of nodes attempt to connect to that one at the same time and overwhelmed it.&nbsp; (Or it was slow and everyone ganged up on it as they worked their way around the ring.&nbsp;<span class="Apple-converted-space">&nbsp;</span><span style="font-family: Wingdings; ">J</span>&nbsp; Is there a supported/suggested way to work around this?&nbsp; It was very repeatable.<o:p></o:p></div><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; "><o:p>&nbsp;</o:p></div><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; ">I was able to work around this by using the primary definitions for MPI_Init() and MPI_Init_thread() by calling the ‘P’ version of the routine, and then having each rank send its rank number to the rank one to the right, then two to the right, and so-on around the ring.&nbsp; I added a MPI_Barrier( MPI_COMM_WORLD ), call every N messages to keep things at a controlled pace.&nbsp; N was 64 by default, but settable via environment variable in case that number didn’t work well for some reason.&nbsp; This fully connected the mesh (110k socket connections per host!) and allowed the tests to run.&nbsp; Not a great solution, I know, but I’ll throw it out there until I know the right way.<o:p></o:p></div><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; "><o:p>&nbsp;</o:p></div><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; ">Once I had this in place, I used the workaround with HPCC as well.&nbsp; Without it, it would not get very far at all.&nbsp; With it, I was able to make it through the entire test.<o:p></o:p></div><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; "><o:p>&nbsp;</o:p></div><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; ">Looking forward to getting the experts thoughts about the best way to handle big TCP clusters – thanks!<o:p></o:p></div><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; "><o:p>&nbsp;</o:p></div><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; ">Brent<o:p></o:p></div><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; "><o:p>&nbsp;</o:p></div><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; ">P.S.&nbsp; v1.5.4 worked *<b>much</b>* better that v1.4.3 on this cluster – not sure why, but kudos to those working on changes since then!<o:p></o:p></div><div style="margin-top: 0in; margin-right: 0in; margin-left: 0in; margin-bottom: 0.0001pt; font-size: 11pt; font-family: Calibri, sans-serif; "><o:p>&nbsp;</o:p></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" style="color: blue; text-decoration: underline; ">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" style="color: blue; text-decoration: underline; ">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></span></blockquote></div><br></div></body></html>
