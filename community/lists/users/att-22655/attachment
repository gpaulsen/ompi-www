<div dir="ltr">Hi Jeff, <div><br></div><div style>Thank you very much. About the QRECS, it is an allocatable array, and it is introduced like:</div><div style><br></div><div style><div style="font-family:arial,sans-serif;font-size:13px">

if(myrank .ne. 0) then</div><div class="im" style="font-family:arial,sans-serif;font-size:13px"><div>     itag = myrank</div><div>     call mpi_send(Q.............., 0, itag, .................)</div><div>else </div><div>
     do i=1, N-1</div>
<div style>          <font color="#ff0000">allocate (QRECS(A(i)))</font></div><div>          itag = i</div><div>         call mpi_recv(QRECS......., i, itag, .................)</div><div style>          <font color="#ff0000">deallocate(QRECS)</font></div>

<div>     enddo</div><div>   </div><div>endif</div><div><br></div><div>call mpi_bcast(YVAR............., 0, ..............)</div><div style>Will this cause any problem using this way to introducing QRECS?</div><div><br></div>

<div><br></div><div style>Besides, the reasonable why I did not choose mpi_gatherv is that the QRECS will put into YVAR in a non-consecutive way. for instance, if I have 4 processors, the first element in YVAR is from rank 0, second from rank 1 ......fourth from rank 3, and then fifth from rank 0 again, sixth from rank 1 again....... But I will try your suggestion. </div>

<div style><br></div><div style>Thanks.</div></div></div></div><div class="gmail_extra"><br clear="all"><div><div dir="ltr"><div>best regards,<br>Huangwei</div><div><span style="font-family:&quot;Monotype Corsiva&quot;;font-size:12pt"><br>

</span></div><div> </div><div><br> </div><span></span><span></span><span></span></div></div>
<br><br><div class="gmail_quote">On 14 September 2013 09:19, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">

At first glance, your code doesn&#39;t look problematic.  First thing I&#39;d check is ensure that QRECS is large enough to hold the incoming data (i.e., that you aren&#39;t overwriting the buffer and causing memory corruption, which can cause weird/unexplained faults like this).<br>


<br>
Also, you might well be able to accomplish the same communication pattern with MPI_GATHER (or MPI_GATHERV, if each rank is sending a different amount of information).<br>
<br>
<br>
On Sep 14, 2013, at 12:27 AM, Huangwei &lt;<a href="mailto:hz283@cam.ac.uk">hz283@cam.ac.uk</a>&gt;<br>
<div><div class="h5"> wrote:<br>
<br>
&gt; The code I would like to post is like this:<br>
&gt;<br>
&gt; if(myrank .ne. 0) then<br>
&gt;      itag = myrank<br>
&gt;      call mpi_send(Q.............., 0, itag, .................)<br>
&gt; else<br>
&gt;      do i=1, N-1<br>
&gt;           itag = i<br>
&gt;          call mpi_recv(QRECS......., i, itag, .................)<br>
&gt;      enddo<br>
&gt;<br>
&gt; endif<br>
&gt;<br>
&gt; call mpi_bcast(YVAR............., 0, ..............)<br>
&gt;<br>
&gt; best regards,<br>
&gt; Huangwei<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; On 13 September 2013 23:25, Huangwei &lt;<a href="mailto:hz283@cam.ac.uk">hz283@cam.ac.uk</a>&gt; wrote:<br>
&gt; Dear All,<br>
&gt;<br>
&gt; I have a question about using MPI_send and MPI_recv.<br>
&gt;<br>
&gt; The object  is as follows:<br>
&gt; I would like to send an array Q from rank=1, N-1 to rank=0, and then rank 0 receives the Q from all other processors. Q will be put into a new array Y in rank 0. (of couse this is not realized by MPI). and then MPI_bcast is used (from rank 0) to broadcast the Y to all the processors.<br>


&gt;<br>
&gt; Fortran Code is like:<br>
&gt; if(myrank .eq. 0) then<br>
&gt;      itag = myrank<br>
&gt;      call mpi_send(Q.............., 0, itag, .................)<br>
&gt; else<br>
&gt;      do i=1, N-1<br>
&gt;           itag = i<br>
&gt;          call mpi_recv(QRECS......., i, itag, .................)<br>
&gt;      enddo<br>
&gt;<br>
&gt; endif<br>
&gt;<br>
&gt; call mpi_bcast(YVAR............., 0, ..............)<br>
&gt;<br>
&gt; Problem I met is:<br>
&gt; In my simulation, time marching is performed, These mpi_send and recv are fine for the first three time steps. However, for the fourth time step, the looping is only finished from i=1 to i=13, (totally 48 processors). That mean after 14th processors, the mpi_recv did not receive the date from them. Thus the code hangs there forever. Does deadlock occur for this situation? How can I figure out this problem?<br>


&gt;<br>
&gt; Thank you so much if anyone can give me some suggestions.<br>
&gt;<br>
&gt; best regards,<br>
&gt; Huangwei<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
</div></div>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<span class="HOEnZb"><font color="#888888"><br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</font></span></blockquote></div><br></div>

