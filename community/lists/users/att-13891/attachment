<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta content="text/html;charset=ISO-8859-15"
 http-equiv="Content-Type">
</head>
<body bgcolor="#ffffff" text="#000000">
In the posted irecv case if you are reading from the posted receive
buffer the problem is you may get one of three values:<br>
<br>
1.  pre irecv value<br>
2.  value received from the irecv in progress<br>
3.  possibly garbage if you are unlucky enough to access memory that is
at the same time being updated.  <br>
<br>
--td<br>
Alberto Canestrelli wrote:
<blockquote cite="mid:4C56CB44.4090702@idra.unipd.it" type="cite">Thanks,
  <br>
it was late in the night yesterday and i highlighted STORES but I
meanted to highlight LOADS! I know that
  <br>
stores are not allowed when you are doing non blocking send-recv. But I
was impressed about LOADS case. I always do some loads of the data
  <br>
between all my ISEND-IRECVs and my WAITs. Could  you please confirm me
that OMPI can handle the LOAD case? And if it cannot handle it, which
could be the consequence? What could happen in the worst of the case
when there is a data race in reading a data?
  <br>
thanks
  <br>
alberto
  <br>
  <br>
Il 02/08/2010 9.32, Alberto Canestrelli ha scritto:
  <br>
  <blockquote type="cite">I believe it is definitely a no-no to STORE
(write) into a send buffer
    <br>
while a send is posted. I know there have been debate in the forum to
    <br>
relax LOADS (reads) from a send buffer. I think OMPI can handle the
    <br>
latter case (LOADS). On the posted receive side you open yourself up
    <br>
for some race conditions and overwrites if you do STORES or LOADS from
a
    <br>
posted receive buffer.
    <br>
    <br>
--td
    <br>
    <br>
Alberto Canestrelli wrote:
    <br>
    <blockquote type="cite"> Hi,
      <br>
 I have a problem with a fortran code that I have parallelized with
      <br>
 MPI. I state in advance that I read the whole ebook "Mit Press - Mpi -
      <br>
 The Complete Reference, Volume 1" and I took different MPI classes, so
      <br>
 I have a discrete MPI knowledge. I was able to solve by myself all the
      <br>
 errors I bumped into but now I am not able to find the bug of my code
      <br>
 that provides erroneous results. Without entering in the details of my
      <br>
 code, I think that the cause of the problem could be reletad to the
      <br>
 following aspect highlighted in the above ebook (in the follow I copy
      <br>
 and paste from the e-book):
      <br>
      <br>
 A nonblocking post-send call indicates that the system may start
      <br>
 copying data
      <br>
 out of the send buffer. The sender must not access any part of the
      <br>
 send buffer
      <br>
 (neither for loads nor for STORES) after a nonblocking send operation
      <br>
 is posted until
      <br>
 the complete send returns.
      <br>
 A nonblocking post-receive indicates that the system may start writing
      <br>
 data into
      <br>
 the receive buffer. The receiver must not access any part of the
      <br>
 receive buffer after
      <br>
 a nonblocking receive operation is posted, until the complete-receive
      <br>
 returns.
      <br>
 Rationale. We prohibit read accesses to a send buffer while it is
      <br>
 being used, even
      <br>
 though the send operation is not supposed to alter the content of this
      <br>
 buffer. This
      <br>
 may seem more stringent than necessary, but the additional restriction
      <br>
 causes little
      <br>
 loss of functionality and allows better performance on some systems-
      <br>
 consider
      <br>
 the case where data transfer is done by a DMA engine that is not
      <br>
 cache-coherent
      <br>
 with the main processor.End of rationale.
      <br>
      <br>
 I use plenty of nonblocking post-send in my code. Is it really true
      <br>
 that the sender must not access any part of the send buffer not even
      <br>
 for STORES? Or was it a MPI 1.0 issue?
      <br>
 Thanks.
      <br>
 alberto
      <br>
 _______________________________________________
      <br>
 users mailing list
      <br>
 users_at_[hidden]
      <br>
 <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
      <br>
    </blockquote>
    <br>
  </blockquote>
  <br>
</blockquote>
<br>
<br>
<div class="moz-signature">-- <br>
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1">
<title></title>
<img moz-do-not-send="false"
 src="cid:part1.04060103.08040609@oracle.com" alt="Oracle"><br>
<div class="moz-signature">
<div class="moz-signature">
<div class="moz-signature">
<div class="moz-signature">Terry D. Dontje | Principal Software Engineer<br>
<div class="moz-signature"><font color="#666666" face="Verdana" size="2">Developer
Tools
Engineering | +1.650.633.7054<br>
</font>
<font color="#ff0000" face="Verdana" size="2">Oracle
</font><font color="#666666" face="Verdana" size="2"><b> - Performance
Technologies</b></font><br>
<font color="#666666" face="Verdana" size="2">
95 Network Drive, Burlington, MA 01803<br>
Email <a href="mailto:terry.dontje@oracle.com">terry.dontje@oracle.com</a><br>
</font><br>
</div>
</div>
</div>
</div>
</div>
</div>
</body>
</html>

