<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"><base></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">It's supposed to, so it sounds like we have a bug in the connection failover mechanism. I'll address it<div><br><div><div>On Jul 23, 2014, at 1:21 AM, Timur Ismagilov &lt;<a href="mailto:tismagilov@mail.ru">tismagilov@mail.ru</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;">Thanks, Ralph!<br>When I add&nbsp;--mca oob_tcp_if_include ib0 (where ib0 is infiniband interface from ifconfig) to mpirun it starts working correct!&nbsp;<br>Why OpenMPI doesn't do it itself?<br><br>Tue, 22 Jul 2014 11:26:16 -0700 от Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;:<br><blockquote style="border-left-width: 1px; border-left-style: solid; border-left-color: rgb(8, 87, 166); margin: 10px; padding: 0px 0px 0px 10px;"><div id=""><div class="js-helper js-readmsg-msg"><base target="_self" href="https://e.mail.ru/"><div id="style_14060536510000000294_BODY">Okay, the problem is that the connection back to mpirun isn't getting thru. We are trying on the 10.0.251.53 address - is that blocked, or should we be using something else? If so, you might want to direct us by adding "-mca oob_tcp_if_include foo", where foo is the interface you want us to use<div><br></div><div><br><div></div><div><div>On Jul 20, 2014, at 10:24 PM, Timur Ismagilov &lt;<a href="x-msg://e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt; wrote:</div><br><blockquote type="cite"><div style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><p><span data-mce-style="font-family: 'Lucida Sans Unicode', 'Lucida Sans', 'Lucida Grande', Verdana, Geneva, sans-serif;font-size: 13px;line-height: 20.812801361083984px;text-align: -webkit-right;" style="font-family: 'Lucida Sans Unicode', 'Lucida Sans', 'Lucida Grande', Verdana, Geneva, sans-serif; font-size: 13px; line-height: 20.812801361083984px; text-align: -webkit-right;">NIC = network interface controller?&nbsp;</span></p><p><span style="font-family: 'Lucida Sans Unicode', 'Lucida Sans', 'Lucida Grande', Verdana, Geneva, sans-serif; font-size: 13px; line-height: 20.812801361083984px; text-align: -webkit-right; color: rgb(34, 34, 34); white-space: nowrap;">There is QDR Infiniband 4x/10G Ethernet/Gigabit Ethernet.<span>&nbsp;</span><br>I want to use&nbsp;<span data-mce-style="color: #222222;font-family: 'Lucida Sans Unicode', 'Lucida Sans', 'Lucida Grande', Verdana, Geneva, sans-serif;font-size: 13.333333969116211px;line-height: 20.812803268432617px;text-align: -webkit-right;white-space: nowrap;" style="color: rgb(34, 34, 34); font-family: 'Lucida Sans Unicode', 'Lucida Sans', 'Lucida Grande', Verdana, Geneva, sans-serif; font-size: 13.333333969116211px; line-height: 20.812803268432617px; text-align: -webkit-right; white-space: nowrap;">QDR Infiniband.</span></span></p><p>Here is a new output:</p><p>$ mpirun -mca mca_base_env_list 'LD_PRELOAD' --debug-daemons --mca plm_base_verbose 5 -mca oob_base_verbose 10 -mca rml_base_verbose 10 -np 2 hello_c |tee hello.out<br>Warning: Conflicting CPU frequencies detected, using: 2927.000000.<br>[compiler-2:30735] mca:base:select:( plm) Querying component [isolated]<br>[compiler-2:30735] mca:base:select:( plm) Query of component [isolated] set priority to 0<br>[compiler-2:30735] mca:base:select:( plm) Querying component [rsh]<br>[compiler-2:30735] mca:base:select:( plm) Query of component [rsh] set priority to 10<br>[compiler-2:30735] mca:base:select:( plm) Querying component [slurm]<br>[compiler-2:30735] mca:base:select:( plm) Query of component [slurm] set priority to 75<br>[compiler-2:30735] mca:base:select:( plm) Selected component [slurm]<br>[compiler-2:30735] mca: base: components_register: registering oob components<br>[compiler-2:30735] mca: base: components_register: found loaded component tcp<br>[compiler-2:30735] mca: base: components_register: component tcp register function successful<br>[compiler-2:30735] mca: base: components_open: opening oob components<br>[compiler-2:30735] mca: base: components_open: found loaded component tcp<br>[compiler-2:30735] mca: base: components_open: component tcp open function successful<br>[compiler-2:30735] mca:oob:select: checking available component tcp<br>[compiler-2:30735] mca:oob:select: Querying component [tcp]<br>[compiler-2:30735] oob:tcp: component_available called<br>[compiler-2:30735] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>[compiler-2:30735] WORKING INTERFACE 2 KERNEL INDEX 3 FAMILY: V4<br>[compiler-2:30735] [[65177,0],0] oob:tcp:init adding 10.0.251.53 to our list of V4 connections<br>[compiler-2:30735] WORKING INTERFACE 3 KERNEL INDEX 4 FAMILY: V4<br>[compiler-2:30735] [[65177,0],0] oob:tcp:init adding 10.0.0.4 to our list of V4 connections<br>[compiler-2:30735] WORKING INTERFACE 4 KERNEL INDEX 5 FAMILY: V4<br>[compiler-2:30735] [[65177,0],0] oob:tcp:init adding 10.2.251.14 to our list of V4 connections<br>[compiler-2:30735] WORKING INTERFACE 5 KERNEL INDEX 6 FAMILY: V4<br>[compiler-2:30735] [[65177,0],0] oob:tcp:init adding 10.128.0.4 to our list of V4 connections<br>[compiler-2:30735] WORKING INTERFACE 6 KERNEL INDEX 7 FAMILY: V4<br>[compiler-2:30735] [[65177,0],0] oob:tcp:init adding 93.180.7.38 to our list of V4 connections<br>[compiler-2:30735] [[65177,0],0] TCP STARTUP<br>[compiler-2:30735] [[65177,0],0] attempting to bind to IPv4 port 0<br>[compiler-2:30735] [[65177,0],0] assigned IPv4 port 49759<br>[compiler-2:30735] mca:oob:select: Adding component to end<br>[compiler-2:30735] mca:oob:select: Found 1 active transports<br>[compiler-2:30735] mca: base: components_register: registering rml components<br>[compiler-2:30735] mca: base: components_register: found loaded component oob<br>[compiler-2:30735] mca: base: components_register: component oob has no register or open function<br>[compiler-2:30735] mca: base: components_open: opening rml components<br>[compiler-2:30735] mca: base: components_open: found loaded component oob<br>[compiler-2:30735] mca: base: components_open: component oob open function successful<br>[compiler-2:30735] orte_rml_base_select: initializing rml component oob<br>[compiler-2:30735] [[65177,0],0] posting recv<br>[compiler-2:30735] [[65177,0],0] posting persistent recv on tag 30 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:30735] [[65177,0],0] posting recv<br>[compiler-2:30735] [[65177,0],0] posting persistent recv on tag 15 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:30735] [[65177,0],0] posting recv<br>[compiler-2:30735] [[65177,0],0] posting persistent recv on tag 32 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:30735] [[65177,0],0] posting recv<br>[compiler-2:30735] [[65177,0],0] posting persistent recv on tag 33 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:30735] [[65177,0],0] posting recv<br>[compiler-2:30735] [[65177,0],0] posting persistent recv on tag 5 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:30735] [[65177,0],0] posting recv<br>[compiler-2:30735] [[65177,0],0] posting persistent recv on tag 10 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:30735] [[65177,0],0] posting recv<br>[compiler-2:30735] [[65177,0],0] posting persistent recv on tag 12 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:30735] [[65177,0],0] posting recv<br>[compiler-2:30735] [[65177,0],0] posting persistent recv on tag 9 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:30735] [[65177,0],0] posting recv<br>[compiler-2:30735] [[65177,0],0] posting persistent recv on tag 34 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:30735] [[65177,0],0] posting recv<br>[compiler-2:30735] [[65177,0],0] posting persistent recv on tag 2 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:30735] [[65177,0],0] posting recv<br>[compiler-2:30735] [[65177,0],0] posting persistent recv on tag 21 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:30735] [[65177,0],0] posting recv<br>[compiler-2:30735] [[65177,0],0] posting persistent recv on tag 22 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:30735] [[65177,0],0] posting recv<br>[compiler-2:30735] [[65177,0],0] posting persistent recv on tag 45 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:30735] [[65177,0],0] posting recv<br>[compiler-2:30735] [[65177,0],0] posting persistent recv on tag 46 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:30735] [[65177,0],0] posting recv<br>[compiler-2:30735] [[65177,0],0] posting persistent recv on tag 1 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:30735] [[65177,0],0] posting recv<br>[compiler-2:30735] [[65177,0],0] posting persistent recv on tag 27 for peer [[WILDCARD],WILDCARD]<br>Daemon was launched on node1-128-17 - beginning to initialize<br>Daemon was launched on node1-128-18 - beginning to initialize<br>[node1-128-17:14779] mca: base: components_register: registering oob components<br>[node1-128-17:14779] mca: base: components_register: found loaded component tcp<br>[node1-128-17:14779] mca: base: components_register: component tcp register function successful<br>[node1-128-17:14779] mca: base: components_open: opening oob components<br>[node1-128-17:14779] mca: base: components_open: found loaded component tcp<br>[node1-128-17:14779] mca: base: components_open: component tcp open function successful<br>[node1-128-17:14779] mca:oob:select: checking available component tcp<br>[node1-128-17:14779] mca:oob:select: Querying component [tcp]<br>[node1-128-17:14779] oob:tcp: component_available called<br>[node1-128-17:14779] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>[node1-128-17:14779] WORKING INTERFACE 2 KERNEL INDEX 3 FAMILY: V4<br>[node1-128-17:14779] [[65177,0],1] oob:tcp:init adding 10.0.128.17 to our list of V4 connections<br>[node1-128-17:14779] WORKING INTERFACE 3 KERNEL INDEX 4 FAMILY: V4<br>[node1-128-17:14779] [[65177,0],1] oob:tcp:init adding 10.128.128.17 to our list of V4 connections<br>[node1-128-17:14779] [[65177,0],1] TCP STARTUP<br>[node1-128-17:14779] [[65177,0],1] attempting to bind to IPv4 port 0<br>[node1-128-17:14779] [[65177,0],1] assigned IPv4 port 46441<br>[node1-128-17:14779] mca:oob:select: Adding component to end<br>[node1-128-17:14779] mca:oob:select: Found 1 active transports<br>[node1-128-17:14779] mca: base: components_register: registering rml components<br>[node1-128-17:14779] mca: base: components_register: found loaded component oob<br>[node1-128-17:14779] mca: base: components_register: component oob has no register or open function<br>[node1-128-17:14779] mca: base: components_open: opening rml components<br>[node1-128-17:14779] mca: base: components_open: found loaded component oob<br>[node1-128-17:14779] mca: base: components_open: component oob open function successful<br>[node1-128-17:14779] orte_rml_base_select: initializing rml component oob<br>[node1-128-18:17849] mca: base: components_register: registering oob components<br>[node1-128-18:17849] mca: base: components_register: found loaded component tcp<br>[node1-128-18:17849] mca: base: components_register: component tcp register function successful<br>[node1-128-18:17849] mca: base: components_open: opening oob components<br>[node1-128-18:17849] mca: base: components_open: found loaded component tcp<br>[node1-128-18:17849] mca: base: components_open: component tcp open function successful<br>[node1-128-18:17849] mca:oob:select: checking available component tcp<br>[node1-128-18:17849] mca:oob:select: Querying component [tcp]<br>[node1-128-18:17849] oob:tcp: component_available called<br>[node1-128-18:17849] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>[node1-128-18:17849] WORKING INTERFACE 2 KERNEL INDEX 3 FAMILY: V4<br>[node1-128-18:17849] [[65177,0],2] oob:tcp:init adding 10.0.128.18 to our list of V4 connections<br>[node1-128-18:17849] WORKING INTERFACE 3 KERNEL INDEX 4 FAMILY: V4<br>[node1-128-18:17849] [[65177,0],2] oob:tcp:init adding 10.128.128.18 to our list of V4 connections<br>[node1-128-18:17849] [[65177,0],2] TCP STARTUP<br>[node1-128-18:17849] [[65177,0],2] attempting to bind to IPv4 port 0<br>[node1-128-18:17849] [[65177,0],2] assigned IPv4 port 60695<br>[node1-128-18:17849] mca:oob:select: Adding component to end<br>[node1-128-18:17849] mca:oob:select: Found 1 active transports<br>[node1-128-18:17849] mca: base: components_register: registering rml components<br>[node1-128-18:17849] mca: base: components_register: found loaded component oob<br>[node1-128-18:17849] mca: base: components_register: component oob has no register or open function<br>[node1-128-18:17849] mca: base: components_open: opening rml components<br>[node1-128-18:17849] mca: base: components_open: found loaded component oob<br>[node1-128-18:17849] mca: base: components_open: component oob open function successful<br>[node1-128-18:17849] orte_rml_base_select: initializing rml component oob<br>Daemon [[65177,0],1] checking in as pid 14779 on host node1-128-17<br>[node1-128-17:14779] [[65177,0],1] orted: up and running - waiting for commands!<br>[node1-128-17:14779] [[65177,0],1] posting recv<br>[node1-128-17:14779] [[65177,0],1] posting persistent recv on tag 30 for peer [[WILDCARD],WILDCARD]<br>[node1-128-17:14779] [[65177,0],1] posting recv<br>[node1-128-17:14779] [[65177,0],1] posting persistent recv on tag 15 for peer [[WILDCARD],WILDCARD]<br>[node1-128-17:14779] [[65177,0],1] posting recv<br>[node1-128-17:14779] [[65177,0],1] posting persistent recv on tag 32 for peer [[WILDCARD],WILDCARD]<br>[node1-128-17:14779] [[65177,0],1] posting recv<br>[node1-128-17:14779] [[65177,0],1] posting persistent recv on tag 11 for peer [[WILDCARD],WILDCARD]<br>[node1-128-17:14779] [[65177,0],1] posting recv<br>[node1-128-17:14779] [[65177,0],1] posting persistent recv on tag 9 for peer [[WILDCARD],WILDCARD]<br>[node1-128-17:14779] [[65177,0],1]: set_addr to uri 4271439872.0;<a target="_blank">tcp://10.0.251.53,10.0.0.4,10.2.251.14,10.128.0.4,93.180.7.38:49759</a><br>[node1-128-17:14779] [[65177,0],1]:set_addr checking if peer [[65177,0],0] is reachable via component tcp<br>[node1-128-17:14779] [[65177,0],1] oob:tcp: working peer [[65177,0],0] address<span>&nbsp;</span><a target="_blank">tcp://10.0.251.53,10.0.0.4,10.2.251.14,10.128.0.4,93.180.7.38:49759</a><br>[node1-128-17:14779] [[65177,0],1] PASSING ADDR 10.0.251.53 TO MODULE<br>[node1-128-17:14779] [[65177,0],1]:tcp set addr for peer [[65177,0],0]<br>[node1-128-17:14779] [[65177,0],1] PASSING ADDR 10.0.0.4 TO MODULE<br>[node1-128-17:14779] [[65177,0],1]:tcp set addr for peer [[65177,0],0]<br>[node1-128-17:14779] [[65177,0],1] PASSING ADDR 10.2.251.14 TO MODULE<br>[node1-128-17:14779] [[65177,0],1]:tcp set addr for peer [[65177,0],0]<br>[node1-128-17:14779] [[65177,0],1] PASSING ADDR 10.128.0.4 TO MODULE<br>[node1-128-17:14779] [[65177,0],1]:tcp set addr for peer [[65177,0],0]<br>[node1-128-17:14779] [[65177,0],1] PASSING ADDR 93.180.7.38 TO MODULE<br>[node1-128-17:14779] [[65177,0],1]:tcp set addr for peer [[65177,0],0]<br>[node1-128-17:14779] [[65177,0],1]: peer [[65177,0],0] is reachable via component tcp<br>[node1-128-17:14779] [[65177,0],1] posting recv<br>[node1-128-17:14779] [[65177,0],1] posting persistent recv on tag 3 for peer [[WILDCARD],WILDCARD]<br>[node1-128-17:14779] [[65177,0],1] posting recv<br>[node1-128-17:14779] [[65177,0],1] posting persistent recv on tag 21 for peer [[WILDCARD],WILDCARD]<br>[node1-128-17:14779] [[65177,0],1] posting recv<br>[node1-128-17:14779] [[65177,0],1] posting persistent recv on tag 45 for peer [[WILDCARD],WILDCARD]<br>[node1-128-17:14779] [[65177,0],1] posting recv<br>[node1-128-17:14779] [[65177,0],1] posting persistent recv on tag 46 for peer [[WILDCARD],WILDCARD]<br>[node1-128-17:14779] [[65177,0],1] posting recv<br>[node1-128-17:14779] [[65177,0],1] posting persistent recv on tag 1 for peer [[WILDCARD],WILDCARD]<br>[node1-128-17:14779] [[65177,0],1] OOB_SEND: rml_oob_send.c:199<br>[node1-128-17:14779] [[65177,0],1]:tcp:processing set_peer cmd<br>[node1-128-17:14779] [[65177,0],1]:tcp:processing set_peer cmd<br>[node1-128-17:14779] [[65177,0],1]:tcp:processing set_peer cmd<br>[node1-128-17:14779] [[65177,0],1]:tcp:processing set_peer cmd<br>[node1-128-17:14779] [[65177,0],1]:tcp:processing set_peer cmd<br>[node1-128-17:14779] [[65177,0],1] oob:base:send to target [[65177,0],0]<br>[node1-128-17:14779] [[65177,0],1] oob:tcp:send_nb to peer [[65177,0],0]:10<br>[node1-128-17:14779] [[65177,0],1] tcp:send_nb to peer [[65177,0],0]<br>[node1-128-17:14779] [[65177,0],1]:[oob_tcp.c:484] post send to [[65177,0],0]<br>[node1-128-17:14779] [[65177,0],1]:[oob_tcp.c:421] processing send to peer [[65177,0],0]:10<br>[node1-128-17:14779] [[65177,0],1]:[oob_tcp.c:455] queue pending to [[65177,0],0]<br>[node1-128-17:14779] [[65177,0],1] tcp:send_nb: initiating connection to [[65177,0],0]<br>[node1-128-17:14779] [[65177,0],1]:[oob_tcp.c:469] connect to [[65177,0],0]<br>[node1-128-17:14779] [[65177,0],1] orte_tcp_peer_try_connect: attempting to connect to proc [[65177,0],0]<br>[node1-128-17:14779] [[65177,0],1] orte_tcp_peer_try_connect: attempting to connect to proc [[65177,0],0] on socket 10<br>[node1-128-17:14779] [[65177,0],1] orte_tcp_peer_try_connect: attempting to connect to proc [[65177,0],0] on 10.0.251.53:49759 - 0 retries<br>[node1-128-17:14779] [[65177,0],1] waiting for connect completion to [[65177,0],0] - activating send event<br>Daemon [[65177,0],2] checking in as pid 17849 on host node1-128-18<br>[node1-128-18:17849] [[65177,0],2] orted: up and running - waiting for commands!<br>[node1-128-18:17849] [[65177,0],2] posting recv<br>[node1-128-18:17849] [[65177,0],2] posting persistent recv on tag 30 for peer [[WILDCARD],WILDCARD]<br>[node1-128-18:17849] [[65177,0],2] posting recv<br>[node1-128-18:17849] [[65177,0],2] posting persistent recv on tag 15 for peer [[WILDCARD],WILDCARD]<br>[node1-128-18:17849] [[65177,0],2] posting recv<br>[node1-128-18:17849] [[65177,0],2] posting persistent recv on tag 32 for peer [[WILDCARD],WILDCARD]<br>[node1-128-18:17849] [[65177,0],2] posting recv<br>[node1-128-18:17849] [[65177,0],2] posting persistent recv on tag 11 for peer [[WILDCARD],WILDCARD]<br>[node1-128-18:17849] [[65177,0],2] posting recv<br>[node1-128-18:17849] [[65177,0],2] posting persistent recv on tag 9 for peer [[WILDCARD],WILDCARD]<br>[node1-128-18:17849] [[65177,0],2]: set_addr to uri 4271439872.0;<a target="_blank">tcp://10.0.251.53,10.0.0.4,10.2.251.14,10.128.0.4,93.180.7.38:49759</a><br>[node1-128-18:17849] [[65177,0],2]:set_addr checking if peer [[65177,0],0] is reachable via component tcp<br>[node1-128-18:17849] [[65177,0],2] oob:tcp: working peer [[65177,0],0] address<span>&nbsp;</span><a target="_blank">tcp://10.0.251.53,10.0.0.4,10.2.251.14,10.128.0.4,93.180.7.38:49759</a><br>[node1-128-18:17849] [[65177,0],2] PASSING ADDR 10.0.251.53 TO MODULE<br>[node1-128-18:17849] [[65177,0],2]:tcp set addr for peer [[65177,0],0]<br>[node1-128-18:17849] [[65177,0],2] PASSING ADDR 10.0.0.4 TO MODULE<br>[node1-128-18:17849] [[65177,0],2]:tcp set addr for peer [[65177,0],0]<br>[node1-128-18:17849] [[65177,0],2] PASSING ADDR 10.2.251.14 TO MODULE<br>[node1-128-18:17849] [[65177,0],2]:tcp set addr for peer [[65177,0],0]<br>[node1-128-18:17849] [[65177,0],2] PASSING ADDR 10.128.0.4 TO MODULE<br>[node1-128-18:17849] [[65177,0],2]:tcp set addr for peer [[65177,0],0]<br>[node1-128-18:17849] [[65177,0],2] PASSING ADDR 93.180.7.38 TO MODULE<br>[node1-128-18:17849] [[65177,0],2]:tcp set addr for peer [[65177,0],0]<br>[node1-128-18:17849] [[65177,0],2]: peer [[65177,0],0] is reachable via component tcp<br>[node1-128-18:17849] [[65177,0],2] posting recv<br>[node1-128-18:17849] [[65177,0],2] posting persistent recv on tag 3 for peer [[WILDCARD],WILDCARD]<br>[node1-128-18:17849] [[65177,0],2] posting recv<br>[node1-128-18:17849] [[65177,0],2] posting persistent recv on tag 21 for peer [[WILDCARD],WILDCARD]<br>[node1-128-18:17849] [[65177,0],2] posting recv<br>[node1-128-18:17849] [[65177,0],2] posting persistent recv on tag 45 for peer [[WILDCARD],WILDCARD]<br>[node1-128-18:17849] [[65177,0],2] posting recv<br>[node1-128-18:17849] [[65177,0],2] posting persistent recv on tag 46 for peer [[WILDCARD],WILDCARD]<br>[node1-128-18:17849] [[65177,0],2] posting recv<br>[node1-128-18:17849] [[65177,0],2] posting persistent recv on tag 1 for peer [[WILDCARD],WILDCARD]<br>[node1-128-18:17849] [[65177,0],2] OOB_SEND: rml_oob_send.c:199<br>[node1-128-18:17849] [[65177,0],2]:tcp:processing set_peer cmd<br>[node1-128-18:17849] [[65177,0],2]:tcp:processing set_peer cmd<br>[node1-128-18:17849] [[65177,0],2]:tcp:processing set_peer cmd<br>[node1-128-18:17849] [[65177,0],2]:tcp:processing set_peer cmd<br>[node1-128-18:17849] [[65177,0],2]:tcp:processing set_peer cmd<br>[node1-128-18:17849] [[65177,0],2] oob:base:send to target [[65177,0],0]<br>[node1-128-18:17849] [[65177,0],2] oob:tcp:send_nb to peer [[65177,0],0]:10<br>[node1-128-18:17849] [[65177,0],2] tcp:send_nb to peer [[65177,0],0]<br>[node1-128-18:17849] [[65177,0],2]:[oob_tcp.c:484] post send to [[65177,0],0]<br>[node1-128-18:17849] [[65177,0],2]:[oob_tcp.c:421] processing send to peer [[65177,0],0]:10<br>[node1-128-18:17849] [[65177,0],2]:[oob_tcp.c:455] queue pending to [[65177,0],0]<br>[node1-128-18:17849] [[65177,0],2] tcp:send_nb: initiating connection to [[65177,0],0]<br>[node1-128-18:17849] [[65177,0],2]:[oob_tcp.c:469] connect to [[65177,0],0]<br>[node1-128-18:17849] [[65177,0],2] orte_tcp_peer_try_connect: attempting to connect to proc [[65177,0],0]<br>[node1-128-18:17849] [[65177,0],2] orte_tcp_peer_try_connect: attempting to connect to proc [[65177,0],0] on socket 10<br>[node1-128-18:17849] [[65177,0],2] orte_tcp_peer_try_connect: attempting to connect to proc [[65177,0],0] on 10.0.251.53:49759 - 0 retries<br>[node1-128-18:17849] [[65177,0],2] waiting for connect completion to [[65177,0],0] - activating send event<br>[node1-128-18:17837] [[61806,0],2] tcp:send_handler called to send to peer [[61806,0],0]<br>[node1-128-18:17837] [[61806,0],2] tcp:send_handler CONNECTING<br>[node1-128-18:17837] [[61806,0],2]:tcp:complete_connect called for peer [[61806,0],0] on socket 10<br>[node1-128-18:17837] [[61806,0],2]-[[61806,0],0] tcp_peer_complete_connect: connection failed: Connection timed out (110)<br>[node1-128-18:17837] [[61806,0],2] tcp_peer_close for [[61806,0],0] sd 10 state CONNECTING<br>[node1-128-18:17837] [[61806,0],2] tcp:lost connection called for peer [[61806,0],0]<br>[node1-128-18:17837] mca: base: close: component oob closed<br>[node1-128-18:17837] mca: base: close: unloading component oob<br>[node1-128-18:17837] [[61806,0],2] TCP SHUTDOWN<br>[node1-128-18:17837] [[61806,0],2] RELEASING PEER OBJ [[61806,0],0]<br>[node1-128-18:17837] [[61806,0],2] CLOSING SOCKET 10<br>[node1-128-18:17837] mca: base: close: component tcp closed<br>[node1-128-18:17837] mca: base: close: unloading component tcp<br>srun: error: node1-128-18: task 1: Exited with exit code 1<br>srun: Terminating job step 647191.1<br>[node1-128-17:14767] [[61806,0],1] tcp:send_handler called to send to peer [[61806,0],0]<br>[node1-128-17:14767] [[61806,0],1] tcp:send_handler CONNECTING<br>[node1-128-17:14767] [[61806,0],1]:tcp:complete_connect called for peer [[61806,0],0] on socket 10<br>[node1-128-17:14767] [[61806,0],1]-[[61806,0],0] tcp_peer_complete_connect: connection failed: Connection timed out (110)<br>[node1-128-17:14767] [[61806,0],1] tcp_peer_close for [[61806,0],0] sd 10 state CONNECTING<br>[node1-128-17:14767] [[61806,0],1] tcp:lost connection called for peer [[61806,0],0]<br>[node1-128-17:14767] mca: base: close: component oob closed<br>[node1-128-17:14767] mca: base: close: unloading component oob<br>[node1-128-17:14767] [[61806,0],1] TCP SHUTDOWN<br>[node1-128-17:14767] [[61806,0],1] RELEASING PEER OBJ [[61806,0],0]<br>[node1-128-17:14767] [[61806,0],1] CLOSING SOCKET 10<br>[node1-128-17:14767] mca: base: close: component tcp closed<br>[node1-128-17:14767] mca: base: close: unloading component tcp<br>srun: error: node1-128-17: task 0: Exited with exit code 1<br>[node1-128-17:14779] [[65177,0],1] tcp:send_handler called to send to peer [[65177,0],0]<br>[node1-128-17:14779] [[65177,0],1] tcp:send_handler CONNECTING<br>[node1-128-17:14779] [[65177,0],1]:tcp:complete_connect called for peer [[65177,0],0] on socket 10<br>[node1-128-17:14779] [[65177,0],1]-[[65177,0],0] tcp_peer_complete_connect: connection failed: Connection timed out (110)<br>[node1-128-17:14779] [[65177,0],1] tcp_peer_close for [[65177,0],0] sd 10 state CONNECTING<br>[node1-128-17:14779] [[65177,0],1] tcp:lost connection called for peer [[65177,0],0]<br>[node1-128-17:14779] mca: base: close: component oob closed<br>[node1-128-17:14779] mca: base: close: unloading component oob<br>[node1-128-17:14779] [[65177,0],1] TCP SHUTDOWN<br>[node1-128-17:14779] [[65177,0],1] RELEASING PEER OBJ [[65177,0],0]<br>[node1-128-17:14779] [[65177,0],1] CLOSING SOCKET 10<br>[node1-128-17:14779] mca: base: close: component tcp closed<br>[node1-128-17:14779] mca: base: close: unloading component tcp<br>[node1-128-18:17849] [[65177,0],2] tcp:send_handler called to send to peer [[65177,0],0]<br>[node1-128-18:17849] [[65177,0],2] tcp:send_handler CONNECTING<br>[node1-128-18:17849] [[65177,0],2]:tcp:complete_connect called for peer [[65177,0],0] on socket 10<br>[node1-128-18:17849] [[65177,0],2]-[[65177,0],0] tcp_peer_complete_connect: connection failed: Connection timed out (110)<br>[node1-128-18:17849] [[65177,0],2] tcp_peer_close for [[65177,0],0] sd 10 state CONNECTING<br>[node1-128-18:17849] [[65177,0],2] tcp:lost connection called for peer [[65177,0],0]<br>[node1-128-18:17849] mca: base: close: component oob closed<br>[node1-128-18:17849] mca: base: close: unloading component oob<br>[node1-128-18:17849] [[65177,0],2] TCP SHUTDOWN<br>[node1-128-18:17849] [[65177,0],2] RELEASING PEER OBJ [[65177,0],0]<br>[node1-128-18:17849] [[65177,0],2] CLOSING SOCKET 10<br>[node1-128-18:17849] mca: base: close: component tcp closed<br>[node1-128-18:17849] mca: base: close: unloading component tcp<br>srun: error: node1-128-17: task 0: Exited with exit code 1<br>srun: Terminating job step 647191.2<br>srun: error: node1-128-18: task 1: Exited with exit code 1<br>--------------------------------------------------------------------------<br>An ORTE daemon has unexpectedly failed after launch and before<br>communicating back to mpirun. This could be caused by a number<br>of factors, including an inability to create a connection back<br>to mpirun due to a lack of common network interfaces and/or no<br>route found between them. Please check network connectivity<br>(including firewalls and network routing requirements).<br>--------------------------------------------------------------------------<br>[compiler-2:30735] [[65177,0],0] orted_cmd: received halt_vm cmd<br>[compiler-2:30735] mca: base: close: component oob closed<br>[compiler-2:30735] mca: base: close: unloading component oob<br>[compiler-2:30735] [[65177,0],0] TCP SHUTDOWN<br>[compiler-2:30735] mca: base: close: component tcp closed<br>[compiler-2:30735] mca: base: close: unloading component tcp</p><br><br><br>Sun, 20 Jul 2014 13:11:19 -0700 от Ralph Castain &lt;<a href="x-msg://e.mail.ru/compose/?mailto=mailto%3arhc@open%2dmpi.org" target="_blank">rhc@open-mpi.org</a>&gt;:<br><blockquote style="border-left-width: 1px; border-left-style: solid; border-left-color: rgb(8, 87, 166); margin: 10px; padding: 0px 0px 0px 10px;">Yeah, we aren't connecting back - is there a firewall running? &nbsp;You need to leave the "--debug-daemons --mca plm_base_verbose 5" on there as well to see the entire problem.<div><br></div><div>What you can see here is that mpirun is listening on several interfaces:</div><div><blockquote type="cite"><p>[access1:24264] [[55095,0],0] oob:tcp:init adding 10.0.251.51 to our list of V4 connections</p></blockquote><blockquote type="cite"><p>[access1:24264] [[55095,0],0] oob:tcp:init adding 10.2.251.11 to our list of V4 connections</p></blockquote><blockquote type="cite"><p>[access1:24264] [[55095,0],0] oob:tcp:init adding 10.0.0.111 to our list of V4 connections<br></p></blockquote><blockquote type="cite"><p>[access1:24264] [[55095,0],0] oob:tcp:init adding 10.128.0.1 to our list of V4 connections<br></p></blockquote></div><div><div><blockquote type="cite"><p>[access1:24264] [[55095,0],0] oob:tcp:init adding 93.180.7.36 to our list of V4 connections<br></p></blockquote><br></div><div>It looks like you have multiple interfaces connected to the same subnet - this is generally a bad idea. I also saw that the last one in the list shows up twice in the kernel array - not sure why, but is there something special about that NIC?</div><div><br></div><div>What do the NICs look like on the remote hosts?</div><div><br></div></div><div><div>On Jul 20, 2014, at 10:59 AM, Timur Ismagilov &lt;<a target="_blank">tismagilov@mail.ru</a>&gt; wrote:</div><br><blockquote type="cite"><div style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><br><br><br>-------- Пересылаемое сообщение --------<br>От кого: Timur Ismagilov &lt;<a target="_blank">tismagilov@mail.ru</a>&gt;<br>Кому: Ralph Castain &lt;<a target="_blank">rhc@open-mpi.org</a>&gt;<br>Дата: Sun, 20 Jul 2014 21:58:41 +0400<br>Тема: Re[2]: [OMPI users] Fwd: Re[4]: Salloc and mpirun problem<br><br><div><p>Here it is:</p><p>$ salloc -N2 --exclusive -p test -J ompi<br>salloc: Granted job allocation 647049<br><br></p><p>$ mpirun -mca mca_base_env_list 'LD_PRELOAD' -mca oob_base_verbose 10 -mca rml_base_verbose 10 -np 2 hello_c<br></p><p>[access1:24264] mca: base: components_register: registering oob components<br>[access1:24264] mca: base: components_register: found loaded component tcp<br>[access1:24264] mca: base: components_register: component tcp register function successful<br>[access1:24264] mca: base: components_open: opening oob components<br>[access1:24264] mca: base: components_open: found loaded component tcp<br>[access1:24264] mca: base: components_open: component tcp open function successful<br>[access1:24264] mca:oob:select: checking available component tcp<br>[access1:24264] mca:oob:select: Querying component [tcp]<br>[access1:24264] oob:tcp: component_available called<br>[access1:24264] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>[access1:24264] WORKING INTERFACE 2 KERNEL INDEX 3 FAMILY: V4<br>[access1:24264] [[55095,0],0] oob:tcp:init adding 10.0.251.51 to our list of V4 connections<br>[access1:24264] WORKING INTERFACE 3 KERNEL INDEX 4 FAMILY: V4<br>[access1:24264] [[55095,0],0] oob:tcp:init adding 10.0.0.111 to our list of V4 connections<br>[access1:24264] WORKING INTERFACE 4 KERNEL INDEX 5 FAMILY: V4<br>[access1:24264] [[55095,0],0] oob:tcp:init adding 10.2.251.11 to our list of V4 connections<br>[access1:24264] WORKING INTERFACE 5 KERNEL INDEX 6 FAMILY: V4<br>[access1:24264] [[55095,0],0] oob:tcp:init adding 10.128.0.1 to our list of V4 connections<br>[access1:24264] WORKING INTERFACE 6 KERNEL INDEX 7 FAMILY: V4<br>[access1:24264] [[55095,0],0] oob:tcp:init adding 93.180.7.36 to our list of V4 connections<br>[access1:24264] WORKING INTERFACE 7 KERNEL INDEX 7 FAMILY: V4<br>[access1:24264] [[55095,0],0] oob:tcp:init adding 93.180.7.36 to our list of V4 connections<br>[access1:24264] [[55095,0],0] TCP STARTUP<br>[access1:24264] [[55095,0],0] attempting to bind to IPv4 port 0<br>[access1:24264] [[55095,0],0] assigned IPv4 port 47756<br>[access1:24264] mca:oob:select: Adding component to end<br>[access1:24264] mca:oob:select: Found 1 active transports<br>[access1:24264] mca: base: components_register: registering rml components<br>[access1:24264] mca: base: components_register: found loaded component oob<br>[access1:24264] mca: base: components_register: component oob has no register or open function<br>[access1:24264] mca: base: components_open: opening rml components<br>[access1:24264] mca: base: components_open: found loaded component oob<br>[access1:24264] mca: base: components_open: component oob open function successful<br>[access1:24264] orte_rml_base_select: initializing rml component oob<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 30 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 15 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 32 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 33 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 5 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 10 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 12 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 9 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 34 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 2 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 21 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 22 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 45 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 46 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 1 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 27 for peer [[WILDCARD],WILDCARD]<br>--------------------------------------------------------------------------<br>An ORTE daemon has unexpectedly failed after launch and before<br>communicating back to mpirun. This could be caused by a number<br>of factors, including an inability to create a connection back<br>to mpirun due to a lack of common network interfaces and/or no<br>route found between them. Please check network connectivity<br>(including firewalls and network routing requirements).<br>--------------------------------------------------------------------------<br>[access1:24264] mca: base: close: component oob closed<br>[access1:24264] mca: base: close: unloading component oob<br>[access1:24264] [[55095,0],0] TCP SHUTDOWN<br>[access1:24264] mca: base: close: component tcp closed<br>[access1:24264] mca: base: close: unloading component tcp</p><p>When i use srun i got:</p><p>$ salloc -N2 --exclusive -p test -J ompi<br>....<br>$srun -N 2 ./hello_c<br>Hello, world, I am 0 of 1, (Open MPI v1.9a1, package: Open MPI semenov@compiler-2 Distribution, ident: 1.9a1r32252, repo rev: r32252, Jul 16, 2014 (nightly snapshot tarball), 146)<br>Hello, world, I am 0 of 1, (Open MPI v1.9a1, package: Open MPI semenov@compiler-2 Distribution, ident: 1.9a1r32252, repo rev: r32252, Jul 16, 2014 (nightly snapshot tarball), 146)</p><p><br>Sun, 20 Jul 2014 09:28:13 -0700 от Ralph Castain &lt;<a target="_blank">rhc@open-mpi.org</a>&gt;:<br></p><blockquote style="border-left-width: 1px; border-left-style: solid; border-left-color: rgb(8, 87, 166); margin: 10px; padding: 0px 0px 0px 10px;">Try adding -mca oob_base_verbose 10 -mca rml_base_verbose 10 to your cmd line. It looks to me like we are unable to connect back to the node where you are running mpirun for some reason.<div><br></div><div><br><div><div>On Jul 20, 2014, at 9:16 AM, Timur Ismagilov &lt;<a target="_blank">tismagilov@mail.ru</a>&gt; wrote:</div><br><blockquote type="cite"><div style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><p>I have the same problem in openmpi 1.8.1(<span data-mce-style="font-family: verdana, arial, helvetica;text-align: -webkit-right;" style="font-family: verdana, arial, helvetica; text-align: -webkit-right;">Apr 23, 2014</span>).<br>Does the srun command have &nbsp;a --map-by&lt;foo&gt; mpirun parameter, or can i chage it from bash enviroment?</p><p><br><br>-------- Пересылаемое сообщение --------<br>От кого: Timur Ismagilov &lt;<a target="_blank">tismagilov@mail.ru</a>&gt;<br>Кому: Mike Dubman &lt;<a target="_blank">miked@dev.mellanox.co.il</a>&gt;<br>Копия: Open MPI Users &lt;<a target="_blank">users@open-mpi.org</a>&gt;<br>Дата: Thu, 17 Jul 2014 16:42:24 +0400<br>Тема: Re[4]: [OMPI users] Salloc and mpirun problem<br><br></p><div><p>With Open MPI&nbsp;1.9a1r32252 (Jul 16, 2014 (nightly snapshot tarball))&nbsp;i got this output&nbsp;(same?):</p><p>$ salloc -N2 --exclusive -p test -J ompi<br>salloc: Granted job allocation 645686<br><br>$LD_PRELOAD=/mnt/data/users/dm2/vol3/semenov/_scratch/mxm/mxm-3.0/lib/libmxm.so &nbsp;mpirun &nbsp;-mca mca_base_env_list 'LD_PRELOAD' &nbsp;--mca plm_base_verbose 10 --debug-daemons -np 1 hello_c<br></p><p>[access1:04312] mca: base: components_register: registering plm components<br>[access1:04312] mca: base: components_register: found loaded component isolated<br>[access1:04312] mca: base: components_register: component isolated has no register or open function<br>[access1:04312] mca: base: components_register: found loaded component rsh<br>[access1:04312] mca: base: components_register: component rsh register function successful<br>[access1:04312] mca: base: components_register: found loaded component slurm<br>[access1:04312] mca: base: components_register: component slurm register function successful<br>[access1:04312] mca: base: components_open: opening plm components<br>[access1:04312] mca: base: components_open: found loaded component isolated<br>[access1:04312] mca: base: components_open: component isolated open function successful<br>[access1:04312] mca: base: components_open: found loaded component rsh<br>[access1:04312] mca: base: components_open: component rsh open function successful<br>[access1:04312] mca: base: components_open: found loaded component slurm<br>[access1:04312] mca: base: components_open: component slurm open function successful<br>[access1:04312] mca:base:select: Auto-selecting plm components<br>[access1:04312] mca:base:select:( plm) Querying component [isolated]<br>[access1:04312] mca:base:select:( plm) Query of component [isolated] set priority to 0<br>[access1:04312] mca:base:select:( plm) Querying component [rsh]<br>[access1:04312] mca:base:select:( plm) Query of component [rsh] set priority to 10<br>[access1:04312] mca:base:select:( plm) Querying component [slurm]<br>[access1:04312] mca:base:select:( plm) Query of component [slurm] set priority to 75<br>[access1:04312] mca:base:select:( plm) Selected component [slurm]<br>[access1:04312] mca: base: close: component isolated closed<br>[access1:04312] mca: base: close: unloading component isolated<br>[access1:04312] mca: base: close: component rsh closed<br>[access1:04312] mca: base: close: unloading component rsh<br>Daemon was launched on node1-128-09 - beginning to initialize<br>Daemon was launched on node1-128-15 - beginning to initialize<br>Daemon [[39207,0],1] checking in as pid 26240 on host node1-128-09<br>[node1-128-09:26240] [[39207,0],1] orted: up and running - waiting for commands!<br>Daemon [[39207,0],2] checking in as pid 30129 on host node1-128-15<br>[node1-128-15:30129] [[39207,0],2] orted: up and running - waiting for commands!<br>srun: error: node1-128-09: task 0: Exited with exit code 1<br>srun: Terminating job step 645686.3<br>srun: error: node1-128-15: task 1: Exited with exit code 1<br>--------------------------------------------------------------------------<br>An ORTE daemon has unexpectedly failed after launch and before<br>communicating back to mpirun. This could be caused by a number<br>of factors, including an inability to create a connection back<br>to mpirun due to a lack of common network interfaces and/or no<br>route found between them. Please check network connectivity<br>(including firewalls and network routing requirements).<br>--------------------------------------------------------------------------<br>[access1:04312] [[39207,0],0] orted_cmd: received halt_vm cmd<br>[access1:04312] mca: base: close: component slurm closed<br>[access1:04312] mca: base: close: unloading component slurm</p><p><br><br></p></div></div></blockquote></div></div></blockquote><br></div><br><hr><br><br>_______________________________________________<br>users mailing list<br><a target="_blank">users@open-mpi.org</a><br>Subscription:<span>&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post:<span>&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/07/24828.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/07/24828.php</a></div></blockquote></div><br></blockquote><br><br></div></blockquote></div><br></div></div><base target="_self" href="https://e.mail.ru/"></div></div></blockquote><br><br></div></blockquote></div><br></div></body></html>
