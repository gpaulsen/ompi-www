Dear all,<div><br></div><div>I am having a problem while running mpirun in OpenMPI 1.5 version. I compiled OpenMPI 1.5 with BLCR 0.8.2 and OFED 1.4.1 as follows:</div><div><br></div><div><div>./configure \</div><div>--with-ft=cr \</div>
<div>--enable-mpi-threads \</div><div>--with-blcr=/home/nguyen/opt/blcr \</div><div>--with-blcr-libdir=/home/nguyen/opt/blcr/lib \</div><div>--prefix=/home/nguyen/opt/openmpi-1.5 \</div><div>--with-openib \</div><div>--enable-mpirun-prefix-by-default</div>
</div><div><br></div><div>For programs under &quot;openmpi-1.5/examples&quot; folder, mpirun tests were successful. But mpirun aborted immediately when running a program in MPI CUDA code, which was tested successfully with OpenMPI 1.4.3. Below is the error message.</div>
<div><br></div><div>Can anyone give me an idea about this error?</div><div>Thank you.</div><div><br></div><div>Best Regards,</div><div>Toan</div><div>----------------------</div><div><br></div><div><br></div><div><div>[rc002.local:17727] [[56831,1],1] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file util/nidmap.c at line 371</div>
<div>--------------------------------------------------------------------------</div><div>It looks like orte_init failed for some reason; your parallel process is</div><div>likely to abort.  There are many reasons that a parallel process can</div>
<div>fail during orte_init; some of which are due to configuration or</div><div>environment problems.  This failure appears to be an internal failure;</div><div>here&#39;s some additional information (which may only be relevant to an</div>
<div>Open MPI developer):</div><div><br></div><div>  orte_ess_base_build_nidmap failed</div><div>  --&gt; Returned value Data unpack would read past end of buffer (-26) instead of ORTE_SUCCESS</div><div>--------------------------------------------------------------------------</div>
<div>[rc002.local:17727] [[56831,1],1] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file base/ess_base_nidmap.c at line 62</div><div>[rc002.local:17727] [[56831,1],1] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file ess_env_module.c at line 173</div>
<div>--------------------------------------------------------------------------</div><div>It looks like orte_init failed for some reason; your parallel process is</div><div>likely to abort.  There are many reasons that a parallel process can</div>
<div>fail during orte_init; some of which are due to configuration or</div><div>environment problems.  This failure appears to be an internal failure;</div><div>here&#39;s some additional information (which may only be relevant to an</div>
<div>Open MPI developer):</div><div><br></div><div>  orte_ess_set_name failed</div><div>  --&gt; Returned value Data unpack would read past end of buffer (-26) instead of ORTE_SUCCESS</div><div>--------------------------------------------------------------------------</div>
<div>[rc002.local:17727] [[56831,1],1] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file runtime/orte_init.c at line 132</div><div>--------------------------------------------------------------------------</div>
<div>It looks like MPI_INIT failed for some reason; your parallel process is</div><div>likely to abort.  There are many reasons that a parallel process can</div><div>fail during MPI_INIT; some of which are due to configuration or environment</div>
<div>problems.  This failure appears to be an internal failure; here&#39;s some</div><div>additional information (which may only be relevant to an Open MPI</div><div>developer):</div><div><br></div><div>  ompi_mpi_init: orte_init failed</div>
<div>  --&gt; Returned &quot;Data unpack would read past end of buffer&quot; (-26) instead of &quot;Success&quot; (0)</div><div>--------------------------------------------------------------------------</div><div>*** An error occurred in MPI_Init</div>
<div>*** before MPI was initialized</div><div>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)</div><div>[rc002.local:17727] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!</div>
<div>--------------------------------------------------------------------------</div><div>mpirun has exited due to process rank 1 with PID 17727 on</div><div>node rc002 exiting improperly. There are two reasons this could occur:</div>
<div><br></div><div>1. this process did not call &quot;init&quot; before exiting, but others in</div><div>the job did. This can cause a job to hang indefinitely while it waits</div><div>for all processes to call &quot;init&quot;. By rule, if one process calls &quot;init&quot;,</div>
<div>then ALL processes must call &quot;init&quot; prior to termination.</div><div><br></div><div>2. this process called &quot;init&quot;, but exited without calling &quot;finalize&quot;.</div><div>By rule, all processes that call &quot;init&quot; MUST call &quot;finalize&quot; prior to</div>
<div>exiting or it will be considered an &quot;abnormal termination&quot;</div><div><br></div><div>This may have caused other processes in the application to be</div><div>terminated by signals sent by mpirun (as reported here).</div>
<div>--------------------------------------------------------------------------</div></div><div><br></div>

