Hello Ralph,<br>

<br>

Thanks for your reply.<br>

<br>
In order to start my job, I tried the following two ways<br>
(1) configured/compiled open-mpi and compiled benchmark on head node. <br>
      submitted a pbs job.<br>
(2) submitted an interactive job to redo config/compile on compute node.<br>
      And then used &quot;/path/to/mpicc -o hello hello_world.c&quot; to compile the benchmark.<br>
      used &quot;/path/tp/mpirun -np 2 /path/to/hello&quot; to run the job. <br>
Actually I also tried to run &quot;/path/tp/mpirun -np 2 hostname&quot; but got the same error. <br>
<br>
The configure line is pretty long. <br><br> 67 $SRCDIR/configure \<br> 68    --prefix=$PREFIX \<br> 69    --enable-static --disable-shared --disable-dlopen --disable-pretty-print-stacktrace --disable-pty-support --disable-io-romio --enable-contrib-no-build=libnbc,vt --enable-debug \<br>

 70    --with-memory-manager=none --with-threads \<br> 71    --without-tm \<br> 72    --with-wrapper-ldflags=&quot;${ADD_WRAPPER_LDFLAGS}&quot; \<br> 73    --with-wrapper-libs=&quot;-lnsl -lpthread -lm&quot; \<br> 74    --with-platform=optimized \<br>

 75    --with-ugni=/opt/cray/ugni/2.3-1.0400.3912.4.29.gem \<br> 76    --with-ugni-libdir=/opt/cray/ugni/2.3-1.0400.3912.4.29.gem/lib64  \<br> 77    --with-ugni-includedir=/opt/cray/gni-headers/2.1-1.0400.3906.5.1.gem/include \<br>

 78    --with-xpmem=/opt/cray/xpmem/0.1-2.0400.29883.4.6.gem \<br> 79    --with-xpmem-libdir=/opt/cray/xpmem/0.1-2.0400.29883.4.6.gem/lib64 \<br> 80    --enable-mem-debug --enable-mem-profile --enable-debug-symbols --enable-binaries \<br>

 81    --enable-picky --enable-mpi-f77 --enable-mpi-f90 --enable-mpi-cxx --enable-mpi-cxx-seek \<br> 82    --without-slurm --with-memory-manager=ptmalloc2 \<br> 83    --with-pmi=/opt/cray/pmi/2.1.4-1.0000.8596.8.9.gem  --with-cray-pmi-ext \<br>

 84    --enable-mca-no-build=maffinity-first_use,maffinity-libnuma,ess-cnos,filem-rsh,grpcomm-cnos,pml-dr \<br> 85    ${ADD_COMPILER} \<br> 86    CPPFLAGS=&quot;${ADD_CPPFLAGS} -I${gniheaders}&quot; \<br> 87    FFLAGS=&quot;${ADD_FFLAGS} -I${gniheaders}&quot; \<br>

 88    FCFLAGS=&quot;${ADD_FCFLAGS} -I/usr/include -I${gniheaders}&quot; \<br> 89    CFLAGS=&quot;-I/usr/include -I${gniheaders}&quot; \<br> 90    LDFLAGS=&quot;--static ${ADD_LDFLAGS} ${UGNILIBS} ${XPMEMLIBS}&quot; \<br>

 91    LIBS=&quot;${ADD_LIBS} -lpthread -lrt -lpthread -lm&quot; | tee build.log<br><br>Any idea?<br><br><br clear="all">Bin WANG<br><br>
<br><br><div class="gmail_quote">On Mon, Mar 5, 2012 at 7:13 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc.openmpi@gmail.com">rhc.openmpi@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">

How did you attempt to start your job, and what does your configure line look like?<br>
<br>
Sent from my iPad<br>
<div><div class="h5"><br>
On Mar 5, 2012, at 2:11 PM, bin Wang &lt;<a href="mailto:bighead521@gmail.com">bighead521@gmail.com</a>&gt; wrote:<br>
<br>
&gt; Hello All,<br>
&gt;<br>
&gt; I&#39;m trying to run the latest OpenMPI code on Jaguar.<br>
&gt; (Cloned from the Open MPI Mercurial mirror of the Subversion repository)<br>
&gt; The configuration and compilation of OpenMPI were fine, and benchmark<br>
&gt; was also successfully compiled. I tried to launch my program using mpirun<br>
&gt; within an interactive job, but it failed immediately.<br>
&gt;<br>
&gt; Core dump file gave me the following information.<br>
&gt; ====================Error Msg=========================<br>
&gt; [jaguarpf-login2:15370] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local<br>
&gt; node in file ess_singleton_module.c at line 220<br>
&gt; --------------------------------------------------------------------------<br>
&gt; It looks like orte_init failed for some reason; your parallel process is<br>
&gt; likely to abort.  There are many reasons that a parallel process can<br>
&gt; fail during orte_init; some of which are due to configuration or<br>
&gt; environment problems.  This failure appears to be an internal failure;<br>
&gt; here&#39;s some additional information (which may only be relevant to an<br>
&gt; Open MPI developer):<br>
&gt; ompi_mpi_init: orte_init failed<br>
&gt; --&gt; Returned value Unable to start a daemon on the local node (-127) instead of ORTE_SUCCESS<br>
&gt;<br>
&gt; --------------------------------------------------------------------------<br>
&gt; It looks like MPI_INIT failed for some reason; your parallel process is<br>
&gt; likely to abort.  There are many reasons that a parallel process can<br>
&gt; fail during MPI_INIT; some of which are due to configuration33r environment<br>
&gt; problems.  This failure appears to be an internal failure; here&#39;s some<br>
&gt; additional information (which may only be relevant to an Open MPI<br>
&gt; developer):<br>
&gt; ompi_mpi_init: orte_init failed<br>
&gt; --&gt; Returned &quot;Unable to start a daemon on40he local node&quot; (-127) instead of &quot;Success&quot; (0)<br>
&gt; --------------------------------------------------------------------------<br>
&gt; [jaguarpf-login2:15370] *** An error occurred in MPI_Init<br>
&gt; [jaguarpf-login2:15370] *** reported by process [4294967295,42949No process In: Line: ??   PC: ??<br>
&gt; [jaguarpf-login2:15370] *** on a NULL communicator<br>
&gt; [jaguarpf-login2:15370] *** Unknown error<br>
&gt; [jaguarpf-login2:15370] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>
&gt; [jaguarpf-login2:15370] *** and potentially your MPI job)<br>
&gt; --------------------------------------------------------------------------<br>
&gt; An MPI process is aborting at a time when it cannot guarantee that all<br>
&gt; of its peer processes in the job will be killed properly.  You should<br>
&gt; double check that everything has shut down cleanly.<br>
&gt; Reason:     Before MPI_INIT completed<br>
&gt; Local host: jaguarpf-login2<br>
&gt; PID:        15370<br>
&gt; --------------------------------------------------------------------------<br>
&gt; Program exited with code 01.<br>
&gt; ====================Error Msg Over=====================<br>
&gt;<br>
&gt; There are several components under ess, but I don&#39;t know why and how the<br>
&gt; singleton component was chosen.<br>
&gt;<br>
&gt; I hope someone could help me to compile and run openmpi successfully on Jaguar.<br>
&gt;<br>
&gt; Any comment and suggestion will be appreciated.<br>
&gt;<br>
&gt; Thanks,<br>
&gt;<br>
&gt; --Bin<br>
&gt;<br>
</div></div>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br>

