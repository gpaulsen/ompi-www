<div dir="ltr">Hi Steve.<div>Regarding Step 3, have you thought of using some shared storage?</div><div>NFS shared drive perhaps, or there are many alternatives!</div></div><div class="gmail_extra"><br><div class="gmail_quote">On 23 January 2016 at 20:47, Steve O&#39;Hara <span dir="ltr">&lt;<a href="mailto:sohara@pivotal-solutions.co.uk" target="_blank">sohara@pivotal-solutions.co.uk</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">





<div lang="EN-GB" link="#0563C1" vlink="#954F72">
<div>
<p class="MsoNormal">Hi,<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">I’m afraid I’m pretty new to both OpenFOAM and openMPI so please excuse me if my questions are either stupid or badly framed.<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">I’ve created a 10 Raspberry pi beowulf cluster for testing out MPI concepts and see how they are harnessed in OpenFOAM.  After a helluva lot of hassle, I’ve got the thing running using OpneMPI to run a solver in parallel.<u></u><u></u></p>
<p class="MsoNormal">The problem I have is that if I switch the server node to not use the cluster (still use 3 cores in an MPI job) the job finishes in x minutes. If I tell it to use the 9 other members of the cluster, the same job takes x times 3!<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">This is what I’m doing;<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p><u></u><span>1.<span style="font:7.0pt &quot;Times New Roman&quot;">      
</span></span><u></u>Create a mesh, adjust it with some other OF stuff<u></u><u></u></p>
<p><u></u><span>2.<span style="font:7.0pt &quot;Times New Roman&quot;">      
</span></span><u></u>Run the process to split the job into processes for each node<u></u><u></u></p>
<p><u></u><span>3.<span style="font:7.0pt &quot;Times New Roman&quot;">      
</span></span><u></u>Copy the process directories to each of the affected nodes using scp<u></u><u></u></p>
<p><u></u><span>4.<span style="font:7.0pt &quot;Times New Roman&quot;">      
</span></span><u></u>Run mpirun with a hosts file<u></u><u></u></p>
<p><u></u><span>5.<span style="font:7.0pt &quot;Times New Roman&quot;">      
</span></span><u></u>Re-constitute the case directory by copying back the processor folders<u></u><u></u></p>
<p><u></u><span>6.<span style="font:7.0pt &quot;Times New Roman&quot;">      
</span></span><u></u>Re-construct the case<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">Only step 4 Uses MPI and the other steps have a reasonably linear response time.<u></u><u></u></p>
<p class="MsoNormal">Step 4 is characterised by a flurry of network activity, followed by all the Pis lighting up with CPU activity followed a long time of no CPU activity but huge network action.<u></u><u></u></p>
<p class="MsoNormal">It’s this last bit that is consuming all the time – is this a tear-down phase of MPI?<u></u><u></u></p>
<p class="MsoNormal">Each of the Pi nodes is set up as slots=4 max_slots=4<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">What is all the network activity?  It seems to happen after the solver has completed its job so I’m guessing it has to be MPI.<u></u><u></u></p>
<p class="MsoNormal">The network interface on the Pi is not a stellar performer so is there anything I can do to minimise the network traffic?<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">Thanks,<u></u><u></u></p>
<p class="MsoNormal">Steve<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
</div>
</div>

<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/01/28349.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/01/28349.php</a><br></blockquote></div><br></div>

