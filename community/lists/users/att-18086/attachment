Hi,<div><br></div><div>    We tried to run the Reduce_scatter test(given in IMB_3.2.1) on our customized OFED, the command used and the error thrown is given below...</div><div><br></div><div><b>command:</b></div><div><span dir="ltr" id=":vp">mpirun --prefix /usr/local/ -np 3 --mca btl openib,self,sm -H 192.168.0.157,192.168.0.167,192.168.0.177 --mca orte_base_help_aggregate 0 /root/subash/imb/src/IMB-MPI1 reduce_scatter</span></div>
<div><br></div><div><b>error:</b></div><div><div class="nH"><div style="height:200px" class="nH ko aXjCH"><div class="Z8Dgfe"><div id=":t6" class="kf"><div class="km"><div class="kk"><span dir="ltr" id=":ul">[localhost.localdomain:06040] <b>*</b> An error occurred in MPI_Reduce_scatter<br>
[localhost.localdomain:06040] <b>*</b> on communicator MPI COMMUNICATOR 3 SPLIT FROM 0<br>[localhost.localdomain:06040] <b>*</b> MPI_ERR_TRUNCATE: message truncated<br>[localhost.localdomain:06040] <b>*</b> MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>
--------------------------------------------------------------------------<br>mpirun has exited due to process rank 1 with PID 6040 on<br>node 192.168.0.167 exiting without calling &quot;finalize&quot;. This may<br>have caused other processes in the application to be<br>
terminated by signals sent by mpirun (as reported here).<br>--------------------------------------------------------------------------</span></div></div></div><div id=":sp" class="kd"> </div><div id=":sp" class="kd">We didn&#39;t get much of the information regarding this error... can you please clarify the reason for that error..??</div>
</div></div></div></div><div>We were able to run all the remaining tests that were there in the intel IMB bench mark tool, and the basic ping-pong tests and q-perf  cases also...</div><div><br></div><div>Thanks for your reply..<br clear="all">
<div><br></div>-- <br><span style="color:rgb(51,51,255)">Thanks &amp; Regards,<br>D.Venkateswara Rao,</span><br style="color:rgb(51,51,255)"><span style="color:rgb(51,51,255)"></span><br>
</div>

