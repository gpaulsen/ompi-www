<html><body>
<p>The need for a &quot;better&quot; timeout depends on what else there is for the CPU to do. <br>
<br>
If you get creative and shift from {99% MPI_WAIT , 1% OS_idle_process} to {1% MPI_Wait, 99% OS_idle_process} at a cost of only a few extra microseconds added lag on MPI_Wait, you may be pleased by the CPU load statistic but still have have only hurt yourself. Perhaps you have not hurt yourself much but for what? The CPU does not get tired of spinning in MPI_Wait rather than in the OS_idle_process<br>
<br>
Most MPI applications run with an essentially dedicated CPU per process. In most MPI applications if even one task is sharing its CPU with other processes, like users doing compiles, the whole job slows down too much.<br>
<br>
There are exceptions. For example, a  work farm, in which a master doles out a chunk of work, takes back the result as a worker produces one and then doles out another chunk can get valuable work from CPUs that have other useful work to do too and in that situation it can be a big win to accept lag time in the MPI_Wait in return for making the CPU available to another process. The symptom that  you need a &quot;better&quot; MPI_Wait then will probably be more like {50% MPI_WAIT , 50% other process}<br>
<br>
Dick Treumann  -  MPI Team           <br>
IBM Systems &amp; Technology Group<br>
Dept X2ZA / MS P963 -- 2455 South Road -- Poughkeepsie, NY 12601<br>
Tele (845) 433-7846         Fax (845) 433-8363<br>
<br>
<br>
<tt>users-bounces@open-mpi.org wrote on 12/06/2009 10:42:26 AM:<br>
<br>
&gt; [image removed] </tt><br>
<tt>&gt; <br>
&gt; Re: [OMPI users] Mimicking timeout for MPI_Wait</tt><br>
<tt>&gt; <br>
&gt; Katz, Jacob </tt><br>
<tt>&gt; <br>
&gt; to:</tt><br>
<tt>&gt; <br>
&gt; Open MPI Users</tt><br>
<tt>&gt; <br>
&gt; 12/06/2009 10:47 AM</tt><br>
<tt>&gt; <br>
&gt; Sent by:</tt><br>
<tt>&gt; <br>
&gt; users-bounces@open-mpi.org</tt><br>
<tt>&gt; <br>
&gt; Please respond to Open MPI Users</tt><br>
<tt>&gt; <br>
&gt; Thanks, Douglas. <br>
&gt; I found your code in the archive.<br>
&gt; --------------------------------<br>
&gt; Jacob M. Katz | jacob.katz@intel.com | Work: +972-4-865-5726 | iNet:<br>
&gt; (8)-465-5726<br>
&gt; <br>
&gt; <br>
&gt; -----Original Message-----<br>
&gt; From: users-bounces@open-mpi.org [<a href="mailto:users-bounces@open-mpi.org">mailto:users-bounces@open-mpi.org</a>]<br>
&gt; On Behalf Of Douglas Guptill<br>
&gt; Sent: Sunday, December 06, 2009 15:53<br>
&gt; To: users@open-mpi.org<br>
&gt; Subject: Re: [OMPI users] Mimicking timeout for MPI_Wait<br>
&gt; <br>
&gt; On Sun, Dec 06, 2009 at 02:29:01PM +0200, Katz, Jacob wrote:<br>
&gt; <br>
&gt; &gt; Thanks.<br>
&gt; &gt; Yes, I meant in the question that I was looking for something <br>
&gt; creative, both fast responding and not using 100% CPU all the time.<br>
&gt; &gt; I guess I’m not the first one to face this question. Have anyone <br>
&gt; done anything “better” than the simple solution?<br>
&gt; <br>
&gt; My MPI application is a two-process thing, in which data is thrown<br>
&gt; back and forth. &nbsp;For the most part, one process is calculating, and<br>
&gt; the other is waiting.<br>
&gt; <br>
&gt; I got tired of seeing both cpus at 100% load, and based on suggestions<br>
&gt; from Jeff Squyres and Eugene Loh, wrote MPI_Recv.c and MPI_Send.c. &nbsp;I<br>
&gt; load these with my application, and bingo! &nbsp;Only one cpu busy at any<br>
&gt; given time.<br>
&gt; <br>
&gt; They use a graduated sleep; the first sleep is short, the second is<br>
&gt; twice as long, and so on, up to a maximum sleep time.<br>
&gt; <br>
&gt; I sent the code along with my last message on the subject (December<br>
&gt; 2008, or later) so it should be in the archives. &nbsp;Failing that, I<br>
&gt; could post it again, if anyone wants it.<br>
&gt; <br>
&gt; Douglas.<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; users@open-mpi.org<br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; ---------------------------------------------------------------------<br>
&gt; Intel Israel (74) Limited<br>
&gt; <br>
&gt; This e-mail and any attachments may contain confidential material for<br>
&gt; the sole use of the intended recipient(s). Any review or distribution<br>
&gt; by others is strictly prohibited. If you are not the intended<br>
&gt; recipient, please contact the sender and delete all copies.<br>
&gt; <br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; users@open-mpi.org<br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></tt></body></html>