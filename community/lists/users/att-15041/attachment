
<br><font size=2 face="sans-serif">Also -</font>
<br>
<br><font size=2 face="sans-serif">HPC clusters are commonly dedicated
to running parallel jobs with exactly one process per CPU. &nbsp;HPC is
about getting computation done and letting a CPU time slice among competing
processes always has overhead (CPU time not spent on the computation).</font>
<br>
<br><font size=2 face="sans-serif">Unless you are trying to run extra processes
that take turns for the available processors, there is no gain from freeing
up a CPU during a blocking call. &nbsp;It is the difference between spinning
in the poll and spinning in the OS &quot;idle process&quot;.</font>
<br>
<br><font size=2 face="sans-serif">Dick Treumann &nbsp;- &nbsp;MPI Team
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br>
IBM Systems &amp; Technology Group<br>
Dept X2ZA / MS P963 -- 2455 South Road -- Poughkeepsie, NY 12601<br>
Tele (845) 433-7846 &nbsp; &nbsp; &nbsp; &nbsp; Fax (845) 433-8363<br>
</font>
<br>
<br>
<br>
<table width=100%>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">From:</font>
<td><font size=1 face="sans-serif">Ralph Castain &lt;rhc@open-mpi.org&gt;</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">To:</font>
<td><font size=1 face="sans-serif">Open MPI Users &lt;users@open-mpi.org&gt;</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Date:</font>
<td><font size=1 face="sans-serif">12/08/2010 10:36 AM</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Subject:</font>
<td><font size=1 face="sans-serif">Re: [OMPI users] curious behavior during
wait for broadcast: 100% &nbsp; &nbsp; &nbsp; &nbsp;cpu</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Sent by:</font>
<td><font size=1 face="sans-serif">users-bounces@open-mpi.org</font></table>
<br>
<hr noshade>
<br>
<br>
<br><tt><font size=2>I know we have said this many times - OMPI made a
design decision to poll hard while waiting for messages to arrive to minimize
latency.<br>
<br>
If you want to decrease cpu usage, you can use the yield_when_idle option
(it will cost you some latency, though) - see ompi_info --param ompi all<br>
<br>
Or don't set affinity and we won't be as aggressive - but you'll lose some
performance<br>
<br>
Choice is yours! :-)<br>
<br>
On Dec 8, 2010, at 8:08 AM, Hicham Mouline wrote:<br>
<br>
&gt; Hello,<br>
&gt; <br>
&gt; on win32 openmpi 1.4.3, I have a slave process that reaches this pseudo-code
and then blocks and the CPU usage for that process stays at 25% all the
time (I have a quadcore processor). When I set the affinity to 1 of the
cores, that core is 100% busy because of my slave process.<br>
&gt; <br>
&gt; main()<br>
&gt; {<br>
&gt; ....<br>
&gt; .....<br>
&gt; MPI_ISEND<br>
&gt; <br>
&gt; std::cout&lt;&lt; &quot;about to get broadcast&quot;&lt;&lt;std::endl;<br>
&gt; MPI_Bcast of an integer<br>
&gt; std::cout&lt;&lt; &quot; broadcast received&quot;&lt;&lt;std::endl;<br>
&gt; ...<br>
&gt; }<br>
&gt; <br>
&gt; The first printout is seen but not the next which makes me thing the
process is inside the MPI_Bcast call. Should the CPU be 100% busy while
this call is waiting for the broadcast message to arrive?<br>
&gt; <br>
&gt; Any ideas? below the output of ompi-info:<br>
&gt; ------------------------------------------------------------------------------------------------------------------------------------------------------------------<br>
&gt; <br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Package: Open
MPI <br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp;Distribution<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Open MPI: 1.4.3<br>
&gt; &nbsp; Open MPI SVN revision: r23834<br>
&gt; &nbsp; Open MPI release date: Oct 05, 2010<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Open RTE: 1.4.3<br>
&gt; &nbsp; Open RTE SVN revision: r23834<br>
&gt; &nbsp; Open RTE release date: Oct 05, 2010<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;OPAL:
1.4.3<br>
&gt; &nbsp; &nbsp; &nbsp; OPAL SVN revision: r23834<br>
&gt; &nbsp; &nbsp; &nbsp; OPAL release date: Oct 05, 2010<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Ident string: 1.4.3<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Prefix:
C:/Program Files/openmpi<br>
&gt; Configured architecture: x86 Windows-5.1<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Configure host: LC12-003-D-055A<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Configured by: hicham.mouline<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Configured on: 18:07 19/11/2010<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Configure host: <br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Built by: hicham.mouline<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Built on: 18:07
19/11/2010<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Built host: <br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;C bindings: yes<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;C++ bindings: yes<br>
&gt; &nbsp; &nbsp; &nbsp;Fortran77 bindings: no<br>
&gt; &nbsp; &nbsp; &nbsp;Fortran90 bindings: no<br>
&gt; Fortran90 bindings size: na<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;C compiler: C:/Program
Files/Microsoft Visual Studio<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp;9.0/VC/bin/cl.exe<br>
&gt; &nbsp; &nbsp; C compiler absolute: C:/Program Files/Microsoft Visual
Studio<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp;9.0/VC/bin/cl.exe<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;C++ compiler: C:/Program
Files/Microsoft Visual Studio<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp;9.0/VC/bin/cl.exe<br>
&gt; &nbsp; C++ compiler absolute: C:/Program Files/Microsoft Visual Studio<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp;9.0/VC/bin/cl.exe<br>
&gt; &nbsp; &nbsp; &nbsp;Fortran77 compiler: CMAKE_Fortran_COMPILER-NOTFOUND<br>
&gt; &nbsp;Fortran77 compiler abs: none<br>
&gt; &nbsp; &nbsp; &nbsp;Fortran90 compiler:<br>
&gt; &nbsp;Fortran90 compiler abs: none<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; C profiling: yes<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; C++ profiling: yes<br>
&gt; &nbsp; &nbsp; Fortran77 profiling: no<br>
&gt; &nbsp; &nbsp; Fortran90 profiling: no<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;C++ exceptions: no<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Thread support: no<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Sparse Groups: no<br>
&gt; &nbsp;Internal debug support: no<br>
&gt; &nbsp; &nbsp; MPI parameter check: runtime<br>
&gt; Memory profiling support: no<br>
&gt; Memory debugging support: no<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; libltdl support: no<br>
&gt; &nbsp; Heterogeneous support: no<br>
&gt; mpirun default --prefix: yes<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; MPI I/O support: yes<br>
&gt; &nbsp; &nbsp; &nbsp; MPI_WTIME support: gettimeofday<br>
&gt; Symbol visibility support: yes<br>
&gt; &nbsp; FT Checkpoint support: yes &nbsp;(checkpoint thread: no)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA backtrace: none (MCA v2.0,
API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA paffinity: windows (MCA v2.0,
API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA carto: auto_detect
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA maffinity: first_use (MCA v2.0,
API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA timer: windows
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; MCA installdirs: windows (MCA v2.0, API
v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; MCA installdirs: env (MCA v2.0, API v2.0,
Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; MCA installdirs: config (MCA v2.0, API
v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA crs: none
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA dpm: orte
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA pubsub: orte (MCA
v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA allocator: basic (MCA v2.0,
API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA allocator: bucket (MCA v2.0,
API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA coll: basic
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA coll: hierarch
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA coll: self
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA coll: sm
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA coll: sync
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA mpool: rdma (MCA
v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA mpool: sm (MCA
v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA pml: ob1
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA bml: r2
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA btl: self
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA btl: sm
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA btl: tcp
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA topo: unity
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA osc: pt2pt
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA osc: rdma
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA iof: hnp
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA iof: orted
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA iof: tool
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA oob: tcp
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA odls: process
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA rmaps: round_robin
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA rmaps: seq (MCA
v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA rml: ftrm
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA rml: oob
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA routed: binomial
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA routed: linear
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA plm: process
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA errmgr: default
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA ess: env
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA ess: hnp
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA ess: singleton
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA ess: tool
(MCA v2.0, API v2.0, Component v1.4.3)<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA grpcomm: basic (MCA
v2.0, API v2.0, Component v1.4.3)<br>
&gt; <br>
&gt; regards,<br>
&gt; <br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; users@open-mpi.org<br>
&gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
users@open-mpi.org<br>
</font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
</font></tt>
<br>
<br>
