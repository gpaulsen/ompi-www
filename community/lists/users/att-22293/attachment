<div dir="ltr"><div style>Hi Tim,</div><div style><br></div><div style><br></div>Well, in general and not on MIC I usually build the MPI stacks using the Intel compiler set. Have you ran into s/w that requires GCC instead of Intel compilers (beside Nvidia Cuda)? Did you try to use Intel compiler to produce MIC native code (the OpenMPI stack for that matter)? <div>
<br></div><div style>regards</div><div style>Michael</div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Mon, Jul 8, 2013 at 4:30 PM, Tim Carlson <span dir="ltr">&lt;<a href="mailto:tim.carlson@pnl.gov" target="_blank">tim.carlson@pnl.gov</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">On Mon, 8 Jul 2013, Elken, Tom wrote:<br>
<br>
It isn&#39;t quite so easy.<br>
<br>
Out of the box, there is no gcc on the Phi card. You can use the cross compiler on the host, but you don&#39;t get gcc on the Phi by default.<br>
<br>
See this post <a href="http://software.intel.com/en-us/forums/topic/382057" target="_blank">http://software.intel.com/en-<u></u>us/forums/topic/382057</a><br>
<br>
I really think you would need to build and install gcc on the Phi first.<br>
<br>
My first pass at doing a cross-compile with the GNU compilers failed to produce something with OFED support (not surprising)<br>
<br>
export PATH=/usr/linux-k1om-4.7/bin:$<u></u>PATH<br>
./configure --build=x86_64-unknown-linux-<u></u>gnu --host=x86_64-k1om-linux \<br>
--disable-mpi-f77<br>
<br>
checking if MCA component btl:openib can compile... no<span class="HOEnZb"><font color="#888888"><br>
<br>
<br>
Tim</font></span><div class="HOEnZb"><div class="h5"><br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<br>
 <br>
<br>
Thanks Tom, that sounds good. I will give it a try as soon as our Phi host<br>
here host gets installed. <br>
<br>
 <br>
<br>
I assume that all the prerequisite libs and bins on the Phi side are<br>
available when we download the Phi s/w stack from Intel&#39;s site, right ?<br>
<br>
[Tom]<br>
<br>
Right.  When you install Intel’s MPSS (Manycore Platform Software Stack),<br>
including following the section on “OFED Support” in the readme file, you<br>
should have all the prerequisite libs and bins.  Note that I have not built<br>
Open MPI for Xeon Phi for your interconnect, but it seems to me that it<br>
should work.<br>
<br>
 <br>
<br>
-Tom<br>
<br>
 <br>
<br>
Cheers<br>
<br>
Michael<br>
<br>
 <br>
<br>
 <br>
<br>
 <br>
<br>
On Mon, Jul 8, 2013 at 12:10 PM, Elken, Tom &lt;<a href="mailto:tom.elken@intel.com" target="_blank">tom.elken@intel.com</a>&gt; wrote:<br>
<br>
Do you guys have any plan to support Intel Phi in the future? That is,<br>
running MPI code on the Phi cards or across the multicore and Phi, as Intel<br>
MPI does?<br>
<br>
[Tom]<br>
<br>
Hi Michael,<br>
<br>
Because a Xeon Phi card acts a lot like a Linux host with an x86<br>
architecture, you can build your own Open MPI libraries to serve this<br>
purpose.<br>
<br>
Our team has used existing (an older 1.4.3 version of) Open MPI source to<br>
build an Open MPI for running MPI code on Intel Xeon Phi cards over Intel’s<br>
(formerly QLogic’s) True Scale InfiniBand fabric, and it works quite well. <br>
We have not released a pre-built Open MPI as part of any Intel software<br>
release.   But I think if you have a compiler for Xeon Phi (Intel Compiler<br>
or GCC) and an interconnect for it, you should be able to build an Open MPI<br>
that works on Xeon Phi. <br>
<br>
Cheers,<br>
Tom Elken<br>
<br>
thanks...<br>
<br>
Michael<br>
<br>
 <br>
<br>
On Sat, Jul 6, 2013 at 2:36 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt; wrote:<br>
<br>
Rolf will have to answer the question on level of support. The CUDA code is<br>
not in the 1.6 series as it was developed after that series went &quot;stable&quot;.<br>
It is in the 1.7 series, although the level of support will likely be<br>
incrementally increasing as that &quot;feature&quot; series continues to evolve.<br>
<br>
<br>
<br>
On Jul 6, 2013, at 12:06 PM, Michael Thomadakis &lt;<a href="mailto:drmichaelt7777@gmail.com" target="_blank">drmichaelt7777@gmail.com</a>&gt;<br>
wrote:<br>
<br>
&gt; Hello OpenMPI,<br>
&gt;<br>
&gt; I am wondering what level of support is there for CUDA and GPUdirect on<br>
OpenMPI 1.6.5 and 1.7.2.<br>
&gt;<br>
&gt; I saw the ./configure --with-cuda=CUDA_DIR option in the FAQ. However, it<br>
seems that with configure v1.6.5 it was ignored.<br>
&gt;<br>
&gt; Can you identify GPU memory and send messages from it directly without<br>
copying to host memory first?<br>
&gt;<br>
&gt;<br>
&gt; Or in general, what level of CUDA support is there on 1.6.5 and 1.7.2 ? Do<br>
you support SDK 5.0 and above?<br>
&gt;<br>
&gt; Cheers ...<br>
&gt; Michael<br>
<br>
&gt; ______________________________<u></u>_________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
<br>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
<br>
 <br>
<br>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
<br>
 <br>
<br>
<br>
<br>
</blockquote>
</div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>

