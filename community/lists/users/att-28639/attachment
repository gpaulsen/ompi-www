<html><head></head><body><div style="color:#000; background-color:#fff; font-family:HelveticaNeue, Helvetica Neue, Helvetica, Arial, Lucida Grande, sans-serif;font-size:16px"><div id="yui_3_16_0_1_1457184546654_7253"><br></div><div class="qtdSeparateBR"><br><br></div><div class="yahoo_quoted" id="yui_3_16_0_1_1457184546654_7248" style="display: block;"><div style="font-family: HelveticaNeue, Helvetica Neue, Helvetica, Arial, Lucida Grande, sans-serif; font-size: 16px;" id="yui_3_16_0_1_1457184546654_7247"><div style="font-family: HelveticaNeue, Helvetica Neue, Helvetica, Arial, Lucida Grande, sans-serif; font-size: 16px;" id="yui_3_16_0_1_1457184546654_7246"><div class="y_msg_container" id="yui_3_16_0_1_1457184546654_7245">&gt; &lt;quote&gt;<br>&gt; I don't think the Open MPI TCP BTL will pass the SDP socket type when creating sockets -- SDP is much lower performance than native verbs/RDMA.&nbsp; You should use a "native" interface to your RDMA network instead (which one you use depends on which kind of network you have).<br>&gt; &lt;/quote&gt;<br>&gt; <br>&gt; I have a rather naive follow-up question along this line: why is there not a native mode for (garden variety) &gt; Ethernet?<br><br>&gt; There's at least three things that Ethernet-based networks do for acceleration / low latency:<br><br>&gt; 1. Bypass the OS for injecting and receiving network packets<br>&gt; 2. Use a wire protocol other than TCP<br>&gt; 3. Include other offload functionality (e.g., RDMA, or RDMA-like capabilities)<br><br>&gt; Enabling these things typically requires additional support from the NIC's drivers and/or firmware.&nbsp; Hence, &gt; you typically can't just take any old Ethernet NIC and expect that the above three things work.<br><br>&gt; Several Ethernet NIC vendors have enabled these kinds of things in their NICs (e.g., I am on the usNIC&nbsp;</div><div class="y_msg_container" id="yui_3_16_0_1_1457184546654_7245">&gt; team at Cisco, where we enable these things on the Cisco NIC in our UCS server line).<br><br>&gt; There was a project a few years ago called OpenMX that used the generic Ethernet driver in Linux to&nbsp;</div><div class="y_msg_container" id="yui_3_16_0_1_1457184546654_7245">&gt; accomplish #2 for just about any Ethernet NIC, but it never really caught on, and has since bit-rotted.<br><br>&gt; Is it because it lacks the end-to-end guarantees of TCP, Infiniband and the like? These days, switched&nbsp;</div><div class="y_msg_container" id="yui_3_16_0_1_1457184546654_7245">&gt; Ethernet is very reliable, isn't it? (I mean by the rate of packet drop because of congestion). So if the&nbsp;</div><div class="y_msg_container" id="yui_3_16_0_1_1457184546654_7245">&gt; application only needs data chunks of around 8KB max, which would not need to be fragmented (using&nbsp;</div><div class="y_msg_container" id="yui_3_16_0_1_1457184546654_7245">&gt; jumbo frames), won't a native ethernet be much more efficient?<br><br>&gt; The Cisco usNIC stack was initially OS-bypass injection of simple L2 Ethernet frames.&nbsp; It did all of its own &gt; retransmission and whatnot in Open MPI itself (*all* network types have drops and/or frame corruption,&nbsp;</div><div class="y_msg_container" id="yui_3_16_0_1_1457184546654_7245">&gt; due to congestion and lots of other every day kinds of traffic management -- *some* layer in the network&nbsp;</div><div class="y_msg_container" id="yui_3_16_0_1_1457184546654_7245">&gt; has to handle such drops/retransmits if you want them to look like they are reliable to a higher level in the &gt; stack).&nbsp; <br><br>&gt; We eventually "upgraded" usNIC to the UDP wire protocol because our customers told us that they want&nbsp;</div><div class="y_msg_container" id="yui_3_16_0_1_1457184546654_7245">&gt; to switch usNIC traffic around L3 networks in their datacenter.&nbsp; We typically use jumbo frames to get good &gt; bandwidth.&nbsp; The addition of a few bytes per packet (i.e., the size comparison of a raw L2 ethernet frame&nbsp;</div><div class="y_msg_container" id="yui_3_16_0_1_1457184546654_7245">&gt; vs. a UDP packet) is typically not enough to affect the bandwidth curve for large packets -- especially&nbsp;</div><div class="y_msg_container" id="yui_3_16_0_1_1457184546654_7245">&gt; when using jumbo frames.&nbsp; Additionally, Cisco gear switches L2 and L3 packets at exactly the same &gt;speed, so we don't lose any native fabric performance by upgrading from L2 frames to UDP packets.</div><div class="y_msg_container" id="yui_3_16_0_1_1457184546654_7245"><br></div><div class="y_msg_container" id="yui_3_16_0_1_1457184546654_7245" dir="ltr">I am fairly incompetent with anything other than the TCP/IP stack used in most OS, so my first instinct is to use as TCP as it is all I am familiar with. I am also working in the embedded world where a cluster/domain may only involve one type of interconnect, where Ethernet is used for one situation and PCIe for something else. Essentially native Ethernet may not be an option all the time.<br><br>-- <br>Jeff Squyres<br><a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank" id="yui_3_16_0_1_1457184546654_7557">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br><br>_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" id="yui_3_16_0_1_1457184546654_9360">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/03/28616.php" target="_blank" id="yui_3_16_0_1_1457184546654_9361">http://www.open-mpi.org/community/lists/users/2016/03/28616.php</a><br><br><br></div>  </div> </div>  </div></div></body></html>
