
<br><font size=2 face="sans-serif">I did not take the time to try to fully
understand your approach so this may sound like a dumb question; &nbsp;</font>
<br>
<br><font size=2 face="sans-serif">Do you have an MPI_Bcast ROOT process
in every MPI_COMM_WORLD and does every non-ROOT MPI_Bcast call correctly
identify the rank of ROOT in its MPI_COMM_WORLD ? &nbsp;</font>
<br>
<br><font size=2 face="sans-serif">An MPI_Bcast call when there is not
root task in the communicator or when the root task rank is given incorrectly
will hang.</font>
<br>
<br>
<br><font size=2 face="sans-serif">Dick Treumann &nbsp;- &nbsp;MPI Team
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br>
IBM Systems &amp; Technology Group<br>
Dept X2ZA / MS P963 -- 2455 South Road -- Poughkeepsie, NY 12601<br>
Tele (845) 433-7846 &nbsp; &nbsp; &nbsp; &nbsp; Fax (845) 433-8363<br>
</font>
<br>
<br>
<br>
<table width=100%>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">From:</font>
<td><font size=1 face="sans-serif">Randolph Pullen &lt;randolph_pullen@yahoo.com.au&gt;</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">To:</font>
<td><font size=1 face="sans-serif">users@open-mpi.org</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Date:</font>
<td><font size=1 face="sans-serif">08/07/2010 01:23 AM</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Subject:</font>
<td><font size=1 face="sans-serif">[OMPI users] MPI_Bcast issue</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Sent by:</font>
<td><font size=1 face="sans-serif">users-bounces@open-mpi.org</font></table>
<br>
<hr noshade>
<br>
<br>
<br>
<table>
<tr valign=top>
<td><font size=3>I seem to be having a problem with MPI_Bcast.<br>
My massive I/O intensive data movement program must broadcast from n to
n nodes. My problem starts because I require 2 processes per node, a sender
and a receiver and I have implemented these using MPI processes rather
than tackle the complexities of threads on MPI.<br>
<br>
Consequently, broadcast and calls like alltoall are not completely helpful.
&nbsp;The dataset is huge and each node must end up with a complete copy
built by the large number of contributing broadcasts from the sending nodes.
&nbsp;Network efficiency and run time are paramount.<br>
<br>
As I don’t want to needlessly broadcast all this data to the sending nodes
and I have a perfectly good MPI program that distributes globally from
a single node (1 to N), I took the unusual decision to start N copies of
this program by spawning the MPI system from the PVM system in an effort
to get my N to N concurrent transfers.<br>
<br>
It seems that the broadcasts running on concurrent MPI environments collide
and cause all but the first process to hang waiting for their broadcasts.
&nbsp;This theory seems to be confirmed by introducing a sleep of n-1 seconds
before the first MPI_Bcast &nbsp;call on each node, which results in the
code working perfectly. &nbsp;(total run time 55 seconds, 3 nodes, standard
TCP stack)<br>
<br>
My guess is that unlike PVM, OpenMPI implements broadcasts with broadcasts
rather than multicasts. &nbsp;Can someone confirm this? &nbsp;Is this a
bug?<br>
<br>
Is there any multicast or N to N broadcast where sender processes can avoid
participating when they don’t need to?<br>
<br>
Thanks in advance<br>
Randolph<br>
</font></table>
<br><font size=3><br>
 </font><tt><font size=2>_______________________________________________<br>
users mailing list<br>
users@open-mpi.org<br>
</font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a>
<br>
<br>
