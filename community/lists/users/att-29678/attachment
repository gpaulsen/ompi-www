<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">Yes, sadly the terminology is badly overloaded at this stage :-(<div class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Jul 18, 2016, at 9:20 AM, John Hearns &lt;<a href="mailto:hearnsj@googlemail.com" class="">hearnsj@googlemail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><div dir="ltr" class="">Thankyou Ralph. &nbsp; i guess the information I did not have in my head was that &nbsp; core = physical core (not hyperthreaded core)</div><div class="gmail_extra"><br class=""><div class="gmail_quote">On 18 July 2016 at 14:45, Ralph Castain <span dir="ltr" class="">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank" class="">rhc@open-mpi.org</a>&gt;</span> wrote:<br class=""><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word" class="">It sounds like you just want to bind procs to cores since each core is composed of 2 HTs. So a simple “--map-by core --bind-to core" should do the trick.<div class=""><br class=""></div><div class="">FWIW: the affinity settings are controlled by the bind-to &lt;foo&gt; option. You can use “mpirun -h” &nbsp;to get the list of supported options and a little explanation:</div><div class=""><br class=""></div><div class=""><dt class=""><b class="">--bind-to
&lt;foo&gt;</b> </dt>
<dd class="">Bind processes to the specified object, defaults to <i class="">core</i>. Supported
options include slot, hwthread, core, l1cache, l2cache, l3cache, socket,
numa, board, and none.  </dd><div class=""><br class=""></div></div><div class=""><a href="https://www.open-mpi.org/doc/current/man1/mpirun.1.php#sect9" target="_blank" class="">https://www.open-mpi.org/doc/current/man1/mpirun.1.php#sect9</a></div><div class=""><br class=""></div><div class=""><br class=""></div><div class=""><br class=""><div class=""><br class=""><div class=""><blockquote type="cite" class=""><div class=""><div class="h5"><div class="">On Jul 17, 2016, at 11:25 PM, John Hearns &lt;<a href="mailto:hearnsj@googlemail.com" target="_blank" class="">hearnsj@googlemail.com</a>&gt; wrote:</div><br class=""></div></div><div class=""><div class=""><div class="h5"><div dir="ltr" class="">Please can someone point me towards the affinity settings for:<div class="">OpenMPI 1.10 &nbsp; used with Slurm version 15</div><div class=""><br class=""></div><div class="">I have some nodes with 2630-v4 processors.</div><div class="">So 10 cores per socket / 20 hyperthreads</div><div class="">Hyperthreading is enabled.</div><div class="">I would like to set affinity for 20 processes per node,<br class=""></div><div class="">so that the processes are pinned to every second HT core - ie one process per physical thread.</div><div class=""><br class=""></div><div class="">I'm sure this is quite easy...</div><div class=""><br class=""></div><div class="">Thankyou</div></div></div></div>
_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" class="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/07/29674.php" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2016/07/29674.php</a></div></blockquote></div><br class=""></div></div></div><br class="">_______________________________________________<br class="">
users mailing list<br class="">
<a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank" class="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/07/29676.php" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2016/07/29676.php</a><br class=""></blockquote></div><br class=""></div>
_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: https://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2016/07/29677.php</div></blockquote></div><br class=""></div></body></html>
