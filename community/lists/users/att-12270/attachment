<br>Hello,<br><br>I changed the test code (hetero.c, in attach) so that the master (where data is centralized) can be rank 1 or 2. <br>I tested with a master,rank 2 or rank 1 : same probleme, when the master is a 64bit machine, as soon as it receive data from a 32bit machines it got segfault. no probleme with a 32bit master. It seems to not be rank dependent ... <br>

<br>Regards,<br><br><br><div class="gmail_quote">On Mon, Mar 8, 2010 at 1:27 PM, Terry Dontje <span dir="ltr">&lt;<a href="mailto:terry.dontje@oracle.com">terry.dontje@oracle.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">

We (Oracle) have not done that much extensive limits testing going between 32 to 64bit applications.  Most of the testing we&#39;ve done is more around endianess (SPARC vs x86_64).<br>
<br>
Though the below is kind of interesting.  Sounds like the eager limit isn&#39;t being normalized on the 64 bit machines.  Though a 32 bit rank 0 solving the problem also is very interesting, I wonder if that is not more due to which rank is send and receiving?<br>


<br>
--td<br>
<br>
<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<br>
Message: 3<br>
Date: Sun, 7 Mar 2010 05:34:21 -0600<br>
From: &quot;Jeff Squyres (jsquyres)&quot; &lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;<br>
Subject: Re: [OMPI users] Segmentation fault when Send/Recv<br>
        onheterogeneouscluster (32/64 bit machines)<br>
To: &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
Message-ID:<br>
        &lt;<a href="mailto:58D723FE08DC6A4398E6596E38F3FA1705670F@XMB-RCD-205.cisco.com" target="_blank">58D723FE08DC6A4398E6596E38F3FA1705670F@XMB-RCD-205.cisco.com</a>&gt;<br>
Content-Type: text/plain; charset=&quot;utf-8&quot;<br>
<br>
Ibm and sun (oracle) have probably done the most heterogeneous testing, but its probably not as stable as our homogeneous code paths.<br>
<br>
Terry/brad - do you have any insight here?<br>
<br>
Yes, setting eager limit high can impact performance. Its the amount of data that ompi will send eagerly without waiting for an ack from the receiver. There are several secondary performance effects that can occur if you are using sockets for transport and/or your program is only loosely synchronized. If your prog is tightly synchronous, it may not have too huge of an overall perf impact. <br>


-jms<br>
Sent from my PDA.  No type good.<br>
<br>
----- Original Message -----<br>
From: <a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a> &lt;<a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a>&gt;<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
Sent: Thu Mar 04 09:02:19 2010<br>
Subject: Re: [OMPI users] Segmentation fault when Send/Recv onheterogeneouscluster (32/64 bit machines)<br>
<br>
Hi,<br>
<br>
I have some new discovery about this problem :<br>
<br>
It seems that the array size sendable from a 32bit to 64bit machines<br>
is proportional to the parameter &quot;btl_tcp_eager_limit&quot;<br>
When I set it to 200 000 000 (2e08 bytes, about 190MB), I can send an<br>
array up to 2e07 double (152MB).<br>
<br>
I didn&#39;t found much informations about btl_tcp_eager_limit other than<br>
in the &quot;ompi_info --all&quot; command. If I let it at 2e08, will it impacts<br>
the performance of OpenMPI ?<br>
<br>
It may be noteworth also that if the master (rank 0) is a 32bit<br>
machines, I don&#39;t have segfault. I can send big array with small<br>
&quot;btl_tcp_eager_limit&quot; from a 64bit machine to a 32bit one.<br>
<br>
Do I have to move this thread to devel mailing list ?<br>
<br>
Regards,<br>
<br>
   TMHieu<br>
<br>
On Tue, Mar 2, 2010 at 2:54 PM, TRINH Minh Hieu &lt;<a href="mailto:mhtrinh@gmail.com" target="_blank">mhtrinh@gmail.com</a>&gt; wrote:<br>
  <br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Hello,<br>
<br>
Yes, I compiled OpenMPI with --enable-heterogeneous. More precisely I<br>
compiled with :<br>
$ ./configure --prefix=/tmp/openmpi --enable-heterogeneous<br>
--enable-cxx-exceptions --enable-shared<br>
--enable-orterun-prefix-by-default<br>
$ make all install<br>
<br>
I attach the output of ompi_info of my 2 machines.<br>
<br>
? ?TMHieu<br>
<br>
On Tue, Mar 2, 2010 at 1:57 PM, Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt; wrote:<br>
    <br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Did you configure Open MPI with --enable-heterogeneous?<br>
<br>
On Feb 28, 2010, at 1:22 PM, TRINH Minh Hieu wrote:<br>
<br>
      <br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Hello,<br>
<br>
I have some problems running MPI on my heterogeneous cluster. More<br>
precisley i got segmentation fault when sending a large array (about<br>
10000) of double from a i686 machine to a x86_64 machine. It does not<br>
happen with small array. Here is the send/recv code source (complete<br>
source is in attached file) :<br>
========code ================<br>
? ? if (me == 0 ) {<br>
? ? ? ? for (int pe=1; pe&lt;nprocs; pe++)<br>
? ? ? ? {<br>
? ? ? ? ? ? ? ? printf(&quot;Receiving from proc %d : &quot;,pe); fflush(stdout);<br>
? ? ? ? ? ? d=(double *)malloc(sizeof(double)*n);<br>
? ? ? ? ? ? MPI_Recv(d,n,MPI_DOUBLE,pe,999,MPI_COMM_WORLD,&amp;status);<br>
? ? ? ? ? ? printf(&quot;OK\n&quot;); fflush(stdout);<br>
? ? ? ? }<br>
? ? ? ? printf(&quot;All done.\n&quot;);<br>
? ? }<br>
? ? else {<br>
? ? ? d=(double *)malloc(sizeof(double)*n);<br>
? ? ? MPI_Send(d,n,MPI_DOUBLE,0,999,MPI_COMM_WORLD);<br>
? ? }<br>
======== code ================<br>
<br>
I got segmentation fault with n=10000 but no error with n=1000<br>
I have 2 machines :<br>
sbtn155 : Intel Xeon, ? ? ? ? x86_64<br>
sbtn211 : Intel Pentium 4, i686<br>
<br>
The code is compiled in x86_64 and i686 machine, using OpenMPI 1.4.1,<br>
installed in /tmp/openmpi :<br>
[mhtrinh@sbtn211 heterogenous]$ make hetero<br>
gcc -Wall -I. -std=c99 -O3 -I/tmp/openmpi/include -c hetero.c -o hetero.i686.o<br>
/tmp/openmpi/bin/mpicc -Wall -I. -std=c99 -O3 -I/tmp/openmpi/include<br>
hetero.i686.o -o hetero.i686 -lm<br>
<br>
[mhtrinh@sbtn155 heterogenous]$ make hetero<br>
gcc -Wall -I. -std=c99 -O3 -I/tmp/openmpi/include -c hetero.c -o hetero.x86_64.o<br>
/tmp/openmpi/bin/mpicc -Wall -I. -std=c99 -O3 -I/tmp/openmpi/include<br>
hetero.x86_64.o -o hetero.x86_64 -lm<br>
<br>
I run with the code using appfile and got thoses error :<br>
$ cat appfile<br>
--host sbtn155 -np 1 hetero.x86_64<br>
--host sbtn155 -np 1 hetero.x86_64<br>
--host sbtn211 -np 1 hetero.i686<br>
<br>
$ mpirun -hetero --app appfile<br>
Input array length :<br>
10000<br>
Receiving from proc 1 : OK<br>
Receiving from proc 2 : [sbtn155:26386] *** Process received signal ***<br>
[sbtn155:26386] Signal: Segmentation fault (11)<br>
[sbtn155:26386] Signal code: Address not mapped (1)<br>
[sbtn155:26386] Failing at address: 0x200627bd8<br>
[sbtn155:26386] [ 0] /lib64/libpthread.so.0 [0x3fa4e0e540]<br>
[sbtn155:26386] [ 1] /tmp/openmpi/lib/openmpi/mca_pml_ob1.so [0x2aaaad8d7908]<br>
[sbtn155:26386] [ 2] /tmp/openmpi/lib/openmpi/mca_btl_tcp.so [0x2aaaae2fc6e3]<br>
[sbtn155:26386] [ 3] /tmp/openmpi/lib/libopen-pal.so.0 [0x2aaaaafe39db]<br>
[sbtn155:26386] [ 4]<br>
/tmp/openmpi/lib/libopen-pal.so.0(opal_progress+0x9e) [0x2aaaaafd8b9e]<br>
[sbtn155:26386] [ 5] /tmp/openmpi/lib/openmpi/mca_pml_ob1.so [0x2aaaad8d4b25]<br>
[sbtn155:26386] [ 6] /tmp/openmpi/lib/libmpi.so.0(MPI_Recv+0x13b)<br>
[0x2aaaaab30f9b]<br>
[sbtn155:26386] [ 7] hetero.x86_64(main+0xde) [0x400cbe]<br>
[sbtn155:26386] [ 8] /lib64/libc.so.6(__libc_start_main+0xf4) [0x3fa421e074]<br>
[sbtn155:26386] [ 9] hetero.x86_64 [0x400b29]<br>
[sbtn155:26386] *** End of error message ***<br>
--------------------------------------------------------------------------<br>
mpirun noticed that process rank 0 with PID 26386 on node sbtn155<br>
exited on signal 11 (Segmentation fault).<br>
--------------------------------------------------------------------------<br>
<br>
Am I missing an option in order to run in heterogenous cluster ?<br>
MPI_Send/Recv have limit array size when using heterogeneous cluster ?<br>
Thanks for your help. Regards<br>
<br>
--<br>
============================================<br>
? ?M. TRINH Minh Hieu<br>
? ?CEA, IBEB, SBTN/LIRM,<br>
? ?F-30207 Bagnols-sur-C?ze, FRANCE<br>
============================================<br>
<br>
&lt;hetero.c.bz2&gt;_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
        <br>
</blockquote>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a><br>
For corporate legal information go to:<br>
<a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
      <br>
</blockquote></blockquote>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
-------------- next part --------------<br>
HTML attachment scrubbed and removed<br>
<br>
  **************************************<br>
  <br>
</blockquote>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br><br clear="all"><br>-- <br>============================================<br>   M. TRINH Minh Hieu<br>   CEA, IBEB, SBTN/LIRM,<br>   F-30207 Bagnols-sur-Cèze, FRANCE<br>============================================<br>



