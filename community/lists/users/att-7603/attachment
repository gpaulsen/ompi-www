I got the solution. I just need to set the appropriate tag to send and receive.<div>sorry for asking</div><div>thanks</div><div>winthan<br><br><div class="gmail_quote">On Wed, Dec 24, 2008 at 10:36 PM, Win Than Aung <span dir="ltr">&lt;<a href="mailto:keshunli@gmail.com">keshunli@gmail.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;"><span style="border-collapse:collapse"><div class="Ih2E3d">thanks Eugene for your example, it helps me a lot.<div>I bump into one more problems</div>
<div>lets say I have the file content as follow</div><div>I have total of six files which all contain real and imaginary value.</div>
<div>&quot;</div><div>1.001212 &nbsp; &nbsp; 1.0012121 &nbsp;//0th</div><div>1.001212 &nbsp; &nbsp; 1.0012121 &nbsp;//1st</div><div>1.001212 &nbsp; &nbsp; 1.0012121 &nbsp;//2nd<br></div><div>1.001212 &nbsp; &nbsp; 1.0012121 &nbsp;//3rd<br></div><div>1.001212 &nbsp; &nbsp; 1.0012121 &nbsp;//4th<br>

</div><div>1.001212 &nbsp; &nbsp; 1.0012121 //5th<br></div><div>1.001212 &nbsp; &nbsp; 1.0012121 //6th<br></div><div>&quot;</div><div>char send_buffer[1000];</div><div>i use &quot;mpirun -np 6 a.out&quot; which mean i let each processor get access to one file</div>

<div>each processor will add &quot;0th and 2nd&quot;(even values) (those values will be sent to root processor and save as file_even_add.dat&quot; and also each processor will add &quot;1st and 3rd&quot;(odd values) (those values will be sent to root processor(here is 0) and saved as &quot;file_odd_add.dat&quot;.</div>

<div><br></div></div><div>char recv_buffer[1000];<br></div><div>File* file_even_dat;</div><div>File* file_odd_dat;</div><div class="Ih2E3d"><div>if(mpi_my_id == root)</div><div>{</div></div><div>&nbsp;&nbsp; filepteven = fopen(&quot;C:\\fileeven.dat&quot;);</div>

<div>&nbsp;&nbsp; fileptodd = fopen(&quot;C:\\fileodd.dat&quot;);</div><div>&nbsp;&nbsp; &nbsp; int peer =0;</div><div>&nbsp;&nbsp; &nbsp;for(peer =0;peer&lt;mpi_total_size;peer++)</div><div>&nbsp;&nbsp; {</div><div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(peer!=root)</div><div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{</div>

<div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MPI_Recv(recv_buffer,MAX_STR_LEN,MPI_BYTE,MPI_ANY_TAG,MPI_COMM_WORLD,&amp;status);</div><div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}</div><div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;fprintf(filepteven, &quot;%s \n&quot; ,recv_buffer);</div>

<div>&nbsp;&nbsp; }</div><div>}</div></span><div><br></div><div>My question is how do i know which sentbuffer has even add values and which sentbuffer has odd add values? in which order did they get sent?</div><div>thanks</div><div>

winthan</div><br><div class="gmail_quote"><div><div></div><div class="Wj3C7c">On Tue, Dec 23, 2008 at 3:53 PM, Eugene Loh <span dir="ltr">&lt;<a href="mailto:Eugene.Loh@sun.com" target="_blank">Eugene.Loh@sun.com</a>&gt;</span> wrote:<br>
</div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div></div><div class="Wj3C7c">



  
  

<div bgcolor="#ffffff" text="#000000"><div>
Win Than Aung wrote:
<blockquote cite="http://mid9247a3360812231156m795ae044u49749b69f9ca2926@mail.gmail.com" type="cite">thanks for your reply jeff
  <div><br>
  </div>
  <div>so i tried following</div>
  <div><br>
  </div>
  <div><br>
  </div>
  <div><br>
  </div>
  <div><span style="border-collapse:collapse">#include
&lt;stdio.h&gt;<br>
#include &lt;mpi.h&gt;<br>
  <br>
int main(int argc, char **argv) {<br>
&nbsp;int np, me, sbuf = -1, rbuf = -2,mbuf=1000;<br>
int data[2];<br>
&nbsp;MPI_Init(&amp;argc,&amp;argv);<br>
&nbsp;MPI_Comm_size(MPI_COMM_WORLD,&amp;np);<br>
&nbsp;MPI_Comm_rank(MPI_COMM_WORLD,&amp;me);<br>
&nbsp;if ( np &lt; 2 ) MPI_Abort(MPI_COMM_WORLD,-1);<br>
  <br>
&nbsp;if ( me == 1 ) MPI_Send(&amp;sbuf,1,MPI_INT,0,344,MPI_COMM_WORLD);<br>
if(me==2) MPI_Send( &amp;mbuf,1,MPI_INT,0,344,MPI_COMM_WORLD);</span></div>
  <div><span style="border-collapse:collapse">if
( me == 0 ) {<br>
&nbsp;
MPI_Recv(data,2,MPI_INT,MPI_ANY_SOURCE,344,MPI_COMM_WORLD,MPI_STATUS_IGNORE);<br>
&nbsp;}<br>
  </span></div>
  <div><span style="border-collapse:collapse"><br>
&nbsp;MPI_Finalize();<br>
  <br>
&nbsp;return 0;<br>
}</span><br>
  </div>
  <div><br>
it can successfuly receive the one sent from processor 1(me==1) but it
failed to receive the one sent from processor 2(me==2)</div>
  <div>mpirun -np 3 hello</div>
</blockquote></div>
There is only one receive, so it receives only one message.&nbsp; When you
specify the element count for the receive, you&#39;re only specifying the
size of the buffer into which the message will be received.&nbsp; Only after
the message has been received can you inquire how big the message
actually was.<br>
<br>
Here is an example:<div><br>
<br>
% cat a.c<br>
#include &lt;stdio.h&gt;<br>
#include &lt;mpi.h&gt;<br>
<br>
int main(int argc, char **argv) {<br></div>
&nbsp; int np, me, peer, value;<div><br>
<br>
&nbsp; MPI_Init(&amp;argc,&amp;argv);<br>
&nbsp; MPI_Comm_size(MPI_COMM_WORLD,&amp;np);<br>
&nbsp; MPI_Comm_rank(MPI_COMM_WORLD,&amp;me);<br>
<br></div>
&nbsp; value = me * me + 1;<br>
&nbsp; if ( me == 0 ) {<br>
&nbsp;&nbsp;&nbsp; for ( peer = 0; peer &lt; np; peer++ ) {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if ( peer != 0 )
MPI_Recv(&amp;value,1,MPI_INT,peer,343,MPI_COMM_WORLD,MPI_STATUS_IGNORE);<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; printf(&quot;peer %d had value %d\n&quot;, peer, value);<br>
&nbsp;&nbsp;&nbsp; }<br>
&nbsp; }<br>
&nbsp; else MPI_Send(&amp;value,1,MPI_INT,0,343,MPI_COMM_WORLD);<br>
<br>
&nbsp; MPI_Finalize();<br>
<br>
&nbsp; return 0;<br>
}<br>
% mpirun -np 3 a.out<br>
peer 0 had value 1<br>
peer 1 had value 2<br>
peer 2 had value 5<br>
%<br>
<br>
Alternatively,<div><br>
<br>
#include &lt;stdio.h&gt;<br>
#include &lt;mpi.h&gt;<br>
<br></div>
#define MAXNP 1024<div><br>
int main(int argc, char **argv) {<br></div>
&nbsp; int np, me, peer, value, values[MAXNP];<div><br>
<br>
&nbsp; MPI_Init(&amp;argc,&amp;argv);<br>
&nbsp; MPI_Comm_size(MPI_COMM_WORLD,&amp;np);<br></div>
&nbsp; if ( np &gt; MAXNP ) MPI_Abort(MPI_COMM_WORLD,-1);<div><br>
&nbsp; MPI_Comm_rank(MPI_COMM_WORLD,&amp;me);<br></div>
&nbsp; value = me * me + 1;<br>
<br>
&nbsp; MPI_Gather(&amp;value, 1, MPI_INT,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; values, 1, MPI_INT, 0, MPI_COMM_WORLD);<br>
<br>
&nbsp; if ( me == 0 )<br>
&nbsp;&nbsp;&nbsp; for ( peer = 0; peer &lt; np; peer++ )<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; printf(&quot;peer %d had value %d\n&quot;, peer, values[peer]);<br>
<br>
&nbsp; MPI_Finalize();<br>
&nbsp; return 0;<br>
}<br>
% mpirun -np 3 a.out<br>
peer 0 had value 1<br>
peer 1 had value 2<br>
peer 2 had value 5<br>
%<br>
<br>
Which is better?&nbsp; Up to you.&nbsp; The collective routines (like MPI_Gather)
do offer MPI implementors (like people developing Open MPI) the
opportunity to perform special optimizations (e.g., gather using a
binary tree instead of having the root process perform so many
receives).<br>
</div>

<br></div></div><div class="Ih2E3d">_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></blockquote></div><br>
</blockquote></div><br></div>

