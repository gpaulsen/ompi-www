<html><head><meta http-equiv="Content-Type" content="text/html charset=windows-1252"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">If I understand correctly the communication parroter is a one-to-all type of communication isn't it (from your server to your clients)? In this case this might be a credit management issue, where the master is running out of ack buffers and the clients can't acknowledge the retrieval of the data.<div><br></div><div>Let's try to add "--mca&nbsp;btl_openib_flags 9" to the mpirun command (this disable the RMA communication and forces everything to have a pure send/recv semantics).</div><div><br></div><div>&nbsp; George.</div><div><br><div><div>On Jun 27, 2013, at 15:01 , Ed Blosch &lt;<a href="mailto:eblosch@1scom.net">eblosch@1scom.net</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><div><div>It ran a bit longer but still deadlocked. &nbsp;All matching sends are posted 1:1with posted recvs so it is a delivery issue of some kind. &nbsp;I'm running a debug compiled version tonight to see what that might turn up. &nbsp;I may try to rewrite with blocking sends and see if that works. &nbsp;I can also try adding a barrier (irecvs, barrier, isends, waitall) to make sure sends are not buffering waiting for recvs to be posted.</div><div><br></div><div><br></div><div><div style="font-size:75%;color:#575757">Sent via the Samsung Galaxy S™ III, an AT&amp;T 4G LTE smartphone</div></div> <br><br><br>-------- Original message --------<br>From: George Bosilca &lt;<a href="mailto:bosilca@icl.utk.edu">bosilca@icl.utk.edu</a>&gt; <br>Date:  <br>To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt; <br>Subject: Re: [OMPI users] Application hangs on mpi_waitall <br> <br><br>Ed,<br><br>Im not sure but there might be a case where the BTL is getting overwhelmed by the nob-blocking operations while trying to setup the connection. There is a simple test for this. Add an MPI_Alltoall with a reasonable size (100k) before you start posting the non-blocking receives, and let's see if this solves your issue.<br><br>&nbsp; George.<br><br><br>On Jun 26, 2013, at 04:02 , <a href="mailto:eblosch@1scom.net">eblosch@1scom.net</a> wrote:<br><br>&gt; An update: I recoded the mpi_waitall as a loop over the requests with<br>&gt; mpi_test and a 30 second timeout.&nbsp; The timeout happens unpredictably,<br>&gt; sometimes after 10 minutes of run time, other times after 15 minutes, for<br>&gt; the exact same case.<br>&gt; <br>&gt; After 30 seconds, I print out the status of all outstanding receive<br>&gt; requests.&nbsp; The message tags that are outstanding have definitely been<br>&gt; sent, so I am wondering why they are not getting received?<br>&gt; <br>&gt; As I said before, everybody posts non-blocking standard receives, then<br>&gt; non-blocking standard sends, then calls mpi_waitall. Each process is<br>&gt; typically waiting on 200 to 300 requests. Is deadlock possible via this<br>&gt; implementation approach under some kind of unusual conditions?<br>&gt; <br>&gt; Thanks again,<br>&gt; <br>&gt; Ed<br>&gt; <br>&gt;&gt; I'm running OpenMPI 1.6.4 and seeing a problem where mpi_waitall never<br>&gt;&gt; returns.&nbsp; The case runs fine with MVAPICH.&nbsp; The logic associated with the<br>&gt;&gt; communications has been extensively debugged in the past; we don't think<br>&gt;&gt; it has errors.&nbsp;&nbsp; Each process posts non-blocking receives, non-blocking<br>&gt;&gt; sends, and then does waitall on all the outstanding requests.<br>&gt;&gt; <br>&gt;&gt; The work is broken down into 960 chunks. If I run with 960 processes (60<br>&gt;&gt; nodes of 16 cores each), things seem to work.&nbsp; If I use 160 processes<br>&gt;&gt; (each process handling 6 chunks of work), then each process is handling 6<br>&gt;&gt; times as much communication, and that is the case that hangs with OpenMPI<br>&gt;&gt; 1.6.4; again, seems to work with MVAPICH.&nbsp; Is there an obvious place to<br>&gt;&gt; start, diagnostically?&nbsp; We're using the openib btl.<br>&gt;&gt; <br>&gt;&gt; Thanks,<br>&gt;&gt; <br>&gt;&gt; Ed<br>&gt;&gt; _______________________________________________<br>&gt;&gt; users mailing list<br>&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; <br>&gt; <br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
