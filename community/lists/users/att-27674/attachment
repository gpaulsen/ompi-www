<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">Try adding —novm to that cmd line - if all the nodes are topologically identical (minus the GPU), then this will speed things up.<div class=""><br class=""><div class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Sep 24, 2015, at 9:26 AM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com" class="">fortran@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><div dir="ltr" style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><div class="gmail_extra"><div class="gmail_quote">On Thu, Sep 24, 2015 at 12:10 PM, Ralph Castain<span class="Apple-converted-space">&nbsp;</span><span dir="ltr" class="">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank" class="">rhc@open-mpi.org</a>&gt;</span><span class="Apple-converted-space">&nbsp;</span>wrote:<br class=""><blockquote class="gmail_quote" style="margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: rgb(204, 204, 204); border-left-style: solid; padding-left: 1ex;"><div style="word-wrap: break-word;" class="">Ah, sorry - wrong param. It’s the out-of-band that is having the problem. Try adding —mca oob_tcp_if_include &lt;foo&gt;</div></blockquote><div class=""><br class=""></div><div class="">Ooh. Okay. Look at this:</div><div class=""><br class=""></div><div class=""><div class=""><font face="monospace, monospace" class="">(13) $ mpirun --mca oob_tcp_if_include ib0 -np 2 ./helloWorld.x</font></div><div class=""><font face="monospace, monospace" class="">Process 1 of 2 is on r509i2n17&nbsp;</font></div><div class=""><font face="monospace, monospace" class="">Process 0 of 2 is on r509i2n17&nbsp;</font></div></div><div class=""><br class=""></div><div class="">So that is nice. Now the spin up if I have 8 or so nodes is rather...slow. But at this point I'll take working over efficient. Quick startup can come later.</div><div class=""><br class=""></div><div class="">Matt</div><div class=""><br class=""></div><div class="">&nbsp;</div><blockquote class="gmail_quote" style="margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: rgb(204, 204, 204); border-left-style: solid; padding-left: 1ex;"><div style="word-wrap: break-word;" class=""><div class=""><br class=""></div><div class=""><br class=""><div class=""><blockquote type="cite" class=""><div class=""><div class="h5"><div class="">On Sep 24, 2015, at 8:56 AM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com" target="_blank" class="">fortran@gmail.com</a>&gt; wrote:</div><br class=""></div></div><div class=""><div class=""><div class="h5"><div dir="ltr" class="">Ralph,<div class=""><br class=""></div><div class="">I believe these nodes might have both an Ethernet and Infiniband port where the Ethernet port is not the one to use. Is there a way to tell Open MPI to ignore any ethernet devices it sees? I've tried:</div><pre class="">--mca btl sm,openib,self</pre><div class="">and (based on the advice of the much more intelligent support at NAS):</div><div class=""><pre class="">--mca btl openib,self --mca btl_openib_if_include mlx4_0,mlx4_1
</pre></div><div class="">But neither worked.</div><div class=""><br class=""></div><div class="">Matt</div><div class=""><br class=""></div></div><div class="gmail_extra"><br class=""><div class="gmail_quote">On Thu, Sep 24, 2015 at 11:41 AM, Ralph Castain<span class="Apple-converted-space">&nbsp;</span><span dir="ltr" class="">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank" class="">rhc@open-mpi.org</a>&gt;</span><span class="Apple-converted-space">&nbsp;</span>wrote:<br class=""><blockquote class="gmail_quote" style="margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: rgb(204, 204, 204); border-left-style: solid; padding-left: 1ex;"><div style="word-wrap: break-word;" class="">Starting in the 1.7 series, OMPI by default launches daemons on all nodes in the allocation during startup. This is done so we can “probe” the topology of the nodes and use that info during the process mapping procedure - e.g., if you want to map-by NUMA regions.<div class=""><br class=""></div><div class="">What is happening here is that some of the nodes in your allocation aren’t allowing those daemons to callback to mpirun. Either a firewall is in the way, or something is preventing it.</div><div class=""><br class=""></div><div class="">If you don’t want to launch on those other nodes, you could just add —novm to your cmd line, or use the —host option to restrict us to your local node. However, I imagine you got the bigger allocation so you could use it :-)</div><div class=""><br class=""></div><div class="">In which case, you need to remove the obstacle. You might check for firewall, or check to see if multiple NICs are on the non-maia nodes (this can sometimes confuse things, especially if someone put the NICs on the same IP subnet)</div><div class=""><br class=""></div><div class="">HTH</div><div class="">Ralph</div><div class=""><br class=""></div><div class=""><br class=""></div><div class=""><br class=""><div class=""><blockquote type="cite" class=""><div class=""><div class=""><div class="">On Sep 24, 2015, at 8:18 AM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com" target="_blank" class="">fortran@gmail.com</a>&gt; wrote:</div><br class=""></div></div><div class=""><div class=""><div class=""><div dir="ltr" class="">Open MPI Users,<div class=""><br class=""></div><div class="">I'm hoping someone here can help. I built Open MPI 1.10.0 with PGI 15.7 using this configure string:</div><div class=""><br class=""></div><div class=""><div class=""><font face="monospace, monospace" class="">&nbsp;./configure --disable-vt --with-tm=/PBS --with-verbs --disable-wrapper-rpath \</font></div><div class=""><font face="monospace, monospace" class="">&nbsp; &nbsp;<span class="Apple-converted-space">&nbsp;</span>CC=pgcc CXX=pgCC FC=pgf90 F77=pgf77 CFLAGS='-fpic -m64' \</font></div><div class=""><font face="monospace, monospace" class="">&nbsp; &nbsp;<span class="Apple-converted-space">&nbsp;</span>CXXFLAGS='-fpic -m64' FCFLAGS='-fpic -m64' FFLAGS='-fpic -m64' \</font></div><div class=""><font face="monospace, monospace" class="">&nbsp; &nbsp;<span class="Apple-converted-space">&nbsp;</span>--prefix=/nobackup/gmao_SIteam/MPI/pgi_15.7-openmpi_1.10.0 |&amp; tee configure.pgi15.7.log</font></div><div class=""><br class=""></div><div class="">It seemed to pass 'make check'.&nbsp;</div><div class=""><br class=""></div><div class="">I'm working at pleiades at NAS, and there they have both Sandy Bridge nodes with GPUs (maia) and regular Sandy Bridge compute nodes (here after called Sandy) without. To be extra careful (since PGI compiles to the architecture you build on) I took a Westmere node and built Open MPI there just in case.</div><div class=""><br class=""></div><div class="">So, as I said, all seems to work with a test. I now grab a maia node, maia1, of an allocation of 4 I had:</div><div class=""><br class=""></div><div class=""><div class=""><font face="monospace, monospace" class="">(102) $ mpicc -tp=px-64 -o helloWorld.x helloWorld.c</font></div><div class=""><font face="monospace, monospace" class="">(103) $ mpirun -np 2 ./helloWorld.x</font></div><div class=""><font face="monospace, monospace" class="">Process 0 of 2 is on maia1&nbsp;</font></div><div class=""><font face="monospace, monospace" class="">Process 1 of 2 is on maia1&nbsp;</font></div></div><div class=""><br class=""></div><div class="">Good. Now, let's go to a Sandy Bridge (non-GPU) node, r321i7n16, of an allocation of 8 I had:</div><div class=""><br class=""></div><div class=""><div class=""><font face="monospace, monospace" class="">(49) $ mpicc -tp=px-64 -o helloWorld.x helloWorld.c</font></div><div class=""><font face="monospace, monospace" class="">(50) $ mpirun -np 2 ./helloWorld.x</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n11:13063] [[62995,0],7] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n6:57417] [[62995,0],2] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n7:67287] [[62995,0],3] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n8:57429] [[62995,0],4] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n10:35329] [[62995,0],6] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n9:13456] [[62995,0],5] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div></div><div class=""><br class=""></div><div class="">Hmm. Let's try turning off tcp (often my first thought when on an Infiniband system):</div><div class=""><br class=""></div><div class=""><div class=""><font face="monospace, monospace" class="">(51) $ mpirun --mca btl sm,openib,self -np 2 ./helloWorld.x</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n6:57420] [[62996,0],2] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n9:13459] [[62996,0],5] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n8:57432] [[62996,0],4] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n7:67290] [[62996,0],3] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n11:13066] [[62996,0],7] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n10:35332] [[62996,0],6] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div></div><div class=""><br class=""></div><div class="">Now, the nodes reporting the issue seem to be the "other" nodes on the allocation that are in a different rack:</div><div class=""><br class=""></div><div class=""><div class=""><font face="monospace, monospace" class="">(52) $ cat $PBS_NODEFILE | uniq</font></div><div class=""><font face="monospace, monospace" class="">r321i7n16</font></div><div class=""><font face="monospace, monospace" class="">r321i7n17</font></div><div class=""><font face="monospace, monospace" class="">r323i5n6</font></div><div class=""><font face="monospace, monospace" class="">r323i5n7</font></div><div class=""><font face="monospace, monospace" class="">r323i5n8</font></div><div class=""><font face="monospace, monospace" class="">r323i5n9</font></div><div class=""><font face="monospace, monospace" class="">r323i5n10</font></div><div class=""><font face="monospace, monospace" class="">r323i5n11</font></div></div><div class=""><br class=""></div><div class="">Maybe that's a clue? I didn't think this would matter if I only ran two processes...and it works on the multi-node maia allocation.</div><div class=""><br class=""></div><div class="">I've tried searching the web, but the only place I've seen tcp_peer_send_blocking is in a PDF where they say it's an error that can be seen:</div><div class=""><br class=""></div><div class=""><a href="http://www.hpc.mcgill.ca/downloads/checkpointing_workshop/20150326%20-%20McGill%20-%20Checkpointing%20Techniques.pdf" target="_blank" class="">http://www.hpc.mcgill.ca/downloads/checkpointing_workshop/20150326%20-%20McGill%20-%20Checkpointing%20Techniques.pdf</a><br class=""></div><div class=""><br class=""></div><div class="">Any ideas for what this error can mean?</div><div class=""><br class=""></div>--<span class="Apple-converted-space">&nbsp;</span><br class=""><div class=""><div dir="ltr" class=""><div class=""><div dir="ltr" class=""><div class="">Matt Thompson</div></div></div><blockquote style="margin: 0px 0px 0px 40px; border: none; padding: 0px;" class=""><div class=""><div class=""><div class="">Man Among Men</div></div></div><div class=""><div class=""><div class="">Fulcrum of History</div></div></div></blockquote></div></div></div></div></div></div>_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">Subscription:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this post:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2015/09/27669.php" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/09/27669.php</a></div></blockquote></div><br class=""></div></div><br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">Subscription:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this post:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2015/09/27670.php" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/09/27670.php</a><br class=""></blockquote></div><br class=""><br clear="all" class=""><div class=""><br class=""></div>--<span class="Apple-converted-space">&nbsp;</span><br class=""><div class=""><div dir="ltr" class=""><div class=""><div dir="ltr" class=""><div class="">Matt Thompson</div></div></div><blockquote style="margin: 0px 0px 0px 40px; border: none; padding: 0px;" class=""><div class=""><div class=""><div class="">Man Among Men</div></div></div><div class=""><div class=""><div class="">Fulcrum of History</div></div></div></blockquote></div></div></div>_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">Subscription:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class=""></div></div>Link to this post:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2015/09/27671.php" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/09/27671.php</a></div></blockquote></div><br class=""></div></div><br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this post:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2015/09/27672.php" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/09/27672.php</a><br class=""></blockquote></div><br class=""><br clear="all" class=""><div class=""><br class=""></div>--<span class="Apple-converted-space">&nbsp;</span><br class=""><div class="gmail_signature"><div dir="ltr" class=""><div class=""><div dir="ltr" class=""><div class="">Matt Thompson</div></div></div><blockquote style="margin: 0px 0px 0px 40px; border: none; padding: 0px;" class=""><div class=""><div class=""><div class="">Man Among Men</div></div></div><div class=""><div class=""><div class="">Fulcrum of History</div></div></div></blockquote></div></div></div></div><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">_______________________________________________</span><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">users mailing list</span><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><a href="mailto:users@open-mpi.org" style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class="">users@open-mpi.org</a><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">Subscription:<span class="Apple-converted-space">&nbsp;</span></span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><span style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !important;" class="">Link to this post:<span class="Apple-converted-space">&nbsp;</span></span><a href="http://www.open-mpi.org/community/lists/users/2015/09/27673.php" style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class="">http://www.open-mpi.org/community/lists/users/2015/09/27673.php</a></div></blockquote></div><br class=""></div></div></body></html>
