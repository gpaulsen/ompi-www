Remember also that the RTE API&#39;s changed between 1.2 and 1.3 - so I&#39;m not sure what will happen in that case. It could be that the ones touching the MPI layer remained stable (don&#39;t honestly recall), though I believe there are RTE calls in 1.3 that don&#39;t exist in 1.2. I would think you would have a problem if you hit one of those (e.g., when doing a comm_spawn).<br>
<br><br><br><div class="gmail_quote">On Mon, Apr 27, 2009 at 12:36 PM, Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
I&#39;d actually be surprised if it works.<br>
<br>
The back-end sizes of Open MPI structures definitely changed between 1.2 and 1.3.  We used to think that this didn&#39;t matter, but then we found out that we were wrong.  :-)  Hence, I&#39;d think that the same exact issues you have with taking a 1.2-compiled MPI application and running with 1.3 would also occur if you took a 1.3-compiled application and ran it with 1.2.  If it works at all, I&#39;m guessing that you&#39;re getting lucky.<br>

<br>
We only finally put in some ABI fixes in 1.3.2.  So the ABI *should* be steady throughout the rest of the 1.3 and 1.4 series.<div><div></div><div class="h5"><br>
<br>
<br>
On Apr 27, 2009, at 2:30 PM, Brian W. Barrett wrote:<br>
<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
I think Serge is talking about compiling the application against one<br>
version of Open MPI, linking dynamically, then running against another<br>
version of Open MPI.  Since it&#39;s dynamically linked, the ORTE/OMPI<br>
interactions are covered (the version of mpirun, libopen-rte, and libmpi<br>
all match).  The question of application binary compatibility can<br>
generally be traced to a couple of issues:<br>
<br>
   - function signatures of all MPI functions<br>
   - constants in mpi.h changing<br>
   - size of structures due to the bss optimization for globals<br>
<br>
I can&#39;t remember when we changed function signatures last, but it probably<br>
has happened.  They may be minor enough to not matter, and definitely<br>
wouldn&#39;t be in the usual set of functions people use (send,recv,wait,<br>
etc.)<br>
<br>
The constants in mpi.h have been pretty steady since day 1, although I<br>
haven&#39;t checked when they last changed.<br>
<br>
The final one actually should be ok for going from later versions of Open<br>
MPI to newer versions, as the structures in question usually grow and<br>
rarely shrink in size.<br>
<br>
In other words, it&#39;ll probably work, but no one in the group is going to<br>
say anything stronger than that.<br>
<br>
Brian<br>
<br>
On Mon, 27 Apr 2009, Ralph Castain wrote:<br>
<br>
&gt; It&#39;s hard for me to believe that would work as there are fundamental<br>
&gt; differences in the MPI-to-RTE interactions between those releases. If it<br>
&gt; does, it could be a fluke - I personally would not trust it.<br>
&gt;<br>
&gt; Ralph<br>
&gt;<br>
&gt; On Mon, Apr 27, 2009 at 12:04 PM, Serge &lt;<a href="mailto:skhan@ap.smu.ca" target="_blank">skhan@ap.smu.ca</a>&gt; wrote:<br>
&gt;       Hi Jeff,<br>
&gt;<br>
&gt;       &gt; That being said, we have fixed this issue and expect to<br>
&gt;       support binary<br>
&gt;       &gt; compatibility between Open MPI releases starting with<br>
&gt;       v1.3.2 (v1.3.1<br>
&gt;<br>
&gt;       As far as I can tell from reading the release notes for<br>
&gt;       v1.3.2, the binary compatibility has not been announced yet.<br>
&gt;       It was rather a bug fix release. Is it correct? Does it mean<br>
&gt;       that the compatibility feature is pushed to later releases,<br>
&gt;       v1.3.3, 1.3.4?<br>
&gt;<br>
&gt;       In my original message (see below) I was looking for advice<br>
&gt;       as for a seamless transition from v1.2.x to v1.3.x in a<br>
&gt;       shared multi-user environment.<br>
&gt;<br>
&gt;       Interestingly enough, recently I noticed that although it&#39;s<br>
&gt;       impossible to run an application compiled with v1.2.x under<br>
&gt;       v1.3.x, the opposite does actually work. An application<br>
&gt;       compiled with v1.3.x runs using Open MPI v1.2.x.<br>
&gt;       Specifically, I tested an application compiled with v1.3.0<br>
&gt;       and v1.3.2, running under Open MPI v1.2.7.<br>
&gt;<br>
&gt;       This gives me a perfect opportunity to recompile all the<br>
&gt;       parallel applications with v1.3.x, transparently to users;<br>
&gt;       and then switch the default Open MPI library from v1.2.7 to<br>
&gt;       v1.3.x, when all the apps have been rebuilt.<br>
&gt;<br>
&gt;       The problem is that I am not 100% sure in this approach, even<br>
&gt;       having some successful tests done.<br>
&gt;<br>
&gt;       Is it safe to run an application built with 1.3.x under<br>
&gt;       1.2.x? Does it make sense to you?<br>
&gt;<br>
&gt;       = Serge<br>
&gt;<br>
&gt;<br>
&gt;       Jeff Squyres wrote:<br>
&gt;             Unfortunately, binary compatibility between Open<br>
&gt;             MPI release versions has never been guaranteed<br>
&gt;             (even between subreleases).<br>
&gt;<br>
&gt;             That being said, we have fixed this issue and<br>
&gt;             expect to support binary compatibility between<br>
&gt;             Open MPI releases starting with v1.3.2 (v1.3.1<br>
&gt;             should be released soon; we&#39;re aiming for v1.3.2<br>
&gt;             towards the beginning of next month).<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;             On Mar 10, 2009, at 11:59 AM, Serge wrote:<br>
&gt;<br>
&gt;                   Hello,<br>
&gt;<br>
&gt;                   We have a number of applications<br>
&gt;                   built with Open MPI 1.2 in a shared<br>
&gt;                   multi-user environment. The Open MPI<br>
&gt;                   library upgrade has been always<br>
&gt;                   transparent and painless within the<br>
&gt;                   v1.2 branch. Now we would like to<br>
&gt;                   switch to Open MPI 1.3 as seamlessly.<br>
&gt;                   However, an application built with<br>
&gt;                   ompi v1.2 will not run with the 1.3<br>
&gt;                   library; the typical error messages<br>
&gt;                   are given below. Apparently, the type<br>
&gt;                   ompi_communicator_t has changed.<br>
&gt;<br>
&gt;                   Symbol `ompi_mpi_comm_null&#39; has<br>
&gt;                   different size in shared object,<br>
&gt;                   consider re-linking<br>
&gt;                   Symbol `ompi_mpi_comm_world&#39; has<br>
&gt;                   different size in shared object,<br>
&gt;                   consider re-linking<br>
&gt;<br>
&gt;                   Do I have to rebuild all the<br>
&gt;                   applications with Open MPI 1.3?<br>
&gt;<br>
&gt;                   Is there a better way to do a smooth<br>
&gt;                   upgrade?<br>
&gt;<br>
&gt;                   Thank you.<br>
&gt;<br>
&gt;                   = Serge<br>
&gt;<br>
&gt;       _______________________________________________<br>
&gt;       users mailing list<br>
&gt;       <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt;       <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote>
<br>
<br></div></div><font color="#888888">
-- <br>
Jeff Squyres<br>
Cisco Systems</font><div><div></div><div class="h5"><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

