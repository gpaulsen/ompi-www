<span style="color: rgb(51, 51, 255);">Jeff,</span><br style="color: rgb(51, 51, 255);"><br style="color: rgb(51, 51, 255);"><span style="color: rgb(51, 51, 255);">Thank you for your thoughts. Some more comments inlined.</span>
<br style="color: rgb(51, 51, 255);"><br style="color: rgb(51, 51, 255);"><span style="color: rgb(51, 51, 255);">Regards,</span><br style="color: rgb(51, 51, 255);"><span style="color: rgb(51, 51, 255);">--Oleg</span><br>
<br><div><span class="gmail_quote">On 10/9/07, <b class="gmail_sendername">Jeff Squyres</b> &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:</span><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Interesting idea.<br><br>One obvious solution would be to mpirun your controller tasks and, as<br>you mentioned, use MPI to communicate between them.&nbsp;&nbsp;Then you can use<br>MPI_COMM_SPAWN to launch the actual MPI job that you want to monitor.
</blockquote><div><br><span style="color: rgb(51, 51, 255);">Well. Yes, it&#39;s certainly could be done, but would not work in my scenario.&nbsp; As I said before, <br>I use dynamic
instrumentation API (DynInst API) to control and instrument MPI tasks. <br>DynInst is sort of a debugger, it uses ptrace() on Linux to control processes. So I need to use dyninst API<br>to create a controlled process (and not fork() it or MPI_Spawn () it),or eventually I could fork it, and later
<br>attach (with DynInst) to a running process in order to to control it. In the latter case however, I would loose control<br>over the first several seconds of execution.<br><br></span></div><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
However, this will only more-or-less work.&nbsp;&nbsp;OMPI currently polls<br>aggressively to make message passing progress, so if you end up over-<br>subscribing nodes (because you filled up the cores on one node with<br>all the target MPI processes but also have 1 or more controller
<br>processes running on the same node), they&#39;ll thrash each other and<br>you&#39;ll get -- at best -- unreliable/unrepeatable performance fraught<br>with lots of race conditions.</blockquote><div><br><span style="color: rgb(51, 51, 255);">
This actually is a less serious issue than it seems. The daemon itself is a very lightweight process. After executing the startup code (binary parsing, process creation and instrumentation) it lets the MPI process go without any additional overhead and than it sits waiting on certain events, so normally the intrusion is less than 2%. The overhead of instrumentation inserted into MPI task is controlled with a threshold and if placed reasonably stays low (egg. not in a tight loop that executes lots of times, but on entry/exit of let&#39;s say MPI_xxx comm calls).
<br><br></span></div><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">Another issue is that OMPI&#39;s MPI_COMM_SPAWN does not give good<br>
options to allow specific process placement, so it might be a little<br>dicey to get processes to land exactly where you want them.</blockquote><div><br><span style="color: rgb(51, 51, 255);">Not an option, as daemon and task must sit on the same host. The best scenario is dual-core host, one cpu per task and another per daemon.
</span><br></div><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">Alternatively, you could simply locally fork()/exec() your target<br>process from the controller.&nbsp;&nbsp;But the MPI spec does state that the
<br>use of fork()&nbsp;&nbsp;is undefined within an MPI process.&nbsp;&nbsp;Indeed, if you<br>are using a high-speed network such as InfiniBand or Myrinet, calling<br>fork() after you call MPI_INIT, Bad Things(tm) will happen (we can<br>explain more if you care).&nbsp;&nbsp;But if you&#39;re only using TCP, you should
<br>be fine.</blockquote><div><br><span style="color: rgb(51, 51, 255);">More less this is what I was doing. Daemon is mpirun, but it does not call MPI_Init itself but DynInst-forks the mpi task that calls MPI_Init. I tested this on OpenMPI using TCP/IP and Infiniband and MPICH and LAMMPI (on TCP) and it worked.
</span><br style="color: rgb(51, 51, 255);"><br></div><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">Another option might be to mpirun your target MPI app, have it wait
<br>in some kind of local barrier, and then mpirun your controllers on<br>the same machines.&nbsp;&nbsp;The controllers find/attach to your target<br>processes, release them from the local barrier, and then you&#39;re good<br>to go -- both your controllers and your target app are fully up and
<br>running under MPI.&nbsp;&nbsp;You&#39;ll still have the spinning/performance issue,<br>though -- so you won&#39;t want to oversubscribe nodes.</blockquote><div><br><span style="color: rgb(51, 51, 255);">Absolutely, this would be attach scenario for the daemons and they could use MPI. Nice idea.
</span><br style="color: rgb(51, 51, 255);"><span style="color: rgb(51, 51, 255);">Unfortunately it would make the tool usage more complicated and their would be no control on what happens during first several seconds.</span>
<br style="color: rgb(51, 51, 255);"><br></div><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">Does this help?</blockquote><div><br><span style="color: rgb(51, 51, 255);">
Open-thinking always helps. Thank you.</span><br style="color: rgb(51, 51, 255);"><br><span style="color: rgb(51, 51, 255);">Finally I decided not to use MPI for inter daemon communication, but opted for MRNet infrastructure (multicast/reduction network, 
<a href="http://www.paradyn.org/mrnet/release-1.1/UG.html">http://www.paradyn.org/mrnet/release-1.1/UG.html</a>)</span>. <br><span style="color: rgb(51, 51, 255);"><br style="color: rgb(51, 51, 255);"></span><br></div><br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">On Oct 1, 2007, at 10:49 PM, Oleg Morajko wrote:<br><br>&gt; Hello,<br>&gt;<br>&gt; In the context of my PhD research, I have been developing a run-
<br>&gt; time performance analyzer for MPI-based applications.<br>&gt; My tool provides a controller process for each MPI task. In<br>&gt; particular, when a MPI job is started, a special wrapper script is<br>&gt; generated that first starts my controller processes and next each
<br>&gt; controller spawns an actual MPI task (that performs MPI_Init etc.).<br>&gt; I use dynamic instrumentation API (DynInst API) to control and<br>&gt; instrument MPI tasks.<br>&gt;<br>&gt; The point is I need to intercommunicate my controller processes, in
<br>&gt; particular I need a point-to-point communication between arbitrary<br>&gt; pair of controllers. So it seems reasonable to take advantage of<br>&gt; MPI itself and use it for communication. However I am not sure what
<br>&gt; would be the impact of calling MPI_Init and communicating from<br>&gt; controller processes taking into account both controllers and<br>&gt; actual MPI&nbsp;&nbsp;processes where started with the same mpirun<br>&gt; invocation. Actually I would need to assure that controllers have a
<br>&gt; separate MPI execution enviroment while the application has another<br>&gt; one.<br>&gt;<br>&gt; Any suggestions how to achive that? Obviously another option is to<br>&gt; use sockets to intercommunicate controllers, but having MPI this
<br>&gt; seems to be overkill.<br>&gt;<br>&gt; Thank you in advance for your help.<br>&gt;<br>&gt; Regards,<br>&gt; --Oleg<br>&gt;<br>&gt; PhD student, Universitat Autonoma de Barcelona, Spain<br>&gt;<br>&gt; _______________________________________________
<br>&gt; users mailing list<br>&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br><br>--<br>Jeff Squyres<br>Cisco Systems<br><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

