<html xmlns:v="urn:schemas-microsoft-com:vml" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:w="urn:schemas-microsoft-com:office:word" xmlns:m="http://schemas.microsoft.com/office/2004/12/omml" xmlns="http://www.w3.org/TR/REC-html40"><head><meta http-equiv=Content-Type content="text/html; charset=us-ascii"><meta name=Generator content="Microsoft Word 14 (filtered medium)"><style><!--
/* Font Definitions */
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:Tahoma;
	panose-1:2 11 6 4 3 5 4 4 2 4;}
/* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin:0in;
	margin-bottom:.0001pt;
	font-size:12.0pt;
	font-family:"Times New Roman","serif";}
a:link, span.MsoHyperlink
	{mso-style-priority:99;
	color:blue;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{mso-style-priority:99;
	color:purple;
	text-decoration:underline;}
span.EmailStyle17
	{mso-style-type:personal-reply;
	font-family:"Calibri","sans-serif";
	color:#1F497D;}
.MsoChpDefault
	{mso-style-type:export-only;
	font-family:"Calibri","sans-serif";}
@page WordSection1
	{size:8.5in 11.0in;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
	{page:WordSection1;}
--></style><!--[if gte mso 9]><xml>
<o:shapedefaults v:ext="edit" spidmax="1026" />
</xml><![endif]--><!--[if gte mso 9]><xml>
<o:shapelayout v:ext="edit">
<o:idmap v:ext="edit" data="1" />
</o:shapelayout></xml><![endif]--></head><body lang=EN-US link=blue vlink=purple><div class=WordSection1><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>To answer the original questions, Open MPI will look at taking advantage of the RDMA CUDA when it is available.&nbsp; Obviously, work needs to be done to figure out the best way to integrate into the library.&nbsp; Much like there are a variety of protocols under the hood to support host transfer of data via IB, we will have to see what works&nbsp; best for transferring GPU buffers.<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>It is unclear how this will affect the send/receive latency.&nbsp; <o:p></o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>Lastly, the support will be for Kepler &#8211;class Quadro and Tesla devices.&nbsp; <o:p></o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>Rolf<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><div style='border:none;border-left:solid blue 1.5pt;padding:0in 0in 0in 4.0pt'><div><div style='border:none;border-top:solid #B5C4DF 1.0pt;padding:3.0pt 0in 0in 0in'><p class=MsoNormal><b><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'>From:</span></b><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'> users-bounces@open-mpi.org [mailto:users-bounces@open-mpi.org] <b>On Behalf Of </b>Durga Choudhury<br><b>Sent:</b> Tuesday, August 14, 2012 4:46 PM<br><b>To:</b> Open MPI Users<br><b>Subject:</b> Re: [OMPI users] RDMA GPUDirect CUDA...<o:p></o:p></span></p></div></div><p class=MsoNormal><o:p>&nbsp;</o:p></p><p class=MsoNormal style='margin-bottom:12.0pt'>Dear OpenMPI developers<br><br>I'd like to add my 2 cents that this would be a very desirable feature enhancement for me as well (and perhaps others).<br><br>Best regards<br>Durga<br><br><o:p></o:p></p><div><p class=MsoNormal>On Tue, Aug 14, 2012 at 4:29 PM, Zbigniew Koza &lt;<a href="mailto:zzkoza@gmail.com" target="_blank">zzkoza@gmail.com</a>&gt; wrote:<o:p></o:p></p><p class=MsoNormal>Hi,<br><br>I've just found this information on &nbsp;nVidia's plans regarding enhanced support for MPI in their CUDA toolkit:<br><a href="http://developer.nvidia.com/cuda/nvidia-gpudirect" target="_blank">http://developer.nvidia.com/cuda/nvidia-gpudirect</a><br><br>The idea that two GPUs can talk to each other via network cards without CPU as a middleman looks very promising.<br>This technology is supposed to be revealed and released in September.<br><br>My questions:<br><br>1. Will OpenMPI include &nbsp; RDMA support in its CUDA interface?<br>2. Any idea how much can this technology reduce the CUDA Send/Recv latency?<br>3. Any idea whether this technology will be available for Fermi-class Tesla devices or only for Keplers?<br><br>Regards,<br><br>Z Koza<br><br><br><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><o:p></o:p></p></div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div></div>
<DIV>
<HR>
</DIV>
<DIV>This email message is for the sole use of the intended recipient(s) and may 
contain confidential information.&nbsp; Any unauthorized review, use, disclosure 
or distribution is prohibited.&nbsp; If you are not the intended recipient, 
please contact the sender by reply email and destroy all copies of the original 
message. </DIV>
<DIV>
<HR>
</DIV>
<P></P>
</body></html>

