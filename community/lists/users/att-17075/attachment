<meta http-equiv="content-type" content="text/html; charset=utf-8">Thanks, YK and Pavel!<div>It works.<br><br><div class="gmail_quote">On Tue, Aug 2, 2011 at 4:52 PM, Yevgeny Kliteynik <span dir="ltr">&lt;<a href="mailto:kliteyn@dev.mellanox.co.il">kliteyn@dev.mellanox.co.il</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">See this FAQ entry:<br>
<a href="http://www.open-mpi.org/faq/?category=openfabrics#ib-xrc" target="_blank">http://www.open-mpi.org/faq/?category=openfabrics#ib-xrc</a><br>
<font color="#888888"><br>
-- YK<br>
</font><div><div></div><div class="h5"><br>
On 02-Aug-11 12:27 AM, Shamis, Pavel wrote:<br>
&gt; You may find some initial XRC tuning documentation here :<br>
&gt;<br>
&gt; <a href="https://svn.open-mpi.org/trac/ompi/ticket/1260" target="_blank">https://svn.open-mpi.org/trac/ompi/ticket/1260</a><br>
&gt;<br>
&gt; Pavel (Pasha) Shamis<br>
&gt; ---<br>
&gt; Application Performance Tools Group<br>
&gt; Computer Science and Math Division<br>
&gt; Oak Ridge National Laboratory<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; On Aug 1, 2011, at 11:41 AM, Yevgeny Kliteynik wrote:<br>
&gt;<br>
&gt;&gt; Hi,<br>
&gt;&gt;<br>
&gt;&gt; Please try running OMPI with XRC:<br>
&gt;&gt;<br>
&gt;&gt;   mpirun --mca btl openib... --mca btl_openib_receive_queues X,128,256,192,128:X,2048,256,128,32:X,12288,256,128,32:X,65536,256,128,32 ...<br>
&gt;&gt;<br>
&gt;&gt; XRC (eXtended Reliable Connection) decreases memory consumption<br>
&gt;&gt; of Open MPI by decreasing number of QP per machine.<br>
&gt;&gt;<br>
&gt;&gt; I&#39;m not entirely sure that XRC is supported on OMPI 1.4, but I&#39;m<br>
&gt;&gt; sure it is on later version of the 1.4 series (1.4.3).<br>
&gt;&gt;<br>
&gt;&gt; BTW, I do know that the command line is extremely user friendly<br>
&gt;&gt; and completely intuitive... :-)<br>
&gt;&gt; I&#39;ll have an XRC entry on the OMPI FAQ web page in a day or two,<br>
&gt;&gt; and you can find more details about this issue.<br>
&gt;&gt;<br>
&gt;&gt; OMPI FAQ: hxxp://<a href="http://www.open-mpi.org/faq/?category=openfabrics" target="_blank">www.open-mpi.org/faq/?category=openfabrics</a><br>
&gt;&gt;<br>
&gt;&gt; -- YK<br>
&gt;&gt;<br>
&gt;&gt; On 28-Jul-11 7:53 AM, 吕慧伟 wrote:<br>
&gt;&gt;&gt; Dear all,<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; I have encounted a problem concerns running large jobs on SMP cluster with Open MPI 1.4.<br>
&gt;&gt;&gt; The application need all-to-all communication, each process send messages to all other processes via MPI_Isend. It goes fine when running 256 processes, the problems occurs when process number&gt;=512.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; The error message is:<br>
&gt;&gt;&gt;          mpirun -np 512 -machinefile machinefile.512 ./my_app<br>
&gt;&gt;&gt;          [gh30][[23246,1],311][connect/btl_openib_connect_oob.c:463:qp_create_one] error creating qp errno says Cannot allocate memory<br>
&gt;&gt;&gt;          ...<br>
&gt;&gt;&gt;          [gh26][[23246,1],106][connect/btl_openib_connect_oob.c:809:rml_recv_cb] error in endpoint reply start connect<br>
&gt;&gt;&gt;          [gh26][[23246,1],117][connect/btl_openib_connect_oob.c:463:qp_create_one] error creating qp errno says Cannot allocate memory<br>
&gt;&gt;&gt;          ...<br>
&gt;&gt;&gt;          mpirun has exited due to process rank 424 with PID 26841 on<br>
&gt;&gt;&gt;          node gh31 exiting without calling &quot;finalize&quot;.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Related post (hxxp://<a href="http://www.open-mpi.org/community/lists/users/2009/07/9786.php" target="_blank">www.open-mpi.org/community/lists/users/2009/07/9786.php</a>) suggests it may run out of HCA QP resources. So I checked my system configuration with &#39;ibv_devinfo -v&#39; and get: &#39;max_qp: 261056&#39;. In my case, running with 256 processes would be under the limit: 256^2 = 65536&lt;  261056, but 512^2 = 262144&gt;  261056.<br>

&gt;&gt;&gt; My question is: how to increase the max_qp number of InfiniBand or how to get around this problem in MPI?<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Thanks in advance for any help you may give!<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Huiwei Lv<br>
&gt;&gt;&gt; PhD Student at Institute of Computing Technology<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; -------------------------<br>
&gt;&gt;&gt; p.s. The system informantion is provided below:<br>
&gt;&gt;&gt; $ ompi_info -v ompi full --parsable<br>
&gt;&gt;&gt; ompi:version:full:1.4<br>
&gt;&gt;&gt; ompi:version:svn:r22285<br>
&gt;&gt;&gt; ompi:version:release_date:Dec 08, 2009<br>
&gt;&gt;&gt; $ uname -a<br>
&gt;&gt;&gt; Linux gh26 2 . 6 . 18-128 . el5 #1 SMP Wed Jan 21 10:41:14 EST 2009 x86_64 x86_64 x86_64 GNU/Linux<br>
&gt;&gt;&gt; $ ulimit -l<br>
&gt;&gt;&gt; unlimited<br>
&gt;&gt;&gt; $ ibv_devinfo -v<br>
&gt;&gt;&gt; hca_id: mlx4_0<br>
&gt;&gt;&gt;          transport:                      InfiniBand (0)<br>
&gt;&gt;&gt;          fw_ver:                         2.7.000<br>
&gt;&gt;&gt;          node_guid:                      00d2:c910:0001:b6c0<br>
&gt;&gt;&gt;          sys_image_guid:                 00d2:c910:0001:b6c3<br>
&gt;&gt;&gt;          vendor_id:                      0x02c9<br>
&gt;&gt;&gt;          vendor_part_id:                 26428<br>
&gt;&gt;&gt;          hw_ver:                         0xB0<br>
&gt;&gt;&gt;          board_id:                       MT_0D20110009<br>
&gt;&gt;&gt;          phys_port_cnt:                  1<br>
&gt;&gt;&gt;          max_mr_size:                    0xffffffffffffffff<br>
&gt;&gt;&gt;          page_size_cap:                  0xfffffe00<br>
&gt;&gt;&gt;          max_qp:                         261056<br>
&gt;&gt;&gt;          max_qp_wr:                      16351<br>
&gt;&gt;&gt;          device_cap_flags:               0x00fc9c76<br>
&gt;&gt;&gt;          max_sge:                        32<br>
&gt;&gt;&gt;          max_sge_rd:                     0<br>
&gt;&gt;&gt;          max_cq:                         65408<br>
&gt;&gt;&gt;          max_cqe:                        4194303<br>
&gt;&gt;&gt;          max_mr:                         524272<br>
&gt;&gt;&gt;          max_pd:                         32764<br>
&gt;&gt;&gt;          max_qp_rd_atom:                 16<br>
&gt;&gt;&gt;          max_ee_rd_atom:                 0<br>
&gt;&gt;&gt;          max_res_rd_atom:                4176896<br>
&gt;&gt;&gt;          max_qp_init_rd_atom:            128<br>
&gt;&gt;&gt;          max_ee_init_rd_atom:            0<br>
&gt;&gt;&gt;          atomic_cap:                     ATOMIC_HCA (1)<br>
&gt;&gt;&gt;          max_ee:                         0<br>
&gt;&gt;&gt;          max_rdd:                        0<br>
&gt;&gt;&gt;          max_mw:                         0<br>
&gt;&gt;&gt;          max_raw_ipv6_qp:                0<br>
&gt;&gt;&gt;          max_raw_ethy_qp:                1<br>
&gt;&gt;&gt;          max_mcast_grp:                  8192<br>
&gt;&gt;&gt;          max_mcast_qp_attach:            56<br>
&gt;&gt;&gt;          max_total_mcast_qp_attach:      458752<br>
&gt;&gt;&gt;          max_ah:                         0<br>
&gt;&gt;&gt;          max_fmr:                        0<br>
&gt;&gt;&gt;          max_srq:                        65472<br>
&gt;&gt;&gt;          max_srq_wr:                     16383<br>
&gt;&gt;&gt;          max_srq_sge:                    31<br>
&gt;&gt;&gt;          max_pkeys:                      128<br>
&gt;&gt;&gt;          local_ca_ack_delay:             15<br>
&gt;&gt;&gt;                  port:   1<br>
&gt;&gt;&gt;                          state:                  PORT_ACTIVE (4)<br>
&gt;&gt;&gt;                          max_mtu:                2048 (4)<br>
&gt;&gt;&gt;                          active_mtu:             2048 (4)<br>
&gt;&gt;&gt;                          sm_lid:                 86<br>
&gt;&gt;&gt;                          port_lid:               73<br>
&gt;&gt;&gt;                          port_lmc:               0x00<br>
&gt;&gt;&gt;                          link_layer:             IB<br>
&gt;&gt;&gt;                          max_msg_sz:             0x40000000<br>
&gt;&gt;&gt;                          port_cap_flags:         0x02510868<br>
&gt;&gt;&gt;                          max_vl_num:             8 (4)<br>
&gt;&gt;&gt;                          bad_pkey_cntr:          0x0<br>
&gt;&gt;&gt;                          qkey_viol_cntr:         0x0<br>
&gt;&gt;&gt;                          sm_sl:                  0<br>
&gt;&gt;&gt;                          pkey_tbl_len:           128<br>
&gt;&gt;&gt;                          gid_tbl_len:            128<br>
&gt;&gt;&gt;                          subnet_timeout:         18<br>
&gt;&gt;&gt;                          init_type_reply:        0<br>
&gt;&gt;&gt;                          active_width:           4X (2)<br>
&gt;&gt;&gt;                          active_speed:           10.0 Gbps (4)<br>
&gt;&gt;&gt;                          phys_state:             LINK_UP (5)<br>
&gt;&gt;&gt;                          GID[  0]:               fe80:0000:0000:0000:00d2:c910:0001:b6c1<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Related threads in the list:<br>
&gt;&gt;&gt; hxxp://<a href="http://www.open-mpi.org/community/lists/users/2009/07/9786.php" target="_blank">www.open-mpi.org/community/lists/users/2009/07/9786.php</a><br>
&gt;&gt;&gt; hxxp://<a href="http://www.open-mpi.org/community/lists/users/2009/08/10456.php" target="_blank">www.open-mpi.org/community/lists/users/2009/08/10456.php</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; hxxp://<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; hxxp://<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
<br>
</div></div></blockquote></div><br><br>
</div>
