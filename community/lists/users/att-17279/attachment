<div>&gt;&gt; Are you overwhelming the receiver with short, unexpected messages such that MPI keeps mallocing &gt;&gt; and mallocing and mallocing in an attempt to eagerly receive all the messages?  I ask because Open &gt;&gt; MPI only eagerly sends short messages -- long messages are queued up at the sender and not</div>
<div>&gt;&gt; actually transferred until the receiver starts to receive (aka a &quot;rendezvous protocol&quot;).<br></div><div><br></div><div><span class="Apple-style-span" style="font-family: arial, sans-serif; font-size: 13px; background-color: rgb(255, 255, 255); ">This is probably what is happening. In general, my processes send a massive number of short messages and it is </span>overwhelming the receivers. As I have some stage of computation (processes) much slower than others, the first ones can not handle the incoming messages in the same rate they are delivered to them.</div>
<div><br></div><div>&gt;&gt; Are you sure that you don&#39;t have some other kind of memory error in your application?</div><div><br></div><div>I have checked and there are not memory problems within the application.</div>
<div><br></div><div>&gt;&gt; FWIW, you can use MPI_SSEND to do a &quot;synchronous&quot; send, which means that it won&#39;t complete &gt;&gt; until the receiver has started to receive the message.  This may slow your sender down dramatically, &gt;&gt; however.  If it slows down your sender too much, you may have to implement your own flow control.</div>
<div><br></div><div>MPI_SSEND worked for my application and I did not have the problem, but as you said, it slows the senders. A better solution was implement my own flow control, as suggested. I have implemented a simple credit-based flow control scheme and it solved my problem.</div>
<div><br></div><div>Thanks a lot for the explanation and suggestions.</div><div><div></div></div><div><br></div><div class="gmail_quote">On Tue, Sep 6, 2011 at 9:43 AM, Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">Are you overwhelming the receiver with short, unexpected messages such that MPI keeps mallocing and mallocing and mallocing in an attempt to eagerly receive all the messages?  I ask because Open MPI only eagerly sends short messages -- long messages are queued up at the sender and not actually transferred until the receiver starts to receive (aka a &quot;rendezvous protocol&quot;).<br>

<br>
While that *can* happen, I&#39;d be a little surprised if it did.  Indeed, it would probably take a little while for that to happen (i.e., the time necessary for the receiver to malloc a small amount N times, where N is large enough to exhaust the virtual memory on your machine, coupled with all the time delay to page out all the old memory and page in on-demand as Open MPI scans for new incoming matches... this could be pretty darn slow).  Is that what is happening?<br>

<br>
Are you sure that you don&#39;t have some other kind of memory error in your application?<br>
<br>
FWIW, you can use MPI_SSEND to do a &quot;synchronous&quot; send, which means that it won&#39;t complete until the receiver has started to receive the message.  This may slow your sender down dramatically, however.  If it slows down your sender too much, you may have to implement your own flow control.<br>

<div><div></div><div class="h5"><br>
<br>
On Aug 25, 2011, at 10:58 PM, Rodrigo Oliveira wrote:<br>
<br>
&gt; Hi there,<br>
&gt;<br>
&gt; I am facing some problems in an Open MPI application. Part of the application is composed by a sender and a receiver. The problem is that the sender is so much faster than the receiver, what causes the receiver&#39;s memory to be completely used, aborting the application.<br>

&gt;<br>
&gt; I would like to know if there is a flow control scheme implemented in open mpi or if this issue have to be treated at the user application&#39;s layer. If exists, how it works and how can I use it in my application?<br>

&gt;<br>
&gt; I did some research about this subject, but I did not find a conclusive explanation.<br>
&gt;<br>
&gt; Thanks a lot.<br>
</div></div>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to:<br>
<a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br>

