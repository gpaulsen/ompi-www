<html><head></head><body><div style="color:#000; background-color:#fff; font-family:verdana, helvetica, sans-serif;font-size:16px"><div id="yui_3_16_0_ym19_1_1464079512482_3816"><span id="yui_3_16_0_ym19_1_1464079512482_3975">Yes, Empire does the fluid structure coupling. It couples OpenFoam (fluid analysis) and Abaqus (structural analysis).</span></div><div id="yui_3_16_0_ym19_1_1464079512482_3816"><span><br></span></div><div id="yui_3_16_0_ym19_1_1464079512482_3816"><span id="yui_3_16_0_ym19_1_1464079512482_4494">Does all the software need to have the same MPI architecture in order to communicate ?</span></div><div id="yui_3_16_0_ym19_1_1464079512482_3816"><span><br></span></div><div id="yui_3_16_0_ym19_1_1464079512482_3816"><span>Regards,</span></div><div id="yui_3_16_0_ym19_1_1464079512482_3816"><span>Islem</span></div> <div class="qtdSeparateBR"><br><br></div><div class="yahoo_quoted" style="display: block;"> <div style="font-family: verdana, helvetica, sans-serif; font-size: 16px;"> <div style="font-family: HelveticaNeue, Helvetica Neue, Helvetica, Arial, Lucida Grande, sans-serif; font-size: 16px;"> <div dir="ltr"><font size="2" face="Arial"> Le Mardi 24 mai 2016 1h02, Gilles Gouaillardet &lt;gilles@rist.or.jp&gt; a écrit :<br></font></div>  <br><br> <div class="y_msg_container"><div id="yiv3303829720"><div>
    <div>what do you mean by coupling ?</div>
    <div>does Empire and OpenFoam communicate via MPI ?</div>
    <div>wouldn't it be much easier if you rebuild OpenFoam with mpich or
      intelmpi ?</div>
    <div><br clear="none">
    </div>
    <div>Cheers,</div>
    <div><br clear="none">
    </div>
    <div>Gilles<br clear="none">
    </div>
    <br clear="none">
    <div class="yiv3303829720yqt4197036687" id="yiv3303829720yqt51420"><div class="yiv3303829720moz-cite-prefix">On 5/24/2016 8:44 AM, Megdich Islem
      wrote:<br clear="none">
    </div>
    <blockquote type="cite">
      <div style="color:#000;background-color:#fff;font-family:verdana, helvetica, sans-serif;font-size:16px;">
        <div dir="ltr" id="yiv3303829720yui_3_16_0_ym19_1_1464044638578_8333"><span id="yiv3303829720yui_3_16_0_ym19_1_1464044638578_8330" style="font-family:'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif;font-size:13px;">"Open MPI does not
            work when MPICH or intel MPI are</span><span id="yiv3303829720yui_3_16_0_ym19_1_1464044638578_8332" style="font-family:'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif;font-size:13px;">&nbsp;installed"</span><span></span></div>
        <div dir="ltr" id="yiv3303829720yui_3_16_0_ym19_1_1464044638578_8333"><span style="font-family:'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif;font-size:13px;"><br clear="none">
          </span></div>
        <div dir="ltr" id="yiv3303829720yui_3_16_0_ym19_1_1464044638578_8333"><span id="yiv3303829720yui_3_16_0_ym19_1_1464044638578_8628" style="font-family:'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif;font-size:13px;">Thank you for your
            suggestion. But I need to run OpenFoam and Empire at the
            same time. In fact, Empire couples OpenFoam with another
            software.</span></div>
        <div dir="ltr" id="yiv3303829720yui_3_16_0_ym19_1_1464044638578_8333"><span style="font-family:'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif;font-size:13px;"><br clear="none">
          </span></div>
        <div dir="ltr" id="yiv3303829720yui_3_16_0_ym19_1_1464044638578_8333"><span id="yiv3303829720yui_3_16_0_ym19_1_1464044638578_8690" style="font-family:'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif;font-size:13px;">Is there any
            solution for this case ?</span></div>
        <div dir="ltr" id="yiv3303829720yui_3_16_0_ym19_1_1464044638578_8333"><span style="font-family:'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif;font-size:13px;"><br clear="none">
          </span></div>
        <div dir="ltr" id="yiv3303829720yui_3_16_0_ym19_1_1464044638578_8333"><span style="font-family:'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif;font-size:13px;"><br clear="none">
          </span></div>
        <div dir="ltr" id="yiv3303829720yui_3_16_0_ym19_1_1464044638578_8333"><span style="font-family:'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif;font-size:13px;">Regards,</span></div>
        <div dir="ltr" id="yiv3303829720yui_3_16_0_ym19_1_1464044638578_8333"><span style="font-family:'Helvetica Neue', 'Segoe UI', Helvetica, Arial, 'Lucida Grande', sans-serif;font-size:13px;">Islem</span></div>
        <div class="yiv3303829720qtdSeparateBR"><br clear="none">
          <br clear="none">
        </div>
        <div class="yiv3303829720yahoo_quoted" style="display:block;">
          <div style="font-family:verdana, helvetica, sans-serif;font-size:16px;">
            <div style="font-family:HelveticaNeue, Helvetica Neue, Helvetica, Arial, Lucida Grande, sans-serif;font-size:16px;">
              <div dir="ltr"><font face="Arial" size="2"> Le Lundi 23
                  mai 2016 17h00, <a rel="nofollow" shape="rect" class="yiv3303829720moz-txt-link-rfc2396E" ymailto="mailto:users-request@open-mpi.org" target="_blank" href="mailto:users-request@open-mpi.org">"users-request@open-mpi.org"</a>
                  <a rel="nofollow" shape="rect" class="yiv3303829720moz-txt-link-rfc2396E" ymailto="mailto:users-request@open-mpi.org" target="_blank" href="mailto:users-request@open-mpi.org">&lt;users-request@open-mpi.org&gt;</a> a écrit :<br clear="none">
                </font></div>
              <br clear="none">
              <br clear="none">
              <div class="yiv3303829720y_msg_container">Send users mailing list
                submissions to<br clear="none">
                &nbsp;&nbsp;&nbsp; <a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">users@open-mpi.org</a><br clear="none">
                <br clear="none">
                To subscribe or unsubscribe via the World Wide Web,
                visit<br clear="none">
                &nbsp;&nbsp;&nbsp; <a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br clear="none">
                or, via email, send a message with subject or body
                'help' to<br clear="none">
                &nbsp;&nbsp;&nbsp; <a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">users-request@open-mpi.org</a><br clear="none">
                <br clear="none">
                You can reach the person managing the list at<br clear="none">
                &nbsp;&nbsp;&nbsp; <a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">users-owner@open-mpi.org</a><br clear="none">
                <br clear="none">
                When replying, please edit your Subject line so it is
                more specific<br clear="none">
                than "Re: Contents of users digest..."<br clear="none">
                <br clear="none">
                <br clear="none">
                Today's Topics:<br clear="none">
                <br clear="none">
                &nbsp; 1. Re: Open MPI does not work when MPICH or intel MPI
                are<br clear="none">
                &nbsp; &nbsp; &nbsp; installed (Andy Riebs)<br clear="none">
                &nbsp; 2. segmentation fault for slot-list and
                openmpi-1.10.3rc2<br clear="none">
                &nbsp; &nbsp; &nbsp; (Siegmar Gross)<br clear="none">
                &nbsp; 3. Re: problem about mpirun on two nodes (Jeff Squyres
                (jsquyres))<br clear="none">
                &nbsp; 4. Re: Open MPI does not work when MPICH or intel MPI
                are<br clear="none">
                &nbsp; &nbsp; &nbsp; installed (Gilles Gouaillardet)<br clear="none">
                &nbsp; 5. Re: segmentation fault for slot-list and&nbsp;&nbsp;&nbsp;
                openmpi-1.10.3rc2<br clear="none">
                &nbsp; &nbsp; &nbsp; (Ralph Castain)<br clear="none">
                &nbsp; 6. mpirun java (Claudio Stamile)<br clear="none">
                <br clear="none">
                <br clear="none">
----------------------------------------------------------------------<br clear="none">
                <br clear="none">
                [Message discarded by content filter]<br clear="none">
                ------------------------------<br clear="none">
                <br clear="none">
                Message: 2<br clear="none">
                Date: Mon, 23 May 2016 15:26:52 +0200<br clear="none">
                From: Siegmar Gross &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">siegmar.gross@informatik.hs-fulda.de</a>&gt;<br clear="none">
                To: Open MPI Users &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">users@open-mpi.org</a>&gt;<br clear="none">
                Subject: [OMPI users] segmentation fault for slot-list
                and<br clear="none">
                &nbsp;&nbsp;&nbsp; openmpi-1.10.3rc2<br clear="none">
                Message-ID:<br clear="none">
                &nbsp;&nbsp;&nbsp; &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">241613b1-ada6-292f-eeb9-722fc8fa2f94@informatik.hs-fulda.de</a>&gt;<br clear="none">
                Content-Type: text/plain; charset=utf-8; format=flowed<br clear="none">
                <br clear="none">
                Hi,<br clear="none">
                <br clear="none">
                I installed openmpi-1.10.3rc2 on my "SUSE Linux
                Enterprise Server<br clear="none">
                12 (x86_64)" with Sun C 5.13&nbsp; and gcc-6.1.0.
                Unfortunately I get<br clear="none">
                a segmentation fault for "--slot-list" for one of my
                small programs.<br clear="none">
                <br clear="none">
                <br clear="none">
                loki spawn 119 ompi_info | grep -e "OPAL repo revision:"
                -e "C compiler absolute:"<br clear="none">
                &nbsp; &nbsp; &nbsp; OPAL repo revision: v1.10.2-201-gd23dda8<br clear="none">
                &nbsp; &nbsp; &nbsp; C compiler absolute: /usr/local/gcc-6.1.0/bin/gcc<br clear="none">
                <br clear="none">
                <br clear="none">
                loki spawn 120 mpiexec -np 1 --host
                loki,loki,loki,loki,loki spawn_master<br clear="none">
                <br clear="none">
                Parent process 0 running on loki<br clear="none">
                &nbsp; I create 4 slave processes<br clear="none">
                <br clear="none">
                Parent process 0: tasks in MPI_COMM_WORLD:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp; &nbsp; &nbsp; 1<br clear="none">
                &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; tasks in COMM_CHILD_PROCESSES local
                group:&nbsp; 1<br clear="none">
                &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; tasks in COMM_CHILD_PROCESSES remote
                group: 4<br clear="none">
                <br clear="none">
                Slave process 0 of 4 running on loki<br clear="none">
                Slave process 1 of 4 running on loki<br clear="none">
                Slave process 2 of 4 running on loki<br clear="none">
                spawn_slave 2: argv[0]: spawn_slave<br clear="none">
                Slave process 3 of 4 running on loki<br clear="none">
                spawn_slave 0: argv[0]: spawn_slave<br clear="none">
                spawn_slave 1: argv[0]: spawn_slave<br clear="none">
                spawn_slave 3: argv[0]: spawn_slave<br clear="none">
                <br clear="none">
                <br clear="none">
                <br clear="none">
                <br clear="none">
                loki spawn 121 mpiexec -np 1 --host loki --slot-list
                0:0-5,1:0-5 spawn_master<br clear="none">
                <br clear="none">
                Parent process 0 running on loki<br clear="none">
                &nbsp; I create 4 slave processes<br clear="none">
                <br clear="none">
                [loki:17326] *** Process received signal ***<br clear="none">
                [loki:17326] Signal: Segmentation fault (11)<br clear="none">
                [loki:17326] Signal code: Address not mapped (1)<br clear="none">
                [loki:17326] Failing at address: 0x8<br clear="none">
                [loki:17326] [ 0]
                /lib64/libpthread.so.0(+0xf870)[0x7f4e469b3870]<br clear="none">
                [loki:17326] [ 1] *** An error occurred in MPI_Init<br clear="none">
                *** on a NULL communicator<br clear="none">
                *** MPI_ERRORS_ARE_FATAL (processes in this communicator
                will now abort,<br clear="none">
                ***&nbsp; &nbsp; and potentially your MPI job)<br clear="none">
                [loki:17324] Local abort before MPI_INIT completed
                successfully; not able to <br clear="none">
                aggregate error messages, and not able to guarantee that
                all other processes <br clear="none">
                were killed!<br clear="none">
/usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_proc_self+0x35)[0x7f4e46c165b0]<br clear="none">
                [loki:17326] [ 2] <br clear="none">
/usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_comm_init+0x68b)[0x7f4e46bf5b08]<br clear="none">
                [loki:17326] [ 3] *** An error occurred in MPI_Init<br clear="none">
                *** on a NULL communicator<br clear="none">
                *** MPI_ERRORS_ARE_FATAL (processes in this communicator
                will now abort,<br clear="none">
                ***&nbsp; &nbsp; and potentially your MPI job)<br clear="none">
                [loki:17325] Local abort before MPI_INIT completed
                successfully; not able to <br clear="none">
                aggregate error messages, and not able to guarantee that
                all other processes <br clear="none">
                were killed!<br clear="none">
/usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_mpi_init+0xa90)[0x7f4e46c1be8a]<br clear="none">
                [loki:17326] [ 4] <br clear="none">
/usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(MPI_Init+0x180)[0x7f4e46c5828e]<br clear="none">
                [loki:17326] [ 5] spawn_slave[0x40097e]<br clear="none">
                [loki:17326] [ 6]
                /lib64/libc.so.6(__libc_start_main+0xf5)[0x7f4e4661db05]<br clear="none">
                [loki:17326] [ 7] spawn_slave[0x400a54]<br clear="none">
                [loki:17326] *** End of error message ***<br clear="none">
                -------------------------------------------------------<br clear="none">
                Child job 2 terminated normally, but 1 process returned<br clear="none">
                a non-zero exit code.. Per user-direction, the job has
                been aborted.<br clear="none">
                -------------------------------------------------------<br clear="none">
--------------------------------------------------------------------------<br clear="none">
                mpiexec detected that one or more processes exited with
                non-zero status, thus <br clear="none">
                causing<br clear="none">
                the job to be terminated. The first process to do so
                was:<br clear="none">
                <br clear="none">
                &nbsp; Process name: [[56340,2],0]<br clear="none">
                &nbsp; Exit code:&nbsp; &nbsp; 1<br clear="none">
--------------------------------------------------------------------------<br clear="none">
                loki spawn 122<br clear="none">
                <br clear="none">
                <br clear="none">
                <br clear="none">
                <br clear="none">
                I would be grateful, if somebody can fix the problem.
                Thank you<br clear="none">
                very much for any help in advance.<br clear="none">
                <br clear="none">
                <br clear="none">
                Kind regards<br clear="none">
                <br clear="none">
                Siegmar<br clear="none">
                <br clear="none">
                <br clear="none">
                ------------------------------<br clear="none">
                <br clear="none">
                Message: 3<br clear="none">
                Date: Mon, 23 May 2016 14:13:11 +0000<br clear="none">
                From: "Jeff Squyres (jsquyres)" &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href=""></a><a rel="nofollow" shape="rect" class="yiv3303829720moz-txt-link-abbreviated" ymailto="mailto:jsquyres@cisco.com" target="_blank" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br clear="none">
                To: "Open MPI User's List" &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">users@open-mpi.org</a>&gt;<br clear="none">
                Subject: Re: [OMPI users] problem about mpirun on two
                nodes<br clear="none">
                Message-ID: &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">B2033C1D-8AA4-4823-B984-92756DC1E756@cisco.com</a>&gt;<br clear="none">
                Content-Type: text/plain; charset="us-ascii"<br clear="none">
                <br clear="none">
                On May 21, 2016, at 11:31 PM, <a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">douraku@aol.com</a> wrote:<br clear="none">
                &gt; <br clear="none">
                &gt; I encountered a problem about mpirun and SSH when
                using OMPI 1.10.0 compiled with gcc, running on
                centos7.2.<br clear="none">
                &gt; When I execute mpirun on my 2 node cluster, I get
                the following errors pasted below.<br clear="none">
                &gt; <br clear="none">
                &gt; [<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">douraku@master</a> home]$ mpirun -np 12 a.out<br clear="none">
                &gt; Permission denied
                (publickey,gssapi-keyex,gssapi-with-mic).<br clear="none">
                <br clear="none">
                This is the key right here: you got a permission denied
                error when you (assumedly) tried to execute on the
                remote server.<br clear="none">
                <br clear="none">
                Triple check your ssh settings to ensure that you can
                run on the remote server(s) without a password or
                interactive passphrase entry.<br clear="none">
                <br clear="none">
                -- <br clear="none">
                Jeff Squyres<br clear="none">
                <a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">jsquyres@cisco.com</a><br clear="none">
                For corporate legal information go to: <a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href=""></a><a rel="nofollow" shape="rect" class="yiv3303829720moz-txt-link-freetext" target="_blank" href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br clear="none">
                <br clear="none">
                <br clear="none">
                <br clear="none">
                ------------------------------<br clear="none">
                <br clear="none">
                Message: 4<br clear="none">
                Date: Mon, 23 May 2016 23:31:30 +0900<br clear="none">
                From: Gilles Gouaillardet &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">gilles.gouaillardet@gmail.com</a>&gt;<br clear="none">
                To: Open MPI Users &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">users@open-mpi.org</a>&gt;<br clear="none">
                Subject: Re: [OMPI users] Open MPI does not work when
                MPICH or intel<br clear="none">
                &nbsp;&nbsp;&nbsp; MPI are&nbsp;&nbsp;&nbsp; installed<br clear="none">
                Message-ID:<br clear="none">
                &nbsp;&nbsp;&nbsp; &lt;CAAkFZ5u86Q0ev=ospehnKvd0kumYBoeMD8WF=J+<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href=""></a><a rel="nofollow" shape="rect" class="yiv3303829720moz-txt-link-abbreviated" ymailto="mailto:TaDUH3xYecQ@mail.gmail.com" target="_blank" href="mailto:TaDUH3xYecQ@mail.gmail.com">TaDUH3xYecQ@mail.gmail.com</a>&gt;<br clear="none">
                Content-Type: text/plain; charset="utf-8"<br clear="none">
                <br clear="none">
                modules are way more friendly that manually setting and
                exporting your<br clear="none">
                environment.<br clear="none">
                the issue here is you are setting your environment in
                your .bashrc, and<br clear="none">
                that cannot work if your account is used with various
                MPI implementations.<br clear="none">
                (unless your .bashrc checks a third party variable to
                select the<br clear="none">
                appropriate mpi, in this case, simply extend the logic
                to select openmpi)<br clear="none">
                <br clear="none">
                if you configure'd with
                --enable-mpirun-prefix-by-default, you should not<br clear="none">
                need anything in your environment.<br clear="none">
                <br clear="none">
                Cheers,<br clear="none">
                <br clear="none">
                Gilles<br clear="none">
                <br clear="none">
                On Monday, May 23, 2016, Andy Riebs &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href=""></a><a rel="nofollow" shape="rect" class="yiv3303829720moz-txt-link-abbreviated" ymailto="mailto:andy.riebs@hpe.com" target="_blank" href="mailto:andy.riebs@hpe.com">andy.riebs@hpe.com</a>&gt;
                wrote:<br clear="none">
                <br clear="none">
                &gt; Hi,<br clear="none">
                &gt;<br clear="none">
                &gt; The short answer: Environment module files are
                probably the best solution<br clear="none">
                &gt; for your problem.<br clear="none">
                &gt;<br clear="none">
                &gt; The long answer: See<br clear="none">
                &gt; &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">http://www.admin-magazine.com/HPC/Articles/Environment-Modules</a>&gt;<br clear="none">
                &gt; &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">http://www.admin-magazine.com/HPC/Articles/Environment-Modules</a>&gt;,
                which<br clear="none">
                &gt; pretty much addresses your question.<br clear="none">
                &gt;<br clear="none">
                &gt; Andy<br clear="none">
                &gt;<br clear="none">
                &gt; On 05/23/2016 07:40 AM, Megdich Islem wrote:<br clear="none">
                &gt;<br clear="none">
                &gt; Hi,<br clear="none">
                &gt;<br clear="none">
                &gt; I am using 2 software, one is called Open Foam and
                the other called EMPIRE<br clear="none">
                &gt; that need to run together at the same time.<br clear="none">
                &gt; Open Foam uses&nbsp; Open MPI implementation and EMPIRE
                uses either MPICH or<br clear="none">
                &gt; intel mpi.<br clear="none">
                &gt; The version of Open MPI that comes with Open Foam
                is 1.6.5.<br clear="none">
                &gt; I am using Intel (R) MPI Library for linux * OS,
                version 5.1.3 and MPICH<br clear="none">
                &gt; 3.0.4.<br clear="none">
                &gt;<br clear="none">
                &gt; My problem is when I have the environment variables
                of&nbsp; either mpich or<br clear="none">
                &gt; Intel MPI&nbsp; sourced to bashrc, I fail to run a case
                of Open Foam with<br clear="none">
                &gt; parallel processing ( You find attached a picture
                of the error I got )<br clear="none">
                &gt; This is an example of a command line I use to run
                Open Foam<br clear="none">
                &gt; mpirun -np 4 interFoam -parallel<br clear="none">
                &gt;<br clear="none">
                &gt; Once I keep the environment variable of OpenFoam
                only, the parallel<br clear="none">
                &gt; processing works without any problem, so I won't be
                able to run EMPIRE.<br clear="none">
                &gt;<br clear="none">
                &gt; I am sourcing the environment variables in this
                way:<br clear="none">
                &gt;<br clear="none">
                &gt; For Open Foam:<br clear="none">
                &gt; source /opt/openfoam30/etc/bashrc<br clear="none">
                &gt;<br clear="none">
                &gt; For MPICH 3.0.4<br clear="none">
                &gt;<br clear="none">
                &gt; export PATH=/home/islem/Desktop/mpich/bin:$PATH<br clear="none">
                &gt; export
                LD_LIBRARY_PATH="/home/islem/Desktop/mpich/lib/:$LD_LIBRARY_PATH"<br clear="none">
                &gt; export MPICH_F90=gfortran<br clear="none">
                &gt; export MPICH_CC=/opt/intel/bin/icc<br clear="none">
                &gt; export MPICH_CXX=/opt/intel/bin/icpc<br clear="none">
                &gt; export
                MPICH-LINK_CXX="-L/home/islem/Desktop/mpich/lib/
                -Wl,-rpath<br clear="none">
                &gt; -Wl,/home/islem/Desktop/mpich/lib -lmpichcxx
                -lmpich -lopa -lmpl -lrt<br clear="none">
                &gt; -lpthread"<br clear="none">
                &gt;<br clear="none">
                &gt; For intel<br clear="none">
                &gt;<br clear="none">
                &gt; export PATH=$PATH:/opt/intel/bin/<br clear="none">
                &gt;
                LD_LIBRARY_PATH="/opt/intel/lib/intel64:$LD_LIBRARY_PATH"<br clear="none">
                &gt; export LD_LIBRARY_PATH<br clear="none">
                &gt; source<br clear="none">
                &gt;
/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/intel64/bin/mpivars.sh<br clear="none">
                &gt; intel64<br clear="none">
                &gt;<br clear="none">
                &gt; If Only Open Foam is sourced, mpirun --version
                gives OPEN MPI (1.6.5)<br clear="none">
                &gt; If Open Foam and MPICH are sourced, mpirun
                --version gives mpich 3.0.1<br clear="none">
                &gt; If Open Foam and intel MPI are sourced, mpirun
                --version gives intel (R)<br clear="none">
                &gt; MPI libarary for linux, version 5.1.3<br clear="none">
                &gt;<br clear="none">
                &gt; My question is why I can't have two MPI
                implementation installed and<br clear="none">
                &gt; sourced together. How can I solve the problem ?<br clear="none">
                &gt;<br clear="none">
                &gt; Regards,<br clear="none">
                &gt; Islem Megdiche<br clear="none">
                &gt;<br clear="none">
                &gt;<br clear="none">
                &gt;<br clear="none">
                &gt;<br clear="none">
                &gt;<br clear="none">
                &gt;<br clear="none">
                &gt; _______________________________________________<br clear="none">
                &gt; users mailing <a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">listusers@open-mpi.org</a>
                &lt;<a rel="nofollow" shape="rect" class="yiv3303829720moz-txt-link-freetext" href="">javascript:_e(%7B%7D,'cvml</a>','<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href=""></a><a rel="nofollow" shape="rect" class="yiv3303829720moz-txt-link-abbreviated" ymailto="mailto:users@open-mpi.org" target="_blank" href="mailto:users@open-mpi.org">users@open-mpi.org</a>');&gt;<br clear="none">
                &gt; Subscription: <a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br clear="none">
                &gt; Link to this post: <a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">http://www.open-mpi.org/community/lists/users/2016/05/29279.php</a><br clear="none">
                &gt;<br clear="none">
                &gt;<br clear="none">
                &gt;<br clear="none">
                -------------- next part --------------<br clear="none">
                HTML attachment scrubbed and removed<br clear="none">
                <br clear="none">
                ------------------------------<br clear="none">
                <br clear="none">
                Message: 5<br clear="none">
                Date: Mon, 23 May 2016 08:45:53 -0700<br clear="none">
                From: Ralph Castain &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">rhc@open-mpi.org</a>&gt;<br clear="none">
                To: Open MPI Users &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">users@open-mpi.org</a>&gt;<br clear="none">
                Subject: Re: [OMPI users] segmentation fault for
                slot-list and<br clear="none">
                &nbsp;&nbsp;&nbsp; openmpi-1.10.3rc2<br clear="none">
                Message-ID: &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">73195D72-CEA7-4AFC-9527-8F725C8B1FA1@open-mpi.org</a>&gt;<br clear="none">
                Content-Type: text/plain; charset="utf-8"<br clear="none">
                <br clear="none">
                I cannot replicate the problem - both scenarios work
                fine for me. I?m not convinced your test code is
                correct, however, as you call Comm_free the
                inter-communicator but didn?t call Comm_disconnect.
                Checkout the attached for a correct code and see if it
                works for you.<br clear="none">
                <br clear="none">
                FWIW: I don?t know how many cores you have on your
                sockets, but if you have 6 cores/socket, then your
                slot-list is equivalent to ??bind-to none? as the
                slot-list applies to every process being launched<br clear="none">
                <br clear="none">
                -------------- next part --------------<br clear="none">
                A non-text attachment was scrubbed...<br clear="none">
                Name: simple_spawn.c<br clear="none">
                Type: application/octet-stream<br clear="none">
                Size: 1926 bytes<br clear="none">
                Desc: not available<br clear="none">
                URL: &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">http://www.open-mpi.org/MailArchives/users/attachments/20160523/7554b3ec/attachment.obj</a>&gt;<br clear="none">
                -------------- next part --------------<br clear="none">
                <br clear="none">
                <br clear="none">
                &gt; On May 23, 2016, at 6:26 AM, Siegmar Gross &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href=""></a><a rel="nofollow" shape="rect" class="yiv3303829720moz-txt-link-abbreviated" ymailto="mailto:Siegmar.Gross@informatik.hs-fulda.de" target="_blank" href="mailto:Siegmar.Gross@informatik.hs-fulda.de">Siegmar.Gross@informatik.hs-fulda.de</a>&gt;
                wrote:<br clear="none">
                &gt; <br clear="none">
                &gt; Hi,<br clear="none">
                &gt; <br clear="none">
                &gt; I installed openmpi-1.10.3rc2 on my "SUSE Linux
                Enterprise Server<br clear="none">
                &gt; 12 (x86_64)" with Sun C 5.13&nbsp; and gcc-6.1.0.
                Unfortunately I get<br clear="none">
                &gt; a segmentation fault for "--slot-list" for one of
                my small programs.<br clear="none">
                &gt; <br clear="none">
                &gt; <br clear="none">
                &gt; loki spawn 119 ompi_info | grep -e "OPAL repo
                revision:" -e "C compiler absolute:"<br clear="none">
                &gt;&nbsp; &nbsp; &nbsp; OPAL repo revision: v1.10.2-201-gd23dda8<br clear="none">
                &gt;&nbsp; &nbsp; C compiler absolute:
                /usr/local/gcc-6.1.0/bin/gcc<br clear="none">
                &gt; <br clear="none">
                &gt; <br clear="none">
                &gt; loki spawn 120 mpiexec -np 1 --host
                loki,loki,loki,loki,loki spawn_master<br clear="none">
                &gt; <br clear="none">
                &gt; Parent process 0 running on loki<br clear="none">
                &gt;&nbsp; I create 4 slave processes<br clear="none">
                &gt; <br clear="none">
                &gt; Parent process 0: tasks in MPI_COMM_WORLD:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1<br clear="none">
                &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; tasks in COMM_CHILD_PROCESSES
                local group:&nbsp; 1<br clear="none">
                &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; tasks in COMM_CHILD_PROCESSES
                remote group: 4<br clear="none">
                &gt; <br clear="none">
                &gt; Slave process 0 of 4 running on loki<br clear="none">
                &gt; Slave process 1 of 4 running on loki<br clear="none">
                &gt; Slave process 2 of 4 running on loki<br clear="none">
                &gt; spawn_slave 2: argv[0]: spawn_slave<br clear="none">
                &gt; Slave process 3 of 4 running on loki<br clear="none">
                &gt; spawn_slave 0: argv[0]: spawn_slave<br clear="none">
                &gt; spawn_slave 1: argv[0]: spawn_slave<br clear="none">
                &gt; spawn_slave 3: argv[0]: spawn_slave<br clear="none">
                &gt; <br clear="none">
                &gt; <br clear="none">
                &gt; <br clear="none">
                &gt; <br clear="none">
                &gt; loki spawn 121 mpiexec -np 1 --host loki
                --slot-list 0:0-5,1:0-5 spawn_master<br clear="none">
                &gt; <br clear="none">
                &gt; Parent process 0 running on loki<br clear="none">
                &gt;&nbsp; I create 4 slave processes<br clear="none">
                &gt; <br clear="none">
                &gt; [loki:17326] *** Process received signal ***<br clear="none">
                &gt; [loki:17326] Signal: Segmentation fault (11)<br clear="none">
                &gt; [loki:17326] Signal code: Address not mapped (1)<br clear="none">
                &gt; [loki:17326] Failing at address: 0x8<br clear="none">
                &gt; [loki:17326] [ 0]
                /lib64/libpthread.so.0(+0xf870)[0x7f4e469b3870]<br clear="none">
                &gt; [loki:17326] [ 1] *** An error occurred in MPI_Init<br clear="none">
                &gt; *** on a NULL communicator<br clear="none">
                &gt; *** MPI_ERRORS_ARE_FATAL (processes in this
                communicator will now abort,<br clear="none">
                &gt; ***&nbsp; &nbsp; and potentially your MPI job)<br clear="none">
                &gt; [loki:17324] Local abort before MPI_INIT completed
                successfully; not able to aggregate error messages, and
                not able to guarantee that all other processes were
                killed!<br clear="none">
                &gt;
/usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_proc_self+0x35)[0x7f4e46c165b0]<br clear="none">
                &gt; [loki:17326] [ 2]
/usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_comm_init+0x68b)[0x7f4e46bf5b08]<br clear="none">
                &gt; [loki:17326] [ 3] *** An error occurred in MPI_Init<br clear="none">
                &gt; *** on a NULL communicator<br clear="none">
                &gt; *** MPI_ERRORS_ARE_FATAL (processes in this
                communicator will now abort,<br clear="none">
                &gt; ***&nbsp; &nbsp; and potentially your MPI job)<br clear="none">
                &gt; [loki:17325] Local abort before MPI_INIT completed
                successfully; not able to aggregate error messages, and
                not able to guarantee that all other processes were
                killed!<br clear="none">
                &gt;
/usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_mpi_init+0xa90)[0x7f4e46c1be8a]<br clear="none">
                &gt; [loki:17326] [ 4]
/usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(MPI_Init+0x180)[0x7f4e46c5828e]<br clear="none">
                &gt; [loki:17326] [ 5] spawn_slave[0x40097e]<br clear="none">
                &gt; [loki:17326] [ 6]
                /lib64/libc.so.6(__libc_start_main+0xf5)[0x7f4e4661db05]<br clear="none">
                &gt; [loki:17326] [ 7] spawn_slave[0x400a54]<br clear="none">
                &gt; [loki:17326] *** End of error message ***<br clear="none">
                &gt;
                -------------------------------------------------------<br clear="none">
                &gt; Child job 2 terminated normally, but 1 process
                returned<br clear="none">
                &gt; a non-zero exit code.. Per user-direction, the job
                has been aborted.<br clear="none">
                &gt;
                -------------------------------------------------------<br clear="none">
                &gt;
--------------------------------------------------------------------------<br clear="none">
                &gt; mpiexec detected that one or more processes exited
                with non-zero status, thus causing<br clear="none">
                &gt; the job to be terminated. The first process to do
                so was:<br clear="none">
                &gt; <br clear="none">
                &gt;&nbsp; Process name: [[56340,2],0]<br clear="none">
                &gt;&nbsp; Exit code:&nbsp; &nbsp; 1<br clear="none">
                &gt;
--------------------------------------------------------------------------<br clear="none">
                &gt; loki spawn 122<br clear="none">
                &gt; <br clear="none">
                &gt; <br clear="none">
                &gt; <br clear="none">
                &gt; <br clear="none">
                &gt; I would be grateful, if somebody can fix the
                problem. Thank you<br clear="none">
                &gt; very much for any help in advance.<br clear="none">
                &gt; <br clear="none">
                &gt; <br clear="none">
                &gt; Kind regards<br clear="none">
                &gt; <br clear="none">
                &gt; Siegmar<br clear="none">
                &gt; _______________________________________________<br clear="none">
                &gt; users mailing list<br clear="none">
                &gt; <a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">users@open-mpi.org</a><br clear="none">
                &gt; Subscription: <a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br clear="none">
                &gt; Link to this post: <a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">http://www.open-mpi.org/community/lists/users/2016/05/29281.php</a><br clear="none">
                <br clear="none">
                <br clear="none">
                ------------------------------<br clear="none">
                <br clear="none">
                Message: 6<br clear="none">
                Date: Mon, 23 May 2016 17:47:53 +0200<br clear="none">
                From: Claudio Stamile &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">claudiostamile@gmail.com</a>&gt;<br clear="none">
                To: <a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">users@open-mpi.org</a><br clear="none">
                Subject: [OMPI users] mpirun java<br clear="none">
                Message-ID:<br clear="none">
                &nbsp;&nbsp;&nbsp; &lt;<a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">CAAdD79zz1wAonmr5hOd3Jp51QEDUhmP5WW8Tp7EuJLDfsHeFxw@mail.gmail.com</a>&gt;<br clear="none">
                Content-Type: text/plain; charset="utf-8"<br clear="none">
                <br clear="none">
                Dear all,<br clear="none">
                <br clear="none">
                I'm using openmpi for Java.<br clear="none">
                I've a problem when I try to use more option parameters
                in my java command.<br clear="none">
                More in detail I run mpirun as follow:<br clear="none">
                <br clear="none">
                mpirun -n 5 java -cp path1:path2
                -Djava.library.path=pathLibs<br clear="none">
                classification.MyClass<br clear="none">
                <br clear="none">
                It seems that the option "-Djava.library.path" is
                ignored when i execute<br clear="none">
                the command.<br clear="none">
                <br clear="none">
                Is it normal ?<br clear="none">
                <br clear="none">
                Do you know how to solve this problem ?<br clear="none">
                <br clear="none">
                Thank you.<br clear="none">
                <br clear="none">
                Best,<br clear="none">
                Claudio<br clear="none">
                <br clear="none">
                -- <br clear="none">
                C.<br clear="none">
                -------------- next part --------------<br clear="none">
                HTML attachment scrubbed and removed<br clear="none">
                <br clear="none">
                ------------------------------<br clear="none">
                <br clear="none">
                Subject: Digest Footer<br clear="none">
                <br clear="none">
                _______________________________________________<br clear="none">
                users mailing list<br clear="none">
                <a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">users@open-mpi.org</a><br clear="none">
                <a rel="nofollow" shape="rect" class="yiv3303829720removed-link" href="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br clear="none">
                <br clear="none">
                ------------------------------<br clear="none">
                <br clear="none">
                End of users Digest, Vol 3510, Issue 2<br clear="none">
                **************************************<br clear="none">
                <br clear="none">
                <br clear="none">
              </div>
            </div>
          </div>
        </div>
      </div>
      <br clear="none">
      <fieldset class="yiv3303829720mimeAttachmentHeader"></fieldset>
      <br clear="none">
      <pre>_______________________________________________
users mailing list
<a rel="nofollow" shape="rect" class="yiv3303829720moz-txt-link-abbreviated" ymailto="mailto:users@open-mpi.org" target="_blank" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Subscription: <a rel="nofollow" shape="rect" class="yiv3303829720moz-txt-link-freetext" target="_blank" href="https://www.open-mpi.org/mailman/listinfo.cgi/users">https://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a rel="nofollow" shape="rect" class="yiv3303829720moz-txt-link-freetext" target="_blank" href="http://www.open-mpi.org/community/lists/users/2016/05/29295.php">http://www.open-mpi.org/community/lists/users/2016/05/29295.php</a></pre>
    </blockquote></div>
    <br clear="none">
  </div></div><br><br></div>  </div> </div>  </div></div></body></html>
