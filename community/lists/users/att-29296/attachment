<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    <p>what do you mean by coupling ?</p>
    <p>does Empire and OpenFoam communicate via MPI ?</p>
    <p>wouldn't it be much easier if you rebuild OpenFoam with mpich or
      intelmpi ?</p>
    <p><br>
    </p>
    <p>Cheers,</p>
    <p><br>
    </p>
    <p>Gilles<br>
    </p>
    <br>
    <div class="moz-cite-prefix">On 5/24/2016 8:44 AM, Megdich Islem
      wrote:<br>
    </div>
    <blockquote
cite="mid:1659377143.2420437.1464047087622.JavaMail.yahoo@mail.yahoo.com"
      type="cite">
      <div style="color:#000; background-color:#fff;
        font-family:verdana, helvetica, sans-serif;font-size:16px">
        <div dir="ltr" id="yui_3_16_0_ym19_1_1464044638578_8333"><span
            style="font-family: 'Helvetica Neue', 'Segoe UI', Helvetica,
            Arial, 'Lucida Grande', sans-serif; font-size: 13px;"
            id="yui_3_16_0_ym19_1_1464044638578_8330">"Open MPI does not
            work when MPICH or intel MPI are</span><span
            style="font-family: 'Helvetica Neue', 'Segoe UI', Helvetica,
            Arial, 'Lucida Grande', sans-serif; font-size: 13px;"
            id="yui_3_16_0_ym19_1_1464044638578_8332"> installed"</span><span></span></div>
        <div dir="ltr" id="yui_3_16_0_ym19_1_1464044638578_8333"><span
            style="font-family: 'Helvetica Neue', 'Segoe UI', Helvetica,
            Arial, 'Lucida Grande', sans-serif; font-size: 13px;"><br>
          </span></div>
        <div dir="ltr" id="yui_3_16_0_ym19_1_1464044638578_8333"><span
            style="font-family: 'Helvetica Neue', 'Segoe UI', Helvetica,
            Arial, 'Lucida Grande', sans-serif; font-size: 13px;"
            id="yui_3_16_0_ym19_1_1464044638578_8628">Thank you for your
            suggestion. But I need to run OpenFoam and Empire at the
            same time. In fact, Empire couples OpenFoam with another
            software.</span></div>
        <div dir="ltr" id="yui_3_16_0_ym19_1_1464044638578_8333"><span
            style="font-family: 'Helvetica Neue', 'Segoe UI', Helvetica,
            Arial, 'Lucida Grande', sans-serif; font-size: 13px;"><br>
          </span></div>
        <div dir="ltr" id="yui_3_16_0_ym19_1_1464044638578_8333"><span
            style="font-family: 'Helvetica Neue', 'Segoe UI', Helvetica,
            Arial, 'Lucida Grande', sans-serif; font-size: 13px;"
            id="yui_3_16_0_ym19_1_1464044638578_8690">Is there any
            solution for this case ?</span></div>
        <div dir="ltr" id="yui_3_16_0_ym19_1_1464044638578_8333"><span
            style="font-family: 'Helvetica Neue', 'Segoe UI', Helvetica,
            Arial, 'Lucida Grande', sans-serif; font-size: 13px;"><br>
          </span></div>
        <div dir="ltr" id="yui_3_16_0_ym19_1_1464044638578_8333"><span
            style="font-family: 'Helvetica Neue', 'Segoe UI', Helvetica,
            Arial, 'Lucida Grande', sans-serif; font-size: 13px;"><br>
          </span></div>
        <div dir="ltr" id="yui_3_16_0_ym19_1_1464044638578_8333"><span
            style="font-family: 'Helvetica Neue', 'Segoe UI', Helvetica,
            Arial, 'Lucida Grande', sans-serif; font-size: 13px;">Regards,</span></div>
        <div dir="ltr" id="yui_3_16_0_ym19_1_1464044638578_8333"><span
            style="font-family: 'Helvetica Neue', 'Segoe UI', Helvetica,
            Arial, 'Lucida Grande', sans-serif; font-size: 13px;">Islem</span></div>
        <div class="qtdSeparateBR"><br>
          <br>
        </div>
        <div class="yahoo_quoted" style="display: block;">
          <div style="font-family: verdana, helvetica, sans-serif;
            font-size: 16px;">
            <div style="font-family: HelveticaNeue, Helvetica Neue,
              Helvetica, Arial, Lucida Grande, sans-serif; font-size:
              16px;">
              <div dir="ltr"><font face="Arial" size="2"> Le Lundi 23
                  mai 2016 17h00, <a class="moz-txt-link-rfc2396E" href="mailto:users-request@open-mpi.org">"users-request@open-mpi.org"</a>
                  <a class="moz-txt-link-rfc2396E" href="mailto:users-request@open-mpi.org">&lt;users-request@open-mpi.org&gt;</a> a écrit :<br>
                </font></div>
              <br>
              <br>
              <div class="y_msg_container">Send users mailing list
                submissions to<br>
                    <a moz-do-not-send="true"
                  ymailto="mailto:users@open-mpi.org"
                  class="removed-link" href="">users@open-mpi.org</a><br>
                <br>
                To subscribe or unsubscribe via the World Wide Web,
                visit<br>
                    <a moz-do-not-send="true" target="_blank"
                  class="removed-link" href="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                or, via email, send a message with subject or body
                'help' to<br>
                    <a moz-do-not-send="true"
                  ymailto="mailto:users-request@open-mpi.org"
                  class="removed-link" href="">users-request@open-mpi.org</a><br>
                <br>
                You can reach the person managing the list at<br>
                    <a moz-do-not-send="true"
                  ymailto="mailto:users-owner@open-mpi.org"
                  class="removed-link" href="">users-owner@open-mpi.org</a><br>
                <br>
                When replying, please edit your Subject line so it is
                more specific<br>
                than "Re: Contents of users digest..."<br>
                <br>
                <br>
                Today's Topics:<br>
                <br>
                  1. Re: Open MPI does not work when MPICH or intel MPI
                are<br>
                      installed (Andy Riebs)<br>
                  2. segmentation fault for slot-list and
                openmpi-1.10.3rc2<br>
                      (Siegmar Gross)<br>
                  3. Re: problem about mpirun on two nodes (Jeff Squyres
                (jsquyres))<br>
                  4. Re: Open MPI does not work when MPICH or intel MPI
                are<br>
                      installed (Gilles Gouaillardet)<br>
                  5. Re: segmentation fault for slot-list and   
                openmpi-1.10.3rc2<br>
                      (Ralph Castain)<br>
                  6. mpirun java (Claudio Stamile)<br>
                <br>
                <br>
----------------------------------------------------------------------<br>
                <br>
                [Message discarded by content filter]<br>
                ------------------------------<br>
                <br>
                Message: 2<br>
                Date: Mon, 23 May 2016 15:26:52 +0200<br>
                From: Siegmar Gross &lt;<a moz-do-not-send="true"
                  ymailto="mailto:siegmar.gross@informatik.hs-fulda.de"
                  class="removed-link" href="">siegmar.gross@informatik.hs-fulda.de</a>&gt;<br>
                To: Open MPI Users &lt;<a moz-do-not-send="true"
                  ymailto="mailto:users@open-mpi.org"
                  class="removed-link" href="">users@open-mpi.org</a>&gt;<br>
                Subject: [OMPI users] segmentation fault for slot-list
                and<br>
                    openmpi-1.10.3rc2<br>
                Message-ID:<br>
                    &lt;<a moz-do-not-send="true"
ymailto="mailto:241613b1-ada6-292f-eeb9-722fc8fa2f94@informatik.hs-fulda.de"
                  class="removed-link" href="">241613b1-ada6-292f-eeb9-722fc8fa2f94@informatik.hs-fulda.de</a>&gt;<br>
                Content-Type: text/plain; charset=utf-8; format=flowed<br>
                <br>
                Hi,<br>
                <br>
                I installed openmpi-1.10.3rc2 on my "SUSE Linux
                Enterprise Server<br>
                12 (x86_64)" with Sun C 5.13  and gcc-6.1.0.
                Unfortunately I get<br>
                a segmentation fault for "--slot-list" for one of my
                small programs.<br>
                <br>
                <br>
                loki spawn 119 ompi_info | grep -e "OPAL repo revision:"
                -e "C compiler absolute:"<br>
                      OPAL repo revision: v1.10.2-201-gd23dda8<br>
                      C compiler absolute: /usr/local/gcc-6.1.0/bin/gcc<br>
                <br>
                <br>
                loki spawn 120 mpiexec -np 1 --host
                loki,loki,loki,loki,loki spawn_master<br>
                <br>
                Parent process 0 running on loki<br>
                  I create 4 slave processes<br>
                <br>
                Parent process 0: tasks in MPI_COMM_WORLD:             
                      1<br>
                                  tasks in COMM_CHILD_PROCESSES local
                group:  1<br>
                                  tasks in COMM_CHILD_PROCESSES remote
                group: 4<br>
                <br>
                Slave process 0 of 4 running on loki<br>
                Slave process 1 of 4 running on loki<br>
                Slave process 2 of 4 running on loki<br>
                spawn_slave 2: argv[0]: spawn_slave<br>
                Slave process 3 of 4 running on loki<br>
                spawn_slave 0: argv[0]: spawn_slave<br>
                spawn_slave 1: argv[0]: spawn_slave<br>
                spawn_slave 3: argv[0]: spawn_slave<br>
                <br>
                <br>
                <br>
                <br>
                loki spawn 121 mpiexec -np 1 --host loki --slot-list
                0:0-5,1:0-5 spawn_master<br>
                <br>
                Parent process 0 running on loki<br>
                  I create 4 slave processes<br>
                <br>
                [loki:17326] *** Process received signal ***<br>
                [loki:17326] Signal: Segmentation fault (11)<br>
                [loki:17326] Signal code: Address not mapped (1)<br>
                [loki:17326] Failing at address: 0x8<br>
                [loki:17326] [ 0]
                /lib64/libpthread.so.0(+0xf870)[0x7f4e469b3870]<br>
                [loki:17326] [ 1] *** An error occurred in MPI_Init<br>
                *** on a NULL communicator<br>
                *** MPI_ERRORS_ARE_FATAL (processes in this communicator
                will now abort,<br>
                ***    and potentially your MPI job)<br>
                [loki:17324] Local abort before MPI_INIT completed
                successfully; not able to <br>
                aggregate error messages, and not able to guarantee that
                all other processes <br>
                were killed!<br>
/usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_proc_self+0x35)[0x7f4e46c165b0]<br>
                [loki:17326] [ 2] <br>
/usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_comm_init+0x68b)[0x7f4e46bf5b08]<br>
                [loki:17326] [ 3] *** An error occurred in MPI_Init<br>
                *** on a NULL communicator<br>
                *** MPI_ERRORS_ARE_FATAL (processes in this communicator
                will now abort,<br>
                ***    and potentially your MPI job)<br>
                [loki:17325] Local abort before MPI_INIT completed
                successfully; not able to <br>
                aggregate error messages, and not able to guarantee that
                all other processes <br>
                were killed!<br>
/usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_mpi_init+0xa90)[0x7f4e46c1be8a]<br>
                [loki:17326] [ 4] <br>
/usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(MPI_Init+0x180)[0x7f4e46c5828e]<br>
                [loki:17326] [ 5] spawn_slave[0x40097e]<br>
                [loki:17326] [ 6]
                /lib64/libc.so.6(__libc_start_main+0xf5)[0x7f4e4661db05]<br>
                [loki:17326] [ 7] spawn_slave[0x400a54]<br>
                [loki:17326] *** End of error message ***<br>
                -------------------------------------------------------<br>
                Child job 2 terminated normally, but 1 process returned<br>
                a non-zero exit code.. Per user-direction, the job has
                been aborted.<br>
                -------------------------------------------------------<br>
--------------------------------------------------------------------------<br>
                mpiexec detected that one or more processes exited with
                non-zero status, thus <br>
                causing<br>
                the job to be terminated. The first process to do so
                was:<br>
                <br>
                  Process name: [[56340,2],0]<br>
                  Exit code:    1<br>
--------------------------------------------------------------------------<br>
                loki spawn 122<br>
                <br>
                <br>
                <br>
                <br>
                I would be grateful, if somebody can fix the problem.
                Thank you<br>
                very much for any help in advance.<br>
                <br>
                <br>
                Kind regards<br>
                <br>
                Siegmar<br>
                <br>
                <br>
                ------------------------------<br>
                <br>
                Message: 3<br>
                Date: Mon, 23 May 2016 14:13:11 +0000<br>
                From: "Jeff Squyres (jsquyres)" &lt;<a
                  moz-do-not-send="true"
                  ymailto="mailto:jsquyres@cisco.com"
                  class="removed-link" href=""><a class="moz-txt-link-abbreviated" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a></a>&gt;<br>
                To: "Open MPI User's List" &lt;<a moz-do-not-send="true"
                  ymailto="mailto:users@open-mpi.org"
                  class="removed-link" href="">users@open-mpi.org</a>&gt;<br>
                Subject: Re: [OMPI users] problem about mpirun on two
                nodes<br>
                Message-ID: &lt;<a moz-do-not-send="true"
                  ymailto="mailto:B2033C1D-8AA4-4823-B984-92756DC1E756@cisco.com"
                  class="removed-link" href="">B2033C1D-8AA4-4823-B984-92756DC1E756@cisco.com</a>&gt;<br>
                Content-Type: text/plain; charset="us-ascii"<br>
                <br>
                On May 21, 2016, at 11:31 PM, <a moz-do-not-send="true"
                  ymailto="mailto:douraku@aol.com" class="removed-link"
                  href="">douraku@aol.com</a> wrote:<br>
                &gt; <br>
                &gt; I encountered a problem about mpirun and SSH when
                using OMPI 1.10.0 compiled with gcc, running on
                centos7.2.<br>
                &gt; When I execute mpirun on my 2 node cluster, I get
                the following errors pasted below.<br>
                &gt; <br>
                &gt; [<a moz-do-not-send="true"
                  ymailto="mailto:douraku@master" class="removed-link"
                  href="">douraku@master</a> home]$ mpirun -np 12 a.out<br>
                &gt; Permission denied
                (publickey,gssapi-keyex,gssapi-with-mic).<br>
                <br>
                This is the key right here: you got a permission denied
                error when you (assumedly) tried to execute on the
                remote server.<br>
                <br>
                Triple check your ssh settings to ensure that you can
                run on the remote server(s) without a password or
                interactive passphrase entry.<br>
                <br>
                -- <br>
                Jeff Squyres<br>
                <a moz-do-not-send="true"
                  ymailto="mailto:jsquyres@cisco.com"
                  class="removed-link" href="">jsquyres@cisco.com</a><br>
                For corporate legal information go to: <a
                  moz-do-not-send="true" target="_blank"
                  class="removed-link" href=""><a class="moz-txt-link-freetext" href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a></a><br>
                <br>
                <br>
                <br>
                ------------------------------<br>
                <br>
                Message: 4<br>
                Date: Mon, 23 May 2016 23:31:30 +0900<br>
                From: Gilles Gouaillardet &lt;<a moz-do-not-send="true"
                  ymailto="mailto:gilles.gouaillardet@gmail.com"
                  class="removed-link" href="">gilles.gouaillardet@gmail.com</a>&gt;<br>
                To: Open MPI Users &lt;<a moz-do-not-send="true"
                  ymailto="mailto:users@open-mpi.org"
                  class="removed-link" href="">users@open-mpi.org</a>&gt;<br>
                Subject: Re: [OMPI users] Open MPI does not work when
                MPICH or intel<br>
                    MPI are    installed<br>
                Message-ID:<br>
                    &lt;CAAkFZ5u86Q0ev=ospehnKvd0kumYBoeMD8WF=J+<a
                  moz-do-not-send="true"
                  ymailto="mailto:TaDUH3xYecQ@mail.gmail.com"
                  class="removed-link" href=""><a class="moz-txt-link-abbreviated" href="mailto:TaDUH3xYecQ@mail.gmail.com">TaDUH3xYecQ@mail.gmail.com</a></a>&gt;<br>
                Content-Type: text/plain; charset="utf-8"<br>
                <br>
                modules are way more friendly that manually setting and
                exporting your<br>
                environment.<br>
                the issue here is you are setting your environment in
                your .bashrc, and<br>
                that cannot work if your account is used with various
                MPI implementations.<br>
                (unless your .bashrc checks a third party variable to
                select the<br>
                appropriate mpi, in this case, simply extend the logic
                to select openmpi)<br>
                <br>
                if you configure'd with
                --enable-mpirun-prefix-by-default, you should not<br>
                need anything in your environment.<br>
                <br>
                Cheers,<br>
                <br>
                Gilles<br>
                <br>
                On Monday, May 23, 2016, Andy Riebs &lt;<a
                  moz-do-not-send="true"
                  ymailto="mailto:andy.riebs@hpe.com"
                  class="removed-link" href=""><a class="moz-txt-link-abbreviated" href="mailto:andy.riebs@hpe.com">andy.riebs@hpe.com</a></a>&gt;
                wrote:<br>
                <br>
                &gt; Hi,<br>
                &gt;<br>
                &gt; The short answer: Environment module files are
                probably the best solution<br>
                &gt; for your problem.<br>
                &gt;<br>
                &gt; The long answer: See<br>
                &gt; &lt;<a moz-do-not-send="true" target="_blank"
                  class="removed-link" href="">http://www.admin-magazine.com/HPC/Articles/Environment-Modules</a>&gt;<br>
                &gt; &lt;<a moz-do-not-send="true" target="_blank"
                  class="removed-link" href="">http://www.admin-magazine.com/HPC/Articles/Environment-Modules</a>&gt;,
                which<br>
                &gt; pretty much addresses your question.<br>
                &gt;<br>
                &gt; Andy<br>
                &gt;<br>
                &gt; On 05/23/2016 07:40 AM, Megdich Islem wrote:<br>
                &gt;<br>
                &gt; Hi,<br>
                &gt;<br>
                &gt; I am using 2 software, one is called Open Foam and
                the other called EMPIRE<br>
                &gt; that need to run together at the same time.<br>
                &gt; Open Foam uses  Open MPI implementation and EMPIRE
                uses either MPICH or<br>
                &gt; intel mpi.<br>
                &gt; The version of Open MPI that comes with Open Foam
                is 1.6.5.<br>
                &gt; I am using Intel (R) MPI Library for linux * OS,
                version 5.1.3 and MPICH<br>
                &gt; 3.0.4.<br>
                &gt;<br>
                &gt; My problem is when I have the environment variables
                of  either mpich or<br>
                &gt; Intel MPI  sourced to bashrc, I fail to run a case
                of Open Foam with<br>
                &gt; parallel processing ( You find attached a picture
                of the error I got )<br>
                &gt; This is an example of a command line I use to run
                Open Foam<br>
                &gt; mpirun -np 4 interFoam -parallel<br>
                &gt;<br>
                &gt; Once I keep the environment variable of OpenFoam
                only, the parallel<br>
                &gt; processing works without any problem, so I won't be
                able to run EMPIRE.<br>
                &gt;<br>
                &gt; I am sourcing the environment variables in this
                way:<br>
                &gt;<br>
                &gt; For Open Foam:<br>
                &gt; source /opt/openfoam30/etc/bashrc<br>
                &gt;<br>
                &gt; For MPICH 3.0.4<br>
                &gt;<br>
                &gt; export PATH=/home/islem/Desktop/mpich/bin:$PATH<br>
                &gt; export
                LD_LIBRARY_PATH="/home/islem/Desktop/mpich/lib/:$LD_LIBRARY_PATH"<br>
                &gt; export MPICH_F90=gfortran<br>
                &gt; export MPICH_CC=/opt/intel/bin/icc<br>
                &gt; export MPICH_CXX=/opt/intel/bin/icpc<br>
                &gt; export
                MPICH-LINK_CXX="-L/home/islem/Desktop/mpich/lib/
                -Wl,-rpath<br>
                &gt; -Wl,/home/islem/Desktop/mpich/lib -lmpichcxx
                -lmpich -lopa -lmpl -lrt<br>
                &gt; -lpthread"<br>
                &gt;<br>
                &gt; For intel<br>
                &gt;<br>
                &gt; export PATH=$PATH:/opt/intel/bin/<br>
                &gt;
                LD_LIBRARY_PATH="/opt/intel/lib/intel64:$LD_LIBRARY_PATH"<br>
                &gt; export LD_LIBRARY_PATH<br>
                &gt; source<br>
                &gt;
/opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/intel64/bin/mpivars.sh<br>
                &gt; intel64<br>
                &gt;<br>
                &gt; If Only Open Foam is sourced, mpirun --version
                gives OPEN MPI (1.6.5)<br>
                &gt; If Open Foam and MPICH are sourced, mpirun
                --version gives mpich 3.0.1<br>
                &gt; If Open Foam and intel MPI are sourced, mpirun
                --version gives intel (R)<br>
                &gt; MPI libarary for linux, version 5.1.3<br>
                &gt;<br>
                &gt; My question is why I can't have two MPI
                implementation installed and<br>
                &gt; sourced together. How can I solve the problem ?<br>
                &gt;<br>
                &gt; Regards,<br>
                &gt; Islem Megdiche<br>
                &gt;<br>
                &gt;<br>
                &gt;<br>
                &gt;<br>
                &gt;<br>
                &gt;<br>
                &gt; _______________________________________________<br>
                &gt; users mailing <a moz-do-not-send="true"
                  ymailto="mailto:listusers@open-mpi.org"
                  class="removed-link" href="">listusers@open-mpi.org</a>
                &lt;<a class="moz-txt-link-freetext" href="javascript:_e(%7B%7D,'cvml">javascript:_e(%7B%7D,'cvml</a>','<a
                  moz-do-not-send="true"
                  ymailto="mailto:users@open-mpi.org"
                  class="removed-link" href=""><a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a></a>');&gt;<br>
                &gt; Subscription: <a moz-do-not-send="true"
                  target="_blank" class="removed-link" href="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                &gt; Link to this post: <a moz-do-not-send="true"
                  target="_blank" class="removed-link" href="">http://www.open-mpi.org/community/lists/users/2016/05/29279.php</a><br>
                &gt;<br>
                &gt;<br>
                &gt;<br>
                -------------- next part --------------<br>
                HTML attachment scrubbed and removed<br>
                <br>
                ------------------------------<br>
                <br>
                Message: 5<br>
                Date: Mon, 23 May 2016 08:45:53 -0700<br>
                From: Ralph Castain &lt;<a moz-do-not-send="true"
                  ymailto="mailto:rhc@open-mpi.org" class="removed-link"
                  href="">rhc@open-mpi.org</a>&gt;<br>
                To: Open MPI Users &lt;<a moz-do-not-send="true"
                  ymailto="mailto:users@open-mpi.org"
                  class="removed-link" href="">users@open-mpi.org</a>&gt;<br>
                Subject: Re: [OMPI users] segmentation fault for
                slot-list and<br>
                    openmpi-1.10.3rc2<br>
                Message-ID: &lt;<a moz-do-not-send="true"
                  ymailto="mailto:73195D72-CEA7-4AFC-9527-8F725C8B1FA1@open-mpi.org"
                  class="removed-link" href="">73195D72-CEA7-4AFC-9527-8F725C8B1FA1@open-mpi.org</a>&gt;<br>
                Content-Type: text/plain; charset="utf-8"<br>
                <br>
                I cannot replicate the problem - both scenarios work
                fine for me. I?m not convinced your test code is
                correct, however, as you call Comm_free the
                inter-communicator but didn?t call Comm_disconnect.
                Checkout the attached for a correct code and see if it
                works for you.<br>
                <br>
                FWIW: I don?t know how many cores you have on your
                sockets, but if you have 6 cores/socket, then your
                slot-list is equivalent to ??bind-to none? as the
                slot-list applies to every process being launched<br>
                <br>
                -------------- next part --------------<br>
                A non-text attachment was scrubbed...<br>
                Name: simple_spawn.c<br>
                Type: application/octet-stream<br>
                Size: 1926 bytes<br>
                Desc: not available<br>
                URL: &lt;<a moz-do-not-send="true" target="_blank"
                  class="removed-link" href="">http://www.open-mpi.org/MailArchives/users/attachments/20160523/7554b3ec/attachment.obj</a>&gt;<br>
                -------------- next part --------------<br>
                <br>
                <br>
                &gt; On May 23, 2016, at 6:26 AM, Siegmar Gross &lt;<a
                  moz-do-not-send="true"
                  ymailto="mailto:Siegmar.Gross@informatik.hs-fulda.de"
                  class="removed-link" href=""><a class="moz-txt-link-abbreviated" href="mailto:Siegmar.Gross@informatik.hs-fulda.de">Siegmar.Gross@informatik.hs-fulda.de</a></a>&gt;
                wrote:<br>
                &gt; <br>
                &gt; Hi,<br>
                &gt; <br>
                &gt; I installed openmpi-1.10.3rc2 on my "SUSE Linux
                Enterprise Server<br>
                &gt; 12 (x86_64)" with Sun C 5.13  and gcc-6.1.0.
                Unfortunately I get<br>
                &gt; a segmentation fault for "--slot-list" for one of
                my small programs.<br>
                &gt; <br>
                &gt; <br>
                &gt; loki spawn 119 ompi_info | grep -e "OPAL repo
                revision:" -e "C compiler absolute:"<br>
                &gt;      OPAL repo revision: v1.10.2-201-gd23dda8<br>
                &gt;    C compiler absolute:
                /usr/local/gcc-6.1.0/bin/gcc<br>
                &gt; <br>
                &gt; <br>
                &gt; loki spawn 120 mpiexec -np 1 --host
                loki,loki,loki,loki,loki spawn_master<br>
                &gt; <br>
                &gt; Parent process 0 running on loki<br>
                &gt;  I create 4 slave processes<br>
                &gt; <br>
                &gt; Parent process 0: tasks in MPI_COMM_WORLD:         
                          1<br>
                &gt;                  tasks in COMM_CHILD_PROCESSES
                local group:  1<br>
                &gt;                  tasks in COMM_CHILD_PROCESSES
                remote group: 4<br>
                &gt; <br>
                &gt; Slave process 0 of 4 running on loki<br>
                &gt; Slave process 1 of 4 running on loki<br>
                &gt; Slave process 2 of 4 running on loki<br>
                &gt; spawn_slave 2: argv[0]: spawn_slave<br>
                &gt; Slave process 3 of 4 running on loki<br>
                &gt; spawn_slave 0: argv[0]: spawn_slave<br>
                &gt; spawn_slave 1: argv[0]: spawn_slave<br>
                &gt; spawn_slave 3: argv[0]: spawn_slave<br>
                &gt; <br>
                &gt; <br>
                &gt; <br>
                &gt; <br>
                &gt; loki spawn 121 mpiexec -np 1 --host loki
                --slot-list 0:0-5,1:0-5 spawn_master<br>
                &gt; <br>
                &gt; Parent process 0 running on loki<br>
                &gt;  I create 4 slave processes<br>
                &gt; <br>
                &gt; [loki:17326] *** Process received signal ***<br>
                &gt; [loki:17326] Signal: Segmentation fault (11)<br>
                &gt; [loki:17326] Signal code: Address not mapped (1)<br>
                &gt; [loki:17326] Failing at address: 0x8<br>
                &gt; [loki:17326] [ 0]
                /lib64/libpthread.so.0(+0xf870)[0x7f4e469b3870]<br>
                &gt; [loki:17326] [ 1] *** An error occurred in MPI_Init<br>
                &gt; *** on a NULL communicator<br>
                &gt; *** MPI_ERRORS_ARE_FATAL (processes in this
                communicator will now abort,<br>
                &gt; ***    and potentially your MPI job)<br>
                &gt; [loki:17324] Local abort before MPI_INIT completed
                successfully; not able to aggregate error messages, and
                not able to guarantee that all other processes were
                killed!<br>
                &gt;
/usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_proc_self+0x35)[0x7f4e46c165b0]<br>
                &gt; [loki:17326] [ 2]
/usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_comm_init+0x68b)[0x7f4e46bf5b08]<br>
                &gt; [loki:17326] [ 3] *** An error occurred in MPI_Init<br>
                &gt; *** on a NULL communicator<br>
                &gt; *** MPI_ERRORS_ARE_FATAL (processes in this
                communicator will now abort,<br>
                &gt; ***    and potentially your MPI job)<br>
                &gt; [loki:17325] Local abort before MPI_INIT completed
                successfully; not able to aggregate error messages, and
                not able to guarantee that all other processes were
                killed!<br>
                &gt;
/usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_mpi_init+0xa90)[0x7f4e46c1be8a]<br>
                &gt; [loki:17326] [ 4]
/usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(MPI_Init+0x180)[0x7f4e46c5828e]<br>
                &gt; [loki:17326] [ 5] spawn_slave[0x40097e]<br>
                &gt; [loki:17326] [ 6]
                /lib64/libc.so.6(__libc_start_main+0xf5)[0x7f4e4661db05]<br>
                &gt; [loki:17326] [ 7] spawn_slave[0x400a54]<br>
                &gt; [loki:17326] *** End of error message ***<br>
                &gt;
                -------------------------------------------------------<br>
                &gt; Child job 2 terminated normally, but 1 process
                returned<br>
                &gt; a non-zero exit code.. Per user-direction, the job
                has been aborted.<br>
                &gt;
                -------------------------------------------------------<br>
                &gt;
--------------------------------------------------------------------------<br>
                &gt; mpiexec detected that one or more processes exited
                with non-zero status, thus causing<br>
                &gt; the job to be terminated. The first process to do
                so was:<br>
                &gt; <br>
                &gt;  Process name: [[56340,2],0]<br>
                &gt;  Exit code:    1<br>
                &gt;
--------------------------------------------------------------------------<br>
                &gt; loki spawn 122<br>
                &gt; <br>
                &gt; <br>
                &gt; <br>
                &gt; <br>
                &gt; I would be grateful, if somebody can fix the
                problem. Thank you<br>
                &gt; very much for any help in advance.<br>
                &gt; <br>
                &gt; <br>
                &gt; Kind regards<br>
                &gt; <br>
                &gt; Siegmar<br>
                &gt; _______________________________________________<br>
                &gt; users mailing list<br>
                &gt; <a moz-do-not-send="true"
                  ymailto="mailto:users@open-mpi.org"
                  class="removed-link" href="">users@open-mpi.org</a><br>
                &gt; Subscription: <a moz-do-not-send="true"
                  target="_blank" class="removed-link" href="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                &gt; Link to this post: <a moz-do-not-send="true"
                  target="_blank" class="removed-link" href="">http://www.open-mpi.org/community/lists/users/2016/05/29281.php</a><br>
                <br>
                <br>
                ------------------------------<br>
                <br>
                Message: 6<br>
                Date: Mon, 23 May 2016 17:47:53 +0200<br>
                From: Claudio Stamile &lt;<a moz-do-not-send="true"
                  ymailto="mailto:claudiostamile@gmail.com"
                  class="removed-link" href="">claudiostamile@gmail.com</a>&gt;<br>
                To: <a moz-do-not-send="true"
                  ymailto="mailto:users@open-mpi.org"
                  class="removed-link" href="">users@open-mpi.org</a><br>
                Subject: [OMPI users] mpirun java<br>
                Message-ID:<br>
                    &lt;<a moz-do-not-send="true"
ymailto="mailto:CAAdD79zz1wAonmr5hOd3Jp51QEDUhmP5WW8Tp7EuJLDfsHeFxw@mail.gmail.com"
                  class="removed-link" href="">CAAdD79zz1wAonmr5hOd3Jp51QEDUhmP5WW8Tp7EuJLDfsHeFxw@mail.gmail.com</a>&gt;<br>
                Content-Type: text/plain; charset="utf-8"<br>
                <br>
                Dear all,<br>
                <br>
                I'm using openmpi for Java.<br>
                I've a problem when I try to use more option parameters
                in my java command.<br>
                More in detail I run mpirun as follow:<br>
                <br>
                mpirun -n 5 java -cp path1:path2
                -Djava.library.path=pathLibs<br>
                classification.MyClass<br>
                <br>
                It seems that the option "-Djava.library.path" is
                ignored when i execute<br>
                the command.<br>
                <br>
                Is it normal ?<br>
                <br>
                Do you know how to solve this problem ?<br>
                <br>
                Thank you.<br>
                <br>
                Best,<br>
                Claudio<br>
                <br>
                -- <br>
                C.<br>
                -------------- next part --------------<br>
                HTML attachment scrubbed and removed<br>
                <br>
                ------------------------------<br>
                <br>
                Subject: Digest Footer<br>
                <br>
                _______________________________________________<br>
                users mailing list<br>
                <a moz-do-not-send="true"
                  ymailto="mailto:users@open-mpi.org"
                  class="removed-link" href="">users@open-mpi.org</a><br>
                <a moz-do-not-send="true" target="_blank"
                  class="removed-link" href="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                <br>
                ------------------------------<br>
                <br>
                End of users Digest, Vol 3510, Issue 2<br>
                **************************************<br>
                <br>
                <br>
              </div>
            </div>
          </div>
        </div>
      </div>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="https://www.open-mpi.org/mailman/listinfo.cgi/users">https://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/05/29295.php">http://www.open-mpi.org/community/lists/users/2016/05/29295.php</a></pre>
    </blockquote>
    <br>
  </body>
</html>

