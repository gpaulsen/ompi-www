<html>
<head>
<style><!--
.hmmessage P
{
margin:0px;
padding:0px
}
body.hmmessage
{
font-size: 12pt;
font-family:Calibri
}
--></style></head>
<body class='hmmessage'><div dir='ltr'>Guys<br><br>I changed the line to run the program in the script with both options<br>/usr/bin/time -f "%E" /opt/openmpi/bin/mpirun -v --bind-to-none -np $NSLOTS ./inverse.exe<br>/usr/bin/time -f "%E" /opt/openmpi/bin/mpirun -v --bind-to-socket -np $NSLOTS ./inverse.exe<br><br>but I got the same results. When I use man mpirun appears:<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -bind-to-none, --bind-to-none<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Do not bind processes.&nbsp; (Default.)<br><br>and the output of 'qconf -sp orte' is<br><br>pe_name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; orte<br>slots&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 9999<br>user_lists&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NONE<br>xuser_lists&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NONE<br>start_proc_args&nbsp;&nbsp;&nbsp; /bin/true<br>stop_proc_args&nbsp;&nbsp;&nbsp;&nbsp; /bin/true<br>allocation_rule&nbsp;&nbsp;&nbsp; $fill_up<br>control_slaves&nbsp;&nbsp;&nbsp;&nbsp; TRUE<br>job_is_first_task&nbsp; FALSE<br>urgency_slots&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; min<br>accounting_summary TRUE<br><br>I don't know if the installed Open MPI was compiled with '--with-sge'. How can i know that?<br>before to think in an hybrid application i was using only MPI and the program used few processors (14). The cluster possesses 28 machines, 15 with 16 cores and 13 with 8 cores totalizing 344 units of processing. When I submitted the job (only MPI), the MPI processes were spread to the cores directly, for that reason I created a new queue with 14 machines trying to gain more time.&nbsp; the results were the same in both cases. In the last case i could prove that the processes were distributed to all machines correctly.<br><br>What I must to do?<br>Thanks <br><br><u>Oscar Fabian Mojica Ladino</u><br><font style="font-size:8pt" size="1">Geologist M.S. in&nbsp; Geophysics</font><br><br><br><div>&gt; Date: Thu, 14 Aug 2014 10:10:17 -0400<br>&gt; From: maxime.boissonneault@calculquebec.ca<br>&gt; To: users@open-mpi.org<br>&gt; Subject: Re: [OMPI users] Running a hybrid MPI+openMP program<br>&gt; <br>&gt; Hi,<br>&gt; You DEFINITELY need to disable OpenMPI's new default binding. Otherwise, <br>&gt; your N threads will run on a single core. --bind-to socket would be my <br>&gt; recommendation for hybrid jobs.<br>&gt; <br>&gt; Maxime<br>&gt; <br>&gt; <br>&gt; Le 2014-08-14 10:04, Jeff Squyres (jsquyres) a écrit :<br>&gt; &gt; I don't know much about OpenMP, but do you need to disable Open MPI's default bind-to-core functionality (I'm assuming you're using Open MPI 1.8.x)?<br>&gt; &gt;<br>&gt; &gt; You can try "mpirun --bind-to none ...", which will have Open MPI not bind MPI processes to cores, which might allow OpenMP to think that it can use all the cores, and therefore it will spawn num_cores threads...?<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt; On Aug 14, 2014, at 9:50 AM, Oscar Mojica &lt;o_mojical@hotmail.com&gt; wrote:<br>&gt; &gt;<br>&gt; &gt;&gt; Hello everybody<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; I am trying to run a hybrid mpi + openmp program in a cluster.  I created a queue with 14 machines, each one with 16 cores. The program divides the work among the 14 processors with MPI and within each processor a loop is also divided into 8 threads for example, using openmp. The problem is that when I submit the job to the queue the MPI processes don't divide the work into threads and the program prints the number of threads  that are working within each process as one.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; I made a simple test program that uses openmp and  I logged in one machine of the fourteen. I compiled it using gfortran -fopenmp program.f -o exe,  set the OMP_NUM_THREADS environment variable equal to 8  and when I ran directly in the terminal the loop was effectively divided among the cores and for example in this case the program printed the number of threads equal to 8<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; This is my Makefile<br>&gt; &gt;&gt;   <br>&gt; &gt;&gt; # Start of the makefile<br>&gt; &gt;&gt; # Defining variables<br>&gt; &gt;&gt; objects = inv_grav3d.o funcpdf.o gr3dprm.o fdjac.o dsvd.o<br>&gt; &gt;&gt; #f90comp = /opt/openmpi/bin/mpif90<br>&gt; &gt;&gt; f90comp = /usr/bin/mpif90<br>&gt; &gt;&gt; #switch = -O3<br>&gt; &gt;&gt; executable = inverse.exe<br>&gt; &gt;&gt; # Makefile<br>&gt; &gt;&gt; all : $(executable)<br>&gt; &gt;&gt; $(executable) : $(objects)	<br>&gt; &gt;&gt; 	$(f90comp) -fopenmp -g -O -o $(executable) $(objects)<br>&gt; &gt;&gt; 	rm $(objects)<br>&gt; &gt;&gt; %.o: %.f<br>&gt; &gt;&gt; 	$(f90comp) -c $&lt;<br>&gt; &gt;&gt; # Cleaning everything<br>&gt; &gt;&gt; clean:<br>&gt; &gt;&gt; 	rm $(executable)<br>&gt; &gt;&gt; #	rm $(objects)<br>&gt; &gt;&gt; # End of the makefile<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; and the script that i am using is<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; #!/bin/bash<br>&gt; &gt;&gt; #$ -cwd<br>&gt; &gt;&gt; #$ -j y<br>&gt; &gt;&gt; #$ -S /bin/bash<br>&gt; &gt;&gt; #$ -pe orte 14<br>&gt; &gt;&gt; #$ -N job<br>&gt; &gt;&gt; #$ -q new.q<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; export OMP_NUM_THREADS=8<br>&gt; &gt;&gt; /usr/bin/time -f "%E" /opt/openmpi/bin/mpirun -v -np $NSLOTS ./inverse.exe<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; am I forgetting something?<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Thanks,<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Oscar Fabian Mojica Ladino<br>&gt; &gt;&gt; Geologist M.S. in  Geophysics<br>&gt; &gt;&gt; _______________________________________________<br>&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; users@open-mpi.org<br>&gt; &gt;&gt; Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt; Link to this post: http://www.open-mpi.org/community/lists/users/2014/08/25016.php<br>&gt; &gt;<br>&gt; <br>&gt; <br>&gt; -- <br>&gt; ---------------------------------<br>&gt; Maxime Boissonneault<br>&gt; Analyste de calcul - Calcul Québec, Université Laval<br>&gt; Ph. D. en physique<br>&gt; <br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; users@open-mpi.org<br>&gt; Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; Link to this post: http://www.open-mpi.org/community/lists/users/2014/08/25020.php<br></div> 		 	   		  </div></body>
</html>
