<html><head><style type="text/css"><!-- DIV {margin:0px;} --></style></head><body><div style="font-family:times new roman,new york,times,serif;font-size:12pt"><div>Dear Paul,<br><br>I checked the way 'mpirun -np N &lt;cmd&gt;' you mentioned, but it was the same problem.<br><br>I guess it may related to the system I used, because I have used it correctly in another XP 32 bit system.<br><br>I look forward to more advice.Thanks.<br><br>Zhangping <br></div><div style="font-family: times new roman,new york,times,serif; font-size: 12pt;"><br><div style="font-family: times new roman,new york,times,serif; font-size: 10pt;"><font face="Tahoma" size="2"><hr size="1"><b><span style="font-weight: bold;">发件人：</span></b> "users-request@open-mpi.org" &lt;users-request@open-mpi.org&gt;<br><b><span style="font-weight: bold;">收件人：</span></b> users@open-mpi.org<br><b><span style="font-weight: bold;">发送日期：</span></b> 2011/5/19 (周四) 11:00:02
 上午<br><b><span style="font-weight: bold;">主   题：</span></b> users Digest, Vol 1910, Issue 2<br></font><br>Send users mailing list submissions to<br>&nbsp;&nbsp;&nbsp; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><br>To subscribe or unsubscribe via the World Wide Web, visit<br>&nbsp;&nbsp;&nbsp; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>or, via email, send a message with subject or body 'help' to<br>&nbsp;&nbsp;&nbsp; <a ymailto="mailto:users-request@open-mpi.org" href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a><br><br>You can reach the person managing the list at<br>&nbsp;&nbsp;&nbsp; <a ymailto="mailto:users-owner@open-mpi.org" href="mailto:users-owner@open-mpi.org">users-owner@open-mpi.org</a><br><br>When replying, please edit your Subject line so it is more specific<br>than
 "Re: Contents of users digest..."<br><br><br>Today's Topics:<br><br>&nbsp;  1. Re: Error: Entry Point Not Found (Paul van der Walt)<br>&nbsp;  2. Re: Openib with &gt; 32 cores per node (Robert Horton)<br>&nbsp;  3. Re: Openib with &gt; 32 cores per node (Samuel K. Gutierrez)<br><br><br>----------------------------------------------------------------------<br><br>Message: 1<br>Date: Thu, 19 May 2011 16:14:02 +0100<br>From: Paul van der Walt &lt;<a ymailto="mailto:paul@denknerd.nl" href="mailto:paul@denknerd.nl">paul@denknerd.nl</a>&gt;<br>Subject: Re: [OMPI users] Error: Entry Point Not Found<br>To: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID: &lt;<a ymailto="mailto:BANLkTinjZ0CNtchQJCZYhfGSnR51jPuP7w@mail.gmail.com" href="mailto:BANLkTinjZ0CNtchQJCZYhfGSnR51jPuP7w@mail.gmail.com">BANLkTinjZ0CNtchQJCZYhfGSnR51jPuP7w@mail.gmail.com</a>&gt;<br>Content-Type: text/plain;
 charset=UTF-8<br><br>Hi,<br><br>On 19 May 2011 15:54, Zhangping Wei &lt;<a ymailto="mailto:zhangping_wei@yahoo.com" href="mailto:zhangping_wei@yahoo.com">zhangping_wei@yahoo.com</a>&gt; wrote:<br>&gt; 4, I use command window to run it in this way: ?mpirun ?n 4 ?**.exe ?,then I<br><br>Probably not the problem, but shouldn't that be 'mpirun -np N &lt;cmd&gt;' ?<br><br>Paul<br><br>-- <br><span>O&lt; ascii ribbon campaign - stop html mail - <a target="_blank" href="http://www.asciiribbon.org">www.asciiribbon.org</a></span><br><br><br><br>------------------------------<br><br>Message: 2<br>Date: Thu, 19 May 2011 16:37:56 +0100<br>From: Robert Horton &lt;<a ymailto="mailto:r.horton@qmul.ac.uk" href="mailto:r.horton@qmul.ac.uk">r.horton@qmul.ac.uk</a>&gt;<br>Subject: Re: [OMPI users] Openib with &gt; 32 cores per node<br>To: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID:
 &lt;1305819476.9663.148.camel@moelwyn&gt;<br>Content-Type: text/plain; charset="UTF-8"<br><br>On Thu, 2011-05-19 at 08:27 -0600, Samuel K. Gutierrez wrote:<br>&gt; Hi,<br>&gt; <br>&gt; Try the following QP parameters that only use shared receive queues.<br>&gt; <br>&gt; -mca btl_openib_receive_queues S,12288,128,64,32:S,65536,128,64,32<br>&gt; <br><br>Thanks for that. If I run the job over 2 x 48 cores it now works and the<br>performance seems reasonable (I need to do some more tuning) but when I<br>go up to 4 x 48 cores I'm getting the same problem:<br><br>[compute-1-7.local][[14383,1],86][../../../../../ompi/mca/btl/openib/connect/btl_openib_connect_oob.c:464:qp_create_one] error creating qp errno says Cannot allocate memory<br>[compute-1-7.local:18106] *** An error occurred in MPI_Isend<br>[compute-1-7.local:18106] *** on communicator MPI_COMM_WORLD<br>[compute-1-7.local:18106] *** MPI_ERR_OTHER: known error not in list<br>[compute-1-7.local:18106]
 *** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br><br>Any thoughts?<br><br>Thanks,<br>Rob<br>-- <br>Robert Horton<br>System Administrator (Research Support) - School of Mathematical Sciences<br>Queen Mary, University of London<br><a ymailto="mailto:r.horton@qmul.ac.uk" href="mailto:r.horton@qmul.ac.uk">r.horton@qmul.ac.uk</a>&nbsp; -&nbsp; +44 (0) 20 7882 7345<br><br><br><br>------------------------------<br><br>Message: 3<br>Date: Thu, 19 May 2011 09:59:13 -0600<br>From: "Samuel K. Gutierrez" &lt;<a ymailto="mailto:samuel@lanl.gov" href="mailto:samuel@lanl.gov">samuel@lanl.gov</a>&gt;<br>Subject: Re: [OMPI users] Openib with &gt; 32 cores per node<br>To: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID: &lt;<a ymailto="mailto:B3E83138-9AF0-48C0-871C-DBBB2E712E12@lanl.gov"
 href="mailto:B3E83138-9AF0-48C0-871C-DBBB2E712E12@lanl.gov">B3E83138-9AF0-48C0-871C-DBBB2E712E12@lanl.gov</a>&gt;<br>Content-Type: text/plain; charset=us-ascii<br><br>Hi,<br><br>On May 19, 2011, at 9:37 AM, Robert Horton wrote<br><br>&gt; On Thu, 2011-05-19 at 08:27 -0600, Samuel K. Gutierrez wrote:<br>&gt;&gt; Hi,<br>&gt;&gt; <br>&gt;&gt; Try the following QP parameters that only use shared receive queues.<br>&gt;&gt; <br>&gt;&gt; -mca btl_openib_receive_queues S,12288,128,64,32:S,65536,128,64,32<br>&gt;&gt; <br>&gt; <br>&gt; Thanks for that. If I run the job over 2 x 48 cores it now works and the<br>&gt; performance seems reasonable (I need to do some more tuning) but when I<br>&gt; go up to 4 x 48 cores I'm getting the same problem:<br>&gt; <br>&gt; [compute-1-7.local][[14383,1],86][../../../../../ompi/mca/btl/openib/connect/btl_openib_connect_oob.c:464:qp_create_one] error creating qp errno says Cannot allocate memory<br>&gt;
 [compute-1-7.local:18106] *** An error occurred in MPI_Isend<br>&gt; [compute-1-7.local:18106] *** on communicator MPI_COMM_WORLD<br>&gt; [compute-1-7.local:18106] *** MPI_ERR_OTHER: known error not in list<br>&gt; [compute-1-7.local:18106] *** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>&gt; <br>&gt; Any thoughts?<br><br>How much memory does each node have?&nbsp; Does this happen at startup?<br><br>Try adding:<br><br>-mca btl_openib_cpc_include rdmacm<br><br>I'm not sure if your version of OFED supports this feature, but maybe using XRC may help.&nbsp; I **think** other tweaks are needed to get this going, but I'm not familiar with the details.<br><br>Hope that helps,<br><br>Samuel K. Gutierrez<br>Los Alamos National Laboratory<br><br><br>&gt; <br>&gt; Thanks,<br>&gt; Rob<br>&gt; -- <br>&gt; Robert Horton<br>&gt; System Administrator (Research Support) - School of Mathematical Sciences<br>&gt; Queen Mary, University of London<br>&gt; <a
 ymailto="mailto:r.horton@qmul.ac.uk" href="mailto:r.horton@qmul.ac.uk">r.horton@qmul.ac.uk</a>&nbsp; -&nbsp; +44 (0) 20 7882 7345<br>&gt; <br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><br><br><br><br><br>------------------------------<br><br>_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br>End of users Digest, Vol 1910, Issue 2<br>**************************************<br></div></div>
</div></body></html>
