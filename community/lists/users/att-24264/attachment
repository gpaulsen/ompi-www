<font size=2 face="sans-serif">Wouldn't you save yourself work and your
users confusion if you disabled options that don't currently work?</font>
<br>
<br><font size=2 face="sans-serif"><br>
Jeffrey A. Cummings<br>
Engineering Specialist<br>
Performance Modeling and Analysis Department<br>
Systems Analysis and Simulation Subdivision<br>
Systems Engineering Division<br>
Engineering and Technology Group<br>
The Aerospace Corporation<br>
571-307-4220<br>
jeffrey.a.cummings@aero.org</font>
<br>
<br>
<br>
<br><font size=1 color=#5f5f5f face="sans-serif">From: &nbsp; &nbsp; &nbsp;
&nbsp;</font><font size=1 face="sans-serif">Ralph Castain &lt;rhc@open-mpi.org&gt;</font>
<br><font size=1 color=#5f5f5f face="sans-serif">To: &nbsp; &nbsp; &nbsp;
&nbsp;</font><font size=1 face="sans-serif">Open MPI Users &lt;users@open-mpi.org&gt;,
</font>
<br><font size=1 color=#5f5f5f face="sans-serif">Date: &nbsp; &nbsp; &nbsp;
&nbsp;</font><font size=1 face="sans-serif">04/25/2014 05:40 PM</font>
<br><font size=1 color=#5f5f5f face="sans-serif">Subject: &nbsp; &nbsp;
&nbsp; &nbsp;</font><font size=1 face="sans-serif">Re: [OMPI users]
Deadlocks and warnings from libevent when using &nbsp; &nbsp; &nbsp; &nbsp;MPI_THREAD_MULTIPLE</font>
<br><font size=1 color=#5f5f5f face="sans-serif">Sent by: &nbsp; &nbsp;
&nbsp; &nbsp;</font><font size=1 face="sans-serif">&quot;users&quot;
&lt;users-bounces@open-mpi.org&gt;</font>
<br>
<hr noshade>
<br>
<br>
<br><tt><font size=2>We don't fully support THREAD_MULTIPLE, and most definitely
not when using IB. We are planning on extending that coverage in the 1.9
series<br>
<br>
<br>
On Apr 25, 2014, at 2:22 PM, Markus Wittmann &lt;markus.wittmann@fau.de&gt;
wrote:<br>
<br>
&gt; Hi everyone,<br>
&gt; <br>
&gt; I'm using the current Open MPI 1.8.1 release and observe<br>
&gt; non-deterministic deadlocks and warnings from libevent when using<br>
&gt; MPI_THREAD_MULTIPLE. Open MPI has been configured with<br>
&gt; --enable-mpi-thread-multiple --with-tm --with-verbs (see attached<br>
&gt; config.log)<br>
&gt; <br>
&gt; Attached is a sample application that spawns a thread for each process<br>
&gt; after MPI_Init_thread has been called. The thread then calls MPI_Recv<br>
&gt; which blocks until the matching MPI_Send is called just before<br>
&gt; MPI_Finalize is called in the main thread. (AFAIK MPICH uses such
kind<br>
&gt; of facility to implement a progress thread) Meanwhile the main thread<br>
&gt; exchanges data with its right/left neighbor via ISend/IRecv.<br>
&gt; <br>
&gt; I only see this, when the MPI processes run on separate nodes like
in<br>
&gt; the following:<br>
&gt; <br>
&gt; $ mpiexec -n 2 -map-by node ./test<br>
&gt; [0] isend/irecv.<br>
&gt; [0] progress thread...<br>
&gt; [0] waitall.<br>
&gt; [warn] opal_libevent2021_event_base_loop: reentrant invocation. Only
one event_base_loop can run on each event_base at once.<br>
&gt; [1] isend/irecv.<br>
&gt; [1] progress thread...<br>
&gt; [1] waitall.<br>
&gt; [warn] opal_libevent2021_event_base_loop: reentrant invocation. Only
one event_base_loop can run on each event_base at once.<br>
&gt; <br>
&gt; &lt;no further output...&gt;<br>
&gt; <br>
&gt; Can anybody confirm this?<br>
&gt; <br>
&gt; Best regards,<br>
&gt; Markus<br>
&gt; <br>
&gt; -- <br>
&gt; Markus Wittmann, HPC Services<br>
&gt; Friedrich-Alexander-Universität Erlangen-Nürnberg<br>
&gt; Regionales Rechenzentrum Erlangen (RRZE)<br>
&gt; Martensstrasse 1, 91058 Erlangen, Germany<br>
&gt; </font></tt><a href=http://www.rrze.fau.de/hpc/><tt><font size=2>http://www.rrze.fau.de/hpc/</font></tt></a><tt><font size=2><br>
&gt; &lt;info.tar.bz2&gt;&lt;test.c&gt;_______________________________________________<br>
&gt; users mailing list<br>
&gt; users@open-mpi.org<br>
&gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
<br>
_______________________________________________<br>
users mailing list<br>
users@open-mpi.org<br>
</font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
</font></tt>
<br>
