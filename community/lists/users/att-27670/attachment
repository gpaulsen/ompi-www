<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">Starting in the 1.7 series, OMPI by default launches daemons on all nodes in the allocation during startup. This is done so we can “probe” the topology of the nodes and use that info during the process mapping procedure - e.g., if you want to map-by NUMA regions.<div class=""><br class=""></div><div class="">What is happening here is that some of the nodes in your allocation aren’t allowing those daemons to callback to mpirun. Either a firewall is in the way, or something is preventing it.</div><div class=""><br class=""></div><div class="">If you don’t want to launch on those other nodes, you could just add —novm to your cmd line, or use the —host option to restrict us to your local node. However, I imagine you got the bigger allocation so you could use it :-)</div><div class=""><br class=""></div><div class="">In which case, you need to remove the obstacle. You might check for firewall, or check to see if multiple NICs are on the non-maia nodes (this can sometimes confuse things, especially if someone put the NICs on the same IP subnet)</div><div class=""><br class=""></div><div class="">HTH</div><div class="">Ralph</div><div class=""><br class=""></div><div class=""><br class=""></div><div class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Sep 24, 2015, at 8:18 AM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com" class="">fortran@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><div dir="ltr" class="">Open MPI Users,<div class=""><br class=""></div><div class="">I'm hoping someone here can help. I built Open MPI 1.10.0 with PGI 15.7 using this configure string:</div><div class=""><br class=""></div><div class=""><div class=""><font face="monospace, monospace" class="">&nbsp;./configure --disable-vt --with-tm=/PBS --with-verbs --disable-wrapper-rpath \</font></div><div class=""><font face="monospace, monospace" class="">&nbsp; &nbsp; CC=pgcc CXX=pgCC FC=pgf90 F77=pgf77 CFLAGS='-fpic -m64' \</font></div><div class=""><font face="monospace, monospace" class="">&nbsp; &nbsp; CXXFLAGS='-fpic -m64' FCFLAGS='-fpic -m64' FFLAGS='-fpic -m64' \</font></div><div class=""><font face="monospace, monospace" class="">&nbsp; &nbsp; --prefix=/nobackup/gmao_SIteam/MPI/pgi_15.7-openmpi_1.10.0 |&amp; tee configure.pgi15.7.log</font></div><div class=""><br class=""></div><div class="">It seemed to pass 'make check'.&nbsp;</div><div class=""><br class=""></div><div class="">I'm working at pleiades at NAS, and there they have both Sandy Bridge nodes with GPUs (maia) and regular Sandy Bridge compute nodes (here after called Sandy) without. To be extra careful (since PGI compiles to the architecture you build on) I took a Westmere node and built Open MPI there just in case.</div><div class=""><br class=""></div><div class="">So, as I said, all seems to work with a test. I now grab a maia node, maia1, of an allocation of 4 I had:</div><div class=""><br class=""></div><div class=""><div class=""><font face="monospace, monospace" class="">(102) $ mpicc -tp=px-64 -o helloWorld.x helloWorld.c</font></div><div class=""><font face="monospace, monospace" class="">(103) $ mpirun -np 2 ./helloWorld.x</font></div><div class=""><font face="monospace, monospace" class="">Process 0 of 2 is on maia1&nbsp;</font></div><div class=""><font face="monospace, monospace" class="">Process 1 of 2 is on maia1&nbsp;</font></div></div><div class=""><br class=""></div><div class="">Good. Now, let's go to a Sandy Bridge (non-GPU) node, r321i7n16, of an allocation of 8 I had:</div><div class=""><br class=""></div><div class=""><div class=""><font face="monospace, monospace" class="">(49) $ mpicc -tp=px-64 -o helloWorld.x helloWorld.c</font></div><div class=""><font face="monospace, monospace" class="">(50) $ mpirun -np 2 ./helloWorld.x</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n11:13063] [[62995,0],7] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n6:57417] [[62995,0],2] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n7:67287] [[62995,0],3] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n8:57429] [[62995,0],4] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n10:35329] [[62995,0],6] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n9:13456] [[62995,0],5] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div></div><div class=""><br class=""></div><div class="">Hmm. Let's try turning off tcp (often my first thought when on an Infiniband system):</div><div class=""><br class=""></div><div class=""><div class=""><font face="monospace, monospace" class="">(51) $ mpirun --mca btl sm,openib,self -np 2 ./helloWorld.x</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n6:57420] [[62996,0],2] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n9:13459] [[62996,0],5] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n8:57432] [[62996,0],4] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n7:67290] [[62996,0],3] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n11:13066] [[62996,0],7] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div class=""><font face="monospace, monospace" class="">[r323i5n10:35332] [[62996,0],6] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div></div><div class=""><br class=""></div><div class="">Now, the nodes reporting the issue seem to be the "other" nodes on the allocation that are in a different rack:</div><div class=""><br class=""></div><div class=""><div class=""><font face="monospace, monospace" class="">(52) $ cat $PBS_NODEFILE | uniq</font></div><div class=""><font face="monospace, monospace" class="">r321i7n16</font></div><div class=""><font face="monospace, monospace" class="">r321i7n17</font></div><div class=""><font face="monospace, monospace" class="">r323i5n6</font></div><div class=""><font face="monospace, monospace" class="">r323i5n7</font></div><div class=""><font face="monospace, monospace" class="">r323i5n8</font></div><div class=""><font face="monospace, monospace" class="">r323i5n9</font></div><div class=""><font face="monospace, monospace" class="">r323i5n10</font></div><div class=""><font face="monospace, monospace" class="">r323i5n11</font></div></div><div class=""><br class=""></div><div class="">Maybe that's a clue? I didn't think this would matter if I only ran two processes...and it works on the multi-node maia allocation.</div><div class=""><br class=""></div><div class="">I've tried searching the web, but the only place I've seen tcp_peer_send_blocking is in a PDF where they say it's an error that can be seen:</div><div class=""><br class=""></div><div class=""><a href="http://www.hpc.mcgill.ca/downloads/checkpointing_workshop/20150326%20-%20McGill%20-%20Checkpointing%20Techniques.pdf" class="">http://www.hpc.mcgill.ca/downloads/checkpointing_workshop/20150326%20-%20McGill%20-%20Checkpointing%20Techniques.pdf</a><br class=""></div><div class=""><br class=""></div><div class="">Any ideas for what this error can mean?</div><div class=""><br class=""></div>-- <br class=""><div class="gmail_signature"><div dir="ltr" class=""><div class=""><div dir="ltr" class=""><div class="">Matt Thompson</div></div></div><blockquote style="margin:0px 0px 0px 40px;border:none;padding:0px" class=""><div class=""><div class=""><div class="">Man Among Men</div></div></div><div class=""><div class=""><div class="">Fulcrum of History</div></div></div></blockquote></div></div>
</div></div>
_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2015/09/27669.php</div></blockquote></div><br class=""></div></body></html>
