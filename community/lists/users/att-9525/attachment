The changes Jeff mentioned are not in the 1.3 branch - not sure if they will come over there or not.<br><br>I&#39;m a little concerned in this thread that someone is reporting the process affinity binding changing - that shouldn&#39;t be happening, and my guess is that something outside of our control may be changing it.<br>
<br>One other thing to consider that has been an issue around here, and will be an even bigger issue with the change to bind at app start. If your app is threaded, we will bind *all* threads to the same processor, thus potentially hampering performance. We have found that multi-threaded apps often provide better performance if users do *not* set processor affinity via MPI, but instead embed binding calls inside the individual threads so they can be placed on separate processors.<br>
<br>All depends on the exact nature of the application, of course!<br><br>HTH<br>Ralph<br><br><br><div class="gmail_quote">On Wed, Jun 3, 2009 at 10:02 AM, Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div class="im">On Jun 3, 2009, at 11:40 AM, Ashley Pittman wrote:<br>
<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Wasn&#39;t there a discussion about this recently on the list, OMPI binds<br>
during MPI_Init() so it&#39;s possible for memory to be allocated on the<br>
wrong quad, the discussion was about moving the binding to the orte<br>
process as I recall?<br>
<br>
</blockquote>
<br></div>
Yes.  It&#39;s been fixed in OMPI devel trunk.  I&#39;m not sure it made it to the v1.3 branch, but it&#39;s definitely not in a released version yet.<br>
<br>
I *thought* that HPL did all allocation after MPI_INIT.  But I could be wrong.  If so, then using numactl to bind before the MPI app starts will likely give better results -- you&#39;re right (until we get our fixes in such that we bind pre-main).<br>

<br>
Regardless, if something is *changing* the affinity after MPI_INIT, then there&#39;s little OMPI can do about that.<div class="im"><br>
<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
&gt;From my testing of process affinity you tend to get much more consistent<br>
results with it on and much more unpredictable results with it off, I&#39;d<br>
questing that it&#39;s working properly if you are seeing a 88-93% range in<br>
the results.<br>
<br>
Ashley Pittman.<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
</blockquote>
<br>
<br></div><font color="#888888">
-- <br>
Jeff Squyres<br>
Cisco Systems</font><div><div></div><div class="h5"><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

