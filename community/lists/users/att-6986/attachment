<div dir="ltr"><br><br><div class="gmail_quote">On Fri, Oct 10, 2008 at 10:40 PM, Brian Dobbins <span dir="ltr">&lt;<a href="mailto:bdobbins@gmail.com">bdobbins@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<div dir="ltr"><br>Hi guys,<br><br><div class="gmail_quote"><div class="Ih2E3d">On Fri, Oct 10, 2008 at 12:57 PM, Brock Palen <span dir="ltr">&lt;<a href="mailto:brockp@umich.edu" target="_blank">brockp@umich.edu</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Actually I had a much differnt results,<br>
<br>
gromacs-3.3.1 &nbsp;one node dual core dual socket opt2218 &nbsp;openmpi-1.2.7 &nbsp;pgi/7.2<br>
mpich2 gcc<br>
</blockquote></div><div><br>&nbsp;&nbsp; For some reason, the difference in minutes didn&#39;t come through, it seems, but I would guess that if it&#39;s a medium-large difference, then it has its roots in PGI7.2 vs. GCC rather than MPICH2 vs. OpenMPI.&nbsp; Though, to be fair, I find GCC vs. PGI (for C code) is often a toss-up - one may beat the other handily on one code, and then lose just as badly on another.<br>

<br></div><div class="Ih2E3d"><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">I think my install of mpich2 may be bad, I have never installed it before, &nbsp;only mpich1, OpenMPI and LAM. So take my mpich2 numbers with salt, Lots of salt.</blockquote>

</div><div><br>&nbsp; I think the biggest difference in performance with various MPICH2 install comes from differences in the &#39;channel&#39; used..&nbsp; I tend to make sure that I use the &#39;nemesis&#39; channel, which may or may not be the default these days.&nbsp; If not, though, most people would probably want it.&nbsp; I think it has issues with threading (or did ages ago?), but I seem to recall it being considerably faster than even the &#39;ssm&#39; channel.<br>

&nbsp;<br>&nbsp; Sangamesh:&nbsp; My advice to you would be to recompile Gromacs and specify, in the <i>Gromacs</i> compile / configure, to use the same CFLAGS you used with MPICH2.&nbsp; Eg, &quot;-O2 -m64&quot;, whatever.&nbsp; If you do that, I bet the times between MPICH2 and OpenMPI will be pretty comparable for your benchmark case - especially when run on a single processor.<br>

</div></div></div></blockquote><div><br>I reinstalled all softwares with -O3 optimization. Following are the performance numbers for a 4 process job on a single node:<br><br>MPICH2:&nbsp;&nbsp;&nbsp;&nbsp; 26 m 54 s<br>OpenMPI:&nbsp;&nbsp; 24 m 39 s<br>
<br>More details:<br><br>$ /home/san/PERF_TEST/mpich2/bin/mpich2version <br>MPICH2 Version:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.0.7<br>MPICH2 Release date:&nbsp;&nbsp;&nbsp; Unknown, built on Mon Oct 13 18:02:13 IST 2008<br>MPICH2 Device:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ch3:sock<br>MPICH2 configure:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --prefix=/home/san/PERF_TEST/mpich2<br>
MPICH2 CC:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /usr/bin/gcc -O3 -O2<br>MPICH2 CXX:&nbsp;&nbsp;&nbsp;&nbsp; /usr/bin/g++&nbsp; -O2<br>MPICH2 F77:&nbsp;&nbsp;&nbsp;&nbsp; /usr/bin/gfortran -O3 -O2<br>MPICH2 F90:&nbsp;&nbsp;&nbsp;&nbsp; /usr/bin/gfortran&nbsp; -O2<br><br><br>$ /home/san/PERF_TEST/openmpi/bin/ompi_info <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Open MPI: 1.2.7<br>
&nbsp;&nbsp; Open MPI SVN revision: r19401<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Open RTE: 1.2.7<br>&nbsp;&nbsp; Open RTE SVN revision: r19401<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; OPAL: 1.2.7<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; OPAL SVN revision: r19401<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Prefix: /home/san/PERF_TEST/openmpi<br>
&nbsp;Configured architecture: x86_64-unknown-linux-gnu<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Configured by: san<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Configured on: Mon Oct 13 19:10:13 IST 2008<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Configure host: <a href="http://locuzcluster.org">locuzcluster.org</a><br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Built by: san<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Built on: Mon Oct 13 19:18:25 IST 2008<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Built host: <a href="http://locuzcluster.org">locuzcluster.org</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C bindings: yes<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C++ bindings: yes<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fortran77 bindings: yes (all)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fortran90 bindings: yes<br>&nbsp;Fortran90 bindings size: small<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C compiler: /usr/bin/gcc<br>&nbsp;&nbsp;&nbsp;&nbsp; C compiler absolute: /usr/bin/gcc<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C++ compiler: /usr/bin/g++<br>
&nbsp;&nbsp; C++ compiler absolute: /usr/bin/g++<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fortran77 compiler: /usr/bin/gfortran<br>&nbsp; Fortran77 compiler abs: /usr/bin/gfortran<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fortran90 compiler: /usr/bin/gfortran<br>&nbsp; Fortran90 compiler abs: /usr/bin/gfortran<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C profiling: yes<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C++ profiling: yes<br>&nbsp;&nbsp;&nbsp;&nbsp; Fortran77 profiling: yes<br>&nbsp;&nbsp;&nbsp;&nbsp; Fortran90 profiling: yes<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C++ exceptions: no<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Thread support: posix (mpi: no, progress: no)<br>
&nbsp; Internal debug support: no<br>&nbsp;&nbsp;&nbsp;&nbsp; MPI parameter check: runtime<br>Memory profiling support: no<br>Memory debugging support: no<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; libltdl support: yes<br>&nbsp;&nbsp; Heterogeneous support: yes<br>&nbsp;mpirun default --prefix: no<br>
<br>Thanks,<br>Sangamesh<br></div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div dir="ltr"><div class="gmail_quote"><div><br>&nbsp; Cheers,<br>
&nbsp; - Brian<br></div></div><br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>

