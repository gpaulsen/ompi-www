<div dir="ltr"><div><div><div><div><div><div>Indeed, I simply commented out all the MPI_Info stuff, which you essentially did by passing a dummy argument.  I&#39;m still not able to get it to succeed.<br><br></div>So here we go, my results defy logic.  I&#39;m sure this could be my fault...I&#39;ve only been an occasional user of OpenMPI and MPI in general over the years and I&#39;ve never used MPI_Comm_spawn before this project. I tested simple_spawn like so:<br></div>mpicc simple_spawn.c -o simple_spawn<br></div>./simple_spawn<br><br></div>When my default hostfile points to a file that just lists localhost, this test completes successfully.  If it points to my hostfile with localhost and 5 remote hosts, here&#39;s the output:<br>evan@lasarti:~/devel/toy_progs/mpi_spawn$ mpicc simple_spawn.c -o simple_spawn<br>evan@lasarti:~/devel/toy_progs/mpi_spawn$ ./simple_spawn<br>[pid 5703] starting up!<br>0 completed MPI_Init<br>Parent [pid 5703] about to spawn!<br>[lasarti:05703] [[14661,1],0] FORKING HNP: orted --hnp --set-sid --report-uri 14 --singleton-died-pipe 15 -mca state_novm_select 1 -mca ess_base_jobid 960823296<br>[lasarti:05705] *** Process received signal ***<br>[lasarti:05705] Signal: Segmentation fault (11)<br>[lasarti:05705] Signal code: Address not mapped (1)<br>[lasarti:05705] Failing at address: (nil)<br>[lasarti:05705] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x10340)[0x7fc185dcf340]<br>[lasarti:05705] [ 1] /opt/openmpi-v1.8.4-54-g07f735a/lib/libopen-rte.so.7(orte_rmaps_base_compute_bindings+0x650)[0x7fc186033bb0]<br>[lasarti:05705] [ 2] /opt/openmpi-v1.8.4-54-g07f735a/lib/libopen-rte.so.7(orte_rmaps_base_map_job+0x939)[0x7fc18602fb99]<br>[lasarti:05705] [ 3] /opt/openmpi-v1.8.4-54-g07f735a/lib/libopen-pal.so.6(opal_libevent2021_event_base_loop+0x6e4)[0x7fc18577dcc4]<br>[lasarti:05705] [ 4] /opt/openmpi-v1.8.4-54-g07f735a/lib/libopen-rte.so.7(orte_daemon+0xdf8)[0x7fc186010438]<br>[lasarti:05705] [ 5] orted(main+0x47)[0x400887]<br>[lasarti:05705] [ 6] /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf5)[0x7fc185a1aec5]<br>[lasarti:05705] [ 7] orted[0x4008db]<br>[lasarti:05705] *** End of error message ***<br><br></div>You can see from the message that this particular run IS from the latest snapshot, though the failure happens on v.1.8.4 as well.  I didn&#39;t bother installing the snapshot on the remote nodes though.  Should I do that?  It looked to me like this error happened well before we got to a remote node, so that&#39;s why I didn&#39;t.<br></div><div><br></div><div>Your thoughts?<br></div><div><br></div><div>Evan<br></div><div><div><br><br></div></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Tue, Feb 3, 2015 at 7:40 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">I confess I am sorely puzzled. I replace the Info key with MPI_INFO_NULL, but still had to pass a bogus argument to master since you still have the Info_set code in there - otherwise, info_set segfaults due to a NULL argv[1]. Doing that (and replacing &quot;hostname&quot; with an MPI example code) makes everything work just fine.<div><br></div><div>I&#39;ve attached one of our example comm_spawn codes that we test against - it also works fine with the current head of the 1.8 code base. I confess that some changes have been made since 1.8.4 was released, and it is entirely possible that this was a problem in 1.8.4 and has since been fixed.</div><div><br></div><div>So I&#39;d suggest trying with the nightly 1.8 tarball and seeing if it works for you. You can download it from here:</div><div><br></div><div><a href="http://www.open-mpi.org/nightly/v1.8/" target="_blank">http://www.open-mpi.org/nightly/v1.8/</a><br></div><div><br></div><div>HTH</div><div>Ralph</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote"><div><div class="h5">On Tue, Feb 3, 2015 at 6:20 PM, Evan Samanas <span dir="ltr">&lt;<a href="mailto:evan.samanas@gmail.com" target="_blank">evan.samanas@gmail.com</a>&gt;</span> wrote:<br></div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div class="h5"><div dir="ltr">Yes, I did.  I replaced the info argument of MPI_Comm_spawn with MPI_INFO_NULL.<br></div><div class="gmail_extra"><br><div class="gmail_quote"><div><div>On Tue, Feb 3, 2015 at 5:54 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br></div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div><div dir="ltr">When running your comm_spawn code, did you remove the Info key code? You wouldn&#39;t need to provide a hostfile or hosts any more, which is why it should resolve that problem.<div><br></div><div>I agree that providing either hostfile or host as an Info key will cause the program to segfault - I&#39;m woking on that issue.</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote"><div><div>On Tue, Feb 3, 2015 at 3:46 PM, Evan Samanas <span dir="ltr">&lt;<a href="mailto:evan.samanas@gmail.com" target="_blank">evan.samanas@gmail.com</a>&gt;</span> wrote:<br></div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div><div dir="ltr"><div>Setting these environment variables did indeed change the way mpirun maps things, and I didn&#39;t have to specify a hostfile.  However, setting these for my MPI_Comm_spawn code still resulted in the same segmentation fault.<br><br></div>Evan<br></div><div class="gmail_extra"><br><div class="gmail_quote"><div><div>On Tue, Feb 3, 2015 at 10:09 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br></div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div><div dir="ltr">If you add the following to your environment, you should run on multiple nodes:<div><br></div><div>OMPI_MCA_<span style="font-family:Menlo;font-size:11px">rmaps_base_mapping_policy=node</span></div><div><span style="font-family:Menlo;font-size:11px">OMPI_MCA_</span><span style="font-family:Menlo;font-size:11px">orte_default_hostfile=&lt;your hostfile&gt;</span></div><div><span style="font-family:Menlo;font-size:11px"><br></span></div><div><span style="font-family:Menlo;font-size:11px">The first tells OMPI to map-by node. The second passes in your default hostfile so you don&#39;t need to specify it as an Info key.</span></div><div><span style="font-family:Menlo;font-size:11px"><br></span></div><div><span style="font-family:Menlo;font-size:11px">HTH</span></div><div><span style="font-family:Menlo;font-size:11px">Ralph</span></div><div><span style="font-family:Menlo;font-size:11px"><br></span></div></div><div class="gmail_extra"><br><div class="gmail_quote"><div><div>On Tue, Feb 3, 2015 at 9:23 AM, Evan Samanas <span dir="ltr">&lt;<a href="mailto:evan.samanas@gmail.com" target="_blank">evan.samanas@gmail.com</a>&gt;</span> wrote:<br></div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div><div dir="ltr"><div><div><div>Hi Ralph,<br><br></div>Good to know you&#39;ve reproduced it.  I was experiencing this using both the hostfile and host key.  A simple comm_spawn was working for me as well, but it was only launching locally, and I&#39;m pretty sure each node only has 4 slots given past behavior (the mpirun -np 8 example I gave in my first email launches on both hosts).  Is there a way to specify the hosts I want to launch on without the hostfile or host key so I can test remote launch?<br><br></div>And to the &quot;hostname&quot; response...no wonder it was hanging!  I just constructed that as a basic example.  In my real use I&#39;m launching something that calls MPI_Init.<span><font color="#888888"><br><br></font></span></div><span><font color="#888888">Evan<br><div><div><div><div><br><br></div></div></div></div></font></span></div>
<br></div></div>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/02/26271.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/02/26271.php</a><br></blockquote></div><br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/02/26272.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/02/26272.php</a><br></blockquote></div><br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/02/26281.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/02/26281.php</a><br></blockquote></div><br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/02/26285.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/02/26285.php</a><br></blockquote></div><br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/02/26286.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/02/26286.php</a><br></blockquote></div><br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/02/26287.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/02/26287.php</a><br></blockquote></div><br></div>

