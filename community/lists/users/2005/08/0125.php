<? include("../../include/msg-header.inc"); ?>
<!-- received="Tue Aug 30 11:24:03 2005" -->
<!-- isoreceived="20050830162403" -->
<!-- sent="Tue, 30 Aug 2005 12:24:01 -0400" -->
<!-- isosent="20050830162401" -->
<!-- name="Jeff Squyres" -->
<!-- email="jsquyres_at_[hidden]" -->
<!-- subject="Re: [O-MPI users] deadlock and SEGV in collectives using btl gm and tcp" -->
<!-- id="8eaaa1fcb7fd9daad8f016af3e88cf34_at_open-mpi.org" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="43142E87.20505_at_ccrl-nece.de" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>From:</strong> Jeff Squyres (<em>jsquyres_at_[hidden]</em>)<br>
<strong>Date:</strong> 2005-08-30 11:24:01
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="http://www.open-mpi.org/community/lists/users/2005/09/0126.php">Borenstein, Bernard S: "[O-MPI users] open-mpi team - good job and a question"</a>
<li><strong>Previous message:</strong> <a href="0124.php">Joachim Worringen: "[O-MPI users] deadlock and SEGV in collectives using btl gm and tcp"</a>
<li><strong>In reply to:</strong> <a href="0124.php">Joachim Worringen: "[O-MPI users] deadlock and SEGV in collectives using btl gm and tcp"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
Greetings!
<br>
<p>We actually had some problems in some of our collectives with some 
<br>
optimizations that were added in the last month or so, and we just 
<br>
noticed/corrected them yesterday.  It looks like your tarball is about 
<br>
a week old or so -- you might want to update to a newer one.  Last 
<br>
night's tarball should include all the fixes that we made yesterday; 
<br>
I'm artifically making another one right now that includes some fixes 
<br>
from this morning.
<br>
<p>Thanks for your patience; we're actually getting pretty close to 
<br>
stable, but aren't quite there yet...
<br>
<p><p>On Aug 30, 2005, at 6:01 AM, Joachim Worringen wrote:
<br>
<p><span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Dear *,
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; I'm currently testing  OpenMPI 1.0a1r7026 on a Linux 2.6.6 32-node 
</span><br>
<span class="quotelev1">&gt; Dual-Athlon
</span><br>
<span class="quotelev1">&gt; cluster with Myrinet (GM 2.1.1 on M3M-PCI64C boards). gcc is 3.3.3. 
</span><br>
<span class="quotelev1">&gt; 4GB RAM per
</span><br>
<span class="quotelev1">&gt; node.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Compilation from the snapshot and startup went fine, congratulations. 
</span><br>
<span class="quotelev1">&gt; Surely not
</span><br>
<span class="quotelev1">&gt; trivial.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Point-to-point tests (mpptest) pass. However, running a rather simple 
</span><br>
<span class="quotelev1">&gt; benchmark
</span><br>
<span class="quotelev1">&gt; to test the performance of collective operations (not PMB, but a 
</span><br>
<span class="quotelev1">&gt; custom one)
</span><br>
<span class="quotelev1">&gt; seems to deadlock. So far, I could figure out:
</span><br>
<span class="quotelev1">&gt; - using btl 'gm' (default)
</span><br>
<span class="quotelev1">&gt;    o 16 processes on 8 nodes: &quot;deadlock&quot; in Allreduce
</span><br>
<span class="quotelev1">&gt;    o 2 processes on 2 nodes: &quot;deadlock&quot; in Reduce_scatter
</span><br>
<span class="quotelev1">&gt; - explicitely using btl 'tcp'
</span><br>
<span class="quotelev1">&gt;    o 2 processes on 2 nodes: &quot;deadlock&quot; in Reduce_scatter
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Additionally, I sporadically receive SEGV's using gm:
</span><br>
<span class="quotelev1">&gt; Core was generated by `collmeas_open-mpi'.
</span><br>
<span class="quotelev1">&gt; Program terminated with signal 11, Segmentation fault.
</span><br>
<span class="quotelev1">&gt; (gdb) bt
</span><br>
<span class="quotelev1">&gt; #0  0x00000000 in ?? ()
</span><br>
<span class="quotelev1">&gt; #1  0x4006d04c in mca_mpool_base_registration_destructor () from
</span><br>
<span class="quotelev1">&gt; /home/joachim/local/open-mpi/lib/libmpi.so.0
</span><br>
<span class="quotelev1">&gt; #2  0x40179a0c in mca_mpool_gm_free () from
</span><br>
<span class="quotelev1">&gt; /home/joachim/local/open-mpi//lib/openmpi/mca_mpool_gm.so
</span><br>
<span class="quotelev1">&gt; #3  0x4006cf9c in mca_mpool_base_free () from
</span><br>
<span class="quotelev1">&gt; /home/joachim/local/open-mpi/lib/libmpi.so.0
</span><br>
<span class="quotelev1">&gt; #4  0x4004efbc in PMPI_Free_mem () from 
</span><br>
<span class="quotelev1">&gt; /home/joachim/local/open-mpi/lib/libmpi.so.0
</span><br>
<span class="quotelev1">&gt; #5  0x0804b1c9 in main ()
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Sometimes, this seems to happen when aborting an application (via 
</span><br>
<span class="quotelev1">&gt; CTRL-C to mpirun):
</span><br>
<span class="quotelev1">&gt; Core was generated by `collmeas_open-mpi'.
</span><br>
<span class="quotelev1">&gt; Program terminated with signal 11, Segmentation fault.
</span><br>
<span class="quotelev1">&gt; (gdb) bt
</span><br>
<span class="quotelev1">&gt; #0  0x401d0633 in mca_btl_tcp_proc_remove () from
</span><br>
<span class="quotelev1">&gt; /home/joachim/local/open-mpi//lib/openmpi/mca_btl_tcp.so
</span><br>
<span class="quotelev1">&gt; Cannot access memory at address 0xbfffe2bc
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Of course, I'm not sure if the deadlock really is a deadlock, but the 
</span><br>
<span class="quotelev1">&gt; respective
</span><br>
<span class="quotelev1">&gt; tests takes way to much time. Needless to say that other MPI 
</span><br>
<span class="quotelev1">&gt; implementations run
</span><br>
<span class="quotelev1">&gt; this benchmark (which we are using for some time on a variety of 
</span><br>
<span class="quotelev1">&gt; platforms)
</span><br>
<span class="quotelev1">&gt; reliably on the same machine (MPICH-GM, our own MPI).
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Any ideas or comments? I will try to run PMB.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;   Joachim
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; -- 
</span><br>
<span class="quotelev1">&gt; Joachim Worringen - NEC C&amp;C research lab St.Augustin
</span><br>
<span class="quotelev1">&gt; fon +49-2241-9252.20 - fax .99 - <a href="http://www.ccrl-nece.de">http://www.ccrl-nece.de</a>
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; _______________________________________________
</span><br>
<span class="quotelev1">&gt; users mailing list
</span><br>
<span class="quotelev1">&gt; users_at_[hidden]
</span><br>
<span class="quotelev1">&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1">&gt;
</span><br>
<p><pre>
-- 
{+} Jeff Squyres
{+} The Open MPI Project
{+} <a href="http://www.open-mpi.org/">http://www.open-mpi.org/</a>
</pre>
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="http://www.open-mpi.org/community/lists/users/2005/09/0126.php">Borenstein, Bernard S: "[O-MPI users] open-mpi team - good job and a question"</a>
<li><strong>Previous message:</strong> <a href="0124.php">Joachim Worringen: "[O-MPI users] deadlock and SEGV in collectives using btl gm and tcp"</a>
<li><strong>In reply to:</strong> <a href="0124.php">Joachim Worringen: "[O-MPI users] deadlock and SEGV in collectives using btl gm and tcp"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
