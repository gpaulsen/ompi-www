<? include("../../include/msg-header.inc"); ?>
<!-- received="Tue Aug 30 05:01:55 2005" -->
<!-- isoreceived="20050830100155" -->
<!-- sent="Tue, 30 Aug 2005 12:01:43 +0200" -->
<!-- isosent="20050830100143" -->
<!-- name="Joachim Worringen" -->
<!-- email="joachim_at_[hidden]" -->
<!-- subject="[O-MPI users] deadlock and SEGV in collectives using btl gm and tcp" -->
<!-- id="43142E87.20505_at_ccrl-nece.de" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>From:</strong> Joachim Worringen (<em>joachim_at_[hidden]</em>)<br>
<strong>Date:</strong> 2005-08-30 05:01:43
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0125.php">Jeff Squyres: "Re: [O-MPI users] deadlock and SEGV in collectives using btl gm and tcp"</a>
<li><strong>Previous message:</strong> <a href="0123.php">George Bosilca: "Re: [O-MPI users] How pass an structure"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0125.php">Jeff Squyres: "Re: [O-MPI users] deadlock and SEGV in collectives using btl gm and tcp"</a>
<li><strong>Reply:</strong> <a href="0125.php">Jeff Squyres: "Re: [O-MPI users] deadlock and SEGV in collectives using btl gm and tcp"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
Dear *,
<br>
<p>I'm currently testing  OpenMPI 1.0a1r7026 on a Linux 2.6.6 32-node Dual-Athlon
<br>
cluster with Myrinet (GM 2.1.1 on M3M-PCI64C boards). gcc is 3.3.3. 4GB RAM per
<br>
node.
<br>
<p>Compilation from the snapshot and startup went fine, congratulations. Surely not
<br>
trivial.
<br>
<p>Point-to-point tests (mpptest) pass. However, running a rather simple benchmark
<br>
to test the performance of collective operations (not PMB, but a custom one)
<br>
seems to deadlock. So far, I could figure out:
<br>
- using btl 'gm' (default)
<br>
&nbsp;&nbsp;&nbsp;o 16 processes on 8 nodes: &quot;deadlock&quot; in Allreduce
<br>
&nbsp;&nbsp;&nbsp;o 2 processes on 2 nodes: &quot;deadlock&quot; in Reduce_scatter
<br>
- explicitely using btl 'tcp'
<br>
&nbsp;&nbsp;&nbsp;o 2 processes on 2 nodes: &quot;deadlock&quot; in Reduce_scatter
<br>
<p>Additionally, I sporadically receive SEGV's using gm:
<br>
Core was generated by `collmeas_open-mpi'.
<br>
Program terminated with signal 11, Segmentation fault.
<br>
(gdb) bt
<br>
#0  0x00000000 in ?? ()
<br>
#1  0x4006d04c in mca_mpool_base_registration_destructor () from
<br>
/home/joachim/local/open-mpi/lib/libmpi.so.0
<br>
#2  0x40179a0c in mca_mpool_gm_free () from
<br>
/home/joachim/local/open-mpi//lib/openmpi/mca_mpool_gm.so
<br>
#3  0x4006cf9c in mca_mpool_base_free () from
<br>
/home/joachim/local/open-mpi/lib/libmpi.so.0
<br>
#4  0x4004efbc in PMPI_Free_mem () from /home/joachim/local/open-mpi/lib/libmpi.so.0
<br>
#5  0x0804b1c9 in main ()
<br>
<p>Sometimes, this seems to happen when aborting an application (via CTRL-C to mpirun):
<br>
Core was generated by `collmeas_open-mpi'.
<br>
Program terminated with signal 11, Segmentation fault.
<br>
(gdb) bt
<br>
#0  0x401d0633 in mca_btl_tcp_proc_remove () from
<br>
/home/joachim/local/open-mpi//lib/openmpi/mca_btl_tcp.so
<br>
Cannot access memory at address 0xbfffe2bc
<br>
<p>Of course, I'm not sure if the deadlock really is a deadlock, but the respective
<br>
tests takes way to much time. Needless to say that other MPI implementations run
<br>
this benchmark (which we are using for some time on a variety of platforms)
<br>
reliably on the same machine (MPICH-GM, our own MPI).
<br>
<p>Any ideas or comments? I will try to run PMB.
<br>
<p>&nbsp;&nbsp;Joachim
<br>
<p><pre>
-- 
Joachim Worringen - NEC C&amp;C research lab St.Augustin
fon +49-2241-9252.20 - fax .99 - <a href="http://www.ccrl-nece.de">http://www.ccrl-nece.de</a>
</pre>
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0125.php">Jeff Squyres: "Re: [O-MPI users] deadlock and SEGV in collectives using btl gm and tcp"</a>
<li><strong>Previous message:</strong> <a href="0123.php">George Bosilca: "Re: [O-MPI users] How pass an structure"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="0125.php">Jeff Squyres: "Re: [O-MPI users] deadlock and SEGV in collectives using btl gm and tcp"</a>
<li><strong>Reply:</strong> <a href="0125.php">Jeff Squyres: "Re: [O-MPI users] deadlock and SEGV in collectives using btl gm and tcp"</a>
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
