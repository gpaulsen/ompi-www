<? include("../../include/msg-header.inc"); ?>
<!-- received="Sun Nov 13 14:52:42 2005" -->
<!-- isoreceived="20051113195242" -->
<!-- sent="Sun, 13 Nov 2005 14:52:27 -0500" -->
<!-- isosent="20051113195227" -->
<!-- name="Jeff Squyres" -->
<!-- email="jsquyres_at_[hidden]" -->
<!-- subject="Re: [O-MPI users] can't get openmpi to run across twomulti-NICmachines" -->
<!-- id="ca1ce89627a7d7a7aad5d91b29a870f3_at_open-mpi.org" -->
<!-- charset="US-ASCII" -->
<!-- inreplyto="00a101c5e5ff$59501990$3f898f80_at_Roger2" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>From:</strong> Jeff Squyres (<em>jsquyres_at_[hidden]</em>)<br>
<strong>Date:</strong> 2005-11-13 14:52:27
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0327.php">Jeff Squyres: "Re: [O-MPI users] 1.0rc5 is up"</a>
<li><strong>Previous message:</strong> <a href="0325.php">Brian Barrett: "Re: [O-MPI users] Synchronizing C++ STL objects"</a>
<li><strong>In reply to:</strong> <a href="0306.php">Marty Humphrey: "Re: [O-MPI users] can't get openmpi to run across twomulti-NICmachines"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
To close this thread for the web archives...
<br>
<p>We iterated about this quite a bit off the list and fixed a pair of  
<br>
bugs that didn't make it into RC5.  Many thanks to Marty for his  
<br>
patience in helping us fix this!
<br>
<p>For those who care, the bugs were:
<br>
<p>- The shared memory btl had a problem if mmap() returned different  
<br>
addresses to the same shared memory segment in different processes.
<br>
- The TCP btl does subnet mask checking to help determine which IP  
<br>
addresses to hook up amongst peers (remember that Open MPI can utilize  
<br>
multiple TCP interfaces in a single job); there was a bug that did not  
<br>
allow arbitrary, non-subnet-mask-matched connections.
<br>
<p>Both have been fixed in both the SVN trunk and v1.0 branch and are in  
<br>
the nightly snapshot tarballs from this morning.
<br>
<p><p><p>On Nov 10, 2005, at 9:02 AM, Marty Humphrey wrote:
<br>
<p><span class="quotelev1">&gt; Here's a core I'm getting...
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; [humphrey_at_zelda01 humphrey]$ mpiexec --mca btl_tcp_if_include eth0   
</span><br>
<span class="quotelev1">&gt; --mca
</span><br>
<span class="quotelev1">&gt; oob_tcp_include eth0  -np 2 a.out
</span><br>
<span class="quotelev1">&gt; mpiexec noticed that job rank 1 with PID 20028 on node &quot;localhost&quot;  
</span><br>
<span class="quotelev1">&gt; exited on
</span><br>
<span class="quotelev1">&gt; signal 11.
</span><br>
<span class="quotelev1">&gt; 1 process killed (possibly by Open MPI)
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; [humphrey_at_zelda01 humphrey]$ gdb a.out core.20028
</span><br>
<span class="quotelev1">&gt; GNU gdb Red Hat Linux (6.3.0.0-1.62rh)
</span><br>
<span class="quotelev1">&gt; Copyright 2004 Free Software Foundation, Inc.
</span><br>
<span class="quotelev1">&gt; GDB is free software, covered by the GNU General Public License, and  
</span><br>
<span class="quotelev1">&gt; you are
</span><br>
<span class="quotelev1">&gt; welcome to change it and/or distribute copies of it under certain
</span><br>
<span class="quotelev1">&gt; conditions.
</span><br>
<span class="quotelev1">&gt; Type &quot;show copying&quot; to see the conditions.
</span><br>
<span class="quotelev1">&gt; There is absolutely no warranty for GDB.  Type &quot;show warranty&quot; for  
</span><br>
<span class="quotelev1">&gt; details.
</span><br>
<span class="quotelev1">&gt; This GDB was configured as &quot;i386-redhat-linux-gnu&quot;...Using host  
</span><br>
<span class="quotelev1">&gt; libthread_db
</span><br>
<span class="quotelev1">&gt; library &quot;/lib/tls/libthread_db.so.1&quot;.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Core was generated by `a.out'.
</span><br>
<span class="quotelev1">&gt; Program terminated with signal 11, Segmentation fault.
</span><br>
<span class="quotelev1">&gt; Reading symbols from  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/libmpi.so.0...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for /home/humphrey/ompi-install/lib/libmpi.so.0
</span><br>
<span class="quotelev1">&gt; Reading symbols from  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/liborte.so.0...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for /home/humphrey/ompi-install/lib/liborte.so.0
</span><br>
<span class="quotelev1">&gt; Reading symbols from  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/libopal.so.0...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for /home/humphrey/ompi-install/lib/libopal.so.0
</span><br>
<span class="quotelev1">&gt; Reading symbols from /lib/libutil.so.1...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for /lib/libutil.so.1
</span><br>
<span class="quotelev1">&gt; Reading symbols from /lib/libnsl.so.1...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for /lib/libnsl.so.1
</span><br>
<span class="quotelev1">&gt; Reading symbols from /lib/libdl.so.2...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for /lib/libdl.so.2
</span><br>
<span class="quotelev1">&gt; Reading symbols from /usr/lib/libaio.so.1...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for /usr/lib/libaio.so.1
</span><br>
<span class="quotelev1">&gt; Reading symbols from /usr/lib/libg2c.so.0...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for /usr/lib/libg2c.so.0
</span><br>
<span class="quotelev1">&gt; Reading symbols from /lib/tls/libm.so.6...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for /lib/tls/libm.so.6
</span><br>
<span class="quotelev1">&gt; Reading symbols from /lib/libgcc_s.so.1...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for /lib/libgcc_s.so.1
</span><br>
<span class="quotelev1">&gt; Reading symbols from /lib/tls/libpthread.so.0...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for /lib/tls/libpthread.so.0
</span><br>
<span class="quotelev1">&gt; Reading symbols from /lib/tls/libc.so.6...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for /lib/tls/libc.so.6
</span><br>
<span class="quotelev1">&gt; Reading symbols from /lib/ld-linux.so.2...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for /lib/ld-linux.so.2
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_paffinity_linux.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_paffinity_linux.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from /lib/libnss_files.so.2...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for /lib/libnss_files.so.2
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ns_proxy.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ns_proxy.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ns_replica.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ns_replica.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_rml_oob.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_rml_oob.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_oob_tcp.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_oob_tcp.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_gpr_null.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_gpr_null.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_gpr_proxy.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_gpr_proxy.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_gpr_replica.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_gpr_replica.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_rmgr_proxy.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_rmgr_proxy.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_rmgr_urm.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_rmgr_urm.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_rds_hostfile.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_rds_hostfile.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_rds_resfile.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_rds_resfile.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ras_dash_host.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ras_dash_host.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ras_hostfile.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ras_hostfile.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ras_localhost.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ras_localhost.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ras_slurm.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ras_slurm.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/ 
</span><br>
<span class="quotelev1">&gt; mca_rmaps_round_robin.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_rmaps_round_robin.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_pls_fork.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_pls_fork.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_pls_proxy.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_pls_proxy.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_pls_rsh.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_pls_rsh.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_pls_slurm.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_pls_slurm.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_iof_proxy.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_iof_proxy.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_allocator_basic.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_allocator_basic.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_allocator_bucket.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_allocator_bucket.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_rcache_rb.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_rcache_rb.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_mpool_sm.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_mpool_sm.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/libmca_common_sm.so.0...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/libmca_common_sm.so.0
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_pml_ob1.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_pml_ob1.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_bml_r2.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_bml_r2.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_btl_self.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_btl_self.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_btl_sm.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_btl_sm.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_btl_tcp.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_btl_tcp.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ptl_self.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ptl_self.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ptl_sm.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ptl_sm.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ptl_tcp.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_ptl_tcp.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_coll_basic.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_coll_basic.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_coll_hierarch.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_coll_hierarch.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_coll_self.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_coll_self.so
</span><br>
<span class="quotelev1">&gt; Reading symbols from
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_coll_sm.so...done.
</span><br>
<span class="quotelev1">&gt; Loaded symbols for  
</span><br>
<span class="quotelev1">&gt; /home/humphrey/ompi-install/lib/openmpi/mca_coll_sm.so
</span><br>
<span class="quotelev1">&gt; #0  0x009c4cbd in mca_btl_sm_add_procs_same_base_addr (btl=0x9c97c0,
</span><br>
<span class="quotelev1">&gt; nprocs=2, procs=0x8c28628, peers=0x8c28660,
</span><br>
<span class="quotelev1">&gt;     reachability=0xbfffde80) at btl_sm.c:412
</span><br>
<span class="quotelev1">&gt; 412             mca_btl_sm_component.sm_ctl_header-&gt;segment_header.
</span><br>
<span class="quotelev1">&gt; (gdb) bt
</span><br>
<span class="quotelev1">&gt; #0  0x009c4cbd in mca_btl_sm_add_procs_same_base_addr (btl=0x9c97c0,
</span><br>
<span class="quotelev1">&gt; nprocs=2, procs=0x8c28628, peers=0x8c28660,
</span><br>
<span class="quotelev1">&gt;     reachability=0xbfffde80) at btl_sm.c:412
</span><br>
<span class="quotelev1">&gt; #1  0x005e7245 in mca_bml_r2_add_procs (nprocs=2, procs=0x8c28628,
</span><br>
<span class="quotelev1">&gt; bml_endpoints=0x8c28608, reachable=0xbfffde80) at bml_r2.c:220
</span><br>
<span class="quotelev1">&gt; #2  0x00323671 in mca_pml_ob1_add_procs (procs=0x8c285f8, nprocs=2) at
</span><br>
<span class="quotelev1">&gt; pml_ob1.c:131
</span><br>
<span class="quotelev1">&gt; #3  0x00ed6e81 in ompi_mpi_init (argc=0, argv=0x0, requested=0,
</span><br>
<span class="quotelev1">&gt; provided=0xbfffdf2c) at runtime/ompi_mpi_init.c:396
</span><br>
<span class="quotelev1">&gt; #4  0x00f00c62 in PMPI_Init (argc=0xbfffdf60, argv=0xbfffdf5c) at  
</span><br>
<span class="quotelev1">&gt; pinit.c:71
</span><br>
<span class="quotelev1">&gt; #5  0x00f2b23b in mpi_init_f (ierr=0x8052580) at pinit_f.c:65
</span><br>
<span class="quotelev1">&gt; #6  0x08049362 in MAIN__ () at Halo.f:19
</span><br>
<span class="quotelev1">&gt; #7  0x0804b7e6 in main ()
</span><br>
<span class="quotelev1">&gt; (gdb) up
</span><br>
<span class="quotelev1">&gt; #1  0x005e7245 in mca_bml_r2_add_procs (nprocs=2, procs=0x8c28628,
</span><br>
<span class="quotelev1">&gt; bml_endpoints=0x8c28608, reachable=0xbfffde80) at bml_r2.c:220
</span><br>
<span class="quotelev1">&gt; 220             rc = btl-&gt;btl_add_procs(btl, n_new_procs, new_procs,
</span><br>
<span class="quotelev1">&gt; btl_endpoints, reachable);
</span><br>
<span class="quotelev1">&gt; (gdb) up
</span><br>
<span class="quotelev1">&gt; #2  0x00323671 in mca_pml_ob1_add_procs (procs=0x8c285f8, nprocs=2) at
</span><br>
<span class="quotelev1">&gt; pml_ob1.c:131
</span><br>
<span class="quotelev1">&gt; 131         rc = mca_bml.bml_add_procs(
</span><br>
<span class="quotelev1">&gt; (gdb) up
</span><br>
<span class="quotelev1">&gt; #3  0x00ed6e81 in ompi_mpi_init (argc=0, argv=0x0, requested=0,
</span><br>
<span class="quotelev1">&gt; provided=0xbfffdf2c) at runtime/ompi_mpi_init.c:396
</span><br>
<span class="quotelev1">&gt; 396         ret = MCA_PML_CALL(add_procs(procs, nprocs));
</span><br>
<span class="quotelev1">&gt; (gdb) up
</span><br>
<span class="quotelev1">&gt; #4  0x00f00c62 in PMPI_Init (argc=0xbfffdf60, argv=0xbfffdf5c) at  
</span><br>
<span class="quotelev1">&gt; pinit.c:71
</span><br>
<span class="quotelev1">&gt; 71            err = ompi_mpi_init(*argc, *argv, required, &amp;provided);
</span><br>
<span class="quotelev1">&gt; (gdb) up
</span><br>
<span class="quotelev1">&gt; #5  0x00f2b23b in mpi_init_f (ierr=0x8052580) at pinit_f.c:65
</span><br>
<span class="quotelev1">&gt; 65          *ierr = OMPI_INT_2_FINT(MPI_Init( &amp;argc, &amp;argv ));
</span><br>
<span class="quotelev1">&gt; (gdb) up
</span><br>
<span class="quotelev1">&gt; #6  0x08049362 in MAIN__ () at Halo.f:19
</span><br>
<span class="quotelev1">&gt; 19            CALL MPI_INIT(MPIERR)
</span><br>
<span class="quotelev1">&gt; Current language:  auto; currently fortran
</span><br>
<span class="quotelev1">&gt; (gdb)
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev2">&gt;&gt; -----Original Message-----
</span><br>
<span class="quotelev2">&gt;&gt; From: users-bounces_at_[hidden] [mailto:users-bounces_at_[hidden]]  
</span><br>
<span class="quotelev2">&gt;&gt; On
</span><br>
<span class="quotelev2">&gt;&gt; Behalf Of Jeff Squyres
</span><br>
<span class="quotelev2">&gt;&gt; Sent: Wednesday, November 09, 2005 10:41 PM
</span><br>
<span class="quotelev2">&gt;&gt; To: Open MPI Users
</span><br>
<span class="quotelev2">&gt;&gt; Subject: Re: [O-MPI users] can't get openmpi to run across twomulti-
</span><br>
<span class="quotelev2">&gt;&gt; NICmachines
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Sorry for the delay in replying -- it's a crazy week here preparing  
</span><br>
<span class="quotelev2">&gt;&gt; for
</span><br>
<span class="quotelev2">&gt;&gt; SC next week.
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; I'm double checking the code, and I don't see any obvious problems  
</span><br>
<span class="quotelev2">&gt;&gt; with
</span><br>
<span class="quotelev2">&gt;&gt; the btl tcp include stuff.
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Can you also specify that you want OMPI's &quot;out of band&quot; communication
</span><br>
<span class="quotelev2">&gt;&gt; to use a specific network?
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; mpiexec -d --mca btl_tcp_if_include eth0 --mca oob_tcp_include eth0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; -np 2 a.out
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; With the segv's, do you get meaningful core dumps?  Can you send
</span><br>
<span class="quotelev2">&gt;&gt; backtraces?
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; On Nov 8, 2005, at 3:02 PM, Marty Humphrey wrote:
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; It's taken me a while, but I've simplified the experiment...
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; In a nutshell, I'm seeing strange behavior in my multi-NIC box when I
</span><br>
<span class="quotelev3">&gt;&gt;&gt; attempt to execute &quot; mpiexec -d --mca btl_tcp_if_include eth0  -np 2
</span><br>
<span class="quotelev3">&gt;&gt;&gt; a.out&quot;.
</span><br>
<span class="quotelev3">&gt;&gt;&gt; I have three different observed behaviors:
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [1] mpi worker rank 0 displays the banner and then just hangs
</span><br>
<span class="quotelev3">&gt;&gt;&gt; (apparently
</span><br>
<span class="quotelev3">&gt;&gt;&gt; trying to exchange MPI messages, which don't get delivered)
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 2 PE'S AS A  2 BY  1 GRID
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [2] it starts progressing (spitting out domain-specific msgs):
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 2 PE'S AS A  2 BY  1 GRID
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   HALO2A  NPES,N =  2    2  TIME =  0.000007 SECONDS
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   HALO2A  NPES,N =  2    4  TIME =  0.000007 SECONDS
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   HALO2A  NPES,N =  2    8  TIME =  0.000007 SECONDS
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   HALO2A  NPES,N =  2   16  TIME =  0.000008 SECONDS
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   HALO2A  NPES,N =  2   32  TIME =  0.000009 SECONDS
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [3] I get failure pretty quickly, with the line &quot; mpiexec noticed  
</span><br>
<span class="quotelev3">&gt;&gt;&gt; that
</span><br>
<span class="quotelev3">&gt;&gt;&gt; job
</span><br>
<span class="quotelev3">&gt;&gt;&gt; rank 1 with PID 20425 on node &quot;localhost&quot; exited on signal 11.&quot;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Here's the output of &quot;ifconfig&quot;:
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [humphrey_at_zelda01 humphrey]$ /sbin/ifconfig
</span><br>
<span class="quotelev3">&gt;&gt;&gt; eth0      Link encap:Ethernet  HWaddr 00:11:43:DC:EA:EE
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           inet addr:130.207.252.131  Bcast:130.207.252.255
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Mask:255.255.255.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           RX packets:2441905 errors:0 dropped:0 overruns:0 frame:0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           TX packets:112786 errors:0 dropped:0 overruns:0 carrier:0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           collisions:0 txqueuelen:1000
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           RX bytes:197322445 (188.1 Mb)  TX bytes:32906750 (31.3 Mb)
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           Base address:0xecc0 Memory:dfae0000-dfb00000
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; eth2      Link encap:Ethernet  HWaddr 00:11:95:C7:28:82
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           inet addr:10.0.0.11  Bcast:10.0.0.255  Mask:255.255.255.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           RX packets:11598757 errors:0 dropped:0 overruns:0 frame:0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           TX packets:7224590 errors:0 dropped:0 overruns:0 carrier:0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           collisions:0 txqueuelen:1000
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           RX bytes:3491651158 (3329.8 Mb)  TX bytes:1916674000  
</span><br>
<span class="quotelev3">&gt;&gt;&gt; (1827.8
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Mb)
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           Interrupt:77 Base address:0xcc00
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; ipsec0    Link encap:Ethernet  HWaddr 00:11:43:DC:EA:EE
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           inet addr:130.207.252.131  Mask:255.255.255.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           UP RUNNING NOARP  MTU:16260  Metric:1
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           RX packets:40113 errors:0 dropped:40113 overruns:0 frame:0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           collisions:0 txqueuelen:10
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           RX bytes:0 (0.0 b)  TX bytes:0 (0.0 b)
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; lo        Link encap:Local Loopback
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           inet addr:127.0.0.1  Mask:255.0.0.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           UP LOOPBACK RUNNING  MTU:16436  Metric:1
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           RX packets:4742 errors:0 dropped:0 overruns:0 frame:0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           TX packets:4742 errors:0 dropped:0 overruns:0 carrier:0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           collisions:0 txqueuelen:0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;           RX bytes:2369841 (2.2 Mb)  TX bytes:2369841 (2.2 Mb)
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; This is with openmpi-1.1a1r8038 .
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Here is the output of a hanging invocation....
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; ----- begin hanging invocation ----
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [humphrey_at_zelda01 humphrey]$ mpiexec -d --mca btl_tcp_if_include eth0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; -np 2
</span><br>
<span class="quotelev3">&gt;&gt;&gt; a.out
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] procdir: (null)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] jobdir: (null)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] unidir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_zelda01.localdomain_0/default-universe
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] top:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; openmpi-sessions-humphrey_at_zelda01.localdomain_0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] tmp: /tmp
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] connect_uni: contact info read
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] connect_uni: connection not allowed
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] [0,0,0] setting up session dir with
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455]     tmpdir /tmp
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455]     universe default-universe-20455
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455]     user humphrey
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455]     host zelda01.localdomain
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455]     jobid 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455]     procid 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] procdir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_zelda01.localdomain_0/default-universe
</span><br>
<span class="quotelev3">&gt;&gt;&gt; -20455/
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 0/0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] jobdir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_zelda01.localdomain_0/default-universe
</span><br>
<span class="quotelev3">&gt;&gt;&gt; -20455/
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] unidir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_zelda01.localdomain_0/default-universe
</span><br>
<span class="quotelev3">&gt;&gt;&gt; -20455
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] top:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; openmpi-sessions-humphrey_at_zelda01.localdomain_0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] tmp: /tmp
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] [0,0,0] contact_file
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_zelda01.localdomain_0/default-universe
</span><br>
<span class="quotelev3">&gt;&gt;&gt; -20455/
</span><br>
<span class="quotelev3">&gt;&gt;&gt; universe-setup.txt
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] [0,0,0] wrote setup file
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] pls:rsh: local csh: 0, local bash: 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] pls:rsh: assuming same remote shell as
</span><br>
<span class="quotelev3">&gt;&gt;&gt; local
</span><br>
<span class="quotelev3">&gt;&gt;&gt; shell
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] pls:rsh: remote csh: 0, remote bash: 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] pls:rsh: final template argv:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] pls:rsh:     ssh &lt;template&gt; orted --debug
</span><br>
<span class="quotelev3">&gt;&gt;&gt; --bootproxy 1 --name &lt;template&gt; --num_procs 2 --vpid_start 0  
</span><br>
<span class="quotelev3">&gt;&gt;&gt; --nodename
</span><br>
<span class="quotelev3">&gt;&gt;&gt; &lt;template&gt; --universe
</span><br>
<span class="quotelev3">&gt;&gt;&gt; humphrey_at_zelda01.localdomain:default-universe-20455
</span><br>
<span class="quotelev3">&gt;&gt;&gt; --nsreplica
</span><br>
<span class="quotelev3">&gt;&gt;&gt; &quot;0.0.0;tcp://130.207.252.131:35465;tcp://10.0.0.11:35465;tcp://
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 130.207.252.1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 31:35465&quot; --gprreplica
</span><br>
<span class="quotelev3">&gt;&gt;&gt; &quot;0.0.0;tcp://130.207.252.131:35465;tcp://10.0.0.11:35465;tcp://
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 130.207.252.1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 31:35465&quot; --mpi-call-yield 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] pls:rsh: launching on node localhost
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] pls:rsh: oversubscribed -- setting
</span><br>
<span class="quotelev3">&gt;&gt;&gt; mpi_yield_when_idle to 1 (1 2)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] pls:rsh: localhost is a LOCAL node
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] pls:rsh: executing: orted --debug
</span><br>
<span class="quotelev3">&gt;&gt;&gt; --bootproxy 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; --name 0.0.1 --num_procs 2 --vpid_start 0 --nodename localhost
</span><br>
<span class="quotelev3">&gt;&gt;&gt; --universe
</span><br>
<span class="quotelev3">&gt;&gt;&gt; humphrey_at_zelda01.localdomain:default-universe-20455 --nsreplica
</span><br>
<span class="quotelev3">&gt;&gt;&gt; &quot;0.0.0;tcp://130.207.252.131:35465;tcp://10.0.0.11:35465;tcp://
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 130.207.252.1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 31:35465&quot; --gprreplica
</span><br>
<span class="quotelev3">&gt;&gt;&gt; &quot;0.0.0;tcp://130.207.252.131:35465;tcp://10.0.0.11:35465;tcp://
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 130.207.252.1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 31:35465&quot; --mpi-call-yield 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20456] [0,0,1] setting up session dir with
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20456]     universe default-universe-20455
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20456]     user humphrey
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20456]     host localhost
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20456]     jobid 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20456]     procid 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20456] procdir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20455/0/1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20456] jobdir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20455/0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20456] unidir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20455
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20456] top:  
</span><br>
<span class="quotelev3">&gt;&gt;&gt; openmpi-sessions-humphrey_at_localhost_0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20456] tmp: /tmp
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20457] [0,1,1] setting up session dir with
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20457]     universe default-universe-20455
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20457]     user humphrey
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20457]     host localhost
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20457]     jobid 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20457]     procid 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20457] procdir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20455/1/1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20457] jobdir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20455/1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20457] unidir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20455
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20457] top:  
</span><br>
<span class="quotelev3">&gt;&gt;&gt; openmpi-sessions-humphrey_at_localhost_0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20457] tmp: /tmp
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20458] [0,1,0] setting up session dir with
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20458]     universe default-universe-20455
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20458]     user humphrey
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20458]     host localhost
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20458]     jobid 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20458]     procid 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20458] procdir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20455/1/0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20458] jobdir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20455/1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20458] unidir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20455
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20458] top:  
</span><br>
<span class="quotelev3">&gt;&gt;&gt; openmpi-sessions-humphrey_at_localhost_0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20458] tmp: /tmp
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] spawn: in job_state_callback(jobid = 1,
</span><br>
<span class="quotelev3">&gt;&gt;&gt; state =
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 0x3)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] Info: Setting up debugger process table  
</span><br>
<span class="quotelev3">&gt;&gt;&gt; for
</span><br>
<span class="quotelev3">&gt;&gt;&gt; applications
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   MPIR_being_debugged = 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   MPIR_debug_gate = 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   MPIR_debug_state = 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   MPIR_acquired_pre_main = 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   MPIR_i_am_starter = 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   MPIR_proctable_size = 2
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   MPIR_proctable:
</span><br>
<span class="quotelev3">&gt;&gt;&gt;     (i, host, exe, pid) = (0, localhost, /home/humphrey/a.out, 20457)
</span><br>
<span class="quotelev3">&gt;&gt;&gt;     (i, host, exe, pid) = (1, localhost, /home/humphrey/a.out, 20458)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20455] spawn: in job_state_callback(jobid = 1,
</span><br>
<span class="quotelev3">&gt;&gt;&gt; state =
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 0x4)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20458] [0,1,0] ompi_mpi_init completed
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20457] [0,1,1] ompi_mpi_init completed
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;  2 PE'S AS A  2 BY  1 GRID
</span><br>
<span class="quotelev3">&gt;&gt;&gt; ------ end hanging invocation  -----
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Here's the 1-in-approximately-20 that started working...
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; ------- begin non-hanging invocation -----
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [humphrey_at_zelda01 humphrey]$ mpiexec -d --mca btl_tcp_if_include eth0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; -np 2
</span><br>
<span class="quotelev3">&gt;&gt;&gt; a.out
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] procdir: (null)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] jobdir: (null)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] unidir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_zelda01.localdomain_0/default-universe
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] top:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; openmpi-sessions-humphrey_at_zelda01.localdomain_0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] tmp: /tmp
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] connect_uni: contact info read
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] connect_uni: connection not allowed
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] [0,0,0] setting up session dir with
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659]     tmpdir /tmp
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659]     universe default-universe-20659
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659]     user humphrey
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659]     host zelda01.localdomain
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659]     jobid 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659]     procid 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] procdir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_zelda01.localdomain_0/default-universe
</span><br>
<span class="quotelev3">&gt;&gt;&gt; -20659/
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 0/0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] jobdir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_zelda01.localdomain_0/default-universe
</span><br>
<span class="quotelev3">&gt;&gt;&gt; -20659/
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] unidir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_zelda01.localdomain_0/default-universe
</span><br>
<span class="quotelev3">&gt;&gt;&gt; -20659
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] top:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; openmpi-sessions-humphrey_at_zelda01.localdomain_0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] tmp: /tmp
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] [0,0,0] contact_file
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_zelda01.localdomain_0/default-universe
</span><br>
<span class="quotelev3">&gt;&gt;&gt; -20659/
</span><br>
<span class="quotelev3">&gt;&gt;&gt; universe-setup.txt
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] [0,0,0] wrote setup file
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] pls:rsh: local csh: 0, local bash: 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] pls:rsh: assuming same remote shell as
</span><br>
<span class="quotelev3">&gt;&gt;&gt; local
</span><br>
<span class="quotelev3">&gt;&gt;&gt; shell
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] pls:rsh: remote csh: 0, remote bash: 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] pls:rsh: final template argv:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] pls:rsh:     ssh &lt;template&gt; orted --debug
</span><br>
<span class="quotelev3">&gt;&gt;&gt; --bootproxy 1 --name &lt;template&gt; --num_procs 2 --vpid_start 0  
</span><br>
<span class="quotelev3">&gt;&gt;&gt; --nodename
</span><br>
<span class="quotelev3">&gt;&gt;&gt; &lt;template&gt; --universe
</span><br>
<span class="quotelev3">&gt;&gt;&gt; humphrey_at_zelda01.localdomain:default-universe-20659
</span><br>
<span class="quotelev3">&gt;&gt;&gt; --nsreplica
</span><br>
<span class="quotelev3">&gt;&gt;&gt; &quot;0.0.0;tcp://130.207.252.131:35654;tcp://10.0.0.11:35654;tcp://
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 130.207.252.1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 31:35654&quot; --gprreplica
</span><br>
<span class="quotelev3">&gt;&gt;&gt; &quot;0.0.0;tcp://130.207.252.131:35654;tcp://10.0.0.11:35654;tcp://
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 130.207.252.1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 31:35654&quot; --mpi-call-yield 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] pls:rsh: launching on node localhost
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] pls:rsh: oversubscribed -- setting
</span><br>
<span class="quotelev3">&gt;&gt;&gt; mpi_yield_when_idle to 1 (1 2)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] pls:rsh: localhost is a LOCAL node
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] pls:rsh: executing: orted --debug
</span><br>
<span class="quotelev3">&gt;&gt;&gt; --bootproxy 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; --name 0.0.1 --num_procs 2 --vpid_start 0 --nodename localhost
</span><br>
<span class="quotelev3">&gt;&gt;&gt; --universe
</span><br>
<span class="quotelev3">&gt;&gt;&gt; humphrey_at_zelda01.localdomain:default-universe-20659 --nsreplica
</span><br>
<span class="quotelev3">&gt;&gt;&gt; &quot;0.0.0;tcp://130.207.252.131:35654;tcp://10.0.0.11:35654;tcp://
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 130.207.252.1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 31:35654&quot; --gprreplica
</span><br>
<span class="quotelev3">&gt;&gt;&gt; &quot;0.0.0;tcp://130.207.252.131:35654;tcp://10.0.0.11:35654;tcp://
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 130.207.252.1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 31:35654&quot; --mpi-call-yield 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660] [0,0,1] setting up session dir with
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660]     universe default-universe-20659
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660]     user humphrey
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660]     host localhost
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660]     jobid 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660]     procid 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660] procdir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20659/0/1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660] jobdir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20659/0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660] unidir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20659
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660] top:  
</span><br>
<span class="quotelev3">&gt;&gt;&gt; openmpi-sessions-humphrey_at_localhost_0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660] tmp: /tmp
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20661] [0,1,1] setting up session dir with
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20661]     universe default-universe-20659
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20661]     user humphrey
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20661]     host localhost
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20661]     jobid 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20661]     procid 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20661] procdir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20659/1/1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20661] jobdir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20659/1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20661] unidir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20659
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20661] top:  
</span><br>
<span class="quotelev3">&gt;&gt;&gt; openmpi-sessions-humphrey_at_localhost_0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20661] tmp: /tmp
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20662] [0,1,0] setting up session dir with
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20662]     universe default-universe-20659
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20662]     user humphrey
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20662]     host localhost
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20662]     jobid 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20662]     procid 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20662] procdir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20659/1/0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20662] jobdir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20659/1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20662] unidir:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; /tmp/openmpi-sessions-humphrey_at_localhost_0/default-universe-20659
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20662] top:  
</span><br>
<span class="quotelev3">&gt;&gt;&gt; openmpi-sessions-humphrey_at_localhost_0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20662] tmp: /tmp
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] spawn: in job_state_callback(jobid = 1,
</span><br>
<span class="quotelev3">&gt;&gt;&gt; state =
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 0x3)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] Info: Setting up debugger process table  
</span><br>
<span class="quotelev3">&gt;&gt;&gt; for
</span><br>
<span class="quotelev3">&gt;&gt;&gt; applications
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   MPIR_being_debugged = 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   MPIR_debug_gate = 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   MPIR_debug_state = 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   MPIR_acquired_pre_main = 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   MPIR_i_am_starter = 0
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   MPIR_proctable_size = 2
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   MPIR_proctable:
</span><br>
<span class="quotelev3">&gt;&gt;&gt;     (i, host, exe, pid) = (0, localhost, /home/humphrey/a.out, 20661)
</span><br>
<span class="quotelev3">&gt;&gt;&gt;     (i, host, exe, pid) = (1, localhost, /home/humphrey/a.out, 20662)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] spawn: in job_state_callback(jobid = 1,
</span><br>
<span class="quotelev3">&gt;&gt;&gt; state =
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 0x4)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20662] [0,1,0] ompi_mpi_init completed
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20661] [0,1,1] ompi_mpi_init completed
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;  2 PE'S AS A  2 BY  1 GRID
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   HALO2A  NPES,N =  2    2  TIME =  0.000007 SECONDS
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   HALO2A  NPES,N =  2    4  TIME =  0.000007 SECONDS
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   HALO2A  NPES,N =  2    8  TIME =  0.000007 SECONDS
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   HALO2A  NPES,N =  2   16  TIME =  0.000008 SECONDS
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   HALO2A  NPES,N =  2   32  TIME =  0.000009 SECONDS
</span><br>
<span class="quotelev3">&gt;&gt;&gt;   HALO2A  NPES,N =  2   64  TIME =  0.000011 SECONDS
</span><br>
<span class="quotelev3">&gt;&gt;&gt; mpiexec: killing job...
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Interrupt
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Interrupt
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660] sess_dir_finalize: found proc session dir
</span><br>
<span class="quotelev3">&gt;&gt;&gt; empty
</span><br>
<span class="quotelev3">&gt;&gt;&gt; - deleting
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660] sess_dir_finalize: job session dir not
</span><br>
<span class="quotelev3">&gt;&gt;&gt; empty -
</span><br>
<span class="quotelev3">&gt;&gt;&gt; leaving
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660] sess_dir_finalize: found proc session dir
</span><br>
<span class="quotelev3">&gt;&gt;&gt; empty
</span><br>
<span class="quotelev3">&gt;&gt;&gt; - deleting
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660] sess_dir_finalize: found job session dir
</span><br>
<span class="quotelev3">&gt;&gt;&gt; empty -
</span><br>
<span class="quotelev3">&gt;&gt;&gt; deleting
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660] sess_dir_finalize: univ session dir not
</span><br>
<span class="quotelev3">&gt;&gt;&gt; empty -
</span><br>
<span class="quotelev3">&gt;&gt;&gt; leaving
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] spawn: in job_state_callback(jobid = 1,
</span><br>
<span class="quotelev3">&gt;&gt;&gt; state =
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 0xa)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660] orted: job_state_callback(jobid = 1,  
</span><br>
<span class="quotelev3">&gt;&gt;&gt; state
</span><br>
<span class="quotelev3">&gt;&gt;&gt; =
</span><br>
<span class="quotelev3">&gt;&gt;&gt; ORTE_PROC_STATE_ABORTED)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] spawn: in job_state_callback(jobid = 1,
</span><br>
<span class="quotelev3">&gt;&gt;&gt; state =
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 0x9)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 2 processes killed (possibly by Open MPI)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660] orted: job_state_callback(jobid = 1,  
</span><br>
<span class="quotelev3">&gt;&gt;&gt; state
</span><br>
<span class="quotelev3">&gt;&gt;&gt; =
</span><br>
<span class="quotelev3">&gt;&gt;&gt; ORTE_PROC_STATE_TERMINATED)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660] sess_dir_finalize: found proc session dir
</span><br>
<span class="quotelev3">&gt;&gt;&gt; empty
</span><br>
<span class="quotelev3">&gt;&gt;&gt; - deleting
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660] sess_dir_finalize: found job session dir
</span><br>
<span class="quotelev3">&gt;&gt;&gt; empty -
</span><br>
<span class="quotelev3">&gt;&gt;&gt; deleting
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660] sess_dir_finalize: found univ session dir
</span><br>
<span class="quotelev3">&gt;&gt;&gt; empty
</span><br>
<span class="quotelev3">&gt;&gt;&gt; - deleting
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20660] sess_dir_finalize: found top session dir
</span><br>
<span class="quotelev3">&gt;&gt;&gt; empty -
</span><br>
<span class="quotelev3">&gt;&gt;&gt; deleting
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] sess_dir_finalize: found proc session dir
</span><br>
<span class="quotelev3">&gt;&gt;&gt; empty
</span><br>
<span class="quotelev3">&gt;&gt;&gt; - deleting
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] sess_dir_finalize: found job session dir
</span><br>
<span class="quotelev3">&gt;&gt;&gt; empty -
</span><br>
<span class="quotelev3">&gt;&gt;&gt; deleting
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] sess_dir_finalize: found univ session dir
</span><br>
<span class="quotelev3">&gt;&gt;&gt; empty
</span><br>
<span class="quotelev3">&gt;&gt;&gt; - deleting
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [zelda01.localdomain:20659] sess_dir_finalize: top session dir not
</span><br>
<span class="quotelev3">&gt;&gt;&gt; empty -
</span><br>
<span class="quotelev3">&gt;&gt;&gt; leaving
</span><br>
<span class="quotelev3">&gt;&gt;&gt; [humphrey_at_zelda01 humphrey]$
</span><br>
<span class="quotelev3">&gt;&gt;&gt; -------- end non-hanging invocation ------
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Any thoughts?
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; -- Marty
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; -----Original Message-----
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; From: users-bounces_at_[hidden] [mailto:users-bounces_at_[hidden]]
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; On
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Behalf Of Jeff Squyres
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Sent: Tuesday, November 01, 2005 2:17 PM
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; To: Open MPI Users
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Subject: Re: [O-MPI users] can't get openmpi to run across two  
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; multi-
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; NICmachines
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; On Nov 1, 2005, at 12:02 PM, Marty Humphrey wrote:
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; wukong: eth0 (152.48.249.102, no MPI traffic), eth1
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; (128.109.34.20,yes
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; MPI
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; traffic)
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; zelda01: eth0 (130.207.252.131, yes MPI traffic), eth2 (10.0.0.12,  
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; no
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; MPI
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; traffic)
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; on wukong, I have :
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [humphrey_at_wukong ~]$ more ~/.openmpi/mca-params.conf
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; btl_tcp_if_include=eth1
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; on zelda01, I have :
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [humphrey_at_zelda01 humphrey]$ more ~/.openmpi/mca-params.conf
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; btl_tcp_if_include=eth0
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Just to make sure I'm reading this right -- 128.109.34.20 is  
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; supposed
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; to be routable to 130.207.252.131, right?  Can you ssh directly from
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; one machine to the other?  (I'm guessing that you can because OMPI  
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; was
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; able to start processes)  Can you ping one machine from the other?
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Most importantly -- can you open arbitrary TCP ports between the two
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; machines?  (i.e., not just well-known ports like 22 [ssh], etc.)
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; --
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; {+} Jeff Squyres
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; {+} The Open MPI Project
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; {+} <a href="http://www.open-mpi.org/">http://www.open-mpi.org/</a>
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; _______________________________________________
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; users mailing list
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; _______________________________________________
</span><br>
<span class="quotelev3">&gt;&gt;&gt; users mailing list
</span><br>
<span class="quotelev3">&gt;&gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev3">&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; --
</span><br>
<span class="quotelev2">&gt;&gt; {+} Jeff Squyres
</span><br>
<span class="quotelev2">&gt;&gt; {+} The Open MPI Project
</span><br>
<span class="quotelev2">&gt;&gt; {+} <a href="http://www.open-mpi.org/">http://www.open-mpi.org/</a>
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; _______________________________________________
</span><br>
<span class="quotelev2">&gt;&gt; users mailing list
</span><br>
<span class="quotelev2">&gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev2">&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; _______________________________________________
</span><br>
<span class="quotelev1">&gt; users mailing list
</span><br>
<span class="quotelev1">&gt; users_at_[hidden]
</span><br>
<span class="quotelev1">&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1">&gt;
</span><br>
<p><pre>
-- 
{+} Jeff Squyres
{+} The Open MPI Project
{+} <a href="http://www.open-mpi.org/">http://www.open-mpi.org/</a>
</pre>
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="0327.php">Jeff Squyres: "Re: [O-MPI users] 1.0rc5 is up"</a>
<li><strong>Previous message:</strong> <a href="0325.php">Brian Barrett: "Re: [O-MPI users] Synchronizing C++ STL objects"</a>
<li><strong>In reply to:</strong> <a href="0306.php">Marty Humphrey: "Re: [O-MPI users] can't get openmpi to run across twomulti-NICmachines"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
