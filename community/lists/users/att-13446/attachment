<br><br><div class="gmail_quote"><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div bgcolor="#ffffff" text="#000000">
and it&#39;s conceivable that you might have better performance with<br>
<br>
    CALL MPI_ISEND()<br>
    DO I = 1, N<br>
        call do_a_little_of_my_work()  ! no MPI progress is being made
here<br>
        CALL MPI_TEST()            ! enough MPI progress is being made
here that the receiver has something to do<br>
    END DO<br>
    CALL MPI_WAIT()<br>
<br>
Whether performance improves or not is not guaranteed by the MPI
standard.<div class="im"><br>
<blockquote cite="http://midAANLkTinls8gUyzGkYEfPKYN_eKDw6lhjPUQpancykqzA@mail.gmail.com" type="cite">And the SECOND desire is to use Persistent communication
for even better speedup.<br>
</blockquote></div>
Right.  That&#39;s a separate issue.</div><br></blockquote><div><br> </div></div>So actually I am focusing on the persistent communication at this time. Based on your suggestions, I developed:<br><br>sending, receiving buffers, and the request array is defined in declared in the global module. And their sizes are allocated in the main program. But following is not working. Segmentation fault messages at just from the underline blue line lace.<br>
<br><b style="color: rgb(51, 204, 0);">Main program starts------@@@@@@@@@@@@@@@@@@@@@@@.</b><br style="color: rgb(51, 204, 0);"><b style="color: rgb(51, 204, 0);"><br></b><b style="color: rgb(51, 51, 255);"><u>CALL MPI_RECV_INIT for each neighboring process  </u></b><b style="color: rgb(51, 204, 0);"><br>

CALL MPI_SEND_INIT for each neighboring process</b><br><b style="color: rgb(51, 204, 0);">Loop Calling the subroutine1--------------------(10000 times in the main program).<br><br></b><b style="color: rgb(51, 204, 0);"> Call subroutine1</b><br style="color: rgb(51, 204, 0);">

<b style="color: rgb(204, 0, 0);"><br></b><b style="color: rgb(204, 0, 0);">Subroutine1 starts===================================</b><br>
<b style="color: rgb(204, 0, 0);"><div class="im">   Loop A starts here &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; (three passes)<br></div>   Call subroutine2<br><br>   Subroutine2 starts----------------------------<div class="im">
<br>

         <span style="color: rgb(255, 0, 0);">Pick local data from array U in separate arrays for each neighboring processor</span><br style="color: rgb(255, 0, 0);"></div><span style="color: rgb(255, 0, 0);">         CALL MPI_STARTALL</span><br style="color: rgb(255, 0, 0);">


<span style="color: rgb(255, 0, 0);"></span><span style="color: rgb(255, 0, 0);">         -------perform work that could be done with local data</span><br style="color: rgb(255, 0, 0);">
<span style="color: rgb(255, 0, 0);">         CALL MPI_WAITALL( )</span><br style="color: rgb(255, 0, 0);"><span style="color: rgb(255, 0, 0);">         -------perform work using the received data</span><br style="color: rgb(255, 0, 0);">


   Subroutine</b><b style="color: rgb(204, 0, 0);">2</b><b style="color: rgb(204, 0, 0);"> ends</b><b style="color: rgb(204, 0, 0);">----------------------------</b><div class="im"><br><br><b style="color: rgb(204, 0, 0);"><span style="color: rgb(255, 0, 0);">         -------perform work to update array U</span></b><br>


<b style="color: rgb(204, 0, 0);">   Loop A ends here &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</b><br></div><b style="color: rgb(204, 0, 0);">Subroutine1 ends====================================</b><br>

<br><b style="color: rgb(51, 204, 0);">Loop Calling the subroutine1 ends------------(10000 times in the main program).</b><br style="color: rgb(51, 204, 0);"><br style="color: rgb(51, 204, 0);"><b style="color: rgb(51, 204, 0);">CALL MPI_Request_free( )</b><br style="color: rgb(51, 204, 0);">

<br style="color: rgb(51, 204, 0);"><b style="color: rgb(51, 204, 0);">Main program ends------@@@@@@@@@@@@@@@@@@@@@@@.</b><br><br>How to tackle all this.<br>

