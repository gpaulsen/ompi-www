<div dir="ltr"><div><div><div>Are you wanting to run the solvers on different nodes within the allocation? Or on different cores across all nodes?<br><br></div>For different nodes, you can just use -host to specify which nodes you want that specific mpirun to use, or a hostfile should also be fine. The FAQ&#39;s comment was aimed at people who were giving us the PBS_NODEFILE as the hostfile - which could confuse older versions of OMPI into using the rsh launcher instead of Torque. Remember that we have the relative node syntax so you don&#39;t actually have to name the nodes - helps if you want to execute batch scripts and won&#39;t know the node names in advance.<br>
<br></div>For different cores across all nodes, you would need to use some binding trickery that may not be in the 1.4 series, so you might need to update to the 1.6 series. You have two options: (a) have Torque bind your mpirun to specific cores (I believe it can do that), or (b) use --slot-list to specify which cores that particular mpirun is to use. You can then separate the two solvers but still run on all the nodes, if that is of concern.<br>
<br></div>HTH<br>Ralph<br><br></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Wed, Nov 27, 2013 at 6:10 AM,  <span dir="ltr">&lt;<a href="mailto:Ola.Widlund@se.abb.com" target="_blank">Ola.Widlund@se.abb.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><font face="sans-serif">Hi,</font>
<br>
<br><font face="sans-serif">We have an in-house application where
we run two solvers in a loosely coupled manner: The first solver runs a
timestep, then the second solver does work on the same timestep, etc. As
the two solvers never execute at the same time, we would like to run the
two solvers in the same allocation (launching mpirun once for each of them).
RAM is not an issue, so there should not be any risk of excessive swapping
degrading performance.</font>
<br>
<br><font face="sans-serif">We use openmpi-1.4.5 compiled with torque
integration. The torque integration means we do not give a hostfile to
mpirun, it will itself query torque for the allocation info. </font>
<br>
<br><font face="sans-serif">Question: </font>
<br>
<br><font face="sans-serif">Can we force one of the solvers to run
in a *subset* of the full allocation? How do we do that? I read in the
FAQ that providing a hostfile to mpirun in this case (when it&#39;s not needed
due to torque integration) would cause a lot of problems...</font>
<br>
<br><font face="sans-serif">Thanks in advance,</font>
<br>
<br><font face="sans-serif">Ola</font>
<br><font face="sans-serif"><br>
</font><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>

