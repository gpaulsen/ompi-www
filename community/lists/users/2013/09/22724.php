<?
$subject_val = "[OMPI users] help";
include("../../include/msg-header.inc");
?>
<!-- received="Thu Sep 26 00:28:25 2013" -->
<!-- isoreceived="20130926042825" -->
<!-- sent="Thu, 26 Sep 2013 12:28:21 +0800 (SGT)" -->
<!-- isosent="20130926042821" -->
<!-- name="sri pramoda" -->
<!-- email="sri_pramoda_at_[hidden]" -->
<!-- subject="[OMPI users] help" -->
<!-- id="1380169701.67326.YahooMailBasic_at_web190802.mail.sg3.yahoo.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="mailman.26.1379692804.28240.users_at_open-mpi.org" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>Subject:</strong> [OMPI users] help<br>
<strong>From:</strong> sri pramoda (<em>sri_pramoda_at_[hidden]</em>)<br>
<strong>Date:</strong> 2013-09-26 00:28:21
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="22725.php">Syed Ahsan Ali: "Re: [OMPI users] compilation aborted for Handler.cpp (code 2)"</a>
<li><strong>Previous message:</strong> <a href="22723.php">Jeff Squyres (jsquyres): "Re: [OMPI users] OpenMPI 1.6.3 problem"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
&nbsp;&nbsp;Help : unsubscribe
<br>
<p><p>--------------------------------------------
<br>
On Fri, 20/9/13, users-request_at_[hidden] &lt;users-request_at_[hidden]&gt; wrote:
<br>
<p>&nbsp;Subject: users Digest, Vol 2685, Issue 2
<br>
&nbsp;To: users_at_[hidden]
<br>
&nbsp;Date: Friday, 20 September, 2013, 11:00 AM
<br>
&nbsp;
<br>
&nbsp;Send users mailing list submissions
<br>
&nbsp;to
<br>
&nbsp;&#160;&#160;&#160; users_at_[hidden]
<br>
&nbsp;
<br>
&nbsp;To subscribe or unsubscribe via the World Wide Web, visit
<br>
&nbsp;&#160;&#160;&#160; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
<br>
&nbsp;or, via email, send a message with subject or body 'help'
<br>
&nbsp;to
<br>
&nbsp;&#160;&#160;&#160; users-request_at_[hidden]
<br>
&nbsp;
<br>
&nbsp;You can reach the person managing the list at
<br>
&nbsp;&#160;&#160;&#160; users-owner_at_[hidden]
<br>
&nbsp;
<br>
&nbsp;When replying, please edit your Subject line so it is more
<br>
&nbsp;specific
<br>
&nbsp;than &quot;Re: Contents of users digest...&quot;
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;Today's Topics:
<br>
&nbsp;
<br>
&nbsp;&#160;&#160;&#160;1. error building openmpi-1.7.3a1r29213 on
<br>
&nbsp;Solaris (Siegmar Gross)
<br>
&nbsp;&#160;&#160;&#160;2. intermittent node file error running
<br>
&nbsp;with torque/maui
<br>
&nbsp;&#160; &#160; &#160; integration (Noam Bernstein)
<br>
&nbsp;&#160;&#160;&#160;3. Re: intermittent node file error
<br>
&nbsp;running with&#160;&#160;&#160; torque/maui
<br>
&nbsp;&#160; &#160; &#160; integration (Noam Bernstein)
<br>
&nbsp;&#160;&#160;&#160;4. Re: intermittent node file error
<br>
&nbsp;running with&#160;&#160;&#160; torque/maui
<br>
&nbsp;&#160; &#160; &#160; integration (Noam Bernstein)
<br>
&nbsp;&#160;&#160;&#160;5. Re: intermittent node file error
<br>
&nbsp;running with&#160;&#160;&#160; torque/maui
<br>
&nbsp;&#160; &#160; &#160; integration (Reuti)
<br>
&nbsp;&#160;&#160;&#160;6. Re: intermittent node file error
<br>
&nbsp;running with&#160;&#160;&#160; torque/maui
<br>
&nbsp;&#160; &#160; &#160; integration (Noam Bernstein)
<br>
&nbsp;&#160;&#160;&#160;7. Re: intermittent node file error
<br>
&nbsp;running with&#160;&#160;&#160; torque/maui
<br>
&nbsp;&#160; &#160; &#160; integration (Noam Bernstein)
<br>
&nbsp;&#160;&#160;&#160;8. Debugging Runtime/Ethernet Problems
<br>
&nbsp;(Lloyd Brown)
<br>
&nbsp;&#160;&#160;&#160;9. Re: Debugging Runtime/Ethernet Problems
<br>
&nbsp;(Elken, Tom)
<br>
&nbsp;&#160; 10. Re: Debugging Runtime/Ethernet Problems (Ralph
<br>
&nbsp;Castain)
<br>
&nbsp;&#160; 11. Re: compilation aborted for Handler.cpp (code 2)
<br>
&nbsp;&#160; &#160; &#160; (Jeff Squyres (jsquyres))
<br>
&nbsp;&#160; 12. Re: Debugging Runtime/Ethernet Problems (Jeff
<br>
&nbsp;Squyres (jsquyres))
<br>
&nbsp;&#160; 13. Re: compilation aborted for Handler.cpp (code 2)
<br>
&nbsp;&#160; &#160; &#160; (Jeff Squyres (jsquyres))
<br>
&nbsp;&#160; 14. Re: intermittent node file error running with
<br>
&nbsp;torque/maui
<br>
&nbsp;&#160; &#160; &#160; integration (Gus Correa)
<br>
&nbsp;&#160; 15. Re: error building openmpi-1.7.3a1r29213 on
<br>
&nbsp;Solaris
<br>
&nbsp;&#160; &#160; &#160; (Jeff Squyres (jsquyres))
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;----------------------------------------------------------------------
<br>
&nbsp;
<br>
&nbsp;Message: 1
<br>
&nbsp;Date: Fri, 20 Sep 2013 13:00:41 +0200 (CEST)
<br>
&nbsp;From: Siegmar Gross &lt;Siegmar.Gross_at_[hidden]&gt;
<br>
&nbsp;To: users_at_[hidden]
<br>
&nbsp;Subject: [OMPI users] error building openmpi-1.7.3a1r29213
<br>
&nbsp;on Solaris
<br>
&nbsp;Message-ID: &lt;201309201100.r8KB0fTr022555_at_[hidden]&gt;
<br>
&nbsp;Content-Type: TEXT/plain; charset=us-ascii
<br>
&nbsp;
<br>
&nbsp;Hi,
<br>
&nbsp;
<br>
&nbsp;I tried to install openmpi-1.7.3a1r29213 on &quot;openSuSE Linux
<br>
&nbsp;12.1&quot;,
<br>
&nbsp;&quot;Solaris 10 x86_64&quot;, and &quot;Solaris 10 sparc&quot; with &quot;Sun C
<br>
&nbsp;5.12&quot; and
<br>
&nbsp;gcc-4.8.0 in 64-bit mode. Unfortunately &quot;make&quot; breaks with
<br>
&nbsp;the same
<br>
&nbsp;error for both compilers on both Solaris platforms.
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;tyr openmpi-1.7.3a1r29213-SunOS.sparc.64_cc 126 tail -10 \
<br>
&nbsp;&#160; log.make.SunOS.sparc.64_cc
<br>
&nbsp;Making all in mca/if/posix_ipv4
<br>
&nbsp;make[2]: Entering directory `.../opal/mca/if/posix_ipv4'
<br>
&nbsp;&#160; CC&#160; &#160; &#160;&#160;&#160;if_posix.lo
<br>
&nbsp;&quot;../../../../../openmpi-1.7.3a1r29213/opal/mca/if/posix_ipv4/if_posix.c&quot;,
<br>
&nbsp;&#160; line 277: undefined struct/union member: ifr_mtu
<br>
&nbsp;cc: acomp failed for
<br>
&nbsp;&#160;
<br>
&nbsp;../../../../../openmpi-1.7.3a1r29213/opal/mca/if/posix_ipv4/if_posix.c
<br>
&nbsp;make[2]: *** [if_posix.lo] Error 1
<br>
&nbsp;make[2]: Leaving directory `.../opal/mca/if/posix_ipv4'
<br>
&nbsp;make[1]: *** [all-recursive] Error 1
<br>
&nbsp;make[1]: Leaving directory `.../opal'
<br>
&nbsp;make: *** [all-recursive] Error 1
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;tyr openmpi-1.7.3a1r29213-SunOS.sparc.64_gcc 131 tail -12 \
<br>
&nbsp;&#160; log.make.SunOS.sparc.64_gcc
<br>
&nbsp;Making all in mca/if/posix_ipv4
<br>
&nbsp;make[2]: Entering directory `.../opal/mca/if/posix_ipv4'
<br>
&nbsp;&#160; CC&#160; &#160; &#160;&#160;&#160;if_posix.lo
<br>
&nbsp;../../../../../openmpi-1.7.3a1r29213/opal/mca/if/posix_ipv4/if_posix.c:
<br>
&nbsp;&#160; In function 'if_posix_open':
<br>
&nbsp;../../../../../openmpi-1.7.3a1r29213/opal/mca/if/posix_ipv4/if_posix.c:
<br>
&nbsp;&#160; 277:31: error: 'struct ifreq' has no member named
<br>
&nbsp;'ifr_mtu'
<br>
&nbsp;&#160; &#160; &#160; &#160; &#160;
<br>
&nbsp;&#160;&#160;&#160;intf-&gt;if_mtu = ifr-&gt;ifr_mtu;
<br>
&nbsp;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;
<br>
&nbsp;&#160; &#160; &#160; &#160; &#160; &#160;
<br>
&nbsp;&#160;&#160;&#160;^
<br>
&nbsp;make[2]: *** [if_posix.lo] Error 1
<br>
&nbsp;make[2]: Leaving directory `.../opal/mca/if/posix_ipv4'
<br>
&nbsp;make[1]: *** [all-recursive] Error 1
<br>
&nbsp;make[1]: Leaving directory `.../opal'
<br>
&nbsp;make: *** [all-recursive] Error 1
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;I have had this problem before and Jeff solved it. Here is
<br>
&nbsp;my
<br>
&nbsp;old e-mail.
<br>
&nbsp;
<br>
&nbsp;Date: Tue, 7 May 2013 19:38:11 +0200 (CEST)
<br>
&nbsp;From: Siegmar Gross &lt;Siegmar.Gross_at_[hidden]&gt;
<br>
&nbsp;Subject: Re: commit/ompi-java: jsquyres: Up to SVN r28392
<br>
&nbsp;To: jsquyres_at_[hidden]
<br>
&nbsp;Cc: Siegmar.Gross_at_[hidden]
<br>
&nbsp;MIME-Version: 1.0
<br>
&nbsp;Content-MD5: O1pjPK/1JiMXXZ/EHyMU0Q==
<br>
&nbsp;X-HRZ-JLUG-MailScanner-Information: Passed JLUG virus check
<br>
&nbsp;X-HRZ-JLUG-MailScanner: No virus found
<br>
&nbsp;X-Envelope-From: fd1026_at_[hidden]
<br>
&nbsp;X-Spam-Status: No
<br>
&nbsp;
<br>
&nbsp;Hello Jeff
<br>
&nbsp;
<br>
<span class="quotelev1"> &gt; Ok, I made a change in the OMPI trunk that should fix
</span><br>
&nbsp;this:
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt;&#160; &#160;&#160;&#160;<a href="https://svn.open-mpi.org/trac/ompi/changeset/28460">https://svn.open-mpi.org/trac/ompi/changeset/28460</a>
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; And I pulled it into the ompi-java hg repo.&#160; Could
</span><br>
&nbsp;you give
<br>
<span class="quotelev1"> &gt; it a whirl and let me know if this works for you?
</span><br>
&nbsp;
<br>
&nbsp;Perfect :-)))).&#160; Now I can build Open MPI on Solaris
<br>
&nbsp;without
<br>
&nbsp;&quot;#if 0&quot; :-). Thank you very much for your help.
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;&quot;make check&quot;&#160; still produces the old bus error on
<br>
&nbsp;Solaris Sparc.
<br>
&nbsp;All checks are fine on Linux and Solaris x86_64.
<br>
&nbsp;
<br>
&nbsp;...
<br>
&nbsp;PASS: ddt_test
<br>
&nbsp;/bin/bash: line 5: 12453 Bus Error&#160; &#160; &#160;
<br>
&nbsp;&#160; &#160; &#160; &#160;&#160;&#160;${dir}$tst
<br>
&nbsp;FAIL: ddt_raw
<br>
&nbsp;========================================================
<br>
&nbsp;1 of 5 tests failed
<br>
&nbsp;Please report to <a href="http://www.open-mpi.org/community/help/">http://www.open-mpi.org/community/help/</a>
<br>
&nbsp;========================================================
<br>
&nbsp;make[3]: *** [check-TESTS] Error 1
<br>
&nbsp;...
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;Kind regards
<br>
&nbsp;
<br>
&nbsp;Siegmar
<br>
&nbsp;
<br>
&nbsp;
<br>
<span class="quotelev1"> &gt; On May 6, 2013, at 7:20 AM, Siegmar Gross 
</span><br>
&nbsp;&lt;Siegmar.Gross_at_[hidden]&gt;
<br>
&nbsp;wrote:
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev2"> &gt; &gt; Hello Jeff
</span><br>
<span class="quotelev2"> &gt; &gt; 
</span><br>
<span class="quotelev1"> &gt; &gt;&gt;&gt;&gt;
</span><br>
&nbsp;&quot;../../../../../ompi-java/opal/mca/if/posix_ipv4/if_posix.c&quot;,
<br>
<span class="quotelev1"> &gt; &gt;&gt;&gt;&gt; line 279: undefined struct/union
</span><br>
&nbsp;member: ifr_mtu
<br>
<span class="quotelev1"> &gt; &gt;&gt;&gt;&gt; 
</span><br>
<span class="quotelev1"> &gt; &gt;&gt;&gt;&gt; Sigh.&#160; Solaris kills me.&#160;
</span><br>
&nbsp;:-\
<br>
<span class="quotelev1"> &gt; &gt;&gt;&gt;&gt; 
</span><br>
<span class="quotelev1"> &gt; &gt;&gt;&gt;&gt; Just so I understand -- Solaris has
</span><br>
&nbsp;SIOCGIFMTU, but doesn't
<br>
<span class="quotelev1"> &gt; &gt;&gt;&gt;&gt; have struct ifreq.ifr_mtu?
</span><br>
<span class="quotelev4"> &gt; &gt;&gt;&gt; 
</span><br>
<span class="quotelev4"> &gt; &gt;&gt;&gt; I found SIOCGIFMTU in sys/sockio.h with
</span><br>
&nbsp;the following comment.
<br>
<span class="quotelev3"> &gt; &gt;&gt; 
</span><br>
<span class="quotelev3"> &gt; &gt;&gt; Is there a Solaris-defined constant we can use
</span><br>
&nbsp;here to know
<br>
<span class="quotelev3"> &gt; &gt;&gt; that we're on Solaris?&#160; If so, I can
</span><br>
&nbsp;effectively make that code
<br>
<span class="quotelev3"> &gt; &gt;&gt; only be there if SIOCFIGMTU exists and we're
</span><br>
&nbsp;not on Solaris.
<br>
<span class="quotelev2"> &gt; &gt; 
</span><br>
<span class="quotelev2"> &gt; &gt; I searched our header files for &quot;sunos&quot; and
</span><br>
&nbsp;&quot;solaris&quot; with
<br>
<span class="quotelev2"> &gt; &gt; &quot;-ignore-case&quot;, but didn't find anything useful.
</span><br>
&nbsp;You have a very
<br>
<span class="quotelev2"> &gt; &gt; minimal environment, if you use &quot;sh&quot; and you would
</span><br>
&nbsp;have a useful
<br>
<span class="quotelev2"> &gt; &gt; environment variable, if you use &quot;tcsh&quot;.
</span><br>
<span class="quotelev2"> &gt; &gt; 
</span><br>
<span class="quotelev2"> &gt; &gt; tyr java 321 su -
</span><br>
<span class="quotelev2"> &gt; &gt; ...
</span><br>
<span class="quotelev2"> &gt; &gt; # env
</span><br>
<span class="quotelev2"> &gt; &gt; HOME=/root
</span><br>
<span class="quotelev2"> &gt; &gt; HZ=
</span><br>
<span class="quotelev2"> &gt; &gt; LANG=C
</span><br>
<span class="quotelev2"> &gt; &gt; LC_ALL=C
</span><br>
<span class="quotelev2"> &gt; &gt; LOGNAME=root
</span><br>
<span class="quotelev2"> &gt; &gt; MAIL=/var/mail/root
</span><br>
<span class="quotelev2"> &gt; &gt; PATH=/usr/sbin:/usr/bin
</span><br>
<span class="quotelev2"> &gt; &gt; SHELL=/sbin/sh
</span><br>
<span class="quotelev2"> &gt; &gt; TERM=dtterm
</span><br>
<span class="quotelev2"> &gt; &gt; TZ=Europe/Berlin
</span><br>
<span class="quotelev2"> &gt; &gt; # tcsh
</span><br>
<span class="quotelev2"> &gt; &gt; # env | grep TYPE
</span><br>
<span class="quotelev2"> &gt; &gt; HOSTTYPE=sun4
</span><br>
<span class="quotelev2"> &gt; &gt; OSTYPE=solaris
</span><br>
<span class="quotelev2"> &gt; &gt; MACHTYPE=sparc
</span><br>
<span class="quotelev2"> &gt; &gt; # 
</span><br>
<span class="quotelev2"> &gt; &gt; 
</span><br>
<span class="quotelev2"> &gt; &gt; The best solution would be &quot;uname -s&quot;, if that is
</span><br>
&nbsp;possible.
<br>
<span class="quotelev2"> &gt; &gt; 
</span><br>
<span class="quotelev2"> &gt; &gt; # /usr/bin/uname -s
</span><br>
<span class="quotelev2"> &gt; &gt; SunOS
</span><br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;I would be grateful, if somebody can solve the problem once
<br>
&nbsp;more.
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;Kind regards
<br>
&nbsp;
<br>
&nbsp;Siegmar
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;------------------------------
<br>
&nbsp;
<br>
&nbsp;Message: 2
<br>
&nbsp;Date: Fri, 20 Sep 2013 09:55:44 -0400
<br>
&nbsp;From: Noam Bernstein &lt;noam.bernstein_at_[hidden]&gt;
<br>
&nbsp;To: Open MPI Users &lt;users_at_[hidden]&gt;
<br>
&nbsp;Subject: [OMPI users] intermittent node file error running
<br>
&nbsp;with
<br>
&nbsp;&#160;&#160;&#160; torque/maui&#160;&#160;&#160;
<br>
&nbsp;integration
<br>
&nbsp;Message-ID: &lt;B695C61E-461C-47E1-8634-FB492CA04947_at_[hidden]&gt;
<br>
&nbsp;Content-Type: text/plain; charset=us-ascii
<br>
&nbsp;
<br>
&nbsp;Hi - we've been using openmpi for a while, but only for the
<br>
&nbsp;last few months
<br>
&nbsp;with torque/maui.&#160; Intermittently (maybe 1/10 jobs), we
<br>
&nbsp;get mpi jobs that fail with the error:
<br>
&nbsp;
<br>
&nbsp;[compute-2-4:32448] [[52041,0],0] ORTE_ERROR_LOG: File open
<br>
&nbsp;failure in file ras_tm_module.c at line 142
<br>
&nbsp;[compute-2-4:32448] [[52041,0],0] ORTE_ERROR_LOG: File open
<br>
&nbsp;failure in file ras_tm_module.c at line 82
<br>
&nbsp;[compute-2-4:32448] [[52041,0],0] ORTE_ERROR_LOG: File open
<br>
&nbsp;failure in file base/ras_base_allocate.c at line 149
<br>
&nbsp;[compute-2-4:32448] [[52041,0],0] ORTE_ERROR_LOG: File open
<br>
&nbsp;failure in file base/plm_base_launch_support.c at line 99
<br>
&nbsp;[compute-2-4:32448] [[52041,0],0] ORTE_ERROR_LOG: File open
<br>
&nbsp;failure in file plm_tm_module.c at line 194
<br>
&nbsp;
<br>
&nbsp;This is completely unrepeatable - resubmitting the same job
<br>
&nbsp;almost
<br>
&nbsp;always works the second time around.&#160; The line appears
<br>
&nbsp;to be
<br>
&nbsp;associated with looking for the torque/maui generated node
<br>
&nbsp;file,
<br>
&nbsp;and when I do something like
<br>
&nbsp;&#160; echo $PBS_NODEFILE
<br>
&nbsp;&#160; cat $PBS_NODEFILE
<br>
&nbsp;it appears that the file is present and correct.&#160; 
<br>
&nbsp;
<br>
&nbsp;We're running OpenMPI 1.6.4, configured with 
<br>
&nbsp;./configure \
<br>
&nbsp;&#160; &#160; &#160; &#160; --prefix=${DEST} \
<br>
&nbsp;&#160; &#160; &#160; &#160; --with-tm=/usr/local/torque \
<br>
&nbsp;&#160; &#160; &#160; &#160;
<br>
&nbsp;--enable-mpirun-prefix-by-default \
<br>
&nbsp;&#160; &#160; &#160; &#160; --with-openib=/usr \
<br>
&nbsp;&#160; &#160; &#160; &#160; --with-openib-libdir=/usr/lib64
<br>
&nbsp;
<br>
&nbsp;Has anyone seen anything like this before, or has any ideas
<br>
&nbsp;of what might
<br>
&nbsp;be happening?&#160; It appears to be a line where openmpi
<br>
&nbsp;looks for
<br>
&nbsp;the PBS node file, which is on a local filesystem (e.g.
<br>
&nbsp;PBS_NODEFILE=/var/spool/torque/aux//4600.tin).
<br>
&nbsp;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;thanks,
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;Noam
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;Noam Bernstein
<br>
&nbsp;Center for Computational Materials Science
<br>
&nbsp;NRL Code 6390
<br>
&nbsp;noam.bernstein_at_[hidden]
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;------------------------------
<br>
&nbsp;
<br>
&nbsp;Message: 3
<br>
&nbsp;Date: Fri, 20 Sep 2013 10:04:43 -0400
<br>
&nbsp;From: Noam Bernstein &lt;noam.bernstein_at_[hidden]&gt;
<br>
&nbsp;To: Open MPI Users &lt;users_at_[hidden]&gt;
<br>
&nbsp;Subject: Re: [OMPI users] intermittent node file error
<br>
&nbsp;running with
<br>
&nbsp;&#160;&#160;&#160; torque/maui integration
<br>
&nbsp;Message-ID: &lt;75E58DCB-47A5-45AC-A9FB-35C0478C22CC_at_[hidden]&gt;
<br>
&nbsp;Content-Type: text/plain; charset=us-ascii
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;On Sep 20, 2013, at 9:55 AM, Noam Bernstein &lt;noam.bernstein_at_[hidden]&gt;
<br>
&nbsp;wrote:
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; This is completely unrepeatable - resubmitting the same
</span><br>
&nbsp;job almost
<br>
<span class="quotelev1"> &gt; always works the second time around.&#160; The line
</span><br>
&nbsp;appears to be
<br>
<span class="quotelev1"> &gt; associated with looking for the torque/maui generated
</span><br>
&nbsp;node file,
<br>
<span class="quotelev1"> &gt; and when I do something like
</span><br>
<span class="quotelev1"> &gt;&#160; echo $PBS_NODEFILE
</span><br>
<span class="quotelev1"> &gt;&#160; cat $PBS_NODEFILE
</span><br>
<span class="quotelev1"> &gt; it appears that the file is present and correct.&#160;
</span><br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;Never mind - I was sure that my earlier tests showed that
<br>
&nbsp;the $PBS_NODEFILE
<br>
&nbsp;was there, but now it seems like every time the job fails
<br>
&nbsp;it's because this
<br>
&nbsp;file really is missing.&#160; Time to check why torque isn't
<br>
&nbsp;always creating
<br>
&nbsp;the nodefile.
<br>
&nbsp;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;Noam
<br>
&nbsp;
<br>
&nbsp;------------------------------
<br>
&nbsp;
<br>
&nbsp;Message: 4
<br>
&nbsp;Date: Fri, 20 Sep 2013 10:12:39 -0400
<br>
&nbsp;From: Noam Bernstein &lt;noam.bernstein_at_[hidden]&gt;
<br>
&nbsp;To: Open MPI Users &lt;users_at_[hidden]&gt;
<br>
&nbsp;Subject: Re: [OMPI users] intermittent node file error
<br>
&nbsp;running with
<br>
&nbsp;&#160;&#160;&#160; torque/maui integration
<br>
&nbsp;Message-ID: &lt;A3A2843B-6AF1-4E0D-AAC2-DF0B55A6A005_at_[hidden]&gt;
<br>
&nbsp;Content-Type: text/plain; charset=us-ascii
<br>
&nbsp;
<br>
&nbsp;On Sep 20, 2013, at 10:04 AM, Noam Bernstein &lt;noam.bernstein_at_[hidden]&gt;
<br>
&nbsp;wrote:
<br>
&nbsp;
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; Never mind - I was sure that my earlier tests showed
</span><br>
&nbsp;that the $PBS_NODEFILE
<br>
<span class="quotelev1"> &gt; was there, but now it seems like every time the job
</span><br>
&nbsp;fails it's because this
<br>
<span class="quotelev1"> &gt; file really is missing.&#160; Time to check why torque
</span><br>
&nbsp;isn't always creating
<br>
<span class="quotelev1"> &gt; the nodefile.
</span><br>
&nbsp;
<br>
&nbsp;Even weirder now - most of the time jobs fail it's because
<br>
&nbsp;the PBS_NODEFILE
<br>
&nbsp;is really missing.&#160; But a small fraction of the time
<br>
&nbsp;(&lt; 1%) the PBS_NODEFILE
<br>
&nbsp;is there, but mpirun still fails in the way my original
<br>
&nbsp;message specified.
<br>
&nbsp;
<br>
&nbsp;Has anyone ever seen anything like this before?
<br>
&nbsp;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; Noam
<br>
&nbsp;
<br>
&nbsp;------------------------------
<br>
&nbsp;
<br>
&nbsp;Message: 5
<br>
&nbsp;Date: Fri, 20 Sep 2013 16:22:31 +0200
<br>
&nbsp;From: Reuti &lt;reuti_at_[hidden]&gt;
<br>
&nbsp;To: Open MPI Users &lt;users_at_[hidden]&gt;
<br>
&nbsp;Subject: Re: [OMPI users] intermittent node file error
<br>
&nbsp;running with
<br>
&nbsp;&#160;&#160;&#160; torque/maui integration
<br>
&nbsp;Message-ID:
<br>
&nbsp;&#160;&#160;&#160; &lt;FE881348-7073-4A81-86AB-DE1968A010D4_at_[hidden]&gt;
<br>
&nbsp;Content-Type: text/plain; charset=us-ascii
<br>
&nbsp;
<br>
&nbsp;Hi,
<br>
&nbsp;
<br>
&nbsp;Am 20.09.2013 um 16:12 schrieb Noam Bernstein:
<br>
&nbsp;
<br>
<span class="quotelev1"> &gt; On Sep 20, 2013, at 10:04 AM, Noam Bernstein &lt;noam.bernstein_at_[hidden]&gt;
</span><br>
&nbsp;wrote:
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; Never mind - I was sure that my earlier tests
</span><br>
&nbsp;showed that the $PBS_NODEFILE
<br>
<span class="quotelev2"> &gt;&gt; was there, but now it seems like every time the job
</span><br>
&nbsp;fails it's because this
<br>
<span class="quotelev2"> &gt;&gt; file really is missing.&#160; Time to check why
</span><br>
&nbsp;torque isn't always creating
<br>
<span class="quotelev2"> &gt;&gt; the nodefile.
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; Even weirder now - most of the time jobs fail it's
</span><br>
&nbsp;because the PBS_NODEFILE
<br>
<span class="quotelev1"> &gt; is really missing.&#160; But a small fraction of the
</span><br>
&nbsp;time (&lt; 1%) the PBS_NODEFILE
<br>
<span class="quotelev1"> &gt; is there, but mpirun still fails in the way my original
</span><br>
&nbsp;message specified.
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; Has anyone ever seen anything like this before?
</span><br>
&nbsp;
<br>
&nbsp;Is the location for the spool directory local or shared by
<br>
&nbsp;NFS? Disk full?
<br>
&nbsp;
<br>
&nbsp;-- Reuti
<br>
&nbsp;
<br>
&nbsp;------------------------------
<br>
&nbsp;
<br>
&nbsp;Message: 6
<br>
&nbsp;Date: Fri, 20 Sep 2013 10:36:21 -0400
<br>
&nbsp;From: Noam Bernstein &lt;noam.bernstein_at_[hidden]&gt;
<br>
&nbsp;To: Open MPI Users &lt;users_at_[hidden]&gt;
<br>
&nbsp;Subject: Re: [OMPI users] intermittent node file error
<br>
&nbsp;running with
<br>
&nbsp;&#160;&#160;&#160; torque/maui integration
<br>
&nbsp;Message-ID: &lt;AB5DDE1F-887A-45CC-B0FE-0F8BF4D110EF_at_[hidden]&gt;
<br>
&nbsp;Content-Type: text/plain; charset=us-ascii
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;On Sep 20, 2013, at 10:22 AM, Reuti &lt;reuti_at_[hidden]&gt;
<br>
&nbsp;wrote:
<br>
&nbsp;
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; Is the location for the spool directory local or shared
</span><br>
&nbsp;by NFS? Disk full?
<br>
&nbsp;
<br>
&nbsp;No - locally mounted, and far from full on all the nodes.
<br>
&nbsp;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; Noam
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;------------------------------
<br>
&nbsp;
<br>
&nbsp;Message: 7
<br>
&nbsp;Date: Fri, 20 Sep 2013 10:40:58 -0400
<br>
&nbsp;From: Noam Bernstein &lt;noam.bernstein_at_[hidden]&gt;
<br>
&nbsp;To: Open MPI Users &lt;users_at_[hidden]&gt;
<br>
&nbsp;Subject: Re: [OMPI users] intermittent node file error
<br>
&nbsp;running with
<br>
&nbsp;&#160;&#160;&#160; torque/maui integration
<br>
&nbsp;Message-ID: &lt;CBE710D4-2D84-4392-BD1E-85E8DE5D5398_at_[hidden]&gt;
<br>
&nbsp;Content-Type: text/plain; charset=us-ascii
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;On Sep 20, 2013, at 10:36 AM, Noam Bernstein &lt;noam.bernstein_at_[hidden]&gt;
<br>
&nbsp;wrote:
<br>
&nbsp;
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; On Sep 20, 2013, at 10:22 AM, Reuti &lt;reuti_at_[hidden]&gt;
</span><br>
&nbsp;wrote:
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; Is the location for the spool directory local or
</span><br>
&nbsp;shared by NFS? Disk full?
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; No - locally mounted, and far from full on all the
</span><br>
&nbsp;nodes.
<br>
&nbsp;
<br>
&nbsp;Another new observation, which may shift the focus to
<br>
&nbsp;torque.&#160; I
<br>
&nbsp;just rebooted some of the nodes that were showing this
<br>
&nbsp;behavior.
<br>
&nbsp;So far, none of them have shown it in a few hundred test
<br>
&nbsp;jobs,
<br>
&nbsp;while before at least 1-5 of each set of 100 had failures.
<br>
&nbsp;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; Noam
<br>
&nbsp;
<br>
&nbsp;------------------------------
<br>
&nbsp;
<br>
&nbsp;Message: 8
<br>
&nbsp;Date: Fri, 20 Sep 2013 08:49:20 -0600
<br>
&nbsp;From: Lloyd Brown &lt;lloyd_brown_at_[hidden]&gt;
<br>
&nbsp;To: Open MPI Users &lt;users_at_[hidden]&gt;
<br>
&nbsp;Subject: [OMPI users] Debugging Runtime/Ethernet Problems
<br>
&nbsp;Message-ID: &lt;523C6070.7000704_at_[hidden]&gt;
<br>
&nbsp;Content-Type: text/plain; charset=ISO-8859-1
<br>
&nbsp;
<br>
&nbsp;Hi, all.
<br>
&nbsp;
<br>
&nbsp;We've got a couple of clusters running RHEL 6.2, and have
<br>
&nbsp;several
<br>
&nbsp;centrally-installed versions/compilations of OpenMPI.&#160;
<br>
&nbsp;Some of the nodes
<br>
&nbsp;have 4xQDR Infiniband, and all the nodes have 1 gigabit
<br>
&nbsp;ethernet.&#160; I was
<br>
&nbsp;gathering some bandwidth and latency numbers using the
<br>
&nbsp;OSU/OMB tests,
<br>
&nbsp;and noticed some weird behavior.
<br>
&nbsp;
<br>
&nbsp;When I run a simple &quot;mpirun ./osu_bw&quot; on a couple of
<br>
&nbsp;IB-enabled node, I
<br>
&nbsp;get numbers consistent with our IB speed (up to about 3800
<br>
&nbsp;MB/s), and
<br>
&nbsp;when I run the same thing on two nodes with only Ethernet, I
<br>
&nbsp;get speeds
<br>
&nbsp;consistent with that (up to about 120 MB/s).&#160; So far,
<br>
&nbsp;so good.
<br>
&nbsp;
<br>
&nbsp;The trouble is when I try to add some &quot;--mca&quot; parameters to
<br>
&nbsp;force it to
<br>
&nbsp;use TCP/Ethernet, the program seems to hang.&#160; I get the
<br>
&nbsp;headers of the
<br>
&nbsp;&quot;osu_bw&quot; output, but no results, even on the first case (1
<br>
&nbsp;byte payload
<br>
&nbsp;per packet).&#160; This is occurring on both the IB-enabled
<br>
&nbsp;nodes, and on the
<br>
&nbsp;Ethernet-only nodes.&#160; The specific syntax I was using
<br>
&nbsp;was:&#160; &quot;mpirun
<br>
&nbsp;--mca btl ^openib --mca btl_tcp_if_exclude ib0 ./osu_bw&quot;
<br>
&nbsp;
<br>
&nbsp;The problem occurs at least with OpenMPI 1.6.3 compiled with
<br>
&nbsp;GNU 4.4
<br>
&nbsp;compilers, with 1.6.3 compiled with Intel 13.0.1 compilers,
<br>
&nbsp;and with
<br>
&nbsp;1.6.5 compiled with Intel 13.0.1 compilers.&#160; I haven't
<br>
&nbsp;tested any other
<br>
&nbsp;combinations yet.
<br>
&nbsp;
<br>
&nbsp;Any ideas here?&#160; It's very possible this is a system
<br>
&nbsp;configuration
<br>
&nbsp;problem, but I don't know where to look.&#160; At this
<br>
&nbsp;point, any ideas would
<br>
&nbsp;be welcome, either about the specific situation, or general
<br>
&nbsp;pointers on
<br>
&nbsp;mpirun debugging flags to use.&#160; I can't find much in
<br>
&nbsp;the docs yet on
<br>
&nbsp;run-time debugging for OpenMPI, as opposed to debugging the
<br>
&nbsp;application.
<br>
&nbsp;&nbsp;Maybe I'm just looking in the wrong place.
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;Thanks,
<br>
&nbsp;
<br>
&nbsp;-- 
<br>
&nbsp;Lloyd Brown
<br>
&nbsp;Systems Administrator
<br>
&nbsp;Fulton Supercomputing Lab
<br>
&nbsp;Brigham Young University
<br>
&nbsp;<a href="http://marylou.byu.edu">http://marylou.byu.edu</a>
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;------------------------------
<br>
&nbsp;
<br>
&nbsp;Message: 9
<br>
&nbsp;Date: Fri, 20 Sep 2013 15:05:14 +0000
<br>
&nbsp;From: &quot;Elken, Tom&quot; &lt;tom.elken_at_[hidden]&gt;
<br>
&nbsp;To: Open MPI Users &lt;users_at_[hidden]&gt;
<br>
&nbsp;Subject: Re: [OMPI users] Debugging Runtime/Ethernet
<br>
&nbsp;Problems
<br>
&nbsp;Message-ID:
<br>
&nbsp;&#160;&#160;&#160; &lt;1182FB2B5679CE4B8BAD97725F014BB73284E992_at_[hidden]&gt;
<br>
&nbsp;&#160;&#160;&#160; 
<br>
&nbsp;Content-Type: text/plain; charset=&quot;us-ascii&quot;
<br>
&nbsp;
<br>
<span class="quotelev1"> &gt; The trouble is when I try to add some &quot;--mca&quot;
</span><br>
&nbsp;parameters to force it to
<br>
<span class="quotelev1"> &gt; use TCP/Ethernet, the program seems to hang.&#160; I
</span><br>
&nbsp;get the headers of the
<br>
<span class="quotelev1"> &gt; &quot;osu_bw&quot; output, but no results, even on the first case
</span><br>
&nbsp;(1 byte payload
<br>
<span class="quotelev1"> &gt; per packet).&#160; This is occurring on both the
</span><br>
&nbsp;IB-enabled nodes, and on the
<br>
<span class="quotelev1"> &gt; Ethernet-only nodes.&#160; The specific syntax I was
</span><br>
&nbsp;using was:&#160; &quot;mpirun
<br>
<span class="quotelev1"> &gt; --mca btl ^openib --mca btl_tcp_if_exclude ib0
</span><br>
&nbsp;./osu_bw&quot;
<br>
&nbsp;&nbsp;
<br>
&nbsp;When we want to run over TCP and IPoIB on an IB/PSM equipped
<br>
&nbsp;cluster, we use:
<br>
&nbsp;--mca btl sm --mca btl tcp,self --mca btl_tcp_if_exclude
<br>
&nbsp;eth0 --mca btl_tcp_if_include ib0 --mca mtl ^psm
<br>
&nbsp;
<br>
&nbsp;based on this, it looks like the following might work for
<br>
&nbsp;you:
<br>
&nbsp;--mca btl sm,tcp,self --mca btl_tcp_if_exclude ib0 --mca
<br>
&nbsp;btl_tcp_if_include eth0 --mca btl ^openib
<br>
&nbsp;
<br>
&nbsp;If you don't have ib0 ports configured on the IB nodes,
<br>
&nbsp;probably you don't need the&quot; --mca btl_tcp_if_exclude ib0.&quot;
<br>
&nbsp;
<br>
&nbsp;-Tom
<br>
&nbsp;
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; The problem occurs at least with OpenMPI 1.6.3 compiled
</span><br>
&nbsp;with GNU 4.4
<br>
<span class="quotelev1"> &gt; compilers, with 1.6.3 compiled with Intel 13.0.1
</span><br>
&nbsp;compilers, and with
<br>
<span class="quotelev1"> &gt; 1.6.5 compiled with Intel 13.0.1 compilers.&#160; I
</span><br>
&nbsp;haven't tested any other
<br>
<span class="quotelev1"> &gt; combinations yet.
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; Any ideas here?&#160; It's very possible this is a
</span><br>
&nbsp;system configuration
<br>
<span class="quotelev1"> &gt; problem, but I don't know where to look.&#160; At this
</span><br>
&nbsp;point, any ideas would
<br>
<span class="quotelev1"> &gt; be welcome, either about the specific situation, or
</span><br>
&nbsp;general pointers on
<br>
<span class="quotelev1"> &gt; mpirun debugging flags to use.&#160; I can't find much
</span><br>
&nbsp;in the docs yet on
<br>
<span class="quotelev1"> &gt; run-time debugging for OpenMPI, as opposed to debugging
</span><br>
&nbsp;the application.
<br>
<span class="quotelev1"> &gt;&#160; Maybe I'm just looking in the wrong place.
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; Thanks,
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; --
</span><br>
<span class="quotelev1"> &gt; Lloyd Brown
</span><br>
<span class="quotelev1"> &gt; Systems Administrator
</span><br>
<span class="quotelev1"> &gt; Fulton Supercomputing Lab
</span><br>
<span class="quotelev1"> &gt; Brigham Young University
</span><br>
<span class="quotelev1"> &gt; <a href="http://marylou.byu.edu">http://marylou.byu.edu</a>
</span><br>
<span class="quotelev1"> &gt; _______________________________________________
</span><br>
<span class="quotelev1"> &gt; users mailing list
</span><br>
<span class="quotelev1"> &gt; users_at_[hidden]
</span><br>
<span class="quotelev1"> &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;------------------------------
<br>
&nbsp;
<br>
&nbsp;Message: 10
<br>
&nbsp;Date: Fri, 20 Sep 2013 08:17:43 -0700
<br>
&nbsp;From: Ralph Castain &lt;rhc_at_[hidden]&gt;
<br>
&nbsp;To: Open MPI Users &lt;users_at_[hidden]&gt;
<br>
&nbsp;Subject: Re: [OMPI users] Debugging Runtime/Ethernet
<br>
&nbsp;Problems
<br>
&nbsp;Message-ID: &lt;917B367F-1687-4A91-B173-DE1BBA7C7866_at_[hidden]&gt;
<br>
&nbsp;Content-Type: text/plain; charset=us-ascii
<br>
&nbsp;
<br>
&nbsp;I don't think you are allowed to specify both include and
<br>
&nbsp;exclude options at the same time as they conflict - you
<br>
&nbsp;should either exclude ib0 or include eth0 (or whatever).
<br>
&nbsp;
<br>
&nbsp;My guess is that the various nodes are trying to communicate
<br>
&nbsp;across disjoint networks. We've seen that before when, for
<br>
&nbsp;example, eth0 on one node is on one subnet, and eth0 on
<br>
&nbsp;another node is on a different subnet. You might look for
<br>
&nbsp;that kind of arrangement.
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;On Sep 20, 2013, at 8:05 AM, &quot;Elken, Tom&quot; &lt;tom.elken_at_[hidden]&gt;
<br>
&nbsp;wrote:
<br>
&nbsp;
<br>
<span class="quotelev2"> &gt;&gt; The trouble is when I try to add some &quot;--mca&quot;
</span><br>
&nbsp;parameters to force it to
<br>
<span class="quotelev2"> &gt;&gt; use TCP/Ethernet, the program seems to hang.&#160;
</span><br>
&nbsp;I get the headers of the
<br>
<span class="quotelev2"> &gt;&gt; &quot;osu_bw&quot; output, but no results, even on the first
</span><br>
&nbsp;case (1 byte payload
<br>
<span class="quotelev2"> &gt;&gt; per packet).&#160; This is occurring on both the
</span><br>
&nbsp;IB-enabled nodes, and on the
<br>
<span class="quotelev2"> &gt;&gt; Ethernet-only nodes.&#160; The specific syntax I
</span><br>
&nbsp;was using was:&#160; &quot;mpirun
<br>
<span class="quotelev2"> &gt;&gt; --mca btl ^openib --mca btl_tcp_if_exclude ib0
</span><br>
&nbsp;./osu_bw&quot;
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; When we want to run over TCP and IPoIB on an IB/PSM
</span><br>
&nbsp;equipped cluster, we use:
<br>
<span class="quotelev1"> &gt; --mca btl sm --mca btl tcp,self --mca
</span><br>
&nbsp;btl_tcp_if_exclude eth0 --mca btl_tcp_if_include ib0 --mca
<br>
&nbsp;mtl ^psm
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; based on this, it looks like the following might work
</span><br>
&nbsp;for you:
<br>
<span class="quotelev1"> &gt; --mca btl sm,tcp,self --mca btl_tcp_if_exclude ib0
</span><br>
&nbsp;--mca btl_tcp_if_include eth0 --mca btl ^openib
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; If you don't have ib0 ports configured on the IB nodes,
</span><br>
&nbsp;probably you don't need the&quot; --mca btl_tcp_if_exclude ib0.&quot;
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; -Tom
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; The problem occurs at least with OpenMPI 1.6.3
</span><br>
&nbsp;compiled with GNU 4.4
<br>
<span class="quotelev2"> &gt;&gt; compilers, with 1.6.3 compiled with Intel 13.0.1
</span><br>
&nbsp;compilers, and with
<br>
<span class="quotelev2"> &gt;&gt; 1.6.5 compiled with Intel 13.0.1 compilers.&#160; I
</span><br>
&nbsp;haven't tested any other
<br>
<span class="quotelev2"> &gt;&gt; combinations yet.
</span><br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; Any ideas here?&#160; It's very possible this is a
</span><br>
&nbsp;system configuration
<br>
<span class="quotelev2"> &gt;&gt; problem, but I don't know where to look.&#160; At
</span><br>
&nbsp;this point, any ideas would
<br>
<span class="quotelev2"> &gt;&gt; be welcome, either about the specific situation, or
</span><br>
&nbsp;general pointers on
<br>
<span class="quotelev2"> &gt;&gt; mpirun debugging flags to use.&#160; I can't find
</span><br>
&nbsp;much in the docs yet on
<br>
<span class="quotelev2"> &gt;&gt; run-time debugging for OpenMPI, as opposed to
</span><br>
&nbsp;debugging the application.
<br>
<span class="quotelev2"> &gt;&gt; Maybe I'm just looking in the wrong place.
</span><br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; Thanks,
</span><br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; --
</span><br>
<span class="quotelev2"> &gt;&gt; Lloyd Brown
</span><br>
<span class="quotelev2"> &gt;&gt; Systems Administrator
</span><br>
<span class="quotelev2"> &gt;&gt; Fulton Supercomputing Lab
</span><br>
<span class="quotelev2"> &gt;&gt; Brigham Young University
</span><br>
<span class="quotelev2"> &gt;&gt; <a href="http://marylou.byu.edu">http://marylou.byu.edu</a>
</span><br>
<span class="quotelev2"> &gt;&gt; _______________________________________________
</span><br>
<span class="quotelev2"> &gt;&gt; users mailing list
</span><br>
<span class="quotelev2"> &gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev2"> &gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1"> &gt; _______________________________________________
</span><br>
<span class="quotelev1"> &gt; users mailing list
</span><br>
<span class="quotelev1"> &gt; users_at_[hidden]
</span><br>
<span class="quotelev1"> &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;------------------------------
<br>
&nbsp;
<br>
&nbsp;Message: 11
<br>
&nbsp;Date: Fri, 20 Sep 2013 15:28:48 +0000
<br>
&nbsp;From: &quot;Jeff Squyres (jsquyres)&quot; &lt;jsquyres_at_[hidden]&gt;
<br>
&nbsp;To: Open MPI Users &lt;users_at_[hidden]&gt;
<br>
&nbsp;Subject: Re: [OMPI users] compilation aborted for
<br>
&nbsp;Handler.cpp (code 2)
<br>
&nbsp;Message-ID:
<br>
&nbsp;&#160;&#160;&#160; &lt;EF66BBEB19BADC41AC8CCF5F684F07FC4F8BC7EC_at_[hidden]&gt;
<br>
&nbsp;Content-Type: text/plain; charset=&quot;iso-8859-1&quot;
<br>
&nbsp;
<br>
&nbsp;I can't tell if this is a busted compiler installation or
<br>
&nbsp;not.&#160; The first error is:
<br>
&nbsp;
<br>
&nbsp;-----
<br>
&nbsp;/usr/include/c++/4.6.3/bits/stl_algobase.h(573): error: type
<br>
&nbsp;name is not allowed
<br>
&nbsp;&#160; &#160; &#160; &#160; const bool __simple =
<br>
&nbsp;(__is_trivial(_ValueType1)
<br>
&nbsp;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;
<br>
&nbsp;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;
<br>
&nbsp;&#160; &#160; &#160; &#160; &#160; &#160; ^
<br>
&nbsp;&#160; &#160; &#160; &#160; &#160; detected during:
<br>
&nbsp;&#160; &#160; &#160; &#160; &#160; &#160; instantiation of
<br>
&nbsp;&quot;_BI2
<br>
&nbsp;std::__copy_move_backward_a2&lt;_IsMove,_BI1,_BI2&gt;(_BI1,
<br>
&nbsp;_BI1, _BI2) [with _IsMove=false, _BI1=uint32_t={unsigned
<br>
&nbsp;int} *, _BI2=uint32_t={unsigned int} *]&quot; at line 625
<br>
&nbsp;&#160; &#160; &#160; &#160; &#160; &#160; instantiation of
<br>
&nbsp;&quot;_BI2 std::copy_backward(_BI1, _BI1, _BI2) [with
<br>
&nbsp;_BI1=uint32_t={unsigned int} *, _BI2=uint32_t={unsigned int}
<br>
&nbsp;*]&quot; at line 315 of &quot;/usr/include/c++/4.6.3/bits/vector.tcc&quot;
<br>
&nbsp;&#160; &#160; &#160; &#160; &#160; &#160; instantiation of
<br>
&nbsp;&quot;void std::vector&lt;_Tp,
<br>
&nbsp;_Alloc&gt;::_M_insert_aux(__gnu_cxx::__normal_iterator&lt;std::_Vector_base&lt;_Tp,
<br>
&nbsp;_Alloc&gt;::_Tp_alloc_type::pointer, std::vector&lt;_Tp,
<br>
&nbsp;_Alloc&gt;&gt;, const _Tp &amp;) [with
<br>
&nbsp;_Tp=uint32_t={unsigned int},
<br>
&nbsp;_Alloc=std::allocator&lt;uint32_t={unsigned int}&gt;]&quot; at
<br>
&nbsp;line 834 of &quot;/usr/include/c++/4.6.3/bits/stl_vector.h&quot;
<br>
&nbsp;&#160; &#160; &#160; &#160; &#160; &#160; instantiation of
<br>
&nbsp;&quot;void std::vector&lt;_Tp, _Alloc&gt;::push_back(const _Tp
<br>
&nbsp;&amp;) [with _Tp=uint32_t={unsigned int},
<br>
&nbsp;_Alloc=std::allocator&lt;uint32_t={unsigned int}&gt;]&quot; at
<br>
&nbsp;line 42 of &quot;Handler.cpp&quot;
<br>
&nbsp;-----
<br>
&nbsp;
<br>
&nbsp;I verified that OMPI 1.6.5 builds fine for me for
<br>
&nbsp;icpc/13.1.0.146 Build 20130121 on RHEL 6.
<br>
&nbsp;
<br>
&nbsp;Perhaps you have some kind of bad interaction between your
<br>
&nbsp;icpc installation and your local g++ installation...?
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;On Sep 18, 2013, at 12:58 PM, Syed Ahsan Ali &lt;ahsanshah01_at_[hidden]&gt;
<br>
&nbsp;wrote:
<br>
&nbsp;
<br>
<span class="quotelev1"> &gt; Please find attached again.
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; On Tue, Sep 17, 2013 at 11:35 AM, Jeff Squyres
</span><br>
&nbsp;(jsquyres)
<br>
<span class="quotelev1"> &gt; &lt;jsquyres_at_[hidden]&gt;
</span><br>
&nbsp;wrote:
<br>
<span class="quotelev2"> &gt;&gt; On Sep 16, 2013, at 9:00 AM, Syed Ahsan Ali &lt;ahsanshah01_at_[hidden]&gt;
</span><br>
&nbsp;wrote:
<br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; I am trying to compile openmpi-1.6.5 on
</span><br>
&nbsp;fc16.x86_64 with icc and ifort
<br>
<span class="quotelev3"> &gt;&gt;&gt; but getting the subject error. config.out and
</span><br>
&nbsp;make.out is attached.
<br>
<span class="quotelev3"> &gt;&gt;&gt; Following command was used for configure
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; 
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; ./configure CC=icc CXX=icpc FC=ifort F77=ifort
</span><br>
&nbsp;F90=ifort
<br>
<span class="quotelev3"> &gt;&gt;&gt; --prefix=/home/openmpi_gfortran -enable-mpi-f90
</span><br>
&nbsp;--enable-mpi-f77 |&amp;
<br>
<span class="quotelev3"> &gt;&gt;&gt; tee config.out
</span><br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; I'm sorry; I can't open a .rar file.&#160; Can you
</span><br>
&nbsp;send the logs compressed with a conventional compression
<br>
&nbsp;program like gzip, bzip2, or zip?
<br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; Thanks.
</span><br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; --
</span><br>
<span class="quotelev2"> &gt;&gt; Jeff Squyres
</span><br>
<span class="quotelev2"> &gt;&gt; jsquyres_at_[hidden]
</span><br>
<span class="quotelev2"> &gt;&gt; For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a>
</span><br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; _______________________________________________
</span><br>
<span class="quotelev2"> &gt;&gt; users mailing list
</span><br>
<span class="quotelev2"> &gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev2"> &gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; -- 
</span><br>
<span class="quotelev1"> &gt; Syed Ahsan Ali Bokhari
</span><br>
<span class="quotelev1"> &gt; Electronic Engineer (EE)
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; Research &amp; Development Division
</span><br>
<span class="quotelev1"> &gt; Pakistan Meteorological Department H-8/4, Islamabad.
</span><br>
<span class="quotelev1"> &gt; Phone # off&#160; +92518358714
</span><br>
<span class="quotelev1"> &gt; Cell # +923155145014
</span><br>
<span class="quotelev1"> &gt;
</span><br>
&nbsp;&lt;logs.zip&gt;_______________________________________________
<br>
<span class="quotelev1"> &gt; users mailing list
</span><br>
<span class="quotelev1"> &gt; users_at_[hidden]
</span><br>
<span class="quotelev1"> &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;-- 
<br>
&nbsp;Jeff Squyres
<br>
&nbsp;jsquyres_at_[hidden]
<br>
&nbsp;For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a>
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;------------------------------
<br>
&nbsp;
<br>
&nbsp;Message: 12
<br>
&nbsp;Date: Fri, 20 Sep 2013 15:33:43 +0000
<br>
&nbsp;From: &quot;Jeff Squyres (jsquyres)&quot; &lt;jsquyres_at_[hidden]&gt;
<br>
&nbsp;To: Open MPI Users &lt;users_at_[hidden]&gt;
<br>
&nbsp;Subject: Re: [OMPI users] Debugging Runtime/Ethernet
<br>
&nbsp;Problems
<br>
&nbsp;Message-ID:
<br>
&nbsp;&#160;&#160;&#160; &lt;EF66BBEB19BADC41AC8CCF5F684F07FC4F8BC892_at_[hidden]&gt;
<br>
&nbsp;Content-Type: text/plain; charset=&quot;us-ascii&quot;
<br>
&nbsp;
<br>
&nbsp;Correct -- it doesn't make sense to specify both include
<br>
&nbsp;*and* exclude: by specifying one, you're implicitly (but
<br>
&nbsp;exactly/precisely) specifying the other.
<br>
&nbsp;
<br>
&nbsp;My suggestion would be to use positive notation, not
<br>
&nbsp;negative notation.&#160; For example:
<br>
&nbsp;
<br>
&nbsp;mpirun --mca btl tcp,self --mca btl_tcp_if_include eth0 ...
<br>
&nbsp;
<br>
&nbsp;That way, you *know* you're only getting the TCP and self
<br>
&nbsp;BTLs, and you *know* you're only getting eth0.&#160; If that
<br>
&nbsp;works, then spread out from there, e.g.:
<br>
&nbsp;
<br>
&nbsp;mpirun --mca btl tcp,sm,self --mca btl_tcp_if_include
<br>
&nbsp;eth0,eth1 ...
<br>
&nbsp;
<br>
&nbsp;E.g., also include the &quot;sm&quot; BTL (which is only used for
<br>
&nbsp;shared memory communications between 2 procs on the same
<br>
&nbsp;server, and is therefore useless for a
<br>
&nbsp;2-proc-across-2-server run of osu_bw, but you get the idea),
<br>
&nbsp;but also use eth0 and eth1.&#160; 
<br>
&nbsp;
<br>
&nbsp;And so on.
<br>
&nbsp;
<br>
&nbsp;The problem with using ^openib and/or btl_tcp_if_exclude is
<br>
&nbsp;that you might end up using some BTLs and/or TCP interfaces
<br>
&nbsp;that you don't expect, and therefore can run into problems.
<br>
&nbsp;
<br>
&nbsp;Make sense?
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;On Sep 20, 2013, at 11:17 AM, Ralph Castain &lt;rhc_at_[hidden]&gt;
<br>
&nbsp;wrote:
<br>
&nbsp;
<br>
<span class="quotelev1"> &gt; I don't think you are allowed to specify both include
</span><br>
&nbsp;and exclude options at the same time as they conflict - you
<br>
&nbsp;should either exclude ib0 or include eth0 (or whatever).
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; My guess is that the various nodes are trying to
</span><br>
&nbsp;communicate across disjoint networks. We've seen that before
<br>
&nbsp;when, for example, eth0 on one node is on one subnet, and
<br>
&nbsp;eth0 on another node is on a different subnet. You might
<br>
&nbsp;look for that kind of arrangement.
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; On Sep 20, 2013, at 8:05 AM, &quot;Elken, Tom&quot; &lt;tom.elken_at_[hidden]&gt;
</span><br>
&nbsp;wrote:
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; The trouble is when I try to add some &quot;--mca&quot;
</span><br>
&nbsp;parameters to force it to
<br>
<span class="quotelev3"> &gt;&gt;&gt; use TCP/Ethernet, the program seems to
</span><br>
&nbsp;hang.&#160; I get the headers of the
<br>
<span class="quotelev3"> &gt;&gt;&gt; &quot;osu_bw&quot; output, but no results, even on the
</span><br>
&nbsp;first case (1 byte payload
<br>
<span class="quotelev3"> &gt;&gt;&gt; per packet).&#160; This is occurring on both
</span><br>
&nbsp;the IB-enabled nodes, and on the
<br>
<span class="quotelev3"> &gt;&gt;&gt; Ethernet-only nodes.&#160; The specific syntax
</span><br>
&nbsp;I was using was:&#160; &quot;mpirun
<br>
<span class="quotelev3"> &gt;&gt;&gt; --mca btl ^openib --mca btl_tcp_if_exclude ib0
</span><br>
&nbsp;./osu_bw&quot;
<br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; When we want to run over TCP and IPoIB on an IB/PSM
</span><br>
&nbsp;equipped cluster, we use:
<br>
<span class="quotelev2"> &gt;&gt; --mca btl sm --mca btl tcp,self --mca
</span><br>
&nbsp;btl_tcp_if_exclude eth0 --mca btl_tcp_if_include ib0 --mca
<br>
&nbsp;mtl ^psm
<br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; based on this, it looks like the following might
</span><br>
&nbsp;work for you:
<br>
<span class="quotelev2"> &gt;&gt; --mca btl sm,tcp,self --mca btl_tcp_if_exclude ib0
</span><br>
&nbsp;--mca btl_tcp_if_include eth0 --mca btl ^openib
<br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; If you don't have ib0 ports configured on the IB
</span><br>
&nbsp;nodes, probably you don't need the&quot; --mca btl_tcp_if_exclude
<br>
&nbsp;ib0.&quot;
<br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; -Tom
</span><br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; 
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; The problem occurs at least with OpenMPI 1.6.3
</span><br>
&nbsp;compiled with GNU 4.4
<br>
<span class="quotelev3"> &gt;&gt;&gt; compilers, with 1.6.3 compiled with Intel
</span><br>
&nbsp;13.0.1 compilers, and with
<br>
<span class="quotelev3"> &gt;&gt;&gt; 1.6.5 compiled with Intel 13.0.1
</span><br>
&nbsp;compilers.&#160; I haven't tested any other
<br>
<span class="quotelev3"> &gt;&gt;&gt; combinations yet.
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; 
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; Any ideas here?&#160; It's very possible this
</span><br>
&nbsp;is a system configuration
<br>
<span class="quotelev3"> &gt;&gt;&gt; problem, but I don't know where to look.&#160;
</span><br>
&nbsp;At this point, any ideas would
<br>
<span class="quotelev3"> &gt;&gt;&gt; be welcome, either about the specific
</span><br>
&nbsp;situation, or general pointers on
<br>
<span class="quotelev3"> &gt;&gt;&gt; mpirun debugging flags to use.&#160; I can't
</span><br>
&nbsp;find much in the docs yet on
<br>
<span class="quotelev3"> &gt;&gt;&gt; run-time debugging for OpenMPI, as opposed to
</span><br>
&nbsp;debugging the application.
<br>
<span class="quotelev3"> &gt;&gt;&gt; Maybe I'm just looking in the wrong place.
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; 
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; 
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; Thanks,
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; 
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; --
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; Lloyd Brown
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; Systems Administrator
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; Fulton Supercomputing Lab
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; Brigham Young University
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; <a href="http://marylou.byu.edu">http://marylou.byu.edu</a>
</span><br>
<span class="quotelev3"> &gt;&gt;&gt;
</span><br>
&nbsp;_______________________________________________
<br>
<span class="quotelev3"> &gt;&gt;&gt; users mailing list
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev2"> &gt;&gt; _______________________________________________
</span><br>
<span class="quotelev2"> &gt;&gt; users mailing list
</span><br>
<span class="quotelev2"> &gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev2"> &gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; _______________________________________________
</span><br>
<span class="quotelev1"> &gt; users mailing list
</span><br>
<span class="quotelev1"> &gt; users_at_[hidden]
</span><br>
<span class="quotelev1"> &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;-- 
<br>
&nbsp;Jeff Squyres
<br>
&nbsp;jsquyres_at_[hidden]
<br>
&nbsp;For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a>
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;------------------------------
<br>
&nbsp;
<br>
&nbsp;Message: 13
<br>
&nbsp;Date: Fri, 20 Sep 2013 15:35:59 +0000
<br>
&nbsp;From: &quot;Jeff Squyres (jsquyres)&quot; &lt;jsquyres_at_[hidden]&gt;
<br>
&nbsp;To: Open MPI Users &lt;users_at_[hidden]&gt;
<br>
&nbsp;Subject: Re: [OMPI users] compilation aborted for
<br>
&nbsp;Handler.cpp (code 2)
<br>
&nbsp;Message-ID:
<br>
&nbsp;&#160;&#160;&#160; &lt;EF66BBEB19BADC41AC8CCF5F684F07FC4F8BC914_at_[hidden]&gt;
<br>
&nbsp;Content-Type: text/plain; charset=&quot;iso-8859-1&quot;
<br>
&nbsp;
<br>
&nbsp;Sorry for the delay replying -- I actually replied on the
<br>
&nbsp;original thread yesterday, but it got hung up in my outbox
<br>
&nbsp;and I didn't notice that it didn't actually go out until a
<br>
&nbsp;few moments ago.&#160; :-(
<br>
&nbsp;
<br>
&nbsp;I'm *guessing* that this is a problem with your local icpc
<br>
&nbsp;installation.
<br>
&nbsp;
<br>
&nbsp;Can you compile / run other C++ codes that use the STL with
<br>
&nbsp;icpc?
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;On Sep 20, 2013, at 6:59 AM, Syed Ahsan Ali &lt;ahsanshah01_at_[hidden]&gt;
<br>
&nbsp;wrote:
<br>
&nbsp;
<br>
<span class="quotelev1"> &gt; Output of make V=1 is attached. Again same error. If
</span><br>
&nbsp;intel compiler is
<br>
<span class="quotelev1"> &gt; using C++ headers from gfortran then how can we avoid
</span><br>
&nbsp;this.
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; On Fri, Sep 20, 2013 at 11:07 AM, Bert Wesarg
</span><br>
<span class="quotelev1"> &gt; &lt;bert.wesarg_at_[hidden]&gt;
</span><br>
&nbsp;wrote:
<br>
<span class="quotelev2"> &gt;&gt; Hi,
</span><br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; On Fri, Sep 20, 2013 at 4:49 AM, Syed Ahsan Ali
</span><br>
&nbsp;&lt;ahsanshah01_at_[hidden]&gt;
<br>
&nbsp;wrote:
<br>
<span class="quotelev3"> &gt;&gt;&gt; I am trying to compile openmpi-1.6.5 on
</span><br>
&nbsp;fc16.x86_64 with icc and ifort
<br>
<span class="quotelev3"> &gt;&gt;&gt; but getting the subject error. config.out and
</span><br>
&nbsp;make.out is attached.
<br>
<span class="quotelev3"> &gt;&gt;&gt; Following command was used for configure
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; 
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; ./configure CC=icc CXX=icpc FC=ifort F77=ifort
</span><br>
&nbsp;F90=ifort
<br>
<span class="quotelev3"> &gt;&gt;&gt; --prefix=/home/openmpi_gfortran -enable-mpi-f90
</span><br>
&nbsp;--enable-mpi-f77 |&amp;
<br>
<span class="quotelev3"> &gt;&gt;&gt; tee config.out
</span><br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; could you also run make with 'make V=1' and send
</span><br>
&nbsp;the output. Anyway it
<br>
<span class="quotelev2"> &gt;&gt; looks like the intel compiler uses the C++ headers
</span><br>
&nbsp;from GCC 4.6.3 and
<br>
<span class="quotelev2"> &gt;&gt; I don't know if this is supported.
</span><br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; Bert
</span><br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; Please help/advise.
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; Thank you and best regards
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; Ahsan
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; 
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; -- 
</span><br>
<span class="quotelev1"> &gt; Syed Ahsan Ali Bokhari
</span><br>
<span class="quotelev1"> &gt; Electronic Engineer (EE)
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; Research &amp; Development Division
</span><br>
<span class="quotelev1"> &gt; Pakistan Meteorological Department H-8/4, Islamabad.
</span><br>
<span class="quotelev1"> &gt; Phone # off&#160; +92518358714
</span><br>
<span class="quotelev1"> &gt; Cell # +923155145014
</span><br>
<span class="quotelev1"> &gt;
</span><br>
&nbsp;&lt;makeV.zip&gt;_______________________________________________
<br>
<span class="quotelev1"> &gt; users mailing list
</span><br>
<span class="quotelev1"> &gt; users_at_[hidden]
</span><br>
<span class="quotelev1"> &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;-- 
<br>
&nbsp;Jeff Squyres
<br>
&nbsp;jsquyres_at_[hidden]
<br>
&nbsp;For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a>
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;------------------------------
<br>
&nbsp;
<br>
&nbsp;Message: 14
<br>
&nbsp;Date: Fri, 20 Sep 2013 11:52:11 -0400
<br>
&nbsp;From: Gus Correa &lt;gus_at_[hidden]&gt;
<br>
&nbsp;To: Open MPI Users &lt;users_at_[hidden]&gt;
<br>
&nbsp;Subject: Re: [OMPI users] intermittent node file error
<br>
&nbsp;running with
<br>
&nbsp;&#160;&#160;&#160; torque/maui integration
<br>
&nbsp;Message-ID: &lt;523C6F2B.60401_at_[hidden]&gt;
<br>
&nbsp;Content-Type: text/plain; charset=ISO-8859-1; format=flowed
<br>
&nbsp;
<br>
&nbsp;Hi Noam
<br>
&nbsp;
<br>
&nbsp;Could it be that Torque, or probably more likely NFS,
<br>
&nbsp;is too slow to create/make available the PBS_NODEFILE?
<br>
&nbsp;
<br>
&nbsp;What if you insert a &quot;sleep 2&quot;,
<br>
&nbsp;or whatever number of seconds you want,
<br>
&nbsp;before the mpiexec command line?
<br>
&nbsp;Or maybe better, a &quot;ls -l $PBS_NODEFILE; cat
<br>
&nbsp;$PBS_NODEFILE&quot;,
<br>
&nbsp;just to make sure the file it is available and
<br>
&nbsp;filled with the node list, before mpiexec takes over?
<br>
&nbsp;
<br>
&nbsp;My two cents,
<br>
&nbsp;Gus Correa
<br>
&nbsp;
<br>
&nbsp;On 09/20/2013 09:55 AM, Noam Bernstein wrote:
<br>
<span class="quotelev1"> &gt; Hi - we've been using openmpi for a while, but only for
</span><br>
&nbsp;the last few months
<br>
<span class="quotelev1"> &gt; with torque/maui.&#160; Intermittently (maybe 1/10
</span><br>
&nbsp;jobs), we get mpi jobs that fail with the error:
<br>
<span class="quotelev1"> &gt;
</span><br>
<span class="quotelev1"> &gt; [compute-2-4:32448] [[52041,0],0] ORTE_ERROR_LOG: File
</span><br>
&nbsp;open failure in file ras_tm_module.c at line 142
<br>
<span class="quotelev1"> &gt; [compute-2-4:32448] [[52041,0],0] ORTE_ERROR_LOG: File
</span><br>
&nbsp;open failure in file ras_tm_module.c at line 82
<br>
<span class="quotelev1"> &gt; [compute-2-4:32448] [[52041,0],0] ORTE_ERROR_LOG: File
</span><br>
&nbsp;open failure in file base/ras_base_allocate.c at line 149
<br>
<span class="quotelev1"> &gt; [compute-2-4:32448] [[52041,0],0] ORTE_ERROR_LOG: File
</span><br>
&nbsp;open failure in file base/plm_base_launch_support.c at line
<br>
&nbsp;99
<br>
<span class="quotelev1"> &gt; [compute-2-4:32448] [[52041,0],0] ORTE_ERROR_LOG: File
</span><br>
&nbsp;open failure in file plm_tm_module.c at line 194
<br>
<span class="quotelev1"> &gt;
</span><br>
<span class="quotelev1"> &gt; This is completely unrepeatable - resubmitting the same
</span><br>
&nbsp;job almost
<br>
<span class="quotelev1"> &gt; always works the second time around.&#160; The line
</span><br>
&nbsp;appears to be
<br>
<span class="quotelev1"> &gt; associated with looking for the torque/maui generated
</span><br>
&nbsp;node file,
<br>
<span class="quotelev1"> &gt; and when I do something like
</span><br>
<span class="quotelev1"> &gt;&#160; &#160; echo $PBS_NODEFILE
</span><br>
<span class="quotelev1"> &gt;&#160; &#160; cat $PBS_NODEFILE
</span><br>
<span class="quotelev1"> &gt; it appears that the file is present and correct.
</span><br>
<span class="quotelev1"> &gt;
</span><br>
<span class="quotelev1"> &gt; We're running OpenMPI 1.6.4, configured with
</span><br>
<span class="quotelev1"> &gt; ./configure \
</span><br>
<span class="quotelev1"> &gt;&#160; &#160; &#160; &#160; &#160; --prefix=${DEST} \
</span><br>
<span class="quotelev1"> &gt;&#160; &#160; &#160; &#160; &#160;
</span><br>
&nbsp;--with-tm=/usr/local/torque \
<br>
<span class="quotelev1"> &gt;&#160; &#160; &#160; &#160; &#160;
</span><br>
&nbsp;--enable-mpirun-prefix-by-default \
<br>
<span class="quotelev1"> &gt;&#160; &#160; &#160; &#160; &#160; --with-openib=/usr \
</span><br>
<span class="quotelev1"> &gt;&#160; &#160; &#160; &#160; &#160;
</span><br>
&nbsp;--with-openib-libdir=/usr/lib64
<br>
<span class="quotelev1"> &gt;
</span><br>
<span class="quotelev1"> &gt; Has anyone seen anything like this before, or has any
</span><br>
&nbsp;ideas of what might
<br>
<span class="quotelev1"> &gt; be happening?&#160; It appears to be a line where
</span><br>
&nbsp;openmpi looks for
<br>
<span class="quotelev1"> &gt; the PBS node file, which is on a local filesystem (e.g.
</span><br>
&nbsp;PBS_NODEFILE=/var/spool/torque/aux//4600.tin).
<br>
<span class="quotelev1"> &gt;
</span><br>
<span class="quotelev1"> &gt; &#160;&#160;&#160; &#160;&#160;&#160;
</span><br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; thanks,
<br>
<span class="quotelev1"> &gt; &#160;&#160;&#160; &#160;&#160;&#160;
</span><br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; &#160;&#160;&#160; &#160;&#160;&#160;
<br>
&nbsp;&#160;&#160;&#160; Noam
<br>
<span class="quotelev1"> &gt;
</span><br>
<span class="quotelev1"> &gt;
</span><br>
<span class="quotelev1"> &gt;
</span><br>
<span class="quotelev1"> &gt; Noam Bernstein
</span><br>
<span class="quotelev1"> &gt; Center for Computational Materials Science
</span><br>
<span class="quotelev1"> &gt; NRL Code 6390
</span><br>
<span class="quotelev1"> &gt; noam.bernstein_at_[hidden]
</span><br>
<span class="quotelev1"> &gt;
</span><br>
<span class="quotelev1"> &gt;
</span><br>
<span class="quotelev1"> &gt;
</span><br>
<span class="quotelev1"> &gt;
</span><br>
<span class="quotelev1"> &gt; _______________________________________________
</span><br>
<span class="quotelev1"> &gt; users mailing list
</span><br>
<span class="quotelev1"> &gt; users_at_[hidden]
</span><br>
<span class="quotelev1"> &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;------------------------------
<br>
&nbsp;
<br>
&nbsp;Message: 15
<br>
&nbsp;Date: Fri, 20 Sep 2013 15:52:43 +0000
<br>
&nbsp;From: &quot;Jeff Squyres (jsquyres)&quot; &lt;jsquyres_at_[hidden]&gt;
<br>
&nbsp;To: Siegmar Gross &lt;Siegmar.Gross_at_[hidden]&gt;,
<br>
&nbsp;Open MPI
<br>
&nbsp;&#160;&#160;&#160; Users&#160;&#160;&#160; &lt;users_at_[hidden]&gt;
<br>
&nbsp;Subject: Re: [OMPI users] error building
<br>
&nbsp;openmpi-1.7.3a1r29213 on
<br>
&nbsp;&#160;&#160;&#160; Solaris
<br>
&nbsp;Message-ID:
<br>
&nbsp;&#160;&#160;&#160; &lt;EF66BBEB19BADC41AC8CCF5F684F07FC4F8BCB02_at_[hidden]&gt;
<br>
&nbsp;Content-Type: text/plain; charset=&quot;us-ascii&quot;
<br>
&nbsp;
<br>
&nbsp;Looks like Ralph noticed that we fixed this on the trunk and
<br>
&nbsp;forgot to bring it over to v1.7.&#160; I just committed it
<br>
&nbsp;on v1.7 in r29215.&#160; Give it a whirl in tonight's v1.7
<br>
&nbsp;nightly tarball.
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;On Sep 20, 2013, at 7:00 AM, Siegmar Gross &lt;Siegmar.Gross_at_[hidden]&gt;
<br>
&nbsp;wrote:
<br>
&nbsp;
<br>
<span class="quotelev1"> &gt; Hi,
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; I tried to install openmpi-1.7.3a1r29213 on &quot;openSuSE
</span><br>
&nbsp;Linux 12.1&quot;,
<br>
<span class="quotelev1"> &gt; &quot;Solaris 10 x86_64&quot;, and &quot;Solaris 10 sparc&quot; with &quot;Sun C
</span><br>
&nbsp;5.12&quot; and
<br>
<span class="quotelev1"> &gt; gcc-4.8.0 in 64-bit mode. Unfortunately &quot;make&quot; breaks
</span><br>
&nbsp;with the same
<br>
<span class="quotelev1"> &gt; error for both compilers on both Solaris platforms.
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; tyr openmpi-1.7.3a1r29213-SunOS.sparc.64_cc 126 tail
</span><br>
&nbsp;-10 \
<br>
<span class="quotelev1"> &gt;&#160; log.make.SunOS.sparc.64_cc
</span><br>
<span class="quotelev1"> &gt; Making all in mca/if/posix_ipv4
</span><br>
<span class="quotelev1"> &gt; make[2]: Entering directory
</span><br>
&nbsp;`.../opal/mca/if/posix_ipv4'
<br>
<span class="quotelev1"> &gt;&#160; CC&#160; &#160; &#160;&#160;&#160;if_posix.lo
</span><br>
<span class="quotelev1"> &gt;
</span><br>
&nbsp;&quot;../../../../../openmpi-1.7.3a1r29213/opal/mca/if/posix_ipv4/if_posix.c&quot;,
<br>
<span class="quotelev1"> &gt;&#160; line 277: undefined struct/union member: ifr_mtu
</span><br>
<span class="quotelev1"> &gt; cc: acomp failed for
</span><br>
<span class="quotelev1"> &gt;&#160;
</span><br>
&nbsp;../../../../../openmpi-1.7.3a1r29213/opal/mca/if/posix_ipv4/if_posix.c
<br>
<span class="quotelev1"> &gt; make[2]: *** [if_posix.lo] Error 1
</span><br>
<span class="quotelev1"> &gt; make[2]: Leaving directory
</span><br>
&nbsp;`.../opal/mca/if/posix_ipv4'
<br>
<span class="quotelev1"> &gt; make[1]: *** [all-recursive] Error 1
</span><br>
<span class="quotelev1"> &gt; make[1]: Leaving directory `.../opal'
</span><br>
<span class="quotelev1"> &gt; make: *** [all-recursive] Error 1
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; tyr openmpi-1.7.3a1r29213-SunOS.sparc.64_gcc 131 tail
</span><br>
&nbsp;-12 \
<br>
<span class="quotelev1"> &gt;&#160; log.make.SunOS.sparc.64_gcc
</span><br>
<span class="quotelev1"> &gt; Making all in mca/if/posix_ipv4
</span><br>
<span class="quotelev1"> &gt; make[2]: Entering directory
</span><br>
&nbsp;`.../opal/mca/if/posix_ipv4'
<br>
<span class="quotelev1"> &gt;&#160; CC&#160; &#160; &#160;&#160;&#160;if_posix.lo
</span><br>
<span class="quotelev1"> &gt;
</span><br>
&nbsp;../../../../../openmpi-1.7.3a1r29213/opal/mca/if/posix_ipv4/if_posix.c:
<br>
<span class="quotelev1"> &gt;&#160; In function 'if_posix_open':
</span><br>
<span class="quotelev1"> &gt;
</span><br>
&nbsp;../../../../../openmpi-1.7.3a1r29213/opal/mca/if/posix_ipv4/if_posix.c:
<br>
<span class="quotelev1"> &gt;&#160; 277:31: error: 'struct ifreq' has no member named
</span><br>
&nbsp;'ifr_mtu'
<br>
<span class="quotelev1"> &gt;&#160; &#160; &#160; &#160; &#160;
</span><br>
&nbsp;&#160;&#160;&#160;intf-&gt;if_mtu = ifr-&gt;ifr_mtu;
<br>
<span class="quotelev1"> &gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;
</span><br>
&nbsp;&#160; &#160; &#160; &#160; &#160; &#160;
<br>
&nbsp;&#160;&#160;&#160;^
<br>
<span class="quotelev1"> &gt; make[2]: *** [if_posix.lo] Error 1
</span><br>
<span class="quotelev1"> &gt; make[2]: Leaving directory
</span><br>
&nbsp;`.../opal/mca/if/posix_ipv4'
<br>
<span class="quotelev1"> &gt; make[1]: *** [all-recursive] Error 1
</span><br>
<span class="quotelev1"> &gt; make[1]: Leaving directory `.../opal'
</span><br>
<span class="quotelev1"> &gt; make: *** [all-recursive] Error 1
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; I have had this problem before and Jeff solved it. Here
</span><br>
&nbsp;is my
<br>
<span class="quotelev1"> &gt; old e-mail.
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; Date: Tue, 7 May 2013 19:38:11 +0200 (CEST)
</span><br>
<span class="quotelev1"> &gt; From: Siegmar Gross &lt;Siegmar.Gross_at_[hidden]&gt;
</span><br>
<span class="quotelev1"> &gt; Subject: Re: commit/ompi-java: jsquyres: Up to SVN
</span><br>
&nbsp;r28392
<br>
<span class="quotelev1"> &gt; To: jsquyres_at_[hidden]
</span><br>
<span class="quotelev1"> &gt; Cc: Siegmar.Gross_at_[hidden]
</span><br>
<span class="quotelev1"> &gt; MIME-Version: 1.0
</span><br>
<span class="quotelev1"> &gt; Content-MD5: O1pjPK/1JiMXXZ/EHyMU0Q==
</span><br>
<span class="quotelev1"> &gt; X-HRZ-JLUG-MailScanner-Information: Passed JLUG virus
</span><br>
&nbsp;check
<br>
<span class="quotelev1"> &gt; X-HRZ-JLUG-MailScanner: No virus found
</span><br>
<span class="quotelev1"> &gt; X-Envelope-From: fd1026_at_[hidden]
</span><br>
<span class="quotelev1"> &gt; X-Spam-Status: No
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; Hello Jeff
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; Ok, I made a change in the OMPI trunk that should
</span><br>
&nbsp;fix this:
<br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt;&#160; &#160; <a href="https://svn.open-mpi.org/trac/ompi/changeset/28460">https://svn.open-mpi.org/trac/ompi/changeset/28460</a>
</span><br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; And I pulled it into the ompi-java hg repo.&#160;
</span><br>
&nbsp;Could you give
<br>
<span class="quotelev2"> &gt;&gt; it a whirl and let me know if this works for you?
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; Perfect :-)))).&#160; Now I can build Open MPI on
</span><br>
&nbsp;Solaris without
<br>
<span class="quotelev1"> &gt; &quot;#if 0&quot; :-). Thank you very much for your help.
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; &quot;make check&quot;&#160; still produces the old bus error on
</span><br>
&nbsp;Solaris Sparc.
<br>
<span class="quotelev1"> &gt; All checks are fine on Linux and Solaris x86_64.
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; ...
</span><br>
<span class="quotelev1"> &gt; PASS: ddt_test
</span><br>
<span class="quotelev1"> &gt; /bin/bash: line 5: 12453 Bus Error&#160; &#160; &#160;
</span><br>
&nbsp;&#160; &#160; &#160; &#160;&#160;&#160;${dir}$tst
<br>
<span class="quotelev1"> &gt; FAIL: ddt_raw
</span><br>
<span class="quotelev1"> &gt;
</span><br>
&nbsp;========================================================
<br>
<span class="quotelev1"> &gt; 1 of 5 tests failed
</span><br>
<span class="quotelev1"> &gt; Please report to <a href="http://www.open-mpi.org/community/help/">http://www.open-mpi.org/community/help/</a>
</span><br>
<span class="quotelev1"> &gt;
</span><br>
&nbsp;========================================================
<br>
<span class="quotelev1"> &gt; make[3]: *** [check-TESTS] Error 1
</span><br>
<span class="quotelev1"> &gt; ...
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; Kind regards
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; Siegmar
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev2"> &gt;&gt; On May 6, 2013, at 7:20 AM, Siegmar Gross 
</span><br>
<span class="quotelev1"> &gt; &lt;Siegmar.Gross_at_[hidden]&gt;
</span><br>
&nbsp;wrote:
<br>
<span class="quotelev2"> &gt;&gt; 
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; Hello Jeff
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
&nbsp;&quot;../../../../../ompi-java/opal/mca/if/posix_ipv4/if_posix.c&quot;,
<br>
<span class="quotelev2"> &gt;&gt;&gt;&gt;&gt;&gt; line 279: undefined struct/union
</span><br>
&nbsp;member: ifr_mtu
<br>
<span class="quotelev2"> &gt;&gt;&gt;&gt;&gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt;&gt;&gt;&gt;&gt; Sigh.&#160; Solaris kills me.&#160;
</span><br>
&nbsp;:-\
<br>
<span class="quotelev2"> &gt;&gt;&gt;&gt;&gt;&gt; 
</span><br>
<span class="quotelev2"> &gt;&gt;&gt;&gt;&gt;&gt; Just so I understand -- Solaris has
</span><br>
&nbsp;SIOCGIFMTU, but doesn't
<br>
<span class="quotelev2"> &gt;&gt;&gt;&gt;&gt;&gt; have struct ifreq.ifr_mtu?
</span><br>
<span class="quotelev1"> &gt;&gt;&gt;&gt;&gt; 
</span><br>
<span class="quotelev1"> &gt;&gt;&gt;&gt;&gt; I found SIOCGIFMTU in sys/sockio.h with
</span><br>
&nbsp;the following comment.
<br>
<span class="quotelev4"> &gt;&gt;&gt;&gt; 
</span><br>
<span class="quotelev4"> &gt;&gt;&gt;&gt; Is there a Solaris-defined constant we can
</span><br>
&nbsp;use here to know
<br>
<span class="quotelev4"> &gt;&gt;&gt;&gt; that we're on Solaris?&#160; If so, I can
</span><br>
&nbsp;effectively make that code
<br>
<span class="quotelev4"> &gt;&gt;&gt;&gt; only be there if SIOCFIGMTU exists and
</span><br>
&nbsp;we're not on Solaris.
<br>
<span class="quotelev3"> &gt;&gt;&gt; 
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; I searched our header files for &quot;sunos&quot; and
</span><br>
&nbsp;&quot;solaris&quot; with
<br>
<span class="quotelev3"> &gt;&gt;&gt; &quot;-ignore-case&quot;, but didn't find anything
</span><br>
&nbsp;useful. You have a very
<br>
<span class="quotelev3"> &gt;&gt;&gt; minimal environment, if you use &quot;sh&quot; and you
</span><br>
&nbsp;would have a useful
<br>
<span class="quotelev3"> &gt;&gt;&gt; environment variable, if you use &quot;tcsh&quot;.
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; 
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; tyr java 321 su -
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; ...
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; # env
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; HOME=/root
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; HZ=
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; LANG=C
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; LC_ALL=C
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; LOGNAME=root
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; MAIL=/var/mail/root
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; PATH=/usr/sbin:/usr/bin
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; SHELL=/sbin/sh
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; TERM=dtterm
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; TZ=Europe/Berlin
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; # tcsh
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; # env | grep TYPE
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; HOSTTYPE=sun4
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; OSTYPE=solaris
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; MACHTYPE=sparc
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; # 
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; 
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; The best solution would be &quot;uname -s&quot;, if that
</span><br>
&nbsp;is possible.
<br>
<span class="quotelev3"> &gt;&gt;&gt; 
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; # /usr/bin/uname -s
</span><br>
<span class="quotelev3"> &gt;&gt;&gt; SunOS
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; I would be grateful, if somebody can solve the problem
</span><br>
&nbsp;once more.
<br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; Kind regards
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; Siegmar
</span><br>
<span class="quotelev1"> &gt; 
</span><br>
<span class="quotelev1"> &gt; _______________________________________________
</span><br>
<span class="quotelev1"> &gt; users mailing list
</span><br>
<span class="quotelev1"> &gt; users_at_[hidden]
</span><br>
<span class="quotelev1"> &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;-- 
<br>
&nbsp;Jeff Squyres
<br>
&nbsp;jsquyres_at_[hidden]
<br>
&nbsp;For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a>
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;
<br>
&nbsp;------------------------------
<br>
&nbsp;
<br>
&nbsp;Subject: Digest Footer
<br>
&nbsp;
<br>
&nbsp;_______________________________________________
<br>
&nbsp;users mailing list
<br>
&nbsp;users_at_[hidden]
<br>
&nbsp;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
<br>
&nbsp;
<br>
&nbsp;------------------------------
<br>
&nbsp;
<br>
&nbsp;End of users Digest, Vol 2685, Issue 2
<br>
&nbsp;**************************************
<br>
&nbsp;
<br>
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="22725.php">Syed Ahsan Ali: "Re: [OMPI users] compilation aborted for Handler.cpp (code 2)"</a>
<li><strong>Previous message:</strong> <a href="22723.php">Jeff Squyres (jsquyres): "Re: [OMPI users] OpenMPI 1.6.3 problem"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
