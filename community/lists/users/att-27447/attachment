<div dir="ltr">David,<div><br></div><div>This is because of hcoll symbols conflict with ml coll module inside OMPI. HCOLL is derived from ml module. This issue is fixed in hcoll library and will be available in next HPCX release.</div><div><br></div><div>Some earlier discussion on this issue:</div><div><a href="http://www.open-mpi.org/community/lists/users/2015/06/27154.php">http://www.open-mpi.org/community/lists/users/2015/06/27154.php</a><br></div><div><a href="http://www.open-mpi.org/community/lists/devel/2015/06/17562.php">http://www.open-mpi.org/community/lists/devel/2015/06/17562.php</a><br></div><div><br></div><div>-Devendar</div><div class="gmail_extra"><br><div class="gmail_quote">On Wed, Aug 12, 2015 at 2:52 PM, David Shrader <span dir="ltr">&lt;<a href="mailto:dshrader@lanl.gov" target="_blank">dshrader@lanl.gov</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
  
    
  
  <div text="#000000" bgcolor="#FFFFFF">
    Interesting... the seg faults went away:<br>
    <br>
    
    <div>
      <span style="font-family:monospace"><span style="color:#000000;background-color:#ffffff">[dshrader@zo-fe1
          tests]$ export LD_PRELOAD=/usr/lib64/libhcoll.so
        </span><br><span class="">
        [dshrader@zo-fe1 tests]$ mpirun -n 2 -mca coll ^ml ./a.out
               <br>
        App launch reported: 1 (out of 1) daemons - 2 (out of 2) procs
        <br></span>
        [1439416182.732720] [zo-fe1:14690:0]         shm.c:65   MXM
         WARN  Could not open the KNEM device file at /dev/knem : No
        such file or direc<span class=""><br>
        tory. Won&#39;t use knem.
        <br></span>
        [1439416182.733640] [zo-fe1:14689:0]         shm.c:65   MXM
         WARN  Could not open the KNEM device file at /dev/knem : No
        such file or direc<span class=""><br>
        tory. Won&#39;t use knem.
        <br></span>
        0: Running on host <a href="http://zo-fe1.lanl.gov" target="_blank">zo-fe1.lanl.gov</a>
        <br>
        0: We have 2 processors
        <br>
        0: Hello 1! Processor 1 on host <a href="http://zo-fe1.lanl.gov" target="_blank">zo-fe1.lanl.gov</a> reporting for
        duty<br>
      </span></div>
    <br>
    This implies to me that some other library is being used instead of
    /usr/lib64/libhcoll.so, but I am not sure how that could be...<br>
    <br>
    Thanks,<br>
    David
    
    <br><div><div class="h5">
    <br>
    <div>On 08/12/2015 03:30 PM, Deva wrote:<br>
    </div>
    <blockquote type="cite">
      
      <div dir="ltr">Hi David,
        <div><br>
        </div>
        <div>I tried same tarball on OFED-1.5.4.1 and I could not
          reproduce the issue.  Can you do one more quick test with
          seeing LD_PRELOAD to hcoll lib?</div>
        <div><br>
        </div>
        <div><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#00b050">$LD_PRELOAD=&lt;path/to/hcoll/lib/libhcoll.so&gt;
             </span><span style="color:rgb(0,0,0);font-family:monospace;font-size:10.4000005722046px">mpirun
            -n 2 </span><span style="color:rgb(0,176,80);font-family:Calibri,sans-serif;font-size:14.6666669845581px"> </span><span style="color:rgb(0,176,80);font-family:Calibri,sans-serif;font-size:14.6666669845581px">-mca
            coll ^ml </span><span style="color:rgb(0,0,0);font-family:monospace;font-size:10.4000005722046px">./a.out</span><span style="color:rgb(0,176,80);font-family:Calibri,sans-serif;font-size:11pt"> 
             </span></div>
        <br>
        -Devendar</div>
      <div class="gmail_extra"><br>
        <div class="gmail_quote">On Wed, Aug 12, 2015 at 12:52 PM, David
          Shrader <span dir="ltr">&lt;<a href="mailto:dshrader@lanl.gov" target="_blank">dshrader@lanl.gov</a>&gt;</span>
          wrote:<br>
          <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
            <div text="#000000" bgcolor="#FFFFFF"> The admin that rolled
              the hcoll rpm that we&#39;re using (and got it in system
              space) said that she got it from
              hpcx-v1.3.336-gcc-OFED-1.5.4.1-redhat6.6-x86_64.tar.<br>
              <br>
              Thanks,<br>
              David
              <div>
                <div><br>
                  <br>
                  <div>On 08/12/2015 10:51 AM, Deva wrote:<br>
                  </div>
                </div>
              </div>
              <blockquote type="cite">
                <div>
                  <div>
                    <div dir="ltr">From where did you grab this HCOLL
                      lib?  MOFED or HPCX? what version?</div>
                    <div class="gmail_extra"><br>
                      <div class="gmail_quote">On Wed, Aug 12, 2015 at
                        9:47 AM, David Shrader <span dir="ltr">&lt;<a href="mailto:dshrader@lanl.gov" target="_blank"></a><a href="mailto:dshrader@lanl.gov" target="_blank">dshrader@lanl.gov</a>&gt;</span>
                        wrote:<br>
                        <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
                          <div text="#000000" bgcolor="#FFFFFF"> Hey
                            Devendar,<br>
                            <br>
                            It looks like I still get the error:<br>
                            <br>
                            <div> <span style="font-family:monospace"><span style="color:#000000;background-color:#ffffff">[dshrader@zo-fe1 tests]$
                                  mpirun -n 2 -mca coll ^ml ./a.out    </span><span><br>
                                  App launch reported: 1 (out of 1)
                                  daemons - 2 (out of 2) procs <br>
                                </span> [1439397957.351764]
                                [zo-fe1:14678:0]         shm.c:65   MXM
                                 WARN  Could not open the KNEM device
                                file at /dev/knem : No such file or
                                direc<span><br>
                                  tory. Won&#39;t use knem. <br>
                                </span> [1439397957.352704]
                                [zo-fe1:14677:0]         shm.c:65   MXM
                                 WARN  Could not open the KNEM device
                                file at /dev/knem : No such file or
                                direc<span><br>
                                  tory. Won&#39;t use knem. <br>
                                </span> [zo-fe1:14677:0] Caught signal
                                11 (Segmentation fault) <br>
                                [zo-fe1:14678:0] Caught signal 11
                                (Segmentation fault) <br>
                                <span> ==== backtrace ==== <br>
                                  2 0x0000000000056cdc
                                  mxm_handle_error()
 /scrap/jenkins/workspace/hpc-power-pack/label/r-vmb-rhel6-u6-x86-64-MOFED-CHECKER/hpcx_root/src/h<br>
                                  pcx-v1.3.336-gcc-OFED-1.5.4.1-redhat6.6-x86_64/mxm-v3.3/src/mxm/util/debug/debug.c:641


                                  <br>
                                  3 0x0000000000056e4c
                                  mxm_error_signal_handler()
 /scrap/jenkins/workspace/hpc-power-pack/label/r-vmb-rhel6-u6-x86-64-MOFED-CHECKER/hpcx_ro<br>
                                  ot/src/hpcx-v1.3.336-gcc-OFED-1.5.4.1-redhat6.6-x86_64/mxm-v3.3/src/mxm/util/debug/debug.c:616


                                  <br>
                                  4 0x00000000000326a0 killpg()  ??:0 <br>
                                </span> 5 0x00000000000b82cb
                                base_bcol_basesmuma_setup_library_buffers()
                                 ??:0 <br>
                                <span> 6 0x00000000000969e3
                                  hmca_bcol_basesmuma_comm_query()  ??:0
                                  <br>
                                  7 0x0000000000032ee3
                                  hmca_coll_ml_tree_hierarchy_discovery()
                                   coll_ml_module.c:0 <br>
                                  8 0x000000000002fda2
                                  hmca_coll_ml_comm_query()  ??:0 <br>
                                  9 0x000000000006ace9
                                  hcoll_create_context()  ??:0 <br>
                                </span> 10 0x00000000000f9706
                                mca_coll_hcoll_comm_query()  ??:0 <br>
                                11 0x00000000000f684e
                                mca_coll_base_comm_select()  ??:0 <br>
                                12 0x0000000000073fc4 ompi_mpi_init()
                                 ??:0 <br>
                                13 0x0000000000092ea0 PMPI_Init()  ??:0
                                <br>
                                <span> 14 0x00000000004009b6 main()
                                   ??:0 <br>
                                  15 0x000000000001ed5d
                                  __libc_start_main()  ??:0 <br>
                                  16 0x00000000004008c9 _start()  ??:0 <br>
                                  =================== <br>
                                  ==== backtrace ==== <br>
                                  2 0x0000000000056cdc
                                  mxm_handle_error()
 /scrap/jenkins/workspace/hpc-power-pack/label/r-vmb-rhel6-u6-x86-64-MOFED-CHECKER/hpcx_root/src/h<br>
                                  pcx-v1.3.336-gcc-OFED-1.5.4.1-redhat6.6-x86_64/mxm-v3.3/src/mxm/util/debug/debug.c:641


                                  <br>
                                  3 0x0000000000056e4c
                                  mxm_error_signal_handler()
 /scrap/jenkins/workspace/hpc-power-pack/label/r-vmb-rhel6-u6-x86-64-MOFED-CHECKER/hpcx_ro<br>
                                  ot/src/hpcx-v1.3.336-gcc-OFED-1.5.4.1-redhat6.6-x86_64/mxm-v3.3/src/mxm/util/debug/debug.c:616


                                  <br>
                                  4 0x00000000000326a0 killpg()  ??:0 <br>
                                </span> 5 0x00000000000b82cb
                                base_bcol_basesmuma_setup_library_buffers()
                                 ??:0 <br>
                                <span> 6 0x00000000000969e3
                                  hmca_bcol_basesmuma_comm_query()  ??:0
                                  <br>
                                  7 0x0000000000032ee3
                                  hmca_coll_ml_tree_hierarchy_discovery()
                                   coll_ml_module.c:0 <br>
                                  8 0x000000000002fda2
                                  hmca_coll_ml_comm_query()  ??:0 <br>
                                  9 0x000000000006ace9
                                  hcoll_create_context()  ??:0 <br>
                                </span> 10 0x00000000000f9706
                                mca_coll_hcoll_comm_query()  ??:0 <br>
                                11 0x00000000000f684e
                                mca_coll_base_comm_select()  ??:0 <br>
                                12 0x0000000000073fc4 ompi_mpi_init()
                                 ??:0 <br>
                                13 0x0000000000092ea0 PMPI_Init()  ??:0
                                <br>
                                <span> 14 0x00000000004009b6 main()
                                   ??:0 <br>
                                  15 0x000000000001ed5d
                                  __libc_start_main()  ??:0 <br>
                                  16 0x00000000004008c9 _start()  ??:0 <br>
                                  =================== <br>
                                  --------------------------------------------------------------------------


                                  <br>
                                </span> mpirun noticed that process rank
                                1 with PID 14678 on node zo-fe1 exited
                                on signal 11 (Segmentation fault). <br>
--------------------------------------------------------------------------<br>
                              </span></div>
                            <br>
                            Thanks,<br>
                            David <br>
                            <div>
                              <div> <br>
                                <div>On 08/12/2015 10:42 AM, Deva wrote:<br>
                                </div>
                              </div>
                            </div>
                            <blockquote type="cite">
                              <div>
                                <div>
                                  <div dir="ltr">
                                    <div>Hi David,</div>
                                    <div><br>
                                    </div>
                                    This issue is from hcoll library.
                                    This could be because of symbol
                                    conflict with ml module.  This is
                                    fixed recently in HCOLL.  Can you
                                    try with &quot;-mca coll ^ml&quot; and see if
                                    this workaround works in your setup?
                                    <div><br>
                                    </div>
                                    <div>-Devendar</div>
                                  </div>
                                  <div class="gmail_extra"><br>
                                    <div class="gmail_quote">On Wed, Aug
                                      12, 2015 at 9:30 AM, David Shrader
                                      <span dir="ltr">&lt;<a href="mailto:dshrader@lanl.gov" target="_blank"></a><a href="mailto:dshrader@lanl.gov" target="_blank">dshrader@lanl.gov</a>&gt;</span>
                                      wrote:<br>
                                      <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
                                        <div text="#000000" bgcolor="#FFFFFF"> Hello
                                          Gilles,<br>
                                          <br>
                                          Thank you very much for the
                                          patch! It is much more
                                          complete than mine. Using that
                                          patch and re-running <a href="http://autogen.pl" target="_blank">autogen.pl</a>,
                                          I am able to build 1.8.8 with
                                          &#39;./configure --with-hcoll&#39;
                                          without errors.<br>
                                          <br>
                                          I do have issues when it comes
                                          to running 1.8.8 with hcoll
                                          built in, however. In my quick
                                          sanity test of running a basic
                                          parallel hello world C
                                          program, I get the following:<br>
                                          <br>
                                          <div> <span style="font-family:monospace"><span style="color:#000000;background-color:#ffffff">[dshrader@zo-fe1 tests]$
                                                mpirun -n 2 ./a.out   </span><br>
                                              App launch reported: 1
                                              (out of 1) daemons - 2
                                              (out of 2) procs <br>
                                              [1439390789.039197]
                                              [zo-fe1:31354:0]
                                                      shm.c:65   MXM
                                               WARN  Could not open the
                                              KNEM device file at
                                              /dev/knem : No such file
                                              or direc<br>
                                              tory. Won&#39;t use knem. <br>
                                              [1439390789.040265]
                                              [zo-fe1:31353:0]
                                                      shm.c:65   MXM
                                               WARN  Could not open the
                                              KNEM device file at
                                              /dev/knem : No such file
                                              or direc<br>
                                              tory. Won&#39;t use knem. <br>
                                              [zo-fe1:31353:0] Caught
                                              signal 11 (Segmentation
                                              fault) <br>
                                              [zo-fe1:31354:0] Caught
                                              signal 11 (Segmentation
                                              fault) <br>
                                              ==== backtrace ==== <br>
                                              2 0x0000000000056cdc
                                              mxm_handle_error()
 /scrap/jenkins/workspace/hpc-power-pack/label/r-vmb-rhel6-u6-x86-64-MOFED-CHECKER/hpcx_root/src/h<br>
                                              pcx-v1.3.336-gcc-OFED-1.5.4.1-redhat6.6-x86_64/mxm-v3.3/src/mxm/util/debug/debug.c:641



                                              <br>
                                              3 0x0000000000056e4c
                                              mxm_error_signal_handler()
 /scrap/jenkins/workspace/hpc-power-pack/label/r-vmb-rhel6-u6-x86-64-MOFED-CHECKER/hpcx_ro<br>
                                              ot/src/hpcx-v1.3.336-gcc-OFED-1.5.4.1-redhat6.6-x86_64/mxm-v3.3/src/mxm/util/debug/debug.c:616



                                              <br>
                                              4 0x00000000000326a0
                                              killpg()  ??:0 <br>
                                              5 0x00000000000b91eb
                                              base_bcol_basesmuma_setup_library_buffers()
                                               ??:0 <br>
                                              6 0x00000000000969e3
                                              hmca_bcol_basesmuma_comm_query()
                                               ??:0 <br>
                                              7 0x0000000000032ee3
                                              hmca_coll_ml_tree_hierarchy_discovery()
                                               coll_ml_module.c:0 <br>
                                              8 0x000000000002fda2
                                              hmca_coll_ml_comm_query()
                                               ??:0 <br>
                                              9 0x000000000006ace9
                                              hcoll_create_context()
                                               ??:0 <br>
                                              10 0x00000000000fa626
                                              mca_coll_hcoll_comm_query()
                                               ??:0 <br>
                                              11 0x00000000000f776e
                                              mca_coll_base_comm_select()
                                               ??:0 <br>
                                              12 0x0000000000074ee4
                                              ompi_mpi_init()  ??:0 <br>
                                              13 0x0000000000093dc0
                                              PMPI_Init()  ??:0 <br>
                                              14 0x00000000004009b6
                                              main()  ??:0 <br>
                                              15 0x000000000001ed5d
                                              __libc_start_main()  ??:0
                                              <br>
                                              16 0x00000000004008c9
                                              _start()  ??:0 <br>
                                              =================== <br>
                                              ==== backtrace ==== <br>
                                              2 0x0000000000056cdc
                                              mxm_handle_error()
 /scrap/jenkins/workspace/hpc-power-pack/label/r-vmb-rhel6-u6-x86-64-MOFED-CHECKER/hpcx_root/src/h<br>
                                              pcx-v1.3.336-gcc-OFED-1.5.4.1-redhat6.6-x86_64/mxm-v3.3/src/mxm/util/debug/debug.c:641



                                              <br>
                                              3 0x0000000000056e4c
                                              mxm_error_signal_handler()
 /scrap/jenkins/workspace/hpc-power-pack/label/r-vmb-rhel6-u6-x86-64-MOFED-CHECKER/hpcx_ro<br>
                                              ot/src/hpcx-v1.3.336-gcc-OFED-1.5.4.1-redhat6.6-x86_64/mxm-v3.3/src/mxm/util/debug/debug.c:616



                                              <br>
                                              4 0x00000000000326a0
                                              killpg()  ??:0 <br>
                                              5 0x00000000000b91eb
                                              base_bcol_basesmuma_setup_library_buffers()
                                               ??:0 <br>
                                              6 0x00000000000969e3
                                              hmca_bcol_basesmuma_comm_query()
                                               ??:0 <br>
                                              7 0x0000000000032ee3
                                              hmca_coll_ml_tree_hierarchy_discovery()
                                               coll_ml_module.c:0 <br>
                                              8 0x000000000002fda2
                                              hmca_coll_ml_comm_query()
                                               ??:0 <br>
                                              9 0x000000000006ace9
                                              hcoll_create_context()
                                               ??:0 <br>
                                              10 0x00000000000fa626
                                              mca_coll_hcoll_comm_query()
                                               ??:0 <br>
                                              11 0x00000000000f776e
                                              mca_coll_base_comm_select()
                                               ??:0 <br>
                                              12 0x0000000000074ee4
                                              ompi_mpi_init()  ??:0 <br>
                                              13 0x0000000000093dc0
                                              PMPI_Init()  ??:0 <br>
                                              14 0x00000000004009b6
                                              main()  ??:0 <br>
                                              15 0x000000000001ed5d
                                              __libc_start_main()  ??:0
                                              <br>
                                              16 0x00000000004008c9
                                              _start()  ??:0 <br>
                                              =================== <br>
                                              --------------------------------------------------------------------------



                                              <br>
                                              mpirun noticed that
                                              process rank 0 with PID
                                              31353 on node zo-fe1
                                              exited on signal 11
                                              (Segmentation fault). <br>
--------------------------------------------------------------------------<br>
                                            </span></div>
                                          <br>
                                          I do not get this message with
                                          only 1 process.<br>
                                          <br>
                                          I am using hcoll 3.2.748.
                                          Could this be an issue with
                                          hcoll itself or something with
                                          my ompi build?<br>
                                          <br>
                                          Thanks,<br>
                                          David <br>
                                          <div>
                                            <div> <br>
                                              <div>On 08/12/2015 12:26
                                                AM, Gilles Gouaillardet
                                                wrote:<br>
                                              </div>
                                            </div>
                                          </div>
                                          <blockquote type="cite">
                                            <div>
                                              <div> Thanks David,<br>
                                                <br>
                                                i made a PR for the v1.8
                                                branch at <a href="https://github.com/open-mpi/ompi-release/pull/492" target="_blank"></a><a href="https://github.com/open-mpi/ompi-release/pull/492" target="_blank">https://github.com/open-mpi/ompi-release/pull/492</a><br>
                                                <br>
                                                the patch is attached
                                                (it required some
                                                back-porting)<br>
                                                <br>
                                                Cheers,<br>
                                                <br>
                                                Gilles<br>
                                                <br>
                                                <div>On 8/12/2015 4:01
                                                  AM, David Shrader
                                                  wrote:<br>
                                                </div>
                                                <blockquote type="cite">
                                                  I have cloned Gilles&#39;
                                                  topic/hcoll_config
                                                  branch and, after
                                                  running <a href="http://autogen.pl" target="_blank">autogen.pl</a>, have found that
                                                  &#39;./configure
                                                  --with-hcoll&#39; does
                                                  indeed work now. I
                                                  used Gilles&#39; branch as
                                                  I wasn&#39;t sure how best
                                                  to get the pull
                                                  request changes in to
                                                  my own clone of
                                                  master. It looks like
                                                  the proper checks are
                                                  happening, too:<br>
                                                  <br>
                                                  <div> <span style="font-family:monospace"><span style="color:#000000;background-color:#ffffff">--- MCA component coll:</span><span style="color:#000000;background-color:#ffff54">hcoll</span><span style="color:#000000;background-color:#ffffff">
                                                        (m4
                                                        configuration
                                                        macro) </span><br>
                                                      checking for MCA
                                                      component coll:<span style="color:#000000;background-color:#ffff54">hcoll</span><span style="color:#000000;background-color:#ffffff">
                                                        compile mode...
                                                        dso </span><br>
                                                      checking --with-<span style="color:#000000;background-color:#ffff54">hcoll</span><span style="color:#000000;background-color:#ffffff">
                                                        value... simple
                                                        ok (unspecified)
                                                      </span><br>
                                                      checking <span style="color:#000000;background-color:#ffff54">hcoll</span><span style="color:#000000;background-color:#ffffff">/api/</span><span style="color:#000000;background-color:#ffff54">hcoll</span><span style="color:#000000;background-color:#ffffff">_api.h usability... yes </span><br>
                                                      checking <span style="color:#000000;background-color:#ffff54">hcoll</span><span style="color:#000000;background-color:#ffffff">/api/</span><span style="color:#000000;background-color:#ffff54">hcoll</span><span style="color:#000000;background-color:#ffffff">_api.h presence... yes </span><br>
                                                      checking for <span style="color:#000000;background-color:#ffff54">hcoll</span><span style="color:#000000;background-color:#ffffff">/api/</span><span style="color:#000000;background-color:#ffff54">hcoll</span><span style="color:#000000;background-color:#ffffff">_api.h...
                                                        yes </span><br>
                                                      looking for
                                                      library without
                                                      search path <br>
                                                      checking for
                                                      library containing
                                                      <span style="color:#000000;background-color:#ffff54">hcoll</span><span style="color:#000000;background-color:#ffffff">_get_version... -l</span><span style="color:#000000;background-color:#ffff54">hcoll</span><span style="color:#000000;background-color:#ffffff">
                                                      </span><br>
                                                      checking if MCA
                                                      component coll:<span style="color:#000000;background-color:#ffff54">hcoll</span><span style="color:#000000;background-color:#ffffff">
                                                        can compile...
                                                        yes</span><br>
                                                    </span></div>
                                                  <br>
                                                  I haven&#39;t checked
                                                  whether or not Open
                                                  MPI builds
                                                  successfully as I
                                                  don&#39;t have much
                                                  experience running off
                                                  of the latest source.
                                                  For now, I think I
                                                  will try to generate a
                                                  patch to the 1.8.8
                                                  configure script and
                                                  see if that works as
                                                  expected.<br>
                                                  <br>
                                                  Thanks,<br>
                                                  David <br>
                                                  <br>
                                                  <div>On 08/11/2015
                                                    06:34 AM, Jeff
                                                    Squyres (jsquyres)
                                                    wrote:<br>
                                                  </div>
                                                  <blockquote type="cite">
                                                    <pre>On Aug 11, 2015, at 1:39 AM, Åke Sandgren <a href="mailto:ake.sandgren@hpc2n.umu.se" target="_blank">&lt;ake.sandgren@hpc2n.umu.se&gt;</a> wrote:
</pre>
                                                    <blockquote type="cite">
                                                      <pre>Please fix the hcoll test (and code) to be correct.

Any configure test that adds /usr/lib and/or /usr/include to any compile flags is broken.
</pre>
                                                    </blockquote>
                                                    <pre>+1

Gilles filed <a href="https://github.com/open-mpi/ompi/pull/796" target="_blank">https://github.com/open-mpi/ompi/pull/796</a>; I just added some comments to it.

</pre>
                                                  </blockquote>
                                                  <br>
                                                  <pre cols="72">-- 
David Shrader
HPC-3 High Performance Computer Systems
Los Alamos National Lab
Email: dshrader &lt;at&gt; <a href="http://lanl.gov" target="_blank">lanl.gov</a>
</pre>
                                                  <br>
                                                  <fieldset></fieldset>
                                                  <br>
                                                  <pre>_______________________________________________
users mailing list
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/08/27432.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/08/27432.php</a></pre>
                                                </blockquote>
                                                <br>
                                                <br>
                                                <fieldset></fieldset>
                                                <br>
                                              </div>
                                            </div>
                                            <pre><div><div>_______________________________________________
users mailing list
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/08/27434.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/08/27434.php</a></pre>
                                          </blockquote>
                                          <span> <br>
                                            <pre cols="72">-- 
David Shrader
HPC-3 High Performance Computer Systems
Los Alamos National Lab
Email: dshrader &lt;at&gt; <a href="http://lanl.gov" target="_blank">lanl.gov</a>
</pre>
                                          </span></div>
                                        <br>
_______________________________________________<br>
                                        users mailing list<br>
                                        <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
                                        Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank"></a><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                                        Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/08/27438.php" rel="noreferrer" target="_blank"></a><a href="http://www.open-mpi.org/community/lists/users/2015/08/27438.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/08/27438.php</a><br>
                                      </blockquote>
                                    </div>
                                    <br>
                                    <br clear="all">
                                    <div><br>
                                    </div>
                                    -- <br>
                                    <div><br>
                                      <br>
                                      -Devendar</div>
                                  </div>
                                  <br>
                                  <fieldset></fieldset>
                                  <br>
                                </div>
                              </div>
                              <pre><div><div>_______________________________________________
users mailing list
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/08/27439.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/08/27439.php</a></pre>
                            </blockquote>
                            <span> <br>
                              <pre cols="72">-- 
David Shrader
HPC-3 High Performance Computer Systems
Los Alamos National Lab
Email: dshrader &lt;at&gt; <a href="http://lanl.gov" target="_blank">lanl.gov</a>
</pre>
                            </span></div>
                          <br>
_______________________________________________<br>
                          users mailing list<br>
                          <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
                          Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                          Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/08/27440.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/08/27440.php</a><br>
                        </blockquote>
                      </div>
                      <br>
                      <br clear="all">
                      <div><br>
                      </div>
                      -- <br>
                      <div><br>
                        <br>
                        -Devendar</div>
                    </div>
                    <br>
                    <fieldset></fieldset>
                    <br>
                  </div>
                </div>
                <pre><div><div>_______________________________________________
users mailing list
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/08/27441.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/08/27441.php</a></pre>
              </blockquote>
              <span> <br>
                <pre cols="72">-- 
David Shrader
HPC-3 High Performance Computer Systems
Los Alamos National Lab
Email: dshrader &lt;at&gt; <a href="http://lanl.gov" target="_blank">lanl.gov</a>
</pre>
              </span></div>
            <br>
            _______________________________________________<br>
            users mailing list<br>
            <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
            Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
            Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/08/27445.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/08/27445.php</a><br>
          </blockquote>
        </div>
        <br>
        <br clear="all">
        <div><br>
        </div>
        -- <br>
        <div><br>
          <br>
          -Devendar</div>
      </div>
    </blockquote>
    <br>
    <pre cols="72">-- 
David Shrader
HPC-3 High Performance Computer Systems
Los Alamos National Lab
Email: dshrader &lt;at&gt; <a href="http://lanl.gov" target="_blank">lanl.gov</a>
</pre>
  </div></div></div>

</blockquote></div><br><br clear="all"><div><br></div>-- <br><div class="gmail_signature"><br><br>-Devendar</div>
</div></div>

