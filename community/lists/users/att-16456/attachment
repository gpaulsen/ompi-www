<div class="gmail_quote">On Thu, May 5, 2011 at 23:15, Paul Monday (Parallel Scientific) <span dir="ltr">&lt;<a href="mailto:paul.monday@parsci.com">paul.monday@parsci.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
<div style="word-wrap:break-word">Hi, I&#39;m hoping someone can help me locate a SpMV benchmark that runs w/ Open MPI so I can benchmark how my systems are interacting with the network as I add nodes / cores to the pool of systems.  I can find SpMV benchmarks for single processor / OpenMP all over, but these networked ones are proving harder to come by.  I located Lis (<a href="http://www.ssisc.org/lis/" target="_blank">http://www.ssisc.org/lis/</a>) but it seems more of a solver then a benchmarking program.</div>
</blockquote></div><br><div>I would suggest using PETSc. It is a solvers library rather than a contrived benchmark suite, but the examples give you access to many different matrices and you can use many different formats without changing the code. If you run with -log_summary, you will get a useful table showing the performance of different operations (time/balance/communication/reductions/flops/etc). Also note that SpMV is usually not an end in its own right, usually it is part of a preconditioned Krylov iteration, so the performance of all the pieces matter.</div>
<div><br></div><div>If you are concerned with absolute performance then you should consider using petsc-dev since it tends to have better memory performance due to software prefetch. This is important for good reuse of high-level caches since otherwise the matrix entries flush out the useful stuff. It usually makes between a 20 and 30% improvement, a bit more for some symmetric and triangular kernels. Many of the sparse matrix kernels did not have software prefetch as of the 3.1 release. Remember:</div>
<div><br></div><div>&quot;The easiest way to make software scalable is to make it sequentially inefficient.&quot; (Gropp, 1999)</div>

