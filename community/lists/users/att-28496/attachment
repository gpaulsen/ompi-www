<div dir="ltr">I don&#39;t know about bulletproof, but Win_shared_query is the *only* valid way to get the addresses of memory in other processes associated with a window.<div><br></div><div>The default for Win_allocate_shared is contiguous memory, but it can and likely will be mapped differently into each process, in which case only relative offsets are transferrable.<br><div><br>Jeff<br><div><div class="gmail_extra"><br><div class="gmail_quote">On Wed, Feb 10, 2016 at 4:19 AM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles.gouaillardet@gmail.com" target="_blank">gilles.gouaillardet@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Peter,<div><br></div><div>The bulletproof way is to use MPI_Win_shared_query after MPI_Win_allocate_shared.</div><div>I do not know if current behavior is a bug or a feature...</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<div><div class="h5"><br><br>On Wednesday, February 10, 2016, Peter Wind &lt;<a href="mailto:peter.wind@met.no" target="_blank">peter.wind@met.no</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi,<br>
<br>
Under fortran, MPI_Win_allocate_shared is called with a window size of zero for some processes.<br>
The output pointer is then not valid for these processes (null pointer).<br>
Did I understood this wrongly? shouldn&#39;t the pointers be contiguous, so that for a zero sized window, the pointer should point to the start of the segment of the next rank?<br>
The documentation explicitly specifies &quot;size = 0 is valid&quot;.<br>
<br>
Attached a small code, where rank=0 allocate a window of size zero. All the other ranks get valid pointers, except rank 0.<br>
<br>
Best regards,<br>
Peter<br>
_______________________________________________<br>
users mailing list<br>
<a>users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/02/28485.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/02/28485.php</a><br>
</blockquote></div></div></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/02/28493.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/02/28493.php</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br><div class="gmail_signature">Jeff Hammond<br><a href="mailto:jeff.science@gmail.com" target="_blank">jeff.science@gmail.com</a><br><a href="http://jeffhammond.github.io/" target="_blank">http://jeffhammond.github.io/</a></div>
</div></div></div></div></div>

