<html>
  <head>
    <meta content="text/html; charset=ISO-8859-1"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    <tt>Hi,<br>
      &nbsp;&nbsp; I'd like to point out that Cray doesn't run a Work Load Manager
      (WLM)<br>
      &nbsp; on the compute nodes. So if you use PBS or Torque/Moab, your job<br>
      &nbsp; ends up on the login node. You have to use something like
      "aprun"<br>
      &nbsp; or "ccmrun" to launch the job on the compute nodes.<br>
      &nbsp; Unless "mpirun" or "mpiexec" is Cray aware, it is trying to <br>
      &nbsp; launch processes on the login or MOM node.<br>
      <br>
      &nbsp;&nbsp; I've only run OpenMPI linked codes in CCM (Cray Cluster<br>
      &nbsp; Compatability) mode. On a system with PBS/Torque/Moab, I use:<br>
      <br>
      &nbsp; qsub -I -lgres=ccm -lmppwidth=32 -lmppnppn=16<br>
      <br>
      &nbsp; This gives me an interactive</tt><tt><tt> PBS/Torque/Moab</tt>
      session:<br>
      &nbsp;&nbsp; I then do:<br>
      &nbsp;&nbsp;&nbsp;&nbsp; 1) cd $PBS_O_WORKDIR<br>
      &nbsp;&nbsp;&nbsp;&nbsp; 2) module load ccm # gets access to ccmrun command<br>
      &nbsp;&nbsp;&nbsp;&nbsp; 3) setenv PATH /lus/scratch/whitaker/OpenMPI/bin:$PATH<br>
      &nbsp;&nbsp;&nbsp;&nbsp; 4) setenv LD_LIBRARY_PATH /lus/scratch/whitaker/OpenMPI/lib<br>
      &nbsp;&nbsp;&nbsp;&nbsp; 5) \rm -rf hosts <br>
      &nbsp;&nbsp;&nbsp;&nbsp; 6) cat $PBS_NODEFILE &gt; hosts<br>
      &nbsp;&nbsp;&nbsp;&nbsp; 7) ccmrun /lus/scratch/whitaker/OpenMPI/bin/mpirun --mca plm
      ^tm --mca ras ^tm --mca btl openib,sm,self&nbsp; -np 32 -machinefile
      hosts ./hello<br>
      <br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If you are running under Torque/Moab, OpenMPI will attempt<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; to use the Torque/Moab API to launch the job. This won't
      work<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; since Cray does not run a Torque/Moab MOM process<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; on the compute nodes. Hence, you have to turn off OpenMPI's
      <br>
      &nbsp; &nbsp; &nbsp; attempt to use Torque/Moab.<br>
      <br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; OpenMPI has a version that speaks uGNI natively on the<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Cray. I have no experience with that.<br>
      <br>
      ccmrun /lus/scratch/whitaker/OpenMPI/bin/mpirun --mca plm ^tm
      --mca ras ^tm -np 32 -machinefile hosts ./hello<br>
      &nbsp;Hello World!, I am 0 of 32 (NodeID=nid00056)<br>
      &nbsp;Hello World!, I am 1 of 32 (NodeID=nid00056)<br>
      &nbsp;Hello World!, I am 2 of 32 (NodeID=nid00056)<br>
      &nbsp;Hello World!, I am 3 of 32 (NodeID=nid00056)<br>
      &nbsp;Hello World!, I am 4 of 32 (NodeID=nid00056)<br>
      &nbsp;Hello World!, I am 5 of 32 (NodeID=nid00056)<br>
      &nbsp;Hello World!, I am 6 of 32 (NodeID=nid00056)<br>
      &nbsp;Hello World!, I am 7 of 32 (NodeID=nid00056)<br>
      &nbsp;Hello World!, I am 8 of 32 (NodeID=nid00056)<br>
      &nbsp;Hello World!, I am 9 of 32 (NodeID=nid00056)<br>
      &nbsp;Hello World!, I am 10 of 32 (NodeID=nid00056)<br>
      &nbsp;Hello World!, I am 11 of 32 (NodeID=nid00056)<br>
      &nbsp;Hello World!, I am 12 of 32 (NodeID=nid00056)<br>
      &nbsp;Hello World!, I am 13 of 32 (NodeID=nid00056)<br>
      &nbsp;Hello World!, I am 14 of 32 (NodeID=nid00056)<br>
      &nbsp;Hello World!, I am 15 of 32 (NodeID=nid00056)<br>
      &nbsp;Hello World!, I am 16 of 32 (NodeID=nid00057)<br>
      &nbsp;Hello World!, I am 17 of 32 (NodeID=nid00057)<br>
      &nbsp;Hello World!, I am 18 of 32 (NodeID=nid00057)<br>
      &nbsp;Hello World!, I am 21 of 32 (NodeID=nid00057)<br>
      &nbsp;Hello World!, I am 19 of 32 (NodeID=nid00057)<br>
      &nbsp;Hello World!, I am 20 of 32 (NodeID=nid00057)<br>
      &nbsp;Hello World!, I am 22 of 32 (NodeID=nid00057)<br>
      &nbsp;Hello World!, I am 23 of 32 (NodeID=nid00057)<br>
      &nbsp;Hello World!, I am 24 of 32 (NodeID=nid00057)<br>
      &nbsp;Hello World!, I am 25 of 32 (NodeID=nid00057)<br>
      &nbsp;Hello World!, I am 26 of 32 (NodeID=nid00057)<br>
      &nbsp;Hello World!, I am 27 of 32 (NodeID=nid00057)<br>
      &nbsp;Hello World!, I am 28 of 32 (NodeID=nid00057)<br>
      &nbsp;Hello World!, I am 29 of 32 (NodeID=nid00057)<br>
      &nbsp;Hello World!, I am 30 of 32 (NodeID=nid00057)<br>
      &nbsp;Hello World!, I am 31 of 32 (NodeID=nid00057)<br>
      <br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Hope this helps,<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Dave<br>
      <br>
      <br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br>
    </tt>
    <div class="moz-cite-prefix">On 11/23/2013 05:27 PM, Teranishi,
      Keita wrote:<br>
    </div>
    <blockquote cite="mid:CEB6781B.45D39%25knteran@sandia.gov"
      type="cite">
      <meta http-equiv="Content-Type" content="text/html;
        charset=ISO-8859-1">
      <div>
        <div>
          <div>Here is the module environment, and I allocate
            interactive node by "qsub -I -lmppwidth=32 -lmppnppn=16."</div>
          <div>
            <div>module list</div>
            <div>Currently Loaded Modulefiles:</div>
            <div>&nbsp; 1) modules/3.2.6.7</div>
            <div>&nbsp; 2) craype-network-gemini</div>
            <div>&nbsp; 3) cray-mpich2/5.6.4</div>
            <div>&nbsp; 4) atp/1.6.3</div>
            <div>&nbsp; 5) xe-sysroot/4.1.40</div>
            <div>&nbsp; 6) switch/1.0-1.0401.36779.2.72.gem</div>
            <div>&nbsp; 7) shared-root/1.0-1.0401.37253.3.50.gem</div>
            <div>&nbsp; 8) pdsh/2.26-1.0401.37449.1.1.gem</div>
            <div>&nbsp; 9) nodehealth/5.0-1.0401.38460.12.18.gem</div>
            <div>&nbsp;10) lbcd/2.1-1.0401.35360.1.2.gem</div>
            <div>&nbsp;11) hosts/1.0-1.0401.35364.1.115.gem</div>
            <div>&nbsp;12) configuration/1.0-1.0401.35391.1.2.gem</div>
            <div>&nbsp;13) ccm/2.2.0-1.0401.37254.2.142</div>
            <div>&nbsp;14) audit/1.0.0-1.0401.37969.2.32.gem</div>
            <div>&nbsp;15) rca/1.0.0-2.0401.38656.2.2.gem</div>
            <div>&nbsp;16) dvs/1.8.6_0.9.0-1.0401.1401.1.120</div>
            <div>&nbsp;17) csa/3.0.0-1_2.0401.37452.4.50.gem</div>
            <div>&nbsp;18) job/1.5.5-0.1_2.0401.35380.1.10.gem</div>
            <div>&nbsp;19) xpmem/0.1-2.0401.36790.4.3.gem</div>
            <div>&nbsp;20) gni-headers/2.1-1.0401.5675.4.4.gem</div>
            <div>&nbsp;21) dmapp/3.2.1-1.0401.5983.4.5.gem</div>
            <div>&nbsp;22) pmi/4.0.1-1.0000.9421.73.3.gem</div>
            <div>&nbsp;23) ugni/4.0-1.0401.5928.9.5.gem</div>
            <div>&nbsp;24) udreg/2.3.2-1.0401.5929.3.3.gem</div>
            <div>&nbsp;25) xt-libsci/12.0.00</div>
            <div>&nbsp;26) xt-totalview/8.12.0</div>
            <div>&nbsp;27) totalview-support/1.1.5</div>
            <div>&nbsp;28) gcc/4.7.2</div>
            <div>&nbsp;29) xt-asyncpe/5.22</div>
            <div>&nbsp;30) eswrap/1.0.8</div>
            <div>&nbsp;31) craype-mc8</div>
            <div>&nbsp;32) PrgEnv-gnu/4.1.40</div>
            <div>&nbsp;33) moab/5.4.4</div>
          </div>
          <div><br>
          </div>
          <div><br>
          </div>
          <div>In interactive mode (as well as batch mode), "aprun &#8211;np
            32" can run my OpenMPI code. &nbsp;</div>
          <div>
            <div>aprun -n 32 ./cpi</div>
            <div>Process 5 of 32 is on nid00015</div>
            <div>Process 7 of 32 is on nid00015</div>
            <div>Process 2 of 32 is on nid00015</div>
            <div>Process 0 of 32 is on nid00015</div>
            <div>Process 13 of 32 is on nid00015</div>
            <div>Process 10 of 32 is on nid00015</div>
            <div>Process 3 of 32 is on nid00015</div>
            <div>Process 1 of 32 is on nid00015</div>
            <div>Process 6 of 32 is on nid00015</div>
            <div>Process 4 of 32 is on nid00015</div>
            <div>Process 15 of 32 is on nid00015</div>
            <div>Process 9 of 32 is on nid00015</div>
            <div>Process 12 of 32 is on nid00015</div>
            <div>Process 8 of 32 is on nid00015</div>
            <div>Process 11 of 32 is on nid00015</div>
            <div>Process 14 of 32 is on nid00015</div>
            <div>Process 29 of 32 is on nid00014</div>
            <div>Process 22 of 32 is on nid00014</div>
            <div>Process 17 of 32 is on nid00014</div>
            <div>Process 28 of 32 is on nid00014</div>
            <div>Process 31 of 32 is on nid00014</div>
            <div>Process 26 of 32 is on nid00014</div>
            <div>Process 30 of 32 is on nid00014</div>
            <div>Process 16 of 32 is on nid00014</div>
            <div>Process 25 of 32 is on nid00014</div>
            <div>Process 24 of 32 is on nid00014</div>
            <div>Process 21 of 32 is on nid00014</div>
            <div>Process 20 of 32 is on nid00014</div>
            <div>Process 27 of 32 is on nid00014</div>
            <div>Process 19 of 32 is on nid00014</div>
            <div>Process 18 of 32 is on nid00014</div>
            <div>Process 23 of 32 is on nid00014</div>
            <div>pi is approximately 3.1415926544231265, Error is
              0.0000000008333334</div>
            <div>wall clock time = 0.004645</div>
          </div>
          <div><br>
          </div>
          <div><br>
          </div>
          <div>Here is what I have with openmpi.</div>
          <div>mpiexec --bind-to-core &nbsp;--mca
            plm_base_strip_prefix_from_node_names 0 -np 32 ./cpi</div>
          <div>
            <div>--------------------------------------------------------------------------</div>
            <div>There are not enough slots available in the system to
              satisfy the 32 slots&nbsp;</div>
            <div>that were requested by the application:</div>
            <div>&nbsp; ./cpi</div>
            <div><br>
            </div>
            <div>Either request fewer slots for your application, or
              make more slots available</div>
            <div>for use.</div>
            <div>--------------------------------------------------------------------------</div>
          </div>
          <div>
            <div><br>
            </div>
          </div>
          <div>
            <div><br>
            </div>
          </div>
        </div>
      </div>
      <div><br>
      </div>
      <span id="OLK_SRC_BODY_SECTION">
        <div style="font-family:Calibri; font-size:11pt;
          text-align:left; color:black; BORDER-BOTTOM: medium none;
          BORDER-LEFT: medium none; PADDING-BOTTOM: 0in; PADDING-LEFT:
          0in; PADDING-RIGHT: 0in; BORDER-TOP: #b5c4df 1pt solid;
          BORDER-RIGHT: medium none; PADDING-TOP: 3pt">
          <span style="font-weight:bold">From: </span>Ralph Castain
          &lt;<a moz-do-not-send="true" href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;<br>
          <span style="font-weight:bold">Reply-To: </span>Open MPI
          Users &lt;<a moz-do-not-send="true"
            href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
          <span style="font-weight:bold">Date: </span>Saturday,
          November 23, 2013 2:27 PM<br>
          <span style="font-weight:bold">To: </span>Open MPI Users &lt;<a
            moz-do-not-send="true" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
          <span style="font-weight:bold">Subject: </span>[EXTERNAL] Re:
          [OMPI users] (OpenMPI for Cray XE6 ) How to set mca parameters
          through aprun?<br>
        </div>
        <div><br>
        </div>
        <div>
          <div style="word-wrap: break-word; -webkit-nbsp-mode: space;
            -webkit-line-break: after-white-space;">
            My guess is that you aren't doing the allocation correctly -
            since you are using qsub, can I assume you have Moab as your
            scheduler?
            <div><br>
            </div>
            <div>aprun should be forwarding the envars - do you see them
              if you just run "aprun -n 1 printenv"?</div>
            <div><br>
              <div>
                <div>
                  <div>On Nov 23, 2013, at 2:13 PM, Teranishi, Keita
                    &lt;<a moz-do-not-send="true"
                      href="mailto:knteran@sandia.gov">knteran@sandia.gov</a>&gt;
                    wrote:</div>
                  <br class="Apple-interchange-newline">
                  <blockquote type="cite">
                    <div style="word-wrap: break-word;
                      -webkit-nbsp-mode: space; -webkit-line-break:
                      after-white-space; font-size: 14px; font-family:
                      Calibri, sans-serif;">
                      <div>
                        <div>
                          <div>Hi,</div>
                          <div><br>
                          </div>
                          <div>I installed OpenMPI on our small XE6
                            using the configure options under /contrib
                            directory. &nbsp;It appears it is working fine,
                            but it ignores MCA parameters (set in env
                            var). &nbsp;So I switched to mpirun (in OpenMPI)
                            and it can handle MCA parameters somehow.
                            &nbsp;However, &nbsp;mpirun fails to allocate process
                            by cores. &nbsp;For example, I allocated 32 cores
                            (on 2 nodes) by "qsub &#8211;lmppwidth=32
                            &#8211;lmppnppn=16", mpirun recognizes it as 2
                            slots. &nbsp; &nbsp;Is it possible to mpirun to handle
                            mluticore nodes of XE6 properly or is there
                            any options to handle MCA parameters for
                            aprun?</div>
                          <div><br>
                          </div>
                          <div>Regards,</div>
                          <div>
                            <div>
                              <div>-----------------------------------------------------------------------------</div>
                              <div>Keita Teranishi</div>
                            </div>
                            <div>Principal Member of Technical Staff</div>
                            <div>Scalable Modeling and Analysis Systems</div>
                            <div>Sandia National Laboratories</div>
                            <div>Livermore, CA 94551</div>
                            <div>+1 (925) 294-3738</div>
                            <div><br>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                    _______________________________________________<br>
                    users mailing list<br>
                    <a moz-do-not-send="true"
                      href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
                    <a moz-do-not-send="true"
                      href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
                </div>
                <br>
              </div>
            </div>
          </div>
        </div>
      </span>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></pre>
    </blockquote>
    <br>
    <pre class="moz-signature" cols="80">-- 
CCCCCCCCCCCCCCCCCCCCCCFFFFFFFFFFFFFFFFFFFFFFFFFDDDDDDDDDDDDDDDDDDDDD
David Whitaker, Ph.D.                              <a class="moz-txt-link-abbreviated" href="mailto:whitaker@cray.com">whitaker@cray.com</a>
Aerospace CFD Specialist                        phone: (651)605-9078
ISV Applications/Cray Inc                         fax: (651)605-9001
CCCCCCCCCCCCCCCCCCCCCCFFFFFFFFFFFFFFFFFFFFFFFFFDDDDDDDDDDDDDDDDDDDDD
</pre>
  </body>
</html>

