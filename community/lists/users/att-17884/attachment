Hi,<br><br>I tried execute the Osu_latency_mt as mentioned below <br><br>First build the openmpi with Multi-threaded support as Osu_latency_mt needed <br><br>&gt; [root@localhost openmpi-1.4.3]# ./configure --with-threads=posix --enable-mpi-threads<br>
<br>&gt; make &amp;&amp; make install<br><br><br>&gt;  [root@localhost openmpi-1.4.3]# mpirun --prefix /usr/local/ -np 2 --mca btl openib,self -H 192.168.0.174,192.168.0.175 /root/ramu/ofed_pkgs/osu_benchmarks-3.1.1/osu_latency_mt<br>
<br>--------------------------------------------------------------------------<br>WARNING: No preset parameters were found for the device that Open MPI<br>detected:<br><br>  Local host:            test2<br>  Device name:           plx2_0<br>
  Device vendor ID:      0x10b5<br>  Device vendor part ID: 4277<br><br>Default device parameters will be used, which may result in lower<br>performance.  You can edit any of the files specified by the<br>btl_openib_device_param_files MCA parameter to set values for your<br>
device.<br><br>NOTE: You can turn off this warning by setting the MCA parameter<br>      btl_openib_warn_no_device_params_found to 0.<br>--------------------------------------------------------------------------<br>--------------------------------------------------------------------------<br>
At least one pair of MPI processes are unable to reach each other for<br>MPI communications.  This means that no Open MPI device has indicated<br>that it can be used to communicate between these processes.  This is<br>an error; Open MPI requires that all MPI processes be able to reach<br>
each other.  This error can sometimes be the result of forgetting to<br>specify the &quot;self&quot; BTL.<br><br>  Process 1 ([[29990,1],0]) is on host: localhost.localdomain<br>  Process 2 ([[29990,1],1]) is on host: 192<br>
  BTLs attempted: self<br><br>Your MPI job is now going to abort; sorry.<br>--------------------------------------------------------------------------<br>--------------------------------------------------------------------------<br>
It looks like MPI_INIT failed for some reason; your parallel process is<br>likely to abort.  There are many reasons that a parallel process can<br>fail during MPI_INIT; some of which are due to configuration or environment<br>
problems.  This failure appears to be an internal failure; here&#39;s some<br>additional information (which may only be relevant to an Open MPI<br>developer):<br><br>  PML add procs failed<br>  --&gt; Returned &quot;Unreachable&quot; (-12) instead of &quot;Success&quot; (0)<br>
--------------------------------------------------------------------------<br>*** The MPI_Init_thread() function was called before MPI_INIT was invoked.<br>*** This is disallowed by the MPI standard.<br>*** Your MPI job will now abort.<br>
[localhost.localdomain:32216] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>--------------------------------------------------------------------------<br>mpirun has exited due to process rank 0 with PID 32216 on<br>
node localhost.localdomain exiting without calling &quot;finalize&quot;. This may<br>have caused other processes in the application to be<br>terminated by signals sent by mpirun (as reported here).<br>--------------------------------------------------------------------------<br>
*** The MPI_Init_thread() function was called before MPI_INIT was invoked.<br>*** This is disallowed by the MPI standard.<br>*** Your MPI job will now abort.<br>[test2:2104] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>
[localhost.localdomain:32214] 1 more process has sent help message help-mca-bml-r2.txt / unreachable proc<br>[localhost.localdomain:32214] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>
[localhost.localdomain:32214] 1 more process has sent help message help-mpi-runtime / mpi_init:startup:internal-failure<br><br><br>Remaining all MPI cases executed well only this case creating problem .. &quot;The MPI_Init_thread() function was called before MPI_INIT was invoked &quot;<br>
<br>Please give suggestions to execute this.<br>

