
<br><font size=2 face="sans-serif">You said &nbsp;&quot;</font><tt><font size=2>separate
MPI &nbsp;applications doing 1 to &gt; N broadcasts over PVM&quot;. &nbsp;You
do not mean you are using pvm_bcast though - right?</font></tt>
<br>
<br><tt><font size=2>If these N MPI applications are so independent that
you could run one at a time or run them on N different clusters and still
get the result you want (not the time to solution) then I cannot imagine
how there could be cross talk. &nbsp;</font></tt>
<br>
<br><tt><font size=2>I have been assuming that when you describe this as
an NxN problem, you mean there is some desired interaction among the N
MPI worlds. &nbsp;</font></tt>
<br>
<br><tt><font size=2>If I have misunderstood and the N MPI worlds stared
with N mpirun operations under PVM are each semantically independent of
the other (N-1) then I am totally at a loss for an explanation.</font></tt>
<br>
<br><tt><font size=2>&nbsp;</font></tt>
<br><font size=2 face="sans-serif">Dick Treumann &nbsp;- &nbsp;MPI Team
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br>
IBM Systems &amp; Technology Group<br>
Dept X2ZA / MS P963 -- 2455 South Road -- Poughkeepsie, NY 12601<br>
Tele (845) 433-7846 &nbsp; &nbsp; &nbsp; &nbsp; Fax (845) 433-8363<br>
</font>
<br>
<br><tt><font size=2>users-bounces@open-mpi.org wrote on 08/11/2010 08:59:16
PM:<br>
<br>
&gt; [image removed] </font></tt>
<br><tt><font size=2>&gt; <br>
&gt; Re: [OMPI users] MPI_Bcast issue</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; Randolph Pullen </font></tt>
<br><tt><font size=2>&gt; <br>
&gt; to:</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; Open MPI Users</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; 08/11/2010 09:01 PM</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; Sent by:</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; users-bounces@open-mpi.org</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; Please respond to Open MPI Users</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; I (a single user) am running N separate MPI &nbsp;applications doing
1 to<br>
&gt; N broadcasts over PVM, each MPI application is started on each <br>
&gt; machine simultaneously by PVM - the reasons are back in the post history.<br>
&gt; <br>
&gt; The problem is that they somehow collide - yes I know this should
<br>
&gt; not happen, the question is why.<br>
&gt; <br>
&gt; --- On Wed, 11/8/10, Richard Treumann &lt;treumann@us.ibm.com&gt;
wrote:</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; From: Richard Treumann &lt;treumann@us.ibm.com&gt;<br>
&gt; Subject: Re: [OMPI users] MPI_Bcast issue<br>
&gt; To: &quot;Open MPI Users&quot; &lt;users@open-mpi.org&gt;<br>
&gt; Received: Wednesday, 11 August, 2010, 11:34 PM<br>
</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; Randolf <br>
&gt; <br>
&gt; I am confused about using multiple, concurrent mpirun operations.
&nbsp;<br>
&gt; If there are M uses of mpirun and each starts N tasks (carried out
<br>
&gt; under pvm or any other way) I would expect you to have M completely
<br>
&gt; independent MPI jobs with N tasks (processes) each. &nbsp;You could
have <br>
&gt; some root in each of the M MPI jobs do an MPI_Bcast to the other <br>
&gt; N-1) in that job but there is no way in MPI (without using <br>
&gt; accept.connect) to get tasks of job 0 to give data to tasks of jobs
1-(m-1). <br>
&gt; <br>
&gt; With M uses of mpirun, you have M worlds that are forever isolated
<br>
&gt; from the other M-1 worlds (again, unless you do accept/connect) <br>
&gt; <br>
&gt; In what sense are you treating this as an single MxN application?
&nbsp; <br>
&gt; ( I use M &amp; N to keep them distinct. I assume if M == N, we have
your case) <br>
&gt; <br>
&gt; <br>
&gt; Dick Treumann &nbsp;- &nbsp;MPI Team &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
<br>
&gt; IBM Systems &amp; Technology Group<br>
&gt; Dept X2ZA / MS P963 -- 2455 South Road -- Poughkeepsie, NY 12601<br>
&gt; Tele (845) 433-7846 &nbsp; &nbsp; &nbsp; &nbsp; Fax (845) 433-8363</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; -----Inline Attachment Follows-----<br>
</font></tt>
<br><tt><font size=2>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; users@open-mpi.org<br>
&gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a>
<br><tt><font size=2>&gt; <br>
&gt; <br>
&gt; &nbsp;_______________________________________________<br>
&gt; users mailing list<br>
&gt; users@open-mpi.org<br>
&gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a>
