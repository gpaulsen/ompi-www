<html><body>
<p>One difference is that putting a blocking send before the irecv is a classic &quot;unsafe&quot; MPI program. It depends on eager send buffering to complete the MPI_Send so the MPI_Irecv can be posted.  The example with MPI_Send first would be allowed to hang.<br>
<br>
The original program is correct and safe MPI.<br>
<br>
<br>
Dick Treumann  -  MPI Team           <br>
IBM Systems &amp; Technology Group<br>
Dept X2ZA / MS P963 -- 2455 South Road -- Poughkeepsie, NY 12601<br>
Tele (845) 433-7846         Fax (845) 433-8363<br>
<br>
<br>
<tt>users-bounces@open-mpi.org wrote on 02/05/2009 05:48:34 AM:<br>
<br>
&gt; [image removed] </tt><br>
<tt>&gt; <br>
&gt; Re: [OMPI users] MPI_Test bug?</tt><br>
<tt>&gt; <br>
&gt; Gabriele Fatigati </tt><br>
<tt>&gt; <br>
&gt; to:</tt><br>
<tt>&gt; <br>
&gt; Open MPI Users</tt><br>
<tt>&gt; <br>
&gt; 02/05/2009 05:49 AM</tt><br>
<tt>&gt; <br>
&gt; Sent by:</tt><br>
<tt>&gt; <br>
&gt; users-bounces@open-mpi.org</tt><br>
<tt>&gt; <br>
&gt; Please respond to Open MPI Users</tt><br>
<tt>&gt; <br>
&gt; Hi Jody,<br>
&gt; thanks four your quick reply. But what's the difference?<br>
&gt; <br>
&gt; 2009/2/5 jody &lt;jody.xha@gmail.com&gt;:<br>
&gt; &gt; Hi Gabriele<br>
&gt; &gt;<br>
&gt; &gt; Shouldn't you reverse the order of your send and recv from<br>
&gt; &gt; &nbsp; &nbsp;MPI_Irecv(buffer_recv, bufferLen, MPI_INT, recv_to, tag,<br>
&gt; &gt; MPI_COMM_WORLD, &amp;request);<br>
&gt; &gt; &nbsp; &nbsp;MPI_Send(buffer_send, bufferLen, MPI_INT, send_to, tag, MPI_COMM_WORLD);<br>
&gt; &gt;<br>
&gt; &gt; to<br>
&gt; &gt;<br>
&gt; &gt; &nbsp; &nbsp;MPI_Send(buffer_send, bufferLen, MPI_INT, send_to, tag, MPI_COMM_WORLD);<br>
&gt; &gt; &nbsp; &nbsp;MPI_Irecv(buffer_recv, bufferLen, MPI_INT, recv_to, tag,<br>
&gt; &gt; MPI_COMM_WORLD, &amp;request);<br>
&gt; &gt;<br>
&gt; &gt; ?<br>
&gt; &gt; Jody<br>
&gt; &gt;<br>
&gt; &gt; On Thu, Feb 5, 2009 at 11:37 AM, Gabriele Fatigati <br>
&gt; &lt;g.fatigati@cineca.it&gt; wrote:<br>
&gt; &gt;&gt; Dear OpenMPI developer,<br>
&gt; &gt;&gt; i have found a very strange behaviour of MPI_Test. I'm using OpenMPI<br>
&gt; &gt;&gt; 1.2 over Infiniband interconnection net.<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; I've tried to implement net check with a series of MPI_Irecv and<br>
&gt; &gt;&gt; MPI_Send &nbsp;beetwen processors, testing with MPI_Wait the end of Irecv.<br>
&gt; &gt;&gt; For strange reasons, i've noted that, when i launch the test in one<br>
&gt; &gt;&gt; node, it works well. If i launch over 2 or more procs over different<br>
&gt; &gt;&gt; nodes, MPI_Test fails many time before to tell that the IRecv is<br>
&gt; &gt;&gt; finished.<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; I've tried that it fails also after one minutes, with very small<br>
&gt; &gt;&gt; buffer( less than eager limit). It's impossible that the communication<br>
&gt; &gt;&gt; is pending after one minutes, with 10 integer sended. To solve this,<br>
&gt; &gt;&gt; I need to implement a loop over MPI_Test, and only after 3 or 4<br>
&gt; &gt;&gt; MPI_Test it returns that IRecv finished successful. Is it possible<br>
&gt; &gt;&gt; that MPI_Test needs to call many time also if the communication is<br>
&gt; &gt;&gt; already finished?<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; In attach you have my simple C test program.<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; Thanks in advance.<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; --<br>
&gt; &gt;&gt; Ing. Gabriele Fatigati<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; Parallel programmer<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; CINECA Systems &amp; Tecnologies Department<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; Supercomputing Group<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; www.cineca.it &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Tel: &nbsp; +39 051 6171722<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; g.fatigati [AT] cineca.it<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; _______________________________________________<br>
&gt; &gt;&gt; users mailing list<br>
&gt; &gt;&gt; users@open-mpi.org<br>
&gt; &gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; &gt;&gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; users@open-mpi.org<br>
&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; <br>
&gt; <br>
&gt; <br>
&gt; -- <br>
&gt; Ing. Gabriele Fatigati<br>
&gt; <br>
&gt; Parallel programmer<br>
&gt; <br>
&gt; CINECA Systems &amp; Tecnologies Department<br>
&gt; <br>
&gt; Supercomputing Group<br>
&gt; <br>
&gt; Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>
&gt; <br>
&gt; www.cineca.it &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Tel: &nbsp; +39 051 6171722<br>
&gt; <br>
&gt; g.fatigati [AT] cineca.it<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; users@open-mpi.org<br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</tt></body></html>
