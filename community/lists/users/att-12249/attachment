Hi,<br><br>Thank you for those informations. <br>For the moment, I didn&#39;t encountered those problems yet. Maybe because, my program don&#39;t use much memory (100MB) and the master machine have huge RAM (8GB).<br>So meanwhile, the solution seems to be the parameter &quot;btl_tcp_eager_limit&quot; but a cleaner solution is very welcome :-)<br>

<br>   TMHieu<br>
<br>2010/3/5 Aurélien Bouteiller &lt;<a href="mailto:bouteill@eecs.utk.edu" target="_blank">bouteill@eecs.utk.edu</a>&gt;:<br>&gt; Hi,<br>&gt;<br>&gt; setting the eager limit to such a drastically high value will have the effect of generating gigantic memory consumption for unexpected messages. Any message you send which does not have a preposted ready recv will mallocate 150mb of temporary storage, and will be memcopied from that internal buffer to the recv buffer when the recv is posted. You should expect very poor bandwidth and probably crash/abort due to memory exhaustion on the receivers.<br>


&gt;<br>&gt; Aurelien<br>&gt; --<br>&gt; Dr. Aurelien Bouteiller<br>&gt; Innovative Computing Laboratory<br>&gt; University of Tennessee<br>&gt; Knoxville, TN, USA<br>&gt;<br>&gt;<br>&gt; Le 4 mars 2010 à 09:02, TRINH Minh Hieu a écrit :<br>


&gt;<br>&gt;&gt; Hi,<br>&gt;&gt;<br>&gt;&gt; I have some new discovery about this problem :<br>&gt;&gt;<br>&gt;&gt; It seems that the array size sendable from a 32bit to 64bit machines<br>&gt;&gt; is proportional to the parameter &quot;btl_tcp_eager_limit&quot;<br>


&gt;&gt; When I set it to 200 000 000 (2e08 bytes, about 190MB), I can send an<br>&gt;&gt; array up to 2e07 double (152MB).<br>&gt;&gt;<br>&gt;&gt; I didn&#39;t found much informations about btl_tcp_eager_limit other than<br>


&gt;&gt; in the &quot;ompi_info --all&quot; command. If I let it at 2e08, will it impacts<br>&gt;&gt; the performance of OpenMPI ?<br>&gt;&gt;<br>&gt;&gt; It may be noteworth also that if the master (rank 0) is a 32bit<br>


&gt;&gt; machines, I don&#39;t have segfault. I can send big array with small<br>&gt;&gt; &quot;btl_tcp_eager_limit&quot; from a 64bit machine to a 32bit one.<br>&gt;&gt;<br>&gt;&gt; Do I have to move this thread to devel mailing list ?<br>


&gt;&gt;<br>&gt;&gt; Regards,<br>&gt;&gt;<br>&gt;&gt;   TMHieu<br>&gt;&gt;<br>&gt;&gt; On Tue, Mar 2, 2010 at 2:54 PM, TRINH Minh Hieu &lt;<a href="mailto:mhtrinh@gmail.com" target="_blank">mhtrinh@gmail.com</a>&gt; wrote:<br>

&gt;&gt;&gt; Hello,<br>
&gt;&gt;&gt;<br>&gt;&gt;&gt; Yes, I compiled OpenMPI with --enable-heterogeneous. More precisely I<br>&gt;&gt;&gt; compiled with :<br>&gt;&gt;&gt; $ ./configure --prefix=/tmp/openmpi --enable-heterogeneous<br>&gt;&gt;&gt; --enable-cxx-exceptions --enable-shared<br>


&gt;&gt;&gt; --enable-orterun-prefix-by-default<br>&gt;&gt;&gt; $ make all install<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; I attach the output of ompi_info of my 2 machines.<br>&gt;&gt;&gt;<br>&gt;&gt;&gt;    TMHieu<br>&gt;&gt;&gt;<br>


&gt;&gt;&gt; On Tue, Mar 2, 2010 at 1:57 PM, Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt; wrote:<br>&gt;&gt;&gt;&gt; Did you configure Open MPI with --enable-heterogeneous?<br>

&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; On Feb 28, 2010, at 1:22 PM, TRINH Minh Hieu wrote:<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; Hello,<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; I have some problems running MPI on my heterogeneous cluster. More<br>


&gt;&gt;&gt;&gt;&gt; precisley i got segmentation fault when sending a large array (about<br>&gt;&gt;&gt;&gt;&gt; 10000) of double from a i686 machine to a x86_64 machine. It does not<br>&gt;&gt;&gt;&gt;&gt; happen with small array. Here is the send/recv code source (complete<br>


&gt;&gt;&gt;&gt;&gt; source is in attached file) :<br>&gt;&gt;&gt;&gt;&gt; ========code ================<br>&gt;&gt;&gt;&gt;&gt;     if (me == 0 ) {<br>&gt;&gt;&gt;&gt;&gt;         for (int pe=1; pe&lt;nprocs; pe++)<br>&gt;&gt;&gt;&gt;&gt;         {<br>


&gt;&gt;&gt;&gt;&gt;                 printf(&quot;Receiving from proc %d : &quot;,pe); fflush(stdout);<br>&gt;&gt;&gt;&gt;&gt;             d=(double *)malloc(sizeof(double)*n);<br>&gt;&gt;&gt;&gt;&gt;             MPI_Recv(d,n,MPI_DOUBLE,pe,999,MPI_COMM_WORLD,&amp;status);<br>


&gt;&gt;&gt;&gt;&gt;             printf(&quot;OK\n&quot;); fflush(stdout);<br>&gt;&gt;&gt;&gt;&gt;         }<br>&gt;&gt;&gt;&gt;&gt;         printf(&quot;All done.\n&quot;);<br>&gt;&gt;&gt;&gt;&gt;     }<br>&gt;&gt;&gt;&gt;&gt;     else {<br>


&gt;&gt;&gt;&gt;&gt;       d=(double *)malloc(sizeof(double)*n);<br>&gt;&gt;&gt;&gt;&gt;       MPI_Send(d,n,MPI_DOUBLE,0,999,MPI_COMM_WORLD);<br>&gt;&gt;&gt;&gt;&gt;     }<br>&gt;&gt;&gt;&gt;&gt; ======== code ================<br>


&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; I got segmentation fault with n=10000 but no error with n=1000<br>&gt;&gt;&gt;&gt;&gt; I have 2 machines :<br>&gt;&gt;&gt;&gt;&gt; sbtn155 : Intel Xeon,         x86_64<br>&gt;&gt;&gt;&gt;&gt; sbtn211 : Intel Pentium 4, i686<br>


&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; The code is compiled in x86_64 and i686 machine, using OpenMPI 1.4.1,<br>&gt;&gt;&gt;&gt;&gt; installed in /tmp/openmpi :<br>&gt;&gt;&gt;&gt;&gt; [mhtrinh@sbtn211 heterogenous]$ make hetero<br>


&gt;&gt;&gt;&gt;&gt; gcc -Wall -I. -std=c99 -O3 -I/tmp/openmpi/include -c hetero.c -o hetero.i686.o<br>&gt;&gt;&gt;&gt;&gt; /tmp/openmpi/bin/mpicc -Wall -I. -std=c99 -O3 -I/tmp/openmpi/include<br>&gt;&gt;&gt;&gt;&gt; hetero.i686.o -o hetero.i686 -lm<br>


&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; [mhtrinh@sbtn155 heterogenous]$ make hetero<br>&gt;&gt;&gt;&gt;&gt; gcc -Wall -I. -std=c99 -O3 -I/tmp/openmpi/include -c hetero.c -o hetero.x86_64.o<br>&gt;&gt;&gt;&gt;&gt; /tmp/openmpi/bin/mpicc -Wall -I. -std=c99 -O3 -I/tmp/openmpi/include<br>


&gt;&gt;&gt;&gt;&gt; hetero.x86_64.o -o hetero.x86_64 -lm<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; I run with the code using appfile and got thoses error :<br>&gt;&gt;&gt;&gt;&gt; $ cat appfile<br>&gt;&gt;&gt;&gt;&gt; --host sbtn155 -np 1 hetero.x86_64<br>


&gt;&gt;&gt;&gt;&gt; --host sbtn155 -np 1 hetero.x86_64<br>&gt;&gt;&gt;&gt;&gt; --host sbtn211 -np 1 hetero.i686<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; $ mpirun -hetero --app appfile<br>&gt;&gt;&gt;&gt;&gt; Input array length :<br>


&gt;&gt;&gt;&gt;&gt; 10000<br>&gt;&gt;&gt;&gt;&gt; Receiving from proc 1 : OK<br>&gt;&gt;&gt;&gt;&gt; Receiving from proc 2 : [sbtn155:26386] *** Process received signal ***<br>&gt;&gt;&gt;&gt;&gt; [sbtn155:26386] Signal: Segmentation fault (11)<br>


&gt;&gt;&gt;&gt;&gt; [sbtn155:26386] Signal code: Address not mapped (1)<br>&gt;&gt;&gt;&gt;&gt; [sbtn155:26386] Failing at address: 0x200627bd8<br>&gt;&gt;&gt;&gt;&gt; [sbtn155:26386] [ 0] /lib64/libpthread.so.0 [0x3fa4e0e540]<br>


&gt;&gt;&gt;&gt;&gt; [sbtn155:26386] [ 1] /tmp/openmpi/lib/openmpi/mca_pml_ob1.so [0x2aaaad8d7908]<br>&gt;&gt;&gt;&gt;&gt; [sbtn155:26386] [ 2] /tmp/openmpi/lib/openmpi/mca_btl_tcp.so [0x2aaaae2fc6e3]<br>&gt;&gt;&gt;&gt;&gt; [sbtn155:26386] [ 3] /tmp/openmpi/lib/libopen-pal.so.0 [0x2aaaaafe39db]<br>


&gt;&gt;&gt;&gt;&gt; [sbtn155:26386] [ 4]<br>&gt;&gt;&gt;&gt;&gt; /tmp/openmpi/lib/libopen-pal.so.0(opal_progress+0x9e) [0x2aaaaafd8b9e]<br>&gt;&gt;&gt;&gt;&gt; [sbtn155:26386] [ 5] /tmp/openmpi/lib/openmpi/mca_pml_ob1.so [0x2aaaad8d4b25]<br>


&gt;&gt;&gt;&gt;&gt; [sbtn155:26386] [ 6] /tmp/openmpi/lib/libmpi.so.0(MPI_Recv+0x13b)<br>&gt;&gt;&gt;&gt;&gt; [0x2aaaaab30f9b]<br>&gt;&gt;&gt;&gt;&gt; [sbtn155:26386] [ 7] hetero.x86_64(main+0xde) [0x400cbe]<br>&gt;&gt;&gt;&gt;&gt; [sbtn155:26386] [ 8] /lib64/libc.so.6(__libc_start_main+0xf4) [0x3fa421e074]<br>


&gt;&gt;&gt;&gt;&gt; [sbtn155:26386] [ 9] hetero.x86_64 [0x400b29]<br>&gt;&gt;&gt;&gt;&gt; [sbtn155:26386] *** End of error message ***<br>&gt;&gt;&gt;&gt;&gt; --------------------------------------------------------------------------<br>


&gt;&gt;&gt;&gt;&gt; mpirun noticed that process rank 0 with PID 26386 on node sbtn155<br>&gt;&gt;&gt;&gt;&gt; exited on signal 11 (Segmentation fault).<br>&gt;&gt;&gt;&gt;&gt; --------------------------------------------------------------------------<br>


&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; Am I missing an option in order to run in heterogenous cluster ?<br>&gt;&gt;&gt;&gt;&gt; MPI_Send/Recv have limit array size when using heterogeneous cluster ?<br>&gt;&gt;&gt;&gt;&gt; Thanks for your help. Regards<br>


&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; --<br>&gt;&gt;&gt;&gt;&gt; ============================================<br>&gt;&gt;&gt;&gt;&gt;    M. TRINH Minh Hieu<br>&gt;&gt;&gt;&gt;&gt;    CEA, IBEB, SBTN/LIRM,<br>&gt;&gt;&gt;&gt;&gt;    F-30207 Bagnols-sur-Cèze, FRANCE<br>


&gt;&gt;&gt;&gt;&gt; ============================================<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; &lt;hetero.c.bz2&gt;_______________________________________________<br>&gt;&gt;&gt;&gt;&gt; users mailing list<br>


&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>


&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; --<br>&gt;&gt;&gt;&gt; Jeff Squyres<br>&gt;&gt;&gt;&gt; <a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a><br>&gt;&gt;&gt;&gt; For corporate legal information go to:<br>


&gt;&gt;&gt;&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; _______________________________________________<br>


&gt;&gt;&gt;&gt; users mailing list<br>&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>


&gt;&gt;&gt;<br>&gt;&gt;<br>&gt;&gt; _______________________________________________<br>&gt;&gt; users mailing list<br>&gt;&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>


&gt;<br>&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>


&gt;<br><br><br><br>-- <br>============================================<br>   M. TRINH Minh Hieu<br>   CEA, IBEB, SBTN/LIRM,<br>   F-30207 Bagnols-sur-Cèze, FRANCE<br>============================================<br><br>

