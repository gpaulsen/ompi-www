<div dir="ltr">Also, what ofed version (ofed_info -s) and mxm version (rpm -qi mxm) do you use?</div><div class="gmail_extra"><br><br><div class="gmail_quote">On Wed, Jun 12, 2013 at 3:30 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Great! Would you mind showing the revised table? I&#39;m curious as to the relative performance.<br>
<div class="HOEnZb"><div class="h5"><br>
<br>
On Jun 11, 2013, at 4:53 PM, <a href="mailto:eblosch@1scom.net">eblosch@1scom.net</a> wrote:<br>
<br>
&gt; Problem solved. I did not configure with --with-mxm=/opt/mellanox/mcm and<br>
&gt; this location was not auto-detected.  Once I rebuilt with this option,<br>
&gt; everything worked fine. Scaled better than MVAPICH out to 800. MVAPICH<br>
&gt; configure log showed that it had found this component of the OFED stack.<br>
&gt;<br>
&gt; Ed<br>
&gt;<br>
&gt;<br>
&gt;&gt; If you run at 224 and things look okay, then I would suspect something in<br>
&gt;&gt; the upper level switch that spans cabinets. At that point, I&#39;d have to<br>
&gt;&gt; leave it to Mellanox to advise.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; On Jun 11, 2013, at 6:55 AM, &quot;Blosch, Edwin L&quot; &lt;<a href="mailto:edwin.l.blosch@lmco.com">edwin.l.blosch@lmco.com</a>&gt;<br>
&gt;&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt;&gt; I tried adding &quot;-mca btl openib,sm,self&quot;  but it did not make any<br>
&gt;&gt;&gt; difference.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Jesus’ e-mail this morning has got me thinking.  In our system, each<br>
&gt;&gt;&gt; cabinet has 224 cores, and we are reaching a different level of the<br>
&gt;&gt;&gt; system architecture when we go beyond 224.  I got an additional data<br>
&gt;&gt;&gt; point at 256 and found that performance is already falling off. Perhaps<br>
&gt;&gt;&gt; I did not build OpenMPI properly to support the Mellanox adapters that<br>
&gt;&gt;&gt; are used in the backplane, or I need some configuration setting similar<br>
&gt;&gt;&gt; to FAQ #19 in the Tuning/Openfabrics section.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; From: <a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a> [mailto:<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>] On<br>
&gt;&gt;&gt; Behalf Of Ralph Castain<br>
&gt;&gt;&gt; Sent: Sunday, June 09, 2013 6:48 PM<br>
&gt;&gt;&gt; To: Open MPI Users<br>
&gt;&gt;&gt; Subject: Re: [OMPI users] EXTERNAL: Re: Need advice on performance<br>
&gt;&gt;&gt; problem<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Strange - it looks like a classic oversubscription behavior. Another<br>
&gt;&gt;&gt; possibility is that it isn&#39;t using IB for some reason when extended to<br>
&gt;&gt;&gt; the other nodes. What does your cmd line look like? Have you tried<br>
&gt;&gt;&gt; adding &quot;-mca btl openib,sm,self&quot; just to ensure it doesn&#39;t use TCP for<br>
&gt;&gt;&gt; some reason?<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Jun 9, 2013, at 4:31 PM, &quot;Blosch, Edwin L&quot; &lt;<a href="mailto:edwin.l.blosch@lmco.com">edwin.l.blosch@lmco.com</a>&gt;<br>
&gt;&gt;&gt; wrote:<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Correct.  20 nodes, 8 cores per dual-socket on each node = 360.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; From: <a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a> [mailto:<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>] On<br>
&gt;&gt;&gt; Behalf Of Ralph Castain<br>
&gt;&gt;&gt; Sent: Sunday, June 09, 2013 6:18 PM<br>
&gt;&gt;&gt; To: Open MPI Users<br>
&gt;&gt;&gt; Subject: Re: [OMPI users] EXTERNAL: Re: Need advice on performance<br>
&gt;&gt;&gt; problem<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; So, just to be sure - when you run 320 &quot;cores&quot;, you are running across<br>
&gt;&gt;&gt; 20 nodes?<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Just want to ensure we are using &quot;core&quot; the same way - some people<br>
&gt;&gt;&gt; confuse cores with hyperthreads.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Jun 9, 2013, at 3:50 PM, &quot;Blosch, Edwin L&quot; &lt;<a href="mailto:edwin.l.blosch@lmco.com">edwin.l.blosch@lmco.com</a>&gt;<br>
&gt;&gt;&gt; wrote:<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; 16.  dual-socket Xeon, E5-2670.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; I am trying a larger model to see if the performance drop-off happens at<br>
&gt;&gt;&gt; a different number of cores.<br>
&gt;&gt;&gt; Also I’m running some intermediate core-count sizes to refine the curve<br>
&gt;&gt;&gt; a bit.<br>
&gt;&gt;&gt; I also added mpi_show_mca_params all, and at the same time,<br>
&gt;&gt;&gt; btl_openib_use_eager_rdma 1, just to see if that does anything.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; From: <a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a> [mailto:<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>] On<br>
&gt;&gt;&gt; Behalf Of Ralph Castain<br>
&gt;&gt;&gt; Sent: Sunday, June 09, 2013 5:04 PM<br>
&gt;&gt;&gt; To: Open MPI Users<br>
&gt;&gt;&gt; Subject: EXTERNAL: Re: [OMPI users] Need advice on performance problem<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Looks to me like things are okay thru 160, and then things fall apart<br>
&gt;&gt;&gt; after that point. How many cores are on a node?<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Jun 9, 2013, at 1:59 PM, &quot;Blosch, Edwin L&quot; &lt;<a href="mailto:edwin.l.blosch@lmco.com">edwin.l.blosch@lmco.com</a>&gt;<br>
&gt;&gt;&gt; wrote:<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; I’m having some trouble getting good scaling with OpenMPI 1.6.4 and I<br>
&gt;&gt;&gt; don’t know where to start looking. This is an Infiniband FDR network<br>
&gt;&gt;&gt; with Sandy Bridge nodes.  I am using affinity (--bind-to-core) but no<br>
&gt;&gt;&gt; other options. As the number of cores goes up, the message sizes are<br>
&gt;&gt;&gt; typically going down. There seem to be lots of options in the FAQ, and I<br>
&gt;&gt;&gt; would welcome any advice on where to start.  All these timings are on a<br>
&gt;&gt;&gt; completely empty system except for me.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Thanks<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;    MPI              # cores   Ave. Rate   Std. Dev. %  # timings<br>
&gt;&gt;&gt; Speedup    Efficiency<br>
&gt;&gt;&gt; ================================================================================================<br>
&gt;&gt;&gt; MVAPICH            |   16   |    8.6783  |   0.995 % |       2  |<br>
&gt;&gt;&gt; 16.000  |  1.0000<br>
&gt;&gt;&gt; MVAPICH            |   48   |    8.7665  |   1.937 % |       3  |<br>
&gt;&gt;&gt; 47.517  |  0.9899<br>
&gt;&gt;&gt; MVAPICH            |   80   |    8.8900  |   2.291 % |       3  |<br>
&gt;&gt;&gt; 78.095  |  0.9762<br>
&gt;&gt;&gt; MVAPICH            |  160   |    8.9897  |   2.409 % |       3  |<br>
&gt;&gt;&gt; 154.457  |  0.9654<br>
&gt;&gt;&gt; MVAPICH            |  320   |    8.9780  |   2.801 % |       3  |<br>
&gt;&gt;&gt; 309.317  |  0.9666<br>
&gt;&gt;&gt; MVAPICH            |  480   |    8.9704  |   2.316 % |       3  |<br>
&gt;&gt;&gt; 464.366  |  0.9674<br>
&gt;&gt;&gt; MVAPICH            |  640   |    9.0792  |   1.138 % |       3  |<br>
&gt;&gt;&gt; 611.739  |  0.9558<br>
&gt;&gt;&gt; MVAPICH            |  720   |    9.1328  |   1.052 % |       3  |<br>
&gt;&gt;&gt; 684.162  |  0.9502<br>
&gt;&gt;&gt; MVAPICH            |  800   |    9.1945  |   0.773 % |       3  |<br>
&gt;&gt;&gt; 755.079  |  0.9438<br>
&gt;&gt;&gt; OpenMPI            |   16   |    8.6743  |   2.335 % |       2  |<br>
&gt;&gt;&gt; 16.000  |  1.0000<br>
&gt;&gt;&gt; OpenMPI            |   48   |    8.7826  |   1.605 % |       2  |<br>
&gt;&gt;&gt; 47.408  |  0.9877<br>
&gt;&gt;&gt; OpenMPI            |   80   |    8.8861  |   0.120 % |       2  |<br>
&gt;&gt;&gt; 78.093  |  0.9762<br>
&gt;&gt;&gt; OpenMPI            |  160   |    8.9774  |   0.785 % |       2  |<br>
&gt;&gt;&gt; 154.598  |  0.9662<br>
&gt;&gt;&gt; OpenMPI            |  320   |   12.0585  |  16.950 % |       2  |<br>
&gt;&gt;&gt; 230.191  |  0.7193<br>
&gt;&gt;&gt; OpenMPI            |  480   |   14.8330  |   1.300 % |       2  |<br>
&gt;&gt;&gt; 280.701  |  0.5848<br>
&gt;&gt;&gt; OpenMPI            |  640   |   17.1723  |   2.577 % |       3  |<br>
&gt;&gt;&gt; 323.283  |  0.5051<br>
&gt;&gt;&gt; OpenMPI            |  720   |   18.2153  |   2.798 % |       3  |<br>
&gt;&gt;&gt; 342.868  |  0.4762<br>
&gt;&gt;&gt; OpenMPI            |  800   |   19.3603  |   2.254 % |       3  |<br>
&gt;&gt;&gt; 358.434  |  0.4480<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div>

