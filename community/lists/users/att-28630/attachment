<html><head></head><body><div>Hi all,</div><div>In our cluster the nodes are interconnected with RoCE and I want to set up OpenMPI to run on it via SLURM.</div><div>I initially compiled OpenMPI 1.10.2 only with IB verbs support and I have no problem making it run over RoCE.</div><div>Then I have successfully built it with SLURM support as follows:</div><div><br></div><div>./configure&nbsp;--with-slurm&nbsp;--with-pmi=/usr/scheduler/slurm&nbsp;--with-verbs&nbsp;--with-hwloc</div><div><br></div><div>The problem is that I cannot let it use the RoCE network when I'm using srun. I also tried to export the OpenMPI runtime options but still I cannot correctly initialize the network:</div><div><br></div><div>$ echo $OMPI_MCA_btl</div><div>openib,self,sm</div><div>$ echo $OMPI_MCA_btl_openib_cpc_include&nbsp;</div><div>rdmacm</div><div>$ srun -n 2 --mpi=pmi2 ./osu_latency</div><div>--------------------------------------------------------------------------</div><div>No OpenFabrics connection schemes reported that they were able to be</div><div>used on a specific port.&nbsp;&nbsp;As such, the openib BTL (OpenFabrics</div><div>support) will be disabled for this port.</div><div><br></div><div>&nbsp; Local host:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;test-vmp1245</div><div>&nbsp; Local device:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mlx4_0</div><div>&nbsp; Local port:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2</div><div>&nbsp; CPCs attempted:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;udcm</div><div>--------------------------------------------------------------------------</div><div>--------------------------------------------------------------------------</div><div>No OpenFabrics connection schemes reported that they were able to be</div><div>used on a specific port.&nbsp;&nbsp;As such, the openib BTL (OpenFabrics</div><div>support) will be disabled for this port.</div><div><br></div><div>&nbsp; Local host:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;test-vmp1244</div><div>&nbsp; Local device:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mlx4_0</div><div>&nbsp; Local port:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2</div><div>&nbsp; CPCs attempted:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;udcm</div><div>--------------------------------------------------------------------------</div><div>--------------------------------------------------------------------------</div><div>At least one pair of MPI processes are unable to reach each other for</div><div>MPI communications.&nbsp;&nbsp;This means that no Open MPI device has indicated</div><div>that it can be used to communicate between these processes.&nbsp;&nbsp;This is</div><div>an error; Open MPI requires that all MPI processes be able to reach</div><div>each other.&nbsp;&nbsp;This error can sometimes be the result of forgetting to</div><div>specify the "self" BTL.</div><div><br></div><div>&nbsp; Process 1 ([[27,4],0]) is on host: test-vmp1244</div><div>&nbsp; Process 2 ([[27,4],1]) is on host: test-vmp1245</div><div>&nbsp; BTLs attempted: self</div><div><br></div><div>Your MPI job is now going to abort; sorry.</div><div>--------------------------------------------------------------------------</div><div>--------------------------------------------------------------------------</div><div>MPI_INIT has failed because at least one MPI process is unreachable</div><div>from another.&nbsp;&nbsp;This *usually* means that an underlying communication</div><div>plugin -- such as a BTL or an MTL -- has either not loaded or not</div><div>allowed itself to be used.&nbsp;&nbsp;Your MPI job will now abort.</div><div><br></div><div>You may wish to try to narrow down the problem;</div><div><br></div><div>&nbsp;* Check the output of ompi_info to see which BTL/MTL plugins are</div><div>&nbsp;&nbsp;&nbsp;available.</div><div>&nbsp;* Run your application with MPI_THREAD_SINGLE.</div><div>&nbsp;* Set the MCA parameter btl_base_verbose to 100 (or mtl_base_verbose,</div><div>&nbsp;&nbsp;&nbsp;if using MTL-based communications) to see exactly which</div><div>&nbsp;&nbsp;&nbsp;communication plugins were considered and/or discarded.</div><div>--------------------------------------------------------------------------</div><div>*** An error occurred in MPI_Init</div><div>*** on a NULL communicator</div><div>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,</div><div>***&nbsp;&nbsp;&nbsp;&nbsp;and potentially your MPI job)</div><div>[test-vmp1245:3603] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!</div><div>srun: error: test-vmp1244: task 0: Exited with exit code 1</div><div>srun: error: test-vmp1245: task 1: Exited with exit code 1</div><div><br></div><div>Any suggestion?</div><div>Thanks!</div><div><br></div><div>Davide</div></body></html>
