<div dir="ltr">Jeff,<div><br></div><div>I&#39;ve tried moving the backing file and it doesn&#39;t matter. I can say that PGI 14.7 + Open MPI 1.8.1 does not show this issue. I can run that on 96 cores just fine. Heck, I&#39;ve run it on a few hundred.</div>

<div><br></div><div>As for the 96, they are either on 8 Westmere nodes (8 nodes with 2 6-core sockets) or 6 Sandy Bridge nodes (6 nodes with 2 8-core sockets). I think each set is on a different Infiniband fabric, but I&#39;m not sure of that. However, since the PGI 14.7/Open MPI 1.8.1 works just fine on the exact same sets of nodes (grabbed via an interactive SLURM job), I can&#39;t see how the Infiniband fabric would matter. </div>

<div><br></div><div>I also tried various combinations of:</div><div><br></div><div>  mpirun --np</div><div>  mpirun --map-by core -np</div><div>  mpirun --map-by socket -np</div><div><br></div><div>and maybe a few -bind-to as well all with --report-bindings on to make sure it was doing what I expect, and it was. It wasn&#39;t putting 96 processes on a single node, for example, or all on the same socket or core by some freak accident.</div>

<div><br></div><div>The only difference between the Open MPI installs are the compilers they were built with (I&#39;m pretty sure the admins just downloaded the source once). Looking at &quot;mpif90 -showme&quot; I can see that the PGI 14.7 compile built the mpi_f90 and mpi modules while it looks like the GCC 4.9.1 did not, but our main code and this reproducer only use mpif.h, so that shouldn&#39;t matter.</div>

<div><br></div><div>Matt</div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Sat, Aug 16, 2014 at 7:33 AM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Have you tried moving your shared memory backing file directory, like the warning message suggests?<br>
<br>
I haven&#39;t seen a shared memory file on a network share cause correctness issues before (just performance issues), but I could see how that could be in the realm of possibility...<br>
<br>
Also, are you running 96 processes on a single machine, or spread across multiple machines?<br>
<br>
Note that Open MPI 1.8.x binds each MPI process to a core by default, so if you&#39;re oversubscribing the machine, it could be fairly disastrous...?<br>
<div><div class="h5"><br>
<br>
On Aug 14, 2014, at 1:29 PM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com">fortran@gmail.com</a>&gt; wrote:<br>
<br>
&gt; Open MPI Users,<br>
&gt;<br>
&gt; I work on a large climate model called GEOS-5 and we&#39;ve recently managed to get it to compile with gfortran 4.9.1 (our usual compilers are Intel and PGI for performance). In doing so, we asked our admins to install Open MPI 1.8.1 as the MPI stack instead of MVAPICH2 2.0 mainly because we figure the gfortran port is more geared to a desktop.<br>


&gt;<br>
&gt; So, the model builds just fine but when we run it, it stalls in our &quot;History&quot; component whose job is to write out netCDF files of output. The odd thing is, though, this stall seems to happen more on our Sandy Bridge nodes than on our Westmere nodes, but both hang.<br>


&gt;<br>
&gt; A colleague has made a single-file code that emulates our History component (the MPI traffic part) that we&#39;ve used to report bugs to MVAPICH and I asked him to try it with this issue and it seems to duplicate it.<br>


&gt;<br>
&gt; To wit, a &quot;successful&quot; run of the code is:<br>
&gt;<br>
&gt; (1003) $ mpirun -np 96 ./mpi_reproducer.x 4 24<br>
&gt; srun.slurm: cluster configuration lacks support for cpu binding<br>
&gt; srun.slurm: cluster configuration lacks support for cpu binding<br>
&gt; --------------------------------------------------------------------------<br>
&gt; WARNING: Open MPI will create a shared memory backing file in a<br>
&gt; directory that appears to be mounted on a network filesystem.<br>
&gt; Creating the shared memory backup file on a network file system, such<br>
&gt; as NFS or Lustre is not recommended -- it may cause excessive network<br>
&gt; traffic to your file servers and/or cause shared memory traffic in<br>
&gt; Open MPI to be much slower than expected.<br>
&gt;<br>
&gt; You may want to check what the typical temporary directory is on your<br>
&gt; node.  Possible sources of the location of this temporary directory<br>
&gt; include the $TEMPDIR, $TEMP, and $TMP environment variables.<br>
&gt;<br>
&gt; Note, too, that system administrators can set a list of filesystems<br>
&gt; where Open MPI is disallowed from creating temporary files by setting<br>
&gt; the MCA parameter &quot;orte_no_session_dir&quot;.<br>
&gt;<br>
&gt;   Local host: borg01s026<br>
&gt;   Fileame:    /gpfsm/dnb31/tdirs/pbs/slurm.2202701.mathomp4/openmpi-sessions-mathomp4@borg01s026_0/60464/1/shared_mem_pool.borg01s026<br>
&gt;<br>
&gt; You can set the MCA paramter shmem_mmap_enable_nfs_warning to 0 to<br>
&gt; disable this message.<br>
&gt; --------------------------------------------------------------------------<br>
&gt;  nx:            4<br>
&gt;  ny:           24<br>
&gt;  comm size is           96<br>
&gt;  local array sizes are          12          12<br>
&gt;  filling local arrays<br>
&gt;  creating requests<br>
&gt;  igather<br>
&gt;  before collective wait<br>
&gt;  after collective wait<br>
&gt;  result is            1   1.00000000       1.00000000<br>
&gt;  result is            2   1.41421354       1.41421354<br>
&gt;  result is            3   1.73205078       1.73205078<br>
&gt;  result is            4   2.00000000       2.00000000<br>
&gt;  result is            5   2.23606801       2.23606801<br>
&gt;  result is            6   2.44948983       2.44948983<br>
&gt;  result is            7   2.64575124       2.64575124<br>
&gt;  result is            8   2.82842708       2.82842708<br>
&gt; ...snip...<br>
&gt;  result is          939   30.6431065       30.6431065<br>
&gt;  result is          940   30.6594200       30.6594200<br>
&gt;  result is          941   30.6757240       30.6757240<br>
&gt;  result is          942   30.6920185       30.6920185<br>
&gt;  result is          943   30.7083054       30.7083054<br>
&gt;  result is          944   30.7245827       30.7245827<br>
&gt;  result is          945   30.7408524       30.7408524<br>
&gt;<br>
&gt; Where the second and third columns of numbers are just the square root of the first.<br>
&gt;<br>
&gt; But, often, the runs do this (note I&#39;m removing the shmem_mmap_enable_nfs_warning message for sanity&#39;s sake from these copy and pastes):<br>
&gt;<br>
&gt; (1196) $ mpirun -np 96 ./mpi_reproducer.x 4 24<br>
&gt; srun.slurm: cluster configuration lacks support for cpu binding<br>
&gt; srun.slurm: cluster configuration lacks support for cpu binding<br>
&gt;  nx:            4<br>
&gt;  ny:           24<br>
&gt;  comm size is           96<br>
&gt;  local array sizes are          12          12<br>
&gt;  filling local arrays<br>
&gt;  creating requests<br>
&gt;  igather<br>
&gt;  before collective wait<br>
&gt;  after collective wait<br>
&gt;  result is            1   1.00000000       1.00000000<br>
&gt;  result is            2   1.41421354       1.41421354<br>
&gt; [borg01w021:09264] 89 more processes have sent help message help-opal-shmem-mmap.txt / mmap on nfs<br>
&gt; [borg01w021:09264] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>
&gt;<br>
&gt; where it prints out a few results.<br>
&gt;<br>
&gt; The worst case is most often seen on Sandy Bridge and is the most often failure:<br>
&gt;<br>
&gt; (1197) $ mpirun -np 96 ./mpi_reproducer.x 4 24<br>
&gt; srun.slurm: cluster configuration lacks support for cpu binding<br>
&gt; srun.slurm: cluster configuration lacks support for cpu binding<br>
&gt;  nx:            4<br>
&gt;  ny:           24<br>
&gt;  comm size is           96<br>
&gt;  local array sizes are          12          12<br>
&gt;  filling local arrays<br>
&gt;  creating requests<br>
&gt;  igather<br>
&gt;  before collective wait<br>
&gt; [borg01w021:09367] 89 more processes have sent help message help-opal-shmem-mmap.txt / mmap on nfs<br>
&gt; [borg01w021:09367] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>
&gt;<br>
&gt; This halt best compares to our full model code. It halts much at the same &quot;place&quot; around a collective wait.<br>
&gt;<br>
&gt; Finally, if I setenv OMPI_MCA_orte_base_help_aggregate 0 (to see all help/error messages) I usually just &quot;hang&quot; with no error message at all (additionally turning off the warning):<br>
&gt;<br>
&gt; (1203) $ setenv OMPI_MCA_orte_base_help_aggregate 0<br>
&gt; (1203) $ setenv OMPI_MCA_shmem_mmap_enable_nfs_warning 0<br>
&gt; (1204) $ mpirun -np 96 ./mpi_reproducer.x 4 24<br>
&gt; srun.slurm: cluster configuration lacks support for cpu binding<br>
&gt; srun.slurm: cluster configuration lacks support for cpu binding<br>
&gt;  nx:            4<br>
&gt;  ny:           24<br>
&gt;  comm size is           96<br>
&gt;  local array sizes are          12          12<br>
&gt;  filling local arrays<br>
&gt;  creating requests<br>
&gt;  igather<br>
&gt;  before collective wait<br>
&gt;<br>
&gt; Note, this problem doesn&#39;t seem to appear at lower number of processes (16, 24, 32) but does seem pretty consistent at 96, especially on Sandy Bridges.<br>
&gt;<br>
&gt; Also, yes, we get that weird srun.slurm warning but we always seem to get that (Open MPI, MVAPICH) so while our admins are trying to correct that, at present it is not our worry.<br>
&gt;<br>
&gt; The MPI stack was compiled with (per our admins):<br>
&gt;<br>
&gt; export CFLAGS=&quot;-fPIC -m64&quot;<br>
&gt; export CXXFLAGS=&quot;-fPIC -m64&quot;<br>
&gt; export FFLAGS=&quot;-fPIC&quot;<br>
&gt; export FCFLAGS=&quot;-fPIC&quot;<br>
&gt; export F90FLAGS=&quot;-fPIC&quot;<br>
&gt;<br>
&gt; export LDFLAGS=&quot;-L/usr/nlocal/slurm/2.6.3/lib64&quot;<br>
&gt; export CPPFLAGS=&quot;-I/usr/nlocal/slurm/2.6.3/include&quot;<br>
&gt;<br>
&gt; export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/nlocal/slurm/2.6.3/lib64<br>
&gt;<br>
&gt; ../configure --with-slurm --disable-wrapper-rpath --enable-shared<br>
&gt; --enable-mca-no-build=btl-usnic --prefix=${PREFIX}<br>
&gt;<br>
&gt; The output of &quot;ompi_info --all&quot; is found:<br>
&gt;<br>
&gt;   <a href="https://gist.github.com/mathomp4/301723165efbbb616184#file-ompi_info-out" target="_blank">https://gist.github.com/mathomp4/301723165efbbb616184#file-ompi_info-out</a><br>
&gt;<br>
&gt; The reproducer code can be found here:<br>
&gt;<br>
&gt;   <a href="https://gist.github.com/mathomp4/301723165efbbb616184#file-mpi_reproducer-f90" target="_blank">https://gist.github.com/mathomp4/301723165efbbb616184#file-mpi_reproducer-f90</a><br>
&gt;<br>
&gt; The reproducer is easily built with just &#39;mpif90&#39; and to run it:<br>
&gt;<br>
&gt;   mpirun -np NPROCS ./mpi_reproducer.x NX NY<br>
&gt;<br>
&gt; where NX*NY has to equal NPROCS and it&#39;s best to keep them even numbers. (There might be a few more restrictions and the code will die if you violate them.)<br>
&gt;<br>
&gt; Thanks,<br>
&gt; Matt Thompson<br>
&gt;<br>
&gt; --<br>
&gt; Matt Thompson          SSAI, Sr Software Test Engr<br>
&gt; NASA GSFC, Global Modeling and Assimilation Office<br>
&gt; Code 610.1, 8800 Greenbelt Rd, Greenbelt, MD 20771<br>
&gt; Phone: <a href="tel:301-614-6712" value="+13016146712">301-614-6712</a>              Fax: <a href="tel:301-614-6246" value="+13016146246">301-614-6246</a><br>
&gt;<br>
</div></div>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25022.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25022.php</a><br>
<span class="HOEnZb"><font color="#888888"><br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25045.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25045.php</a><br>
</font></span></blockquote></div><br><br clear="all"><div><br></div>-- <br><div dir="ltr"><div>&quot;And, isn&#39;t sanity really just a one-trick pony anyway? I mean all you</div><div> get is one trick: rational thinking. But when you&#39;re good and crazy, </div>

<div> oooh, oooh, oooh, the sky is the limit!&quot; -- The Tick</div><div><br></div></div>
</div>

