Dear OpenMPI developers<br><br>I&#39;d like to add my 2 cents that this would be a very desirable feature enhancement for me as well (and perhaps others).<br><br>Best regards<br>Durga<br><br><br><div class="gmail_quote">On Tue, Aug 14, 2012 at 4:29 PM, Zbigniew Koza <span dir="ltr">&lt;<a href="mailto:zzkoza@gmail.com" target="_blank">zzkoza@gmail.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi,<br>
<br>
I&#39;ve just found this information on  nVidia&#39;s plans regarding enhanced support for MPI in their CUDA toolkit:<br>
<a href="http://developer.nvidia.com/cuda/nvidia-gpudirect" target="_blank">http://developer.nvidia.com/<u></u>cuda/nvidia-gpudirect</a><br>
<br>
The idea that two GPUs can talk to each other via network cards without CPU as a middleman looks very promising.<br>
This technology is supposed to be revealed and released in September.<br>
<br>
My questions:<br>
<br>
1. Will OpenMPI include   RDMA support in its CUDA interface?<br>
2. Any idea how much can this technology reduce the CUDA Send/Recv latency?<br>
3. Any idea whether this technology will be available for Fermi-class Tesla devices or only for Keplers?<br>
<br>
Regards,<br>
<br>
Z Koza<br>
<br>
<br>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
</blockquote></div><br>

