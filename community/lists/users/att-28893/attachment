<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; color: rgb(0, 0, 0); font-size: 15px; font-family: Calibri, sans-serif;">
<div>
<div>
<div>Thanks Ralph and Gilles.</div>
<div>
<div><br>
</div>
<div>Thanks,</div>
<div>Murali</div>
<div><br>
</div>
<br>
</div>
</div>
</div>
<div><br>
</div>
<span id="OLK_SRC_BODY_SECTION">
<div style="font-family:Calibri; font-size:11pt; text-align:left; color:black; BORDER-BOTTOM: medium none; BORDER-LEFT: medium none; PADDING-BOTTOM: 0in; PADDING-LEFT: 0in; PADDING-RIGHT: 0in; BORDER-TOP: #b5c4df 1pt solid; BORDER-RIGHT: medium none; PADDING-TOP: 3pt">
<span style="font-weight:bold">From: </span>users &lt;<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>&gt; on behalf of Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;<br>
<span style="font-weight:bold">Reply-To: </span>Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
<span style="font-weight:bold">Date: </span>Sunday, April 3, 2016 at 6:41 AM<br>
<span style="font-weight:bold">To: </span>Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
<span style="font-weight:bold">Subject: </span>Re: [OMPI users] Question on MPI_Comm_spawn timing<br>
</div>
<div><br>
</div>
<div>
<div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">
I honestly don’t think anyone has been concerned about the speed of MPI_Comm_spawn, and so there hasn’t been any effort made to optimize it
<div class=""><br class="">
</div>
<div class=""><br class="">
<div>
<blockquote type="cite" class="">
<div class="">On Apr 3, 2016, at 2:52 AM, Gilles Gouaillardet &lt;<a href="mailto:gilles.gouaillardet@gmail.com" class="">gilles.gouaillardet@gmail.com</a>&gt; wrote:</div>
<br class="Apple-interchange-newline">
<div class="">Hi,
<div class=""><br class="">
</div>
<div class="">performance of MPI_Comm_spawn in the v1.8/v1.10 series&nbsp;is known to be poor, especially compared to v1.6</div>
<div class=""><br class="">
</div>
<div class="">generally speaking, I cannot recommend v1.6 since it is no more maintained.</div>
<div class="">that being said, if performance is critical, you might want to give it a try.</div>
<div class=""><br class="">
</div>
<div class="">I did not run any performance measurement with master, especially since we moved to PMIx,</div>
<div class="">that might be worth a try too</div>
<div class=""><br class="">
</div>
<div class="">Cheers,</div>
<div class=""><br class="">
</div>
<div class="">Gilles<br class="">
<br class="">
On Sunday, April 3, 2016, Emani, Murali &lt;<a href="mailto:emani1@llnl.gov" class="">emani1@llnl.gov</a>&gt; wrote:<br class="">
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Hi all,<br class="">
<br class="">
I am trying to evaluate the time taken for MPI_Comm_spawn operation in the<br class="">
latest version of OpenMPI. Here a parent communicator (all processes, not<br class="">
just the root) spawns one new child process (separate executable). The<br class="">
code I¹m executing is<br class="">
<br class="">
main(){<br class="">
{<br class="">
Š..<br class="">
// MPI initialization<br class="">
Š..<br class="">
start1 = MPI_Wtime();<br class="">
MPI_Comm_spawn(³./child&quot;, MPI_ARGV_NULL,1, MPI_INFO_NULL, 0,<br class="">
MPI_COMM_WORLD, &amp;inter_communicator, MPI_ERRCODES_IGNORE );<br class="">
End = MPI_Wtime();<br class="">
<br class="">
printf(³ spawn time: %f², (end-start));<br class="">
MPI_Barrier(inter_communicator); // spawn is collective, but still want to<br class="">
ensure it using a barrier<br class="">
..<br class="">
..<br class="">
// MPI finalize<br class="">
}<br class="">
<br class="">
<br class="">
In child.c<br class="">
main(){<br class="">
{<br class="">
Š..<br class="">
// MPI initialization<br class="">
Š..<br class="">
<br class="">
MPI_Comm_get_parent(&amp;parentcomm); // gets the inter-communicator<br class="">
MPI_Barrier(parentcomm);<br class="">
..<br class="">
..<br class="">
// MPI finalize<br class="">
}<br class="">
<br class="">
My observation is that the spawn time is very high (almost 80% of the<br class="">
total program execution time). It increases exponentially with the number<br class="">
of processes in the parent communicator. Is this method correct, and is<br class="">
the MPI_Comm_spawn operation expensive.<br class="">
I have also tried MPI_Comm_spawn_multiple but it still measures the same<br class="">
time.<br class="">
<br class="">
Could kindly someone guide me in this issue.<br class="">
<br class="">
Thanks,<br class="">
Murali<br class="">
<br class="">
<br class="">
<br class="">
_______________________________________________<br class="">
users mailing list<br class="">
<a href="javascript:;" onclick="_e(event, 'cvml', 'users@open-mpi.org')" class="">users@open-mpi.org</a><br class="">
Subscription: <a href="http://secure-web.cisco.com/1z0v_U78rf_0ofSUeyHRS36Fj-mk74BguweaGfG7pX9MxfOcN1eiC_hUDhW9vqTMtTPbrFNAMQHqAtrLtbFTpAjduzGF-kqmEYhcbTlFzHJ1zzF6H0czF7KD40VyYqVvjMk3GhonQ4c-TX7IpOmyqwdsds5OIz01wDIsfGBVxLqsYKCDNsS2ulGqDi3aoOT2VIeTn1yYAOAzLdVkdqP4cnPbmpreqJwAdREmXahmtoD5lAQV2FJXI6Fzm1Hdk0lpO6gHzDuQ7aAUW4jlUuTczHpYKKg9t_JpfzcF-WWZgKGPvB-9YhFQL-SPHw6iWqpCFho36EeumgHWN3oRw-nOHp1QZEh6fPaMb3_yaeErV3Gc/http%3A%2F%2Fwww.open-mpi.org%2Fmailman%2Flistinfo.cgi%2Fusers" target="_blank" class="">
http://secure-web.cisco.com/1z0v_U78rf_0ofSUeyHRS36Fj-mk74BguweaGfG7pX9MxfOcN1eiC_hUDhW9vqTMtTPbrFNAMQHqAtrLtbFTpAjduzGF-kqmEYhcbTlFzHJ1zzF6H0czF7KD40VyYqVvjMk3GhonQ4c-TX7IpOmyqwdsds5OIz01wDIsfGBVxLqsYKCDNsS2ulGqDi3aoOT2VIeTn1yYAOAzLdVkdqP4cnPbmpreqJwAdREmXahmtoD5lAQV2FJXI6Fzm1Hdk0lpO6gHzDuQ7aAUW4jlUuTczHpYKKg9t_JpfzcF-WWZgKGPvB-9YhFQL-SPHw6iWqpCFho36EeumgHWN3oRw-nOHp1QZEh6fPaMb3_yaeErV3Gc/http%3A%2F%2Fwww.open-mpi.org%2Fmailman%2Flistinfo.cgi%2Fusers</a><br class="">
Link to this post: <a href="http://secure-web.cisco.com/13MtbvneBMZbxflPfKcY3Ej3Lqiwgo-u3nP2qeSvXFzeJ5lrH_QoikbeMEiFrL1D2BGSXO2U7qcdCyzPyKzhCWHiYm4O92e3jpXTu4lX2cEAQUo-o8DSsAhMi_UQeIKIYLIkTvELf3WM-qqo7oK2VU6uvtyrJO6WpJ_0OW-Nupk-V4sRGUb3WXFTT2Bq9GnU6NtjpNql2If90LZkTsaBAlsoxVx-4oNdLmiOuHIyIb5xvRx-FRvSL8Pr8ZHNmUYMSqdx-tU2PgMFbjivrVbXcjPfDkYCvcyOz9i3BSCxRfJwSdeDu1sRwfkk8Jf6kEcrNiIGO5EXzUo1xQyjNJyCd73gR3bGcqT-i_uwyn_Iw2_I/http%3A%2F%2Fwww.open-mpi.org%2Fcommunity%2Flists%2Fusers%2F2016%2F04%2F28871.php" target="_blank" class="">
http://secure-web.cisco.com/13MtbvneBMZbxflPfKcY3Ej3Lqiwgo-u3nP2qeSvXFzeJ5lrH_QoikbeMEiFrL1D2BGSXO2U7qcdCyzPyKzhCWHiYm4O92e3jpXTu4lX2cEAQUo-o8DSsAhMi_UQeIKIYLIkTvELf3WM-qqo7oK2VU6uvtyrJO6WpJ_0OW-Nupk-V4sRGUb3WXFTT2Bq9GnU6NtjpNql2If90LZkTsaBAlsoxVx-4oNdLmiOuHIyIb5xvRx-FRvSL8Pr8ZHNmUYMSqdx-tU2PgMFbjivrVbXcjPfDkYCvcyOz9i3BSCxRfJwSdeDu1sRwfkk8Jf6kEcrNiIGO5EXzUo1xQyjNJyCd73gR3bGcqT-i_uwyn_Iw2_I/http%3A%2F%2Fwww.open-mpi.org%2Fcommunity%2Flists%2Fusers%2F2016%2F04%2F28871.php</a><br class="">
</blockquote>
</div>
_______________________________________________<br class="">
users mailing list<br class="">
<a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">
Subscription: <a href="http://secure-web.cisco.com/1z0v_U78rf_0ofSUeyHRS36Fj-mk74BguweaGfG7pX9MxfOcN1eiC_hUDhW9vqTMtTPbrFNAMQHqAtrLtbFTpAjduzGF-kqmEYhcbTlFzHJ1zzF6H0czF7KD40VyYqVvjMk3GhonQ4c-TX7IpOmyqwdsds5OIz01wDIsfGBVxLqsYKCDNsS2ulGqDi3aoOT2VIeTn1yYAOAzLdVkdqP4cnPbmpreqJwAdREmXahmtoD5lAQV2FJXI6Fzm1Hdk0lpO6gHzDuQ7aAUW4jlUuTczHpYKKg9t_JpfzcF-WWZgKGPvB-9YhFQL-SPHw6iWqpCFho36EeumgHWN3oRw-nOHp1QZEh6fPaMb3_yaeErV3Gc/http%3A%2F%2Fwww.open-mpi.org%2Fmailman%2Flistinfo.cgi%2Fusers">
http://secure-web.cisco.com/1z0v_U78rf_0ofSUeyHRS36Fj-mk74BguweaGfG7pX9MxfOcN1eiC_hUDhW9vqTMtTPbrFNAMQHqAtrLtbFTpAjduzGF-kqmEYhcbTlFzHJ1zzF6H0czF7KD40VyYqVvjMk3GhonQ4c-TX7IpOmyqwdsds5OIz01wDIsfGBVxLqsYKCDNsS2ulGqDi3aoOT2VIeTn1yYAOAzLdVkdqP4cnPbmpreqJwAdREmXahmtoD5lAQV2FJXI6Fzm1Hdk0lpO6gHzDuQ7aAUW4jlUuTczHpYKKg9t_JpfzcF-WWZgKGPvB-9YhFQL-SPHw6iWqpCFho36EeumgHWN3oRw-nOHp1QZEh6fPaMb3_yaeErV3Gc/http%3A%2F%2Fwww.open-mpi.org%2Fmailman%2Flistinfo.cgi%2Fusers</a><br class="">
Link to this post: <a href="http://secure-web.cisco.com/1Iw8n_xjvr1cInNKbFh8730whotP6hbxpFj-u8Z0n_SmcsfaJHY42pPRsNDcvV-fXHjHoyf0UW5vW43x5-724wT6QS5GGEI7zNGcj24W6TfyzVRhhEFfFoFuODUG3HsLB19QyiUx96e3pN62suKOegK-BpnRSinst01viAL5bcJg2YHvuhlSlXaxO6eYx1RQf0GMFihZV_5OWT-GpaRpGW3YoSQZT94z7yWpL92D3bxesZdBWCGgy-uxuXePTekRfFwTZPGi26vu-9kMvABX8OOVzZlhJb8PA4E3urjAVDvJ9Uwclk2m1aM0EQRuqnT2QaXY6FTxMMO0jTcyLQKSoURrJRhH_cnxuOMyo_YrqSUY/http%3A%2F%2Fwww.open-mpi.org%2Fcommunity%2Flists%2Fusers%2F2016%2F04%2F28872.php">
http://secure-web.cisco.com/1Iw8n_xjvr1cInNKbFh8730whotP6hbxpFj-u8Z0n_SmcsfaJHY42pPRsNDcvV-fXHjHoyf0UW5vW43x5-724wT6QS5GGEI7zNGcj24W6TfyzVRhhEFfFoFuODUG3HsLB19QyiUx96e3pN62suKOegK-BpnRSinst01viAL5bcJg2YHvuhlSlXaxO6eYx1RQf0GMFihZV_5OWT-GpaRpGW3YoSQZT94z7yWpL92D3bxesZdBWCGgy-uxuXePTekRfFwTZPGi26vu-9kMvABX8OOVzZlhJb8PA4E3urjAVDvJ9Uwclk2m1aM0EQRuqnT2QaXY6FTxMMO0jTcyLQKSoURrJRhH_cnxuOMyo_YrqSUY/http%3A%2F%2Fwww.open-mpi.org%2Fcommunity%2Flists%2Fusers%2F2016%2F04%2F28872.php</a></div>
</blockquote>
</div>
<br class="">
</div>
</div>
</div>
</span>
</body>
</html>
