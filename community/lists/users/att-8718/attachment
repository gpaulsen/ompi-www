Thanks. I&#39;ve tried your suggestion.<br><br>$ cat hpl-8cpu-test.sge<br>#!/bin/bash<br>#<br>#$ -N HPL_8cpu_GB<br>#$ -pe orte 8<br>#$ -cwd<br>#$ -j y<br>#$ -S /bin/bash<br>#$ -V<br>#<br>/opt/openmpi-gcc/bin/mpirun -mca ras_gridengine_verbose 100 -v -np $NSLOTS --host node0001,node0002 hostname<br>
<br><br>It allocated 2 nodes to run, however all the processes are spawned in node0001.<br><br>$ qstat -f<br>queuename                      qtype resv/used/tot. load_avg arch          states<br>---------------------------------------------------------------------------------<br>
<a href="mailto:all.q@node0001.v5cluster.com">all.q@node0001.v5cluster.com</a>   BIPC  0/4/4          4.79     lx24-amd64<br>     45 0.55500 HPL_8cpu_G admin        r     04/02/2009 00:26:49     4<br>---------------------------------------------------------------------------------<br>
<a href="mailto:all.q@node0002.v5cluster.com">all.q@node0002.v5cluster.com</a>   BIPC  0/4/4          0.00     lx24-amd64<br>     45 0.55500 HPL_8cpu_G admin        r     04/02/2009 00:26:49     4<br><br><br>$ cat HPL_8cpu_GB.o45<br>
[node0001:03194] ras:gridengine: JOB_ID: 45<br>[node0001:03194] ras:gridengine: <a href="http://node0001.v5cluster.com">node0001.v5cluster.com</a>: PE_HOSTFILE shows slots=4<br>[node0001:03194] ras:gridengine: <a href="http://node0002.v5cluster.com">node0002.v5cluster.com</a>: PE_HOSTFILE shows slots=4<br>
node0001<br>node0001<br>node0001<br>node0001<br>node0001<br>node0001<br>node0001<br>node0001<br><br>$ qconf -sq all.q<br>qname                 all.q<br>hostlist              @allhosts<br>seq_no                0<br>load_thresholds       np_load_avg=1.75<br>
suspend_thresholds    NONE<br>nsuspend              1<br>suspend_interval      00:05:00<br>priority              0<br>min_cpu_interval      00:01:00<br>processors            UNDEFINED<br>qtype                 BATCH INTERACTIVE<br>
ckpt_list             blcr<br>pe_list               make mpi-rr mpi-fu orte<br>rerun                 FALSE<br>slots                 4,[node0001=4],[node0002=4]<br>tmpdir                /tmp<br>shell                 /bin/sh<br>
prolog                NONE<br>epilog                NONE<br>shell_start_mode      posix_compliant<br>starter_method        NONE<br>suspend_method        NONE<br>resume_method         NONE<br>terminate_method      NONE<br>
notify                00:00:60<br>owner_list            NONE<br>user_lists            NONE<br>xuser_lists           NONE<br>subordinate_list      NONE<br>complex_values        NONE<br>projects              NONE<br>xprojects             NONE<br>
calendar              NONE<br>initial_state         default<br>s_rt                  INFINITY<br>h_rt                  INFINITY<br>s_cpu                 INFINITY<br>h_cpu                 INFINITY<br>s_fsize               INFINITY<br>
h_fsize               INFINITY<br>s_data                INFINITY<br>h_data                INFINITY<br>s_stack               INFINITY<br>h_stack               INFINITY<br>s_core                INFINITY<br>h_core                INFINITY<br>
s_rss                 INFINITY<br>h_rss                 INFINITY<br>s_vmem                INFINITY<br>h_vmem                INFINITY<br><br>$ qconf -se node0001<br>hostname              <a href="http://node0001.v5cluster.com">node0001.v5cluster.com</a><br>
load_scaling          NONE<br>complex_values        slots=4<br>load_values           arch=lx24-amd64,num_proc=4,mem_total=3949.597656M, \<br>                      swap_total=0.000000M,virtual_total=3949.597656M, \<br>                      load_avg=2.800000,load_short=0.220000, \<br>
                      load_medium=2.800000,load_long=2.320000, \<br>                      mem_free=3818.746094M,swap_free=0.000000M, \<br>                      virtual_free=3818.746094M,mem_used=130.851562M, \<br>                      swap_used=0.000000M,virtual_used=130.851562M, \<br>
                      cpu=0.000000,np_load_avg=0.700000, \<br>                      np_load_short=0.055000,np_load_medium=0.700000, \<br>                      np_load_long=0.580000<br>processors            4<br>user_lists            NONE<br>
xuser_lists           NONE<br>projects              NONE<br>xprojects             NONE<br>usage_scaling         NONE<br>report_variables      NONE<br><br>$ qconf -se node0002<br>hostname              <a href="http://node0002.v5cluster.com">node0002.v5cluster.com</a><br>
load_scaling          NONE<br>complex_values        slots=4<br>load_values           arch=lx24-amd64,num_proc=4,mem_total=3949.597656M, \<br>                      swap_total=0.000000M,virtual_total=3949.597656M, \<br>                      load_avg=0.000000,load_short=0.000000, \<br>
                      load_medium=0.000000,load_long=0.000000, \<br>                      mem_free=3843.074219M,swap_free=0.000000M, \<br>                      virtual_free=3843.074219M,mem_used=106.523438M, \<br>                      swap_used=0.000000M,virtual_used=106.523438M, \<br>
                      cpu=0.000000,np_load_avg=0.000000, \<br>                      np_load_short=0.000000,np_load_medium=0.000000, \<br>                      np_load_long=0.000000<br>processors            4<br>user_lists            NONE<br>
xuser_lists           NONE<br>projects              NONE<br>xprojects             NONE<br>usage_scaling         NONE<br>report_variables      NONE<br><br><br><br><div class="gmail_quote">2009/4/1 Rolf Vandevaart <span dir="ltr">&lt;<a href="mailto:Rolf.Vandevaart@sun.com">Rolf.Vandevaart@sun.com</a>&gt;</span><br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">It turns out that the use of --host and --hostfile act as a filter of which nodes to run on when you are running under SGE.  So, listing them several times does not affect where the processes land.  However, this still does not explain why you are seeing what you are seeing.  One thing you can try is to add this to the mpirun command.<br>

<br>
 -mca ras_gridengine_verbose 100<br>
<br>
This will provide some additional information as to what Open MPI is seeing as nodes and slots from SGE.  (Is there any chance that node0002 actually has 8 slots?)<br>
<br>
I just retried on my cluster of 2 CPU sparc solaris nodes.  When I run with np=2, the two MPI processes will all land on a single node, because that node has two slots.  When I go up to np=4, then they move on to the other node.  The --host acts as a filter to where they should run.<br>

<br>
In terms of the using &quot;IB bonding&quot;, I do not know what that means exactly.  Open MPI does stripe over multiple IB interfaces, so I think the answer is yes.<br>
<br>
Rolf<br>
<br>
PS:  Here is what my np=4 job script looked like.  (I just changed np=2 for the other run)<br>
<br>
 burl-ct-280r-0 148 =&gt;more run.sh<br>
#! /bin/bash<br>
#$ -S /bin/bash<br>
#$ -V<br>
#$ -cwd<br>
#$ -N Job1<br>
#$ -pe orte 200<br>
#$ -j y<br>
#$ -l h_rt=00:20:00      # Run time (hh:mm:ss) - 10 min<br>
<br>
echo $NSLOTS<br>
/opt/SUNWhpc/HPC8.2/sun/bin/mpirun -mca ras_gridengine_verbose 100 -v -np 4 -host burl-ct-280r-1,burl-ct-280r-0 -mca btl self,sm,tcp hostname<br>
<br>
Here is the output (somewhat truncated)<br>
 burl-ct-280r-0 150 =&gt;more Job1.o199<br>
200<br>
[burl-ct-280r-2:22132] ras:gridengine: JOB_ID: 199<br>
[burl-ct-280r-2:22132] ras:gridengine: PE_HOSTFILE: /ws/ompi-tools/orte/sge/sge6_2u1/default/spool/burl-ct-280r-2/active_jobs/199.1/pe_hostfile<br>
[..snip..]<br>
[burl-ct-280r-2:22132] ras:gridengine: burl-ct-280r-0: PE_HOSTFILE shows slots=2<br>
[burl-ct-280r-2:22132] ras:gridengine: burl-ct-280r-1: PE_HOSTFILE shows slots=2<br>
[..snip..]<br>
burl-ct-280r-1<br>
burl-ct-280r-1<br>
burl-ct-280r-0<br>
burl-ct-280r-0<br>
 burl-ct-280r-0 151 =&gt;<div><div></div><div class="h5"><br>
<br>
<br>
On 03/31/09 22:39, PN wrote:<br>
</div></div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div><div></div><div class="h5">
Dear Rolf,<br>
<br>
Thanks for your reply.<br>
I&#39;ve created another PE and changed the submission script, explicitly specify the hostname with &quot;--host&quot;.<br>
However the result is the same.<br>
<br>
# qconf -sp orte<br>
pe_name            orte<br>
slots              8<br>
user_lists         NONE<br>
xuser_lists        NONE<br>
start_proc_args    /bin/true<br>
stop_proc_args     /bin/true<br>
allocation_rule    $fill_up<br>
control_slaves     TRUE<br>
job_is_first_task  FALSE<br>
urgency_slots      min<br>
accounting_summary TRUE<br>
<br>
$ cat hpl-8cpu-test.sge<br>
#!/bin/bash<br>
#<br>
#$ -N HPL_8cpu_GB<br>
#$ -pe orte 8<br>
#$ -cwd<br>
#$ -j y<br>
#$ -S /bin/bash<br>
#$ -V<br>
#<br>
cd /home/admin/hpl-2.0<br>
/opt/openmpi-gcc/bin/mpirun -v -np $NSLOTS --host node0001,node0001,node0001,node0001,node0002,node0002,node0002,node0002 ./bin/goto-openmpi-gcc/xhpl<br>
<br>
<br>
# pdsh -a ps ax --width=200|grep hpl<br>
node0002: 18901 ?        S      0:00 /opt/openmpi-gcc/bin/mpirun -v -np 8 --host node0001,node0001,node0001,node0001,node0002,node0002,node0002,node0002 ./bin/goto-openmpi-gcc/xhpl<br>
node0002: 18902 ?        RLl    0:29 ./bin/goto-openmpi-gcc/xhpl<br>
node0002: 18903 ?        RLl    0:29 ./bin/goto-openmpi-gcc/xhpl<br>
node0002: 18904 ?        RLl    0:28 ./bin/goto-openmpi-gcc/xhpl<br>
node0002: 18905 ?        RLl    0:28 ./bin/goto-openmpi-gcc/xhpl<br>
node0002: 18906 ?        RLl    0:29 ./bin/goto-openmpi-gcc/xhpl<br>
node0002: 18907 ?        RLl    0:28 ./bin/goto-openmpi-gcc/xhpl<br>
node0002: 18908 ?        RLl    0:28 ./bin/goto-openmpi-gcc/xhpl<br>
node0002: 18909 ?        RLl    0:28 ./bin/goto-openmpi-gcc/xhpl<br>
<br>
Any hint to debug this situation?<br>
<br>
Also, if I have 2 IB ports in each node, which IB bonding was done, will Open MPI automatically benefit from the double bandwidth?<br>
<br>
Thanks a lot.<br>
<br>
Best Regards,<br>
PN<br>
<br></div></div>
2009/4/1 Rolf Vandevaart &lt;<a href="mailto:Rolf.Vandevaart@sun.com" target="_blank">Rolf.Vandevaart@sun.com</a> &lt;mailto:<a href="mailto:Rolf.Vandevaart@sun.com" target="_blank">Rolf.Vandevaart@sun.com</a>&gt;&gt;<div>
<div></div><div class="h5"><br>
<br>
    On 03/31/09 11:43, PN wrote:<br>
<br>
        Dear all,<br>
<br>
        I&#39;m using Open MPI 1.3.1 and SGE 6.2u2 on CentOS 5.2<br>
        I have 2 compute nodes for testing, each node has a single quad<br>
        core CPU.<br>
<br>
        Here is my submission script and PE config:<br>
        $ cat hpl-8cpu.sge<br>
        #!/bin/bash<br>
        #<br>
        #$ -N HPL_8cpu_IB<br>
        #$ -pe mpi-fu 8<br>
        #$ -cwd<br>
        #$ -j y<br>
        #$ -S /bin/bash<br>
        #$ -V<br>
        #<br>
        cd /home/admin/hpl-2.0<br>
        # For IB<br>
        /opt/openmpi-gcc/bin/mpirun -v -np $NSLOTS -machinefile<br>
        $TMPDIR/machines ./bin/goto-openmpi-gcc/xhpl<br>
<br>
        I&#39;ve tested the mpirun command can be run correctly in command line.<br>
<br>
        $ qconf -sp mpi-fu<br>
        pe_name            mpi-fu<br>
        slots              8<br>
        user_lists         NONE<br>
        xuser_lists        NONE<br>
        start_proc_args    /opt/sge/mpi/startmpi.sh -catch_rsh $pe_hostfile<br>
        stop_proc_args     /opt/sge/mpi/stopmpi.sh<br>
        allocation_rule    $fill_up<br>
        control_slaves     TRUE<br>
        job_is_first_task  FALSE<br>
        urgency_slots      min<br>
        accounting_summary TRUE<br>
<br>
<br>
        I&#39;ve checked the $TMPDIR/machines after submit, it was correct.<br>
        node0002<br>
        node0002<br>
        node0002<br>
        node0002<br>
        node0001<br>
        node0001<br>
        node0001<br>
        node0001<br>
<br>
        However, I found that if I explicitly specify the &quot;-machinefile<br>
        $TMPDIR/machines&quot;, all 8 mpi processes were spawned within a<br>
        single node, i.e. node0002.<br>
<br>
        However, if I omit &quot;-machinefile $TMPDIR/machines&quot; in the line<br>
        mpirun, i.e.<br>
        /opt/openmpi-gcc/bin/mpirun -v -np $NSLOTS<br>
        ./bin/goto-openmpi-gcc/xhpl<br>
<br>
        The mpi processes can start correctly, 4 processes in node0001<br>
        and 4 processes in node0002.<br>
<br>
        Is this normal behaviour of Open MPI?<br>
<br>
<br>
    I just tried it both ways and I got the same result both times.  The<br>
    processes are split between the nodes.  Perhaps to be extra sure,<br>
    you can just run hostname?  And for what it is worth, as you have<br>
    seen, you do not need to specify a machines file.  Open MPI will use<br>
    the ones that were allocated by SGE.  You can also change your<br>
    parallel queue to not run any scripts.  Like this:<br>
<br>
    start_proc_args    /bin/true<br>
    stop_proc_args     /bin/true<br>
<br>
<br>
<br>
        Also, I wondered if I have IB interface, for example, the<br>
        hostname of IB become node0001-clust and node0002-clust, will<br>
        Open MPI automatically use the IB interface?<br>
<br>
    Yes, it should use the IB interface.<br>
<br>
<br>
        How about if I have 2 IB ports in each node, which IB bonding<br>
        was done, will Open MPI automatically benefit from the double<br>
        bandwidth?<br>
<br>
        Thanks a lot.<br>
<br>
        Best Regards,<br>
        PN<br>
<br>
<br>
        ------------------------------------------------------------------------<br>
<br>
        _______________________________________________<br>
        users mailing list<br></div></div>
        <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<div class="im"><br>
        <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
    -- <br>
    =========================<br></div>
    <a href="mailto:rolf.vandevaart@sun.com" target="_blank">rolf.vandevaart@sun.com</a> &lt;mailto:<a href="mailto:rolf.vandevaart@sun.com" target="_blank">rolf.vandevaart@sun.com</a>&gt;<div class="im"><br>
    781-442-3043<br>
    =========================<br>
    _______________________________________________<br>
    users mailing list<br></div>
    <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<div class="im"><br>
    <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
------------------------------------------------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></blockquote><div><div></div><div class="h5">
<br>
<br>
-- <br>
<br>
=========================<br>
<a href="mailto:rolf.vandevaart@sun.com" target="_blank">rolf.vandevaart@sun.com</a><br>
781-442-3043<br>
=========================<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>
