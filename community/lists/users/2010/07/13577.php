<?
$subject_val = "Re: [OMPI users] OpenMPI how large its buffer size ?";
include("../../include/msg-header.inc");
?>
<!-- received="Sun Jul 11 01:02:23 2010" -->
<!-- isoreceived="20100711050223" -->
<!-- sent="Sat, 10 Jul 2010 23:02:18 -0600" -->
<!-- isosent="20100711050218" -->
<!-- name="Jack Bryan" -->
<!-- email="dtustudy68_at_[hidden]" -->
<!-- subject="Re: [OMPI users] OpenMPI how large its buffer size ?" -->
<!-- id="SNT134-w645689305D8453B0910B1BCBB70_at_phx.gbl" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="4C393D66.8020200_at_oracle.com" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>Subject:</strong> Re: [OMPI users] OpenMPI how large its buffer size ?<br>
<strong>From:</strong> Jack Bryan (<em>dtustudy68_at_[hidden]</em>)<br>
<strong>Date:</strong> 2010-07-11 01:02:18
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13578.php">Eugene Loh: "Re: [OMPI users] OpenMPI how large its buffer size ?"</a>
<li><strong>Previous message:</strong> <a href="13576.php">Eugene Loh: "Re: [OMPI users] OpenMPI how large its buffer size ?"</a>
<li><strong>In reply to:</strong> <a href="13576.php">Eugene Loh: "Re: [OMPI users] OpenMPI how large its buffer size ?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13578.php">Eugene Loh: "Re: [OMPI users] OpenMPI how large its buffer size ?"</a>
<li><strong>Reply:</strong> <a href="13578.php">Eugene Loh: "Re: [OMPI users] OpenMPI how large its buffer size ?"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
Hi, 
<br>
thanks for all your replies. 
<br>
The master node can receive message ( the same size)  from 50 worker nodes. But, it cannot receive message from 51 nodes. It caused &quot;truncate error&quot;. 
<br>
I used the same buffer to get the message in 50 node case. 
<br>
About &quot;&quot;rendezvous&quot; protocol&quot;, what is the meaning of &quot;the sender sends a short portion &quot;?
<br>
What is the &quot;short portion&quot;, is it a small mart of the message of the sender ?This &quot;rendezvous&quot; protocol&quot; can work automatically in background without programmerindicates in his program ? 
<br>
The &quot;acknowledgement &quot; can be generated by the receiver only when thecorresponding mpi_irecv is posted by the receiver ? 
<br>
Any help is appreciated. 
<br>
Jack
<br>
July 10  2010  
<br>
Date: Sat, 10 Jul 2010 20:41:26 -0700
<br>
From: eugene.loh_at_[hidden]
<br>
To: users_at_[hidden]
<br>
Subject: Re: [OMPI users] OpenMPI how large its buffer size ?
<br>
<p><p><p><p><p><p>&nbsp;&nbsp;
<br>
&nbsp;&nbsp;
<br>
<p><p>I hope I understand the question properly.
<br>
<p><p><p>The &quot;truncate error&quot; means that the receive buffer provided by the user
<br>
was too small to receive the designated message.  That's an error in
<br>
the user code.
<br>
<p><p><p>You're asking about some buffering sizes within the MPI
<br>
implementation.  We can talk about that, but it probably first makes
<br>
sense to clarify what MPI is doing.  If a sender posts a large send and
<br>
the receiver has not posted a reply, the MPI implementation is not
<br>
required to move any data.  In particular, most MPI implementations
<br>
will use a &quot;rendezvous&quot; protocol in which the sender sends a short
<br>
portion and then waits for an acknowledgement from the receiver that it
<br>
is ready to receive the message (and knows into which user buffer to
<br>
place the received data).  This protocol is used so that the MPI
<br>
implementation does not have to buffer internally arbitrarily large
<br>
messages.
<br>
<p><p><p>So, if you post a large send but no receive, the MPI implementation is
<br>
probably buffering very little data.  The message won't advance until
<br>
the receive has been posted.  This means that a blocking MPI_Send will
<br>
wait and a nonblocking MPI_Isend will return without having done much.
<br>
<p><p><p>Jack Bryan wrote:
<br>
<p>&nbsp;&nbsp;Hi, 
<br>
&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;
<br>
&nbsp;&nbsp;thanks for the program from Jody. 
<br>
&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;
<br>
&nbsp;&nbsp;David indicated the question that I want to ask. 
<br>
&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;
<br>
&nbsp;&nbsp;But, Jody's approach is ok when the MPI built-in buffer size is
<br>
large enough to hold the 
<br>
&nbsp;&nbsp;message such as 100kB in the buffer. 
<br>
&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;
<br>
&nbsp;&nbsp;In
<br>
asynchronous communication, when the sender posts a mpi_isend, the
<br>
message is put in 
<br>
&nbsp;&nbsp;a
<br>
buffer provided by the MPI. 
<br>
&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;
<br>
&nbsp;&nbsp;At
<br>
this point, the receiver may still not post its corresponding
<br>
mpi_irecv. So, the buffer size is 
<br>
&nbsp;&nbsp;important
<br>
here. 
<br>
&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;
<br>
&nbsp;&nbsp;Without
<br>
knowing the buffer size, I may get &quot; truncate error &quot; on Open MPI. 
<br>
&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;
<br>
&nbsp;&nbsp;How to know the size of the buffer automatically
<br>
created by Open MPI in the background ?
<br>
&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;
<br>
&nbsp;&nbsp;Any
<br>
help is appreciated. 
<br>
&nbsp;&nbsp;
<br>
<p>Jack,
<br>
&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;
<br>
&nbsp;&nbsp;July 10 2010
<br>
&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;From: solarbikedz_at_[hidden]
<br>
<p>Date: Sat, 10 Jul 2010 16:46:12 -0700
<br>
<p>To: users_at_[hidden]
<br>
<p>Subject: Re: [OMPI users] OpenMPI how large its buffer size ?
<br>
<p>&nbsp;&nbsp;
<br>
<p>I believe his question is regarding when under non-blocking send/recv,
<br>
how does MPI know how much memory to allocate to receive the message,
<br>
since the size is determined AFTER the irecv is posted.  So if the send
<br>
post isend, but the receiver hasn't post irecv, what would the MPI do
<br>
with the message.
<br>
<p>&nbsp;&nbsp;
<br>
<p>I believe MPI would automatically create a buffer in the background to
<br>
store the message.
<br>
<p>&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;On Sat, Jul 10, 2010 at 1:55 PM, jody &lt;jody.xha_at_[hidden]&gt;
<br>
wrote:
<br>
<p>&nbsp;&nbsp;Perhaps
<br>
i misunderstand your question...
<br>
<p>Generally, it is the user's job to provide the buffers both to send and
<br>
receive.
<br>
<p>If you call MPI_Recv, you must pass a buffer that is large enough to
<br>
<p>hold the data sent by the
<br>
<p>corresponding MPI_Send. I.e., if you know your sender will send
<br>
<p>messages of 100kB,
<br>
<p>then you must provide a buffer of size 100kB to the receiver.
<br>
<p>If the message size is unknown at compile time, you may have to send
<br>
<p>two messages:
<br>
<p>first an integer which tells the receiver how large a buffer it has to
<br>
<p>allocate, and then
<br>
<p>the actual message (which then nicely fits into the freshly allocated
<br>
buffer)
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>#include &lt;stdio.h&gt;
<br>
<p>#include &lt;stdlib.h&gt;
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>#include &lt;time.h&gt;
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>#include &quot;mpi.h&quot;
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>#define SENDER     1
<br>
<p>#define RECEIVER   0
<br>
<p>#define TAG_LEN   77
<br>
<p>#define TAG_DATA  78
<br>
<p>#define MAX_MESSAGE 16
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>int main(int argc, char *argv[]) {
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;&nbsp;int num_procs;
<br>
<p>&nbsp;&nbsp;&nbsp;int rank;
<br>
<p>&nbsp;&nbsp;&nbsp;int *send_buf;
<br>
<p>&nbsp;&nbsp;&nbsp;int *recv_buf;
<br>
<p>&nbsp;&nbsp;&nbsp;int send_message_size;
<br>
<p>&nbsp;&nbsp;&nbsp;int recv_message_size;
<br>
<p>&nbsp;&nbsp;&nbsp;MPI_Status st;
<br>
<p>&nbsp;&nbsp;&nbsp;int i;
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;&nbsp;/* initialize random numbers */
<br>
<p>&nbsp;&nbsp;&nbsp;srand(time(NULL));
<br>
<p>&nbsp;&nbsp;&nbsp;MPI_Init(&amp;argc, &amp;argv);
<br>
<p>&nbsp;&nbsp;&nbsp;MPI_Comm_size(MPI_COMM_WORLD, &amp;num_procs);
<br>
<p>&nbsp;&nbsp;&nbsp;MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;&nbsp;if (rank == RECEIVER) {
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* the receiver */
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* wait for message length */
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MPI_Recv(&amp;recv_message_size, 1, MPI_INT, SENDER, TAG_LEN,
<br>
<p>MPI_COMM_WORLD, &amp;st);
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* create a buffer of the required size */
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;recv_buf = (int*) malloc(recv_message_size*sizeof(int));
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* get data */
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MPI_Recv(recv_buf, recv_message_size, MPI_INT, SENDER,
<br>
<p>TAG_DATA, MPI_COMM_WORLD, &amp;st);
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf(&quot;Receiver got %d integers:&quot;, recv_message_size);
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for (i = 0; i &lt; recv_message_size; i++) {
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf(&quot; %d&quot;, recv_buf[i]);
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf(&quot;\n&quot;);
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* clean up */
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;free(recv_buf);
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;&nbsp;} else if (rank == SENDER) {
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* the sender */
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* random message size */
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;send_message_size =
<br>
(int)((1.0*MAX_MESSAGE*rand())/(1.0*RAND_MAX));
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* create a buffer of the required size */
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;send_buf = (int*) malloc(send_message_size*sizeof(int));
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* create random message */
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for (i = 0; i &lt; send_message_size; i++) {
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;send_buf[i] = rand();
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf(&quot;Sender has %d integers:&quot;, send_message_size);
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for (i = 0; i &lt; send_message_size; i++) {
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf(&quot; %d&quot;, send_buf[i]);
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf(&quot;\n&quot;);
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* send message size to receiver */
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MPI_Send(&amp;send_message_size,  1, MPI_INT, RECEIVER, TAG_LEN,
<br>
<p>MPI_COMM_WORLD);
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* now send messagge */
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MPI_Send(send_buf, send_message_size, MPI_INT, RECEIVER,
<br>
<p>TAG_DATA, MPI_COMM_WORLD);
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* clean up */
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;free(send_buf);
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;&nbsp;}
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;&nbsp;MPI_Finalize();
<br>
<p>}
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>I hope this helps
<br>
<p>&nbsp;Jody
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
<p>On Sat, Jul 10, 2010 at 7:12 AM, Jack Bryan &lt;dtustudy68_at_[hidden]&gt;
<br>
wrote:
<br>
<p><span class="quotelev1">&gt; Dear All:
</span><br>
<p><span class="quotelev1">&gt; How to find the buffer size of OpenMPI ?
</span><br>
<p><span class="quotelev1">&gt; I need to transfer large data between nodes on a cluster with
</span><br>
OpenMPI 1.3.4.
<br>
<p><span class="quotelev1">&gt; Many nodes need to send data to the same node .
</span><br>
<p><span class="quotelev1">&gt; Workers use mpi_isend, the receiver node use  mpi_irecv.
</span><br>
<p><span class="quotelev1">&gt; because they are non-blocking, the messages are stored in buffers
</span><br>
of
<br>
<p><span class="quotelev1">&gt; senders.
</span><br>
<p><span class="quotelev1">&gt; And then, the receiver collect messages from its buffer.
</span><br>
<p><span class="quotelev1">&gt; If the receiver's buffer is too small, there will be truncate
</span><br>
error.
<br>
<p><span class="quotelev1">&gt; Any help is appreciated.
</span><br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;
<br>
&nbsp;&nbsp;
<br>
&nbsp;&nbsp;
<br>
&nbsp;&nbsp;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<br>
_________________________________________________________________
<br>
Hotmail has tools for the New Busy. Search, chat and e-mail from your inbox.
<br>
<a href="http://www.windowslive.com/campaign/thenewbusy?ocid=PID28326::T:WLMTAGL:ON:WL:en-US:WM_HMP:042010_1">http://www.windowslive.com/campaign/thenewbusy?ocid=PID28326::T:WLMTAGL:ON:WL:en-US:WM_HMP:042010_1</a>
<br>
<hr>
<ul>
<li>text/html attachment: <a href="http://www.open-mpi.org/community/lists/users/att-13577/attachment">attachment</a>
</ul>
<!-- attachment="attachment" -->
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="13578.php">Eugene Loh: "Re: [OMPI users] OpenMPI how large its buffer size ?"</a>
<li><strong>Previous message:</strong> <a href="13576.php">Eugene Loh: "Re: [OMPI users] OpenMPI how large its buffer size ?"</a>
<li><strong>In reply to:</strong> <a href="13576.php">Eugene Loh: "Re: [OMPI users] OpenMPI how large its buffer size ?"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="13578.php">Eugene Loh: "Re: [OMPI users] OpenMPI how large its buffer size ?"</a>
<li><strong>Reply:</strong> <a href="13578.php">Eugene Loh: "Re: [OMPI users] OpenMPI how large its buffer size ?"</a>
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
