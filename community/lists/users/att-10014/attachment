Try adjusting this:<br><font face="sans-serif" size="2">oob_tcp_peer_retries = 10</font><br><br>to be<br><br><font face="sans-serif" size="2">oob_tcp_peer_retries = 10</font>00<br><br>It should have given you an error if this failed, but let&#39;s give it a try anyway.<br>
<br>You might also check to see if you are hitting memory limitations. If so, or if you just want to try anyway, try reducing the value of <font face="sans-serif" size="2">coll_sync_barrier_before.<br><br>Ralph<br><br></font><br>
<div class="gmail_quote">On Mon, Jul 20, 2009 at 9:17 AM, Steven Dale <span dir="ltr">&lt;<a href="mailto:steven_dale@hc-sc.gc.ca">steven_dale@hc-sc.gc.ca</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">

<br><font face="sans-serif" size="2">Okay, now the plot is just getting weirder.</font>
<br>
<br><font face="sans-serif" size="2">I implemented most of the changes you
recommend below. We are not running panasas, and our network is GB ethernet
only, so I left the openib parameters out as well. I also recompiled with
the switches suggested in the tlcc directory for the non-panasas file.</font>
<br>
<br><font face="sans-serif" size="2">Now our test case will run on 10 nodes
with 160 permutations, which is a step forward. It does however still crash
with a routed:binomial error on 10 nodes with 1600 permutations after about
14 minutes. With 800 permutations, it runs quite happily as well.</font>
<br>
<br><font face="sans-serif" size="2">....current openmpi-mca-param.conf is
now:</font>
<br>
<br><font face="sans-serif" size="2"># $sysconf is a directory on a local
disk, it is likely that changes</font>
<br><font face="sans-serif" size="2"># to this file will need to be propagated
to other nodes.  If $sysconf</font>
<br><font face="sans-serif" size="2"># is a directory that is shared via
a networked filesystem, changes to</font>
<br><font face="sans-serif" size="2"># this file will be visible to all nodes
that share this $sysconf.</font>
<br>
<br><font face="sans-serif" size="2"># The format is straightforward: one
per line, mca_param_name =</font>
<br><font face="sans-serif" size="2"># rvalue.  Quoting is ignored (so
if you use quotes or escape</font>
<br><font face="sans-serif" size="2"># characters, they&#39;ll be included as
part of the value).  For example:</font>
<br>
<br><font face="sans-serif" size="2"># Disable run-time MPI parameter checking</font>
<br><font face="sans-serif" size="2">#   mpi_param_check = 0</font>
<br>
<br><font face="sans-serif" size="2"># Note that the value &quot;~/&quot;
will be expanded to the current user&#39;s home</font>
<br><font face="sans-serif" size="2"># directory.  For example:</font>
<br>
<br><font face="sans-serif" size="2"># Change component loading path</font>
<br><font face="sans-serif" size="2">#   component_path = /usr/local/lib/openmpi:~/my_openmpi_components</font>
<br>
<br><font face="sans-serif" size="2"># See &quot;ompi_info --param all all&quot;
for a full listing of Open MPI MCA</font>
<br><font face="sans-serif" size="2"># parameters available and their default
values.</font>
<br><font face="sans-serif" size="2">orte_abort_timeout = 10</font>
<br><font face="sans-serif" size="2">opal_set_max_sys_limits = 1</font>
<br><font face="sans-serif" size="2">orte_no_session_dirs = /usr,/users,/home,/hcadmin</font>
<br><font face="sans-serif" size="2">orte_tmpdir_base = /tmp</font>
<br><font face="sans-serif" size="2">orte_allocation_required = 1</font>
<br><font face="sans-serif" size="2">coll_sync_priority = 100</font>
<br><font face="sans-serif" size="2">coll_sync_barrier_before = 1000</font>
<br><font face="sans-serif" size="2">coll_hierarch_priority = 90</font>
<br><font face="sans-serif" size="2">oob_tcp_if_include=eth3</font>
<br><font face="sans-serif" size="2">oob_tcp_peer_retries = 10</font>
<br><font face="sans-serif" size="2">oob_tcp_disable_family = IPv6</font>
<br><font face="sans-serif" size="2">oob_tcp_listen_mode = listen_thread</font>
<br><font face="sans-serif" size="2">oob_tcp_sndbuf = 65536</font>
<br><font face="sans-serif" size="2">oob_tcp_rcvbuf = 65536</font>
<br><font face="sans-serif" size="2">btl = sm,tcp,self</font>
<br><font face="sans-serif" size="2">## Setup MPI options</font>
<br><font face="sans-serif" size="2">mpi_show_handle_leaks = 0</font>
<br><font face="sans-serif" size="2">mpi_warn_on_fork = 1</font>
<br>
<br><font face="sans-serif" size="2">Current compilation looks like this:</font>
<br>
<br><font face="sans-serif" size="2">#!/bin/sh</font>
<br>
<br><font face="sans-serif" size="2"># Takes about 20-25 minutes</font>
<br>
<br><font face="sans-serif" size="2">PATH=$PATH:/usr/local/bin:;export PATH</font>
<br><font face="sans-serif" size="2">LDFLAGS=&quot;-m64&quot;</font>
<br><font face="sans-serif" size="2">CFLAGS=&quot;-m64&quot;</font>
<br><font face="sans-serif" size="2">CXXFLAGS=&quot;-m64&quot;</font>
<br><font face="sans-serif" size="2">FCFLAGS=&quot;-m64&quot;</font>
<br><font face="sans-serif" size="2">FFLAGS=&quot;-m64&quot;</font>
<br>
<br><font face="sans-serif" size="2"># Build and install OpenMPI</font>
<br>
<br><font face="sans-serif" size="2">cd openmpi/openmpi-1.3.3</font>
<br>
<br><font face="sans-serif" size="2">sh ./configure --enable-dlopen=no --enable-binaries=yes
--enable-shared=yes --enable-ipv6=no --enable-ft-thread=no --enable-mca-no-build=crs,filem,routed-linear,snapc,pml-dr,pml-crcp2,pml-crcpw,pml-v,pml-example,crcp,pml-cm
--with-slurm=yes --with-io-romio-flags=&quot;--with-file-system=ufs+nfs&quot;
--with-memory-manager=ptmalloc2 --with-wrapper-ldflags=&quot;-m64&quot;
--with-wrapper-cxxflags=&quot;-m64&quot; --with-wrapper-fcflags=&quot;-m64&quot;
--with-wrapper-fflags=&quot;-m64&quot;</font>
<br>
<br><font face="sans-serif" size="2">make</font>
<br><font face="sans-serif" size="2">make install</font>
<br><div class="im">
<br><font face="sans-serif" size="2">____________________<br>
Steve Dale<br>
Senior Platform Analyst<br>
Health Canada<br>
</font>
<br>
<br>
<br>
</div><table width="100%">
<tbody><tr valign="top">
<td width="40%"><div class="im"><font face="sans-serif" size="1"><b>Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</b>
</font>
<br><font face="sans-serif" size="1">Sent by: <a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a></font>
</div><p><font face="sans-serif" size="1">07/17/2009 10:35 AM</font></p><div><div></div><div class="h5">
<table border="1">
<tbody><tr valign="top">
<td bgcolor="white">
<div align="center"><font face="sans-serif" size="1">Please respond to<br>
Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;</font></div></td></tr></tbody></table>
<br>
</div></div></td><td width="59%">
<table width="100%">
<tbody><tr valign="top">
<td>
<div align="right"><font face="sans-serif" size="1">To</font></div>
</td><td><font face="sans-serif" size="1">Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;</font>
</td></tr><tr valign="top">
<td>
<div align="right"><font face="sans-serif" size="1">cc</font></div>
</td><td>
</td></tr><tr valign="top">
<td>
<div align="right"><font face="sans-serif" size="1">Subject</font></div>
</td><td><font face="sans-serif" size="1">Re: [OMPI users] Possible openmpi bug?</font></td></tr></tbody></table>
<br>
<table>
<tbody><tr valign="top">
<td>
</td><td></td></tr></tbody></table>
<br></td></tr></tbody></table><div><div></div><div class="h5">
<br>
<br>
<br><font size="3">Okay, just checking the obvious. :-)</font>
<br>
<br><font size="3">We regularly run with the exact same configuration here
(i.e., slurm + 16cpus/node) without problem on jobs that are both short
and long, so it seems doubtful that it would be an OMPI bug. However, it
is possible as the difference could be due to configuration and/or parameter
settings. We have seen some site-specific problems that are easily resolved
with parameter changes.</font>
<br>
<br><font size="3">You might take a look at our (LANL&#39;s) platform files for
our slurm-based system and see if they help. You will find them in the
tarball at</font>
<br>
<br><font size="3">contrib/platform/lanl/tlcc</font>
<br>
<br><font size="3">Specifically, since you probably aren&#39;t running panasas
(?), look at the optimized-nopanasas and optimized-nopanasas.conf (they
are a pair) files to see how we configure the system for build, and the
mca params we use to execute applications. If you can, I would suggest
giving them a try (adjusting as required for your setup - e.g., you may
want not want the -m64 flags) and see if it resolves the problem.</font>
<br>
<br><font size="3">Ralph</font>
<br>
<br><font size="3">On Jul 17, 2009, at 7:15 AM, Steven Dale wrote:</font>
<br>
<br><font face="sans-serif" size="2"><br>
I think it unlikely that its a time limit thing. Firstly, slurm is set
up with no time limit on jobs, and we get the same behaviour whether or
not slurm is in the picture.</font><font size="3"> </font><font face="sans-serif" size="2"><br>
In addition, we&#39;ve run several other much larger jobs with a greater number
of permutations and they complete fine.</font><font size="3"> <br>
</font><font face="sans-serif" size="2"><br>
This job takes about 5-10 minutes to run. We&#39;ve run jobs that take a week
or more and the indivdual R process can be seen to run for days at a time
and they run fine.</font><font size="3"> <br>
</font><font face="sans-serif" size="2"><br>
In addition, I&#39;d find it hard to believe (although I concede the possibility)
that jobs entirely self-contained within the same box run slower that jobs
which span 2 boxes over the network. (14 cpus vs 17 cpus for example).</font><font size="3">
<br>
<br>
</font><font face="sans-serif" size="2"><br>
____________________<br>
Steve Dale<br>
Senior Platform Analyst<br>
Health Canada<br>
Phone: (613)-948-4910<br>
E-mail: </font><a href="mailto:steven_dale@hc-sc.gc.ca" target="_blank"><font color="blue" face="sans-serif" size="2"><u>steven_dale@hc-sc.gc.ca</u></font></a><font size="3">
<br>
<br>
</font>
<table width="100%">
<tbody><tr valign="top">
<td width="46%"><font face="sans-serif" size="1"><b>Ralph Castain &lt;</b></font><a href="mailto:rhc@open-mpi.org" target="_blank"><font color="blue" face="sans-serif" size="1"><b><u>rhc@open-mpi.org</u></b></font></a><font face="sans-serif" size="1"><b>&gt;</b>
<br>
Sent by: </font><a href="mailto:users-bounces@open-mpi.org" target="_blank"><font color="blue" face="sans-serif" size="1"><u>users-bounces@open-mpi.org</u></font></a>
<p><font face="sans-serif" size="1">07/17/2009 01:13 AM</font><font size="3">
</font>
<br>
</p><table border="4" width="100%">
<tbody><tr valign="top">
<td bgcolor="white" width="100%">
<div align="center"><font face="sans-serif" size="1">Please respond to<br>
Open MPI Users &lt;</font><a href="mailto:users@open-mpi.org" target="_blank"><font color="blue" face="sans-serif" size="1"><u>users@open-mpi.org</u></font></a><font face="sans-serif" size="1">&gt;</font></div></td></tr>
</tbody></table>
<p>
</p></td><td width="53%">
<br>
<table width="100%">
<tbody><tr valign="top">
<td width="16%">
<div align="right"><font face="sans-serif" size="1">To</font></div>
</td><td width="83%"><font face="sans-serif" size="1">Open MPI Users &lt;</font><a href="mailto:users@open-mpi.org" target="_blank"><font color="blue" face="sans-serif" size="1"><u>users@open-mpi.org</u></font></a><font face="sans-serif" size="1">&gt;</font><font size="3">
</font>
</td></tr><tr valign="top">
<td>
<div align="right"><font face="sans-serif" size="1">cc</font></div>
</td><td>
</td></tr><tr valign="top">
<td>
<div align="right"><font face="sans-serif" size="1">Subject</font></div>
</td><td><font face="sans-serif" size="1">Re: [OMPI users] Possible openmpi bug?</font></td></tr></tbody></table>
<br>
<br>
<table width="100%">
<tbody><tr valign="top">
<td width="50%">
</td><td width="50%"></td></tr></tbody></table>
<br></td></tr></tbody></table>
<br><font size="3"><br>
<br>
<br>
&gt;From what I can see, it looks like your job is being terminated - something
is killing mpirun. Is it possible that the job runs slowly enough on 14
or less cpus that it simply isn&#39;t completing within your specified time
limit? <br>
<br>
The lifeline message simply indicates that a process self-aborted because
it lost contact with its local daemon - in this case, mpirun (as that is
always daemon 0) - which means that the daemon was terminated for some
reason. <br>
<br>
<br>
On Jul 16, 2009, at 11:15 AM, Steven Dale wrote: <br>
</font><font face="sans-serif" size="2"><br>
<br>
Here is my situation:</font><font size="3"> </font><font face="sans-serif" size="2"><br>
<br>
2 Dell R900&#39;s with 16 cpus each and 64 GB RAM</font><font size="3"> </font><font face="sans-serif" size="2"><br>
OS: SuSE SLES 10 SP2 patched up to date</font><font size="3"> </font><font face="sans-serif" size="2"><br>
R version 2.9.1</font><font size="3"> </font><font face="sans-serif" size="2"><br>
Rmpi version 0.5-7</font><font size="3"> </font><font face="sans-serif" size="2"><br>
snow version 0.3-3</font><font size="3"> </font><font face="sans-serif" size="2"><br>
maanova library version 1.14.0</font><font size="3"> </font><font face="sans-serif" size="2"><br>
openmpi version 1.3.3</font><font size="3"> </font><font face="sans-serif" size="2"><br>
slurm version 2.0.3</font><font size="3"> </font><font face="sans-serif" size="2"><br>
<br>
With a given set of R code, we get abnormal exits when using 14 or less
cpus. When using 15 or more, the job completes normally. <br>
error is a variation on: <br>
<br>
[pdp-dev-r01:22618] [[15549,1],0] routed:binomial: Connection to lifeline
[[15549,0],0] lost</font><font size="3"> </font><font face="sans-serif" size="2"><br>
<br>
during the array permutations.</font><font size="3"> </font><font face="sans-serif" size="2"><br>
<br>
Increasing the number of permutations above 200 also produces similar results.
<br>
<br>
The R code is executed with a typical command line for 14 cpus being:</font><font size="3">
</font><font face="sans-serif" size="2"><br>
<br>
sbatch -n 14 -i ./Rtest.txt --mail-type=ALL </font><a href="mailto:--mail-user=steven_dale@hc-sc.gc.ca" target="_blank"><font color="blue" face="sans-serif" size="2"><u>--mail-user=steven_dale@hc-sc.gc.ca</u></font></a><font face="sans-serif" size="2">
/usr/local/bin/R --no-save</font><font size="3"> <br>
</font><font face="sans-serif" size="2"><br>
<br>
Config.log, ompi_info, Rscript.txt and slurm outputs are attached. Network
is GB Ethernet copper tcp/ip.</font><font size="3"> <br>
</font><font face="sans-serif" size="2"><br>
<br>
I think this to be an openmpi error/bug due to the routed:binomial message.
This also had the same results with openmpi-1.3.2, R 2.9.0, maanova 1.12
and slurm 2.0.1.</font><font size="3"> <br>
</font><font face="sans-serif" size="2"><br>
<br>
No non-default MCA parameters are set.</font><font size="3"> </font><font face="sans-serif" size="2"><br>
<br>
LD_LIBRARY_PATH=/usr/local/lib.</font><font size="3"> </font><font face="sans-serif" size="2"><br>
<br>
Configuration done with defaults.</font><font size="3"> </font><font face="sans-serif" size="2"><br>
<br>
Any ideas are welcome.</font><font size="3"> <br>
<br>
<br>
</font><font face="sans-serif" size="2"><br>
<br>
____________________<br>
Steve Dale</font><font size="3"><br>
&lt;bugrep.tar.bz2&gt;_______________________________________________<br>
users mailing list</font><font color="blue" size="3"><u><br>
</u></font><a href="mailto:users@open-mpi.org" target="_blank"><font color="blue" size="3"><u>users@open-mpi.org</u></font></a><font color="blue" size="3"><u><br>
</u></font><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank"><font color="blue" size="3"><u>http://www.open-mpi.org/mailman/listinfo.cgi/users</u></font></a><font size="3">
</font><font size="2"><tt><br>
_______________________________________________<br>
users mailing list</tt></font><font color="blue" size="2"><tt><u><br>
</u></tt></font><a href="mailto:users@open-mpi.org" target="_blank"><font color="blue" size="2"><tt><u>users@open-mpi.org</u></tt></font></a><font color="blue" size="2"><tt><u><br>
</u></tt></font><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank"><font color="blue" size="2"><tt><u>http://www.open-mpi.org/mailman/listinfo.cgi/users</u></tt></font></a><font size="3">
<br>
_______________________________________________<br>
users mailing list</font><font color="blue" size="3"><u><br>
</u></font><a href="mailto:users@open-mpi.org" target="_blank"><font color="blue" size="3"><u>users@open-mpi.org</u></font></a><font size="3"><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></font>
<br><font size="2"><tt>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></tt></font>
<br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

