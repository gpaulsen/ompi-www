<div dir="ltr">Look at your ifconfig output and select the Ethernet device (instead of the IPoIB one). Traditionally the name lack any fanciness, most distributions using eth0 as a default.<div><br></div><div>  George.</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Tue, Sep 9, 2014 at 11:24 PM, Muhammad Ansar Javed <span dir="ltr">&lt;<a href="mailto:muhammad.ansar@seecs.edu.pk" target="_blank">muhammad.ansar@seecs.edu.pk</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Hi,<br><br>I am currently conducting some testing on system with Gigabit and InfiniBand interconnects. Both Latency and Bandwidth benchmarks are doing well as expected on InfiniBand interconnects but Ethernet interconnect is achieving very high performance from expectations. Ethernet and InfiniBand both are achieving equivalent performance.<br><br>For some reason, it looks like openmpi (v1.8.1) is using the InfiniBand interconnect rather than the Gigabit or TCP communication is being emulated to use InifiniBand interconnect.  <br><br>Here are Latency and Bandwidth benchmark results.<br>#---------------------------------------------------<br># Benchmarking PingPong<br># processes = 2<br># map-by node<br>#---------------------------------------------------<br><br>Hello, world.  I am 1 on node124<br>Hello, world.  I am 0 on node123<br>Size Latency (usec) Bandwidth (Mbps)<br>1    1.65    4.62<br>2    1.67    9.16<br>4    1.66    18.43<br>8    1.66    36.74<br>16    1.85    66.00<br>32    1.83    133.28<br>64    1.83    266.36<br>128    1.88    519.10<br>256    1.99    982.29<br>512    2.23    1752.37<br>1024    2.58    3026.98<br>2048    3.32    4710.76<br><br>I read some of the FAQs and noted that OpenMPI prefers the faster available interconnect. In an effort to force it to use the gigabit interconnect I ran it as follows<br><br>1. mpirun -np 2 -machinefile machines -map-by node --mca btl tcp --mca btl_tcp_if_include em1 ./latency.ompi <br>2. mpirun -np 2 -machinefile machines -map-by node --mca btl tcp,self,sm --mca btl_tcp_if_include em1 ./latency.ompi<br>3. mpirun -np 2 -machinefile machines -map-by node --mca btl ^openib --mca btl_tcp_if_include em1 ./latency.ompi<br>4. mpirun -np 2 -machinefile machines -map-by node --mca btl ^openib ./latency.ompi<br><br>None of them resulted in a significantly different benchmark output. <br><br>I am using OpenMPI by loading module on clustered environment and don&#39;t have admin access. It is configured for both TCP and OpenIB (confirmed from ompi_info). After trying all above mentioned methods without success I installed OpenMPI v1.8.2 in my home directory and disable openib with following configuration options<br><br>--disable-openib-control-hdr-padding --disable-openib-dynamic-sl --disable-openib-connectx-xrc --disable-openib-udcm --disable-openib-rdmacm  --disable-btl-openib-malloc-alignment  --disable-io-romio --without-openib --without-verbs  <br><br>Now, openib is not enabled (confirmed from ompi_info script) and there is no &quot;openib.so&quot; file in $prefix/lib/openmpi directory as well. Still, above mentioned mpirun commands are getting the same latency and bandwidth as that of InfiniBand.<br><br>I tried mpirun in verbose mode with following command and here is the output<br><br>Command: <br>mpirun -np 2 -machinefile machines -map-by node --mca btl tcp --mca btl_base_verbose 30 --mca btl_tcp_if_include em1 ./latency.ompi <br> <br>Output:<br>[node123.prv.sciama.cluster:88310] mca: base: components_register: registering btl components<br>[node123.prv.sciama.cluster:88310] mca: base: components_register: found loaded component tcp<br>[node123.prv.sciama.cluster:88310] mca: base: components_register: component tcp register function successful<br>[node123.prv.sciama.cluster:88310] mca: base: components_open: opening btl components<br>[node123.prv.sciama.cluster:88310] mca: base: components_open: found loaded component tcp<br>[node123.prv.sciama.cluster:88310] mca: base: components_open: component tcp open function successful<br>[node124.prv.sciama.cluster:90465] mca: base: components_register: registering btl components<br>[node124.prv.sciama.cluster:90465] mca: base: components_register: found loaded component tcp<br>[node124.prv.sciama.cluster:90465] mca: base: components_register: component tcp register function successful<br>[node124.prv.sciama.cluster:90465] mca: base: components_open: opening btl components<br>[node124.prv.sciama.cluster:90465] mca: base: components_open: found loaded component tcp<br>[node124.prv.sciama.cluster:90465] mca: base: components_open: component tcp open function successful<br>Hello, world.  I am 1 on node124<br>Hello, world.  I am 0 on node123<br>Size Latency(usec) Bandwidth(Mbps)<br>1    4.18    1.83<br>2    3.66    4.17<br>4    4.08    7.48<br>8    3.12    19.57<br>16    3.83    31.84<br>32    3.40    71.84<br>64    4.10    118.97<br>128    3.89    251.19<br>256    4.22    462.77<br>512    2.95    1325.71<br>1024    2.63    2969.49<br>2048    3.38    4628.29<br>[node123.prv.sciama.cluster:88310] mca: base: close: component tcp closed<br>[node123.prv.sciama.cluster:88310] mca: base: close: unloading component tcp<br>[node124.prv.sciama.cluster:90465] mca: base: close: component tcp closed<br>[node124.prv.sciama.cluster:90465] mca: base: close: unloading component tcp<br> <br>Moreover, same benchmark applications using MPICH are working fine on Ethernet and achieving expected Latency and Bandwidth.<br><br>How can this be fixed? <br><br>Thanks for help,<br><br>--Ansar<br clear="all"><br><br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/09/25297.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/09/25297.php</a><br></blockquote></div><br></div>

