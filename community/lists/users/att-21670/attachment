<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">I agree with Gus - check your stack size. This isn't occurring in OMPI itself, so I suspect it is in the system setup.<div><br></div><div><br><div><div>On Apr 3, 2013, at 10:17 AM, Reza Bakhshayeshi &lt;<a href="mailto:reza.b2008@gmail.com">reza.b2008@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr"><div class="gmail_default" style="font-family:tahoma,sans-serif;color:rgb(0,0,102)">Thanks for your answers.<br><br>@Ralph Castain: <br>Do you mean what error I receive?<br></div><div class="gmail_default" style="font-family:tahoma,sans-serif;color:rgb(0,0,102)">

It's the output when I'm running the program:<br><br>&nbsp; *** Process received signal ***<br>&nbsp; Signal: Segmentation fault (11)<br>&nbsp; Signal code: Address not mapped (1)<br>&nbsp; Failing at address: 0x1b7f000<br>&nbsp; [ 0] /lib/x86_64-linux-gnu/libc.so.6(+0x364a0) [0x7f6a84b524a0]<br>

&nbsp; [ 1] hpcc(HPCC_Power2NodesMPIRandomAccessCheck+0xa04) [0x423834]<br>&nbsp; [ 2] hpcc(HPCC_MPIRandomAccess+0x87a) [0x41e43a]<br>&nbsp; [ 3] hpcc(main+0xfbf) [0x40a1bf]<br>&nbsp; [ 4] /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xed) [0x7f6a84b3d76d]<br>

&nbsp; [ 5] hpcc() [0x40aafd]<br>&nbsp; *** End of error message ***<br>[ ][[53938,1],0][../../../../../../ompi/mca/btl/tcp/btl_tcp_frag.c:216:mca_btl_tcp_frag_recv] mca_btl_tcp_frag_recv: readv failed: Connection reset by peer (104)<br>

--------------------------------------------------------------------------<br>mpirun noticed that process rank 1 with PID 4164 on node 192.168.100.6 exited on signal 11 (Segmentation fault).<br>--------------------------------------------------------------------------<br>

<br>@Gus Correa:<br></div><div class="gmail_default" style="font-family:tahoma,sans-serif;color:rgb(0,0,102)">I did it both on server and on instances but it didn't solve the problem.<br></div></div><div class="gmail_extra">

<br><br><div class="gmail_quote">On 3 April 2013 19:14, Gus Correa <span dir="ltr">&lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">

Hi Reza<br>
<br>
Check the system stacksize first ('limit stacksize' or 'ulimit -s').<br>
If it is small, you can try to increase it<br>
before you run the program.<br>
Say (tcsh):<br>
<br>
limit stacksize unlimited<br>
<br>
or (bash):<br>
<br>
ulimit -s unlimited<br>
<br>
I hope this helps,<br>
Gus Correa<div class="im"><br>
<br>
On 04/03/2013 10:29 AM, Ralph Castain wrote:<br>
</div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div class="im">
Could you perhaps share the stacktrace from the segfault? It's<br>
impossible to advise you on the problem without seeing it.<br>
<br>
<br>
On Apr 3, 2013, at 5:28 AM, Reza Bakhshayeshi &lt;<a href="mailto:reza.b2008@gmail.com" target="_blank">reza.b2008@gmail.com</a><br></div>
&lt;mailto:<a href="mailto:reza.b2008@gmail.com" target="_blank">reza.b2008@gmail.com</a>&gt;&gt; wrote:<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div class="h5">
​Hi<br>
​​I have installed HPCC benchmark suite and openmpi on a private cloud<br>
instances.<br>
Unfortunately I get Segmentation fault error mostly when I want to run<br>
it simultaneously on two or more instances with:<br>
mpirun -np 2 --hostfile ./myhosts hpcc<br>
<br>
Everything is on Ubuntu server 12.04 (updated)<br>
and this is my make.intel64 file:<br>
<br>
shell ------------------------------<u></u>------------------------------<u></u>--<br>
# ------------------------------<u></u>------------------------------<u></u>----------<br>
#<br>
SHELL = /bin/sh<br>
#<br>
CD = cd<br>
CP = cp<br>
LN_S = ln -s<br>
MKDIR = mkdir<br>
RM = /bin/rm -f<br>
TOUCH = touch<br>
#<br>
# ------------------------------<u></u>------------------------------<u></u>----------<br>
# - Platform identifier ------------------------------<u></u>------------------<br>
# ------------------------------<u></u>------------------------------<u></u>----------<br>
#<br>
ARCH = intel64<br>
#<br>
# ------------------------------<u></u>------------------------------<u></u>----------<br>
# - HPL Directory Structure / HPL library ------------------------------<br>
# ------------------------------<u></u>------------------------------<u></u>----------<br>
#<br>
TOPdir = ../../..<br>
INCdir = $(TOPdir)/include<br>
BINdir = $(TOPdir)/bin/$(ARCH)<br>
LIBdir = $(TOPdir)/lib/$(ARCH)<br>
#<br>
HPLlib = $(LIBdir)/libhpl.a<br>
#<br>
# ------------------------------<u></u>------------------------------<u></u>----------<br>
# - Message Passing library (MPI) ------------------------------<u></u>--------<br>
# ------------------------------<u></u>------------------------------<u></u>----------<br>
# MPinc tells the C compiler where to find the Message Passing library<br>
# header files, MPlib is defined to be the name of the library to be<br>
# used. The variable MPdir is only used for defining MPinc and MPlib.<br>
#<br>
MPdir = /usr/lib/openmpi<br>
MPinc = -I$(MPdir)/include<br>
MPlib = $(MPdir)/lib/libmpi.so<br>
#<br>
# ------------------------------<u></u>------------------------------<u></u>----------<br>
# - Linear Algebra library (BLAS or VSIPL) -----------------------------<br>
# ------------------------------<u></u>------------------------------<u></u>----------<br>
# LAinc tells the C compiler where to find the Linear Algebra library<br>
# header files, LAlib is defined to be the name of the library to be<br>
# used. The variable LAdir is only used for defining LAinc and LAlib.<br>
#<br>
LAdir = /usr/local/ATLAS/obj64<br>
LAinc = -I$(LAdir)/include<br>
LAlib = $(LAdir)/lib/libcblas.a $(LAdir)/lib/libatlas.a<br>
#<br>
# ------------------------------<u></u>------------------------------<u></u>----------<br>
# - F77 / C interface ------------------------------<u></u>--------------------<br>
# ------------------------------<u></u>------------------------------<u></u>----------<br>
# You can skip this section if and only if you are not planning to use<br>
# a BLAS library featuring a Fortran 77 interface. Otherwise, it is<br>
# necessary to fill out the F2CDEFS variable with the appropriate<br>
# options. **One and only one** option should be chosen in **each** of<br>
# the 3 following categories:<br>
#<br>
# 1) name space (How C calls a Fortran 77 routine)<br>
#<br>
# -DAdd_ : all lower case and a suffixed underscore (Suns,<br>
# Intel, ...), [default]<br>
# -DNoChange : all lower case (IBM RS6000),<br>
# -DUpCase : all upper case (Cray),<br>
# -DAdd__ : the FORTRAN compiler in use is f2c.<br>
#<br>
# 2) C and Fortran 77 integer mapping<br>
#<br>
# -DF77_INTEGER=int : Fortran 77 INTEGER is a C int, [default]<br>
# -DF77_INTEGER=long : Fortran 77 INTEGER is a C long,<br>
# -DF77_INTEGER=short : Fortran 77 INTEGER is a C short.<br>
#<br>
# 3) Fortran 77 string handling<br>
#<br>
# -DStringSunStyle : The string address is passed at the string loca-<br>
# tion on the stack, and the string length is then<br>
# passed as an F77_INTEGER after all explicit<br>
# stack arguments, [default]<br>
# -DStringStructPtr : The address of a structure is passed by a<br>
# Fortran 77 string, and the structure is of the<br>
# form: struct {char *cp; F77_INTEGER len;},<br>
# -DStringStructVal : A structure is passed by value for each Fortran<br>
# 77 string, and the structure is of the form:<br>
# struct {char *cp; F77_INTEGER len;},<br>
# -DStringCrayStyle : Special option for Cray machines, which uses<br>
# Cray fcd (fortran character descriptor) for<br>
# interoperation.<br>
#<br>
F2CDEFS =<br>
#<br>
# ------------------------------<u></u>------------------------------<u></u>----------<br>
# - HPL includes / libraries / specifics ------------------------------<u></u>-<br>
# ------------------------------<u></u>------------------------------<u></u>----------<br>
#<br>
HPL_INCLUDES = -I$(INCdir) -I$(INCdir)/$(ARCH) $(LAinc) $(MPinc)<br>
HPL_LIBS = $(HPLlib) $(LAlib) $(MPlib) -lm<br>
#<br>
# - Compile time options ------------------------------<u></u>-----------------<br>
#<br>
# -DHPL_COPY_L force the copy of the panel L before bcast;<br>
# -DHPL_CALL_CBLAS call the cblas interface;<br>
# -DHPL_CALL_VSIPL call the vsip library;<br>
# -DHPL_DETAILED_TIMING enable detailed timers;<br>
#<br>
# By default HPL will:<br>
# *) not copy L before broadcast,<br>
# *) call the BLAS Fortran 77 interface,<br>
# *) not display detailed timing information.<br>
#<br>
HPL_OPTS = -DHPL_CALL_CBLAS<br>
#<br>
# ------------------------------<u></u>------------------------------<u></u>----------<br>
#<br>
HPL_DEFS = $(F2CDEFS) $(HPL_OPTS) $(HPL_INCLUDES)<br>
#<br>
# ------------------------------<u></u>------------------------------<u></u>----------<br>
# - Compilers / linkers - Optimization flags ---------------------------<br>
# ------------------------------<u></u>------------------------------<u></u>----------<br>
#<br>
CC = /usr/bin/mpicc<br>
CCNOOPT = $(HPL_DEFS)<br>
CCFLAGS = $(HPL_DEFS) -fomit-frame-pointer -O3 -funroll-loops<br>
#CCFLAGS = $(HPL_DEFS)<br>
#<br>
# On some platforms, it is necessary to use the Fortran linker to find<br>
# the Fortran internals used in the BLAS library.<br>
#<br>
LINKER = /usr/bin/mpif90<br>
LINKFLAGS = $(CCFLAGS)<br>
#<br>
ARCHIVER = ar<br>
ARFLAGS = r<br>
RANLIB = echo<br>
#<br>
# ------------------------------<u></u>------------------------------<u></u>----------<br>
<br>
Would you mind please help me figure this problem out?<br>
<br>
Regards,<br>
Reza<br>
______________________________<u></u>_________________<br>
users mailing list<br>
</div></div><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
</blockquote><div class="im">
<br>
<br>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
</div></blockquote><div class="HOEnZb"><div class="h5">
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a></div></div></blockquote></div><br></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
