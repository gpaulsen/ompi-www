Thanks for your answers, <br><br>the execution of this parallel program works fine at my work, but we used MPICH2. I thought this will run with OPEN-MPI too.<br><br>Here is the f90 source where MPI_CART_SHIFT is called :<br>
<br>      program heat<br>!**************************************************************************<br>!<br>!   This program solves the heat equation on the unit square [0,1]x[0,1]<br>!        | du/dt - Delta(u) = 0<br>
!        |  u/gamma = cste<br>!   by implementing a explicit scheme.<br>!   The discretization is done using a 5 point finite difference scheme<br>!   and the domain is decomposed into sub-domains. <br>!   The PDE is discretized using a 5 point finite difference scheme<br>
!   over a (x_dim+2)*(x_dim+2) grid including the end points<br>!   correspond to the boundary points that are stored. <br>!<br>!   The data on the whole domain are stored in<br>!   the following way :<br>!<br>!    y<br>!           ------------------------------------<br>
!    d      |                                  |<br>!    i      |                                  |<br>!    r      |                                  |<br>!    e      |                                  |<br>!    c      |                                  |<br>
!    t      |                                  |<br>!    i      | x20                              |<br>!    o /\   |                                  |<br>!    n  |   | x10                              |<br>!       |   |                                  |<br>
!       |   | x00  x01 x02 ...                 |<br>!       |   ------------------------------------<br>!        -------&gt; x direction  x(*,j)<br>!<br>!   The boundary conditions are stored in the following submatrices<br>
!<br>!<br>!        x(1:x_dim, 0)          ---&gt; left   temperature<br>!        x(1:x_dim, x_dim+1)    ---&gt; right  temperature<br>!        x(0, 1:x_dim)          ---&gt; top    temperature<br>!        x(x_dim+1, 1:x_dim)    ---&gt; bottom temperature<br>
!<br>!**************************************************************************<br>      implicit none<br>      include &#39;mpif.h&#39;<br>! size of the discretization<br>      integer :: x_dim, nb_iter<br>      double precision, allocatable :: x(:,:),b(:,:),x0(:,:)<br>
      double precision  :: dt, h, epsilon<br>      double precision  :: resLoc, result, t, tstart, tend<br>! <br>      integer :: i,j<br>      integer :: step, maxStep<br>      integer :: size_x, size_y, me, x_domains,y_domains<br>
      integer :: iconf(5), size_x_glo<br>      double precision conf(2)<br>!  <br>! MPI variables<br>      integer :: nproc, infompi, comm, comm2d, lda, ndims<br>      INTEGER, DIMENSION(2)  :: dims<br>      LOGICAL, DIMENSION(2)  :: periods<br>
      LOGICAL, PARAMETER     :: reorganisation = .false.<br>      integer :: row_type<br>      integer, parameter :: nbvi=4<br>      integer, parameter :: S=1, E=2, N=3, W=4<br>      integer, dimension(4) :: neighBor<br><br>
!<br>      intrinsic abs<br>!<br>!<br>      call MPI_INIT(infompi)<br>      comm = MPI_COMM_WORLD<br>      call MPI_COMM_SIZE(comm,nproc,infompi)<br>      call MPI_COMM_RANK(comm,me,infompi)<br>!<br>!<br>      if (me.eq.0) then<br>
          call readparam(iconf, conf)<br>      endif<br>      call MPI_BCAST(iconf,5,MPI_INTEGER,0,comm,infompi)<br>      call MPI_BCAST(conf,2,MPI_DOUBLE_PRECISION,0,comm,infompi)<br>!<br>      size_x    = iconf(1)<br>      size_y    = iconf(1)<br>
      x_domains = iconf(3)<br>      y_domains = iconf(4)<br>      maxStep   = iconf(5)<br>      dt        = conf(1)<br>      epsilon   = conf(2)<br>!<br>      size_x_glo = x_domains*size_x+2<br>      h      = 1.0d0/dble(size_x_glo)<br>
      dt     = 0.25*h*h<br>!<br>!<br>      lda = size_y+2<br>      allocate(x(0:size_y+1,0:size_x+1))<br>      allocate(x0(0:size_y+1,0:size_x+1))<br>      allocate(b(0:size_y+1,0:size_x+1))<br>!<br>! Create 2D cartesian grid<br>
      periods(:) = .false.<br><br>      ndims = 2<br>      dims(1)=x_domains<br>      dims(2)=y_domains<br>      CALL MPI_CART_CREATE(MPI_COMM_WORLD, ndims, dims, periods, &amp;<br>        reorganisation,comm2d,infompi)<br>
!<br>! Identify neighbors<br>!<br>      NeighBor(:) = MPI_PROC_NULL<br>! Left/West and right/Est neigbors<br>      CALL MPI_CART_SHIFT(comm2d,0,1,NeighBor(W),NeighBor(E),infompi)<br>! Bottom/South and Upper/North neigbors<br>
      CALL MPI_CART_SHIFT(comm2d,1,1,NeighBor(S),NeighBor(N),infompi)<br>!<br>! Create row data type to coimmunicate with South and North neighbors<br>!<br>      CALL MPI_TYPE_VECTOR(size_x, 1, size_y+2, MPI_DOUBLE_PRECISION, row_type,infompi)<br>
      CALL MPI_TYPE_COMMIT(row_type, infompi)<br>!<br>! initialization <br>!<br>      call initvalues(x0, b, size_x+1, size_x )<br>!<br>! Update the boundaries<br>!<br>      call updateBound(x0,size_x,size_x, NeighBor, comm2d, row_type)<br>
    <br>      step = 0<br>      t    = 0.0<br>!<br>      tstart = MPI_Wtime()<br>! REPEAT<br> 10   continue<br>! <br>         step = step + 1<br>         t    = t + dt<br>! perform one step of the explicit scheme<br>         call Explicit(x0,x,b, size_x+1, size_x, size_x, dt, h, resLoc)<br>
! update the partial solution along the interface<br>         call updateBound(x0,size_x,size_x, NeighBor, comm2d, row_type)<br>! Check the distance between two iterates<br>         call MPI_ALLREDUCE(resLoc,result,1, MPI_DOUBLE_PRECISION, MPI_SUM,comm,infompi)<br>
         result= sqrt(result)<br>!<br>         if (me.eq.0) write(*,1002) t,result<br>!<br>       if ((result.gt.epsilon).and.(step.lt.maxStep)) goto 10<br>!<br>! UNTIL &quot;Convergence&quot;<br>!<br>       tend = MPI_Wtime()<br>
       if (me.eq.0) then<br>         write(*,*)<br>         write(*,*) &#39; Convergence after &#39;, step,&#39; steps &#39;<br>         write(*,*) &#39;      Problem size              &#39;, size_x*x_domains*size_y*y_domains<br>
         write(*,*) &#39; Wall Clock                     &#39;, tend-tstart<br><br>!<br>! Print the solution at each point of the grid<br>!<br>         write(*,*)<br>         write(*,*) &#39; Computed solution &#39;<br>         write(*,*)<br>
         do 30, j=size_x+1,0,-1<br>            write(*,1000)(x0(j,i),i=0,size_x+1)<br> 30      continue<br>       endif<br>!<br>      call MPI_FINALIZE(infompi)<br>!<br>      deallocate(x)<br>      deallocate(x0)<br>      deallocate(b)<br>
!<br>! Formats available to display the computed values on the grid<br>!<br>1000  format(100(1x, f7.3))<br>1001  format(100(1x, e7.3))<br>1002   format(&#39; At time &#39;,E8.2,&#39; Norm &#39;, E8.2)<br><br>!<br>      stop<br>
      end<br>!<br><br>------------------------------------------------------------------------------<br><div class="gmail_quote">2010/7/28 Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span><br>
<blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">According to the error message (especially since it&#39;s consistent across 2 different platforms), it looks like you have an error in your application.  Open MPI says that you&#39;re using an invalid communicator when calling MPI_Cart_shift.  &quot;Invalid&quot; probably means that it&#39;s not a Cartesian communicator.<br>

<br>
You might want to double check the definition and requirements of the MPI_CART_SHIFT function (see the MPI_Cart_shift(3) man page).<br>
<div><div></div><div class="h5"><br>
<br>
<br>
On Jul 28, 2010, at 12:28 PM, christophe petit wrote:<br>
<br>
&gt; hello,<br>
&gt;<br>
&gt; i have a problem concerning the execution of a f90 program (explicitPar) compiled with openmpi-1.4.2. I get nearly the same error on my debian desktop ( AMD Phenom(tm) 9550 Quad-Core Processor) and my mac pro i7 laptop :<br>

&gt;<br>
&gt; on mac pro i7 :<br>
&gt;<br>
&gt; $ mpiexec -np 2 explicitPar<br>
&gt; [macbook-pro-de-fab.livebox.home:48805] *** An error occurred in MPI_Cart_shift<br>
&gt; [macbook-pro-de-fab.livebox.home:48805] *** on communicator MPI_COMM_WORLD<br>
&gt; [macbook-pro-de-fab.livebox.home:48805] *** MPI_ERR_COMM: invalid communicator<br>
&gt; [macbook-pro-de-fab.livebox.home:48805] *** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>
&gt; --------------------------------------------------------------------------<br>
&gt; mpiexec has exited due to process rank 1 with PID 48805 on<br>
&gt; node macbook-pro-de-fab.livebox.home exiting without calling &quot;finalize&quot;. This may<br>
&gt; have caused other processes in the application to be<br>
&gt; terminated by signals sent by mpiexec (as reported here).<br>
&gt;<br>
&gt; ---------------------------------------------------------------------------<br>
&gt;<br>
&gt; on my debian desktop :<br>
&gt;<br>
&gt; mpirun -np 2 explicitPar<br>
&gt; [pablo:11665] *** An error occurred in MPI_Cart_shift<br>
&gt; [pablo:11665] *** on communicator MPI_COMM_WORLD<br>
&gt; [pablo:11665] *** MPI_ERR_COMM: invalid communicator<br>
&gt; [pablo:11665] *** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>
&gt; --------------------------------------------------------------------------<br>
&gt; mpirun has exited due to process rank 1 with PID 11665 on<br>
&gt; node pablo exiting without calling &quot;finalize&quot;. This may<br>
&gt; have caused other processes in the application to be<br>
&gt; terminated by signals sent by mpirun (as reported here).<br>
&gt; --------------------------------------------------------------------------<br>
&gt;<br>
&gt;<br>
&gt; I have installed openmpi-1.4.2 with the following options :<br>
&gt;<br>
&gt; ./configure --prefix=/usr/local/openmpi --enable-mpi-f77 --enable-mpi-f90<br>
&gt;<br>
&gt; with exported variables on bash shell : FC=gfortran F90=gfortran F77=gfortran CC=gcc CXX=g++<br>
&gt;<br>
&gt; The  installation has been completed, the program compiles fine but i don&#39;t understand what&#39;s wrong. I note that with a single processor (&quot;mpirun -np 1 explicitPar&quot;), execution works fine.<br>
&gt;<br>
&gt; My debian desktop is a quad-core, so, theoretically, i can put up to 4 for &quot;np&quot; parameter.<br>
&gt; On my mac pro i7, i don&#39;t know how processors are there, but the &quot;htop&quot; command makes appear 4 cores too.<br>
&gt;<br>
&gt; Anyone has a solution ?<br>
&gt;<br>
&gt; Regards.<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
</div></div><div class="im">&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
</div><font color="#888888">--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to:<br>
<a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
</font><div><div></div><div class="h5"><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

