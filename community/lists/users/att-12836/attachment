Getting the following error if I remove  --mca btl tcp,self option from the mpirun<br><br>kishore@cache-aware[23]; mpirun -np 2 su3imp_base.solaris <br>--------------------------------------------------------------------------<br>
[[16283,1],0]: A high-performance Open MPI point-to-point messaging module<br>was unable to find any relevant network interfaces:<br><br>Module: uDAPL<br>  Host: cache-aware<br><br>Another transport will be used instead, although this may result in<br>
lower performance.<br>--------------------------------------------------------------------------<br>SU3 with improved KS action<br>Microcanonical simulation with refreshing<br>MIMD version 6<br>Machine = <br>R algorithm<br>
type 0 for no prompts  or 1 for prompts<br>nflavors 2<br>nx 30<br>ny 30<br>nz 56<br>nt 84<br>iseed 1234<br>LAYOUT = Hypercubes, options = EVENFIRST,<br>[cache-aware:00758] 1 more process has sent help message help-mpi-btl-base.txt / btl:no-nics<br>
[cache-aware:00758] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>NODE 1: no room for t_longlink<br>Termination: node 1, status = 1<br>NODE 0: no room for t_longlink<br>--------------------------------------------------------------------------<br>
MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD <br>with errorcode 0.<br><br>NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.<br>You may or may not see output from other processes, depending on<br>
exactly when Open MPI kills them.<br>--------------------------------------------------------------------------<br>termination: Wed Apr 28 10:23:32 2010<br><br>Termination: node 0, status = 1<br>--------------------------------------------------------------------------<br>
mpirun has exited due to process rank 0 with PID 759 on<br>node cache-aware exiting without calling &quot;finalize&quot;. This may<br>have caused other processes in the application to be<br>terminated by signals sent by mpirun (as reported here).<br>
<br clear="all">Best,<br>Kishore Kumar Pusukuri<br><a href="http://www.cs.ucr.edu/~kishore">http://www.cs.ucr.edu/~kishore</a><br><br>
<br><br><div class="gmail_quote">On 28 April 2010 06:32, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">
<div><font color="navy" face="Arial" size="2">
I don&#39;t know much about specmpi, but it seems like it is choosing to abort. Maybe the &quot;no room for lattice&quot; has some meaning...?
<br>
<br>-jms
<br>Sent from my PDA.  No type good.</font></div>
<br><div><hr size="2" width="100%" align="center">
<font face="Tahoma" size="2">
<b>From</b>: <a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a> &lt;<a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a>&gt;
<br><b>To</b>: <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;
<br><b>Sent</b>: Wed Apr 28 01:47:01 2010<br><b>Subject</b>: [OMPI users] MPI_ABORT was invoked on rank 0 in communicatorMPI_COMM_WORLD with errorcode 0.
<br></font><br></div><div><div></div><div class="h5">
Hi,<br>I am trying to run SPEC MPI 2007 workload on a quad-core machine. However getting this error message. I also tried to use hostfile option by specifying localhost slots=4, but still getting the following error. Please help me.<br>

<br>$mpirun  --mca btl tcp,sm,self -np 4 su3imp_base.solaris <br>SU3 with improved KS action<br>Microcanonical simulation with refreshing<br>MIMD version 6<br>Machine = <br>R algorithm<br>type 0 for no prompts  or 1 for prompts<br>

nflavors 2<br>nx 30<br>ny 30<br>nz 56<br>nt 84<br>iseed 1234<br>LAYOUT = Hypercubes, options = EVENFIRST,<br>NODE 0: no room for lattice<br>termination: Tue Apr 27 23:41:44 2010<br><br>Termination: node 0, status = 1<br>
--------------------------------------------------------------------------<br>
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD <br>with errorcode 0.<br><br>NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.<br>You may or may not see output from other processes, depending on<br>

exactly when Open MPI kills them.<br>--------------------------------------------------------------------------<br>--------------------------------------------------------------------------<br>mpirun has exited due to process rank 0 with PID 17239 on<br>

node cache-aware exiting without calling &quot;finalize&quot;. This may<br>have caused other processes in the application to be<br>terminated by signals sent by mpirun (as reported here).<br><br><br clear="all">Best,<br>
Kishore Kumar Pusukuri<br>
<a href="http://www.cs.ucr.edu/%7Ekishore" target="_blank">http://www.cs.ucr.edu/~kishore</a><br><br>
</div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

