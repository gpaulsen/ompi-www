I have the latest version installed: 1.1.1227.<div>And the version which initially came with latest mellanox ofed is 1.0.601. And openmpi fails to get built with this version.</div><div><br></div><div>I have certain progress. I managed to launch the application with openmpi 1.6.0.</div>
<div>But from time to time I get errors. The error occur more often as the number of processes grows.</div><div>Here is what I get:</div><div><div>MXM was unable to create an endpoint. Please make sure that the network link is</div>
<div>active on the node and the hardware is functioning. </div><div><br></div><div>  Error: Shared memory error</div><div><br></div><div>--------------------------------------------------------------------------</div><div>
[1345812628.431295] [b23:8172 :0] shm_queue.c:285  MXM ERROR Slave proccess cannot obtain shared memory segment id.</div><div>[1345812628.431322] [b23:8172 :0]    shm_ep.c:133  MXM ERROR Unable to attach endpoint</div><div>
[b23:08172] [[12734,1],10] selected pml ob1, but peer [[12734,1],0] on b23 selected pml cm</div></div><div><br></div><div><br></div><div>And apart from the errors the performance with mxm is pretty disappointing. Out of 6 collectives (Allreduce Reduce Barrier Bcast Allgather Allgatherv) only reduce shows better performance on large messages. At what scale should the situation change? By default mxm starts working on 128 processes. But even on 256 the results with mxm are worse then with usual openib,sm,self. Am I missing something? May be I need to tune something? It&#39;s just that there is pretty little information on the subject except for readme on mellanox web site.</div>
<div><br></div><div>I&#39;ve tried performing the tests on two configurations:</div><div>16 nodes with Intel SB processors and fdr infiniband</div><div>10 nodes with AMD Interlagos and qdr infiniband.</div><div><br></div>
<div>Regards, Pavel Mezentsev.</div><div><br></div><div><br></div><div><br></div><div><br><div class="gmail_quote">2012/8/24 Mike Dubman <span dir="ltr">&lt;<a href="mailto:mike.ompi@gmail.com" target="_blank">mike.ompi@gmail.com</a>&gt;</span><br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div> </div><div>Hi,</div><div>Could you please download latest mxm from <a href="http://www.mellanox.com/products/mxm/" target="_blank">http://www.mellanox.com/products/mxm/</a> and retry?</div>
<div>The mxm version which comes with OFED 1.5.3 was tested with OMPI 1.6.0.</div>

<div> </div><div>Regards</div><div>M<br><br></div><div class="gmail_quote"><div><div class="h5">On Wed, Aug 22, 2012 at 2:22 PM, Pavel Mezentsev <span dir="ltr">&lt;<a href="mailto:pavel.mezentsev@gmail.com" target="_blank">pavel.mezentsev@gmail.com</a>&gt;</span> wrote:<br>


</div></div><blockquote style="margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid" class="gmail_quote"><div><div class="h5">I&#39;ve tried to launch the application on nodes with QDR Infiniband. The first attempt with 2 processes worked, but the following was printed to the output:<div>


<div>[1345633953.436676] [b01:2523 :0]     mpool.c:99   MXM ERROR Invalid mempool parameter(s)</div>
<div>[1345633953.436676] [b01:2522 :0]     mpool.c:99   MXM ERROR Invalid mempool parameter(s)</div><div>--------------------------------------------------------------------------</div><div>MXM was unable to create an endpoint. Please make sure that the network link is</div>



<div>active on the node and the hardware is functioning. </div><div><br></div><div>  Error: Invalid parameter</div><div><br></div><div>--------------------------------------------------------------------------</div><div>


<br>
</div><div>The results from this launch didn&#39;t differ from the results of the launch without MXM.</div><div><br></div><div>Then I&#39;ve tried to launch it with 256 processes, but got the same message from each process and then the application crashed. After that I&#39;m observing the same behavior as with FDR: application hangs in the beginning. </div>



<div><br></div><div>Best regards, Pavel Mezentsev.</div><div><div><div><br></div><br><div class="gmail_quote">2012/8/22 Pavel Mezentsev <span dir="ltr">&lt;<a href="mailto:pavel.mezentsev@gmail.com" target="_blank">pavel.mezentsev@gmail.com</a>&gt;</span><br>



<blockquote style="margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid" class="gmail_quote">Hello!<div><br></div><div>I&#39;ve built openmpi 1.6.1rc3 with support of MXM. But when I try to launch an application using this mtl it hangs and can&#39;t figure out why.</div>



<div><br></div><div>If I launch it with np below 128 then everything works fine since mxm isn&#39;t used. I&#39;ve tried setting the threshold to 0 and launching 2 processes with the same result: hangs on startup.</div>
<div>What could be causing this problem?</div><div><br></div><div>Here is the command I execute:</div><div><div>/opt/openmpi/1.6.1/mxm-test/bin/mpirun \</div><div>                -np $NP \</div><div>                -hostfile hosts_fdr2 \</div>




<div>                --mca mtl mxm \</div><div>                --mca btl ^tcp \</div><div>                --mca mtl_mxm_np 0 \</div><div>                -x OMP_NUM_THREADS=$NT \</div><div>                -x LD_LIBRARY_PATH \</div>




<div>                --bind-to-core \</div><div>                -npernode 16 \</div><div>                --mca coll_fca_np 0 -mca coll_fca_enable 0 \</div><div>                ./IMB-MPI1 -npmin $NP Allreduce Reduce Barrier Bcast Allgather Allgatherv</div>




</div><div><br></div><div>I&#39;m performing the tests on nodes with Intel SB processors and FDR. Openmpi was configured with the following parameters:</div><div>CC=icc CXX=icpc F77=ifort FC=ifort ./configure --prefix=/opt/openmpi/1.6.1rc3/mxm-test --with-mxm=/opt/mellanox/mxm --with-fca=/opt/mellanox/fca --with-knem=/usr/share/knem</div>




<div>I&#39;m using the latest ofed from mellanox: 1.5.3-3.1.0 on centos 6.1 with default kernel: 2.6.32-131.0.15. </div><div>The compilation with default mxm (1.0.601) failed so I installed the latest version from mellanox: 1.1.1227</div>




<div><br></div><div>Best regards, Pavel Mezentsev.</div>
</blockquote></div><br></div></div></div>
<br></div></div>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>

