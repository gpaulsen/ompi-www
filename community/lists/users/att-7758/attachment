Hello , still a bug ???<br><br>compil03% /tmp/openmpi-1.3/bin/mpirun -n 1 --wdir /tmp --host compil03 a.out : -n 1 --host compil02 a.out<br>Hello world from process 0 of 2<br>
Hello world from process 1 of 2<br><br>compil03% mv a.out a.out_32<br>compil03% /tmp/openmpi-1.3/bin/mpirun -n 1 --wdir /tmp --host compil03 a.out_32 : -n 1 --host compil02 a.out<br>HANGS<br><br>Thanks in advance for your expertise<br>
<br>Geoffroy<br><br><br><br><br><br><div class="gmail_quote">2009/1/22  <span dir="ltr">&lt;<a href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a>&gt;</span><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Send users mailing list submissions to<br>
 &nbsp; &nbsp; &nbsp; &nbsp;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<br>
To subscribe or unsubscribe via the World Wide Web, visit<br>
 &nbsp; &nbsp; &nbsp; &nbsp;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
or, via email, send a message with subject or body &#39;help&#39; to<br>
 &nbsp; &nbsp; &nbsp; &nbsp;<a href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a><br>
<br>
You can reach the person managing the list at<br>
 &nbsp; &nbsp; &nbsp; &nbsp;<a href="mailto:users-owner@open-mpi.org">users-owner@open-mpi.org</a><br>
<br>
When replying, please edit your Subject line so it is more specific<br>
than &quot;Re: Contents of users digest...&quot;<br>
<br>
<br>
Today&#39;s Topics:<br>
<br>
 &nbsp; 1. One additional (unwanted) process when using dynamical<br>
 &nbsp; &nbsp; &nbsp;process management (Evgeniy Gromov)<br>
 &nbsp; 2. Re: One additional (unwanted) process when using &nbsp;dynamical<br>
 &nbsp; &nbsp; &nbsp;process management (Ralph Castain)<br>
 &nbsp; 3. Re: One additional (unwanted) process when using &nbsp;dynamical<br>
 &nbsp; &nbsp; &nbsp;process management (Evgeniy Gromov)<br>
 &nbsp; 4. Re: One additional (unwanted) process when &nbsp; &nbsp; &nbsp; &nbsp;using &nbsp; dynamical<br>
 &nbsp; &nbsp; &nbsp;process management (Ralph Castain)<br>
 &nbsp; 5. Re: openmpi 1.3 and --wdir problem (Ralph Castain)<br>
 &nbsp; 6. Re: Problem compiling open mpi 1.3 with sunstudio12 &nbsp; &nbsp; &nbsp; express<br>
 &nbsp; &nbsp; &nbsp;(Jeff Squyres)<br>
 &nbsp; 7. Handling output of processes (jody)<br>
<br>
<br>
----------------------------------------------------------------------<br>
<br>
Message: 1<br>
Date: Wed, 21 Jan 2009 19:02:48 +0100<br>
From: Evgeniy Gromov &lt;<a href="mailto:Evgeniy.Gromov@pci.uni-heidelberg.de">Evgeniy.Gromov@pci.uni-heidelberg.de</a>&gt;<br>
Subject: [OMPI users] One additional (unwanted) process when using<br>
 &nbsp; &nbsp; &nbsp; &nbsp;dynamical &nbsp; &nbsp; &nbsp; process management<br>
To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Message-ID: &lt;<a href="mailto:49776348.8000900@pci.uni-heidelberg.de">49776348.8000900@pci.uni-heidelberg.de</a>&gt;<br>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed<br>
<br>
Dear OpenMPI users,<br>
<br>
I have the following (problem) related to OpenMPI:<br>
I have recently compiled with OPenMPI the new (4-1)<br>
Global Arrays package using ARMCI_NETWORK=MPI-SPAWN,<br>
which implies the use of dynamic process management<br>
realised in MPI2. It got compiled and tested successfully.<br>
However when it is spawning on different nodes (machine) one<br>
additional process on each node appears, i.e. if nodes=2:ppn=2<br>
then on each node there are 3 running processes. In the case<br>
when it runs just on one pc with a few cores (let say nodes=1:ppn=4),<br>
the number of processes exactly equals the number of cpus (ppn)<br>
requested and there is no additional process.<br>
I am wondering whether it is normal behavior. Thanks!<br>
<br>
Best regards,<br>
Evgeniy<br>
<br>
<br>
<br>
<br>
<br>
------------------------------<br>
<br>
Message: 2<br>
Date: Wed, 21 Jan 2009 11:15:00 -0700<br>
From: Ralph Castain &lt;<a href="mailto:rhc@lanl.gov">rhc@lanl.gov</a>&gt;<br>
Subject: Re: [OMPI users] One additional (unwanted) process when using<br>
 &nbsp; &nbsp; &nbsp; &nbsp;dynamical process management<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Message-ID: &lt;<a href="mailto:4CCBD3F8-937F-4F8B-B953-F9CF9DD45EF5@lanl.gov">4CCBD3F8-937F-4F8B-B953-F9CF9DD45EF5@lanl.gov</a>&gt;<br>
Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes<br>
<br>
Not that I&#39;ve seen. What version of OMPI are you using, and on what<br>
type of machine/environment?<br>
<br>
<br>
On Jan 21, 2009, at 11:02 AM, Evgeniy Gromov wrote:<br>
<br>
&gt; Dear OpenMPI users,<br>
&gt;<br>
&gt; I have the following (problem) related to OpenMPI:<br>
&gt; I have recently compiled with OPenMPI the new (4-1)<br>
&gt; Global Arrays package using ARMCI_NETWORK=MPI-SPAWN,<br>
&gt; which implies the use of dynamic process management<br>
&gt; realised in MPI2. It got compiled and tested successfully.<br>
&gt; However when it is spawning on different nodes (machine) one<br>
&gt; additional process on each node appears, i.e. if nodes=2:ppn=2<br>
&gt; then on each node there are 3 running processes. In the case<br>
&gt; when it runs just on one pc with a few cores (let say nodes=1:ppn=4),<br>
&gt; the number of processes exactly equals the number of cpus (ppn)<br>
&gt; requested and there is no additional process.<br>
&gt; I am wondering whether it is normal behavior. Thanks!<br>
&gt;<br>
&gt; Best regards,<br>
&gt; Evgeniy<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
------------------------------<br>
<br>
Message: 3<br>
Date: Wed, 21 Jan 2009 19:30:27 +0100<br>
From: Evgeniy Gromov &lt;<a href="mailto:Evgeniy.Gromov@pci.uni-heidelberg.de">Evgeniy.Gromov@pci.uni-heidelberg.de</a>&gt;<br>
Subject: Re: [OMPI users] One additional (unwanted) process when using<br>
 &nbsp; &nbsp; &nbsp; &nbsp;dynamical process management<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Message-ID: &lt;<a href="mailto:497769C3.8070201@pci.uni-heidelberg.de">497769C3.8070201@pci.uni-heidelberg.de</a>&gt;<br>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed<br>
<br>
Dear Ralph,<br>
<br>
Thanks for your reply.<br>
I encountered this problem using openmpi-1.2.5,<br>
on a Opteron cluster with Myrinet-mx. I tried for<br>
compilation of Global Arrays different compilers<br>
(gfortran, intel, pathscale), the result is the same.<br>
<br>
As I mentioned in the previous message GA itself works<br>
fine, but the application which uses it doesn&#39;t work<br>
correctly if it runs on several nodes. If it runs on<br>
one node with several cores everything is fine. So I<br>
thought that the problem might be in this additional<br>
process.<br>
<br>
Should I try to use the latest 1.3 version of openmpi?<br>
<br>
Best,<br>
Evgeniy<br>
<br>
Ralph Castain wrote:<br>
&gt; Not that I&#39;ve seen. What version of OMPI are you using, and on what type<br>
&gt; of machine/environment?<br>
&gt;<br>
&gt;<br>
&gt; On Jan 21, 2009, at 11:02 AM, Evgeniy Gromov wrote:<br>
&gt;<br>
&gt;&gt; Dear OpenMPI users,<br>
&gt;&gt;<br>
&gt;&gt; I have the following (problem) related to OpenMPI:<br>
&gt;&gt; I have recently compiled with OPenMPI the new (4-1)<br>
&gt;&gt; Global Arrays package using ARMCI_NETWORK=MPI-SPAWN,<br>
&gt;&gt; which implies the use of dynamic process management<br>
&gt;&gt; realised in MPI2. It got compiled and tested successfully.<br>
&gt;&gt; However when it is spawning on different nodes (machine) one<br>
&gt;&gt; additional process on each node appears, i.e. if nodes=2:ppn=2<br>
&gt;&gt; then on each node there are 3 running processes. In the case<br>
&gt;&gt; when it runs just on one pc with a few cores (let say nodes=1:ppn=4),<br>
&gt;&gt; the number of processes exactly equals the number of cpus (ppn)<br>
&gt;&gt; requested and there is no additional process.<br>
&gt;&gt; I am wondering whether it is normal behavior. Thanks!<br>
&gt;&gt;<br>
&gt;&gt; Best regards,<br>
&gt;&gt; Evgeniy<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
<br>
<br>
--<br>
_______________________________________<br>
Dr. Evgeniy Gromov<br>
Theoretische Chemie<br>
Physikalisch-Chemisches Institut<br>
Im Neuenheimer Feld 229<br>
D-69120 Heidelberg<br>
Germany<br>
<br>
Telefon: +49/(0)6221/545263<br>
Fax: +49/(0)6221/545221<br>
E-mail: <a href="mailto:evgeniy@pci.uni-heidelberg.de">evgeniy@pci.uni-heidelberg.de</a><br>
_______________________________________<br>
<br>
<br>
<br>
<br>
<br>
------------------------------<br>
<br>
Message: 4<br>
Date: Wed, 21 Jan 2009 11:38:48 -0700<br>
From: Ralph Castain &lt;<a href="mailto:rhc@lanl.gov">rhc@lanl.gov</a>&gt;<br>
Subject: Re: [OMPI users] One additional (unwanted) process when &nbsp; &nbsp; &nbsp; &nbsp;using<br>
 &nbsp; &nbsp; &nbsp; &nbsp;dynamical process management<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Message-ID: &lt;<a href="mailto:75C59577-D1EA-422B-A0B9-7F1C28E8D4CF@lanl.gov">75C59577-D1EA-422B-A0B9-7F1C28E8D4CF@lanl.gov</a>&gt;<br>
Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes<br>
<br>
If you can, 1.3 would certainly be a good step to take. I&#39;m not sure<br>
why 1.2.5 would be behaving this way, though, so it may indeed be<br>
something in the application (perhaps in the info key being passed to<br>
us?) that is the root cause.<br>
<br>
Still, if it isn&#39;t too much trouble, moving to 1.3 will provide you<br>
with a better platform for dynamic process management regardless.<br>
<br>
Ralph<br>
<br>
<br>
On Jan 21, 2009, at 11:30 AM, Evgeniy Gromov wrote:<br>
<br>
&gt; Dear Ralph,<br>
&gt;<br>
&gt; Thanks for your reply.<br>
&gt; I encountered this problem using openmpi-1.2.5,<br>
&gt; on a Opteron cluster with Myrinet-mx. I tried for<br>
&gt; compilation of Global Arrays different compilers<br>
&gt; (gfortran, intel, pathscale), the result is the same.<br>
&gt;<br>
&gt; As I mentioned in the previous message GA itself works<br>
&gt; fine, but the application which uses it doesn&#39;t work<br>
&gt; correctly if it runs on several nodes. If it runs on<br>
&gt; one node with several cores everything is fine. So I<br>
&gt; thought that the problem might be in this additional<br>
&gt; process.<br>
&gt;<br>
&gt; Should I try to use the latest 1.3 version of openmpi?<br>
&gt;<br>
&gt; Best,<br>
&gt; Evgeniy<br>
&gt;<br>
&gt; Ralph Castain wrote:<br>
&gt;&gt; Not that I&#39;ve seen. What version of OMPI are you using, and on what<br>
&gt;&gt; type of machine/environment?<br>
&gt;&gt; On Jan 21, 2009, at 11:02 AM, Evgeniy Gromov wrote:<br>
&gt;&gt;&gt; Dear OpenMPI users,<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; I have the following (problem) related to OpenMPI:<br>
&gt;&gt;&gt; I have recently compiled with OPenMPI the new (4-1)<br>
&gt;&gt;&gt; Global Arrays package using ARMCI_NETWORK=MPI-SPAWN,<br>
&gt;&gt;&gt; which implies the use of dynamic process management<br>
&gt;&gt;&gt; realised in MPI2. It got compiled and tested successfully.<br>
&gt;&gt;&gt; However when it is spawning on different nodes (machine) one<br>
&gt;&gt;&gt; additional process on each node appears, i.e. if nodes=2:ppn=2<br>
&gt;&gt;&gt; then on each node there are 3 running processes. In the case<br>
&gt;&gt;&gt; when it runs just on one pc with a few cores (let say<br>
&gt;&gt;&gt; nodes=1:ppn=4),<br>
&gt;&gt;&gt; the number of processes exactly equals the number of cpus (ppn)<br>
&gt;&gt;&gt; requested and there is no additional process.<br>
&gt;&gt;&gt; I am wondering whether it is normal behavior. Thanks!<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Best regards,<br>
&gt;&gt;&gt; Evgeniy<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; _______________________________________<br>
&gt; Dr. Evgeniy Gromov<br>
&gt; Theoretische Chemie<br>
&gt; Physikalisch-Chemisches Institut<br>
&gt; Im Neuenheimer Feld 229<br>
&gt; D-69120 Heidelberg<br>
&gt; Germany<br>
&gt;<br>
&gt; Telefon: +49/(0)6221/545263<br>
&gt; Fax: +49/(0)6221/545221<br>
&gt; E-mail: <a href="mailto:evgeniy@pci.uni-heidelberg.de">evgeniy@pci.uni-heidelberg.de</a><br>
&gt; _______________________________________<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
------------------------------<br>
<br>
Message: 5<br>
Date: Wed, 21 Jan 2009 11:40:28 -0700<br>
From: Ralph Castain &lt;<a href="mailto:rhc@lanl.gov">rhc@lanl.gov</a>&gt;<br>
Subject: Re: [OMPI users] openmpi 1.3 and --wdir problem<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Message-ID: &lt;<a href="mailto:B57EA438-1C8A-467C-B791-96EABE6031F4@lanl.gov">B57EA438-1C8A-467C-B791-96EABE6031F4@lanl.gov</a>&gt;<br>
Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes<br>
<br>
This is now fixed in the trunk and will be in the 1.3.1 release.<br>
<br>
Thanks again for the heads-up!<br>
Ralph<br>
<br>
On Jan 21, 2009, at 8:45 AM, Ralph Castain wrote:<br>
<br>
&gt; You are correct - that is a bug in 1.3.0. I&#39;m working on a fix for<br>
&gt; it now and will report back.<br>
&gt;<br>
&gt; Thanks for catching it!<br>
&gt; Ralph<br>
&gt;<br>
&gt;<br>
&gt; On Jan 21, 2009, at 3:22 AM, Geoffroy Pignot wrote:<br>
&gt;<br>
&gt;&gt; Hello<br>
&gt;&gt;<br>
&gt;&gt; &nbsp; I&#39;m currently trying the new release but I cant reproduce the<br>
&gt;&gt; 1.2.8 behaviour<br>
&gt;&gt; &nbsp; concerning --wdir option<br>
&gt;&gt;<br>
&gt;&gt; &nbsp; Then<br>
&gt;&gt; &nbsp; %% /tmp/openmpi-1.2.8/bin/mpirun -n 1 --wdir /tmp --host r003n030<br>
&gt;&gt; pwd : &nbsp; --wdir /scr1 -n 1 --host r003n031 pwd<br>
&gt;&gt; &nbsp; /scr1<br>
&gt;&gt; &nbsp; /tmp<br>
&gt;&gt;<br>
&gt;&gt; &nbsp; but<br>
&gt;&gt; &nbsp; %% /tmp/openmpi-1.3/bin/mpirun -n 1 --wdir /tmp --host r003n030<br>
&gt;&gt; pwd : --wdir &nbsp;/scr1 -n 1 --host r003n031 pwd<br>
&gt;&gt; &nbsp; /scr1<br>
&gt;&gt; &nbsp; /scr1<br>
&gt;&gt; &nbsp; Thanks in advance<br>
&gt;&gt; &nbsp; Regards<br>
&gt;&gt; &nbsp; Geoffroy<br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
------------------------------<br>
<br>
Message: 6<br>
Date: Wed, 21 Jan 2009 14:06:42 -0500<br>
From: Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>
Subject: Re: [OMPI users] Problem compiling open mpi 1.3 with<br>
 &nbsp; &nbsp; &nbsp; &nbsp;sunstudio12 &nbsp; &nbsp; express<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Message-ID: &lt;<a href="mailto:36FCDF58-9138-46A9-A432-CDF2A99A1CD7@cisco.com">36FCDF58-9138-46A9-A432-CDF2A99A1CD7@cisco.com</a>&gt;<br>
Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes<br>
<br>
FWIW, I have run with my LD_LIBRARY_PATH set to a combination of<br>
multiple OMPI installations; it ended up using the leftmost entry in<br>
the LD_LIBRARY_PATH (as I intended). &nbsp;I&#39;m not quite sure why it<br>
wouldn&#39;t do that for you. &nbsp;:-(<br>
<br>
<br>
On Jan 21, 2009, at 4:53 AM, Olivier Marsden wrote:<br>
<br>
&gt;<br>
&gt;&gt;<br>
&gt;&gt; - Check that /opt/mpi_sun and /opt/mpi_gfortran* are actually<br>
&gt;&gt; distinct subdirectories; there&#39;s no hidden sym/hard links in there<br>
&gt;&gt; somewhere (where directories and/or individual files might<br>
&gt;&gt; accidentally be pointing to the other tree)<br>
&gt;&gt;<br>
&gt;<br>
&gt; no hidden links in the directories<br>
&gt;<br>
&gt;&gt; - does &quot;env | grep mpi_&quot; show anything interesting / revealing?<br>
&gt;&gt; What is your LD_LIBRARY_PATH set to?<br>
&gt;&gt;<br>
&gt; Nothing in env | grep mpi, &nbsp;and for the purposes of building,<br>
&gt; LD_LIBRARY_PATH is set to<br>
&gt; /opt/sun/express/sunstudioceres/lib/:/opt/mpi_sun/lib:xxx<br>
&gt; where xxx is, among other things, the other mpi installations.<br>
&gt; This lead me to find a problem, but which seems to be more related<br>
&gt; to my linux configuration than openmpi:<br>
&gt; I tried redefining ld_library_path to point just to sun, and<br>
&gt; everything works correctly.<br>
&gt; Putting my previous paths back into the variable leads to erroneous<br>
&gt; behaviour, with ldd indicating that mpif90<br>
&gt; is linked to libraries in the gfortran tree.<br>
&gt; I thought that ld looked for libraries in folders in the order that<br>
&gt; the folders are given in ld_library_path, and so<br>
&gt; having mpi_sun as the first folder would suffice for its libraries<br>
&gt; to be used; is that where I was wrong?<br>
&gt; Sorry for the trouble, in any case redefining the ld_library_path to<br>
&gt; remove all references to other installations works.<br>
&gt; Looks like I&#39;ll have to swot up on my linker configuration knowledge!<br>
&gt; Thanks alot for your time,<br>
&gt;<br>
&gt; Olivier Marsden<br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
Cisco Systems<br>
<br>
<br>
<br>
------------------------------<br>
<br>
Message: 7<br>
Date: Thu, 22 Jan 2009 09:58:22 +0100<br>
From: jody &lt;<a href="mailto:jody.xha@gmail.com">jody.xha@gmail.com</a>&gt;<br>
Subject: [OMPI users] Handling output of processes<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Message-ID:<br>
 &nbsp; &nbsp; &nbsp; &nbsp;&lt;<a href="mailto:9b0da5ce0901220058n6e534224i78a6daf6b0afc209@mail.gmail.com">9b0da5ce0901220058n6e534224i78a6daf6b0afc209@mail.gmail.com</a>&gt;<br>
Content-Type: text/plain; charset=ISO-8859-1<br>
<br>
Hi<br>
I have a small cluster consisting of 9 computers (8x2 CPUs, 1x4 CPUs).<br>
I would like to be able to observe the output of the processes<br>
separately during an mpirun.<br>
<br>
What i currently do is to apply the mpirun to a shell script which<br>
opens a xterm for each process,<br>
which then starts the actual application.<br>
<br>
This works, but is a bit complicated, e.g. finding the window you&#39;re<br>
interested in among 19 others.<br>
<br>
So i was wondering is there a possibility to capture the processes&#39;<br>
outputs separately, so<br>
i can make an application in which i can switch between the different<br>
processor outputs?<br>
I could imagine that could be done by wrapper applications which<br>
redirect the output over a TCP<br>
socket to a server application.<br>
<br>
But perhaps there is an easier way, or something like this alread does exist?<br>
<br>
Thank You<br>
 &nbsp;Jody<br>
<br>
<br>
------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
End of users Digest, Vol 1126, Issue 1<br>
**************************************<br>
</blockquote></div><br>

