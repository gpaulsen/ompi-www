<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><div>Does your application do a lot of printing to stdout/stderr?</div><br><div><div>On Jan 11, 2010, at 8:00 AM, Mathieu Gontier wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite">

<div bgcolor="#ffffff" text="#000000">
Hi all<br>
<br>
I want to migrate my CFD application from MPICH-1.2.4 (ch_p4 device) to
OpenMPI-1.4. Hence, I compared the two libraries compiled with my
application and I noted OpenMPI is less efficient thant MPICH on
ethernet (170min with MPICH against 200min with OpenMPI). So, I wonder
if someone has more information/explanation.<br>
<br>
Here the configure options of OpenMPI:<br>
<br>
<small><i>export FC=gfortran<br>
export F77=$FC<br>
export CC=gcc<br>
export PREFIX=/usr/local/bin/openmpi-1.4<br>
./configure --prefix=$PREFIX --enable-cxx-exceptions --enable-mpi-f77
--enable-mpi-f90 --enable-mpi-cxx --enable-mpi-cxx-seek --enable-dist
--enable-mpi-profile --enable-binaries --enable-cxx-exceptions
--enable-mpi-threads --enable-memchecker --with-pic --with-threads
--with-valgrind --with-libnuma --with-openib<br>
</i></small><br>
Despite my OpenMPI compilation supports OpenIB, I did not specified any
mca/btl options because the machine does not have access to a
Infiniband interconnect. So, I guess tcp, sm and self are used (or at
least something close).<br>
<br>
Thank you for your help.<br>
Mathieu.
</div>


_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></body></html>
