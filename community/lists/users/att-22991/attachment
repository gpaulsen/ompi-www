<html><head><meta http-equiv="Content-Type" content="text/html charset=us-ascii"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">FWIW: I verified that this works fine under a slurm allocation of 2 nodes, each with 12 slots. I filled the node without getting an "oversbuscribed" error message<div><br></div><div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">[rhc@bend001 svn-trunk]$ mpirun -n 3 --bind-to core --cpus-per-proc 4 --report-bindings -hostfile hosts hostname</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">[bend001:24318] MCW rank 0 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]]: [BB/BB/BB/BB/../..][../../../../../..]</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">[bend001:24318] MCW rank 1 bound to socket 0[core 4[hwt 0-1]], socket 0[core 5[hwt 0-1]], socket 1[core 6[hwt 0-1]], socket 1[core 7[hwt 0-1]]: [../../../../BB/BB][BB/BB/../../../..]</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">[bend001:24318] MCW rank 2 bound to socket 1[core 8[hwt 0-1]], socket 1[core 9[hwt 0-1]], socket 1[core 10[hwt 0-1]], socket 1[core 11[hwt 0-1]]: [../../../../../..][../../BB/BB/BB/BB]</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">bend001</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">bend001</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">bend001</div><div><br></div><div>where</div><div><br></div><div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">[rhc@bend001 svn-trunk]$ cat hosts</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">bend001 slots=12</div></div><div><br></div><div>The only way I get the "out of resources" error is if I ask for more processes than I have slots - i.e., I give it the hosts file as shown, but ask for 13 or more processes.</div><div><br></div><div><br></div><div>BTW: note one important issue with cpus-per-proc, as shown above. Because I specified 4 cpus/proc, and my sockets each have 6 cpus, one of my procs wound up being split across the two sockets (2 cores on each). That's about the worst situation you can have.</div><div><br></div><div>So a word of caution: it is up to the user to ensure that the mapping is "good". We just do what you asked us to do.</div><div><br></div><div><br></div><div><div>On Nov 13, 2013, at 8:30 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite">Guess I don't see why modifying the allocation is required - we have mapping options that should support such things. If you specify the total number of procs you want, and cpus-per-proc=4, it should do the same thing I would think. You'd get 2 procs on the 8 slot nodes, 8 on the 32 proc nodes, and up to 6 on the 64 slot nodes (since you specified np=16). So I guess I don't understand the issue.<br><br>Regardless, if NPROCS=8 (and you verified that by printing it out, not just assuming wc -l got that value), then it shouldn't think it is oversubscribed. I'll take a look under a slurm allocation as that is all I can access.<br><br><br>On Nov 13, 2013, at 7:23 PM, <a href="mailto:tmishima@jcity.maeda.co.jp">tmishima@jcity.maeda.co.jp</a> wrote:<br><br><blockquote type="cite"><br><br>Our cluster consists of three types of nodes. They have 8, 32<br>and 64 slots respectively. Since the performance of each core is<br>almost same, mixed use of these nodes is possible.<br><br>Furthremore, in this case, for hybrid application with openmpi+openmp,<br>the modification of hostfile is necesarry as follows:<br><br>#PBS -l nodes=1:ppn=32+4:ppn=8<br>export OMP_NUM_THREADS=4<br>modify $PBS_NODEFILE pbs_hosts # 64 lines are condensed to 16 lines<br>mpirun -hostfile pbs_hosts -np 16 -cpus-per-proc 4 -x OMP_NUM_THREADS<br>Myprog<br><br>That's why I want to do that.<br><br>Of course I know, If I quit mixed use, -npernode is better for this<br>purpose.<br><br>(The script I showed you first is just a simplified one to clarify the<br>problem.)<br><br>tmishima<br><br><br><blockquote type="cite">Why do it the hard way? I'll look at the FAQ because that definitely<br></blockquote>isn't a recommended thing to do - better to use -host to specify the<br>subset, or just specify the desired mapping using all the<br><blockquote type="cite">various mappers we provide.<br><br>On Nov 13, 2013, at 6:39 PM, <a href="mailto:tmishima@jcity.maeda.co.jp">tmishima@jcity.maeda.co.jp</a> wrote:<br><br><blockquote type="cite"><br><br>Sorry for cross-post.<br><br>Nodefile is very simple which consists of 8 lines:<br><br>node08<br>node08<br>node08<br>node08<br>node08<br>node08<br>node08<br>node08<br><br>Therefore, NPROCS=8<br><br>My aim is to modify the allocation as you pointed out. According to<br></blockquote></blockquote>Openmpi<br><blockquote type="cite"><blockquote type="cite">FAQ,<br>proper subset of the hosts allocated to the Torque / PBS Pro job should<br></blockquote></blockquote>be<br><blockquote type="cite"><blockquote type="cite">allowed.<br><br>tmishima<br><br><blockquote type="cite">Please - can you answer my question on script2? What is the value of<br></blockquote>NPROCS?<br><blockquote type="cite"><br>Why would you want to do it this way? Are you planning to modify the<br></blockquote>allocation?? That generally is a bad idea as it can confuse the system<br><blockquote type="cite"><br><br>On Nov 13, 2013, at 5:55 PM, <a href="mailto:tmishima@jcity.maeda.co.jp">tmishima@jcity.maeda.co.jp</a> wrote:<br><br><blockquote type="cite"><br><br>Since what I really want is to run script2 correctly, please let us<br>concentrate script2.<br><br>I'm not an expert of the inside of openmpi. What I can do is just<br>obsabation<br>from the outside. I doubt these lines are strange, especially the<br></blockquote></blockquote></blockquote></blockquote>last<br><blockquote type="cite"><blockquote type="cite">one.<br><blockquote type="cite"><blockquote type="cite"><br>[node08.cluster:26952] mca:rmaps:rr: mapping job [56581,1]<br>[node08.cluster:26952] [[56581,0],0] Starting with 1 nodes in list<br>[node08.cluster:26952] [[56581,0],0] Filtering thru apps<br>[node08.cluster:26952] [[56581,0],0] Retained 1 nodes in list<br>[node08.cluster:26952] [[56581,0],0] Removing node node08 slots 0<br></blockquote></blockquote></blockquote></blockquote>inuse<br><blockquote type="cite"><blockquote type="cite">0<br><blockquote type="cite"><blockquote type="cite"><br>These lines come from this part of orte_rmaps_base_get_target_nodes<br>in rmaps_base_support_fns.c:<br><br> &nbsp;&nbsp;&nbsp;&nbsp;} else if (node-&gt;slots &lt;= node-&gt;slots_inuse &amp;&amp;<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(ORTE_MAPPING_NO_OVERSUBSCRIBE &amp;<br>ORTE_GET_MAPPING_DIRECTIVE(policy))) {<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* remove the node as fully used */<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OPAL_OUTPUT_VERBOSE((5,<br>orte_rmaps_base_framework.framework_output,<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"%s Removing node %s slots %d inuse<br></blockquote></blockquote>%d",<br><blockquote type="cite"><blockquote type="cite"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ORTE_NAME_PRINT(ORTE_PROC_MY_NAME),<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;node-&gt;name, node-&gt;slots, node-&gt;<br>slots_inuse));<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;opal_list_remove_item(allocated_nodes, item);<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OBJ_RELEASE(item); &nbsp;/* "un-retain" it */<br><br>I wonder why node-&gt;slots and node-&gt;slots_inuse is 0, which I can read<br>from the above line "Removing node node08 slots 0 inuse 0".<br><br>Or I'm not sure but<br>"else if (node-&gt;slots &lt;= node-&gt;slots_inuse &amp;&amp;" should be<br>"else if (node-&gt;slots &lt; node-&gt;slots_inuse &amp;&amp;" ?<br><br>tmishima<br><br><blockquote type="cite">On Nov 13, 2013, at 4:43 PM, <a href="mailto:tmishima@jcity.maeda.co.jp">tmishima@jcity.maeda.co.jp</a> wrote:<br><br><blockquote type="cite"><br><br>Yes, the node08 has 8 slots but the process I run is also 8.<br><br>#PBS -l nodes=node08:ppn=8<br><br>Therefore, I think it should allow this allocation. Is that right?<br></blockquote><br>Correct<br><br><blockquote type="cite"><br>My question is why scritp1 works and script2 does not. They are<br>almost same.<br><br>#PBS -l nodes=node08:ppn=8<br>export OMP_NUM_THREADS=1<br>cd $PBS_O_WORKDIR<br>cp $PBS_NODEFILE pbs_hosts<br>NPROCS=`wc -l &lt; pbs_hosts`<br><br>#SCRITP1<br>mpirun -report-bindings -bind-to core Myprog<br><br>#SCRIPT2<br>mpirun -machinefile pbs_hosts -np ${NPROCS} -report-bindings<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-bind-to<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">core<br><blockquote type="cite"><br>This version is not only reading the PBS allocation, but also<br></blockquote></blockquote></blockquote></blockquote></blockquote>invoking<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">the hostfile filter on top of it. Different code path. I'll take a<br></blockquote></blockquote></blockquote></blockquote>look<br><blockquote type="cite"><blockquote type="cite">-<br><blockquote type="cite"><blockquote type="cite">it should still match up assuming NPROCS=8. Any<br><blockquote type="cite">possibility that it is a different number? I don't recall, but isn't<br></blockquote>there some extra lines in the nodefile - e.g., comments?<br><blockquote type="cite"><br><br><blockquote type="cite">Myprog<br><br>tmishima<br><br><blockquote type="cite">I guess here's my confusion. If you are using only one node, and<br></blockquote></blockquote></blockquote></blockquote></blockquote>that<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">node has 8 allocated slots, then we will not allow you to run more<br></blockquote></blockquote></blockquote></blockquote>than<br><blockquote type="cite"><blockquote type="cite">8<br><blockquote type="cite"><blockquote type="cite">processes on that node unless you specifically provide<br><blockquote type="cite">the --oversubscribe flag. This is because you are operating in a<br></blockquote></blockquote></blockquote>managed<br><blockquote type="cite"><blockquote type="cite">environment (in this case, under Torque), and so we treat the<br></blockquote></blockquote>allocation as<br><blockquote type="cite"><blockquote type="cite">"mandatory" by default.<br><blockquote type="cite"><br>I suspect that is the issue here, in which case the system is<br></blockquote></blockquote></blockquote></blockquote></blockquote>behaving<br><blockquote type="cite"><blockquote type="cite">as<br><blockquote type="cite"><blockquote type="cite">it should.<br><blockquote type="cite"><br>Is the above accurate?<br><br><br>On Nov 13, 2013, at 4:11 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>wrote:<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br><blockquote type="cite">It has nothing to do with LAMA as you aren't using that mapper.<br><br>How many nodes are in this allocation?<br><br>On Nov 13, 2013, at 4:06 PM, <a href="mailto:tmishima@jcity.maeda.co.jp">tmishima@jcity.maeda.co.jp</a> wrote:<br><br><blockquote type="cite"><br><br>Hi Ralph, this is an additional information.<br><br>Here is the main part of output by adding "-mca<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>rmaps_base_verbose<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">50".<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>[node08.cluster:26952] [[56581,0],0] plm:base:setup_vm<br>[node08.cluster:26952] [[56581,0],0] plm:base:setup_vm creating<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>map<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:26952] [[56581,0],0] plm:base:setup_vm only HNP<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>in<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">allocation<br>[node08.cluster:26952] mca:rmaps: mapping job [56581,1]<br>[node08.cluster:26952] mca:rmaps: creating new map for job<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>[56581,1]<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:26952] mca:rmaps:ppr: job [56581,1] not using<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>ppr<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">mapper<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:26952] [[56581,0],0] rmaps:seq mapping job<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>[56581,1]<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:26952] mca:rmaps:seq: job [56581,1] not using<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>seq<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">mapper<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:26952] mca:rmaps:resilient: cannot perform<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>initial<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">map<br><blockquote type="cite"><blockquote type="cite">of<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">job [56581,1] - no fault groups<br>[node08.cluster:26952] mca:rmaps:mindist: job [56581,1] not<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>using<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">mindist<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">mapper<br>[node08.cluster:26952] mca:rmaps:rr: mapping job [56581,1]<br>[node08.cluster:26952] [[56581,0],0] Starting with 1 nodes in<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>list<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:26952] [[56581,0],0] Filtering thru apps<br>[node08.cluster:26952] [[56581,0],0] Retained 1 nodes in list<br>[node08.cluster:26952] [[56581,0],0] Removing node node08 slots<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>0<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">inuse 0<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>From this result, I guess it's related to oversubscribe.<br>So I added "-oversubscribe" and rerun, then it worked well as<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>show<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">below:<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>[node08.cluster:27019] [[56774,0],0] Starting with 1 nodes in<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>list<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:27019] [[56774,0],0] Filtering thru apps<br>[node08.cluster:27019] [[56774,0],0] Retained 1 nodes in list<br>[node08.cluster:27019] AVAILABLE NODES FOR MAPPING:<br>[node08.cluster:27019] &nbsp;&nbsp;&nbsp;&nbsp;node: node08 daemon: 0<br>[node08.cluster:27019] [[56774,0],0] Starting bookmark at node<br></blockquote></blockquote></blockquote></blockquote></blockquote>node08<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:27019] [[56774,0],0] Starting at node node08<br>[node08.cluster:27019] mca:rmaps:rr: mapping by slot for job<br></blockquote></blockquote></blockquote></blockquote></blockquote>[56774,1]<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">slots 1 num_procs 8<br>[node08.cluster:27019] mca:rmaps:rr:slot working node node08<br>[node08.cluster:27019] mca:rmaps:rr:slot node node08 is full -<br></blockquote></blockquote></blockquote>skipping<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:27019] mca:rmaps:rr:slot job [56774,1] is<br></blockquote></blockquote></blockquote>oversubscribed -<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">performing second pass<br>[node08.cluster:27019] mca:rmaps:rr:slot working node node08<br>[node08.cluster:27019] mca:rmaps:rr:slot adding up to 8 procs to<br></blockquote></blockquote></blockquote></blockquote></blockquote>node<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">node08<br>[node08.cluster:27019] mca:rmaps:base: computing vpids by slot<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>for<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">job<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[56774,1]<br>[node08.cluster:27019] mca:rmaps:base: assigning rank 0 to node<br></blockquote></blockquote></blockquote></blockquote></blockquote>node08<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:27019] mca:rmaps:base: assigning rank 1 to node<br></blockquote></blockquote></blockquote></blockquote></blockquote>node08<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:27019] mca:rmaps:base: assigning rank 2 to node<br></blockquote></blockquote></blockquote></blockquote></blockquote>node08<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:27019] mca:rmaps:base: assigning rank 3 to node<br></blockquote></blockquote></blockquote></blockquote></blockquote>node08<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:27019] mca:rmaps:base: assigning rank 4 to node<br></blockquote></blockquote></blockquote></blockquote></blockquote>node08<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:27019] mca:rmaps:base: assigning rank 5 to node<br></blockquote></blockquote></blockquote></blockquote></blockquote>node08<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:27019] mca:rmaps:base: assigning rank 6 to node<br></blockquote></blockquote></blockquote></blockquote></blockquote>node08<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:27019] mca:rmaps:base: assigning rank 7 to node<br></blockquote></blockquote></blockquote></blockquote></blockquote>node08<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>I think something is wrong with treatment of oversubscription,<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>which<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">might<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">be<br>related to "#3893: LAMA mapper has problems"<br><br>tmishima<br><br><blockquote type="cite">Hmmm...looks like we aren't getting your allocation. Can you<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>rerun<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">and<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">add -mca ras_base_verbose 50?<br><blockquote type="cite"><br>On Nov 12, 2013, at 11:30 PM, <a href="mailto:tmishima@jcity.maeda.co.jp">tmishima@jcity.maeda.co.jp</a> wrote:<br><br><blockquote type="cite"><br><br>Hi Ralph,<br><br>Here is the output of "-mca plm_base_verbose 5".<br><br>[node08.cluster:23573] mca:base:select:( &nbsp;plm) Querying<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>component<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[rsh]<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:23573] [[INVALID],INVALID] plm:rsh_lookup on<br>agent /usr/bin/rsh path NULL<br>[node08.cluster:23573] mca:base:select:( &nbsp;plm) Query of<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>component<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[rsh]<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">set<br><blockquote type="cite"><blockquote type="cite">priority to 10<br>[node08.cluster:23573] mca:base:select:( &nbsp;plm) Querying<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>component<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[slurm]<br><blockquote type="cite"><blockquote type="cite">[node08.cluster:23573] mca:base:select:( &nbsp;plm) Skipping<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>component<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[slurm].<br><blockquote type="cite"><blockquote type="cite">Query failed to return a module<br>[node08.cluster:23573] mca:base:select:( &nbsp;plm) Querying<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>component<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[tm]<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:23573] mca:base:select:( &nbsp;plm) Query of<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>component<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[tm]<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">set<br><blockquote type="cite"><blockquote type="cite">priority to 75<br>[node08.cluster:23573] mca:base:select:( &nbsp;plm) Selected<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>component<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[tm]<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:23573] plm:base:set_hnp_name: initial bias<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>23573<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">nodename<br><blockquote type="cite"><blockquote type="cite">hash 85176670<br>[node08.cluster:23573] plm:base:set_hnp_name: final jobfam<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>59480<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:23573] [[59480,0],0] plm:base:receive start<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>comm<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:23573] [[59480,0],0] plm:base:setup_job<br>[node08.cluster:23573] [[59480,0],0] plm:base:setup_vm<br>[node08.cluster:23573] [[59480,0],0] plm:base:setup_vm<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>creating<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">map<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[node08.cluster:23573] [[59480,0],0] plm:base:setup_vm only<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>HNP<br><blockquote type="cite"><blockquote type="cite">in<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">allocation<br><br></blockquote></blockquote><br></blockquote></blockquote></blockquote><br></blockquote></blockquote><br></blockquote></blockquote><br></blockquote></blockquote>--------------------------------------------------------------------------<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">All nodes which are allocated for this job are already filled.<br><br></blockquote></blockquote><br></blockquote></blockquote></blockquote><br></blockquote></blockquote><br></blockquote></blockquote><br></blockquote></blockquote>--------------------------------------------------------------------------<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>Here, openmpi's configuration is as follows:<br><br>./configure \<br>--prefix=/home/mishima/opt/mpi/openmpi-1.7.4a1-pgi13.10 \<br>--with-tm \<br>--with-verbs \<br>--disable-ipv6 \<br>--disable-vt \<br>--enable-debug \<br>CC=pgcc CFLAGS="-tp k8-64e" \<br>CXX=pgCC CXXFLAGS="-tp k8-64e" \<br>F77=pgfortran FFLAGS="-tp k8-64e" \<br>FC=pgfortran FCFLAGS="-tp k8-64e"<br><br><blockquote type="cite">Hi Ralph,<br><br>Okey, I can help you. Please give me some time to report the<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>output.<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>Tetsuya Mishima<br><br><blockquote type="cite">I can try, but I have no way of testing Torque any more - so<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>all<br><blockquote type="cite"><blockquote type="cite">I<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">can<br><blockquote type="cite"><blockquote type="cite">do<br><blockquote type="cite">is a code review. If you can build --enable-debug and add<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-mca<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">plm_base_verbose 5 to your cmd line, I'd appreciate seeing<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>the<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">output.<br><br><br>On Nov 12, 2013, at 9:58 PM, <a href="mailto:tmishima@jcity.maeda.co.jp">tmishima@jcity.maeda.co.jp</a><br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>wrote:<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br><blockquote type="cite"><br><br>Hi Ralph,<br><br>Thank you for your quick response.<br><br>I'd like to report one more regressive issue about Torque<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>support<br><blockquote type="cite"><blockquote type="cite">of<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">openmpi-1.7.4a1r29646, which might be related to "#3893:<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>LAMA<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">mapper<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">has problems" I reported a few days ago.<br><br>The script below does not work with openmpi-1.7.4a1r29646,<br>although it worked with openmpi-1.7.3 as I told you before.<br><br>#!/bin/sh<br>#PBS -l nodes=node08:ppn=8<br>export OMP_NUM_THREADS=1<br>cd $PBS_O_WORKDIR<br>cp $PBS_NODEFILE pbs_hosts<br>NPROCS=`wc -l &lt; pbs_hosts`<br>mpirun -machinefile pbs_hosts -np ${NPROCS}<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-report-bindings<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">-bind-to<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">core<br><blockquote type="cite"><blockquote type="cite">Myprog<br><br>If I drop "-machinefile pbs_hosts -np ${NPROCS} ", then it<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>works<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">fine.<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">Since this happens without lama request, I guess it's not<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>the<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">problem<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">in lama itself. Anyway, please look into this issue as<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>well.<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br>Regards,<br>Tetsuya Mishima<br><br><blockquote type="cite">Done - thanks!<br><br>On Nov 12, 2013, at 7:35 PM, <a href="mailto:tmishima@jcity.maeda.co.jp">tmishima@jcity.maeda.co.jp</a><br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>wrote:<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><br><blockquote type="cite"><br><br>Dear openmpi developers,<br><br>I got a segmentation fault in traial use of<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>openmpi-1.7.4a1r29646<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">built<br><blockquote type="cite"><blockquote type="cite">by<br><blockquote type="cite"><blockquote type="cite">PGI13.10 as shown below:<br><br>[mishima@manage testbed-openmpi-1.7.3]$ mpirun -np 4<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>-cpus-per-proc<br><blockquote type="cite"><blockquote type="cite">2<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">-report-bindings mPre<br>[manage.cluster:23082] MCW rank 2 bound to socket 0[core<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>4<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[hwt<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">0]],<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">socket<br><blockquote type="cite"><blockquote type="cite">0[core 5[hwt 0]]: [././././B/B][./././././.]<br>[manage.cluster:23082] MCW rank 3 bound to socket 1[core<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>6<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[hwt<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">0]],<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">socket<br><blockquote type="cite"><blockquote type="cite">1[core 7[hwt 0]]: [./././././.][B/B/./././.]<br>[manage.cluster:23082] MCW rank 0 bound to socket 0[core<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>0<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[hwt<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">0]],<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">socket<br><blockquote type="cite"><blockquote type="cite">0[core 1[hwt 0]]: [B/B/./././.][./././././.]<br>[manage.cluster:23082] MCW rank 1 bound to socket 0[core<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>2<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">[hwt<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">0]],<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">socket<br><blockquote type="cite"><blockquote type="cite">0[core 3[hwt 0]]: [././B/B/./.][./././././.]<br>[manage:23082] *** Process received signal ***<br>[manage:23082] Signal: Segmentation fault (11)<br>[manage:23082] Signal code: Address not mapped (1)<br>[manage:23082] Failing at address: 0x34<br>[manage:23082] *** End of error message ***<br>Segmentation fault (core dumped)<br><br>[mishima@manage testbed-openmpi-1.7.3]$ gdb mpirun<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>core.23082<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">GNU gdb (GDB) CentOS (7.0.1-42.el5.centos.1)<br>Copyright (C) 2009 Free Software Foundation, Inc.<br>...<br>Core was generated by `mpirun -np 4 -cpus-per-proc 2<br></blockquote></blockquote></blockquote></blockquote></blockquote>-report-bindings<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">mPre'.<br>Program terminated with signal 11, Segmentation fault.<br>#0 &nbsp;0x00002b5f861c9c4f in recv_connect&gt;&gt;&gt;<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>(mod=0x5f861ca20b00007f,<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">sd=32767,<br><blockquote type="cite"><blockquote type="cite">hdr=0x1ca20b00007fff25) at ./oob_tcp.c:631<br>631 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;peer = OBJ_NEW(mca_oob_tcp_peer_t);<br>(gdb) where<br>#0 &nbsp;0x00002b5f861c9c4f in recv_connect<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>(mod=0x5f861ca20b00007f,<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">sd=32767,<br><blockquote type="cite"><blockquote type="cite">hdr=0x1ca20b00007fff25) at ./oob_tcp.c:631<br>#1 &nbsp;0x00002b5f861ca20b in recv_handler (sd=1778385023,<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>flags=32767,<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">cbdata=0x8eb06a00007fff25) at ./oob_tcp.c:760<br>#2 &nbsp;0x00002b5f848eb06a in<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>event_process_active_single_queue<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">(base=0x5f848eb27000007f, activeq=0x848eb27000007fff)<br>at ./event.c:1366<br>#3 &nbsp;0x00002b5f848eb270 in event_process_active<br></blockquote></blockquote>(base=0x5f848eb84900007f)<br><blockquote type="cite"><blockquote type="cite">at ./event.c:1435<br>#4 &nbsp;0x00002b5f848eb849 in<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>opal_libevent2021_event_base_loop<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">(base=0x4077a000007f, flags=32767) at ./event.c:1645<br>#5 &nbsp;0x00000000004077a0 in orterun (argc=7,<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>argv=0x7fff25bbd4a8)<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">at ./orterun.c:1030<br>#6 &nbsp;0x00000000004067fb in main (argc=7,<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>argv=0x7fff25bbd4a8)<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">at ./main.c:13<br><blockquote type="cite"><blockquote type="cite">(gdb) quit<br><br><br>The line 627 in orte/mca/oob/tcp/oob_tcp.c is apparently<br></blockquote></blockquote></blockquote></blockquote></blockquote>unnecessary,<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">which<br><blockquote type="cite"><blockquote type="cite">causes the segfault.<br><br>624 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/* lookup the corresponding process<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>*/&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 625 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;peer = mca_oob_tcp_peer_lookup(mod, &amp;hdr-&gt;<br></blockquote></blockquote>origin);<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">626 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (NULL == peer) {<br>627 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ui64 = (uint64_t*)(&amp;peer-&gt;name);<br>628 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;opal_output_verbose(OOB_TCP_DEBUG_CONNECT,<br>orte_oob_base_framework.framework_output,<br>629 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"%s<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>mca_oob_tcp_recv_connect:<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">connection from new peer",<br>630 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ORTE_NAME_PRINT<br></blockquote></blockquote></blockquote></blockquote>(ORTE_PROC_MY_NAME));<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">631 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;peer = OBJ_NEW(mca_oob_tcp_peer_t);<br>632 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;peer-&gt;mod = mod;<br>633 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;peer-&gt;name = hdr-&gt;origin;<br>634 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;peer-&gt;state = MCA_OOB_TCP_ACCEPTING;<br>635 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ui64 = (uint64_t*)(&amp;peer-&gt;name);<br>636 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (OPAL_SUCCESS !=<br></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote>opal_hash_table_set_value_uint64<br><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite"><blockquote type="cite">(&amp;mod-&gt;<br><blockquote type="cite"><blockquote type="cite">peers, (*ui64), peer)) {<br>637 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OBJ_RELEASE(peer);<br>638 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return;<br>639 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br><br><br>Please fix this mistake in the next release.<br><br>Regards,<br>Tetsuya Mishima<br><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br></blockquote><br>_______________________________________________<br>users mailing list&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br></blockquote></div><br></div></body></html>
