<font size=2 face="sans-serif">Greetings!</font><br><br><font size=2 face="sans-serif">The following batch script will successfully
demo the use of LSF's task geometry feature using IBM Parallel Environment:</font><br><font size=2 face="Courier New">#BUB -J &quot;task_geometry&quot;</font><br><font size=2 face="Courier New">#BSUB -n 9</font><br><font size=2 face="Courier New">#BSUB -R &quot;span[ptile=3]&quot;</font><br><font size=2 face="Courier New">#BSUB -network &quot;type=sn_single:mode=us&quot;</font><br><font size=2 face="Courier New">#BSUB -R &quot;affinity[core]&quot;</font><br><font size=2 face="Courier New">#BSUB -e &quot;task_geometry.stderr.%J&quot;</font><br><font size=2 face="Courier New">#BSUB -o &quot;task_geometry.stdout.%J&quot;</font><br><font size=2 face="Courier New">#BSUB -q &quot;normal&quot;</font><br><font size=2 face="Courier New">#BSUB -M &quot;800&quot;</font><br><font size=2 face="Courier New">#BSUB -R &quot;rusage[mem=800]&quot;</font><br><font size=2 face="Courier New">#BSUB -x</font><br><br><font size=2 face="Courier New">export LSB_PJL_TASK_GEOMETRY=&quot;{(5)(4,3)(2,1,0)}&quot;</font><br><br><font size=2 face="Courier New">ldd /gpfs/gpfs_stage1/parpia/PE_tests/reporter/bin/reporter_MPI</font><br><br><font size=2 face="Courier New">/gpfs/gpfs_stage1/parpia/PE_tests/reporter/bin/reporter_MPI</font><br><font size=2 face="sans-serif">The </font><font size=2 face="Courier New">reporter_MPI</font><font size=2 face="sans-serif">utility simply reports the hostname and affinitization for each MPI process,
and is what I use to verify that the job is distributed to allocated nodes
and on them with the affinitization expected. &nbsp;Typical output is</font><br><font size=2 face="sans-serif">&nbsp; &nbsp; &nbsp; &nbsp; ,
</font><br><br><font size=2 face="sans-serif">To adapt the above batch script to use
OpenMPI, I modify it to</font><br><font size=2 face="Courier New">#BSUB -J &quot;task_geometry&quot;</font><br><font size=2 face="Courier New">#BSUB -n 9</font><br><font size=2 face="Courier New">#BSUB -R &quot;span[ptile=3]&quot;</font><br><font size=2 face="Courier New">#BSUB -m &quot;p10a30 p10a33 p10a35
p10a55 p10a58&quot;</font><br><font size=2 face="Courier New">#BSUB -R &quot;affinity[core]&quot;</font><br><font size=2 face="Courier New">#BSUB -e &quot;task_geometry.stderr.%J&quot;</font><br><font size=2 face="Courier New">#BSUB -o &quot;task_geometry.stdout.%J&quot;</font><br><font size=2 face="Courier New">#BSUB -q &quot;normal&quot;</font><br><font size=2 face="Courier New">#BSUB -M &quot;800&quot;</font><br><font size=2 face="Courier New">#BSUB -R &quot;rusage[mem=800]&quot;</font><br><font size=2 face="Courier New">#BSUB -x</font><br><br><font size=2 face="Courier New">export PATH=/usr/local/OpenMPI/1.10.2/bin:${PATH}</font><br><font size=2 face="Courier New">export LD_LIBRARY_PATH=/usr/local/OpenMPI/1.10.2/lib:${PATH}</font><br><br><font size=2 face="Courier New">export LSB_PJL_TASK_GEOMETRY=&quot;{(5)(4,3)(2,1,0)}&quot;</font><br><br><font size=2 face="Courier New">echo &quot;=== LSB_DJOB_HOSTFILE ===&quot;</font><br><font size=2 face="Courier New">cat ${LSB_DJOB_HOSTFILE}</font><br><font size=2 face="Courier New">echo &quot;=== LSB_AFFINITY_HOSTFILE
===&quot;</font><br><font size=2 face="Courier New">cat ${LSB_AFFINITY_HOSTFILE}</font><br><font size=2 face="Courier New">echo &quot;=== LSB_DJOB_RANKFILE ===&quot;</font><br><font size=2 face="Courier New">cat ${LSB_DJOB_RANKFILE}</font><br><font size=2 face="Courier New">echo &quot;=========================&quot;</font><br><br><font size=2 face="Courier New">ldd /gpfs/gpfs_stage1/parpia/OpenMPI_tests/reporter/bin/reporter_MPI</font><br><br><font size=2 face="Courier New">mpirun /gpfs/gpfs_stage1/parpia/OpenMPI_tests/reporter/bin/reporter_MPI</font><br><font size=2 face="sans-serif">There are additional lines of scripting
that I have inserted to help with debugging this failing job. &nbsp;Here
are the output files from the job:</font><br><font size=2 face="sans-serif">&nbsp; &nbsp; &nbsp; &nbsp; ,
</font><br><font size=2 face="sans-serif">If I change the last line of the immediately
above job script to</font><br><font size=2 face="sans-serif">&nbsp; &nbsp; &nbsp; &nbsp; </font><font size=2 face="Courier New">mpirun
-bind-to core:overload-allowed /gpfs/gpfs_stage1/parpia/OpenMPI_tests/reporter/bin/reporter_MPI</font><br><font size=2 face="sans-serif">the job runs through, but the host selection
and affinization is completely wrong (you can extract the relevant information
with </font><font size=2 face="Courier New">grep &quot;can be sched&quot;
*.stdout.* | sort -n -k 9</font><font size=2 face="sans-serif">):</font><br><font size=2 face="sans-serif">&nbsp; &nbsp; &nbsp; &nbsp; ,
</font><br><font size=2 face="sans-serif">OpenMPI 1.10.2 was built using this
script:</font><br><font size=2 face="sans-serif">&nbsp; &nbsp; &nbsp; &nbsp; </font><br><font size=2 face="sans-serif">It was installed with</font><br><font size=2 face="Courier New">make install</font><br><font size=2 face="sans-serif">executed from the top if the build tree.
&nbsp;Here</font><br><font size=2 face="sans-serif">&nbsp; &nbsp; &nbsp; &nbsp; </font><br><font size=2 face="sans-serif">is the output of</font><br><font size=2 face="Courier New">ompi_info --all</font><br><br><font size=2 face="sans-serif">Regards,</font><br><br><font size=2 face="sans-serif">Farid Parpia &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;IBM Corporation: 710-2-RF28, 2455 South Road, Poughkeepsie, NY 12601,
USA; Telephone: (845) 433-8420 = Tie Line 293-8420</font><BR>
