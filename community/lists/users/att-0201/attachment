<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta content="text/html;charset=ISO-8859-1" http-equiv="Content-Type">
  <title></title>
</head>
<body bgcolor="#ffffff" text="#000000">
<br>
<br>
<a class="moz-txt-link-abbreviated" href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a> wrote:<br>
<blockquote cite="midmailman.19.1129568404.29467.users@open-mpi.org"
 type="cite">
  <pre wrap="">Send users mailing list submissions to
	<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>

Today's Topics:

   1. Re: Hpl Bench mark and Openmpi rc3 (Jeff Squyres)


----------------------------------------------------------------------

Message: 1
Date: Mon, 17 Oct 2005 10:16:39 -0400
From: Jeff Squyres <a class="moz-txt-link-rfc2396E" href="mailto:jsquyres@open-mpi.org">&lt;jsquyres@open-mpi.org&gt;</a>
Subject: Re: [O-MPI users] Hpl Bench mark and Openmpi rc3
To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a>
Message-ID: <a class="moz-txt-link-rfc2396E" href="mailto:8557a377fe1f131e23274e10e5f6e250@open-mpi.org">&lt;8557a377fe1f131e23274e10e5f6e250@open-mpi.org&gt;</a>
Content-Type: text/plain; charset=US-ASCII; format=flowed

On Oct 13, 2005, at 1:25 AM, Allan Menezes wrote:

  </pre>
  <blockquote type="cite">
    <pre wrap="">   I have a 16 node cluster of x86 machines with FC3 running on the 
head
node. I used a beta version of OSCAR 4.2 for putting together the
cluster. It uses /home/allan as the NFS directory.
    </pre>
  </blockquote>
  <pre wrap=""><!---->
Greetings Allan.  Sorry for the delay in replying -- we were all at an 
Open MPI working meeting last week, and the schedule got a bit hectic.

Your setup sounds find.

  </pre>
  <blockquote type="cite">
    <pre wrap="">I tried Mpich2v1.02p1 and got abench mark of 26GFlops for it approx.
WIth open mpi 1.0RC3 having set the LD_LIBRARY_PATH in .bashrc and the
/opt/openmpi/bin path in .bash_profile in the home directory
    </pre>
  </blockquote>
  <pre wrap=""><!---->
Two quick notes here:

- Open MPI's mpirun supports the "--prefix" option which obviates 
needing to set these variables in your .bashrc (although setting them 
in permanently makes things easier in the long term -- you don't need 
to always specify --prefix).  See the FAQ for more details on the 
--prefix option:

	<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/faq/?category=running#mpirun-prefix">http://www.open-mpi.org/faq/?category=running#mpirun-prefix</a>

- OSCAR makes use of environment modules; it contains setup to 
differentiate between the multiple different MPI implementations that 
OSCAR contains.  You can trivially add a modulefile for Open MPI and 
therefore use the "switcher" command to easily switch between all the 
MPI implementations on your OSCAR cluster (once we hit 1.0, we 
anticipate having an OSCAR package).

  </pre>
  <blockquote type="cite">
    <pre wrap="">I cannnot seeem to get a performance beyond 9 GFlops approximately. 
The block size
for mpich2 was 120 for best results. For open mpi for N = 22000 I have 
to use block sizes of 10 -11 to get a performance of 9GFlops other 
wise for larger block sizes(NB) it's worse. I used the same N=22000 
for mpich2 and have a 16 port Gigabit Netgear ethernet switch with 
Gigabit realtek8169 ethernet cards. Can any one tell me why the 
performance with open mpi is so low compared to mpich2-1.02p1?
    </pre>
  </blockquote>
  <pre wrap=""><!---->
There should clearly not be such a wide disparity in performance here; 
we don't see this kind of difference in our own internal testing.

Can you send the output of "ompi_info --all"?

  </pre>
</blockquote>
<br>
Hi Jeff,<br>
&nbsp;&nbsp; I installed two versions of open mpi slightly different. One on
/opt/openmpi or I would get the gfortran error and the other in
/home/allan/openmpi<br>
However I do not think that is the problem as the path names are
specified in the bahrc and bash_profile files of the /home/allan
directory.<br>
I also log into user allan who is not a superuser.On running the open
mpi with HPL I use the following command line:<br>
a1&gt; mpirun -mca pls_rsh_orted /home/allan/openmpi/bin/orted
-hostfile aa -np 16 ./xhpl <br>
from the directory where xhpl resides such as /homer/open/bench and I
use the -mca command pls_rsh_orted as it otherwise comes up with an
error that it cannot find the ORTED daemon on machines a1, a2 etc. That
is probaly aconfiguration error. However the commands above and the
setup described works fine and there are no errors in the HPL.out file,
except that it is slow.<br>
I use an atlas BLAS library for creating xhpl from hpl.tar.gz. The make
file for hpl uses the atlas libs and the open mpi mpicc compiler for
both compilation and linking. and I have zeroed out the MPI macro paths
in Make.open(that's what I reanmed the hpl makefile) for make arch=open
in hpl directory. Please find attached the ompi_info -all file as
requested. Thank you very much:<br>
Allan<br>
<br>
<br>
</body>
</html>

