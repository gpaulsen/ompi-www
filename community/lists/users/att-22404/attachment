<div dir="ltr">Hi,<div>What OFED vendor and version do you use?</div><div>Regards</div><div>M</div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Tue, Jul 30, 2013 at 8:42 PM, Paul Kapinos <span dir="ltr">&lt;<a href="mailto:kapinos@rz.rwth-aachen.de" target="_blank">kapinos@rz.rwth-aachen.de</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Dear Open MPI experts,<br>
<br>
An user at our cluster has a problem running a kinda of big job:<br>
(- the job using 3024 processes (12 per node, 252 nodes) runs fine)<br>
- the job using 4032 processes (12 per node, 336 nodes) produce the error attached below.<br>
<br>
Well, the <a href="http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages" target="_blank">http://www.open-mpi.org/faq/?<u></u>category=openfabrics#ib-<u></u>locked-pages</a> is well-known one; both recommended tweakables (user limits and registered memory size) are at MAX now, nevertheless someone queue pair could not be created.<br>

<br>
Our blind guess is the number of completion queues is exhausted.<br>
<br>
What happen&#39; when raising the value from standard to max?<br>
What max size of Open MPI jobs have been seen at all?<br>
What max size of Open MPI jobs *using MPI_Alltoallv* have been seen at all?<br>
Is there a way to manage the size/the number of queue pairs? (XRC not availabe)<br>
Is there a way to tell MPI_Alltoallv to use less queue pairs, even when this could lead to slow-down?<br>
<br>
There is a suspicious parameter in the mlx4_core module:<br>
$ modinfo mlx4_core | grep log_num_cq<br>
parm:           log_num_cq:log maximum number of CQs per HCA  (int)<br>
<br>
Is this the tweakable parameter?<br>
What is the default, and max value?<br>
<br>
Any help would be welcome...<br>
<br>
Best,<br>
<br>
Paul Kapinos<br>
<br>
P.S. There should be no connection problen somewhere between the nodes; a test job with 1x process on each node has been ran sucessfully just before starting the actual job, which also ran OK for a while - until calling MPI_Alltoallv.<br>

<br>
<br>
<br>
<br>
<br>
<br>
------------------------------<u></u>------------------------------<u></u>--------------<br>
A process failed to create a queue pair. This usually means either<br>
the device has run out of queue pairs (too many connections) or<br>
there are insufficient resources available to allocate a queue pair<br>
(out of memory). The latter can happen if either 1) insufficient<br>
memory is available, or 2) no more physical memory can be registered<br>
with the device.<br>
<br>
For more information on memory registration see the Open MPI FAQs at:<br>
<a href="http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages" target="_blank">http://www.open-mpi.org/faq/?<u></u>category=openfabrics#ib-<u></u>locked-pages</a><br>
<br>
Local host:             <a href="http://linuxbmc1156.rz.RWTH-Aachen.DE" target="_blank">linuxbmc1156.rz.RWTH-Aachen.DE</a><br>
Local device:           mlx4_0<br>
Queue pair type:        Reliable connected (RC)<br>
------------------------------<u></u>------------------------------<u></u>--------------<br>
[<a href="http://linuxbmc1156.rz.RWTH-Aachen.DE" target="_blank">linuxbmc1156.rz.RWTH-Aachen.<u></u>DE</a>][[3703,1],4021][connect/<u></u>btl_openib_connect_oob.c:867:<u></u>rml_recv_cb] error in endpoint reply start connect<br>

[<a href="http://linuxbmc1156.rz.RWTH-Aachen.DE:9632" target="_blank">linuxbmc1156.rz.RWTH-Aachen.<u></u>DE:9632</a>] *** An error occurred in MPI_Alltoallv<br>
[<a href="http://linuxbmc1156.rz.RWTH-Aachen.DE:9632" target="_blank">linuxbmc1156.rz.RWTH-Aachen.<u></u>DE:9632</a>] *** on communicator MPI_COMM_WORLD<br>
[<a href="http://linuxbmc1156.rz.RWTH-Aachen.DE:9632" target="_blank">linuxbmc1156.rz.RWTH-Aachen.<u></u>DE:9632</a>] *** MPI_ERR_OTHER: known error not in list<br>
[<a href="http://linuxbmc1156.rz.RWTH-Aachen.DE:9632" target="_blank">linuxbmc1156.rz.RWTH-Aachen.<u></u>DE:9632</a>] *** MPI_ERRORS_ARE_FATAL: your MPI job will now abort<br>
[<a href="http://linuxbmc1156.rz.RWTH-Aachen.DE" target="_blank">linuxbmc1156.rz.RWTH-Aachen.<u></u>DE</a>][[3703,1],4024][connect/<u></u>btl_openib_connect_oob.c:867:<u></u>rml_recv_cb] error in endpoint reply start connect<br>

[<a href="http://linuxbmc1156.rz.RWTH-Aachen.DE" target="_blank">linuxbmc1156.rz.RWTH-Aachen.<u></u>DE</a>][[3703,1],4027][connect/<u></u>btl_openib_connect_oob.c:867:<u></u>rml_recv_cb] error in endpoint reply start connect<br>

[<a href="http://linuxbmc0840.rz.RWTH-Aachen.DE" target="_blank">linuxbmc0840.rz.RWTH-Aachen.<u></u>DE</a>][[3703,1],10][connect/btl_<u></u>openib_connect_oob.c:867:rml_<u></u>recv_cb] error in endpoint reply start connect<br>

[<a href="http://linuxbmc0840.rz.RWTH-Aachen.DE" target="_blank">linuxbmc0840.rz.RWTH-Aachen.<u></u>DE</a>][[3703,1],1][connect/btl_<u></u>openib_connect_oob.c:867:rml_<u></u>recv_cb] error in endpoint reply start connect<br>

[<a href="http://linuxbmc0840.rz.RWTH-Aachen.DE:17696" target="_blank">linuxbmc0840.rz.RWTH-Aachen.<u></u>DE:17696</a>] [[3703,0],0]-[[3703,1],10] mca_oob_tcp_msg_recv: readv failed: Connection reset by peer (104)<br>
[<a href="http://linuxbmc0840.rz.RWTH-Aachen.DE:17696" target="_blank">linuxbmc0840.rz.RWTH-Aachen.<u></u>DE:17696</a>] [[3703,0],0]-[[3703,1],8] mca_oob_tcp_msg_recv: readv failed: Connection reset by peer (104)<br>
[<a href="http://linuxbmc0840.rz.RWTH-Aachen.DE:17696" target="_blank">linuxbmc0840.rz.RWTH-Aachen.<u></u>DE:17696</a>] [[3703,0],0]-[[3703,1],9] mca_oob_tcp_msg_recv: readv failed: Connection reset by peer (104)<br>
[<a href="http://linuxbmc0840.rz.RWTH-Aachen.DE:17696" target="_blank">linuxbmc0840.rz.RWTH-Aachen.<u></u>DE:17696</a>] [[3703,0],0]-[[3703,1],1] mca_oob_tcp_msg_recv: readv failed: Connection reset by peer (104)<br>
[<a href="http://linuxbmc0840.rz.RWTH-Aachen.DE:17696" target="_blank">linuxbmc0840.rz.RWTH-Aachen.<u></u>DE:17696</a>] 9 more processes have sent help message help-mpi-btl-openib-cpc-base.<u></u>txt / ibv_create_qp failed<br>

[<a href="http://linuxbmc0840.rz.RWTH-Aachen.DE:17696" target="_blank">linuxbmc0840.rz.RWTH-Aachen.<u></u>DE:17696</a>] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>
[<a href="http://linuxbmc0840.rz.RWTH-Aachen.DE:17696" target="_blank">linuxbmc0840.rz.RWTH-Aachen.<u></u>DE:17696</a>] 3 more processes have sent help message help-mpi-errors.txt / mpi_errors_are_fatal<span class="HOEnZb"><font color="#888888"><br>

<br>
-- <br>
Dipl.-Inform. Paul Kapinos   -   High Performance Computing,<br>
RWTH Aachen University, Center for Computing and Communication<br>
Seffenter Weg 23,  D 52074  Aachen (Germany)<br>
Tel: <a href="tel:%2B49%20241%2F80-24915" value="+492418024915" target="_blank">+49 241/80-24915</a><br>
<br>
</font></span><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>

