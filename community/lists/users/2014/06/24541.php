<?
$subject_val = "Re: [OMPI users] intermittent segfaults with openib on ring_c.c";
include("../../include/msg-header.inc");
?>
<!-- received="Wed Jun  4 16:52:19 2014" -->
<!-- isoreceived="20140604205219" -->
<!-- sent="Wed, 4 Jun 2014 16:52:03 -0400" -->
<!-- isosent="20140604205203" -->
<!-- name="Fischer, Greg A." -->
<!-- email="fischega_at_[hidden]" -->
<!-- subject="Re: [OMPI users] intermittent segfaults with openib on ring_c.c" -->
<!-- id="C86C7F6037A63C44BCCD0D77E9D59BFE159ED31677_at_SWEC9985.w-intra.net" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="465792FF-6E60-413E-95C1-2A20F08E4875_at_open-mpi.org" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>Subject:</strong> Re: [OMPI users] intermittent segfaults with openib on ring_c.c<br>
<strong>From:</strong> Fischer, Greg A. (<em>fischega_at_[hidden]</em>)<br>
<strong>Date:</strong> 2014-06-04 16:52:03
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="24542.php">Ralph Castain: "Re: [OMPI users] intermittent segfaults with openib on ring_c.c"</a>
<li><strong>Previous message:</strong> <a href="24540.php">Ralph Castain: "Re: [OMPI users] intermittent segfaults with openib on ring_c.c"</a>
<li><strong>In reply to:</strong> <a href="24540.php">Ralph Castain: "Re: [OMPI users] intermittent segfaults with openib on ring_c.c"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="24542.php">Ralph Castain: "Re: [OMPI users] intermittent segfaults with openib on ring_c.c"</a>
<li><strong>Reply:</strong> <a href="24542.php">Ralph Castain: "Re: [OMPI users] intermittent segfaults with openib on ring_c.c"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
Ralph,
<br>
<p>Thanks for looking. Let me know if there's any other testing that I can do.
<br>
<p>I recompiled with GCC and it works fine, so that lends credence to your theory that it has something to do with the Intel compilers, and possibly their interplay with SUSE.
<br>
<p>Greg
<br>
<p>-----Original Message-----
<br>
From: users [mailto:users-bounces_at_[hidden]] On Behalf Of Ralph Castain
<br>
Sent: Wednesday, June 04, 2014 4:48 PM
<br>
To: Open MPI Users
<br>
Subject: Re: [OMPI users] intermittent segfaults with openib on ring_c.c
<br>
<p>Urrrrrggg...unfortunately, the people who know the most about that code are all at the MPI Forum this week, so we may not be able to fully address it until their return. It looks like you are still going down into that malloc interceptor, so I'm not correctly blocking it for you.
<br>
<p>This run segfaulted in a completely different call in a different part of the startup procedure - but in the same part of the interceptor, which makes me suspicious. Don't know how much testing we've seen on SLES...
<br>
<p><p>On Jun 4, 2014, at 1:18 PM, Fischer, Greg A. &lt;fischega_at_[hidden]&gt; wrote:
<br>
<p><span class="quotelev1">&gt; Ralph,
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; It segfaults. Here's the backtrace:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Core was generated by `ring_c'.
</span><br>
<span class="quotelev1">&gt; Program terminated with signal 11, Segmentation fault.
</span><br>
<span class="quotelev1">&gt; #0  opal_memory_ptmalloc2_int_malloc (av=0x2b82b5300020, bytes=47840385564856) at ../../../../../openmpi-1.8.1/opal/mca/memory/linux/malloc.c:4098
</span><br>
<span class="quotelev1">&gt; 4098          bck-&gt;fd = unsorted_chunks(av);
</span><br>
<span class="quotelev1">&gt; (gdb) bt
</span><br>
<span class="quotelev1">&gt; #0  opal_memory_ptmalloc2_int_malloc (av=0x2b82b5300020,
</span><br>
<span class="quotelev1">&gt; bytes=47840385564856) at
</span><br>
<span class="quotelev1">&gt; ../../../../../openmpi-1.8.1/opal/mca/memory/linux/malloc.c:4098
</span><br>
<span class="quotelev1">&gt; #1  0x00002b82b1a47e38 in opal_memory_ptmalloc2_malloc
</span><br>
<span class="quotelev1">&gt; (bytes=47840385564704) at
</span><br>
<span class="quotelev1">&gt; ../../../../../openmpi-1.8.1/opal/mca/memory/linux/malloc.c:3433
</span><br>
<span class="quotelev1">&gt; #2  0x00002b82b1a47b36 in opal_memory_linux_malloc_hook
</span><br>
<span class="quotelev1">&gt; (sz=47840385564704, caller=0x2b82b53000b8) at
</span><br>
<span class="quotelev1">&gt; ../../../../../openmpi-1.8.1/opal/mca/memory/linux/hooks.c:691
</span><br>
<span class="quotelev1">&gt; #3  0x00002b82b19e7b18 in opal_malloc (size=47840385564704,
</span><br>
<span class="quotelev1">&gt; file=0x2b82b53000b8 &quot;&quot;, line=12) at
</span><br>
<span class="quotelev1">&gt; ../../../openmpi-1.8.1/opal/util/malloc.c:101
</span><br>
<span class="quotelev1">&gt; #4  0x00002b82b199c017 in opal_hash_table_set_value_uint64
</span><br>
<span class="quotelev1">&gt; (ht=0x2b82b5300020, key=47840385564856, value=0xc) at
</span><br>
<span class="quotelev1">&gt; ../../openmpi-1.8.1/opal/class/opal_hash_table.c:283
</span><br>
<span class="quotelev1">&gt; #5  0x00002b82b170e4ca in process_uri (uri=0x2b82b5300020 &quot;\001&quot;) at
</span><br>
<span class="quotelev1">&gt; ../../../../openmpi-1.8.1/orte/mca/oob/base/oob_base_stubs.c:348
</span><br>
<span class="quotelev1">&gt; #6  0x00002b82b170e941 in orte_oob_base_set_addr (fd=-1255145440,
</span><br>
<span class="quotelev1">&gt; args=184, cbdata=0xc) at
</span><br>
<span class="quotelev1">&gt; ../../../../openmpi-1.8.1/orte/mca/oob/base/oob_base_stubs.c:296
</span><br>
<span class="quotelev1">&gt; #7  0x00002b82b19fba1c in event_process_active_single_queue
</span><br>
<span class="quotelev1">&gt; (base=0x655480, activeq=0x654920) at
</span><br>
<span class="quotelev1">&gt; ../../../../../../openmpi-1.8.1/opal/mca/event/libevent2021/libevent/e
</span><br>
<span class="quotelev1">&gt; vent.c:1367
</span><br>
<span class="quotelev1">&gt; #8  0x00002b82b19fbcd9 in event_process_active (base=0x655480) at
</span><br>
<span class="quotelev1">&gt; ../../../../../../openmpi-1.8.1/opal/mca/event/libevent2021/libevent/e
</span><br>
<span class="quotelev1">&gt; vent.c:1437
</span><br>
<span class="quotelev1">&gt; #9  0x00002b82b19fc4c3 in opal_libevent2021_event_base_loop
</span><br>
<span class="quotelev1">&gt; (base=0x655480, flags=1) at
</span><br>
<span class="quotelev1">&gt; ../../../../../../openmpi-1.8.1/opal/mca/event/libevent2021/libevent/e
</span><br>
<span class="quotelev1">&gt; vent.c:1645
</span><br>
<span class="quotelev1">&gt; #10 0x00002b82b16f8763 in orte_progress_thread_engine
</span><br>
<span class="quotelev1">&gt; (obj=0x2b82b5300020) at
</span><br>
<span class="quotelev1">&gt; ../../../../openmpi-1.8.1/orte/mca/ess/base/ess_base_std_app.c:456
</span><br>
<span class="quotelev1">&gt; #11 0x00002b82b0f1c7b6 in start_thread () from /lib64/libpthread.so.0
</span><br>
<span class="quotelev1">&gt; #12 0x00002b82b1410d6d in clone () from /lib64/libc.so.6
</span><br>
<span class="quotelev1">&gt; #13 0x0000000000000000 in ?? ()
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Greg
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; -----Original Message-----
</span><br>
<span class="quotelev1">&gt; From: users [mailto:users-bounces_at_[hidden]] On Behalf Of Ralph
</span><br>
<span class="quotelev1">&gt; Castain
</span><br>
<span class="quotelev1">&gt; Sent: Wednesday, June 04, 2014 3:49 PM
</span><br>
<span class="quotelev1">&gt; To: Open MPI Users
</span><br>
<span class="quotelev1">&gt; Subject: Re: [OMPI users] intermittent segfaults with openib on
</span><br>
<span class="quotelev1">&gt; ring_c.c
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Sorry for delay - digging my way out of the backlog. This is very strange as you are failing in a simple asprintf call. We check that all the players are non-NULL, and it appears that you are failing to allocate the memory for the resulting (rather short) string.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; I'm wondering if this is some strange interaction between SLES, the Intel compiler, and our malloc interceptor - or if there is some difference between the malloc libraries on the two machines. Let's try running it without the malloc interceptor and see if that helps.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Try running with &quot;-mca memory ^linux&quot; on your cmd line
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; On Jun 4, 2014, at 9:58 AM, Ralph Castain &lt;rhc_at_[hidden]&gt; wrote:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev2">&gt;&gt; He isn't getting that far - he's failing in MPI_Init when the RTE
</span><br>
<span class="quotelev2">&gt;&gt; attempts to connect to the local daemon
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; On Jun 4, 2014, at 9:53 AM, Gus Correa &lt;gus_at_[hidden]&gt; wrote:
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Hi Greg
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; From your original email:
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf102:fischega] $ mpirun -np 2 --mca btl openib,self ring_c
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; This may not fix the problem,
</span><br>
<span class="quotelev3">&gt;&gt;&gt; but have you tried to add the shared memory btl to your mca parameter?
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; mpirun -np 2 --mca btl openib,sm,self ring_c
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; As far as I know, sm is the preferred transport layer for intra-node
</span><br>
<span class="quotelev3">&gt;&gt;&gt; communication.
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Gus Correa
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; On 06/04/2014 11:13 AM, Ralph Castain wrote:
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Thanks!! Really appreciate your help - I'll try to figure out what
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; went wrong and get back to you
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; On Jun 4, 2014, at 8:07 AM, Fischer, Greg A.
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; &lt;fischega_at_[hidden] &lt;mailto:fischega_at_[hidden]&gt;&gt; wrote:
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; I re-ran with 1 processor and got more information. How about this?
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Core was generated by `ring_c'.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Program terminated with signal 11, Segmentation fault.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #0  opal_memory_ptmalloc2_int_malloc (av=0x2b48f6300020,
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; bytes=47592367980728) at
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ../../../../../openmpi-1.8.1/opal/mca/memory/linux/malloc.c:4098
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 4098          bck-&gt;fd = unsorted_chunks(av);
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; (gdb) bt
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #0  opal_memory_ptmalloc2_int_malloc (av=0x2b48f6300020,
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; bytes=47592367980728) at
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ../../../../../openmpi-1.8.1/opal/mca/memory/linux/malloc.c:4098
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #1  0x00002b48f2a15e38 in opal_memory_ptmalloc2_malloc
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; (bytes=47592367980576) at
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ../../../../../openmpi-1.8.1/opal/mca/memory/linux/malloc.c:3433
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #2  0x00002b48f2a15b36 in opal_memory_linux_malloc_hook
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; (sz=47592367980576, caller=0x2b48f63000b8) at
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ../../../../../openmpi-1.8.1/opal/mca/memory/linux/hooks.c:691
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #3  0x00002b48f2374b90 in vasprintf () from /lib64/libc.so.6
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #4  0x00002b48f2354148 in asprintf () from /lib64/libc.so.6
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #5  0x00002b48f26dc7d1 in orte_oob_base_get_addr
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; (uri=0x2b48f6300020) at
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ../../../../openmpi-1.8.1/orte/mca/oob/base/oob_base_stubs.c:234
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #6  0x00002b48f53e7d4a in orte_rml_oob_get_uri () at
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ../../../../../openmpi-1.8.1/orte/mca/rml/oob/rml_oob_contact.c:36
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #7  0x00002b48f26fa181 in orte_routed_base_register_sync (setup=32 '
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ') at
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ../../../../openmpi-1.8.1/orte/mca/routed/base/routed_base_fns.c:3
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 0
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 1
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #8  0x00002b48f4bbcccf in init_routes (job=4130340896,
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ndat=0x2b48f63000b8) at
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ../../../../../openmpi-1.8.1/orte/mca/routed/binomial/routed_binom
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; i
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; al.c:705
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #9  0x00002b48f26c615d in orte_ess_base_app_setup
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; (db_restrict_local=32 ' ') at
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ../../../../openmpi-1.8.1/orte/mca/ess/base/ess_base_std_app.c:245
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #10 0x00002b48f45b069f in rte_init () at
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ../../../../../openmpi-1.8.1/orte/mca/ess/env/ess_env_module.c:146
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #11 0x00002b48f26935ab in orte_init (pargc=0x2b48f6300020,
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; pargv=0x2b48f63000b8, flags=8) at
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ../../openmpi-1.8.1/orte/runtime/orte_init.c:148
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #12 0x00002b48f1739d38 in ompi_mpi_init (argc=1,
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; argv=0x7fffebf0d1f8, requested=8, provided=0x0) at
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ../../openmpi-1.8.1/ompi/runtime/ompi_mpi_init.c:464
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #13 0x00002b48f1760a37 in PMPI_Init (argc=0x2b48f6300020,
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; argv=0x2b48f63000b8) at pinit.c:84
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #14 0x00000000004024ef in main (argc=1, argv=0x7fffebf0d1f8) at
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ring_c.c:19
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; *From:*users [mailto:users-bounces_at_[hidden]]*On Behalf
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Of*Ralph Castain *Sent:*Wednesday, June 04, 2014 11:00 AM
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; *To:*Open MPI Users
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; *Subject:*Re: [OMPI users] intermittent segfaults with openib on
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ring_c.c Does the trace go any further back? Your prior trace
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; seemed to indicate an error in our OOB framework, but in a very basic place.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Looks like it could be an uninitialized variable, and having the
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; line number down as deep as possible might help identify the
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; source On Jun 4, 2014, at 7:55 AM, Fischer, Greg A.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; &lt;fischega_at_[hidden] &lt;mailto:fischega_at_[hidden]&gt;&gt; wrote:
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Oops, ulimit was set improperly. I generated a core file, loaded
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; it in GDB, and ran a backtrace:
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Core was generated by `ring_c'.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Program terminated with signal 11, Segmentation fault.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #0  opal_memory_ptmalloc2_int_malloc (av=0x2b8e4fd00020,
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; bytes=47890224382136) at
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ../../../../../openmpi-1.8.1/opal/mca/memory/linux/malloc.c:4098
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 4098          bck-&gt;fd = unsorted_chunks(av);
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; (gdb) bt
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #0  opal_memory_ptmalloc2_int_malloc (av=0x2b8e4fd00020,
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; bytes=47890224382136) at
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ../../../../../openmpi-1.8.1/opal/mca/memory/linux/malloc.c:4098
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; #1  0x0000000000000000 in ?? ()
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Is that helpful?
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Greg
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; *From:*Fischer, Greg A.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; *Sent:*Wednesday, June 04, 2014 10:17 AM *To:*'Open MPI Users'
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; *Cc:*Fischer, Greg A.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; *Subject:*RE: [OMPI users] intermittent segfaults with openib on
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ring_c.c I recompiled with &quot;-enable-debug&quot; but it doesn't seem to
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; be providing any more information or a core dump. I'm compiling ring.c with:
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; mpicc ring_c.c -g -traceback -o ring_c and running with:
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; mpirun -np 4 --mca btl openib,self ring_c and I'm getting:
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf112:05845] *** Process received signal *** [binf112:05845]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Signal: Segmentation fault (11) [binf112:05845] Signal code:
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Address not mapped (1) [binf112:05845] Failing at address: 0x10
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf112:05845] [ 0]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /lib64/libpthread.so.0(+0xf7c0)[0x2b2fa44d57c0]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf112:05845] [ 1]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.8.1/lib/libopen-p
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; a l.so.6(opal_memory_ptmalloc2_int_malloc+0x4b3)[0x2b2fa4ff2b03]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf112:05845] [ 2]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.8.1/lib/libopen-p
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; a l.so.6(opal_memory_ptmalloc2_malloc+0x58)[0x2b2fa4ff5288]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf112:05845] [ 3]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.8.1/lib/libopen-p
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; a l.so.6(+0xd1f86)[0x2b2fa4ff4f86] [binf112:05845] [ 4]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /lib64/libc.so.6(vasprintf+0x3e)[0x2b2fa4957a7e]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf112:05845] [ 5]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /lib64/libc.so.6(asprintf+0x88)[0x2b2fa4937148]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf112:05845] [ 6]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.8.1/lib/libopen-r
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; t
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; e.so.7(orte_util_convert_process_name_to_string+0xe2)[0x2b2fa4c873
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; e
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 2]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf112:05845] [ 7]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.8.1/lib/libopen-r
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; t e.so.7(orte_oob_base_get_addr+0x25)[0x2b2fa4cbdb15]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf112:05845] [ 8]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.8.1/lib/openmpi/m
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; c a_rml_oob.so(orte_rml_oob_get_uri+0xa)[0x2b2fa79c5d2a]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf112:05845] [ 9]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.8.1/lib/libopen-r
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; t e.so.7(orte_routed_base_register_sync+0x1fd)[0x2b2fa4cdae7d]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf112:05845] [10]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.8.1/lib/openmpi/m
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; c a_routed_binomial.so(+0x3c7b)[0x2b2fa719bc7b]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf112:05845] [11]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.8.1/lib/libopen-r
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; t e.so.7(orte_ess_base_app_setup+0x3ad)[0x2b2fa4ca7c8d]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf112:05845] [12]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.8.1/lib/openmpi/m
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; c a_ess_env.so(+0x169f)[0x2b2fa6b8f69f]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf112:05845] [13]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.8.1/lib/libopen-r
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; t e.so.7(orte_init+0x17b)[0x2b2fa4c764bb]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf112:05845] [14]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.8.1/lib/libmpi.so.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 1(ompi_mpi_init+0x438)[0x2b2fa3d1e198]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf112:05845] [15]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.8.1/lib/libmpi.so.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 1(MPI_Init+0xf7)[0x2b2fa3d44947] [binf112:05845] [16]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ring_c[0x4024ef] [binf112:05845] [17]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /lib64/libc.so.6(__libc_start_main+0xe6)[0x2b2fa4906c36]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf112:05845] [18] ring_c[0x4023f9] [binf112:05845] *** End of
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; error message ***
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ------------------------------------------------------------------
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; -
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ------- mpirun noticed that process rank 3 with PID 5845 on node
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; xxxx112 exited on signal 11 (Segmentation fault).
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ------------------------------------------------------------------
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; -
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; -------
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Does any of that help?
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Greg
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; *From:*users [mailto:users-bounces_at_[hidden]]*On Behalf
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Of*Ralph Castain *Sent:*Tuesday, June 03, 2014 11:54 PM *To:*Open
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; MPI Users
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; *Subject:*Re: [OMPI users] intermittent segfaults with openib on
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ring_c.c Sounds odd - can you configure OMPI --enable-debug and run it again?
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; If it fails and you can get a core dump, could you tell us the
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; line number where it is failing?
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; On Jun 3, 2014, at 9:58 AM, Fischer, Greg A.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; &lt;fischega_at_[hidden] &lt;mailto:fischega_at_[hidden]&gt;&gt; wrote:
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Apologies - I forgot to add some of the information requested by the FAQ:
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 1.OpenFabrics is provided by the Linux distribution:
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf102:fischega] $ rpm -qa | grep ofed
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ofed-kmp-default-1.5.4.1_3.0.76_0.11-0.11.5
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ofed-1.5.4.1-0.11.5
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ofed-doc-1.5.4.1-0.11.5
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 2.Linux Distro / Kernel:
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf102:fischega] $ cat /etc/SuSE-release SUSE Linux Enterprise
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Server 11 (x86_64) VERSION = 11 PATCHLEVEL = 3
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf102:fischega] $ uname -a
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Linux xxxx102 3.0.76-0.11-default #1 SMP Fri Jun 14 08:21:43 UTC
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 2013
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; (ccab990) x86_64 x86_64 x86_64 GNU/Linux
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 3.Not sure which subnet manger is being used - I think OpenSM, but
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; I'll need to check with my administrators.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 4.Output of ibv_devinfo is attached.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 5.Ifconfig output is attached.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 6.Ulimit -l output:
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf102:fischega] $ ulimit -l
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; unlimited
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Greg
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; *From:*Fischer, Greg A.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; *Sent:*Tuesday, June 03, 2014 12:38 PM *To:*Open MPI Users
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; *Cc:*Fischer, Greg A.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; *Subject:*intermittent segfaults with openib on ring_c.c Hello
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; openmpi-users, I'm running into a perplexing problem on a new
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; system, whereby I'm experiencing intermittent segmentation faults
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; when I run the ring_c.c example and use the openib BTL. See an
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; example below. Approximately 50% of the time it provides the
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; expected output, but the other 50% of the time, it segfaults.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; LD_LIBRARY_PATH is set correctly, and the version of &quot;mpirun&quot;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; being invoked is correct. The output of ompi_info -all is attached.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; One potential problem may be that the system that OpenMPI was
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; compiled on is/mostly/the same as the system where it is being
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; executed, but there are some differences in the installed packages.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; I've checked the critical ones (libibverbs, librdmacm,
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; libmlx4-rdmav2, etc.), and they appear to be the same.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Can anyone suggest how I might start tracking this problem down?
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Thanks,
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Greg
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf102:fischega] $ mpirun -np 2 --mca btl openib,self ring_c
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf102:31268] *** Process received signal *** [binf102:31268]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Signal: Segmentation fault (11) [binf102:31268] Signal code:
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Address not mapped (1) [binf102:31268] Failing at address: 0x10
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf102:31268] [ 0] /lib64/libpthread.so.0(+0xf7c0)
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [0x2b42213f57c0] [binf102:31268] [ 1]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.6.5/lib/libmpi.so.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 1(opal_memory_ptmalloc2_int_malloc+0x4b3)
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [0x2b42203fd7e3]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf102:31268] [ 2]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.6.5/lib/libmpi.so.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 1(opal_memory_ptmalloc2_int_memalign+0x8b)
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [0x2b4220400d3b]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf102:31268] [ 3]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.6.5/lib/libmpi.so.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 1(opal_memory_ptmalloc2_memalign+0x6f)
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [0x2b42204008ef]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf102:31268] [ 4]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.6.5/lib/libmpi.so.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 1(+0x117876)
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [0x2b4220400876]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf102:31268] [ 5]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.6.5/lib/openmpi/m
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; c
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; a_btl_openib.so(+0xc34c)
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [0x2b422572334c]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf102:31268] [ 6]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.6.5/lib/libmpi.so.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; 1(opal_class_initialize+0xaa)
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [0x2b422041d64a]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf102:31268] [ 7]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; /xxxx/yyyy_ib/intel-12.1.0.233/toolset/openmpi-1.6.5/lib/openmpi/m
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; c
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; a_btl_openib.so(+0x1f12f)
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [0x2b422573612f]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [binf102:31268] [ 8] /lib64/libpthread.so.0(+0x77b6)
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [0x2b42213ed7b6] [binf102:31268] [ 9] /lib64/libc.so.6(clone+0x6d)
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [0x2b42216dcd6d] [binf102:31268] *** End of error message ***
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ------------------------------------------------------------------
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; -
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ------- mpirun noticed that process rank 0 with PID 31268 on node
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; xxxx102 exited on signal 11 (Segmentation fault).
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ------------------------------------------------------------------
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; -
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; -------
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; &lt;ibv_devinfo.txt&gt;&lt;ifconfig.txt&gt;___________________________________
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; _
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ___________
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; users mailing list
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; users_at_[hidden] &lt;mailto:users_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; _______________________________________________
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; users mailing list
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; users_at_[hidden] &lt;mailto:users_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; _______________________________________________
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; users mailing list
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; users_at_[hidden] &lt;mailto:users_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; _______________________________________________
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; users mailing list
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; _______________________________________________
</span><br>
<span class="quotelev3">&gt;&gt;&gt; users mailing list
</span><br>
<span class="quotelev3">&gt;&gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev3">&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; _______________________________________________
</span><br>
<span class="quotelev1">&gt; users mailing list
</span><br>
<span class="quotelev1">&gt; users_at_[hidden]
</span><br>
<span class="quotelev1">&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; _______________________________________________
</span><br>
<span class="quotelev1">&gt; users mailing list
</span><br>
<span class="quotelev1">&gt; users_at_[hidden]
</span><br>
<span class="quotelev1">&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<p>_______________________________________________
<br>
users mailing list
<br>
users_at_[hidden]
<br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
<br>
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="24542.php">Ralph Castain: "Re: [OMPI users] intermittent segfaults with openib on ring_c.c"</a>
<li><strong>Previous message:</strong> <a href="24540.php">Ralph Castain: "Re: [OMPI users] intermittent segfaults with openib on ring_c.c"</a>
<li><strong>In reply to:</strong> <a href="24540.php">Ralph Castain: "Re: [OMPI users] intermittent segfaults with openib on ring_c.c"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="24542.php">Ralph Castain: "Re: [OMPI users] intermittent segfaults with openib on ring_c.c"</a>
<li><strong>Reply:</strong> <a href="24542.php">Ralph Castain: "Re: [OMPI users] intermittent segfaults with openib on ring_c.c"</a>
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
