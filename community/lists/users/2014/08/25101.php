<?
$subject_val = "Re: [OMPI users] Clarification about OpenMPI, slurm and PMI interface";
include("../../include/msg-header.inc");
?>
<!-- received="Wed Aug 20 19:37:00 2014" -->
<!-- isoreceived="20140820233700" -->
<!-- sent="Wed, 20 Aug 2014 16:36:42 -0700" -->
<!-- isosent="20140820233642" -->
<!-- name="Ralph Castain" -->
<!-- email="rhc_at_[hidden]" -->
<!-- subject="Re: [OMPI users] Clarification about OpenMPI, slurm and PMI interface" -->
<!-- id="14172F89-EA99-4BCB-929C-5E390AC2198D_at_open-mpi.org" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="CAG4F6z-A9H9BSu119Ad4wgriatiFENDyveK92VXt9LOOfZmBRw_at_mail.gmail.com" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>Subject:</strong> Re: [OMPI users] Clarification about OpenMPI, slurm and PMI interface<br>
<strong>From:</strong> Ralph Castain (<em>rhc_at_[hidden]</em>)<br>
<strong>Date:</strong> 2014-08-20 19:36:42
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="25102.php">tmishima_at_[hidden]: "Re: [OMPI users] Running a hybrid MPI+openMP program"</a>
<li><strong>Previous message:</strong> <a href="25100.php">Joshua Ladd: "Re: [OMPI users] Clarification about OpenMPI, slurm and PMI interface"</a>
<li><strong>In reply to:</strong> <a href="25100.php">Joshua Ladd: "Re: [OMPI users] Clarification about OpenMPI, slurm and PMI interface"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="25104.php">Filippo Spiga: "Re: [OMPI users] Clarification about OpenMPI, slurm and PMI interface"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
Or you can add 
<br>
<p>&nbsp;&nbsp;&nbsp;-nolocal|--nolocal    Do not run any MPI applications on the local node
<br>
<p>to your mpirun command line and we won't run any application procs on the node where mpirun is executing
<br>
<p><p>On Aug 20, 2014, at 4:28 PM, Joshua Ladd &lt;jladd.mlnx_at_[hidden]&gt; wrote:
<br>
<p><span class="quotelev1">&gt; Hi, Filippo
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; When launching with mpirun in a SLURM environment, srun is only being used to launch the ORTE daemons (orteds.)  Since the daemon will already exist on the node from which you invoked mpirun, this node will not be included in the list of nodes. SLURM's PMI library is not involved (that functionality is only necessary if you directly launch your MPI application with srun, in which case it is used to exchanged wireup info amongst slurmds.) This is the expected behavior. 
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; ~/ompi-top-level/orte/mca/plm/plm_slurm_module.c +294
</span><br>
<span class="quotelev1">&gt; /* if the daemon already exists on this node, then
</span><br>
<span class="quotelev1">&gt;          * don't include it
</span><br>
<span class="quotelev1">&gt;          */
</span><br>
<span class="quotelev1">&gt;         if (node-&gt;daemon_launched) {
</span><br>
<span class="quotelev1">&gt;             continue;
</span><br>
<span class="quotelev1">&gt;         }
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; Do you have a frontend node that you can launch from? What happens if you set &quot;-np X&quot; where X = 8*ppn. The alternative is to do a direct launch of the MPI application with srun.
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; Best,
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; Josh
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; On Wed, Aug 20, 2014 at 6:48 PM, Filippo Spiga &lt;spiga.filippo_at_[hidden]&gt; wrote:
</span><br>
<span class="quotelev1">&gt; Dear Open MPI experts,
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; I have a problem that is related to the integration of OpenMPI, slurm and PMI interface. I spent some time today with a colleague of mine trying to figure out why we were not able to obtain all H5 profile files (generated by acct_gather_profile) using Open MPI. When I say &quot;all&quot; I mean if I run using 8 nodes (e.g. tesla[121-128]) then I always systematically miss the file related to the first one (the first node in the allocation list, in this case tesla121).
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; By comparing which processes are spawn on the compute nodes, I discovered that mpirun running on tesla121 calls srun only to spawn remotely new MPI processes to the other 7 nodes (maybe this is obvious, for me it was not)...
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; fs395      617  0.0  0.0 106200  1504 ?        S    22:41   0:00 /bin/bash /var/spool/slurm-test/slurmd/job390044/slurm_script
</span><br>
<span class="quotelev1">&gt; fs395      629  0.1  0.0 194552  5288 ?        Sl   22:41   0:00  \_ mpirun -bind-to socket --map-by ppr:1:socket --host tesla121,tesla122,tesla123,tesla124,tesla125,tesla126,tes
</span><br>
<span class="quotelev1">&gt; fs395      632  0.0  0.0 659740  9148 ?        Sl   22:41   0:00  |   \_ srun --ntasks-per-node=1 --kill-on-bad-exit --cpu_bind=none --nodes=7 --nodelist=tesla122,tesla123,tesla1
</span><br>
<span class="quotelev1">&gt; fs395      633  0.0  0.0  55544  1072 ?        S    22:41   0:00  |   |   \_ srun --ntasks-per-node=1 --kill-on-bad-exit --cpu_bind=none --nodes=7 --nodelist=tesla122,tesla123,te
</span><br>
<span class="quotelev1">&gt; fs395      651  0.0  0.0 106072  1392 ?        S    22:41   0:00  |   \_ /bin/bash ./run_linpack ./xhpl
</span><br>
<span class="quotelev1">&gt; fs395      654  295 35.5 120113412 23289280 ?  RLl  22:41   3:12  |   |   \_ ./xhpl
</span><br>
<span class="quotelev1">&gt; fs395      652  0.0  0.0 106072  1396 ?        S    22:41   0:00  |   \_ /bin/bash ./run_linpack ./xhpl
</span><br>
<span class="quotelev1">&gt; fs395      656  307 35.5 120070332 23267728 ?  RLl  22:41   3:19  |       \_ ./xhpl
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; The &quot;xhpl&quot; processes allocated on the first node of a job are not called by srun and because of this the SLURM profile plugin is not activated on the node!!! As result I always miss the first node profile information. Intel MPI does not have this behavior, mpiexec.hydra uses srun on the first node. 
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; I got to the conclusion that SLURM is configured properly, something is wrong in the way I lunch Open MPI using mpirun. If I disable SLURM support and I revert back to rsh (--mca plm rsh) everything work but there is not profiling because the SLURM plug-in is not activated. During the configure step, Open MPI 1.8.1 detects slurm and libmpi/libpmi2 correctly. Honestly, I would prefer to avoid to use srun as job luncher if possible...
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; Any suggestion to get this sorted out is really appreciated!
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; Best Regards,
</span><br>
<span class="quotelev1">&gt; Filippo
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; --
</span><br>
<span class="quotelev1">&gt; Mr. Filippo SPIGA, M.Sc.
</span><br>
<span class="quotelev1">&gt; <a href="http://filippospiga.info">http://filippospiga.info</a> ~ skype: filippo.spiga
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; &#171;Nobody will drive us out of Cantor's paradise.&#187; ~ David Hilbert
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; *****
</span><br>
<span class="quotelev1">&gt; Disclaimer: &quot;Please note this message and any attachments are CONFIDENTIAL and may be privileged or otherwise protected from disclosure. The contents are not to be disclosed to anyone other than the addressee. Unauthorized recipients are requested to preserve this confidentiality and to advise the sender immediately of any error in transmission.&quot;
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; _______________________________________________
</span><br>
<span class="quotelev1">&gt; users mailing list
</span><br>
<span class="quotelev1">&gt; users_at_[hidden]
</span><br>
<span class="quotelev1">&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1">&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25099.php">http://www.open-mpi.org/community/lists/users/2014/08/25099.php</a>
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; _______________________________________________
</span><br>
<span class="quotelev1">&gt; users mailing list
</span><br>
<span class="quotelev1">&gt; users_at_[hidden]
</span><br>
<span class="quotelev1">&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1">&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25100.php">http://www.open-mpi.org/community/lists/users/2014/08/25100.php</a>
</span><br>
<p><p><hr>
<ul>
<li>text/html attachment: <a href="http://www.open-mpi.org/community/lists/users/att-25101/attachment">attachment</a>
</ul>
<!-- attachment="attachment" -->
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="25102.php">tmishima_at_[hidden]: "Re: [OMPI users] Running a hybrid MPI+openMP program"</a>
<li><strong>Previous message:</strong> <a href="25100.php">Joshua Ladd: "Re: [OMPI users] Clarification about OpenMPI, slurm and PMI interface"</a>
<li><strong>In reply to:</strong> <a href="25100.php">Joshua Ladd: "Re: [OMPI users] Clarification about OpenMPI, slurm and PMI interface"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="25104.php">Filippo Spiga: "Re: [OMPI users] Clarification about OpenMPI, slurm and PMI interface"</a>
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
