<html><body><div style="color:#000; background-color:#fff; font-family:times new roman, new york, times, serif;font-size:12pt"><div><span>Thats very interesting&nbsp;</span><span style="font-family: Arial; font-size: 13px; ">Yevgeny,</span></div><div style="color: rgb(0, 0, 0); font-size: 13px; font-family: Arial; background-color: transparent; font-style: normal; "><span style="font-family: Arial; font-size: 13px; "><br></span></div><div style="color: rgb(0, 0, 0); font-size: 13.333333015441895px; font-family: Arial; background-color: transparent; font-style: normal; "><span style="font-family: Arial; font-size: 13px; ">Yes tcp,self ran in 12 seconds</span></div><div style="color: rgb(0, 0, 0); font-size: 13px; font-family: Arial; background-color: transparent; font-style: normal; "><span style="font-family: Arial; font-size: 13px; ">tcp,self,sm ran in 27 seconds</span></div><div style="color: rgb(0, 0, 0); font-size: 13px; font-family: Arial;
 background-color: transparent; font-style: normal; "><span style="font-family: Arial; font-size: 13px; "><br></span></div><div style="color: rgb(0, 0, 0); font-size: 13.333333015441895px; font-family: Arial; background-color: transparent; font-style: normal; "><span style="font-family: Arial; font-size: 13px; ">Does anyone have any idea how this can be?</span></div><div><br></div><div style="color: rgb(0, 0, 0); font-size: 15.833333015441895px; font-family: 'times new roman', 'new york', times, serif; background-color: transparent; font-style: normal; ">About half the data would go to local processes, so SM should pay dividends.</div><div style="color: rgb(0, 0, 0); font-size: 15.833333015441895px; font-family: 'times new roman', 'new york', times, serif; background-color: transparent; font-style: normal; "><br></div>  <div style="font-family: 'times new roman', 'new york', times, serif; font-size: 12pt; "> <div style="font-family: 'times new roman',
 'new york', times, serif; font-size: 12pt; "> <div dir="ltr"> <font size="2" face="Arial"> <hr size="1">  <b><span style="font-weight:bold;">From:</span></b> Yevgeny Kliteynik &lt;kliteyn@dev.mellanox.co.il&gt;<br> <b><span style="font-weight: bold;">To:</span></b> Randolph Pullen &lt;randolph_pullen@yahoo.com.au&gt; <br><b><span style="font-weight: bold;">Cc:</span></b> OpenMPI Users &lt;users@open-mpi.org&gt; <br> <b><span style="font-weight: bold;">Sent:</span></b> Monday, 10 September 2012 9:11 PM<br> <b><span style="font-weight: bold;">Subject:</span></b> Re: [OMPI users] Infiniband performance Problem and stalling<br> </font> </div> <br>Randolph,<br><br>So what you saying in short, leaving all the numbers aside, is the following:<br>In your particular application on your particular setup with this particular OMPI version,<br> 1. openib BTL performs faster than shared memory BTL<br> 2. TCP BTL performs faster than shared memory<br><br>IMHO, this
 indicates that you have some problem on your machines,<br>and this problem is unrelated to interconnect.<br><br>Shared memory should be much faster than IB, not to mention IPoIB.<br><br>Could you run these two commands?<br><br> mpirun --mca btl tcp,self&nbsp; &nbsp; -H vh2,vh1 -np 9 --bycore prog<br> mpirun --mca btl tcp,self,sm -H vh2,vh1 -np 9 --bycore prog<br><br>You will probably see better number w/o sm.<br>Why? Don't know.<br><br>Perhaps someone who has better knowledge in sm BTL can elaborate?<br><br>-- YK<br><br><br>On 9/10/2012 6:32 AM, Randolph Pullen wrote:<br>&gt; See my comments in line...<br>&gt; <br>&gt;
 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------<br>&gt; *From:* Yevgeny Kliteynik &lt;<a ymailto="mailto:kliteyn@dev.mellanox.co.il" href="mailto:kliteyn@dev.mellanox.co.il">kliteyn@dev.mellanox.co.il</a>&gt;<br>&gt; *To:* Randolph Pullen &lt;<a ymailto="mailto:randolph_pullen@yahoo.com.au" href="mailto:randolph_pullen@yahoo.com.au">randolph_pullen@yahoo.com.au</a>&gt;<br>&gt; *Cc:* OpenMPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>&gt; *Sent:* Sunday, 9 September 2012 6:18 PM<br>&gt; *Subject:* Re: [OMPI users] Infiniband performance Problem and stalling<br>&gt; <br>&gt; Randolph,<br>&gt; <br>&gt; On 9/7/2012 7:43 AM, Randolph Pullen wrote:<br>&gt;&nbsp; &gt; Yevgeny,<br>&gt;&nbsp; &gt; The ibstat results:<br>&gt;&nbsp; &gt; CA 'mthca0'<br>&gt;&nbsp; &gt; CA type: MT25208 (MT23108 compat mode)<br>&gt; <br>&gt; What you have is InfiniHost III HCA, which is 4x SDR card.<br>&gt; This card has theoretical peak
 of 10 Gb/s, which is 1GB/s in IB bit coding.<br>&gt; <br>&gt;&nbsp; &gt; And more interestingly, ib_write_bw:<br>&gt;&nbsp; &gt; Conflicting CPU frequency values detected: 1600.000000 != 3301.000000<br>&gt;&nbsp; &gt;<br>&gt;&nbsp; &gt; What does Conflicting CPU frequency values mean?<br>&gt;&nbsp; &gt;<br>&gt;&nbsp; &gt; Examining the /proc/cpuinfo file however shows:<br>&gt;&nbsp; &gt; processor : 0<br>&gt;&nbsp; &gt; cpu MHz : 3301.000<br>&gt;&nbsp; &gt; processor : 1<br>&gt;&nbsp; &gt; cpu MHz : 3301.000<br>&gt;&nbsp; &gt; processor : 2<br>&gt;&nbsp; &gt; cpu MHz : 1600.000<br>&gt;&nbsp; &gt; processor : 3<br>&gt;&nbsp; &gt; cpu MHz : 1600.000<br>&gt;&nbsp; &gt;<br>&gt;&nbsp; &gt; Which seems oddly wierd to me...<br>&gt; <br>&gt; You need to have all the cores running at highest clock to get better numbers.<br>&gt; May be you have power governor not set to optimal performance on these machines.<br>&gt; Google for "Linux CPU scaling governor" to get
 more info on this subject, or<br>&gt; contact your system admin and ask him to take care of the CPU frequencies.<br>&gt; <br>&gt; Once this is done, check all the pairs of your machines - ensure that you get<br>&gt; a good number with ib_write_br.<br>&gt; Note that if you have a slower machine in the cluster, general application<br>&gt; performance will suffer from this.<br>&gt; <br>&gt; I have anchored the clocks speeds to:<br>&gt; [root@vh1 ~]# cat /sys/devices/system/cpu/*/cpufreq/cpuinfo_cur_freq<br>&gt; 3600000<br>&gt; 3600000<br>&gt; 3600000<br>&gt; 3600000<br>&gt; 3600000<br>&gt; 3600000<br>&gt; 3600000<br>&gt; 3600000<br>&gt; <br>&gt; [root@vh2 ~]# cat /sys/devices/system/cpu/*/cpufreq/cpuinfo_cur_freq<br>&gt; 3200000<br>&gt; 3200000<br>&gt; 3200000<br>&gt; 3200000<br>&gt; <br>&gt; However /proc/cpuinfo still reports them incorrectly<br>&gt; [deepcloud@vh2 c]$ grep MHz /proc/cpuinfo<br>&gt; cpu MHz : 3300.000<br>&gt; cpu MHz : 1600.000<br>&gt;
 cpu MHz : 1600.000<br>&gt; cpu MHz : 1600.000<br>&gt; <br>&gt; I don't think this is the problem, so I used -F option in ib_write_bw to push ahead. ie;<br>&gt; [deepcloud@vh2 c]$ ib_write_bw -F vh1<br>&gt; ------------------------------------------------------------------<br>&gt; RDMA_Write BW Test<br>&gt; Number of qps : 1<br>&gt; Connection type : RC<br>&gt; TX depth : 300<br>&gt; CQ Moderation : 50<br>&gt; Link type : IB<br>&gt; Mtu : 2048<br>&gt; Inline data is used up to 0 bytes message<br>&gt; local address: LID 0x04 QPN 0xaa0408 PSN 0xf9c072 RKey 0x59260052 VAddr 0x002b03a8af3000<br>&gt; remote address: LID 0x03 QPN 0x8b0408 PSN 0xe4890d RKey 0x4a62003c VAddr 0x002b8e44297000<br>&gt; ------------------------------------------------------------------<br>&gt; #bytes #iterations BW peak[MB/sec] BW average[MB/sec]<br>&gt; Conflicting CPU frequency values detected: 3300.000000 != 1600.000000<br>&gt; Test integrity may be harmed !<br>&gt; Conflicting
 CPU frequency values detected: 3300.000000 != 1600.000000<br>&gt; Test integrity may be harmed !<br>&gt; Conflicting CPU frequency values detected: 3300.000000 != 1600.000000<br>&gt; Test integrity may be harmed !<br>&gt; Warning: measured timestamp frequency 3092.95 differs from nominal 3300 MHz<br>&gt; 65536 5000 937.61 937.60<br>&gt; ------------------------------------------------------------------<br>&gt; <br>&gt; <br>&gt; <br>&gt; *&gt; &gt; On 8/31/2012 10:53 AM, Randolph Pullen wrote:*<br>&gt; *&gt; &gt; &gt; (reposted with consolidatedinformation)*<br>&gt; *&gt; &gt; &gt; I have a test rig comprising 2 i7 systems 8GB RAM with Melanox III HCA 10G cards*<br>&gt; *&gt; &gt; &gt; running Centos 5.7 Kernel 2.6.18-274*<br>&gt; *&gt; &gt; &gt; Open MPI 1.4.3*<br>&gt; *&gt; &gt; &gt; MLNX_OFED_LINUX-1.5.3-1.0.0.2 (OFED-1.5.3-1.0.0.2):*<br>&gt; *&gt; &gt; &gt; On a Cisco 24 pt switch*<br>&gt; *&gt; &gt; &gt; Normal performance is:*<br>&gt; *&gt; &gt;
 &gt; $ mpirun --mca btl openib,self -n 2 -hostfile mpi.hosts PingPong*<br>&gt; *&gt; &gt; &gt; results in:*<br>&gt; *&gt; &gt; &gt; Max rate = 958.388867 MB/sec Min latency = 4.529953 usec*<br>&gt; *&gt; &gt; &gt; and:*<br>&gt; *&gt; &gt; &gt; $ mpirun --mca btl tcp,self -n 2 -hostfile mpi.hosts PingPong*<br>&gt; *&gt; &gt; &gt; Max rate = 653.547293 MB/sec Min latency = 19.550323 usec*<br>&gt; <br>&gt; *These numbers look fine - 958 MB/s on IB is close to theoretical limit.*<br>&gt; *654 MB/s for IPoIB look fine too.*<br>&gt; <br>&gt; *&gt; &gt; &gt; My problem is I see better performance under IPoIB then I do on native IB (RDMA_CM).*<br>&gt; <br>&gt; *I don't see this in your numbers. What do I miss?*<br>&gt; <br>&gt; Runs in 9 seconds:<br>&gt; mpirun --mca btl_openib_eager_limit 64 --mca mpi_leave_pinned 1 --mca btl openib,self -H vh2,vh1 -np 9 --bycore prog<br>&gt; mpirun --mca btl_openib_flags 2 --mca mpi_leave_pinned 1 --mca btl tcp,self -H
 vh2,vh1 -np 9 --bycore prog<br>&gt; <br>&gt; Runs in 24 seconds or more:<br>&gt; mpirun --mca btl_openib_flags 2 --mca mpi_leave_pinned 1 --mca btl openib,self -H vh2,vh1 -np 9 --bycore prog<br>&gt; mpirun --mca btl_openib_flags 2 --mca mpi_leave_pinned 1 --mca btl openib,self,sm -H vh2,vh1 -np 9 --bycore prog<br>&gt; mpirun --mca btl_openib_eager_limit 64 --mca mpi_leave_pinned 1 --mca btl openib,self,sm -H vh2,vh1 -np 9 --bycore prog<br>&gt; <br>&gt; Note:<br>&gt; - adding sm to the fastest openib run results in a 13 second penalty<br>&gt; - Subsequent runs with openib usually add at least 10 seconds per run or stall<br>&gt; <br>&gt; *&gt; &gt; &gt; My understanding is that IPoIB is limited to about 1G/s so I am at a loss to know why it is faster.*<br>&gt; <br>&gt; *Again, I see IPoIB performance under 1 GB/s.*<br>&gt; <br>&gt; *&gt; &gt; &gt; And this one produces similar run times but seems to degrade with repeated cycles:*<br>&gt; *&gt; &gt; &gt;
 mpirun --mca btl_openib_eager_limit 64 --mca mpi_leave_pinned 1 --mca btl openib,self -H vh2,vh1 -np 9 --bycore prog*<br>&gt; *&gt; *<br>&gt; *&gt; You're running 9 ranks on two machines, but you're using IB for intra-node communication.*<br>&gt; *&gt; Is it intentional? If not, you can add "sm" btl and have performance improved.*<br>&gt; <br>&gt; *Also, don't forget to include "sm" btl if you have more than 1 MPI rank per node.*<br>&gt; See above: adding sm to the fastest openib run results in a 13 second penalty<br>&gt; <br>&gt; <br>&gt; *-- YK*<br>&gt; <br>&gt; <br>&gt; <br><br><br><br> </div> </div>  </div></body></html>
