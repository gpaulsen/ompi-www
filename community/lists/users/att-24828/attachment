
<HTML><BODY><br><br><br>-------- Пересылаемое сообщение --------<br>
От кого: Timur Ismagilov &lt;tismagilov@mail.ru&gt;<br>
Кому: Ralph Castain &lt;rhc@open-mpi.org&gt;<br>

Дата: Sun, 20 Jul 2014 21:58:41 +0400<br>
Тема: Re[2]: [OMPI users] Fwd: Re[4]:  Salloc and mpirun problem<br>
<br>

	



    









	
	


	
	
	
	
	

	
	

	
	



<div class="js-helper js-readmsg-msg">
	<style type="text/css"></style>
 	<div>
		<base target="_self" href="https://e.mail.ru/">
		
			<div id="style_14058791210000000177_BODY">
<p>Here it is:</p><p>$ salloc -N2 --exclusive -p test -J ompi<br>salloc: Granted job allocation 647049<br><br></p><p>$ mpirun -mca mca_base_env_list 'LD_PRELOAD' -mca oob_base_verbose 10 -mca rml_base_verbose 10 -np 2 hello_c<br></p><p>[access1:24264] mca: base: components_register: registering oob components<br>[access1:24264] mca: base: components_register: found loaded component tcp<br>[access1:24264] mca: base: components_register: component tcp register function successful<br>[access1:24264] mca: base: components_open: opening oob components<br>[access1:24264] mca: base: components_open: found loaded component tcp<br>[access1:24264] mca: base: components_open: component tcp open function successful<br>[access1:24264] mca:oob:select: checking available component tcp<br>[access1:24264] mca:oob:select: Querying component [tcp]<br>[access1:24264] oob:tcp: component_available called<br>[access1:24264] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>[access1:24264] WORKING INTERFACE 2 KERNEL INDEX 3 FAMILY: V4<br>[access1:24264] [[55095,0],0] oob:tcp:init adding 10.0.251.51 to our list of V4 connections<br>[access1:24264] WORKING INTERFACE 3 KERNEL INDEX 4 FAMILY: V4<br>[access1:24264] [[55095,0],0] oob:tcp:init adding 10.0.0.111 to our list of V4 connections<br>[access1:24264] WORKING INTERFACE 4 KERNEL INDEX 5 FAMILY: V4<br>[access1:24264] [[55095,0],0] oob:tcp:init adding 10.2.251.11 to our list of V4 connections<br>[access1:24264] WORKING INTERFACE 5 KERNEL INDEX 6 FAMILY: V4<br>[access1:24264] [[55095,0],0] oob:tcp:init adding 10.128.0.1 to our list of V4 connections<br>[access1:24264] WORKING INTERFACE 6 KERNEL INDEX 7 FAMILY: V4<br>[access1:24264] [[55095,0],0] oob:tcp:init adding 93.180.7.36 to our list of V4 connections<br>[access1:24264] WORKING INTERFACE 7 KERNEL INDEX 7 FAMILY: V4<br>[access1:24264] [[55095,0],0] oob:tcp:init adding 93.180.7.36 to our list of V4 connections<br>[access1:24264] [[55095,0],0] TCP STARTUP<br>[access1:24264] [[55095,0],0] attempting to bind to IPv4 port 0<br>[access1:24264] [[55095,0],0] assigned IPv4 port 47756<br>[access1:24264] mca:oob:select: Adding component to end<br>[access1:24264] mca:oob:select: Found 1 active transports<br>[access1:24264] mca: base: components_register: registering rml components<br>[access1:24264] mca: base: components_register: found loaded component oob<br>[access1:24264] mca: base: components_register: component oob has no register or open function<br>[access1:24264] mca: base: components_open: opening rml components<br>[access1:24264] mca: base: components_open: found loaded component oob<br>[access1:24264] mca: base: components_open: component oob open function successful<br>[access1:24264] orte_rml_base_select: initializing rml component oob<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 30 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 15 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 32 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 33 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 5 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 10 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 12 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 9 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 34 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 2 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 21 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 22 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 45 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 46 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 1 for peer [[WILDCARD],WILDCARD]<br>[access1:24264] [[55095,0],0] posting recv<br>[access1:24264] [[55095,0],0] posting persistent recv on tag 27 for peer [[WILDCARD],WILDCARD]<br>--------------------------------------------------------------------------<br>An ORTE daemon has unexpectedly failed after launch and before<br>communicating back to mpirun. This could be caused by a number<br>of factors, including an inability to create a connection back<br>to mpirun due to a lack of common network interfaces and/or no<br>route found between them. Please check network connectivity<br>(including firewalls and network routing requirements).<br>--------------------------------------------------------------------------<br>[access1:24264] mca: base: close: component oob closed<br>[access1:24264] mca: base: close: unloading component oob<br>[access1:24264] [[55095,0],0] TCP SHUTDOWN<br>[access1:24264] mca: base: close: component tcp closed<br>[access1:24264] mca: base: close: unloading component tcp</p><p>When i use srun i got:</p><p>$ salloc -N2 --exclusive -p test -J ompi<br>....<br>$srun -N 2 ./hello_c<br>Hello, world, I am 0 of 1, (Open MPI v1.9a1, package: Open MPI semenov@compiler-2 Distribution, ident: 1.9a1r32252, repo rev: r32252, Jul 16, 2014 (nightly snapshot tarball), 146)<br>Hello, world, I am 0 of 1, (Open MPI v1.9a1, package: Open MPI semenov@compiler-2 Distribution, ident: 1.9a1r32252, repo rev: r32252, Jul 16, 2014 (nightly snapshot tarball), 146)</p><p><br>Sun, 20 Jul 2014 09:28:13 -0700 от Ralph Castain &lt;rhc@open-mpi.org&gt;:<br></p><blockquote style="border-left:1px solid #0857A6;margin:10px;padding:0 0 0 10px;">
	<div>
	



    









	
	


	
	
	
	
	

	
	

	
	



<div>
	
 	<div>
		
		
			<div>Try adding -mca oob_base_verbose 10 -mca rml_base_verbose 10 to your cmd line. It looks to me like we are unable to connect back to the node where you are running mpirun for some reason.<div><br></div><div><br><div><div>On Jul 20, 2014, at 9:16 AM, Timur Ismagilov &lt;<a href="//e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt; wrote:</div><br><blockquote type="cite"><div style="font-family: Helvetica;font-size: 12px;font-style: normal;font-variant: normal;font-weight: normal;letter-spacing: normal;line-height: normal;orphans: auto;text-align: start;text-indent: 0px;text-transform: none;white-space: normal;widows: auto;word-spacing: 0px;-webkit-text-stroke-width: 0px;"><p>I have the same problem in openmpi 1.8.1(<span data-mce-style="font-family: verdana, arial, helvetica;text-align: -webkit-right;" style="font-family: verdana, arial, helvetica;text-align: -webkit-right;">Apr 23, 2014</span>).<br>Does the srun command have &nbsp;a --map-by&lt;foo&gt; mpirun parameter, or can i chage it from bash enviroment?</p><p><br><br>-------- Пересылаемое сообщение --------<br>От кого: Timur Ismagilov &lt;<a href="//e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;<br>Кому: Mike Dubman &lt;<a href="//e.mail.ru/compose/?mailto=mailto%3amiked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt;<br>Копия: Open MPI Users &lt;<a href="//e.mail.ru/compose/?mailto=mailto%3ausers@open%2dmpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>Дата: Thu, 17 Jul 2014 16:42:24 +0400<br>Тема: Re[4]: [OMPI users] Salloc and mpirun problem<br><br></p><div><div><p>With Open MPI&nbsp;1.9a1r32252 (Jul 16, 2014 (nightly snapshot tarball))&nbsp;i got this output&nbsp;(same?):</p><p>$ salloc -N2 --exclusive -p test -J ompi<br>salloc: Granted job allocation 645686<br><br>$LD_PRELOAD=/mnt/data/users/dm2/vol3/semenov/_scratch/mxm/mxm-3.0/lib/libmxm.so &nbsp;mpirun &nbsp;-mca mca_base_env_list 'LD_PRELOAD' &nbsp;--mca plm_base_verbose 10 --debug-daemons -np 1 hello_c<br></p><p>[access1:04312] mca: base: components_register: registering plm components<br>[access1:04312] mca: base: components_register: found loaded component isolated<br>[access1:04312] mca: base: components_register: component isolated has no register or open function<br>[access1:04312] mca: base: components_register: found loaded component rsh<br>[access1:04312] mca: base: components_register: component rsh register function successful<br>[access1:04312] mca: base: components_register: found loaded component slurm<br>[access1:04312] mca: base: components_register: component slurm register function successful<br>[access1:04312] mca: base: components_open: opening plm components<br>[access1:04312] mca: base: components_open: found loaded component isolated<br>[access1:04312] mca: base: components_open: component isolated open function successful<br>[access1:04312] mca: base: components_open: found loaded component rsh<br>[access1:04312] mca: base: components_open: component rsh open function successful<br>[access1:04312] mca: base: components_open: found loaded component slurm<br>[access1:04312] mca: base: components_open: component slurm open function successful<br>[access1:04312] mca:base:select: Auto-selecting plm components<br>[access1:04312] mca:base:select:( plm) Querying component [isolated]<br>[access1:04312] mca:base:select:( plm) Query of component [isolated] set priority to 0<br>[access1:04312] mca:base:select:( plm) Querying component [rsh]<br>[access1:04312] mca:base:select:( plm) Query of component [rsh] set priority to 10<br>[access1:04312] mca:base:select:( plm) Querying component [slurm]<br>[access1:04312] mca:base:select:( plm) Query of component [slurm] set priority to 75<br>[access1:04312] mca:base:select:( plm) Selected component [slurm]<br>[access1:04312] mca: base: close: component isolated closed<br>[access1:04312] mca: base: close: unloading component isolated<br>[access1:04312] mca: base: close: component rsh closed<br>[access1:04312] mca: base: close: unloading component rsh<br>Daemon was launched on node1-128-09 - beginning to initialize<br>Daemon was launched on node1-128-15 - beginning to initialize<br>Daemon [[39207,0],1] checking in as pid 26240 on host node1-128-09<br>[node1-128-09:26240] [[39207,0],1] orted: up and running - waiting for commands!<br>Daemon [[39207,0],2] checking in as pid 30129 on host node1-128-15<br>[node1-128-15:30129] [[39207,0],2] orted: up and running - waiting for commands!<br>srun: error: node1-128-09: task 0: Exited with exit code 1<br>srun: Terminating job step 645686.3<br>srun: error: node1-128-15: task 1: Exited with exit code 1<br>--------------------------------------------------------------------------<br>An ORTE daemon has unexpectedly failed after launch and before<br>communicating back to mpirun. This could be caused by a number<br>of factors, including an inability to create a connection back<br>to mpirun due to a lack of common network interfaces and/or no<br>route found between them. Please check network connectivity<br>(including firewalls and network routing requirements).<br>--------------------------------------------------------------------------<br>[access1:04312] [[39207,0],0] orted_cmd: received halt_vm cmd<br>[access1:04312] mca: base: close: component slurm closed<br>[access1:04312] mca: base: close: unloading component slurm</p><p><br><br></p></div></div></div></blockquote></div></div></div></div></div></div></blockquote><br>

</div>
			
		
		<base target="_self" href="https://e.mail.ru/">
	</div>

	
</div>



<br><hr>
<br><br></BODY></HTML>
