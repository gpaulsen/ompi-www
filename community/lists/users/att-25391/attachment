<div dir="ltr"><div></div>It returns /usr/bin/mpiexec.<br></div><div class="gmail_extra"><br><div class="gmail_quote">On Thu, Sep 25, 2014 at 8:57 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">Do &quot;which mpiexec&quot; and look at the path. The options you show are from MPICH, not OMPI.<div><br><div><div><div class="h5"><div>On Sep 25, 2014, at 12:15 AM, XingFENG &lt;<a href="mailto:xingfeng@cse.unsw.edu.au" target="_blank">xingfeng@cse.unsw.edu.au</a>&gt; wrote:</div><br></div></div><blockquote type="cite"><div><div class="h5"><div dir="ltr"><div><div>Hi Ralph,<br><br></div>Thanks for your reply.<br><br></div>I am not pretty sure about the version of mpiexec. The documentation claims that two mpi are installed, namely, OpenMPI and MPICH2.<br></div><div class="gmail_extra"><br><div class="gmail_quote">On Thu, Sep 25, 2014 at 11:45 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">No, it doesn&#39;t matter at all for OMPI - any order is fine. The issue I see is that your mpiexec isn&#39;t the OMPI one, but is from someone else. I have no idea whose mpiexec you are using<div><br><div><br><div><div><div><div>On Sep 24, 2014, at 6:38 PM, XingFENG &lt;<a href="mailto:xingfeng@cse.unsw.edu.au" target="_blank">xingfeng@cse.unsw.edu.au</a>&gt; wrote:</div><br></div></div><blockquote type="cite"><div style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px"><div><div><div dir="ltr">I have found the solution. The command<span> </span><i>mpirun -machinefile ./my_hosts -n 3 ./testMPI</i><span> </span>works. I think the order of arguments matters here.<br></div><div class="gmail_extra"><br><div class="gmail_quote">On Thu, Sep 25, 2014 at 11:02 AM, XingFENG<span> </span><span dir="ltr">&lt;<a href="mailto:xingfeng@cse.unsw.edu.au" target="_blank">xingfeng@cse.unsw.edu.au</a>&gt;</span><span> </span>wrote:<br><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div dir="ltr"><div><div>Hi all,<br><br></div>I got problem with running program on a cluster.<br></div>I used the following command.<span> </span><i>my_hosts</i><span> </span>is a file containing 3 hosts while<span> </span><i>testMPI</i><span> </span>is a very simple MPI program.<br><div>==========================================<br><i>mpirun -np 2 --hostfile ./my_hosts ./testMPI<br>mpirun -np 2 --machinefile ./my_hosts ./testMPI<br>mpirun -np 2 --f ./my_hosts ./testMPI<br></i>==========================================<br clear="all"><div><div><br></div><div>And the output is like this.<br>==========================================<br><i>invalid &quot;local&quot; arg: --hostfile<br><br>usage:<br>mpiexec [-h or -help or --help]    # get this message<br>mpiexec -file filename             # (or -f) filename contains XML job description<br>mpiexec [global args] [local args] executable [args]<br>  <span> </span>where global args may be<br>     <span> </span>-l                           # line labels by MPI rank<br>     <span> </span>-bnr                         # MPICH1 compatibility mode<br>     <span> </span>-machinefile                 # file mapping procs to machines<br>     <span> </span>-s &lt;spec&gt;                    # direct stdin to &quot;all&quot; or 1,2 or 2-4,6<span> </span><br>     <span> </span>-1                           # override default of trying 1st proc locally<br>     <span> </span>-ifhn                        # network interface to use locally<br>     <span> </span>-tv                          # run procs under totalview (must be installed)<br>     <span> </span>-tvsu                        # totalview startup only<br>     <span> </span>-gdb                         # run procs under gdb<br>     <span> </span>-m                           # merge output lines (default with gdb)<br>     <span> </span>-a                           # means assign this alias to the job<br>     <span> </span>-ecfn                        # output_xml_exit_codes_filename<br>     <span> </span>-recvtimeout &lt;integer_val&gt;   # timeout for recvs to fail (e.g. from mpd daemon)<br>     <span> </span>-g&lt;local arg name&gt;           # global version of local arg (below)<br>   <span> </span>and local args may be<br>     <span> </span>-n &lt;n&gt; or -np &lt;n&gt;            # number of processes to start<br>     <span> </span>-wdir &lt;dirname&gt;              # working directory to start in<br>     <span> </span>-umask &lt;umask&gt;               # umask for remote process<br>     <span> </span>-path &lt;dirname&gt;              # place to look for executables<br>     <span> </span>-host &lt;hostname&gt;             # host to start on<br>     <span> </span>-soft &lt;spec&gt;                 # modifier of -n value<br>     <span> </span>-arch &lt;arch&gt;                 # arch type to start on (not implemented)<br>     <span> </span>-envall                      # pass all env vars in current environment<br>     <span> </span>-envnone                     # pass no env vars<br>     <span> </span>-envlist &lt;list of env var names&gt; # pass current values of these vars<br>     <span> </span>-env &lt;name&gt; &lt;value&gt;          # pass this value of this env var<br>mpiexec [global args] [local args] executable args : [local args] executable...<br>mpiexec -gdba jobid                # gdb-attach to existing jobid<br>mpiexec -configfile filename       # filename contains cmd line segs as lines<br> <span> </span>(See User Guide for more details)<br><br>Examples:<br>  <span> </span>mpiexec -l -n 10 cpi 100<br>  <span> </span>mpiexec -genv QPL_LICENSE 4705 -n 3 a.out<br><br>  <span> </span>mpiexec -n 1 -host foo master : -n 4 -host mysmp slave</i><br><br>==========================================<br clear="all"><div><br><br></div><div>Another problem is that I cannot get the version of MPI. With command mpirun --version I got<br><br>==========================================<br><i>invalid &quot;local&quot; arg: --version<br><br>usage:<br>mpiexec [-h or -help or --help]    # get this message<br>mpiexec -file filename             # (or -f) filename contains XML job description<br>mpiexec [global args] [local args] executable [args]<br>  <span> </span>where global args may be<br>     <span> </span>-l                           # line labels by MPI rank<br>     <span> </span>-bnr                         # MPICH1 compatibility mode<br>     <span> </span>-machinefile                 # file mapping procs to machines<br>     <span> </span>-s &lt;spec&gt;                    # direct stdin to &quot;all&quot; or 1,2 or 2-4,6<span> </span><br>     <span> </span>-1                           # override default of trying 1st proc locally<br>     <span> </span>-ifhn                        # network interface to use locally<br>     <span> </span>-tv                          # run procs under totalview (must be installed)<br>     <span> </span>-tvsu                        # totalview startup only<br>     <span> </span>-gdb                         # run procs under gdb<br>     <span> </span>-m                           # merge output lines (default with gdb)<br>     <span> </span>-a                           # means assign this alias to the job<br>     <span> </span>-ecfn                        # output_xml_exit_codes_filename<br>     <span> </span>-recvtimeout &lt;integer_val&gt;   # timeout for recvs to fail (e.g. from mpd daemon)<br>     <span> </span>-g&lt;local arg name&gt;           # global version of local arg (below)<br>   <span> </span>and local args may be<br>     <span> </span>-n &lt;n&gt; or -np &lt;n&gt;            # number of processes to start<br>     <span> </span>-wdir &lt;dirname&gt;              # working directory to start in<br>     <span> </span>-umask &lt;umask&gt;               # umask for remote process<br>     <span> </span>-path &lt;dirname&gt;              # place to look for executables<br>     <span> </span>-host &lt;hostname&gt;             # host to start on<br>     <span> </span>-soft &lt;spec&gt;                 # modifier of -n value<br>     <span> </span>-arch &lt;arch&gt;                 # arch type to start on (not implemented)<br>     <span> </span>-envall                      # pass all env vars in current environment<br>     <span> </span>-envnone                     # pass no env vars<br>     <span> </span>-envlist &lt;list of env var names&gt; # pass current values of these vars<br>     <span> </span>-env &lt;name&gt; &lt;value&gt;          # pass this value of this env var<br>mpiexec [global args] [local args] executable args : [local args] executable...<br>mpiexec -gdba jobid                # gdb-attach to existing jobid<br>mpiexec -configfile filename       # filename contains cmd line segs as lines<br> <span> </span>(See User Guide for more details)<br><br>Examples:<br>  <span> </span>mpiexec -l -n 10 cpi 100<br>  <span> </span>mpiexec -genv QPL_LICENSE 4705 -n 3 a.out<br><br>  <span> </span>mpiexec -n 1 -host foo master : -n 4 -host mysmp slave</i><br><br><br>==========================================<br><br></div>Any help would be greatly appreciated!<span><font color="#888888"><br></font></span></div><span><font color="#888888"><div><br></div><div><br>--<span> </span><br><div dir="ltr">Best Regards.<br>---<br>Xing FENG<div>PhD Candidate<br>Database Research Group<br><br></div><div>School of Computer Science and Engineering<div>University of New South Wales<br></div>NSW 2052, Sydney<br></div><div><br></div><div>Phone:<span> </span><a href="tel:%28%2B61%29%20413%20857%20288" value="+61413857288" target="_blank">(+61) 413 857 288</a></div></div></div></font></span></div></div></div></blockquote></div><br><br clear="all"><br>--<span> </span><br><div dir="ltr">Best Regards.<br>---<br>Xing FENG<div>PhD Candidate<br>Database Research Group<br><br></div><div>School of Computer Science and Engineering<div>University of New South Wales<br></div>NSW 2052, Sydney<br></div><div><br></div><div>Phone: <a href="tel:%28%2B61%29%20413%20857%20288" value="+61413857288" target="_blank">(+61) 413 857 288</a></div></div></div></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>Subscription:<span> </span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post:<span> </span><a href="http://www.open-mpi.org/community/lists/users/2014/09/25386.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/09/25386.php</a></div></blockquote></div><br></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/09/25387.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/09/25387.php</a><br></blockquote></div><br><br clear="all"><br>-- <br><div dir="ltr">Best Regards.<br>---<br>Xing FENG<div>PhD Candidate<br>Database Research Group<br><br></div><div>School of Computer Science and Engineering<div>University of New South Wales<br></div>NSW 2052, Sydney<br></div><div><br></div><div>Phone: <a href="tel:%28%2B61%29%20413%20857%20288" value="+61413857288" target="_blank">(+61) 413 857 288</a></div></div>
</div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/09/25388.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/09/25388.php</a></blockquote></div><br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/09/25390.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/09/25390.php</a><br></blockquote></div><br><br clear="all"><br>-- <br><div dir="ltr">Best Regards.<br>---<br>Xing FENG<div>PhD Candidate<br>Database Research Group<br><br></div><div>School of Computer Science and Engineering<div>University of New South Wales<br></div>NSW 2052, Sydney<br></div><div><br></div><div>Phone: (+61) 413 857 288</div></div>
</div>
