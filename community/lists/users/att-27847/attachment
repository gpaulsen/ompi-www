<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
  </head>
  <body text="#000000" bgcolor="#FFFFFF">
    Dear Ralph, Gilles, and Jeff<br>
    <br>
    Thanks a lot for your effort.. Understanding this problem has been a
    very interesting exercise for me that let me understand OpenMPI much
    better (I think:). <br>
    <br>
    I have given it all a little more thought, and done some more tests
    on our production system, and I think that this is not exactly a
    corner-case. First of all, I suspect all of this holds for other job
    scheduling systems besides SLURM (to be thought about..). Moreover,
    on our system a rather common usage scenario involves SLURM job
    allocation using, e.g.,<br>
    <br>
    salloc --ntasks=32<br>
    <br>
    which results in very fragmented allocations - that's specific for
    the type of problems users use this cluster for, but it's a fact.
    Users then run the job using<br>
    <br>
    mpirun ./program<br>
    <br>
    For versions up to 1.10.0, with uneven resource allocation among
    compute nodes the default binding options used in OpenMPI in most
    cases result in some CPU cores not being present in the used cpuset
    at all, others being over/under-subscribed. This certainly is
    job-specific and depends on how fragmented the SLURM allocations
    are, but to give a scary number: in one case I started 512 tasks (1
    per core), and OpenMPI binding created a cpuset that used only 271
    cores, some of them being over/under-subscribed on top of that.
    Effectively, user gets 50% of what he asked for. As already
    discussed, this happens quietly - the user has no idea. <br>
    <br>
    For version 1.10.1rc1 and up the situation is a bit different: it
    seems that in many cases all cores are present in the cpuset, just
    that the binding does not take place in a lot of cases. Instead,
    processes are bound to all cores allocated by SLURM. In other
    scenarios, as discussed before, some cores are
    over/under-subscribed. Again, this is done quietly.<br>
    <br>
    In all cases what is needed is the --hetero-nodes switch. If I apply
    the patch that Gilles has posted, it seems to be enough for
    1.10.1rc1 and up. The switch is not enough for earlier versions of
    OpenMPI and one needs --map-by core in addition.<br>
    <br>
    Given all that I think some sort of fix would be in order soon. I
    agree with Ralph that to address this issue quickly a simplified fix
    would be a good choice. As Ralph has already pointed out (or at
    least how I understood it :) this would essentially involve
    activating --hetero-nodes by default, and using --map-by core in
    cases where the architecture is not homogeneous. Uncovering the
    warning so that the failure to bind is not silent is the last piece
    of puzzle. Maybe adding a sanity check to make sure all allocated
    resources are in use would be helpful - if not by default, then
    maybe with some flag.<br>
    <br>
    Does all this make sense?<br>
    <br>
    Again, thank you all for your help,<br>
    <br>
    Marcin<br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <div class="moz-cite-prefix">On 10/07/2015 04:03 PM, Ralph Castain
      wrote:<br>
    </div>
    <blockquote
      cite="mid:59043101-B4CA-4843-B329-2970AA686078@open-mpi.org"
      type="cite">
      <meta http-equiv="Content-Type" content="text/html;
        charset=windows-1252">
      I’m a little nervous about this one, Gilles. It’s doing a lot more
      than just addressing the immediate issue, and I’m concerned about
      any potential side-effects that we don’t fully unocver prior to
      release.
      <div class=""><br class="">
      </div>
      <div class="">I’d suggest a two-pronged approach:</div>
      <div class=""><br class="">
      </div>
      <div class="">1. use my alternative method for 1.10.1 to solve the
        immediate issue. It only affects this one, rather unusual,
        corner-case that was reported here. So the impact can be easily
        contained and won’t impact anything else.</div>
      <div class=""><br class="">
      </div>
      <div class="">2. push your proposed solution to the master where
        it can soak for awhile and give us a chance to fully discover
        the secondary effects. Removing the unused and “not-allowed”
        cpus from the topology means a substantial scrub of the code
        base in a number of places, and your patch doesn’t really get
        them all. It’s going to take time to ensure everything is
        working correctly again.</div>
      <div class=""><br class="">
      </div>
      <div class="">HTH</div>
      <div class="">Ralph</div>
      <div class=""><br class="">
        <div>
          <blockquote type="cite" class="">
            <div class="">On Oct 7, 2015, at 4:29 AM, Gilles
              Gouaillardet &lt;<a moz-do-not-send="true"
                href="mailto:gilles.gouaillardet@gmail.com" class="">gilles.gouaillardet@gmail.com</a>&gt;
              wrote:</div>
            <br class="Apple-interchange-newline">
            <div class="">Jeff,
              <div class=""><br class="">
              </div>
              <div class="">there are quite a lot of changes, I did not
                update master yet (need extra pairs of eyes to review
                this...)</div>
              <div class="">so unless you want to make rc2 today and rc3
                a week later, it is imho way safer to wait for v1.10.2</div>
              <div class=""><br class="">
              </div>
              <div class="">Ralph,</div>
              <div class="">any thoughts ?</div>
              <div class=""><br class="">
              </div>
              <div class="">Cheers,</div>
              <div class=""><br class="">
              </div>
              <div class="">Gilles<br class="">
                <br class="">
                On Wednesday, October 7, 2015, Jeff Squyres (jsquyres)
                &lt;<a moz-do-not-send="true"
                  href="mailto:jsquyres@cisco.com" class="">jsquyres@cisco.com</a>&gt;
                wrote:<br class="">
                <blockquote class="gmail_quote" style="margin:0 0 0
                  .8ex;border-left:1px #ccc solid;padding-left:1ex">Is
                  this something that needs to go into v1.10.1?<br
                    class="">
                  <br class="">
                  If so, a PR needs to be filed ASAP.  We were supposed
                  to make the next 1.10.1 RC yesterday, but slipped to
                  today due to some last second patches.<br class="">
                  <br class="">
                  <br class="">
                  &gt; On Oct 7, 2015, at 4:32 AM, Gilles Gouaillardet
                  &lt;<a moz-do-not-send="true" href="javascript:;"
                    onclick="_e(event, 'cvml', 'gilles@rist.or.jp')"
                    class="">gilles@rist.or.jp</a>&gt; wrote:<br
                    class="">
                  &gt;<br class="">
                  &gt; Marcin,<br class="">
                  &gt;<br class="">
                  &gt; here is a patch for the master, hopefully it
                  fixes all the issues we discussed<br class="">
                  &gt; i will make sure it applies fine vs latest 1.10
                  tarball from tomorrow<br class="">
                  &gt;<br class="">
                  &gt; Cheers,<br class="">
                  &gt;<br class="">
                  &gt; Gilles<br class="">
                  &gt;<br class="">
                  &gt;<br class="">
                  &gt; On 10/6/2015 7:22 PM, marcin.krotkiewski wrote:<br
                    class="">
                  &gt;&gt; Gilles,<br class="">
                  &gt;&gt;<br class="">
                  &gt;&gt; Yes, it seemed that all was fine with binding
                  in the patched 1.10.1rc1 - thank you. Eagerly waiting
                  for the other patches, let me know and I will test
                  them later this week.<br class="">
                  &gt;&gt;<br class="">
                  &gt;&gt; Marcin<br class="">
                  &gt;&gt;<br class="">
                  &gt;&gt;<br class="">
                  &gt;&gt;<br class="">
                  &gt;&gt; On 10/06/2015 12:09 PM, Gilles Gouaillardet
                  wrote:<br class="">
                  &gt;&gt;&gt; Marcin,<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; my understanding is that in this case,
                  patched v1.10.1rc1 is working just fine.<br class="">
                  &gt;&gt;&gt; am I right ?<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; I prepared two patches<br class="">
                  &gt;&gt;&gt; one to remove the warning when binding on
                  one core if only one core is available,<br class="">
                  &gt;&gt;&gt; an other one to add a warning if the user
                  asks a binding policy that makes no sense with the
                  required mapping policy<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; I will finalize them tomorrow hopefully<br
                    class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; Cheers,<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; Gilles<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; On Tuesday, October 6, 2015,
                  marcin.krotkiewski &lt;<a moz-do-not-send="true"
                    href="javascript:;" onclick="_e(event, 'cvml',
                    'marcin.krotkiewski@gmail.com')" class="">marcin.krotkiewski@gmail.com</a>&gt;
                  wrote:<br class="">
                  &gt;&gt;&gt; Hi, Gilles<br class="">
                  &gt;&gt;&gt;&gt; you mentionned you had one failure
                  with 1.10.1rc1 and -bind-to core<br class="">
                  &gt;&gt;&gt;&gt; could you please send the full
                  details (script, allocation and output)<br class="">
                  &gt;&gt;&gt;&gt; in your slurm script, you can do<br
                    class="">
                  &gt;&gt;&gt;&gt; srun -N $SLURM_NNODES -n
                  $SLURM_NNODES --cpu_bind=none -l grep
                  Cpus_allowed_list /proc/self/status<br class="">
                  &gt;&gt;&gt;&gt; before invoking mpirun<br class="">
                  &gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; It was an interactive job allocated with<br
                    class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; salloc --account=staff --ntasks=32
                  --mem-per-cpu=2G --time=120:0:0<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; The slurm environment is the following<br
                    class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; SLURM_JOBID=12714491<br class="">
                  &gt;&gt;&gt; SLURM_JOB_CPUS_PER_NODE='4,2,5(x2),4,7,5'<br
                    class="">
                  &gt;&gt;&gt; SLURM_JOB_ID=12714491<br class="">
                  &gt;&gt;&gt;
                  SLURM_JOB_NODELIST='c1-[2,4,8,13,16,23,26]'<br
                    class="">
                  &gt;&gt;&gt; SLURM_JOB_NUM_NODES=7<br class="">
                  &gt;&gt;&gt; SLURM_JOB_PARTITION=normal<br class="">
                  &gt;&gt;&gt; SLURM_MEM_PER_CPU=2048<br class="">
                  &gt;&gt;&gt; SLURM_NNODES=7<br class="">
                  &gt;&gt;&gt; SLURM_NODELIST='c1-[2,4,8,13,16,23,26]'<br
                    class="">
                  &gt;&gt;&gt; SLURM_NODE_ALIASES='(null)'<br class="">
                  &gt;&gt;&gt; SLURM_NPROCS=32<br class="">
                  &gt;&gt;&gt; SLURM_NTASKS=32<br class="">
                  &gt;&gt;&gt; SLURM_SUBMIT_DIR=/cluster/home/marcink<br
                    class="">
                  &gt;&gt;&gt; SLURM_SUBMIT_HOST=login-0-1.local<br
                    class="">
                  &gt;&gt;&gt; SLURM_TASKS_PER_NODE='4,2,5(x2),4,7,5'<br
                    class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; The output of the command you asked for
                  is<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; 0: c1-2.local  Cpus_allowed_list:       
                  1-4,17-20<br class="">
                  &gt;&gt;&gt; 1: c1-4.local  Cpus_allowed_list:       
                  1,15,17,31<br class="">
                  &gt;&gt;&gt; 2: c1-8.local  Cpus_allowed_list:       
                  0,5,9,13-14,16,21,25,29-30<br class="">
                  &gt;&gt;&gt; 3: c1-13.local  Cpus_allowed_list:     
                   3-7,19-23<br class="">
                  &gt;&gt;&gt; 4: c1-16.local  Cpus_allowed_list:     
                   12-15,28-31<br class="">
                  &gt;&gt;&gt; 5: c1-23.local  Cpus_allowed_list:     
                   2-4,8,13-15,18-20,24,29-31<br class="">
                  &gt;&gt;&gt; 6: c1-26.local  Cpus_allowed_list:     
                   1,6,11,13,15,17,22,27,29,31<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; Running with command<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; mpirun --mca rmaps_base_verbose 10
                  --hetero-nodes --bind-to core --report-bindings
                  --map-by socket -np 32 ./affinity<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; I have attached two output files: one for
                  the original 1.10.1rc1, one for the patched version.<br
                    class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; When I said 'failed in one case' I was
                  not precise. I got an error on node c1-8, which was
                  the first one to have different number of MPI
                  processes on the two sockets. It would also fail on
                  some later nodes, just                 that because of
                  the error we never got there.<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; Let me know if you need more.<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; Marcin<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt; Cheers,<br class="">
                  &gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt; Gilles<br class="">
                  &gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt; On 10/4/2015 11:55 PM,
                  marcin.krotkiewski wrote:<br class="">
                  &gt;&gt;&gt;&gt;&gt; Hi, all,<br class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt; I played a bit more and it seems
                  that the problem results from<br class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt; trg_obj =
                  opal_hwloc_base_find_min_bound_target_under_obj()<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt; called in rmaps_base_binding.c /
                  bind_downwards being wrong. I do not know the reason,
                  but I think I know when the problem happens (at least
                  on 1.10.1rc1). It seems that by default openmpi maps
                  by socket. The error happens when for a given compute
                  node there is a different number of cores used on each
                  socket. Consider previously studied case (the debug
                  outputs I sent in last post). c1-8, which was source
                  of error, has 5 mpi processes assigned, and the cpuset
                  is the following:<br class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt; 0, 5, 9, 13, 14, 16, 21, 25, 29,
                  30<br class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt; Cores 0,5 are on socket 0, cores
                  9, 13, 14 are on socket 1. Binding progresses
                  correctly up to and including core 13 (see end of file
                  out.1.10.1rc2, before the error). That is 2 cores on
                  socket 0, and 2 cores on socket 1. Error is thrown
                  when core 14 should be bound - extra core on socket 1
                  with no corresponding core on socket 0. At that point
                  the returned trg_obj points to the first core on the
                  node (os_index 0, socket 0).<br class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt; I have submitted a few other jobs
                  and I always had an error in such situation. Moreover,
                  if I now use --map-by core instead of socket, the
                  error is gone, and I get my expected binding:<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 0 @ compute-1-2.local  1,
                  17,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 1 @ compute-1-2.local  2,
                  18,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 2 @ compute-1-2.local  3,
                  19,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 3 @ compute-1-2.local  4,
                  20,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 4 @ compute-1-4.local  1,
                  17,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 5 @ compute-1-4.local  15,
                  31,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 6 @ compute-1-8.local  0,
                  16,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 7 @ compute-1-8.local  5,
                  21,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 8 @ compute-1-8.local  9,
                  25,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 9 @ compute-1-8.local  13,
                  29,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 10 @ compute-1-8.local  14,
                  30,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 11 @ compute-1-13.local  3,
                  19,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 12 @ compute-1-13.local  4,
                  20,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 13 @ compute-1-13.local  5,
                  21,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 14 @ compute-1-13.local  6,
                  22,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 15 @ compute-1-13.local  7,
                  23,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 16 @ compute-1-16.local  12,
                  28,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 17 @ compute-1-16.local  13,
                  29,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 18 @ compute-1-16.local  14,
                  30,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 19 @ compute-1-16.local  15,
                  31,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 20 @ compute-1-23.local  2,
                  18,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 29 @ compute-1-26.local  11,
                  27,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 21 @ compute-1-23.local  3,
                  19,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 30 @ compute-1-26.local  13,
                  29,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 22 @ compute-1-23.local  4,
                  20,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 31 @ compute-1-26.local  15,
                  31,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 23 @ compute-1-23.local  8,
                  24,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 27 @ compute-1-26.local  1,
                  17,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 24 @ compute-1-23.local  13,
                  29,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 28 @ compute-1-26.local  6,
                  22,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 25 @ compute-1-23.local  14,
                  30,<br class="">
                  &gt;&gt;&gt;&gt;&gt; rank 26 @ compute-1-23.local  15,
                  31,<br class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt; Using --map-by core seems to fix
                  the issue on 1.8.8, 1.10.0 and 1.10.1rc1. However,
                  there is still a difference in behavior between
                  1.10.1rc1 and earlier versions. In the SLURM job
                  described in last post, 1.10.1rc1 fails to bind only
                  in 1 case, while the earlier versions fail in 21 out
                  of 32 cases. You mentioned there was a bug in hwloc.
                  Not sure if it can explain the difference in behavior.<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt; Hope this helps to nail this
                  down.<br class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt; Marcin<br class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt; On 10/04/2015 09:55 AM, Gilles
                  Gouaillardet wrote:<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt; Ralph,<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt; I suspect ompi tries to bind
                  to threads outside the cpuset.<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt; this could be pretty similar
                  to a previous issue when ompi tried to bind to cores
                  outside the cpuset.<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt; /* when a core has more than
                  one thread, would ompi assume all the threads are
                  available if the core is available ? */<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt; I will investigate this from
                  tomorrow<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt; Cheers,<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt; Gilles<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt; On Sunday, October 4, 2015,
                  Ralph Castain &lt;<a moz-do-not-send="true"
                    href="javascript:;" onclick="_e(event, 'cvml',
                    'rhc@open-mpi.org')" class="">rhc@open-mpi.org</a>&gt;
                  wrote:<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt; Thanks - please go ahead and
                  release that allocation as I’m not going to get to
                  this immediately. I’ve got several hot irons in the
                  fire right now, and I’m not sure when I’ll get a
                  chance to track this down.<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt; Gilles or anyone else who
                  might have time - feel free to take a gander and see
                  if something pops out at you.<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt; Ralph<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; On Oct 3, 2015, at 11:05
                  AM, marcin.krotkiewski &lt;<a moz-do-not-send="true"
                    href="javascript:;" onclick="_e(event, 'cvml',
                    'marcin.krotkiewski@gmail.com')" class="">marcin.krotkiewski@gmail.com</a>&gt;
                  wrote:<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; Done. I have compiled
                  1.10.0 and 1.10.rc1 with --enable-debug and executed<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; mpirun --mca
                  rmaps_base_verbose 10 --hetero-nodes --report-bindings
                  --bind-to core -np 32 ./affinity<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; In case of 1.10.rc1 I
                  have also added :overload-allowed - output in a
                  separate file. This option did not make much
                  difference for 1.10.0, so I did not attach it here.<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; First thing I noted for
                  1.10.0 are lines like<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; [login-0-1.local:03399]
                  [[37945,0],0] GOT 1 CPUS<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; [login-0-1.local:03399]
                  [[37945,0],0] PROC [[37945,1],27] BITMAP<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; [login-0-1.local:03399]
                  [[37945,0],0] PROC [[37945,1],27] ON c1-26 IS NOT
                  BOUND<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; with an empty BITMAP.<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; The SLURM environment is<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; set | grep SLURM<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; SLURM_JOBID=12714491<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  SLURM_JOB_CPUS_PER_NODE='4,2,5(x2),4,7,5'<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; SLURM_JOB_ID=12714491<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  SLURM_JOB_NODELIST='c1-[2,4,8,13,16,23,26]'<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; SLURM_JOB_NUM_NODES=7<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  SLURM_JOB_PARTITION=normal<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; SLURM_MEM_PER_CPU=2048<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; SLURM_NNODES=7<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  SLURM_NODELIST='c1-[2,4,8,13,16,23,26]'<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  SLURM_NODE_ALIASES='(null)'<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; SLURM_NPROCS=32<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; SLURM_NTASKS=32<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  SLURM_SUBMIT_DIR=/cluster/home/marcink<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  SLURM_SUBMIT_HOST=login-0-1.local<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  SLURM_TASKS_PER_NODE='4,2,5(x2),4,7,5'<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; I have submitted an
                  interactive job on screen for 120 hours now to work
                  with one example, and not change it for every post :)<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; If you need anything
                  else, let me know. I could introduce some
                  patch/printfs and recompile, if you need it.<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; Marcin<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt; On 10/03/2015 07:17 PM,
                  Ralph Castain wrote:<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Rats - just realized
                  I have no way to test this as none of the machines I
                  can access are setup for cgroup-based multi-tenant. Is
                  this a debug version of OMPI? If not, can you rebuild
                  OMPI with —enable-debug?<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Then please run it
                  with —mca rmaps_base_verbose 10 and pass along the
                  output.<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Thanks<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Ralph<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; On Oct 3, 2015,
                  at 10:09 AM, Ralph Castain &lt;<a
                    moz-do-not-send="true" href="javascript:;"
                    onclick="_e(event, 'cvml', 'rhc@open-mpi.org')"
                    class=""><a class="moz-txt-link-abbreviated" href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a></a>&gt; wrote:<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; What version of
                  slurm is this? I might try to debug it here. I’m not
                  sure where the problem lies just yet.<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; On Oct 3,
                  2015, at 8:59 AM, marcin.krotkiewski &lt;<a
                    moz-do-not-send="true" href="javascript:;"
                    onclick="_e(event, 'cvml',
                    'marcin.krotkiewski@gmail.com')" class=""><a class="moz-txt-link-abbreviated" href="mailto:marcin.krotkiewski@gmail.com">marcin.krotkiewski@gmail.com</a></a>&gt;
                  wrote:<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Here is the
                  output of lstopo. In short, (0,16) are core 0, (1,17)
                  - core 1 etc.<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Machine
                  (64GB)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;   NUMANode
                  L#0 (P#0 32GB)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;     Socket
                  L#0 + L3 L#0 (20MB)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       L2 L#0
                  (256KB) + L1d L#0 (32KB) + L1i L#0 (32KB) + Core L#0<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PU
                  L#0 (P#0)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PU
                  L#1 (P#16)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       L2 L#1
                  (256KB) + L1d L#1 (32KB) + L1i L#1 (32KB) + Core L#1<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PU
                  L#2 (P#1)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PU
                  L#3 (P#17)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       L2 L#2
                  (256KB) + L1d L#2 (32KB) + L1i L#2 (32KB) + Core L#2<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PU
                  L#4 (P#2)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PU
                  L#5 (P#18)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       L2 L#3
                  (256KB) + L1d L#3 (32KB) + L1i L#3 (32KB) + Core L#3<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PU
                  L#6 (P#3)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PU
                  L#7 (P#19)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       L2 L#4
                  (256KB) + L1d L#4 (32KB) + L1i L#4 (32KB) + Core L#4<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PU
                  L#8 (P#4)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PU
                  L#9 (P#20)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       L2 L#5
                  (256KB) + L1d L#5 (32KB) + L1i L#5 (32KB) + Core L#5<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PU
                  L#10 (P#5)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PU
                  L#11 (P#21)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       L2 L#6
                  (256KB) + L1d L#6 (32KB) + L1i L#6 (32KB) + Core L#6<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PU
                  L#12 (P#6)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PU
                  L#13 (P#22)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       L2 L#7
                  (256KB) + L1d L#7 (32KB) + L1i L#7 (32KB) + Core L#7<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PU
                  L#14 (P#7)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PU
                  L#15 (P#23)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;   
                   HostBridge L#0<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;     
                   PCIBridge<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PCI
                  8086:1521<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;           Net
                  L#0 "eth0"<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PCI
                  8086:1521<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;           Net
                  L#1 "eth1"<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;     
                   PCIBridge<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PCI
                  15b3:1003<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;           Net
                  L#2 "ib0"<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         
                   OpenFabrics L#3 "mlx4_0"<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;     
                   PCIBridge<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         PCI
                  102b:0532<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       PCI
                  8086:1d02<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;         Block
                  L#4 "sda"<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;   NUMANode
                  L#1 (P#1 32GB) + Socket L#1 + L3 L#1 (20MB)<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;     L2 L#8
                  (256KB) + L1d L#8 (32KB) + L1i L#8 (32KB) + Core L#8<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       PU L#16
                  (P#8)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       PU L#17
                  (P#24)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;     L2 L#9
                  (256KB) + L1d L#9 (32KB) + L1i L#9 (32KB) + Core L#9<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       PU L#18
                  (P#9)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       PU L#19
                  (P#25)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;     L2 L#10
                  (256KB) + L1d L#10 (32KB) + L1i L#10 (32KB) + Core
                  L#10<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       PU L#20
                  (P#10)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       PU L#21
                  (P#26)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;     L2 L#11
                  (256KB) + L1d L#11 (32KB) + L1i L#11 (32KB) + Core
                  L#11<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       PU L#22
                  (P#11)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       PU L#23
                  (P#27)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;     L2 L#12
                  (256KB) + L1d L#12 (32KB) + L1i L#12 (32KB) + Core
                  L#12<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       PU L#24
                  (P#12)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       PU L#25
                  (P#28)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;     L2 L#13
                  (256KB) + L1d L#13 (32KB) + L1i L#13 (32KB) + Core
                  L#13<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       PU L#26
                  (P#13)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       PU L#27
                  (P#29)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;     L2 L#14
                  (256KB) + L1d L#14 (32KB) + L1i L#14 (32KB) + Core
                  L#14<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       PU L#28
                  (P#14)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       PU L#29
                  (P#30)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;     L2 L#15
                  (256KB) + L1d L#15 (32KB) + L1i L#15 (32KB) + Core
                  L#15<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       PU L#30
                  (P#15)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;       PU L#31
                  (P#31)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; On 10/03/2015
                  05:46 PM, Ralph Castain wrote:<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Maybe I’m
                  just misreading your HT map - that slurm nodelist
                  syntax is a new one to me, but they tend to change
                  things around. Could you run lstopo on one of those
                  compute nodes and send the output?<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; I’m just
                  suspicious because I’m not seeing a clear pairing of
                  HT numbers in your output, but HT numbering is
                  BIOS-specific and I may just not be understanding your
                  particular pattern. Our error message is clearly
                  indicating that we are seeing individual HTs (and not
                  complete cores) assigned, and I don’t know the source
                  of that confusion.<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; On
                  Oct 3, 2015, at 8:28 AM, marcin.krotkiewski &lt;<a
                    moz-do-not-send="true" href="javascript:;"
                    onclick="_e(event, 'cvml',
                    'marcin.krotkiewski@gmail.com')" class=""><a class="moz-txt-link-abbreviated" href="mailto:marcin.krotkiewski@gmail.com">marcin.krotkiewski@gmail.com</a></a>&gt;
                  wrote:<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; On
                  10/03/2015 04:38 PM, Ralph Castain wrote:<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  If mpirun isn’t trying to do any binding, then you
                  will of course get the right mapping as we’ll just
                  inherit whatever we received.<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Yes.
                  I meant that whatever you received (what SLURM gives)
                  is a correct cpu map and assigns _whole_ CPUs, not a
                  single HT to MPI processes. In the case mentioned
                  earlier openmpi should start 6 tasks on c1-30. If HT
                  would be treated as separate and independent cores,
                  sched_getaffinity of an MPI process started on c1-30
                  would return a map with 6 entries only. In my case it
                  returns a map                                         
                                   with 12 entries - 2 for each core. So
                  one  process is in fact allocated both HTs, not only
                  one. Is what I'm saying correct?<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  Looking at your output, it’s pretty clear that you are
                  getting independent HTs assigned and not full cores.<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; How
                  do you mean? Is the above understanding wrong? I would
                  expect that on c1-30 with --bind-to core openmpi
                  should bind to logical cores 0 and 16 (rank 0), 1 and
                  17 (rank 2) and so on. All those logical cores are
                  available in sched_getaffinity map, and there is twice
                  as many logical cores as there are MPI processes
                  started on the node.<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  My guess is that something in slurm has changed such
                  that it detects that HT has been enabled, and then
                  begins treating the HTs as completely independent
                  cpus.<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  Try changing “-bind-to core” to “-bind-to hwthread 
                  -use-hwthread-cpus” and see if that works<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; I
                  have and the binding is wrong. For example, I got this
                  output<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; rank
                  0 @ compute-1-30.local  0,<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; rank
                  1 @ compute-1-30.local  16,<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Which
                  means that two ranks have been bound to the same
                  physical core (logical cores 0 and 16 are two HTs of
                  the same core). If I use --bind-to core, I get the
                  following correct binding<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; rank
                  0 @ compute-1-30.local  0, 16,<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; The
                  problem is many other ranks get bad binding with 'rank
                  XXX is not bound (or bound to all available
                  processors)' warning.<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; But I
                  think I was not entirely correct saying that 1.10.1rc1
                  did not fix things. It still might have improved
                  something, but not everything. Consider this job:<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  SLURM_JOB_CPUS_PER_NODE='5,4,6,5(x2),7,5,9,5,7,6'<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  SLURM_JOB_NODELIST='c8-[31,34],c9-[30-32,35-36],c10-[31-34]'<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; If I
                  run 32 tasks as follows (with 1.10.1rc1)<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  mpirun --hetero-nodes --report-bindings --bind-to core
                  -np 32 ./affinity<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; I get
                  the following error:<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
--------------------------------------------------------------------------<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; A
                  request was made to bind to that would result in
                  binding more<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  processes than cpus on a resource:<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;   
                  Bind to:     CORE<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;   
                  Node:        c9-31<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;   
                  #processes:  2<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;   
                  #cpus:       1<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; You
                  can override this protection by adding the
                  "overload-allowed"<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  option to your binding directive.<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
--------------------------------------------------------------------------<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; If I
                  now use --bind-to core:overload-allowed, then openmpi
                  starts and _most_ of the threads are bound correctly
                  (i.e., map contains two logical cores in ALL cases),
                  except this case that required the overload flag:<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; rank
                  15 @ compute-9-31.local   1, 17,<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; rank
                  16 @ compute-9-31.local  11, 27,<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; rank
                  17 @ compute-9-31.local   2, 18,<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; rank
                  18 @ compute-9-31.local  12, 28,<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; rank
                  19 @ compute-9-31.local   1, 17,<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Note
                  pair 1,17 is used twice. The original SLURM delivered
                  map (no binding) on this node is<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; rank
                  15 @ compute-9-31.local  1, 2, 11, 12, 13, 17, 18, 27,
                  28, 29,<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; rank
                  16 @ compute-9-31.local  1, 2, 11, 12, 13, 17, 18, 27,
                  28, 29,<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; rank
                  17 @ compute-9-31.local  1, 2, 11, 12, 13, 17, 18, 27,
                  28, 29,<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; rank
                  18 @ compute-9-31.local  1, 2, 11, 12, 13, 17, 18, 27,
                  28, 29,<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; rank
                  19 @ compute-9-31.local  1, 2, 11, 12, 13, 17, 18, 27,
                  28, 29,<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Why
                  does openmpi use cores (1,17) twice instead of using
                  core (13,29)? Clearly, the original SLURM-delivered
                  map has 5 CPUs included, enough for 5 MPI processes.<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  Cheers,<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  Marcin<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  On Oct 3, 2015, at 7:12 AM, marcin.krotkiewski &lt;<a
                    moz-do-not-send="true" href="javascript:;"
                    onclick="_e(event, 'cvml',
                    'marcin.krotkiewski@gmail.com')" class=""><a class="moz-txt-link-abbreviated" href="mailto:marcin.krotkiewski@gmail.com">marcin.krotkiewski@gmail.com</a></a>&gt;
                  wrote:<br class="">
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  On 10/03/2015 01:06 PM, Ralph Castain wrote:<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  Thanks Marcin. Looking at this, I’m guessing that
                  Slurm may be treating HTs as “cores” - i.e., as
                  independent cpus. Any chance that is true?<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
                  Not to the best of my knowledge, and at least not
                  intentionally. SLURM starts as many processes as there
                  are physical cores, not threads. To verify this,
                  consider this test case:<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;
                  _______________________________________________<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt; users mailing list<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt; <a moz-do-not-send="true"
                    href="javascript:;" onclick="_e(event, 'cvml',
                    'users@open-mpi.org')" class="">users@open-mpi.org</a><br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt; Subscription:<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt; <a moz-do-not-send="true"
                    href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
                    target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;&gt; Link to this post:<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;&gt; <a moz-do-not-send="true"
                    href="http://www.open-mpi.org/community/lists/users/2015/10/27790.php"
                    target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/10/27790.php</a><br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt;
                  _______________________________________________<br
                    class="">
                  &gt;&gt;&gt;&gt;&gt; users mailing list<br class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt; <a moz-do-not-send="true"
                    href="javascript:;" onclick="_e(event, 'cvml',
                    'users@open-mpi.org')" class="">users@open-mpi.org</a><br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt; Subscription:<br class="">
                  &gt;&gt;&gt;&gt;&gt; <a moz-do-not-send="true"
                    href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
                    target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br
                    class="">
                  &gt;&gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;&gt; Link to this post:<br class="">
                  &gt;&gt;&gt;&gt;&gt; <a moz-do-not-send="true"
                    href="http://www.open-mpi.org/community/lists/users/2015/10/27791.php"
                    target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/10/27791.php</a><br
                    class="">
                  &gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt;
                  _______________________________________________<br
                    class="">
                  &gt;&gt;&gt;&gt; users mailing list<br class="">
                  &gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt; <a moz-do-not-send="true"
                    href="javascript:;" onclick="_e(event, 'cvml',
                    'users@open-mpi.org')" class="">users@open-mpi.org</a><br
                    class="">
                  &gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt; Subscription:<br class="">
                  &gt;&gt;&gt;&gt; <a moz-do-not-send="true"
                    href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
                    target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br
                    class="">
                  &gt;&gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;&gt; Link to this post:<br class="">
                  &gt;&gt;&gt;&gt; <a moz-do-not-send="true"
                    href="http://www.open-mpi.org/community/lists/users/2015/10/27792.php"
                    target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/10/27792.php</a><br
                    class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt;
                  _______________________________________________<br
                    class="">
                  &gt;&gt;&gt; users mailing list<br class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; <a moz-do-not-send="true"
                    href="javascript:;" onclick="_e(event, 'cvml',
                    'users@open-mpi.org')" class="">users@open-mpi.org</a><br
                    class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; Subscription:<br class="">
                  &gt;&gt;&gt; <a moz-do-not-send="true"
                    href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
                    target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br
                    class="">
                  &gt;&gt;&gt;<br class="">
                  &gt;&gt;&gt; Link to this post:<br class="">
                  &gt;&gt;&gt; <a moz-do-not-send="true"
                    href="http://www.open-mpi.org/community/lists/users/2015/10/27814.php"
                    target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/10/27814.php</a><br
                    class="">
                  &gt;&gt;<br class="">
                  &gt;&gt;<br class="">
                  &gt;&gt;<br class="">
                  &gt;&gt;
                  _______________________________________________<br
                    class="">
                  &gt;&gt; users mailing list<br class="">
                  &gt;&gt;<br class="">
                  &gt;&gt; <a moz-do-not-send="true"
                    href="javascript:;" onclick="_e(event, 'cvml',
                    'users@open-mpi.org')" class="">users@open-mpi.org</a><br
                    class="">
                  &gt;&gt;<br class="">
                  &gt;&gt; Subscription:<br class="">
                  &gt;&gt; <a moz-do-not-send="true"
                    href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
                    target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br
                    class="">
                  &gt;&gt;<br class="">
                  &gt;&gt; Link to this post:<br class="">
                  &gt;&gt; <a moz-do-not-send="true"
                    href="http://www.open-mpi.org/community/lists/users/2015/10/27815.php"
                    target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/10/27815.php</a><br
                    class="">
                  &gt;<br class="">
                  &gt;
&lt;heterogeneous_topologies.patch&gt;_______________________________________________<br
                    class="">
                  &gt; users mailing list<br class="">
                  &gt; <a moz-do-not-send="true" href="javascript:;"
                    onclick="_e(event, 'cvml', 'users@open-mpi.org')"
                    class="">users@open-mpi.org</a><br class="">
                  &gt; Subscription: <a moz-do-not-send="true"
                    href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
                    target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br
                    class="">
                  &gt; Link to this post: <a moz-do-not-send="true"
                    href="http://www.open-mpi.org/community/lists/users/2015/10/27827.php"
                    target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/10/27827.php</a><br
                    class="">
                  <br class="">
                  <br class="">
                  --<br class="">
                  Jeff Squyres<br class="">
                  <a moz-do-not-send="true" href="javascript:;"
                    onclick="_e(event, 'cvml', 'jsquyres@cisco.com')"
                    class="">jsquyres@cisco.com</a><br class="">
                  For corporate legal information go to: <a
                    moz-do-not-send="true"
                    href="http://www.cisco.com/web/about/doing_business/legal/cri/"
                    target="_blank" class=""><a class="moz-txt-link-freetext" href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a></a><br
                    class="">
                  <br class="">
                  _______________________________________________<br
                    class="">
                  users mailing list<br class="">
                  <a moz-do-not-send="true" href="javascript:;"
                    onclick="_e(event, 'cvml', 'users@open-mpi.org')"
                    class="">users@open-mpi.org</a><br class="">
                  Subscription: <a moz-do-not-send="true"
                    href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
                    target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br
                    class="">
                  Link to this post: <a moz-do-not-send="true"
                    href="http://www.open-mpi.org/community/lists/users/2015/10/27828.php"
                    target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/10/27828.php</a><br
                    class="">
                </blockquote>
              </div>
              _______________________________________________<br
                class="">
              users mailing list<br class="">
              <a moz-do-not-send="true" href="mailto:users@open-mpi.org"
                class="">users@open-mpi.org</a><br class="">
              Subscription:
              <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br
                class="">
              Link to this post:
              <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2015/10/27830.php">http://www.open-mpi.org/community/lists/users/2015/10/27830.php</a></div>
          </blockquote>
        </div>
        <br class="">
      </div>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2015/10/27834.php">http://www.open-mpi.org/community/lists/users/2015/10/27834.php</a></pre>
    </blockquote>
    <br>
  </body>
</html>

