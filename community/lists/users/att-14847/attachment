Hi there,<div><br></div><div>I had a few suggestions in the previous few threads and I have been trying a lot of things to coax mpirun to work. But it does not want to. I am pasting an error file and ompi_info </div><div><br>
</div><div>ERROR FILE:</div><div><br></div><div><div>[uinta-0027:03360] MPI_ABORT invoked on rank 0 in communicator MPI_COMM_WORLD w\</div><div>ith errorcode 4776233</div><div>[uinta-0027:03357] [0,0,0]-[0,1,4] mca_oob_tcp_msg_recv: readv failed: Connecti\</div>
<div>on reset by peer (104)</div><div>mpirun noticed that job rank 1 with PID 3361 on node uinta-0027 exited on signa\</div><div>l 15 (Terminated).</div></div><div><br></div><div>ompi_info:</div><div><br></div><div><div>    Open MPI: 1.2.7</div>
<div>   Open MPI SVN revision: r19401</div><div>                Open RTE: 1.2.7</div><div>   Open RTE SVN revision: r19401</div><div>                    OPAL: 1.2.7</div><div>       OPAL SVN revision: r19401</div><div>                  Prefix: /opt/libraries/openmpi/openmpi-1.2.7-pgi</div>
<div> Configured architecture: x86_64-unknown-linux-gnu</div><div>           Configured by: A00017402</div><div>           Configured on: Thu Sep 18 15:00:05 MDT 2008</div><div>          Configure host: <a href="http://volvox.hpc.usu.edu">volvox.hpc.usu.edu</a></div>
<div>                Built by: A00017402</div><div>                Built on: Thu Sep 18 15:20:06 MDT 2008</div><div>              Built host: <a href="http://volvox.hpc.usu.edu">volvox.hpc.usu.edu</a></div><div>              C bindings: yes</div>
<div>            C++ bindings: yes</div><div>      Fortran77 bindings: yes (all)</div><div>      Fortran90 bindings: yes</div><div> Fortran90 bindings size: large</div><div>              C compiler: pgcc</div><div>     C compiler absolute: /opt/apps/pgi/linux86-64/7.2/bin/pgcc</div>
<div>            C++ compiler: pgCC</div><div>   C++ compiler absolute: /opt/apps/pgi/linux86-64/7.2/bin/pgCC</div><div>      Fortran77 compiler: pgf77</div><div>  Fortran77 compiler abs: /opt/apps/pgi/linux86-64/7.2/bin/pgf77</div>
<div>      Fortran90 compiler: pgf90</div><div>  Fortran90 compiler abs: /opt/apps/pgi/linux86-64/7.2/bin/pgf90</div><div>             C profiling: yes</div><div>           C++ profiling: yes</div><div>     Fortran77 profiling: yes</div>
<div>     Fortran90 profiling: yes</div><div>          C++ exceptions: no</div><div>          Thread support: posix (mpi: no, progress: no)</div><div>  Internal debug support: no</div><div>     MPI parameter check: runtime</div>
<div>Memory profiling support: no</div><div>Memory debugging support: no</div><div>         libltdl support: yes</div><div>   Heterogeneous support: yes</div><div> mpirun default --prefix: no</div><div>           MCA backtrace: execinfo (MCA v1.0, API v1.0, Component v1.2.7)</div>
<div>              MCA memory: ptmalloc2 (MCA v1.0, API v1.0, Component v1.2.7)</div><div>           MCA paffinity: linux (MCA v1.0, API v1.0, Component v1.2.7)</div><div>           MCA maffinity: first_use (MCA v1.0, API v1.0, Component v1.2.7)</div>
<div>           MCA maffinity: libnuma (MCA v1.0, API v1.0, Component v1.2.7)</div><div>               MCA timer: linux (MCA v1.0, API v1.0, Component v1.2.7)</div><div>         MCA installdirs: env (MCA v1.0, API v1.0, Component v1.2.7)</div>
<div>         MCA installdirs: config (MCA v1.0, API v1.0, Component v1.2.7)</div><div>           MCA allocator: basic (MCA v1.0, API v1.0, Component v1.0)</div><div>           MCA allocator: bucket (MCA v1.0, API v1.0, Component v1.0)</div>
<div>                MCA coll: basic (MCA v1.0, API v1.0, Component v1.2.7)</div><div>                MCA coll: self (MCA v1.0, API v1.0, Component v1.2.7)</div><div>                MCA coll: sm (MCA v1.0, API v1.0, Component v1.2.7)</div>
<div>                MCA coll: tuned (MCA v1.0, API v1.0, Component v1.2.7)</div><div>                  MCA io: romio (MCA v1.0, API v1.0, Component v1.2.7)</div><div>               MCA mpool: rdma (MCA v1.0, API v1.0, Component v1.2.7)</div>
<div>               MCA mpool: sm (MCA v1.0, API v1.0, Component v1.2.7)</div><div>                 MCA pml: cm (MCA v1.0, API v1.0, Component v1.2.7)</div><div>                 MCA pml: ob1 (MCA v1.0, API v1.0, Component v1.2.7)</div>
<div>                 MCA bml: r2 (MCA v1.0, API v1.0, Component v1.2.7)</div><div>              MCA rcache: vma (MCA v1.0, API v1.0, Component v1.2.7)</div><div>                 MCA btl: gm (MCA v1.0, API v1.0.1, Component v1.2.7)</div>
<div>                 MCA btl: self (MCA v1.0, API v1.0.1, Component v1.2.7)</div><div>                 MCA btl: sm (MCA v1.0, API v1.0.1, Component v1.2.7)</div><div>                 MCA btl: tcp (MCA v1.0, API v1.0.1, Component v1.0)</div>
<div>                MCA topo: unity (MCA v1.0, API v1.0, Component v1.2.7)</div><div>                 MCA osc: pt2pt (MCA v1.0, API v1.0, Component v1.2.7)</div><div>              MCA errmgr: hnp (MCA v1.0, API v1.3, Component v1.2.7)</div>
<div>              MCA errmgr: orted (MCA v1.0, API v1.3, Component v1.2.7)</div><div>              MCA errmgr: proxy (MCA v1.0, API v1.3, Component v1.2.7)</div><div>                 MCA gpr: null (MCA v1.0, API v1.0, Component v1.2.7)</div>
<div>                 MCA gpr: proxy (MCA v1.0, API v1.0, Component v1.2.7)</div><div>                 MCA gpr: replica (MCA v1.0, API v1.0, Component v1.2.7)</div><div>                 MCA iof: proxy (MCA v1.0, API v1.0, Component v1.2.7)</div>
<div>                 MCA iof: svc (MCA v1.0, API v1.0, Component v1.2.7)</div><div>                  MCA ns: proxy (MCA v1.0, API v2.0, Component v1.2.7)</div><div>                  MCA ns: replica (MCA v1.0, API v2.0, Component v1.2.7)</div>
<div>                 MCA oob: tcp (MCA v1.0, API v1.0, Component v1.0)</div><div>                 MCA ras: dash_host (MCA v1.0, API v1.3, Component v1.2.7)</div><div>                 MCA ras: gridengine (MCA v1.0, API v1.3, Component v1.2.7)</div>
<div>                 MCA ras: localhost (MCA v1.0, API v1.3, Component v1.2.7)</div><div>                 MCA ras: slurm (MCA v1.0, API v1.3, Component v1.2.7)</div><div>                 MCA ras: tm (MCA v1.0, API v1.3, Component v1.2.7)</div>
<div>                 MCA rds: hostfile (MCA v1.0, API v1.3, Component v1.2.7)</div><div>                 MCA rds: proxy (MCA v1.0, API v1.3, Component v1.2.7)</div><div>                 MCA rds: resfile (MCA v1.0, API v1.3, Component v1.2.7)</div>
<div>               MCA rmaps: round_robin (MCA v1.0, API v1.3, Component v1.2.7)</div><div>                MCA rmgr: proxy (MCA v1.0, API v2.0, Component v1.2.7)</div><div>                MCA rmgr: urm (MCA v1.0, API v2.0, Component v1.2.7)</div>
<div>                 MCA rml: oob (MCA v1.0, API v1.0, Component v1.2.7)</div><div>                 MCA pls: gridengine (MCA v1.0, API v1.3, Component v1.2.7)</div><div>                 MCA pls: proxy (MCA v1.0, API v1.3, Component v1.2.7)</div>
<div>                 MCA pls: rsh (MCA v1.0, API v1.3, Component v1.2.7)</div><div>                 MCA pls: slurm (MCA v1.0, API v1.3, Component v1.2.7)</div><div>                 MCA pls: tm (MCA v1.0, API v1.3, Component v1.2.7)</div>
<div>                 MCA sds: env (MCA v1.0, API v1.0, Component v1.2.7)</div><div>                 MCA sds: pipe (MCA v1.0, API v1.0, Component v1.2.7)</div><div>                 MCA sds: seed (MCA v1.0, API v1.0, Component v1.2.7)</div>
<div>                 MCA sds: singleton (MCA v1.0, API v1.0, Component v1.2.7)</div><div>                 MCA sds: slurm (MCA v1.0, API v1.0, Component v1.2.7)</div></div><div><br></div><div>Can anyone please look at the files and tell me what to do?</div>
<div><br></div><div>Tushar</div><div><br></div>

