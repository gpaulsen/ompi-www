<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
  </head>
  <body text="#000000" bgcolor="#FFFFFF">
    <br>
    <br>
    <div class="moz-cite-prefix">On 10/20/2015 04:14 PM, Ralph Castain
      wrote:<br>
    </div>
    <blockquote
      cite="mid:44D83A4E-6ED9-4240-831F-213B283087EA@open-mpi.org"
      type="cite">
      <meta http-equiv="Content-Type" content="text/html;
        charset=windows-1252">
      <br class="">
      <div>
        <blockquote type="cite" class="">
          <div class="">On Oct 20, 2015, at 5:47 AM, Daniel Letai &lt;<a
              moz-do-not-send="true" href="mailto:dani@letai.org.il"
              class=""><a class="moz-txt-link-abbreviated" href="mailto:dani@letai.org.il">dani@letai.org.il</a></a>&gt; wrote:</div>
          <br class="Apple-interchange-newline">
          <div class="">
            <div class="">Thanks for the reply,<br class="">
              <br class="">
              On 10/13/2015 04:04 PM, Ralph Castain wrote:<br class="">
              <blockquote type="cite" class="">
                <blockquote type="cite" class="">On Oct 12, 2015, at
                  6:10 AM, Daniel Letai &lt;<a moz-do-not-send="true"
                    href="mailto:dani@letai.org.il" class="">dani@letai.org.il</a>&gt;
                  wrote:<br class="">
                  <br class="">
                  Hi,<br class="">
                  After upgrading to 1.8.8 I can no longer see the map.
                  When looking at the man page for mpirun, display-map
                  no longer exists. Is there a way to show the map in
                  1.8.8 ?<br class="">
                </blockquote>
                I don’t know why/how it got dropped from the man page,
                but the display-map option certainly still exists - do
                “mpirun -h” to see the full list of options, and you’ll
                see it is there. I’ll ensure it gets restored to the man
                page in the 1.10 series as the 1.8 series is complete.<br
                  class="">
              </blockquote>
              Thanks for clarifying,<br class="">
              <blockquote type="cite" class=""><br class="">
                <blockquote type="cite" class="">Another issue - I'd
                  like to map 2 process per node - 1 to each socket.<br
                    class="">
                  What is the current "correct" syntax? --map-by
                  ppr:2:node doesn't guarantee 1 per Socket. --map-by
                  ppr:1:socket doesn't guarantee 2 per node. I assume
                  it's something obvious, but the documentation is
                  somewhat lacking.<br class="">
                  I'd like to know the general syntax - even if I have 4
                  socket nodes I'd still like to map only 2 procs per
                  node.<br class="">
                </blockquote>
                That’s a tough one. I’m not sure there is a way to do
                that right now. Probably something we’d have to add. Out
                of curiosity, if you have 4 sockets and only 2 procs,
                would you want each proc bound to 2 of the 4 sockets? Or
                are you expecting them to be bound to only 1 socket
                (thus leaving 2 sockets idle), or simply leave them
                unbound?<br class="">
              </blockquote>
              I have 2 pci devices (gpu) per node. I need 1 proc per
              socket to be bound to that socket and "talk" to it's
              respective gpu, so no matter how many sockets I have - I
              must distribute the procs 2 per node, each in it's own
              socket (actually, each in it's own numa) and  be bound.<br
                class="">
              <br class="">
              So I expect them to be "bound to only 1 socket (thus
              leaving 2 sockets idle)”.<br class="">
            </div>
          </div>
        </blockquote>
        <div><br class="">
        </div>
        Are the gpu’s always near the same sockets for every node? If
        so, you might be able to use the cpu-set option to restrict us
        to those sockets, and then just "—map-by ppr:2:node —bind-to
        socket"</div>
      <div><br class="">
      </div>
      <div>
        <div style="margin: 0px; font-size: 11px; line-height: normal;
          font-family: Menlo; background-color: rgb(254, 244, 156);"
          class=""> -cpu-set|--cpu-set &lt;arg0&gt;  </div>
        <div style="margin: 0px; font-size: 11px; line-height: normal;
          font-family: Menlo; background-color: rgb(254, 244, 156);"
          class="">                         Comma-separated list of
          ranges specifying logical</div>
        <div style="margin: 0px; font-size: 11px; line-height: normal;
          font-family: Menlo; background-color: rgb(254, 244, 156);"
          class="">                         cpus allocated to this job
          [default: none]</div>
        <div style="margin: 0px; font-size: 11px; line-height: normal;
          font-family: Menlo; background-color: rgb(254, 244, 156);"
          class=""><br class="">
        </div>
      </div>
    </blockquote>
    <br>
    I Believe this should solve the issue. So the cmdline should be
    something like:<br>
    mpirun --map-by ppr:2:node --bind-to socket --cpu-set 0,2<br>
    BTW --cpu-set also absent from man page.<br>
     <br>
    <blockquote
      cite="mid:44D83A4E-6ED9-4240-831F-213B283087EA@open-mpi.org"
      type="cite">
      <div>
        <blockquote type="cite" class="">
          <div class="">
            <div class=""><br class="">
              I might run other jobs on the idle sockets (depending on
              mem utilization) but that's not an immediate concern at
              this time.<br class="">
              <blockquote type="cite" class=""><br class="">
                <blockquote type="cite" class="">Combining with
                  numa/dist to hca/dist to gpu will be very helpful too.<br
                    class="">
                </blockquote>
                Definitely no way to do this one today.<br class="">
                <br class="">
                <blockquote type="cite" class="">Thanks,<br class="">
                  <br class="">
                  <br class="">
                  _______________________________________________<br
                    class="">
                  users mailing list<br class="">
                  <a moz-do-not-send="true"
                    href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br
                    class="">
                  Subscription:
                  <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br
                    class="">
                  Link to this post:
                  <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2015/10/27860.php">http://www.open-mpi.org/community/lists/users/2015/10/27860.php</a><br
                    class="">
                </blockquote>
                _______________________________________________<br
                  class="">
                users mailing list<br class="">
                <a moz-do-not-send="true"
                  href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br
                  class="">
                Subscription:
                <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br
                  class="">
                Link to this post:
                <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2015/10/27861.php">http://www.open-mpi.org/community/lists/users/2015/10/27861.php</a><br
                  class="">
              </blockquote>
              <br class="">
              _______________________________________________<br
                class="">
              users mailing list<br class="">
              <a moz-do-not-send="true" href="mailto:users@open-mpi.org"
                class="">users@open-mpi.org</a><br class="">
              Subscription:
              <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br
                class="">
              Link to this post:
              <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2015/10/27898.php">http://www.open-mpi.org/community/lists/users/2015/10/27898.php</a></div>
          </div>
        </blockquote>
      </div>
      <br class="">
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2015/10/27899.php">http://www.open-mpi.org/community/lists/users/2015/10/27899.php</a></pre>
    </blockquote>
    <br>
  </body>
</html>

