<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">Hi Ted<div><br></div><div>From what I can tell, you are not using Open MPI, but mpich's mpirun. You might want to ask for help on their mailing list.</div><div><br></div><div>Ralph</div><div><br><div><div>On Feb 6, 2009, at 8:49 AM, Ted Yu wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><table cellspacing="0" cellpadding="0" border="0"><tbody><tr><td valign="top" style="font: inherit;">Thanx for the reply.&nbsp; <br><br>I guess I should go back a step:&nbsp; I had used the openmpi version on my system which is simply:<br>"mpirun -machinefile $PBS_NODEFILE -np $NPROCS ${CODE} >/ul/tedhyu/fuelcell/HOH/test/HH.out"<br><br>This did not work because I was just getting a blank output.<br><br>I tried this older version because at least i was getting an output.<br>"/opt/mpich-1.2.5.10-ch_p4-gcc/bin/mpirun -machinefile $PBS_NODEFILE -np<br><pre>$NPROCS ${CODE} >/ul/tedhyu/fuelcell/HOH/test/HH.out"</pre> I think this older version is failing me for whatever reason.&nbsp; Do you have any clue?&nbsp; I read somewhere that new versions of mpirun adds extra commandline arguments to the end of the line.&nbsp; Therefore the newer version of mpirun may be not be giving an output because it sees all extra commandline arguments after my output file >/ul/tedhyu/fuelcell/HOH/test/HH.out<br><br>This is where I'm reading that there are extra commandline arguments for a version of mpirun:<br><a href="https://lists.sdsc.edu/pipermail/npaci-rocks-discussion/2008-February/029333.html">https://lists.sdsc.edu/pipermail/npaci-rocks-discussion/2008-February/029333.html</a><br><br>Again, I'm new at this, and I'm just guessing.&nbsp; Any ideas of where to turn would be helpful!<br><br>Ted<br><br>--- On <b>Thu, 2/5/09, doriankrause <i>&lt;doriankrause@web.de></i></b> wrote:<br><blockquote style="border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;">From: doriankrause &lt;doriankrause@web.de><br>Subject: Re: [OMPI users] Global Communicator<br>To: tedhyu@wag.caltech.edu, "Open MPI Users" &lt;users@open-mpi.org><br>Date: Thursday, February 5, 2009, 11:14 PM<br><br><pre>Ted Yu wrote:<br>> I'm trying to run a job based on openmpi.  For some reason, the<br>program and the global communicator are not in sync and it reads that there is<br>only one processors, whereas, there should be 2 or more.  Any advice on where to<br>look?  Here is my PBS script.  Thanx!<br>><br>> PBS SCRIPT:<br>> #!/bin/sh<br>> ### Set the job name<br>> #PBS -N HH<br>> ### Declare myprogram non-rerunable<br>> #PBS -r n<br>> ### Combine standard error and standard out to one file.<br>> #PBS -j oe<br>> ### Have PBS mail you results<br>> #PBS -m ae<br>> #PBS -M tedhyu@wag.caltech.edu<br>> ### Set the queue name, given to you when you get a reservation.<br>> #PBS -q workq<br>> ### Specify the number of cpus for your job.  This example will run on 32<br>cpus<br>>
 ### using 8 nodes with 4 processes per node.<br>> #PBS -l nodes=1:ppn=2,walltime=70:00:00<br>> # Switch to the working directory; by default PBS launches processes from<br>your home directory.<br>> # Jobs should only be run from /home, /project, or /work; PBS returns<br>results via NFS.<br>> PBS_O_WORKDIR=/temp1/tedhyu/HH<br>> export<br>CODE=/project/source/seqquest/seqquest_source_v261j/hive_CentOS4.5_parallel/build_261j/quest_ompi.x<br>><br>> echo Working directory is $PBS_O_WORKDIR<br>> mkdir -p $PBS_O_WORKDIR<br>> cd $PBS_O_WORKDIR<br>> rm -rf *<br>> cp /ul/tedhyu/fuelcell/HOH/test/HH.in ./lcao.in<br>> cp /ul/tedhyu/atom_pbe/* .<br>> echo Running on host `hostname`<br>> echo Time is `date`<br>> echo Directory is `pwd`<br>> echo This jobs runs on the following processors:<br>> echo `cat $PBS_NODEFILE`<br>> Number=`wc -l $PBS_NODEFILE | awk '{print $1}'`<br>><br>> export Number<br>> echo
 ${Number}<br>> # Define number of processors<br>> NPROCS=`wc -l &lt; $PBS_NODEFILE`<br>> # And the number or hosts<br>> NHOSTS=`cat $PBS_NODEFILE|uniq|wc -l`<br>> echo This job has allocated $NPROCS cpus<br>> echo NHOSTS<br>> #mpirun  -machinefile $PBS_NODEFILE  ${CODE}<br>>/ul/tedhyu/fuelcell/HOH/test/HH.out<br>> #mpiexec -np 2  ${CODE} >/ul/tedhyu/fuelcell/HOH/test/HH.out<br>> /opt/mpich-1.2.5.10-ch_p4-gcc/bin/mpirun -machinefile $PBS_NODEFILE -np<br>$NPROCS ${CODE} >/ul/tedhyu/fuelcell/HOH/test/HH.out<br>> cd ..<br>> rm -rf HH<br>><br>><br>>   <br><br>Please note, that you are mixing Open MPI (API/Library) with MPICH <br>(mpirun). This is a mistake I like to make, too. If you use<br>the ompi mpiexec program, it probably works.<br><br>Dorian<br><br>><br>>       <br>>   <br>> ------------------------------------------------------------------------<br>><br>>
 _______________________________________________<br>> users mailing list<br>> users@open-mpi.org<br>> http://www.open-mpi.org/mailman/listinfo.cgi/users<br><br></pre></blockquote></td></tr></tbody></table><br>       _______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
