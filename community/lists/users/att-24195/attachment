<div dir="ltr"><div><div><div>Hi,<br><br></div>Thanks for this guys. I think I might have two MPI implementations installed because &#39;locate mpirun&#39; gives (see bold lines) :<br>-----------------------------------------<br>

/etc/alternatives/mpirun<br>/etc/alternatives/mpirun.1.gz<br><b>/home/djordje/Build_WRF/LIBRARIES/mpich/bin/mpirun</b><br>/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/intel/<a href="http://4.1.1.036/linux-x86_64/bin/mpirun">4.1.1.036/linux-x86_64/bin/mpirun</a><br>

/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/intel/<a href="http://4.1.1.036/linux-x86_64/bin64/mpirun">4.1.1.036/linux-x86_64/bin64/mpirun</a><br>/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/intel/<a href="http://4.1.1.036/linux-x86_64/ia32/bin/mpirun">4.1.1.036/linux-x86_64/ia32/bin/mpirun</a><br>

/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/intel/<a href="http://4.1.1.036/linux-x86_64/intel64/bin/mpirun">4.1.1.036/linux-x86_64/intel64/bin/mpirun</a><br>/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/openmpi/1.4.3/linux-x86_64-2.3.4/gnu4.5/bin/mpirun<br>

/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/openmpi/1.4.3/linux-x86_64-2.3.4/gnu4.5/share/man/man1/mpirun.1<br>/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/openmpi/1.6.4/linux-x86_64-2.3.4/gnu4.6/bin/mpirun<br>

/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/openmpi/1.6.4/linux-x86_64-2.3.4/gnu4.6/share/man/man1/mpirun.1<br>/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/bin/mpirun">8.2.0.0/linux64_2.6-x86-glibc_2.3.4/bin/mpirun</a><br>

/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/bin/mpirun.mpich">8.2.0.0/linux64_2.6-x86-glibc_2.3.4/bin/mpirun.mpich</a><br>/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/bin/mpirun.mpich2">8.2.0.0/linux64_2.6-x86-glibc_2.3.4/bin/mpirun.mpich2</a><br>

/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun">8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun</a><br>/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun.mpich">8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun.mpich</a><br>

/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun.mpich2">8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun.mpich2</a><br>/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/lib/linux_amd64/libmpirun.so">8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/lib/linux_amd64/libmpirun.so</a><br>

/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/lib/linux_ia32/libmpirun.so">8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/lib/linux_ia32/libmpirun.so</a><br>

/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/lib/linux_amd64/libmpirun.so">8.2.0.0/linux64_2.6-x86-glibc_2.3.4/lib/linux_amd64/libmpirun.so</a><br>/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/lib/linux_ia32/libmpirun.so">8.2.0.0/linux64_2.6-x86-glibc_2.3.4/lib/linux_ia32/libmpirun.so</a><br>

/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/share/man/man1/mpirun.1.gz">8.2.0.0/linux64_2.6-x86-glibc_2.3.4/share/man/man1/mpirun.1.gz</a><br>/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/bin/mpirun">8.3.0.2/linux64_2.6-x86-glibc_2.3.4/bin/mpirun</a><br>

/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/bin/mpirun.mpich">8.3.0.2/linux64_2.6-x86-glibc_2.3.4/bin/mpirun.mpich</a><br>/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/bin/mpirun.mpich2">8.3.0.2/linux64_2.6-x86-glibc_2.3.4/bin/mpirun.mpich2</a><br>

/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun">8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun</a><br>/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun.mpich">8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun.mpich</a><br>

/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun.mpich2">8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun.mpich2</a><br>/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/lib/linux_amd64/libmpirun.so">8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/lib/linux_amd64/libmpirun.so</a><br>

/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/lib/linux_ia32/libmpirun.so">8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/lib/linux_ia32/libmpirun.so</a><br>

/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/lib/linux_amd64/libmpirun.so">8.3.0.2/linux64_2.6-x86-glibc_2.3.4/lib/linux_amd64/libmpirun.so</a><br>/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/lib/linux_ia32/libmpirun.so">8.3.0.2/linux64_2.6-x86-glibc_2.3.4/lib/linux_ia32/libmpirun.so</a><br>

/home/djordje/StarCCM/Install/STAR-CCM+8.06.007/mpi/platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/share/man/man1/mpirun.1.gz">8.3.0.2/linux64_2.6-x86-glibc_2.3.4/share/man/man1/mpirun.1.gz</a><br><b>/usr/bin/mpirun</b><br>

/usr/bin/mpirun.openmpi<br>/usr/lib/openmpi/include/openmpi/ompi/runtime/mpiruntime.h<br>/usr/share/man/man1/mpirun.1.gz<br>/usr/share/man/man1/mpirun.openmpi.1.gz<br>/var/lib/dpkg/alternatives/mpirun<br>-----------------------------------------<br>

</div><div>This is a single machine. I actually just got it... another user used it for 1-2 years. <br></div><div><br></div>Is this a possible cause of the problem?<br><br></div>Regards,<br>Djordje<br></div><div class="gmail_extra">

<br><br><div class="gmail_quote">On Mon, Apr 14, 2014 at 7:06 PM, Gus Correa <span dir="ltr">&lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">

Apologies for stirring even more the confusion by mispelling<br>
&quot;Open MPI&quot; as &quot;OpenMPI&quot;.<br>
&quot;OMPI&quot; doesn&#39;t help either, because all OpenMP environment<br>
variables and directives start with &quot;OMP&quot;.<br>
Maybe associating the names to<br>
&quot;message passing&quot; vs. &quot;threads&quot; would help?<br>
<br>
Djordje:<br>
<br>
&#39;which mpif90&#39; etc show everything in /usr/bin.<br>
So, very likely they were installed from packages<br>
(yum, apt-get, rpm ...),right?<br>
Have you tried something like<br>
&quot;yum list |grep mpi&quot;<br>
to see what you have?<br>
<br>
As Dave, Jeff and Tom said, this may be a mixup of different<br>
MPI implementations at compilation (mpicc mpif90) and runtime (mpirun).<br>
That is common, you may have different MPI implementations installed.<br>
<br>
Other possibilities that may tell what MPI you have:<br>
<br>
mpirun --version<br>
mpif90 --show<br>
mpicc --show<br>
<br>
Yet another:<br>
<br>
locate mpirun<br>
locate mpif90<br>
locate mpicc<br>
<br>
The ldd didn&#39;t show any MPI libraries, maybe they are static libraries.<br>
<br>
An alternative is to install Open MPI from source,<br>
and put it in a non-system directory<br>
(not /usr/bin, not /usr/local/bin, etc).<br>
<br>
Is this a single machine or a cluster?<br>
Or perhaps a set of PCs that you have access to?<br>
If it is a cluster, do you have access to a filesystem that is<br>
shared across the cluster?<br>
On clusters typically /home is shared, often via NFS.<span class="HOEnZb"><font color="#888888"><br>
<br>
Gus Correa</font></span><div class="HOEnZb"><div class="h5"><br>
<br>
On 04/14/2014 05:15 PM, Jeff Squyres (jsquyres) wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Maybe we should rename OpenMP to be something less confusing --<br>
perhaps something totally unrelated, perhaps even non-sensical.<br>
That&#39;ll end lots of confusion!<br>
<br>
My vote: OpenMP --&gt; SharkBook<br>
<br>
It&#39;s got a ring to it, doesn&#39;t it?  And it sounds fearsome!<br>
<br>
<br>
<br>
On Apr 14, 2014, at 5:04 PM, &quot;Elken, Tom&quot; &lt;<a href="mailto:tom.elken@intel.com" target="_blank">tom.elken@intel.com</a>&gt; wrote:<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
That’s OK.  Many of us make that mistake, though often as a typo.<br>
One thing that helps is that the correct spelling of Open MPI has a space in it,<br>
</blockquote></blockquote>
but OpenMP does not.<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
If not aware what OpenMP is, here is a link: <a href="http://openmp.org/wp/" target="_blank">http://openmp.org/wp/</a><br>
<br>
What makes it more confusing is that more and more apps.<br>
</blockquote></blockquote>
offer the option of running in a hybrid mode, such as WRF,<br>
with OpenMP threads running over MPI ranks with the same executable.<br>
And sometimes that MPI is Open MPI.<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<br>
Cheers,<br>
-Tom<br>
<br>
From: users [mailto:<a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-<u></u>mpi.org</a>] On Behalf Of Djordje Romanic<br>
Sent: Monday, April 14, 2014 1:28 PM<br>
To: Open MPI Users<br>
Subject: Re: [OMPI users] mpirun runs in serial even I set np to several processors<br>
<br>
OK guys... Thanks for all this info. Frankly, I didn&#39;t know these diferences between OpenMP and OpenMPI. The commands:<br>
which mpirun<br>
which mpif90<br>
which mpicc<br>
give,<br>
/usr/bin/mpirun<br>
/usr/bin/mpif90<br>
/usr/bin/mpicc<br>
respectively.<br>
<br>
A tutorial on how to compile WRF (<a href="http://www.mmm.ucar.edu/wrf/OnLineTutorial/compilation_tutorial.php" target="_blank">http://www.mmm.ucar.edu/wrf/<u></u>OnLineTutorial/compilation_<u></u>tutorial.php</a>) provides a test program to test MPI. I ran the program and it gave me the output of successful run, which is:<br>


------------------------------<u></u>---------------<br>
C function called by Fortran<br>
Values are xx = 2.00 and ii = 1<br>
status = 2<br>
SUCCESS test 2 fortran + c + netcdf + mpi<br>
------------------------------<u></u>---------------<br>
It uses mpif90 and mpicc for compiling. Below is the output of &#39;ldd ./wrf.exe&#39;:<br>
<br>
<br>
     linux-vdso.so.1 =&gt;  (0x00007fff584e7000)<br>
     libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/<u></u>libpthread.so.0 (0x00007f4d160ab000)<br>
     libgfortran.so.3 =&gt; /usr/lib/x86_64-linux-gnu/<u></u>libgfortran.so.3 (0x00007f4d15d94000)<br>
     libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.<u></u>6 (0x00007f4d15a97000)<br>
     libgcc_s.so.1 =&gt; /lib/x86_64-linux-gnu/libgcc_<u></u>s.so.1 (0x00007f4d15881000)<br>
     libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.<u></u>6 (0x00007f4d154c1000)<br>
     /lib64/ld-linux-x86-64.so.2 (0x00007f4d162e8000)<br>
     libquadmath.so.0 =&gt; /usr/lib/x86_64-linux-gnu/<u></u>libquadmath.so.0 (0x00007f4d1528a000)<br>
<br>
<br>
<br>
On Mon, Apr 14, 2014 at 4:09 PM, Gus Correa &lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt; wrote:<br>
Djordje<br>
<br>
Your WRF configure file seems to use mpif90 and mpicc (line 115 &amp; following).<br>
In addition, it also seems to have DISABLED OpenMP (NO TRAILING &quot;I&quot;)<br>
(lines 109-111, where OpenMP stuff is commented out).<br>
So, it looks like to me your intent was to compile with MPI.<br>
<br>
Whether it is THIS MPI (OpenMPI) or another MPI (say MPICH, or MVAPICH,<br>
or Intel MPI, or Cray, or ...) only your environment can tell.<br>
<br>
What do you get from these commands:<br>
<br>
which mpirun<br>
which mpif90<br>
which mpicc<br>
<br>
I never built WRF here (but other people here use it).<br>
Which input do you provide to the command that generates the configure<br>
script that you sent before?<br>
Maybe the full command line will shed some light on the problem.<br>
<br>
<br>
I hope this helps,<br>
Gus Correa<br>
<br>
<br>
On 04/14/2014 03:11 PM, Djordje Romanic wrote:<br>
to get help :)<br>
<br>
<br>
<br>
On Mon, Apr 14, 2014 at 3:11 PM, Djordje Romanic &lt;<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a><br>
&lt;mailto:<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a>&gt;&gt; wrote:<br>
<br>
     Yes, but I was hoping to get. :)<br>
<br>
<br>
     On Mon, Apr 14, 2014 at 3:02 PM, Jeff Squyres (jsquyres)<br>
     &lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a> &lt;mailto:<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;&gt; wrote:<br>
<br>
         If you didn&#39;t use Open MPI, then this is the wrong mailing list<br>
         for you.  :-)<br>
<br>
         (this is the Open MPI users&#39; support mailing list)<br>
<br>
<br>
         On Apr 14, 2014, at 2:58 PM, Djordje Romanic &lt;<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a><br>
         &lt;mailto:<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a>&gt;&gt; wrote:<br>
<br>
          &gt; I didn&#39;t use OpenMPI.<br>
          &gt;<br>
          &gt;<br>
          &gt; On Mon, Apr 14, 2014 at 2:37 PM, Jeff Squyres (jsquyres)<br>
         &lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a> &lt;mailto:<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;&gt; wrote:<br>
          &gt; This can also happen when you compile your application with<br>
         one MPI implementation (e.g., Open MPI), but then mistakenly use<br>
         the &quot;mpirun&quot; (or &quot;mpiexec&quot;) from a different MPI implementation<br>
         (e.g., MPICH).<br>
          &gt;<br>
          &gt;<br>
          &gt; On Apr 14, 2014, at 2:32 PM, Djordje Romanic<br>
         &lt;<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a> &lt;mailto:<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a>&gt;&gt; wrote:<br>
          &gt;<br>
          &gt; &gt; I compiled it with: x86_64 Linux, gfortran compiler with<br>
         gcc   (dmpar). dmpar - distributed memory option.<br>
          &gt; &gt;<br>
          &gt; &gt; Attached is the self-generated configuration file. The<br>
         architecture specification settings start at line 107. I didn&#39;t<br>
         use Open MPI (shared memory option).<br>
          &gt; &gt;<br>
          &gt; &gt;<br>
          &gt; &gt; On Mon, Apr 14, 2014 at 1:23 PM, Dave Goodell (dgoodell)<br>
         &lt;<a href="mailto:dgoodell@cisco.com" target="_blank">dgoodell@cisco.com</a> &lt;mailto:<a href="mailto:dgoodell@cisco.com" target="_blank">dgoodell@cisco.com</a>&gt;&gt; wrote:<br>
          &gt; &gt; On Apr 14, 2014, at 12:15 PM, Djordje Romanic<br>
         &lt;<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a> &lt;mailto:<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a>&gt;&gt; wrote:<br>
          &gt; &gt;<br>
          &gt; &gt; &gt; When I start wrf with mpirun -np 4 ./wrf.exe, I get this:<br>
          &gt; &gt; &gt; ------------------------------<u></u>-------------------<br>
          &gt; &gt; &gt;  starting wrf task            0  of            1<br>
          &gt; &gt; &gt;  starting wrf task            0  of            1<br>
          &gt; &gt; &gt;  starting wrf task            0  of            1<br>
          &gt; &gt; &gt;  starting wrf task            0  of            1<br>
          &gt; &gt; &gt; ------------------------------<u></u>-------------------<br>
          &gt; &gt; &gt; This indicates that it is not using 4 processors, but 1.<br>
          &gt; &gt; &gt;<br>
          &gt; &gt; &gt; Any idea what might be the problem?<br>
          &gt; &gt;<br>
          &gt; &gt; It could be that you compiled WRF with a different MPI<br>
         implementation than you are using to run it (e.g., MPICH vs.<br>
         Open MPI).<br>
          &gt; &gt;<br>
          &gt; &gt; -Dave<br>
          &gt; &gt;<br>
          &gt; &gt; ______________________________<u></u>_________________<br>
          &gt; &gt; users mailing list<br>
          &gt; &gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
<br>
          &gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
          &gt; &gt;<br>
          &gt; &gt; &lt;configure.wrf&gt;_______________<u></u>______________________________<u></u>__<br>
          &gt; &gt; users mailing list<br>
          &gt; &gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
<br>
          &gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
          &gt;<br>
          &gt;<br>
          &gt; --<br>
          &gt; Jeff Squyres<br>
          &gt; <a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a> &lt;mailto:<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;<br>
<br>
          &gt; For corporate legal information go to:<br>
         <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/<u></u>about/doing_business/legal/<u></u>cri/</a><br>
          &gt;<br>
          &gt; ______________________________<u></u>_________________<br>
          &gt; users mailing list<br>
          &gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
<br>
          &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
          &gt;<br>
          &gt; ______________________________<u></u>_________________<br>
          &gt; users mailing list<br>
          &gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
<br>
          &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
<br>
<br>
         --<br>
         Jeff Squyres<br>
         <a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a> &lt;mailto:<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;<br>
<br>
         For corporate legal information go to:<br>
         <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/<u></u>about/doing_business/legal/<u></u>cri/</a><br>
<br>
         ______________________________<u></u>_________________<br>
         users mailing list<br>
         <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
<br>
         <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
<br>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
<br>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
</blockquote>
<br>
<br>
</blockquote>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div>

