Hello, i&#39;m posting here another problem of my installation<br>I wanted to benchmark the differences between tcp and openib transport<br><br>if i run a simple non mpi application i get<br>randori ~ # mpirun  --mca btl tcp,self  -np 2 -host randori -host tatami hostname<br>
randori<br>tatami<br><br>but as soon as i switch to my benchmark program i have<br>mpirun  --mca btl tcp,self  -np 2 -host randori -host tatami graph   <br>Master thread reporting<br>matrix size 33554432 kB, time is in [us]<br>
<br>and instead of starting the send/receive functions it just hangs there; i also checked the transmitted packets with wireshark but after the handshake no more packets are exchanged<br><br>I read in the archives that there were some problems in this area and so i tried what was suggested in previous emails<br>
<br>mpirun --mca btl ^openib  -np 2 -host randori -host tatami graph<br>mpirun --mca pml ob1  --mca btl tcp,self  -np 2 -host randori -host tatami graph<br><br>gives exactly the same output as before (no mpisend/receive)<br>
while the next commands gives something more interesting<br><br>mpirun --mca pml cm  --mca btl tcp,self  -np 2 -host randori -host tatami graph<br>--------------------------------------------------------------------------<br>
No available pml components were found!<br><br>This means that there are no components of this type installed on your<br>system or all the components reported that they could not be used.<br><br>This is a fatal error; your MPI process is likely to abort.  Check the<br>
output of the &quot;ompi_info&quot; command and ensure that components of this<br>type are available on your system.  You may also wish to check the<br>value of the &quot;component_path&quot; MCA parameter and ensure that it has at<br>
least one directory that contains valid MCA components.<br><br>--------------------------------------------------------------------------<br>[tatami:06619] PML cm cannot be selected<br>mpirun noticed that job rank 0 with PID 6710 on node randori exited on signal 15 (Terminated). <br>
<br>which is not possible as if i do ompi_info --param all there is the CM pml component<br><br>                 MCA pml: cm (MCA v1.0, API v1.0, Component v1.2.8)<br>                 MCA pml: ob1 (MCA v1.0, API v1.0, Component v1.2.8)<br>
<br><br>my test program is quite simple, just a couple of MPI_Send and MPI_Recv (just after the signature)<br>do you have any ideas that might help me?<br>thanks a lot<br>Vittorio<br><br>========================<br>#include &quot;mpi.h&quot;<br>
#include &lt;stdio.h&gt;<br>#include &lt;stdlib.h&gt;<br>#include &lt;string.h&gt;<br>#include &lt;math.h&gt;<br><br>#define M_COL 4096<br>#define M_ROW 524288<br>#define NUM_MSG 25<br><br>unsigned long int  gigamatrix[M_ROW][M_COL];<br>
<br>int main (int argc, char *argv[]) {<br>    int numtasks, rank, dest, source, rc, tmp, count, tag=1; <br>    unsigned long int  exp, exchanged;<br>    unsigned long int i, j, e;<br>    unsigned long matsize;<br>    MPI_Status Stat;<br>
    struct timeval timing_start, timing_end;<br>    double inittime = 0;<br>    long int totaltime = 0;<br><br>    MPI_Init (&amp;argc, &amp;argv);<br>    MPI_Comm_size (MPI_COMM_WORLD, &amp;numtasks);<br>    MPI_Comm_rank (MPI_COMM_WORLD, &amp;rank);<br>
    <br>    <br>    if (rank == 0) {<br>        fprintf (stderr, &quot;Master thread reporting\n&quot;, numtasks - 1);<br>        matsize = (long) M_COL * M_ROW / 64;<br>        fprintf (stderr, &quot;matrix size %d kB, time is in [us]\n&quot;, matsize);<br>
    <br>        source = 1;<br>        dest = 1;    <br><br>        /*warm up phase*/<br>        rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);<br>        rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &amp;Stat);<br>
        rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);<br>        rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);<br>        rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &amp;Stat);<br>
        rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);<br><br>        for (e = 0; e &lt; NUM_MSG; e++) {<br>            exp = pow (2, e);<br>            exchanged = 64 * exp;<br><br>            /*timing of ops*/<br>
            gettimeofday (&amp;timing_start, NULL);<br>            rc = MPI_Send (&amp;gigamatrix[0], exchanged, MPI_UNSIGNED_LONG, dest, tag, MPI_COMM_WORLD);<br>            rc = MPI_Recv (&amp;gigamatrix[0], exchanged, MPI_UNSIGNED_LONG, source, tag, MPI_COMM_WORLD, &amp;Stat);<br>
            gettimeofday (&amp;timing_end, NULL);<br><br>            totaltime = (timing_end.tv_sec - timing_start.tv_sec) * 1000000 + (timing_end.tv_usec - timing_start.tv_usec);<br>            memset (&amp;timing_start, 0, sizeof(struct timeval));<br>
            memset (&amp;timing_end, 0, sizeof(struct timeval));<br>            fprintf (stdout, &quot;%d kB\t%d\n&quot;, exp, totaltime);<br>        }<br>        <br>        fprintf(stderr, &quot;task complete\n&quot;);<br>
<br>    } else {<br>        if (rank &gt;= 1) {<br>            dest = 0;<br>            source = 0;<br>            <br>            rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &amp;Stat);<br>            rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);<br>
            rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &amp;Stat);<br>            rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &amp;Stat);<br>            rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);<br>
            rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &amp;Stat);<br><br>            for (e = 0; e &lt; NUM_MSG; e++) {<br>                exp = pow (2, e);<br>                exchanged = 64 * exp;<br>
                <br>                rc = MPI_Recv (&amp;gigamatrix[0], (unsigned)  exchanged, MPI_UNSIGNED_LONG, source, tag, MPI_COMM_WORLD, &amp;Stat); <br>                rc = MPI_Send (&amp;gigamatrix[0], (unsigned)  exchanged, MPI_UNSIGNED_LONG, dest, tag, MPI_COMM_WORLD);<br>
                <br>            }<br>        }<br>    }<br>    <br>    MPI_Finalize ();<br>    <br>    return 0;<br>}<br><br><br>

