<div dir="ltr">Just an update. Yes, binding to all is as same as binding to none. I was mistaken by my memory :)</div><div class="gmail_extra"><br><br><div class="gmail_quote">On Fri, Apr 11, 2014 at 1:22 AM, Saliya Ekanayake <span dir="ltr">&lt;<a href="mailto:esaliya@gmail.com" target="_blank">esaliya@gmail.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Thank you Ralph for the details and it&#39;s a good point you mentioned on mapping by node vs socket. We have another program that uses a chain of send receives, which will benefit from having consecutive ranks nearby.<div>

<br></div><div>I&#39;ve a question on bind to none being equal to bind to all. I understand the two concepts mean the same thing, but I remember seeing poor performance when bind to none is explicitly given. I need to check the options I used and will let you know.</div>

<div><br></div><div>Yes, this test was mainly to understand how different patterns perform and it seems P=1 is not suitable for this hardware configuration and may be in general as you&#39;ve mentioned.</div><div><br></div>

<div>Thank you,</div><div>Saliya</div></div><div class="HOEnZb"><div class="h5"><div class="gmail_extra"><br><br><div class="gmail_quote">On Fri, Apr 11, 2014 at 12:30 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">Interesting data. Couple of quick points that might help:<div><br></div><div>option B is equivalent to --map-by node --bind-to none. When you bind to every core on the node, we don&#39;t bind you at all since &quot;bind to all&quot; is exactly equivalent to &quot;bind to none&quot;. So it will definitely run slower as the threads run across the different NUMA regions on the node.</div>

<div><br></div><div>You might also want to try --map-by socket, with no binding directive. This would map one process to each socket, binding it to the socket - which is similar to what your option A actually accomplished. The only difference is that the procs that share a node will differ in rank by 1, whereas option A would have those procs differ in rank by N. Depending on your communication pattern, this could make a big difference.</div>

<div><br></div><div>Map-by socket is typically the fastest performance for threaded apps. You generally don&#39;t want P=1 unless you have a *lot* of threads in the process as it removes any use of shared memory, and so messaging will run slower - and you want the ranks that share a node to be the ones that most frequently communicate to each other, if you can identify them.</div>

<div><br></div><div>HTH</div><div>Ralph</div><div><br></div><div><div><div><div>On Apr 10, 2014, at 5:59 PM, Saliya Ekanayake &lt;<a href="mailto:esaliya@gmail.com" target="_blank">esaliya@gmail.com</a>&gt; wrote:</div>
<br></div><blockquote type="cite"><div dir="ltr"><div><div style="font-family:arial,sans-serif;font-size:13px">Hi,</div><div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">

I am evaluating the performance of a clustering program written in Java with MPI+threads and would like to get some insight in solving a peculiar case. I&#39;ve attached a performance graph to explain this.<br>
</div><div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">In essence the tests were carried out as TxPxN, where T is threads per process, P is processes per node, and N is number of nodes. I noticed an inefficiency with Tx<b>1</b>xN cases in general (tall bars in graph).</div>


<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">To elaborate a bit further, </div><div style="font-family:arial,sans-serif;font-size:13px">1. each node has 2 sockets with 4 cores each (totaling 8 cores) </div>


<div style="font-family:arial,sans-serif;font-size:13px">2. used OpenMPI 1.7.5rc5 (later tested with 1.8 and observed the same)</div><div style="font-family:arial,sans-serif;font-size:13px">3. with options</div><div style="font-family:arial,sans-serif;font-size:13px">


     A.) --map-by node:PE=4 and --bind-to core</div><div style="font-family:arial,sans-serif;font-size:13px">     B.) --map-by node:PE=8 and --bind-to-core</div><div style="font-family:arial,sans-serif;font-size:13px">     C.) --map-by socket and --bind-to none</div>


<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">Timing of A,B,C came out as A &lt; B &lt; C, so used results from option A for Tx<b>1</b>xN in the graph. </div>


<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">Could you please give some suggestion that may help to speed up these Tx<b>1</b>xN cases? Also, I expected B to perform better than A as threads could utilize all 8 cores, but it wasn&#39;t the case.</div>


<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">Thank you,</div><div style="font-family:arial,sans-serif;font-size:13px">Saliya</div><div style="font-family:arial,sans-serif;font-size:13px">


<br></div><div style="font-family:arial,sans-serif;font-size:13px"><br></div></div><div style="font-family:arial,sans-serif;font-size:13px"><span>&lt;image.png&gt;</span></div><div>
<div><br></div>-- <br><div dir="ltr"><span style="color:rgb(136,136,136)">Saliya Ekanayake <a href="mailto:esaliya@gmail.com" target="_blank">esaliya@gmail.com</a></span><span style="color:rgb(136,136,136)"> </span><br style="color:rgb(136,136,136)">


<span style="color:rgb(136,136,136)">Cell <a href="tel:812-391-4914" value="+18123914914" target="_blank">812-391-4914</a> Home <a href="tel:812-961-6383" value="+18129616383" target="_blank">812-961-6383</a></span><br style="color:rgb(136,136,136)">

<font color="#888888"><a href="http://saliya.org/" target="_blank">http://saliya.org</a></font></div>
</div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>

</div><br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br><div dir="ltr"><span style="color:rgb(136,136,136)">Saliya Ekanayake <a href="mailto:esaliya@gmail.com" target="_blank">esaliya@gmail.com</a></span><span style="color:rgb(136,136,136)"> </span><br style="color:rgb(136,136,136)">

<span style="color:rgb(136,136,136)">Cell <a href="tel:812-391-4914" value="+18123914914" target="_blank">812-391-4914</a> Home <a href="tel:812-961-6383" value="+18129616383" target="_blank">812-961-6383</a></span><br style="color:rgb(136,136,136)">
<font color="#888888"><a href="http://saliya.org" target="_blank">http://saliya.org</a></font></div>
</div>
</div></div></blockquote></div><br><br clear="all"><div><br></div>-- <br><div dir="ltr"><span style="color:rgb(136,136,136)">Saliya Ekanayake <a href="mailto:esaliya@gmail.com" target="_blank">esaliya@gmail.com</a></span><span style="color:rgb(136,136,136)"> </span><br style="color:rgb(136,136,136)">
<span style="color:rgb(136,136,136)">Cell 812-391-4914 Home 812-961-6383</span><br style="color:rgb(136,136,136)"><font color="#888888"><a href="http://saliya.org" target="_blank">http://saliya.org</a></font></div>
</div>

