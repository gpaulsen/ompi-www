This was addressed to the Open MPI list; on the SGE<br>list you suggested changing the pe allocation rule from full_up$ to<br>pe_slots$; the pe is now<br><br>[flengyel@nept OPENMPI]$ qconf -sp ompi<br>pe_name           ompi<br>

slots             999<br>user_lists        Research<br>xuser_lists       NONE<br>start_proc_args   /bin/true<br>stop_proc_args    /bin/true<br>allocation_rule   $pe_slots<br>control_slaves    TRUE<br>job_is_first_task FALSE<br>

urgency_slots     min<br><br>but the result is the same:<br><br>[flengyel@nept OPENMPI]$ tail -f sum.e23310 <br>Starting server daemon at host &quot;<a href="http://m18.gc.cuny.edu">m18.gc.cuny.edu</a>&quot;<br>Server daemon successfully started with task id &quot;1.m18&quot;<br>

Establishing /usr/local/sge/utilbin/lx24-amd64/rsh session to host <a href="http://m18.gc.cuny.edu">m18.gc.cuny.edu</a> ...<br>/usr/local/sge/utilbin/lx24-amd64/rsh exited on signal 13 (PIPE)<br>reading exit code from shepherd ... 129<br>

[<a href="http://m18.gc.cuny.edu:26399">m18.gc.cuny.edu:26399</a>] ERROR: A daemon on node <a href="http://m18.gc.cuny.edu">m18.gc.cuny.edu</a> failed to start as expected.<br>[<a href="http://m18.gc.cuny.edu:26399">m18.gc.cuny.edu:26399</a>] ERROR: There may be more information available from<br>

[<a href="http://m18.gc.cuny.edu:26399">m18.gc.cuny.edu:26399</a>] ERROR: the &#39;qstat -t&#39; command on the Grid Engine tasks.<br>[<a href="http://m18.gc.cuny.edu:26399">m18.gc.cuny.edu:26399</a>] ERROR: If the problem persists, please restart the<br>

[<a href="http://m18.gc.cuny.edu:26399">m18.gc.cuny.edu:26399</a>] ERROR: Grid Engine PE job<br>[<a href="http://m18.gc.cuny.edu:26399">m18.gc.cuny.edu:26399</a>] ERROR: The daemon exited unexpectedly with status 129.<br>

<br><br><br><div class="gmail_quote">On Tue, Jul 7, 2009 at 5:05 PM, Reuti <span dir="ltr">&lt;<a href="mailto:reuti@staff.uni-marburg.de">reuti@staff.uni-marburg.de</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">

Hi,<br>
<br>
Am 07.07.2009 um 22:12 schrieb Lengyel, Florian:<div class="im"><br>
<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Hi,<br>
I may have overlooked something in the archives (not to mention Googling)--if so I apologize, however<br>
I have been unable to find info on this particular problem.<br>
<br>
OpenMPI+SGE tight integration works on E6600 core duo systems but not on Q9550 quads.<br>
Could use some troubleshooting assistance. Thanks.<br>
<br>
</blockquote></div>
Is this what you found our your question?<br>
<br>
I&#39;m not aware of this. What should be the cause of it?!? Do you have a link - was it on the SGE list?<br>
<br>
-- Reuti<br>
<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div><div></div><div class="h5">
<br>
I&#39;m running SGE 6.0u10 on a linux cluster running OpenSuse 11.<br>
<br>
OpenMPI was compiled with SGE, and the required components are present:<br>
<br>
[flengyel@nept OPENMPI]$ ompi_info | grep gridengine<br>
                 MCA ras: gridengine (MCA v1.0, API v1.3, Component v1.2.7)<br>
                 MCA pls: gridengine (MCA v1.0, API v1.3, Component v1.2.7)<br>
<br>
<br>
The parallel execution environment for OpenMPI is as follows:<br>
<br>
[flengyel@nept OPENMPI]$ qconf -sp ompi<br>
pe_name           ompi<br>
slots             999<br>
user_lists        Research<br>
xuser_lists       NONE<br>
start_proc_args   /bin/true<br>
stop_proc_args    /bin/true<br>
allocation_rule   $fill_up<br>
control_slaves    TRUE<br>
job_is_first_task FALSE<br>
urgency_slots     min<br>
<br>
A trivial OpenMPI job using this pe will run on a queue for Intel E6600 core duo machines:<br>
<br>
[flengyel@nept OPENMPI]$ cat sum2.sh<br>
<br>
#!/bin/bash<br>
#$ -S /bin/bash<br>
#$ -q x86_64.q<br>
#$ -N sum<br>
#$ -pe ompi 4<br>
<br>
#$ -cwd<br>
<br>
export PATH=/home/nept/apps64/openmpi/bin:$PATH<br>
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/nept/apps64/openmpi/lib<br>
. /usr/local/sge/default/common/settings.sh<br>
mpirun --mca pls_gridengine_verbose 2  --prefix /home/nept/apps64/openmpi -v  ./sum<br>
<br>
Here are the results:<br>
<br>
[flengyel@nept OPENMPI]$ qsub sum2.sh<br>
Your job 23194 (&quot;sum&quot;) has been submitted<br>
<br>
[flengyel@nept OPENMPI]$ qstat -r -u flengyel<br>
<br>
job-ID  prior   name       user         state submit/start at     queue                          slots ja-task-ID<br>
-----------------------------------------------------------------------------------------------------------------<br>
  23194 0.25007 sum        flengyel     r     07/07/2009 14:14:40 <a href="mailto:x86_64.q@m49.gc.cuny.edu" target="_blank">x86_64.q@m49.gc.cuny.edu</a>           4<br>
       Full jobname:     sum<br>
       Master queue:     <a href="mailto:x86_64.q@m49.gc.cuny.edu" target="_blank">x86_64.q@m49.gc.cuny.edu</a><br>
       Requested PE:     ompi 4<br>
       Granted PE:       ompi 4<br>
       Hard Resources:<br>
       Soft Resources:<br>
       Hard requested queues: x86_64.q<br>
<br>
<br>
[flengyel@nept OPENMPI]$ more sum.o23194<br>
<br>
The sum from 1 to 1000 is: 500500<br>
[flengyel@nept OPENMPI]$ more sum.e23194<br>
Starting server daemon at host &quot;<a href="http://m49.gc.cuny.edu" target="_blank">m49.gc.cuny.edu</a>&quot;<br>
Starting server daemon at host &quot;<a href="http://m33.gc.cuny.edu" target="_blank">m33.gc.cuny.edu</a>&quot;<br>
Server daemon successfully started with task id &quot;1.m49&quot;<br>
Establishing /usr/local/sge/utilbin/lx24-amd64/rsh session to host <a href="http://m49.gc.cuny.edu" target="_blank">m49.gc.cuny.edu</a> ...<br>
Server daemon successfully started with task id &quot;1.m33&quot;<br>
Establishing /usr/local/sge/utilbin/lx24-amd64/rsh session to host <a href="http://m33.gc.cuny.edu" target="_blank">m33.gc.cuny.edu</a> ...<br>
/usr/local/sge/utilbin/lx24-amd64/rsh exited with exit code 0<br>
reading exit code from shepherd ...<br>
<br>
But the same job with the queue set to quad.q for the Q9550 quad core machines<br>
has daemon trouble:<br>
<br>
<br>
[flengyel@nept OPENMPI]$ !qstat<br>
qstat -r -u flengyel<br>
job-ID  prior   name       user         state submit/start at     queue                          slots ja-task-ID<br>
-----------------------------------------------------------------------------------------------------------------<br>
  23196 0.25000 sum        flengyel     r     07/07/2009 14:26:21 <a href="mailto:quad.q@m09.gc.cuny.edu" target="_blank">quad.q@m09.gc.cuny.edu</a>             2<br>
       Full jobname:     sum<br>
       Master queue:     <a href="mailto:quad.q@m09.gc.cuny.edu" target="_blank">quad.q@m09.gc.cuny.edu</a><br>
       Requested PE:     ompi 2<br>
       Granted PE:       ompi 2<br>
       Hard Resources:<br>
       Soft Resources:<br>
       Hard requested queues: quad.q<br>
[flengyel@nept OPENMPI]$ more sum.e23196<br>
Starting server daemon at host &quot;<a href="http://m15.gc.cuny.edu" target="_blank">m15.gc.cuny.edu</a>&quot;<br>
Starting server daemon at host &quot;<a href="http://m09.gc.cuny.edu" target="_blank">m09.gc.cuny.edu</a>&quot;<br>
Server daemon successfully started with task id &quot;1.m15&quot;<br>
Server daemon successfully started with task id &quot;1.m09&quot;<br>
Establishing /usr/local/sge/utilbin/lx24-amd64/rsh session to host m15.gc.cuny.e<br>
du ...<br>
/usr/local/sge/utilbin/lx24-amd64/rsh exited on signal 13 (PIPE)<br>
reading exit code from shepherd ... Establishing /usr/local/sge/utilbin/lx24-amd<br>
64/rsh session to host <a href="http://m09.gc.cuny.edu" target="_blank">m09.gc.cuny.edu</a> ...<br>
/usr/local/sge/utilbin/lx24-amd64/rsh exited on signal 13 (PIPE)<br>
reading exit code from shepherd ... 129<br>
[<a href="http://m09.gc.cuny.edu:11413" target="_blank">m09.gc.cuny.edu:11413</a>] ERROR: A daemon on node <a href="http://m15.gc.cuny.edu" target="_blank">m15.gc.cuny.edu</a> failed to start<br>
as expected.<br>
[<a href="http://m09.gc.cuny.edu:11413" target="_blank">m09.gc.cuny.edu:11413</a>] ERROR: There may be more information available from<br>
[<a href="http://m09.gc.cuny.edu:11413" target="_blank">m09.gc.cuny.edu:11413</a>] ERROR: the &#39;qstat -t&#39; command on the Grid Engine tasks.<br>
[<a href="http://m09.gc.cuny.edu:11413" target="_blank">m09.gc.cuny.edu:11413</a>] ERROR: If the problem persists, please restart the<br>
[<a href="http://m09.gc.cuny.edu:11413" target="_blank">m09.gc.cuny.edu:11413</a>] ERROR: Grid Engine PE job<br>
[<a href="http://m09.gc.cuny.edu:11413" target="_blank">m09.gc.cuny.edu:11413</a>] ERROR: The daemon exited unexpectedly with status 129.<br>
129<br>
[<a href="http://m09.gc.cuny.edu:11413" target="_blank">m09.gc.cuny.edu:11413</a>] ERROR: A daemon on node <a href="http://m09.gc.cuny.edu" target="_blank">m09.gc.cuny.edu</a> failed to start<br>
as expected.<br>
[<a href="http://m09.gc.cuny.edu:11413" target="_blank">m09.gc.cuny.edu:11413</a>] ERROR: There may be more information available from<br>
[<a href="http://m09.gc.cuny.edu:11413" target="_blank">m09.gc.cuny.edu:11413</a>] ERROR: the &#39;qstat -t&#39; command on the Grid Engine tasks.<br>
[<a href="http://m09.gc.cuny.edu:11413" target="_blank">m09.gc.cuny.edu:11413</a>] ERROR: If the problem persists, please restart the<br>
[<a href="http://m09.gc.cuny.edu:11413" target="_blank">m09.gc.cuny.edu:11413</a>] ERROR: Grid Engine PE job<br>
[<a href="http://m09.gc.cuny.edu:11413" target="_blank">m09.gc.cuny.edu:11413</a>] ERROR: The daemon exited unexpectedly with status 129.<br>
[flengyel@nept OPENMPI]$<br>
<br>
<br>
-FL<br>
<br>
------------------------------------------------------<br>
<a href="http://gridengine.sunsource.net/ds/viewMessage.do" target="_blank">http://gridengine.sunsource.net/ds/viewMessage.do</a>?dsForumId=38&amp;dsMessageId=206057<br>
<br>
To unsubscribe from this discussion, e-mail: [users-<a href="mailto:unsubscribe@gridengine.sunsource.net" target="_blank">unsubscribe@gridengine.sunsource.net</a>].<br>
<br></div></div>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br>

