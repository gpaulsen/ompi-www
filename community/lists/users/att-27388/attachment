<div dir="ltr">Gilles,<div>Yes I saw that github thread, but wasn&#39;t certain this was the same issue. Very possible that it is. Oddly enough, that github code doesn&#39;t crash for us.</div><div><br></div><div>Adding a sleep call doesn&#39;t help. It&#39;s actually now crashing on the MPI.init(args) call itself, and the JVM is reporting the error. Earlier it would get past this point. I&#39;m not certain why this has changed all of a sudden. We did change a bit in our unrelated java code...</div><div><br></div><div>Below is the output. It does match more closely to that previous report.</div><div><br></div><div><br></div><div>Nate</div><div><br></div><div><div>#</div><div># A fatal error has been detected by the Java Runtime Environment:</div><div>#</div><div>#  SIGSEGV (0xb) at pc=0x00002b00ad2807cf, pid=28537, tid=47281916847872</div><div>#</div><div># JRE version: 7.0_21-b11</div><div># Java VM: Java HotSpot(TM) 64-Bit Server VM (23.21-b01 mixed mode linux-amd64 compressed oops)</div><div># Problematic frame:</div><div># V  [libjvm.so+0x57c7cf]  jni_GetStringUTFChars+0x9f</div><div>#</div><div># Failed to write core dump. Core dumps have been disabled. To enable core dumping, try &quot;ulimit -c unlimited&quot; before starting Java again</div><div>#</div><div># An error report file with more information is saved as:</div><div># /gpfs/home/nchamber/hs_err_pid28537.log</div><div>#</div><div># A fatal error has been detected by the Java Runtime Environment:</div><div>#</div><div>#  SIGSEGV (0xb) at pc=0x00002b198c15b7cf, pid=28538, tid=47388736182016</div><div>#</div><div># JRE version: 7.0_21-b11</div><div># Java VM: Java HotSpot(TM) 64-Bit Server VM (23.21-b01 mixed mode linux-amd64 compressed oops)</div><div># Problematic frame:</div><div># V  [libjvm.so+0x57c7cf]#</div><div># If you would like to submit a bug report, please visit:</div><div>#   <a href="http://bugreport.sun.com/bugreport/crash.jsp">http://bugreport.sun.com/bugreport/crash.jsp</a></div><div>#</div><div>  jni_GetStringUTFChars+0x9f</div><div>#</div><div># Failed to write core dump. Core dumps have been disabled. To enable core dumping, try &quot;ulimit -c unlimited&quot; before starting Java again</div><div>#</div><div># An error report file with more information is saved as:</div><div># /gpfs/home/nchamber/hs_err_pid28538.log</div><div>#</div><div># If you would like to submit a bug report, please visit:</div><div>#   <a href="http://bugreport.sun.com/bugreport/crash.jsp">http://bugreport.sun.com/bugreport/crash.jsp</a></div><div>#</div><div>--------------------------------------------------------------------------</div><div>mpirun noticed that process rank 0 with PID 28537 on node r3n70 exited on signal 6 (Aborted).</div><div>--------------------------------------------------------------------------</div></div><div><br></div><div><br></div><div><br></div><div><br></div><div><br></div><div><br></div><div><br></div><div><br></div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Mon, Aug 3, 2015 at 2:47 PM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles@rist.or.jp" target="_blank">gilles@rist.or.jp</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
  
    
  
  <div bgcolor="#FFFFFF" text="#000000">
    Nate,<br>
    <br>
    a similar issue has already been reported at
    <a href="https://github.com/open-mpi/ompi/issues/369" target="_blank">https://github.com/open-mpi/ompi/issues/369</a>, but we have<br>
    not yet been able to figure out what is going wrong.<br>
    <br>
    right after MPI_Init(), can you add<br>
    Thread.sleep(5000);<br>
    and see if it helps ?<br>
    <br>
    Cheers,<br>
    <br>
    Gilles<div><div class="h5"><br>
    <br>
    <div>On 8/4/2015 8:36 AM, Nate Chambers
      wrote:<br>
    </div>
    </div></div><blockquote type="cite"><div><div class="h5">
      <div dir="ltr">
        <div>We&#39;ve been struggling with this error for a while, so
          hoping someone more knowledgeable can help!</div>
        <div><br>
        </div>
        <div>Our java MPI code exits with a segfault during its normal
          operation, <b>but the segfault occurs before our code ever
            uses MPI functionality like sending/receiving. </b>We&#39;ve
          removed all message calls and any use of MPI.COMM_WORLD from
          the code. The segfault occurs if we call MPI.init(args) in our
          code, and does not if we comment that line out. Further vexing
          us, the crash doesn&#39;t happen at the point of the MPI.init
          call, but later on in the program. I don&#39;t have an easy-to-run
          example here because our non-MPI code is so large and
          complicated. We have run simpler test programs with MPI and
          the segfault does not occur.</div>
        <div><br>
        </div>
        <div>We have isolated the line where the segfault occurs.
          However, if we comment that out, the program will run longer,
          but then randomly (but deterministically) segfault later on in
          the code. Does anyone have tips on how to debug this? We have
          tried several flags with mpirun, but no good clues.</div>
        <div><br>
        </div>
        <div>We have also tried several MPI versions, including stable
          1.8.7 and the most recent 1.8.8rc1</div>
        <div><br>
        </div>
        <div><br>
        </div>
        <div>ATTACHED</div>
        <div>- config.log from installation</div>
        <div>- output from `ompi_info -all`</div>
        <div><br>
        </div>
        <div><br>
        </div>
        <div>OUTPUT FROM RUNNING</div>
        <div><br>
        </div>
        <div>
          <div>&gt; mpirun -np 2 java -mx4g FeaturizeDay datadir/
            days.txt </div>
          <div>...<br>
          </div>
          <div>some normal output from our code</div>
          <div>...</div>
          <div>--------------------------------------------------------------------------</div>
          <div>mpirun noticed that process rank 0 with PID 29646 on node
            r9n69 exited on signal 11 (Segmentation fault).</div>
          <div>--------------------------------------------------------------------------</div>
        </div>
        <div><br>
        </div>
        <div><br>
        </div>
        <div><br>
        </div>
      </div>
      <br>
      <fieldset></fieldset>
      <br>
      </div></div><pre>_______________________________________________
users mailing list
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/08/27386.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/08/27386.php</a></pre>
    </blockquote>
    <br>
  </div>

<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/08/27387.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/08/27387.php</a><br></blockquote></div><br></div>

