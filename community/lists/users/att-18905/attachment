I&#39;m afraid the 1.5 series doesn&#39;t offer any help in this regard. The required changes only exist in the developers trunk, which will be released as the 1.7 series in the not-too-distant future.<br><br><br><div class="gmail_quote">
On Mon, Apr 2, 2012 at 9:42 AM, Reuti <span dir="ltr">&lt;<a href="mailto:reuti@staff.uni-marburg.de">reuti@staff.uni-marburg.de</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Am 02.04.2012 um 17:40 schrieb Ralph Castain:<br>
<div class="im"><br>
&gt; I&#39;m not sure the 1.4 series can support that behavior. Each mpirun only knows about itself - it has no idea something else is going on.<br>
&gt;<br>
&gt; If you attempted to bind, all procs of same rank from each run would bind on the same CPU.<br>
&gt;<br>
&gt; All you can really do is use -host to tell the fourth run not to use the first node. Or use the devel trunk, which has more ability to separate runs.<br>
<br>
</div>Aha, this could be interesting as I face a similar issue on one of the clusters we use: I want to run several `mpiexec &amp;` under SLURM inside one job. But as none knows about the others, I had to disable the SLURM integration by unsetting the job id information.<br>

<br>
Despite the fact I used a proper value for -np *and* work only on one node, the executions of mpiexec twice reveals (i.e. `mpiexec &amp;` twice) otherwise:<br>
<br>
&quot;All nodes which are allocated for this job are already filled.&quot;<br>
<br>
What does 1.5 offer in detail in this area?<br>
<span class="HOEnZb"><font color="#888888"><br>
-- Reuti<br>
</font></span><div class="HOEnZb"><div class="h5"><br>
<br>
&gt; Sent from my iPad<br>
&gt;<br>
&gt; On Apr 2, 2012, at 6:53 AM, Rémi Palancher &lt;<a href="mailto:remi@rezib.org">remi@rezib.org</a>&gt; wrote:<br>
&gt;<br>
&gt;&gt; Hi there,<br>
&gt;&gt;<br>
&gt;&gt; I&#39;m encountering a problem when trying to run multiple mpirun in parallel inside<br>
&gt;&gt; one SLURM allocation on multiple nodes using a QLogic interconnect network with<br>
&gt;&gt; PSM.<br>
&gt;&gt;<br>
&gt;&gt; I&#39;m using Open MPI version 1.4.5 compiled with GCC 4.4.5 on Debian Lenny.<br>
&gt;&gt;<br>
&gt;&gt; My cluster is composed of 12 cores nodes.<br>
&gt;&gt;<br>
&gt;&gt; Here is how I&#39;m able to reproduce the problem:<br>
&gt;&gt;<br>
&gt;&gt; Allocate 20 CPU on 2 nodes :<br>
&gt;&gt;<br>
&gt;&gt; frontend $ salloc -N 2 -n 20<br>
&gt;&gt; frontend $ srun hostname | sort | uniq -c<br>
&gt;&gt;    12 cn1381<br>
&gt;&gt;     8 cn1382<br>
&gt;&gt;<br>
&gt;&gt; My job allocates 12 CPU on node cn1381 and 8 CPU on cn1382.<br>
&gt;&gt;<br>
&gt;&gt; My test MPI program parse for each task the value of Cpus_allowed_list in file<br>
&gt;&gt; /proc/$PID/status and print it.<br>
&gt;&gt;<br>
&gt;&gt; If I run it on all 20 allocated CPU, it works well:<br>
&gt;&gt;<br>
&gt;&gt; frontend $ mpirun get-allowed-cpu-ompi 1<br>
&gt;&gt; Launch 1 Task 00 of 20 (cn1381): 0<br>
&gt;&gt; Launch 1 Task 01 of 20 (cn1381): 1<br>
&gt;&gt; Launch 1 Task 02 of 20 (cn1381): 2<br>
&gt;&gt; Launch 1 Task 03 of 20 (cn1381): 3<br>
&gt;&gt; Launch 1 Task 04 of 20 (cn1381): 4<br>
&gt;&gt; Launch 1 Task 05 of 20 (cn1381): 7<br>
&gt;&gt; Launch 1 Task 06 of 20 (cn1381): 5<br>
&gt;&gt; Launch 1 Task 07 of 20 (cn1381): 9<br>
&gt;&gt; Launch 1 Task 08 of 20 (cn1381): 8<br>
&gt;&gt; Launch 1 Task 09 of 20 (cn1381): 10<br>
&gt;&gt; Launch 1 Task 10 of 20 (cn1381): 6<br>
&gt;&gt; Launch 1 Task 11 of 20 (cn1381): 11<br>
&gt;&gt; Launch 1 Task 12 of 20 (cn1382): 4<br>
&gt;&gt; Launch 1 Task 13 of 20 (cn1382): 5<br>
&gt;&gt; Launch 1 Task 14 of 20 (cn1382): 6<br>
&gt;&gt; Launch 1 Task 15 of 20 (cn1382): 7<br>
&gt;&gt; Launch 1 Task 16 of 20 (cn1382): 8<br>
&gt;&gt; Launch 1 Task 17 of 20 (cn1382): 10<br>
&gt;&gt; Launch 1 Task 18 of 20 (cn1382): 9<br>
&gt;&gt; Launch 1 Task 19 of 20 (cn1382): 11<br>
&gt;&gt;<br>
&gt;&gt; Here you can see that Slurm gave me CPU 0-11 on cn1381 and 4-11 on cn1382.<br>
&gt;&gt;<br>
&gt;&gt; Now I&#39;d like to run multiple MPI runs in parallel, 4 tasks each, inside my job.<br>
&gt;&gt;<br>
&gt;&gt; frontend $ cat params.txt<br>
&gt;&gt; 1<br>
&gt;&gt; 2<br>
&gt;&gt; 3<br>
&gt;&gt; 4<br>
&gt;&gt; 5<br>
&gt;&gt;<br>
&gt;&gt; It works well when I launch 3 runs in parallel, where it only use the 12 CPU of<br>
&gt;&gt; the first node (3 runs x 4 tasks = 12 CPU):<br>
&gt;&gt;<br>
&gt;&gt; frontend $ xargs -P 3 -n 1 mpirun -n 4 get-allowed-cpu-ompi &lt; params.txt<br>
&gt;&gt; Launch 2 Task 00 of 04 (cn1381): 1<br>
&gt;&gt; Launch 2 Task 01 of 04 (cn1381): 2<br>
&gt;&gt; Launch 2 Task 02 of 04 (cn1381): 4<br>
&gt;&gt; Launch 2 Task 03 of 04 (cn1381): 7<br>
&gt;&gt; Launch 1 Task 00 of 04 (cn1381): 0<br>
&gt;&gt; Launch 1 Task 01 of 04 (cn1381): 3<br>
&gt;&gt; Launch 1 Task 02 of 04 (cn1381): 5<br>
&gt;&gt; Launch 1 Task 03 of 04 (cn1381): 6<br>
&gt;&gt; Launch 3 Task 00 of 04 (cn1381): 9<br>
&gt;&gt; Launch 3 Task 01 of 04 (cn1381): 8<br>
&gt;&gt; Launch 3 Task 02 of 04 (cn1381): 10<br>
&gt;&gt; Launch 3 Task 03 of 04 (cn1381): 11<br>
&gt;&gt; Launch 4 Task 00 of 04 (cn1381): 0<br>
&gt;&gt; Launch 4 Task 01 of 04 (cn1381): 3<br>
&gt;&gt; Launch 4 Task 02 of 04 (cn1381): 1<br>
&gt;&gt; Launch 4 Task 03 of 04 (cn1381): 5<br>
&gt;&gt; Launch 5 Task 00 of 04 (cn1381): 2<br>
&gt;&gt; Launch 5 Task 01 of 04 (cn1381): 4<br>
&gt;&gt; Launch 5 Task 02 of 04 (cn1381): 7<br>
&gt;&gt; Launch 5 Task 03 of 04 (cn1381): 6<br>
&gt;&gt;<br>
&gt;&gt; But when I try to launch 4 runs or more in parallel, where it needs to use the<br>
&gt;&gt; CPU of the other node as well, it fails:<br>
&gt;&gt;<br>
&gt;&gt; frontend $ $ xargs -P 4 -n 1 mpirun -n 4 get-allowed-cpu-ompi &lt; params.txt<br>
&gt;&gt; cn1381.23245ipath_userinit: assign_context command failed: Network is down<br>
&gt;&gt; cn1381.23245can&#39;t open /dev/ipath, network down (err=26)<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt; PSM was unable to open an endpoint. Please make sure that the network link is<br>
&gt;&gt; active on the node and the hardware is functioning.<br>
&gt;&gt;<br>
&gt;&gt; Error: Could not detect network connectivity<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt; cn1381.23248ipath_userinit: assign_context command failed: Network is down<br>
&gt;&gt; cn1381.23248can&#39;t open /dev/ipath, network down (err=26)<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt; PSM was unable to open an endpoint. Please make sure that the network link is<br>
&gt;&gt; active on the node and the hardware is functioning.<br>
&gt;&gt;<br>
&gt;&gt; Error: Could not detect network connectivity<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt; cn1381.23247ipath_userinit: assign_context command failed: Network is down<br>
&gt;&gt; cn1381.23247can&#39;t open /dev/ipath, network down (err=26)<br>
&gt;&gt; cn1381.23249ipath_userinit: assign_context command failed: Network is down<br>
&gt;&gt; cn1381.23249can&#39;t open /dev/ipath, network down (err=26)<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt; It looks like MPI_INIT failed for some reason; your parallel process is<br>
&gt;&gt; likely to abort.  There are many reasons that a parallel process can<br>
&gt;&gt; fail during MPI_INIT; some of which are due to configuration or environment<br>
&gt;&gt; problems.  This failure appears to be an internal failure; here&#39;s some<br>
&gt;&gt; additional information (which may only be relevant to an Open MPI<br>
&gt;&gt; developer):<br>
&gt;&gt;<br>
&gt;&gt; PML add procs failed<br>
&gt;&gt; --&gt; Returned &quot;Error&quot; (-1) instead of &quot;Success&quot; (0)<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt; *** The MPI_Init() function was called before MPI_INIT was invoked.<br>
&gt;&gt; *** This is disallowed by the MPI standard.<br>
&gt;&gt; *** Your MPI job will now abort.<br>
&gt;&gt; *** The MPI_Init() function was called before MPI_INIT was invoked.<br>
&gt;&gt; *** This is disallowed by the MPI standard.<br>
&gt;&gt; *** Your MPI job will now abort.<br>
&gt;&gt; *** The MPI_Init() function was called before MPI_INIT was invoked.<br>
&gt;&gt; *** This is disallowed by the MPI standard.<br>
&gt;&gt; *** Your MPI job will now abort.<br>
&gt;&gt; [cn1381:23245] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>
&gt;&gt; *** The MPI_Init() function was called before MPI_INIT was invoked.<br>
&gt;&gt; *** This is disallowed by the MPI standard.<br>
&gt;&gt; *** Your MPI job will now abort.<br>
&gt;&gt; [cn1381:23247] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>
&gt;&gt; [cn1381:23242] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>
&gt;&gt; [cn1381:23243] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt; mpirun has exited due to process rank 2 with PID 23245 on<br>
&gt;&gt; node cn1381 exiting without calling &quot;finalize&quot;. This may<br>
&gt;&gt; have caused other processes in the application to be<br>
&gt;&gt; terminated by signals sent by mpirun (as reported here).<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt; It looks like MPI_INIT failed for some reason; your parallel process is<br>
&gt;&gt; likely to abort.  There are many reasons that a parallel process can<br>
&gt;&gt; fail during MPI_INIT; some of which are due to configuration or environment<br>
&gt;&gt; problems.  This failure appears to be an internal failure; here&#39;s some<br>
&gt;&gt; additional information (which may only be relevant to an Open MPI<br>
&gt;&gt; developer):<br>
&gt;&gt;<br>
&gt;&gt; PML add procs failed<br>
&gt;&gt; --&gt; Returned &quot;Error&quot; (-1) instead of &quot;Success&quot; (0)<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt; *** The MPI_Init() function was called before MPI_INIT was invoked.<br>
&gt;&gt; *** This is disallowed by the MPI standard.<br>
&gt;&gt; *** Your MPI job will now abort.<br>
&gt;&gt; *** The MPI_Init() function was called before MPI_INIT was invoked.<br>
&gt;&gt; *** This is disallowed by the MPI standard.<br>
&gt;&gt; *** Your MPI job will now abort.<br>
&gt;&gt; [cn1381:23246] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>
&gt;&gt; *** The MPI_Init() function was called before MPI_INIT was invoked.<br>
&gt;&gt; *** This is disallowed by the MPI standard.<br>
&gt;&gt; *** Your MPI job will now abort.<br>
&gt;&gt; [cn1381:23248] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>
&gt;&gt; *** The MPI_Init() function was called before MPI_INIT was invoked.<br>
&gt;&gt; *** This is disallowed by the MPI standard.<br>
&gt;&gt; *** Your MPI job will now abort.<br>
&gt;&gt; [cn1381:23249] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>
&gt;&gt; [cn1381:23244] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt; mpirun has exited due to process rank 2 with PID 23248 on<br>
&gt;&gt; node cn1381 exiting without calling &quot;finalize&quot;. This may<br>
&gt;&gt; have caused other processes in the application to be<br>
&gt;&gt; terminated by signals sent by mpirun (as reported here).<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt; [ivanoe1:24981] 3 more processes have sent help message help-mtl-psm.txt / unable to open endpoint<br>
&gt;&gt; [ivanoe1:24981] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>
&gt;&gt; [ivanoe1:24981] 3 more processes have sent help message help-mpi-runtime / mpi_init:startup:internal-failure<br>
&gt;&gt; [ivanoe1:24983] 3 more processes have sent help message help-mtl-psm.txt / unable to open endpoint<br>
&gt;&gt; [ivanoe1:24983] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>
&gt;&gt; [ivanoe1:24983] 3 more processes have sent help message help-mpi-runtime / mpi_init:startup:internal-failure<br>
&gt;&gt; Launch 3 Task 00 of 04 (cn1381): 0<br>
&gt;&gt; Launch 3 Task 01 of 04 (cn1381): 1<br>
&gt;&gt; Launch 3 Task 02 of 04 (cn1381): 2<br>
&gt;&gt; Launch 3 Task 03 of 04 (cn1381): 3<br>
&gt;&gt; Launch 1 Task 00 of 04 (cn1381): 4<br>
&gt;&gt; Launch 1 Task 01 of 04 (cn1381): 5<br>
&gt;&gt; Launch 1 Task 02 of 04 (cn1381): 6<br>
&gt;&gt; Launch 1 Task 03 of 04 (cn1381): 8<br>
&gt;&gt; Launch 5 Task 00 of 04 (cn1381): 7<br>
&gt;&gt; Launch 5 Task 01 of 04 (cn1381): 9<br>
&gt;&gt; Launch 5 Task 02 of 04 (cn1381): 10<br>
&gt;&gt; Launch 5 Task 03 of 04 (cn1381): 11<br>
&gt;&gt;<br>
&gt;&gt; As far as I can understand, Open MPI tries to launch all runs on the same nodes<br>
&gt;&gt; (cn1382 in my case) and it forgets about the other node. Am I right? How can I<br>
&gt;&gt; avoid this behaviour?<br>
&gt;&gt;<br>
&gt;&gt; Here are the Open MPI variables set in my environment:<br>
&gt;&gt; $ env | grep OMPI<br>
&gt;&gt; OMPI_MCA_mtl=psm<br>
&gt;&gt; OMPI_MCA_pml=cm<br>
&gt;&gt;<br>
&gt;&gt; You can find attached to this email the config.log and the output of the<br>
&gt;&gt; following commands:<br>
&gt;&gt; frontend $ ompi_info --all &gt; ompi_info_all.txt<br>
&gt;&gt; frontend $ mpirun --bynode --npernode 1 --tag-output ompi_info -v ompi full \<br>
&gt;&gt;          --parsable &gt; ompi_nodes.txt<br>
&gt;&gt;<br>
&gt;&gt; Thanks in advance for any kind of help!<br>
&gt;&gt;<br>
&gt;&gt; Best regards,<br>
&gt;&gt; --<br>
&gt;&gt; Rémi Palancher<br>
&gt;&gt; <a href="http://rezib.org" target="_blank">http://rezib.org</a><br>
&gt;&gt; &lt;config.log.gz&gt;<br>
&gt;&gt; &lt;ompi_info_all.txt.gz&gt;<br>
&gt;&gt; &lt;ompi_nodes.txt&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

