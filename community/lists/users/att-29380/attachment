Is this really required here ?<div><br></div><div>I was under the impression the web server already run user tasks in a container.</div><div>all tasks run with the same unix id, but that is fine since isolation is provided by the container.</div>did I get it right ?<div><br></div><div>I was thinking of an other approach, which is run the containers in the job, vs run the job in the containers.</div><div>for example, you can wrap orted, so instead of having mpirun</div><div>ssh host orted</div><div>you could basically have</div><div>ssh host run_in_container.sh orted</div><div>that bring said, singularity might already do something similar, so it is likely a better fit.</div><div><br></div><div>an other (far fetched) option is to submit two jobs into two clusters.</div><div>- the first job started containers in the bare-bone cluster, each container runs slurmd and register itself in an elastic cluster with a per job unique resource.</div><div>- the second job &quot;naturally&quot; runs under the elastic container when the container-ized resource is available, and then stop slurmd and the container.</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles</div><div><div><br>On Monday, June 6, 2016, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">Thought about this some more, and I wonder if there isn’t a simpler solution:<div><br></div><div>* create a worker pool of userid’s that represents the maximum number of simultaneous users you are willing to support. This could be very large, if you want</div><div><br></div><div>* when a worker id becomes available, pull the next email from the inbox, assign it to that worker id, and submit that job to the scheduler for execution</div><div><br></div><div>* when the job completes, package the results/output and email them back to the original sender. Then mark the worker id as available.</div><div><br></div><div>You’ll still have security issues around the use of Docker, which is why I’d recommend considering Singularity since it doesn’t require nor allow privileged operations - and it doesn’t require integration with the resource manager like Docker does. Singularity knows how to fully package OMPI apps, so it is rather simple to automate the procedure for “containerizing” the user’s app prior to submitting to the scheduler.</div><div><br></div><div>HTH</div><div>Ralph</div><div><br></div><div><br></div><div><br><div><blockquote type="cite"><div>On Jun 6, 2016, at 2:07 AM, John Hearns &lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;hearnsj@googlemail.com&#39;);" target="_blank">hearnsj@googlemail.com</a>&gt; wrote:</div><br><div><div dir="ltr">Rob, I am not familair with <a href="http://wakari.io/" target="_blank">wakari.io</a><div><br></div><div>However what you say about the Unix userid problem is very relevant to many &#39;shared infrastructure&#39; projects and is a topic which comes up in discussions about them.</div><div>Teh concern there is, as you say, if the managers of the system have a global filesystem, with shared datasets, then if virtual clusters are created on the shared infrastructure, or if containers are used, then if the user have root access they can have privileges over the global filesystem.</div><div><br></div><div>You are making some very relevant points here.</div><div><br></div><div><br></div><div><br></div><div><br></div><div class="gmail_extra"><br><div class="gmail_quote">On 5 June 2016 at 01:51, Rob Nagler <span dir="ltr">&lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;openmpi-wooxi@q33.us&#39;);" target="_blank">openmpi-wooxi@q33.us</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div class="gmail_extra">Thanks! SLURM Elastic Computing seems like it might do the trick. I need to try it out. </div><div class="gmail_extra"><br></div><div class="gmail_extra">xCAT is interesting, too. It seems to be the HPC version of Salt&#39;ed Cobbler. :)  I don&#39;t know that it&#39;s so important for our problem. We have a small cluster for testing against the cloud, primarily. I could see xCAT being quite powerful for large clusters. </div><div class="gmail_extra"><br></div><div class="gmail_extra">I&#39;m not sure how to explain the Unix user id problem other than a gmail account does not have a corresponding Unix user id. Nor do you have one for your representation on this mailing list. That decoupling is important. The actual execution of unix processes on behalf of users of gmail, this mailing list, etc. run as a Unix single user. That&#39;s how JupyterHub containers run. When you click &quot;Start Server&quot; in JupyterHub, it starts a docker container as some system user (uid=1000 in our case), and the container is given access to the user&#39;s files via a Docker volume. The container cannot see any other user&#39;s files. </div><div class="gmail_extra"><br></div><div class="gmail_extra">In a typical HPC context, the files are all in /home/&lt;unix-user&gt;. The &quot;containment&quot; is done by normal Unix file permissions. It&#39;s very easy, but it doesn&#39;t work for web apps as described above. Even being able to list all the other users on a system (via &quot;ls /home&quot;) is a privacy breach in a web app.</div><span><font color="#888888"><div class="gmail_extra"><br></div><div class="gmail_extra">Rob</div><div class="gmail_extra"><br></div></font></span></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;users@open-mpi.org&#39;);" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/06/29369.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/06/29369.php</a><br></blockquote></div><br></div></div>
_______________________________________________<br>users mailing list<br><a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;users@open-mpi.org&#39;);" target="_blank">users@open-mpi.org</a><br>Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/06/29377.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/06/29377.php</a></div></blockquote></div><br></div></div></blockquote></div></div>

