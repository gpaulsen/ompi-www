<div dir="ltr">Hi Stephan,<div><br></div><div>I&quot;m not able to reproduce your problem using master.  How are you running the test and how many ranks?  Are you running on a single node?</div><div>It may be that the problem is only present for a particular transport, i.e. btl:sm, btl:vader, etc.</div><div><br></div><div>Howard</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">2015-04-15 0:32 GMT-06:00 MOHR STEPHAN 239883 <span dir="ltr">&lt;<a href="mailto:STEPHAN.MOHR@cea.fr" target="_blank">STEPHAN.MOHR@cea.fr</a>&gt;</span>:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">




<div>
<div style="direction:ltr;font-family:Tahoma;color:#000000;font-size:10pt"><font face="Tahoma" color="black" size="2"><span style="font-size:10pt" dir="ltr">Hi George,<br>
<br>
I think you forgot the ierror argument in the call to mpi_irecv, but after correcting this it works fine. Thanks a lot for pointing out the issue of the eager limit!<br>
<br>
But as you said, this does not directly solve my one-sided problem...<br>
<br>
Thanks,<br>
Stephan</span></font>
<div style="font-family:Times New Roman;color:#000000;font-size:16px">
<hr>
<div style="direction:ltr"><font face="Tahoma" color="#000000" size="2"><b>From:</b> users [<a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a>] on behalf of George Bosilca [<a href="mailto:bosilca@icl.utk.edu" target="_blank">bosilca@icl.utk.edu</a>]<br>
<b>Sent:</b> Tuesday, April 14, 2015 17:49<br>
<b>To:</b> Open MPI Users<br>
<b>Subject:</b> Re: [OMPI users] mpi_type_create_struct not working for large counts<br>
</font><br>
</div>
<div></div>
<div>
<div><font size="2"><span style="font-size:10pt">
<div>This is one of the most classical bugs with point-to-point applications. Sends behave as non blocking as long as the amount of data is below the eager limit. Once the eager limit is passed, sends have a blocking behavior which requires
 that the peer has a posted receive. This explains why it is working for 100 but not for 1000.<br>
<br>
The correct application is attached below. This doesn’t validate the one-sided run thou.<br>
<br>
  George.<br>
<br>
</div>
</span></font></div>
<div><font size="2"><span style="font-size:10pt">
<div><br>
&gt; On Apr 14, 2015, at 11:07 , MOHR STEPHAN 239883 &lt;<a href="mailto:STEPHAN.MOHR@cea.fr" target="_blank">STEPHAN.MOHR@cea.fr</a>&gt; wrote:<br>
&gt; <br>
&gt; Hi Nathan,<br>
&gt; <br>
&gt; I tried with send/recv, but the outcome is the same. It works for small counts (e.g. n=100), but hangs for larger ones (e.g. n=1000).<br>
&gt; I attached my modified program.<br>
&gt; <br>
&gt; Thanks,<br>
&gt; Stephan<br>
&gt; ________________________________________<br>
&gt; From: users [<a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a>] on behalf of Nathan Hjelm [<a href="mailto:hjelmn@lanl.gov" target="_blank">hjelmn@lanl.gov</a>]<br>
&gt; Sent: Tuesday, April 14, 2015 16:44<br>
&gt; To: Open MPI Users<br>
&gt; Subject: Re: [OMPI users] mpi_type_create_struct not working for large counts<br>
&gt; <br>
&gt; Can you try using send/recv with the datatype in question? It could be a<br>
&gt; problem with either the one-sided code or the datatype code. Could you<br>
&gt; also give master a try?<br>
&gt; <br>
&gt; -Nathan<br>
&gt; <br>
&gt; On Tue, Apr 14, 2015 at 06:43:31AM +0000, MOHR STEPHAN 239883 wrote:<br>
&gt;&gt;   Hi Howard,<br>
&gt;&gt; <br>
&gt;&gt;   I tried with 1.8.5rc1, but it doesn&#39;t work either.<br>
&gt;&gt; <br>
&gt;&gt;   The output of ompi_info is attached.<br>
&gt;&gt; <br>
&gt;&gt;   Thanks,<br>
&gt;&gt;   Stephan<br>
&gt;&gt; <br>
&gt;&gt;     ----------------------------------------------------------------------<br>
&gt;&gt; <br>
&gt;&gt;   From: users [<a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a>] on behalf of Howard Pritchard<br>
&gt;&gt;   [<a href="mailto:hppritcha@gmail.com" target="_blank">hppritcha@gmail.com</a>]<br>
&gt;&gt;   Sent: Monday, April 13, 2015 19:41<br>
&gt;&gt;   To: Open MPI Users<br>
&gt;&gt;   Subject: Re: [OMPI users] mpi_type_create_struct not working for large<br>
&gt;&gt;   counts<br>
&gt;&gt;   HI Stephan,<br>
&gt;&gt;   For starters, would you mind sending the output you get when you run the<br>
&gt;&gt;   ompi_info command?<br>
&gt;&gt;   If you could, it would be great if you could try running the test against<br>
&gt;&gt;   the latest 1.8.5rc1?<br>
&gt;&gt;   The test appears to work without problem using mpich, at least with 4<br>
&gt;&gt;   ranks.<br>
&gt;&gt;   Thanks,<br>
&gt;&gt;   Howard<br>
&gt;&gt;   2015-04-13 10:40 GMT-06:00 MOHR STEPHAN 239883 &lt;<a href="mailto:STEPHAN.MOHR@cea.fr" target="_blank">STEPHAN.MOHR@cea.fr</a>&gt;:<br>
&gt;&gt; <br>
&gt;&gt;     Hi there,<br>
&gt;&gt; <br>
&gt;&gt;     I&#39;ve got an issue when using a derived data type created by<br>
&gt;&gt;     mpi_type_create_struct in a one-sided communication.<br>
&gt;&gt; <br>
&gt;&gt;     The problem can be reproduced using the small standalone program which I<br>
&gt;&gt;     attached. It just creates a type which should be equivalent to n<br>
&gt;&gt;     contiguous elements. This type is then used in a mpi_get. With a value<br>
&gt;&gt;     n=100 it works fine, but with n=1000 it either hangs (version 1.8.1) or<br>
&gt;&gt;     crashes (version 1.6.5).<br>
&gt;&gt; <br>
&gt;&gt;     Any help is appreciated.<br>
&gt;&gt; <br>
&gt;&gt;     Best regards,<br>
&gt;&gt;     Stephan<br>
&gt;&gt;     _______________________________________________<br>
&gt;&gt;     users mailing list<br>
&gt;&gt;     <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt;&gt;     Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;     Link to this post:<br>
&gt;&gt;     <a href="http://www.open-mpi.org/community/lists/users/2015/04/26695.php" target="_blank">
http://www.open-mpi.org/community/lists/users/2015/04/26695.php</a><br>
&gt; <br>
&gt;&gt;                 Package: Open MPI stephanm@girofle Distribution<br>
&gt;&gt;                Open MPI: 1.8.5rc1<br>
&gt;&gt;  Open MPI repo revision: v1.8.4-184-g481d751<br>
&gt;&gt;   Open MPI release date: Apr 05, 2015<br>
&gt;&gt;                Open RTE: 1.8.5rc1<br>
&gt;&gt;  Open RTE repo revision: v1.8.4-184-g481d751<br>
&gt;&gt;   Open RTE release date: Apr 05, 2015<br>
&gt;&gt;                    OPAL: 1.8.5rc1<br>
&gt;&gt;      OPAL repo revision: v1.8.4-184-g481d751<br>
&gt;&gt;       OPAL release date: Apr 05, 2015<br>
&gt;&gt;                 MPI API: 3.0<br>
&gt;&gt;            Ident string: 1.8.5rc1<br>
&gt;&gt;                  Prefix: /local/stephanm/openmpi-1.8.5rc1_intel<br>
&gt;&gt; Configured architecture: x86_64-unknown-linux-gnu<br>
&gt;&gt;          Configure host: girofle<br>
&gt;&gt;           Configured by: stephanm<br>
&gt;&gt;           Configured on: Tue Apr 14 07:32:10 CEST 2015<br>
&gt;&gt;          Configure host: girofle<br>
&gt;&gt;                Built by: stephanm<br>
&gt;&gt;                Built on: Tue Apr 14 08:05:43 CEST 2015<br>
&gt;&gt;              Built host: girofle<br>
&gt;&gt;              C bindings: yes<br>
&gt;&gt;            C++ bindings: yes<br>
&gt;&gt;             Fort mpif.h: yes (all)<br>
&gt;&gt;            Fort use mpi: yes (full: ignore TKR)<br>
&gt;&gt;       Fort use mpi size: deprecated-ompi-info-value<br>
&gt;&gt;        Fort use mpi_f08: yes<br>
&gt;&gt; Fort mpi_f08 compliance: The mpi_f08 module is available, but due to limitations in the ifort compiler, does not support the following: array subsections, direct passthru (where possible) to underlying Open MPI&#39;s C functionality<br>
&gt;&gt;  Fort mpi_f08 subarrays: no<br>
&gt;&gt;           Java bindings: no<br>
&gt;&gt;  Wrapper compiler rpath: runpath<br>
&gt;&gt;              C compiler: icc<br>
&gt;&gt;     C compiler absolute: /local/stephanm/composer_xe_2013_sp1.0.080/bin/intel64/icc<br>
&gt;&gt;  C compiler family name: INTEL<br>
&gt;&gt;      C compiler version: 1400.20130728<br>
&gt;&gt;            C++ compiler: icpc<br>
&gt;&gt;   C++ compiler absolute: /local/stephanm/composer_xe_2013_sp1.0.080/bin/intel64/icpc<br>
&gt;&gt;           Fort compiler: ifort<br>
&gt;&gt;       Fort compiler abs: /local/stephanm/composer_xe_2013_sp1.0.080/bin/intel64/ifort<br>
&gt;&gt;         Fort ignore TKR: yes (!DEC$ ATTRIBUTES NO_ARG_CHECK ::)<br>
&gt;&gt;   Fort 08 assumed shape: no<br>
&gt;&gt;      Fort optional args: yes<br>
&gt;&gt;          Fort INTERFACE: yes<br>
&gt;&gt;    Fort ISO_FORTRAN_ENV: yes<br>
&gt;&gt;       Fort STORAGE_SIZE: yes<br>
&gt;&gt;      Fort BIND(C) (all): yes<br>
&gt;&gt;      Fort ISO_C_BINDING: yes<br>
&gt;&gt; Fort SUBROUTINE BIND(C): yes<br>
&gt;&gt;       Fort TYPE,BIND(C): yes<br>
&gt;&gt; Fort T,BIND(C,name=&quot;a&quot;): yes<br>
&gt;&gt;            Fort PRIVATE: yes<br>
&gt;&gt;          Fort PROTECTED: yes<br>
&gt;&gt;           Fort ABSTRACT: yes<br>
&gt;&gt;       Fort ASYNCHRONOUS: yes<br>
&gt;&gt;          Fort PROCEDURE: yes<br>
&gt;&gt;           Fort C_FUNLOC: yes<br>
&gt;&gt; Fort f08 using wrappers: yes<br>
&gt;&gt;         Fort MPI_SIZEOF: yes<br>
&gt;&gt;             C profiling: yes<br>
&gt;&gt;           C++ profiling: yes<br>
&gt;&gt;   Fort mpif.h profiling: yes<br>
&gt;&gt;  Fort use mpi profiling: yes<br>
&gt;&gt;   Fort use mpi_f08 prof: yes<br>
&gt;&gt;          C++ exceptions: no<br>
&gt;&gt;          Thread support: posix (MPI_THREAD_MULTIPLE: no, OPAL support: yes, OMPI progress: no, ORTE progress: yes, Event lib: yes)<br>
&gt;&gt;           Sparse Groups: no<br>
&gt;&gt;  Internal debug support: no<br>
&gt;&gt;  MPI interface warnings: yes<br>
&gt;&gt;     MPI parameter check: runtime<br>
&gt;&gt; Memory profiling support: no<br>
&gt;&gt; Memory debugging support: no<br>
&gt;&gt;         libltdl support: yes<br>
&gt;&gt;   Heterogeneous support: no<br>
&gt;&gt; mpirun default --prefix: no<br>
&gt;&gt;         MPI I/O support: yes<br>
&gt;&gt;       MPI_WTIME support: gettimeofday<br>
&gt;&gt;     Symbol vis. support: yes<br>
&gt;&gt;   Host topology support: yes<br>
&gt;&gt;          MPI extensions:<br>
&gt;&gt;   FT Checkpoint support: no (checkpoint thread: no)<br>
&gt;&gt;   C/R Enabled Debugging: no<br>
&gt;&gt;     VampirTrace support: yes<br>
&gt;&gt;  MPI_MAX_PROCESSOR_NAME: 256<br>
&gt;&gt;    MPI_MAX_ERROR_STRING: 256<br>
&gt;&gt;     MPI_MAX_OBJECT_NAME: 64<br>
&gt;&gt;        MPI_MAX_INFO_KEY: 36<br>
&gt;&gt;        MPI_MAX_INFO_VAL: 256<br>
&gt;&gt;       MPI_MAX_PORT_NAME: 1024<br>
&gt;&gt;  MPI_MAX_DATAREP_STRING: 128<br>
&gt;&gt;           MCA backtrace: execinfo (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;            MCA compress: bzip (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;            MCA compress: gzip (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA crs: none (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                  MCA db: hash (MCA v2.0, API v1.0, Component v1.8.5)<br>
&gt;&gt;                  MCA db: print (MCA v2.0, API v1.0, Component v1.8.5)<br>
&gt;&gt;               MCA event: libevent2021 (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA hwloc: hwloc191 (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                  MCA if: posix_ipv4 (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                  MCA if: linux_ipv6 (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;         MCA installdirs: env (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;         MCA installdirs: config (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;              MCA memory: linux (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA pstat: linux (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA sec: basic (MCA v2.0, API v1.0, Component v1.8.5)<br>
&gt;&gt;               MCA shmem: mmap (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA shmem: posix (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA shmem: sysv (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA timer: linux (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA dfs: app (MCA v2.0, API v1.0, Component v1.8.5)<br>
&gt;&gt;                 MCA dfs: orted (MCA v2.0, API v1.0, Component v1.8.5)<br>
&gt;&gt;                 MCA dfs: test (MCA v2.0, API v1.0, Component v1.8.5)<br>
&gt;&gt;              MCA errmgr: default_app (MCA v2.0, API v3.0, Component v1.8.5)<br>
&gt;&gt;              MCA errmgr: default_hnp (MCA v2.0, API v3.0, Component v1.8.5)<br>
&gt;&gt;              MCA errmgr: default_orted (MCA v2.0, API v3.0, Component v1.8.5)<br>
&gt;&gt;              MCA errmgr: default_tool (MCA v2.0, API v3.0, Component v1.8.5)<br>
&gt;&gt;                 MCA ess: env (MCA v2.0, API v3.0, Component v1.8.5)<br>
&gt;&gt;                 MCA ess: hnp (MCA v2.0, API v3.0, Component v1.8.5)<br>
&gt;&gt;                 MCA ess: singleton (MCA v2.0, API v3.0, Component v1.8.5)<br>
&gt;&gt;                 MCA ess: slurm (MCA v2.0, API v3.0, Component v1.8.5)<br>
&gt;&gt;                 MCA ess: tool (MCA v2.0, API v3.0, Component v1.8.5)<br>
&gt;&gt;               MCA filem: raw (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;             MCA grpcomm: bad (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA iof: hnp (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA iof: mr_hnp (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA iof: mr_orted (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA iof: orted (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA iof: tool (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                MCA odls: default (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA oob: tcp (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA plm: isolated (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA plm: rsh (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA plm: slurm (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA ras: loadleveler (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA ras: simulator (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA ras: slurm (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA rmaps: lama (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA rmaps: mindist (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA rmaps: ppr (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA rmaps: rank_file (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA rmaps: resilient (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA rmaps: round_robin (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA rmaps: seq (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA rmaps: staged (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA rml: oob (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;              MCA routed: binomial (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;              MCA routed: debruijn (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;              MCA routed: direct (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;              MCA routed: radix (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA state: app (MCA v2.0, API v1.0, Component v1.8.5)<br>
&gt;&gt;               MCA state: hnp (MCA v2.0, API v1.0, Component v1.8.5)<br>
&gt;&gt;               MCA state: novm (MCA v2.0, API v1.0, Component v1.8.5)<br>
&gt;&gt;               MCA state: orted (MCA v2.0, API v1.0, Component v1.8.5)<br>
&gt;&gt;               MCA state: staged_hnp (MCA v2.0, API v1.0, Component v1.8.5)<br>
&gt;&gt;               MCA state: staged_orted (MCA v2.0, API v1.0, Component v1.8.5)<br>
&gt;&gt;               MCA state: tool (MCA v2.0, API v1.0, Component v1.8.5)<br>
&gt;&gt;           MCA allocator: basic (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;           MCA allocator: bucket (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                MCA bcol: basesmuma (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                MCA bcol: ptpcoll (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA bml: r2 (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA btl: openib (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA btl: self (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA btl: sm (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA btl: tcp (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA btl: vader (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                MCA coll: basic (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                MCA coll: hierarch (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                MCA coll: inter (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                MCA coll: libnbc (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                MCA coll: ml (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                MCA coll: self (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                MCA coll: sm (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                MCA coll: tuned (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA dpm: orte (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                MCA fbtl: posix (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA fcoll: dynamic (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA fcoll: individual (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA fcoll: static (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA fcoll: two_phase (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA fcoll: ylib (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                  MCA fs: ufs (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                  MCA io: ompio (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                  MCA io: romio (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA mpool: grdma (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;               MCA mpool: sm (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA osc: rdma (MCA v2.0, API v3.0, Component v1.8.5)<br>
&gt;&gt;                 MCA osc: sm (MCA v2.0, API v3.0, Component v1.8.5)<br>
&gt;&gt;                 MCA pml: v (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA pml: bfo (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA pml: cm (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA pml: ob1 (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;              MCA pubsub: orte (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;              MCA rcache: vma (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                 MCA rte: orte (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                MCA sbgp: basesmsocket (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                MCA sbgp: basesmuma (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                MCA sbgp: p2p (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;            MCA sharedfp: individual (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;            MCA sharedfp: lockedfile (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;            MCA sharedfp: sm (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt;&gt;                MCA topo: basic (MCA v2.0, API v2.1, Component v1.8.5)<br>
&gt;&gt;           MCA vprotocol: pessimist (MCA v2.0, API v2.0, Component v1.8.5)<br>
&gt; <br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/04/26713.php" target="_blank">
http://www.open-mpi.org/community/lists/users/2015/04/26713.php</a><br>
&gt; <br>
&gt; &lt;test_mpi_sendrecv.f90&gt;_______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/04/26720.php" target="_blank">
http://www.open-mpi.org/community/lists/users/2015/04/26720.php</a><br>
<br>
</div>
</span></font></div>
<div><font size="2"><span style="font-size:10pt">
<div>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/04/26721.php" target="_blank">
http://www.open-mpi.org/community/lists/users/2015/04/26721.php</a></div>
</span></font></div>
</div>
</div>
</div>
</div>

<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/04/26733.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/04/26733.php</a><br></blockquote></div><br></div>
