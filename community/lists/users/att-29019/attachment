<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    two comments :<br>
    <br>
    - the program is incorrect : slave() should MPI_Recv(...,
    MPI_ANY_TAG, ...)<br>
    <br>
    - current master uses pmix114, and your traces mention pmix120<br>
      so your master is out of sync, or pmix120 is an old module that
    was not manually removed.<br>
      fwiw, once in a while, i<br>
      rm -rf /.../ompi_install_dir/lib/openmpi<br>
      to get rid of the removed modules<br>
    <br>
    Cheers,<br>
    <br>
    Gilles<br>
    <br>
    <div class="moz-cite-prefix">On 4/25/2016 7:34 AM, dpchoudh . wrote:<br>
    </div>
    <blockquote
cite="mid:CAHXxYDjb-SMdu5wVfj=igAn7dfX2zuwm0mqPPJZw2e1xH7_sCw@mail.gmail.com"
      type="cite">
      <div dir="ltr">
        <div>
          <div>Hello all<br>
            <br>
          </div>
          Attached is a simple MPI program (a modified version of a
          similar program that was posted by another user). This
          program, when run on a single node machine, hangs most of the
          time, as follows: (in all cases, OS was CentOS 7)<br>
          <br>
        </div>
        Scenario 1: OMPI v 1.10, single socket quad core machine, with
        Chelsio T3 card, link down, and GigE, link up<br>
        <br>
        <div>
          <div>mpirun -np 2 &lt;progname&gt;<br>
          </div>
          <div>Backtrace of the two spawned processes as follows:<br>
            <br>
            (gdb) bt<br>
            #0  0x00007f6471647aba in mca_btl_vader_component_progress
            () at btl_vader_component.c:708<br>
            #1  0x00007f6475c6722a in opal_progress () at
            runtime/opal_progress.c:187<br>
            #2  0x00007f64767b7685 in opal_condition_wait
            (c=&lt;optimized out&gt;, m=&lt;optimized out&gt;)<br>
                at ../opal/threads/condition.h:78<br>
            #3  ompi_request_default_wait_all (count=2,
            requests=0x7ffd1d921530, statuses=0x7ffd1d921540)<br>
                at request/req_wait.c:281<br>
            #4  0x00007f64709dd591 in ompi_coll_tuned_sendrecv_zero
            (stag=-16, rtag=-16, <br>
                comm=&lt;optimized out&gt;, source=1, dest=1) at
            coll_tuned_barrier.c:78<br>
            #5  ompi_coll_tuned_barrier_intra_two_procs (comm=0x6022c0
            &lt;ompi_mpi_comm_world&gt;, <br>
                module=&lt;optimized out&gt;) at
            coll_tuned_barrier.c:324<br>
            #6  0x00007f64767c92e6 in PMPI_Barrier (comm=0x6022c0
            &lt;ompi_mpi_comm_world&gt;) at pbarrier.c:70<br>
            #7  0x00000000004010bd in main (argc=1, argv=0x7ffd1d9217d8)
            at mpi_hello_master_slave.c:115<br>
            (gdb) <br>
            <br>
            <br>
            (gdb) bt<br>
            #0  mca_pml_ob1_progress () at pml_ob1_progress.c:45<br>
            #1  0x00007feeae7dc22a in opal_progress () at
            runtime/opal_progress.c:187<br>
            #2  0x00007feea9e125c5 in opal_condition_wait
            (c=&lt;optimized out&gt;, m=&lt;optimized out&gt;)<br>
                at ../../../../opal/threads/condition.h:78<br>
            #3  ompi_request_wait_completion (req=0xe55200) at
            ../../../../ompi/request/request.h:381<br>
            #4  mca_pml_ob1_recv (addr=&lt;optimized out&gt;, count=255,
            datatype=&lt;optimized out&gt;, <br>
                src=&lt;optimized out&gt;, tag=&lt;optimized out&gt;,
            comm=&lt;optimized out&gt;, status=0x7fff4a618000)<br>
                at pml_ob1_irecv.c:118<br>
            #5  0x00007feeaf35068f in PMPI_Recv (buf=0x7fff4a618020,
            count=255, <br>
                type=0x6020c0 &lt;ompi_mpi_char&gt;,
            source=&lt;optimized out&gt;, tag=&lt;optimized out&gt;, <br>
                comm=0x6022c0 &lt;ompi_mpi_comm_world&gt;,
            status=0x7fff4a618000) at precv.c:78<br>
            #6  0x0000000000400d49 in slave () at
            mpi_hello_master_slave.c:67<br>
            #7  0x00000000004010b3 in main (argc=1, argv=0x7fff4a6184d8)
            at mpi_hello_master_slave.c:113<br>
            (gdb) <br>
            <br>
          </div>
          <div><br>
          </div>
          <div>Scenario 2:<br>
          </div>
          <div>Dual socket Hexcore machine with Qlogic IB, Chelsio iWARP
            and Fibre Channel, all link down, GigE, link up, OpenMPI
            compiled from master branch, crashes as follows:<br>
            <br>
            [durga@smallMPI Desktop]$ mpirun -np 2
            ./mpi_hello_master_slave<br>
            <br>
            mpi_hello_master_slave:39570 terminated with signal 11 at
            PC=20 SP=7ffd438c00b8.  Backtrace:<br>
            <br>
            mpi_hello_master_slave:39571 terminated with signal 11 at
            PC=20 SP=7ffee5903e08.  Backtrace:<br>
            -------------------------------------------------------<br>
            Primary job  terminated normally, but 1 process returned<br>
            a non-zero exit code. Per user-direction, the job has been
            aborted.<br>
            -------------------------------------------------------<br>
--------------------------------------------------------------------------<br>
            mpirun noticed that process rank 0 with PID 0 on node
            smallMPI exited on signal 11 (Segmentation fault).<br>
--------------------------------------------------------------------------<br>
            <br>
          </div>
          <div>Scenario 3:<br>
          </div>
          <div>Exactly same as scenario 2, but with command line more
            explicit as follows:<br>
            <br>
            [durga@smallMPI Desktop]$ mpirun -np 2 -mca btl self,sm
            ./mpi_hello_master_slave<br>
          </div>
          <div>This hangs with the following backtrace:<br>
            <br>
            (gdb) bt<br>
            #0  0x00007ff6639f049d in nanosleep () from /lib64/libc.so.6<br>
            #1  0x00007ff663a210d4 in usleep () from /lib64/libc.so.6<br>
            #2  0x00007ff662f72796 in OPAL_PMIX_PMIX120_PMIx_Fence
            (procs=0x0, nprocs=0, info=0x0, ninfo=0)<br>
                at src/client/pmix_client_fence.c:100<br>
            #3  0x00007ff662f4f0bc in pmix120_fence (procs=0x0,
            collect_data=0) at pmix120_client.c:255<br>
            #4  0x00007ff663f941af in ompi_mpi_init (argc=1,
            argv=0x7ffc18c9afd8, requested=0, provided=0x7ffc18c9adac)<br>
                at runtime/ompi_mpi_init.c:813<br>
            #5  0x00007ff663fc9c33 in PMPI_Init (argc=0x7ffc18c9addc,
            argv=0x7ffc18c9add0) at pinit.c:66<br>
            #6  0x000000000040101f in main (argc=1, argv=0x7ffc18c9afd8)
            at mpi_hello_master_slave.c:94<br>
            (gdb) q<br>
            <br>
            (gdb) bt<br>
            #0  0x00007f5af7646117 in sched_yield () from
            /lib64/libc.so.6<br>
            #1  0x00007f5af7323875 in amsh_ep_connreq_wrap () from
            /lib64/libpsm_infinipath.so.1<br>
            #2  0x00007f5af7324254 in amsh_ep_connect () from
            /lib64/libpsm_infinipath.so.1<br>
            #3  0x00007f5af732d0df in psm_ep_connect () from
            /lib64/libpsm_infinipath.so.1<br>
            #4  0x00007f5af7d94a69 in ompi_mtl_psm_add_procs
            (mtl=0x7f5af80f8500 &lt;ompi_mtl_psm&gt;, nprocs=2,
            procs=0xf53e60)<br>
                at mtl_psm.c:312<br>
            #5  0x00007f5af7df3630 in mca_pml_cm_add_procs
            (procs=0xf53e60, nprocs=2) at pml_cm.c:134<br>
            #6  0x00007f5af7bcc0d1 in ompi_mpi_init (argc=1,
            argv=0x7ffc485a2f98, requested=0, provided=0x7ffc485a2d6c)<br>
                at runtime/ompi_mpi_init.c:777<br>
            #7  0x00007f5af7c01c33 in PMPI_Init (argc=0x7ffc485a2d9c,
            argv=0x7ffc485a2d90) at pinit.c:66<br>
            #8  0x000000000040101f in main (argc=1, argv=0x7ffc485a2f98)
            at mpi_hello_master_slave.c:94<br>
            <br>
          </div>
          <div>This seems to suggest that it is trying PSM to connect
            even when the link was down and it was not mentioned in the
            command line. Is this behavior expected?<br>
            <br>
            <br>
          </div>
          <div>Scenario 4:<br>
          </div>
          <div>Exactly same as scenario 3, but with even more explicit
            command line:<br>
            <br>
            [durga@smallMPI Desktop]$ mpirun -np 2 -mca btl self,sm -mca
            pml ob1 ./mpi_hello_master_slave<br>
            <br>
          </div>
          <div>This hangs towards the end, after printing the output (as
            opposed to scenario 3 where it hangs at the connection setup
            stage, without printing anything.)<br>
            <br>
            Process 0 of 2 running on host smallMPI<br>
            <br>
            <br>
            Now 1 slave tasks are sending greetings.<br>
            <br>
            Process 1 of 2 running on host smallMPI<br>
            Greetings from task 1:<br>
              message type:        3<br>
              msg length:          141 characters<br>
              message:             <br>
                hostname:          smallMPI<br>
                operating system:  Linux<br>
                release:           3.10.0-327.13.1.el7.x86_64<br>
                processor:         x86_64<br>
            <br>
            <br>
          </div>
          <div>Backtraces of the two processes are as follows:<br>
            <br>
            (gdb) bt<br>
            #0  opal_timer_base_get_usec_clock_gettime () at
            timer_linux_component.c:180<br>
            #1  0x00007f10f46e50e4 in opal_progress () at
            runtime/opal_progress.c:161<br>
            #2  0x00007f10f58a9d8b in opal_condition_wait
            (c=0x7f10f5df3c40 &lt;ompi_request_cond&gt;, <br>
                m=0x7f10f5df3bc0 &lt;ompi_request_lock&gt;) at
            ../opal/threads/condition.h:76<br>
            #3  0x00007f10f58aa31b in ompi_request_default_wait_all
            (count=2, requests=0x7ffe7edd5a80, <br>
                statuses=0x7ffe7edd5a50) at request/req_wait.c:287<br>
            #4  0x00007f10f596f225 in ompi_coll_base_sendrecv_zero
            (dest=1, stag=-16, source=1, rtag=-16, <br>
                comm=0x6022c0 &lt;ompi_mpi_comm_world&gt;) at
            base/coll_base_barrier.c:63<br>
            #5  0x00007f10f596f92a in
            ompi_coll_base_barrier_intra_two_procs (comm=0x6022c0
            &lt;ompi_mpi_comm_world&gt;, <br>
                module=0xd5a7f0) at base/coll_base_barrier.c:308<br>
            #6  0x00007f10f599ffec in
            ompi_coll_tuned_barrier_intra_dec_fixed (comm=0x6022c0
            &lt;ompi_mpi_comm_world&gt;, <br>
                module=0xd5a7f0) at coll_tuned_decision_fixed.c:196<br>
            #7  0x00007f10f58c86fd in PMPI_Barrier (comm=0x6022c0
            &lt;ompi_mpi_comm_world&gt;) at pbarrier.c:63<br>
            #8  0x00000000004010bd in main (argc=1, argv=0x7ffe7edd5d48)
            at mpi_hello_master_slave.c:115<br>
            <br>
            <br>
            (gdb) bt<br>
            #0  0x00007fffe9d6a988 in clock_gettime ()<br>
            #1  0x00007f704bf64edd in clock_gettime () from
            /lib64/libc.so.6<br>
            #2  0x00007f704b4deea5 in
            opal_timer_base_get_usec_clock_gettime () at
            timer_linux_component.c:183<br>
            #3  0x00007f704b2f50e4 in opal_progress () at
            runtime/opal_progress.c:161<br>
            #4  0x00007f704c6cc39c in opal_condition_wait
            (c=0x7f704ca03c40 &lt;ompi_request_cond&gt;, <br>
                m=0x7f704ca03bc0 &lt;ompi_request_lock&gt;) at
            ../../../../opal/threads/condition.h:76<br>
            #5  0x00007f704c6cc560 in ompi_request_wait_completion
            (req=0x165e580) at ../../../../ompi/request/request.h:383<br>
            #6  0x00007f704c6cd724 in mca_pml_ob1_recv
            (addr=0x7fffe9cafa10, count=255, datatype=0x6020c0
            &lt;ompi_mpi_char&gt;, <br>
                src=0, tag=1, comm=0x6022c0 &lt;ompi_mpi_comm_world&gt;,
            status=0x7fffe9caf9f0) at pml_ob1_irecv.c:123<br>
            #7  0x00007f704c4ff434 in PMPI_Recv (buf=0x7fffe9cafa10,
            count=255, type=0x6020c0 &lt;ompi_mpi_char&gt;, source=0, <br>
                tag=1, comm=0x6022c0 &lt;ompi_mpi_comm_world&gt;,
            status=0x7fffe9caf9f0) at precv.c:79<br>
            #8  0x0000000000400d49 in slave () at
            mpi_hello_master_slave.c:67<br>
            #9  0x00000000004010b3 in main (argc=1, argv=0x7fffe9cafec8)
            at mpi_hello_master_slave.c:113<br>
            (gdb) q<br>
            <br>
          </div>
          <div>I am going to try the tarball shortly, but hopefully
            someone can get some insight out of this much information.
            BTW, the code was compiled with the following flags:<br>
            <br>
          </div>
          <div>-Wall -Wextra -g3 -O0<br>
          </div>
          <div><br>
          </div>
          <div>Let me rehash that NO network communication was involved
            in any of these experiments; they were all single node
            shared memory (sm btl) jobs.<br>
            <br>
          </div>
          <div>Thanks<br>
          </div>
          <div>Durga<br>
          </div>
          <div><br>
          </div>
          <div><br>
            <br clear="all">
            <div>
              <div>
                <div>
                  <div class="gmail_signature">
                    <div dir="ltr">
                      <div>1% of the executables have 99% of CPU
                        privilege!<br>
                      </div>
                      Userspace code! Unite!! Occupy the kernel!!!<br>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/04/29018.php">http://www.open-mpi.org/community/lists/users/2016/04/29018.php</a></pre>
    </blockquote>
    <br>
  </body>
</html>

