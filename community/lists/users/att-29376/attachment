<html><head><meta http-equiv="Content-Type" content="text/html charset=windows-1252"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Jun 5, 2016, at 4:30 PM, Du, Fan &lt;<a href="mailto:fan.du@intel.com" class="">fan.du@intel.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><div class="">Thanks for your reply!<br class=""><br class="">On 2016/6/5 3:01, Ralph Castain wrote:<br class=""><blockquote type="cite" class="">The closest thing we have to what you describe is the “orte-dvm” - this<br class="">allows one to launch a persistent collection of daemons. You can then<br class="">run your applications against it using “mpiexec -hnp &lt;url&gt;” where the<br class="">url is that of the orte-dvm “head” daemon.<br class=""></blockquote><br class="">I tried this, maybe I miss something.<br class=""><br class="">On host1:<br class="">orte-dvm &nbsp;--allow-run-as-root<br class="">VMURI: 2783903744.0;<a href="tcp://192.168.10.55:47325" class="">tcp://192.168.10.55:47325</a><br class="">DVM ready<br class=""><br class="">On host2:<br class="">mpiexec -hnp 2783903744.0;<a href="tcp://192.168.10.55:47325" class="">tcp://192.168.10.55:47325</a><br class=""></div></div></blockquote><div><br class=""></div>Your shell will take the semi-colon to mean the end of the line - you have to enclose it all in quotes</div><div><br class=""><blockquote type="cite" class=""><div class=""><div class="">OMPI_MCA_orte_hnp_uri=2783903744.0<br class="">OMPI_MCA_ess=tool<br class="">[grantleyIPDC01:03305] [[21695,0],0] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161<br class="">-bash: <a href="tcp://192.168.10.55:47325:" class="">tcp://192.168.10.55:47325:</a> No such file or directory<br class=""><br class="">digging the code a bit deeper, the uri expects to have job id and rank id.<br class="">and how to make the subsequent orte-dvm know where the head orte-dvm is?<br class="">I checked orte-dvm help, seems no such option there.<br class=""><br class=""><blockquote type="cite" class="">If I understand you correctly, however, then you would want the orte-dvm<br class="">to assemble itself based on the asynchronous start of the individual<br class="">daemons. In other words, Mesos would start a daemon on each node as that<br class="">node became available. Then, once all the daemons have been started,<br class="">Mesos would execute “mpiexec” to start the application.<br class=""><br class="">Is that correct?<br class=""></blockquote><br class="">Yes<br class=""><br class=""><blockquote type="cite" class="">If so, then we don’t support that mode today, but it could fairly easily<br class="">be added.<br class="">However, I don’t see why you couldn’t just write a small<br class="">standalone tool that collects all the Mesos resources in a file until<br class="">all have been assembled, and then executes “mpiexec -hostfile &lt;myfile&gt;”.<br class=""></blockquote><br class="">Because, mpiexec will eventually rely ssh to run mpi proxy on hosts,</div></div></blockquote><div><br class=""></div>What’s the problem with that? It’s how many HPC clusters work. Is ssh not enabled?</div><div><br class=""><blockquote type="cite" class=""><div class=""><div class="">while<br class="">in Mesos, it works like: framework passes information about on which host<br class="">to run what commands, and pass such information to Mesos master, then Mesos<br class="">master will instruct hosts to run commands.<br class=""><br class="">This is where Mesos work module doesn't fit into Open MPI.<br class=""></div></div></blockquote><div><br class=""></div>Easiest thing would be to add a Mesos PLM plugin to OMPI - IIRC, someone once did that, but nobody was interested and so it died</div><div><br class=""></div><div><br class=""><blockquote type="cite" class=""><div class=""><div class=""><br class=""><blockquote type="cite" class="">Is there some reason this won’t work? It would be much simpler and would<br class="">work with any MPI.<br class=""><br class="">Ralph<br class=""><br class=""><br class=""><blockquote type="cite" class="">On Jun 3, 2016, at 5:10 PM, Du, Fan &lt;<a href="mailto:fan.du@intel.com" class="">fan.du@intel.com</a><br class="">&lt;<a href="mailto:fan.du@intel.com" class="">mailto:fan.du@intel.com</a>&gt;&gt; wrote:<br class=""><br class=""><br class=""><br class="">On 2016/6/2 19:14, Gilles Gouaillardet wrote:<br class=""><blockquote type="cite" class="">Hi,<br class=""><br class="">may I ask why you need/want to launch orted manually ?<br class=""></blockquote><br class="">Good question.<br class=""><br class="">The intention is to get orted commands, and run orted with Mesos.<br class="">This all comes from who Mesos works, in essence it offers<br class="">resources(cpu/memory/ports)<br class="">in a per host basis to framework, framework then builds information of<br class="">how to run<br class="">specific tasks, and pass those information to Mesos master, at last<br class="">Mesos will<br class="">instructs hosts to execute the framework tasks.<br class=""><br class="">Take MPICH2 as example, the framework to support MPICH2 works as above.<br class="">1. framework gets offers from Mesos master, and tells the Mesos master<br class="">to run a wrapper<br class="">of MPICH2 proxy(hydra_pmi_proxy), at this time, the wrapper waits for<br class="">commands to<br class="">execute the proxy.<br class=""><br class="">2. After launch enough MPICH2 proxy wrapper on hosts as user expect,<br class="">then run the<br class="">real mpiexec program with '-launcher manual' to grab commands for the<br class="">proxy, then<br class="">pass those commands to the proxy wrapper, so finally the real MPICH2<br class="">proxy got launched,<br class="">and mpiexec will proceed on normally.<br class=""><br class="">That's why I'm looking for similar functionality as '-launcher manual<br class="">MPICH2.<br class="">Non native speaker, I hope I tell the story clear :)<br class=""><br class=""><br class=""><br class=""><blockquote type="cite" class="">unless you are running under a batch manager, Open MPI uses the rsh pml<br class="">to remotely start orted.<br class="">basically, it does<br class="">ssh host orted &lt;orted params&gt;<br class=""><br class="">the best I can suggest is you do<br class=""><br class="">mpirun --mca orte_rsh_agent myrshagent.sh --mca orte_launch_agent<br class="">mylaunchagent.sh &nbsp;...<br class="">under the hood, mpirun will do<br class="">myrshagent.sh host mylaunchagent.sh &lt;orted params&gt;<br class=""><br class="">Cheers,<br class=""><br class="">Gilles<br class=""><br class="">On Thursday, June 2, 2016, Du, Fan &lt;<a href="mailto:fan.du@intel.com" class="">fan.du@intel.com</a><br class="">&lt;<a href="mailto:fan.du@intel.com" class="">mailto:fan.du@intel.com</a>&gt;<br class="">&lt;<a href="mailto:fan.du@intel.com" class="">mailto:fan.du@intel.com</a>&gt;&gt; wrote:<br class=""><br class=""> &nbsp;&nbsp;Hi folks<br class=""><br class=""> &nbsp;&nbsp;Starting from Open MPI, I can launch mpi application a.out as<br class=""> &nbsp;&nbsp;following on host1<br class=""> &nbsp;&nbsp;mpirun --allow-run-as-root --host host1,host2 -np 4 /tmp/a.out<br class=""><br class=""> &nbsp;&nbsp;On host2, I saw an proxy, say orted here is spawned:<br class=""> &nbsp;&nbsp;orted --hnp-topo-sig 4N:2S:4L3:20L2:20L1:20C:40H:x86_64 -mca ess env<br class=""> &nbsp;&nbsp;-mca orte_ess_jobid 1275133952 -mca orte_ess_vpid 1 -mca<br class=""> &nbsp;&nbsp;orte_ess_num_procs 2 -mca orte_hnp_uri<br class=""> &nbsp;&nbsp;1275133952.0;<a href="tcp://host1_ip:40024" class="">tcp://host1_ip:40024</a> --tree-spawn -mca plm rsh<br class="">--tree-spawn<br class=""><br class=""> &nbsp;&nbsp;It seems mpirun use ssh as launcher on my system.<br class=""> &nbsp;&nbsp;What if I want to run orted things manually, not by mpirun<br class=""> &nbsp;&nbsp;automatically,<br class=""> &nbsp;&nbsp;I mean, does mpirun has any option to produce commands for orted?<br class=""><br class=""> &nbsp;&nbsp;As for MPICH2 implementation, there is "-launcher manual" option to<br class=""> &nbsp;&nbsp;make this works,<br class=""> &nbsp;&nbsp;for example:<br class=""> &nbsp;&nbsp;# mpiexec.hydra -launcher manual -np 4 htop<br class=""> &nbsp;&nbsp;HYDRA_LAUNCH: /usr/local/bin/hydra_pmi_proxy --control-port<br class=""> &nbsp;&nbsp;grantleyIPDC04:34652 --rmk user --launcher manual --demux poll<br class=""> &nbsp;&nbsp;--pgid 0 --retries 10 --usize -2 --proxy-id 0<br class=""> &nbsp;&nbsp;HYDRA_LAUNCH_END<br class=""><br class=""> &nbsp;&nbsp;Then I can manually run hydra_pmi_proxy with commands, and<br class=""> &nbsp;&nbsp;mpiexec.hydra will proceed on.<br class=""><br class=""> &nbsp;&nbsp;Thanks!<br class=""> &nbsp;&nbsp;_______________________________________________<br class=""> &nbsp;&nbsp;users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a> &lt;<a href="mailto:users@open-mpi.org" class="">mailto:users@open-mpi.org</a>&gt;<br class=""> &nbsp;&nbsp;Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" class="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class=""> &nbsp;&nbsp;Link to this post:<br class=""><a href="http://www.open-mpi.org/community/lists/users/2016/06/29346.php" class="">http://www.open-mpi.org/community/lists/users/2016/06/29346.php</a><br class=""><br class=""><br class=""><br class="">_______________________________________________<br class="">users mailing list<br class="">users@open-mpi.org &lt;mailto:users@open-mpi.org&gt;<br class="">Subscription: https://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post:<br class="">http://www.open-mpi.org/community/lists/users/2016/06/29347.php<br class=""><br class=""></blockquote>_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a> &lt;<a href="mailto:users@open-mpi.org" class="">mailto:users@open-mpi.org</a>&gt;<br class="">Subscription:https://<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" class="">www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this<br class="">post:http://<a href="http://www.open-mpi.org/community/lists/users/2016/06/29364.php" class="">www.open-mpi.org/community/lists/users/2016/06/29364.php</a><br class=""></blockquote><br class=""><br class=""><br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: https://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2016/06/29367.php<br class=""><br class=""></blockquote>_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: https://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2016/06/29375.php<br class=""></div></div></blockquote></div><br class=""></body></html>
