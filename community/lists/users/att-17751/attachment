<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">So what you are looking for is an MPI extension API that let's you say "migrate me from my current node to node &lt;foo&gt;"? Or do you have a rank that is the "master" that would order "move rank N to node &lt;foo&gt;"?<div><br></div><div>Either could be provided, I imagine - just want to ensure I understand what you need. Can you pass along a brief description of the syntax and functionality you would need?</div><div><br></div><div><br><div><div>On Nov 10, 2011, at 8:27 AM, Mudassar Majeed wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div><div style="color:#000; background-color:#fff; font-family:times new roman, new york, times, serif;font-size:10pt"><div id="yiv1858009824"><div><div style="color:#000;background-color:#fff;font-family:times new roman, new york, times, serif;font-size:10pt;"><br>Thank you for your reply. In our previous publication, we have figured it out that run more than one processes on cores and balancing the computational load considerably reduces the total execution time. You know the MPI_Graph_create function, we created another function MPI_Load_create that maps the processes on cores such that balance of computational load can be achieved on cores. We were having some issues with increase in communication cost due to ranks rearrangements (due to MPI_Comm_split, with color=0), so in this research work we will see how can we balance both computation load on each core and communication load on each node. Those processes that communicate more will reside
 on the same node keeping the computational load balance over the cores. I solved this problem using ILP but ILP takes time and can't be used
 in run time so I am thinking about an heuristic. That's why I want to see if it is possible to migrate a process from one core to another or not. Then I will see how good my heuristic will be.<br><br>thanks<br>Mudassar<br><br><div class="yiv1858009824yui_3_2_0_15_132093308719761" id="yiv1858009824yui_3_2_0_15_132093308719763" style="font-family:times new roman, new york, times, serif;font-size:10pt;"><div class="yiv1858009824yui_3_2_0_15_132093308719778" style="font-family:times new roman, new york, times, serif;font-size:12pt;"><font face="Arial" size="2"><hr size="1"><b><span style="font-weight:bold;">From:</span></b> Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br><b><span style="font-weight:bold;">To:</span></b> Mudassar Majeed &lt;<a href="mailto:mudassarm30@yahoo.com">mudassarm30@yahoo.com</a>&gt;; Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br><b><span style="font-weight:bold;">Cc:</span></b> Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;<br><b><span style="font-weight:bold;">Sent:</span></b> Thursday,
 November 10, 2011 2:19
 PM<br><b><span style="font-weight:bold;">Subject:</span></b> Re: [OMPI users] Process Migration<br></font><br>
On Nov 10, 2011, at 8:11 AM, Mudassar Majeed wrote:<br><br>&gt; Thank you for your reply. I am implementing a load balancing function for MPI, that will balance the computation load and the communication both at a time. So my algorithm assumes that all the cores may at the end get different number of processes to run.<br><br>Are you talking about over-subscribing cores?&nbsp; I.e., putting more than 1 MPI process on each core?<br><br>In general, that's not a good idea.<br><br>&gt; In the beginning (before that function will be called), each core will have equal number of processes. So I am thinking either to start more processes on each core (than needed) and run my function for load balancing and then block the remaining processes (on each core). In this way I will be able to achieve different number of processes per core.<br><br>Open MPI spins aggressively looking for network progress.&nbsp; For example, if you block in an MPI_RECV waiting for a
 message, Open MPI is actively banging on the CPU looking for network progress.&nbsp; Because of this (and other reasons), you probably do not want to over-subscribe your processors (meaning: you probably don't want to put more than 1 process per core).<br><br>-- <br>Jeff Squyres<br><a rel="nofollow" ymailto="mailto:jsquyres@cisco.com" target="_blank" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>For corporate legal information go to:<br><a href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br><br><br><br><span></span></div></div></div></div></div></div></div></blockquote></div><br></div></body></html>
