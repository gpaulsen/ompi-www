Greetings eveyrong, <br><br>My name is Mike, and I have recently downloaded the OMPI v1.2.1 and decide to run the OSU bandwidth benchmark. However, I have noticed a few weird things during my run.<br><br>Btw, I am using FreeBSD 
6.2.<br><br>The OSU bandwidth test basically pre-post many ISend and IRecv. It tries to measure the max. sustainable bandwidth. <br><br>Here is an output (I didn&#39;t finish running, but it should be sufficient to show the problem that I am seeing):
<br><br>Quick system info:<br>Two nodes testing (running Intel P4 Xeon 3.2Ghz Hyperthreading disabled, 1024Mb RAM).<br>3 1-Gig NiCs, all Intel Pro em1000(em0 and em2 are the private interfaces (10.1.x.x) , while em1 is the public interface)
<br><br>----------------------------------<br><br>[myct@netbed21 ~/mpich/osu_benchmarks]$ mpirun --mca btl_tcp_if_include em0 --hostfile ~/mpd.hosts.private --mca btl tcp,self --mca btl_tcp_sndbuf 233016 --mca btl_tcp_rcvbuf 233016&nbsp; -np 2 ./osu_bw
<br># OSU MPI Bandwidth Test (Version 2.3)<br># Size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Bandwidth (MB/s)<br>1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.12<br>2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.26<br>4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.53<br>8&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.06<br>16&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.12<br>32&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.22
<br>64&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8.26<br>128&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 14.61<br>256&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 28.06<br>512&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 51.27<br>1024&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 82.59<br>2048&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 102.21<br>4096&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 110.53<br>8192&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 114.58<br>16384&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
118.16<br>32768&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 120.71<br>65536&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 33.23<br>131072&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 41.75<br>262144&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 70.42<br>524288&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 82.96<br>^Cmpirun: killing job...<br><br>------------------------------<br><br>The rendezvous&nbsp; threshold is set to 64k by default. 
<br><br>It seems that when the rendezvous starts, the performance dropped tremendously. <br>Btw, this is an out-of-box run, I have not tweaked anything except changing the socket buffer sizes during runtime.<br>Is there something obvious that I am not doing correctly?
<br><br>I have also attached the &quot;ompi-info&quot; output.<br><br>Thanks for everything, <br><br>Mike<br>

