<br><br><div class="gmail_quote">On Wed, Sep 26, 2012 at 10:58 PM, Siegmar Gross <span dir="ltr">&lt;<a href="mailto:Siegmar.Gross@informatik.hs-fulda.de" target="_blank">Siegmar.Gross@informatik.hs-fulda.de</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi,<br>
<br>
the command works without linpc4 or with -mca btl ^sctp.<br></blockquote><div><br>Excellent!<br> <br></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<br>
mpiexec -np 4 -host rs0,sunpc4,linpc4 environ_mpi | &amp; more<br>
<br>
[<a href="http://sunpc4.informatik.hs-fulda.de" target="_blank">sunpc4.informatik.hs-fulda.de</a>][[6074,1],2][../<br>
<br>
<br>
tyr hello_1 162 mpiexec -np 4 -host rs0,sunpc4 environ_mpi | &amp; more<br>
<div class="im"><br>
Now 3 slave tasks are sending their environment.<br>
<br>
<br>
</div>tyr hello_1 163 mpiexec -mca btl ^sctp -np 4 -host rs0,sunpc4,linpc4 \<br>
  environ_mpi | &amp; more<br>
<div class="im"><br>
Now 3 slave tasks are sending their environment.<br>
<br>
<br>
</div>Later I try to build some older tarballs from the trunk so that I can<br>
find out which one broke my binding commands. Is it possible to get<br>
openmpi-1.9a1r27348.tar.bz2 and openmpi-1.9a1r27354.tar.bz2 so that I<br>
can try with every sixth tarball starting from openmpi-1.9a1r27342.tar.bz2<br>
and then continue with the next interval?<br></blockquote><div><br>We don&#39;t generally keep old tarballs of the trunk around. Your best bet is to just do an svn checkout of whatever revision level you want to try and build from that - only difference is that you need to do an &quot;./<a href="http://autogen.pl">autogen.pl</a>&quot; before you can run the configure cmd, and that requires you to have autoconf, automake, and libtool on your machine (which you probably already do).<br>
<br>I would take bigger steps initially. It apparently works for you in the 1.6 series,  which is a continuation of the 1.5 series. Looking at the version timeline, the 1.5 series branched from the trunk in Jan 2010, so we have 2.5 years to cover and a version range that starts around r22500. Quite a lot of changes!<br>
<br>I would suggest starting at r22500 and trying every 1000 revisions initially. Remember that not every single revision will build and work correctly - this is the developer&#39;s trunk and it is occasionally broken for a short while - but shifting a little bit when that happens should solve the problem.<br>
<br>The most likely points will be where either the hwloc code was updated, or the launch code. The former occurs fairly rarely - you might be able to detect if that&#39;s a problem by simply testing the various hwloc releases as we just absorb them as they come. There are only a few of them over that time period so that might be a lot faster. See if hwloc can correctly read your topology and bind processes - if not, then OMPI won&#39;t be able to either.<br>
<br>The launch code changes usually come in big chunks, followed by minor patches and then long periods of quiet. So a coarse sampling pattern should pickup the problem.<br><br>HTH<br>Ralph<br><br></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">

<br>
<br>
Kind regards<br>
<span class="HOEnZb"><font color="#888888"><br>
Siegmar<br>
</font></span><div class="im HOEnZb"><br>
<br>
<br>
&gt; Hmmm...well, this is indeed confusing. I see the following in your attached<br>
&gt; output:<br>
&gt;<br>
&gt;<br>
[<a href="http://sunpc4.informatik.hs-fulda.de" target="_blank">sunpc4.informatik.hs-fulda.de</a>][[4083,1],2][../../../../../openmpi-1.9a1r27362/o<br>
mpi/mca/btl/sctp/btl_sctp_proc.c:143:mca_btl_sctp_proc_create]<br>
&gt; mca_base_modex_recv: failed with return value=-13<br>
&gt;<br>
[<a href="http://rs0.informatik.hs-fulda.de" target="_blank">rs0.informatik.hs-fulda.de</a>][[4083,1],3][../../../../../openmpi-1.9a1r27362/ompi<br>
/mca/btl/sctp/btl_sctp_proc.c:143:mca_btl_sctp_proc_create]<br>
&gt; mca_base_modex_recv: failed with return value=-13<br>
&gt;<br>
[<a href="http://rs0.informatik.hs-fulda.de" target="_blank">rs0.informatik.hs-fulda.de</a>][[4083,1],3][../../../../../openmpi-1.9a1r27362/ompi<br>
/mca/btl/sctp/btl_sctp_proc.c:143:mca_btl_sctp_proc_create]<br>
&gt; mca_base_modex_recv: failed with return value=-13<br>
&gt;<br>
[<a href="http://rs0.informatik.hs-fulda.de" target="_blank">rs0.informatik.hs-fulda.de</a>][[4083,1],3][../../../../../openmpi-1.9a1r27362/ompi<br>
</div><div class="HOEnZb"><div class="h5">/mca/btl/sctp/btl_sctp_proc.c:143:mca_btl_sctp_proc_create]<br>
&gt; mca_base_modex_recv: failed with return value=-13<br>
&gt;<br>
&gt;<br>
&gt; This implies that at least some of the processes started and got all the<br>
&gt; way into MPI_Init. You should probably exclude the sctp BTL as it&#39;s not<br>
&gt; necessarily working - just add -mca btl ^sctp to the cmd line.<br>
&gt;<br>
&gt; Does this work if you leave linpc out of it? I&#39;m wondering if this is the<br>
&gt; heterogeneous problem again. Are you sure that the /usr/local... OMPI<br>
&gt; library on that machine is the Linux x86_64 version, and not the Solaris<br>
&gt; one (e.g., if /usr/local was NFS mounted)?<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; On Wed, Sep 26, 2012 at 7:30 AM, Siegmar Gross &lt;<br>
&gt; <a href="mailto:Siegmar.Gross@informatik.hs-fulda.de">Siegmar.Gross@informatik.hs-fulda.de</a>&gt; wrote:<br>
&gt;<br>
&gt; &gt; Hi,<br>
&gt; &gt;<br>
&gt; &gt; &gt; I&#39;m on the road the rest of this week, but can look at this when I return<br>
&gt; &gt; &gt; next week. It looks like something unrelated to the Java bindings failed<br>
&gt; &gt; to<br>
&gt; &gt; &gt; properly initialize - at a guess, I&#39;d suspect that you are missing the<br>
&gt; &gt; &gt; LD_LIBRARY_PATH setting so none of the OMPI libs were found.<br>
&gt; &gt;<br>
&gt; &gt; Perhaps the output of my environment program is helpful in that case.<br>
&gt; &gt; I attached my environment.<br>
&gt; &gt;<br>
&gt; &gt; mpiexec -np 4 -host linpc4,sunpc4,rs0 environ_mpi \<br>
&gt; &gt;   &gt;&amp; env_linpc_sunpc_sparc.txt<br>
&gt; &gt;<br>
&gt; &gt; Thank you very much for your help in advance.<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; Kind regards<br>
&gt; &gt;<br>
&gt; &gt; Siegmar<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; &gt; On Wed, Sep 26, 2012 at 5:42 AM, Siegmar Gross &lt;<br>
&gt; &gt; &gt; <a href="mailto:Siegmar.Gross@informatik.hs-fulda.de">Siegmar.Gross@informatik.hs-fulda.de</a>&gt; wrote:<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; Hi,<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; yesterday I installed openmpi-1.9a1r27362 on Solaris and Linux and<br>
&gt; &gt; &gt; &gt; I have a problem with mpiJava on Linux (openSUSE-Linux 12.1, x86_64).<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; linpc4 mpi_classfiles 104 javac HelloMainWithoutMPI.java<br>
&gt; &gt; &gt; &gt; linpc4 mpi_classfiles 105 mpijavac HelloMainWithBarrier.java<br>
&gt; &gt; &gt; &gt; linpc4 mpi_classfiles 106 mpijavac -showme<br>
&gt; &gt; &gt; &gt; /usr/local/jdk1.7.0_07-64/bin/javac \<br>
&gt; &gt; &gt; &gt;   -cp ...:.:/usr/local/openmpi-1.9_64_cc/lib64/mpi.jar<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; It works with Java without MPI.<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; linpc4 mpi_classfiles 107 mpiexec java -cp $HOME/mpi_classfiles \<br>
&gt; &gt; &gt; &gt;   HelloMainWithoutMPI<br>
&gt; &gt; &gt; &gt; Hello from <a href="http://linpc4.informatik.hs-fulda.de/193.174.26.225" target="_blank">linpc4.informatik.hs-fulda.de/193.174.26.225</a><br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; It breaks with Java and MPI.<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; linpc4 mpi_classfiles 108 mpiexec java -cp $HOME/mpi_classfiles \<br>
&gt; &gt; &gt; &gt;   HelloMainWithBarrier<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; --------------------------------------------------------------------------<br>
&gt; &gt; &gt; &gt; It looks like opal_init failed for some reason; your parallel process<br>
&gt; &gt; is<br>
&gt; &gt; &gt; &gt; likely to abort.  There are many reasons that a parallel process can<br>
&gt; &gt; &gt; &gt; fail during opal_init; some of which are due to configuration or<br>
&gt; &gt; &gt; &gt; environment problems.  This failure appears to be an internal failure;<br>
&gt; &gt; &gt; &gt; here&#39;s some additional information (which may only be relevant to an<br>
&gt; &gt; &gt; &gt; Open MPI developer):<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;   mca_base_open failed<br>
&gt; &gt; &gt; &gt;   --&gt; Returned value -2 instead of OPAL_SUCCESS<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; --------------------------------------------------------------------------<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; --------------------------------------------------------------------------<br>
&gt; &gt; &gt; &gt; It looks like orte_init failed for some reason; your parallel process<br>
&gt; &gt; is<br>
&gt; &gt; &gt; &gt; likely to abort.  There are many reasons that a parallel process can<br>
&gt; &gt; &gt; &gt; fail during orte_init; some of which are due to configuration or<br>
&gt; &gt; &gt; &gt; environment problems.  This failure appears to be an internal failure;<br>
&gt; &gt; &gt; &gt; here&#39;s some additional information (which may only be relevant to an<br>
&gt; &gt; &gt; &gt; Open MPI developer):<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;   opal_init failed<br>
&gt; &gt; &gt; &gt;   --&gt; Returned value Out of resource (-2) instead of ORTE_SUCCESS<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; --------------------------------------------------------------------------<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; --------------------------------------------------------------------------<br>
&gt; &gt; &gt; &gt; It looks like MPI_INIT failed for some reason; your parallel process is<br>
&gt; &gt; &gt; &gt; likely to abort.  There are many reasons that a parallel process can<br>
&gt; &gt; &gt; &gt; fail during MPI_INIT; some of which are due to configuration or<br>
&gt; &gt; environment<br>
&gt; &gt; &gt; &gt; problems.  This failure appears to be an internal failure; here&#39;s some<br>
&gt; &gt; &gt; &gt; additional information (which may only be relevant to an Open MPI<br>
&gt; &gt; &gt; &gt; developer):<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;   ompi_mpi_init: orte_init failed<br>
&gt; &gt; &gt; &gt;   --&gt; Returned &quot;Out of resource&quot; (-2) instead of &quot;Success&quot; (0)<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; --------------------------------------------------------------------------<br>
&gt; &gt; &gt; &gt; *** An error occurred in MPI_Init<br>
&gt; &gt; &gt; &gt; *** on a NULL communicator<br>
&gt; &gt; &gt; &gt; *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now<br>
&gt; &gt; abort,<br>
&gt; &gt; &gt; &gt; ***    and potentially your MPI job)<br>
&gt; &gt; &gt; &gt; [linpc4:15332] Local abort before MPI_INIT completed successfully; not<br>
&gt; &gt; &gt; &gt; able to<br>
&gt; &gt; &gt; &gt; aggregate error messages, and not able to guarantee that all other<br>
&gt; &gt; &gt; &gt; processes were<br>
&gt; &gt; &gt; &gt; killed!<br>
&gt; &gt; &gt; &gt; -------------------------------------------------------<br>
&gt; &gt; &gt; &gt; Primary job  terminated normally, but 1 process returned<br>
&gt; &gt; &gt; &gt; a non-zero exit code.. Per user-direction, the job has been aborted.<br>
&gt; &gt; &gt; &gt; -------------------------------------------------------<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; --------------------------------------------------------------------------<br>
&gt; &gt; &gt; &gt; mpiexec detected that one or more processes exited with non-zero<br>
&gt; &gt; status,<br>
&gt; &gt; &gt; &gt; thus<br>
&gt; &gt; &gt; &gt; causing<br>
&gt; &gt; &gt; &gt; the job to be terminated. The first process to do so was:<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;   Process name: [[58875,1],0]<br>
&gt; &gt; &gt; &gt;   Exit code:    1<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; --------------------------------------------------------------------------<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; I configured with the following command.<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; ../openmpi-1.9a1r27362/configure --prefix=/usr/local/openmpi-1.9_64_cc<br>
&gt; &gt; \<br>
&gt; &gt; &gt; &gt;   --libdir=/usr/local/openmpi-1.9_64_cc/lib64 \<br>
&gt; &gt; &gt; &gt;   --with-jdk-bindir=/usr/local/jdk1.7.0_07-64/bin \<br>
&gt; &gt; &gt; &gt;   --with-jdk-headers=/usr/local/jdk1.7.0_07-64/include \<br>
&gt; &gt; &gt; &gt;   JAVA_HOME=/usr/local/jdk1.7.0_07-64 \<br>
&gt; &gt; &gt; &gt;   LDFLAGS=&quot;-m64&quot; \<br>
&gt; &gt; &gt; &gt;   CC=&quot;cc&quot; CXX=&quot;CC&quot; FC=&quot;f95&quot; \<br>
&gt; &gt; &gt; &gt;   CFLAGS=&quot;-m64&quot; CXXFLAGS=&quot;-m64 -library=stlport4&quot; FCFLAGS=&quot;-m64&quot; \<br>
&gt; &gt; &gt; &gt;   CPP=&quot;cpp&quot; CXXCPP=&quot;cpp&quot; \<br>
&gt; &gt; &gt; &gt;   CPPFLAGS=&quot;&quot; CXXCPPFLAGS=&quot;&quot; \<br>
&gt; &gt; &gt; &gt;   C_INCL_PATH=&quot;&quot; C_INCLUDE_PATH=&quot;&quot; CPLUS_INCLUDE_PATH=&quot;&quot; \<br>
&gt; &gt; &gt; &gt;   OBJC_INCLUDE_PATH=&quot;&quot; OPENMPI_HOME=&quot;&quot; \<br>
&gt; &gt; &gt; &gt;   --enable-cxx-exceptions \<br>
&gt; &gt; &gt; &gt;   --enable-mpi-java \<br>
&gt; &gt; &gt; &gt;   --enable-heterogeneous \<br>
&gt; &gt; &gt; &gt;   --enable-opal-multi-threads \<br>
&gt; &gt; &gt; &gt;   --enable-mpi-thread-multiple \<br>
&gt; &gt; &gt; &gt;   --with-threads=posix \<br>
&gt; &gt; &gt; &gt;   --with-hwloc=internal \<br>
&gt; &gt; &gt; &gt;   --without-verbs \<br>
&gt; &gt; &gt; &gt;   --without-udapl \<br>
&gt; &gt; &gt; &gt;   --with-wrapper-cflags=-m64 \<br>
&gt; &gt; &gt; &gt;   --enable-debug \<br>
&gt; &gt; &gt; &gt;   |&amp; tee log.configure.$SYSTEM_ENV.$MACHINE_ENV.64_cc<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; It works fine on Solaris machines as long as the hosts belong to the<br>
&gt; &gt; &gt; &gt; same kind (Sparc or x86_64).<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; tyr mpi_classfiles 194 mpiexec -host sunpc0,sunpc1,sunpc4 \<br>
&gt; &gt; &gt; &gt;   java -cp $HOME/mpi_classfiles HelloMainWithBarrier<br>
&gt; &gt; &gt; &gt; Process 1 of 3 running on sunpc1<br>
&gt; &gt; &gt; &gt; Process 2 of 3 running on <a href="http://sunpc4.informatik.hs-fulda.de" target="_blank">sunpc4.informatik.hs-fulda.de</a><br>
&gt; &gt; &gt; &gt; Process 0 of 3 running on sunpc0<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; sunpc4 fd1026 107 mpiexec -host tyr,rs0,rs1 \<br>
&gt; &gt; &gt; &gt;   java -cp $HOME/mpi_classfiles HelloMainWithBarrier<br>
&gt; &gt; &gt; &gt; Process 1 of 3 running on <a href="http://rs0.informatik.hs-fulda.de" target="_blank">rs0.informatik.hs-fulda.de</a><br>
&gt; &gt; &gt; &gt; Process 2 of 3 running on <a href="http://rs1.informatik.hs-fulda.de" target="_blank">rs1.informatik.hs-fulda.de</a><br>
&gt; &gt; &gt; &gt; Process 0 of 3 running on <a href="http://tyr.informatik.hs-fulda.de" target="_blank">tyr.informatik.hs-fulda.de</a><br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; It breaks if the hosts belong to both kinds of machines.<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; sunpc4 fd1026 106 mpiexec -host tyr,rs0,sunpc1 \<br>
&gt; &gt; &gt; &gt;   java -cp $HOME/mpi_classfiles HelloMainWithBarrier<br>
&gt; &gt; &gt; &gt; [<a href="http://rs0.informatik.hs-fulda.de:7718" target="_blank">rs0.informatik.hs-fulda.de:7718</a>] *** An error occurred in<br>
&gt; &gt; MPI_Comm_dup<br>
&gt; &gt; &gt; &gt; [<a href="http://rs0.informatik.hs-fulda.de:7718" target="_blank">rs0.informatik.hs-fulda.de:7718</a>] *** reported by process<br>
&gt; &gt; [565116929,1]<br>
&gt; &gt; &gt; &gt; [<a href="http://rs0.informatik.hs-fulda.de:7718" target="_blank">rs0.informatik.hs-fulda.de:7718</a>] *** on communicator MPI_COMM_WORLD<br>
&gt; &gt; &gt; &gt; [<a href="http://rs0.informatik.hs-fulda.de:7718" target="_blank">rs0.informatik.hs-fulda.de:7718</a>] *** MPI_ERR_INTERN: internal error<br>
&gt; &gt; &gt; &gt; [<a href="http://rs0.informatik.hs-fulda.de:7718" target="_blank">rs0.informatik.hs-fulda.de:7718</a>] *** MPI_ERRORS_ARE_FATAL (processes<br>
&gt; &gt; &gt; &gt;   in this communicator will now abort,<br>
&gt; &gt; &gt; &gt; [<a href="http://rs0.informatik.hs-fulda.de:7718" target="_blank">rs0.informatik.hs-fulda.de:7718</a>] ***    and potentially your MPI job)<br>
&gt; &gt; &gt; &gt; [<a href="http://sunpc4.informatik.hs-fulda.de:07900" target="_blank">sunpc4.informatik.hs-fulda.de:07900</a>] 1 more process has sent help<br>
&gt; &gt; &gt; &gt;   message help-mpi-errors.txt / mpi_errors_are_fatal<br>
&gt; &gt; &gt; &gt; [<a href="http://sunpc4.informatik.hs-fulda.de:07900" target="_blank">sunpc4.informatik.hs-fulda.de:07900</a>] Set MCA parameter<br>
&gt; &gt; &gt; &gt;   &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; Please let me know if I can provide anything else to track these<br>
&gt; &gt; errors.<br>
&gt; &gt; &gt; &gt; Thank you very much for any help in advance.<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; Kind regards<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; Siegmar<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; _______________________________________________<br>
&gt; &gt; &gt; &gt; users mailing list<br>
&gt; &gt; &gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; &gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
[<a href="http://sunpc4.informatik.hs-fulda.de" target="_blank">sunpc4.informatik.hs-fulda.de</a>][[4083,1],2][../../../../../openmpi-1.9a1r27362/o<br>
</div></div><div class="im HOEnZb">mpi/mca/btl/sctp/btl_sctp_proc.c:143:mca_btl_sctp_proc_create]<br>
&gt; &gt; mca_base_modex_recv: failed with return value=-13<br>
&gt; &gt;<br>
[<a href="http://rs0.informatik.hs-fulda.de" target="_blank">rs0.informatik.hs-fulda.de</a>][[4083,1],3][../../../../../openmpi-1.9a1r27362/ompi<br>
/mca/btl/sctp/btl_sctp_proc.c:143:mca_btl_sctp_proc_create]<br>
&gt; &gt; mca_base_modex_recv: failed with return value=-13<br>
&gt; &gt;<br>
[<a href="http://rs0.informatik.hs-fulda.de" target="_blank">rs0.informatik.hs-fulda.de</a>][[4083,1],3][../../../../../openmpi-1.9a1r27362/ompi<br>
/mca/btl/sctp/btl_sctp_proc.c:143:mca_btl_sctp_proc_create]<br>
&gt; &gt; mca_base_modex_recv: failed with return value=-13<br>
&gt; &gt;<br>
[<a href="http://rs0.informatik.hs-fulda.de" target="_blank">rs0.informatik.hs-fulda.de</a>][[4083,1],3][../../../../../openmpi-1.9a1r27362/ompi<br>
</div><div class="HOEnZb"><div class="h5">/mca/btl/sctp/btl_sctp_proc.c:143:mca_btl_sctp_proc_create]<br>
&gt; &gt; mca_base_modex_recv: failed with return value=-13<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; Now 3 slave tasks are sending their environment.<br>
&gt; &gt;<br>
&gt; &gt; Environment from task 1:<br>
&gt; &gt;   message type:        3<br>
&gt; &gt;   msg length:          3911 characters<br>
&gt; &gt;   message:<br>
&gt; &gt;     hostname:          linpc4<br>
&gt; &gt;     operating system:  Linux<br>
&gt; &gt;     release:           3.1.9-1.4-desktop<br>
&gt; &gt;     processor:         x86_64<br>
&gt; &gt;     PATH<br>
&gt; &gt;                        /usr/local/eclipse-3.6.1<br>
&gt; &gt;                        /usr/local/NetBeans-4.0/bin<br>
&gt; &gt;                        /usr/local/jdk1.7.0_07-64/bin<br>
&gt; &gt;                        /usr/local/apache-ant-1.6.2/bin<br>
&gt; &gt;                        /usr/local/icc-9.1/idb/bin<br>
&gt; &gt;                        /usr/local/icc-9.1/cc/bin<br>
&gt; &gt;                        /usr/local/icc-9.1/fc/bin<br>
&gt; &gt;                        /usr/local/gcc-4.7.1/bin<br>
&gt; &gt;                        /opt/solstudio12.3/bin<br>
&gt; &gt;                        /usr/local/bin<br>
&gt; &gt;                        /usr/local/ssl/bin<br>
&gt; &gt;                        /usr/local/pgsql/bin<br>
&gt; &gt;                        /bin<br>
&gt; &gt;                        /usr/bin<br>
&gt; &gt;                        /usr/X11R6/bin<br>
&gt; &gt;                        /usr/local/teTeX-1.0.7/bin/i586-pc-linux-gnu<br>
&gt; &gt;                        /usr/local/bluej-2.1.2<br>
&gt; &gt;                        /usr/local/openmpi-1.9_64_cc/bin<br>
&gt; &gt;                        /home/fd1026/Linux/x86_64/bin<br>
&gt; &gt;                        .<br>
&gt; &gt;                        /usr/sbin<br>
&gt; &gt;     LD_LIBRARY_PATH_32<br>
&gt; &gt;                        /usr/lib<br>
&gt; &gt;                        /usr/local/jdk1.7.0_07-64/jre/lib/i386<br>
&gt; &gt;                        /usr/local/gcc-4.7.1/lib<br>
&gt; &gt;<br>
&gt; &gt;  /usr/local/gcc-4.7.1/libexec/gcc/x86_64-unknown-linux-gnu/4.7.1/32<br>
&gt; &gt;<br>
&gt; &gt;  /usr/local/gcc-4.7.1/lib/gcc/x86_64-unknown-linux-gnu/4.7.1/32<br>
&gt; &gt;                        /usr/local/lib<br>
&gt; &gt;                        /usr/local/ssl/lib<br>
&gt; &gt;                        /lib<br>
&gt; &gt;                        /usr/lib<br>
&gt; &gt;                        /usr/X11R6/lib<br>
&gt; &gt;                        /usr/local/openmpi-1.9_64_cc/lib<br>
&gt; &gt;                        /home/fd1026/Linux/x86_64/lib<br>
&gt; &gt;     LD_LIBRARY_PATH_64<br>
&gt; &gt;                        /usr/lib64<br>
&gt; &gt;                        /usr/local/jdk1.7.0_07-64/jre/lib/amd64<br>
&gt; &gt;                        /usr/local/gcc-4.7.1/lib64<br>
&gt; &gt;<br>
&gt; &gt;  /usr/local/gcc-4.7.1/libexec/gcc/x86_64-unknown-linux-gnu/4.7.1<br>
&gt; &gt;<br>
&gt; &gt;  /usr/local/gcc-4.7.1/lib/gcc/x86_64-unknown-linux-gnu/4.7.1<br>
&gt; &gt;                        /usr/local/lib64<br>
&gt; &gt;                        /usr/local/ssl/lib64<br>
&gt; &gt;                        /usr/lib64<br>
&gt; &gt;                        /usr/X11R6/lib64<br>
&gt; &gt;                        /usr/local/openmpi-1.9_64_cc/lib64<br>
&gt; &gt;                        /home/fd1026/Linux/x86_64/lib64<br>
&gt; &gt;     LD_LIBRARY_PATH<br>
&gt; &gt;                        /usr/lib<br>
&gt; &gt;                        /usr/local/jdk1.7.0_07-64/jre/lib/i386<br>
&gt; &gt;                        /usr/local/gcc-4.7.1/lib<br>
&gt; &gt;<br>
&gt; &gt;  /usr/local/gcc-4.7.1/libexec/gcc/x86_64-unknown-linux-gnu/4.7.1/32<br>
&gt; &gt;<br>
&gt; &gt;  /usr/local/gcc-4.7.1/lib/gcc/x86_64-unknown-linux-gnu/4.7.1/32<br>
&gt; &gt;                        /usr/local/lib<br>
&gt; &gt;                        /usr/local/ssl/lib<br>
&gt; &gt;                        /lib<br>
&gt; &gt;                        /usr/lib<br>
&gt; &gt;                        /usr/X11R6/lib<br>
&gt; &gt;                        /usr/local/openmpi-1.9_64_cc/lib<br>
&gt; &gt;                        /usr/lib64<br>
&gt; &gt;                        /usr/local/jdk1.7.0_07-64/jre/lib/amd64<br>
&gt; &gt;                        /usr/local/gcc-4.7.1/lib64<br>
&gt; &gt;<br>
&gt; &gt;  /usr/local/gcc-4.7.1/libexec/gcc/x86_64-unknown-linux-gnu/4.7.1<br>
&gt; &gt;<br>
&gt; &gt;  /usr/local/gcc-4.7.1/lib/gcc/x86_64-unknown-linux-gnu/4.7.1<br>
&gt; &gt;                        /usr/local/lib64<br>
&gt; &gt;                        /usr/local/ssl/lib64<br>
&gt; &gt;                        /usr/lib64<br>
&gt; &gt;                        /usr/X11R6/lib64<br>
&gt; &gt;                        /usr/local/openmpi-1.9_64_cc/lib64<br>
&gt; &gt;                        /home/fd1026/Linux/x86_64/lib64<br>
&gt; &gt;     CLASSPATH<br>
&gt; &gt;                        /usr/local/junit4.10<br>
&gt; &gt;                        /usr/local/junit4.10/junit-4.10.jar<br>
&gt; &gt;                        //usr/local/jdk1.7.0_07-64/j3d/lib/ext/j3dcore.jar<br>
&gt; &gt;                        //usr/local/jdk1.7.0_07-64/j3d/lib/ext/j3dutils.jar<br>
&gt; &gt;                        //usr/local/jdk1.7.0_07-64/j3d/lib/ext/vecmath.jar<br>
&gt; &gt;                        /usr/local/javacc-5.0/javacc.jar<br>
&gt; &gt;                        .<br>
&gt; &gt;<br>
&gt; &gt; Environment from task 2:<br>
&gt; &gt;   message type:        3<br>
&gt; &gt;   msg length:          4196 characters<br>
&gt; &gt;   message:<br>
&gt; &gt;     hostname:          <a href="http://sunpc4.informatik.hs-fulda.de" target="_blank">sunpc4.informatik.hs-fulda.de</a><br>
&gt; &gt;     operating system:  SunOS<br>
&gt; &gt;     release:           5.10<br>
&gt; &gt;     processor:         i86pc<br>
&gt; &gt;     PATH<br>
&gt; &gt;                        /usr/local/eclipse-3.6.1<br>
&gt; &gt;                        /usr/local/NetBeans-4.0/bin<br>
&gt; &gt;                        /usr/local/jdk1.7.0_07/bin/amd64<br>
&gt; &gt;                        /usr/local/apache-ant-1.6.2/bin<br>
&gt; &gt;                        /usr/local/gcc-4.7.1/bin<br>
&gt; &gt;                        /opt/solstudio12.3/bin<br>
&gt; &gt;                        /usr/local/bin<br>
&gt; &gt;                        /usr/local/ssl/bin<br>
&gt; &gt;                        /usr/local/pgsql/bin<br>
&gt; &gt;                        /usr/bin<br>
&gt; &gt;                        /usr/openwin/bin<br>
&gt; &gt;                        /usr/dt/bin<br>
&gt; &gt;                        /usr/ccs/bin<br>
&gt; &gt;                        /usr/sfw/bin<br>
&gt; &gt;                        /opt/sfw/bin<br>
&gt; &gt;                        /usr/ucb<br>
&gt; &gt;                        /usr/lib/lp/postscript<br>
&gt; &gt;                        /usr/local/teTeX-1.0.7/bin/i386-pc-solaris2.10<br>
&gt; &gt;                        /usr/local/bluej-2.1.2<br>
&gt; &gt;                        /usr/local/openmpi-1.9_64_cc/bin<br>
&gt; &gt;                        /home/fd1026/SunOS/x86_64/bin<br>
&gt; &gt;                        .<br>
&gt; &gt;                        /usr/sbin<br>
&gt; &gt;     LD_LIBRARY_PATH_32<br>
&gt; &gt;                        /usr/lib<br>
&gt; &gt;                        /usr/local/jdk1.7.0_07/jre/lib/i386<br>
&gt; &gt;                        /usr/local/gcc-4.7.1/lib<br>
&gt; &gt;<br>
&gt; &gt;  /usr/local/gcc-4.7.1/lib/gcc/i386-pc-solaris2.10/4.7.1<br>
&gt; &gt;                        /usr/local/lib<br>
&gt; &gt;                        /usr/local/ssl/lib<br>
&gt; &gt;                        /usr/local/oracle<br>
&gt; &gt;                        /usr/local/pgsql/lib<br>
&gt; &gt;                        /usr/lib<br>
&gt; &gt;                        /usr/openwin/lib<br>
&gt; &gt;                        /usr/openwin/server/lib<br>
&gt; &gt;                        /usr/dt/lib<br>
&gt; &gt;                        /usr/X11R6/lib<br>
&gt; &gt;                        /usr/ccs/lib<br>
&gt; &gt;                        /usr/sfw/lib<br>
&gt; &gt;                        /opt/sfw/lib<br>
&gt; &gt;                        /usr/ucblib<br>
&gt; &gt;                        /usr/local/openmpi-1.9_64_cc/lib<br>
&gt; &gt;                        /home/fd1026/SunOS/x86_64/lib<br>
&gt; &gt;     LD_LIBRARY_PATH_64<br>
&gt; &gt;                        /usr/lib/amd64<br>
&gt; &gt;                        /usr/local/jdk1.7.0_07/jre/lib/amd64<br>
&gt; &gt;                        /usr/local/gcc-4.7.1/lib/amd64<br>
&gt; &gt;<br>
&gt; &gt;  /usr/local/gcc-4.7.1/lib/gcc/i386-pc-solaris2.10/4.7.1/amd64<br>
&gt; &gt;                        /usr/local/lib/amd64<br>
&gt; &gt;                        /usr/local/ssl/lib/amd64<br>
&gt; &gt;                        /usr/local/lib64<br>
&gt; &gt;                        /usr/lib/amd64<br>
&gt; &gt;                        /usr/openwin/lib/amd64<br>
&gt; &gt;                        /usr/openwin/server/lib/amd64<br>
&gt; &gt;                        /usr/dt/lib/amd64<br>
&gt; &gt;                        /usr/X11R6/lib/amd64<br>
&gt; &gt;                        /usr/ccs/lib/amd64<br>
&gt; &gt;                        /usr/sfw/lib/amd64<br>
&gt; &gt;                        /opt/sfw/lib/amd64<br>
&gt; &gt;                        /usr/ucblib/amd64<br>
&gt; &gt;                        /usr/local/openmpi-1.9_64_cc/lib64<br>
&gt; &gt;                        /home/fd1026/SunOS/x86_64/lib64<br>
&gt; &gt;     LD_LIBRARY_PATH<br>
&gt; &gt;                        /usr/lib/amd64<br>
&gt; &gt;                        /usr/local/jdk1.7.0_07/jre/lib/amd64<br>
&gt; &gt;                        /usr/local/gcc-4.7.1/lib/amd64<br>
&gt; &gt;<br>
&gt; &gt;  /usr/local/gcc-4.7.1/lib/gcc/i386-pc-solaris2.10/4.7.1/amd64<br>
&gt; &gt;                        /usr/local/lib/amd64<br>
&gt; &gt;                        /usr/local/ssl/lib/amd64<br>
&gt; &gt;                        /usr/local/lib64<br>
&gt; &gt;                        /usr/lib/amd64<br>
&gt; &gt;                        /usr/openwin/lib/amd64<br>
&gt; &gt;                        /usr/openwin/server/lib/amd64<br>
&gt; &gt;                        /usr/dt/lib/amd64<br>
&gt; &gt;                        /usr/X11R6/lib/amd64<br>
&gt; &gt;                        /usr/ccs/lib/amd64<br>
&gt; &gt;                        /usr/sfw/lib/amd64<br>
&gt; &gt;                        /opt/sfw/lib/amd64<br>
&gt; &gt;                        /usr/ucblib/amd64<br>
&gt; &gt;                        /usr/local/openmpi-1.9_64_cc/lib64<br>
&gt; &gt;                        /home/fd1026/SunOS/x86_64/lib64<br>
&gt; &gt;     CLASSPATH<br>
&gt; &gt;                        /usr/local/junit4.10<br>
&gt; &gt;                        /usr/local/junit4.10/junit-4.10.jar<br>
&gt; &gt;                        //usr/local/jdk1.7.0_07/j3d/lib/ext/j3dcore.jar<br>
&gt; &gt;                        //usr/local/jdk1.7.0_07/j3d/lib/ext/j3dutils.jar<br>
&gt; &gt;                        //usr/local/jdk1.7.0_07/j3d/lib/ext/vecmath.jar<br>
&gt; &gt;                        /usr/local/javacc-5.0/javacc.jar<br>
&gt; &gt;                        .<br>
&gt; &gt;<br>
&gt; &gt; Environment from task 3:<br>
&gt; &gt;   message type:        3<br>
&gt; &gt;   msg length:          4394 characters<br>
&gt; &gt;   message:<br>
&gt; &gt;     hostname:          <a href="http://rs0.informatik.hs-fulda.de" target="_blank">rs0.informatik.hs-fulda.de</a><br>
&gt; &gt;     operating system:  SunOS<br>
&gt; &gt;     release:           5.10<br>
&gt; &gt;     processor:         sun4u<br>
&gt; &gt;     PATH<br>
&gt; &gt;                        /usr/local/eclipse-3.6.1<br>
&gt; &gt;                        /usr/local/NetBeans-4.0/bin<br>
&gt; &gt;                        /usr/local/jdk1.7.0_07/bin/sparcv9<br>
&gt; &gt;                        /usr/local/apache-ant-1.6.2/bin<br>
&gt; &gt;                        /usr/local/gcc-4.7.1/bin<br>
&gt; &gt;                        /opt/solstudio12.3/bin<br>
&gt; &gt;                        /usr/local/bin<br>
&gt; &gt;                        /usr/local/ssl/bin<br>
&gt; &gt;                        /usr/local/pgsql/bin<br>
&gt; &gt;                        /usr/bin<br>
&gt; &gt;                        /usr/openwin/bin<br>
&gt; &gt;                        /usr/dt/bin<br>
&gt; &gt;                        /usr/ccs/bin<br>
&gt; &gt;                        /usr/sfw/bin<br>
&gt; &gt;                        /opt/sfw/bin<br>
&gt; &gt;                        /usr/ucb<br>
&gt; &gt;                        /usr/xpg4/bin<br>
&gt; &gt;                        /usr/local/teTeX-1.0.7/bin/sparc-sun-solaris2.10<br>
&gt; &gt;                        /usr/local/bluej-2.1.2<br>
&gt; &gt;                        /usr/local/openmpi-1.9_64_cc/bin<br>
&gt; &gt;                        /home/fd1026/SunOS/sparc/bin<br>
&gt; &gt;                        .<br>
&gt; &gt;                        /usr/sbin<br>
&gt; &gt;     LD_LIBRARY_PATH_32<br>
&gt; &gt;                        /usr/lib<br>
&gt; &gt;                        /usr/local/jdk1.7.0_07/jre/lib/sparc<br>
&gt; &gt;                        /usr/local/gcc-4.7.1/lib<br>
&gt; &gt;<br>
&gt; &gt;  /usr/local/gcc-4.7.1/lib/gcc/sparc-sun-solaris2.10/4.7.1<br>
&gt; &gt;                        /usr/local/lib<br>
&gt; &gt;                        /usr/local/ssl/lib<br>
&gt; &gt;                        /usr/local/oracle<br>
&gt; &gt;                        /usr/local/pgsql/lib<br>
&gt; &gt;                        /lib<br>
&gt; &gt;                        /usr/lib<br>
&gt; &gt;                        /usr/openwin/lib<br>
&gt; &gt;                        /usr/dt/lib<br>
&gt; &gt;                        /usr/X11R6/lib<br>
&gt; &gt;                        /usr/ccs/lib<br>
&gt; &gt;                        /usr/sfw/lib<br>
&gt; &gt;                        /opt/sfw/lib<br>
&gt; &gt;                        /usr/ucblib<br>
&gt; &gt;                        /usr/local/openmpi-1.9_64_cc/lib<br>
&gt; &gt;                        /home/fd1026/SunOS/sparc/lib<br>
&gt; &gt;     LD_LIBRARY_PATH_64<br>
&gt; &gt;                        /usr/lib/sparcv9<br>
&gt; &gt;                        /usr/local/jdk1.7.0_07/jre/lib/sparcv9<br>
&gt; &gt;                        /usr/local/gcc-4.7.1/lib/sparcv9<br>
&gt; &gt;<br>
&gt; &gt;  /usr/local/gcc-4.7.1/lib/gcc/sparc-sun-solaris2.10/4.7.1/sparcv9<br>
&gt; &gt;                        /usr/local/lib/sparcv9<br>
&gt; &gt;                        /usr/local/ssl/lib/sparcv9<br>
&gt; &gt;                        /usr/local/lib64<br>
&gt; &gt;                        /usr/local/oracle/sparcv9<br>
&gt; &gt;                        /usr/local/pgsql/lib/sparcv9<br>
&gt; &gt;                        /lib/sparcv9<br>
&gt; &gt;                        /usr/lib/sparcv9<br>
&gt; &gt;                        /usr/openwin/lib/sparcv9<br>
&gt; &gt;                        /usr/dt/lib/sparcv9<br>
&gt; &gt;                        /usr/X11R6/lib/sparcv9<br>
&gt; &gt;                        /usr/ccs/lib/sparcv9<br>
&gt; &gt;                        /usr/sfw/lib/sparcv9<br>
&gt; &gt;                        /opt/sfw/lib/sparcv9<br>
&gt; &gt;                        /usr/ucblib/sparcv9<br>
&gt; &gt;                        /usr/local/openmpi-1.9_64_cc/lib64<br>
&gt; &gt;                        /home/fd1026/SunOS/sparc/lib64<br>
&gt; &gt;     LD_LIBRARY_PATH<br>
&gt; &gt;                        /usr/lib/sparcv9<br>
&gt; &gt;                        /usr/local/jdk1.7.0_07/jre/lib/sparcv9<br>
&gt; &gt;                        /usr/local/gcc-4.7.1/lib/sparcv9<br>
&gt; &gt;<br>
&gt; &gt;  /usr/local/gcc-4.7.1/lib/gcc/sparc-sun-solaris2.10/4.7.1/sparcv9<br>
&gt; &gt;                        /usr/local/lib/sparcv9<br>
&gt; &gt;                        /usr/local/ssl/lib/sparcv9<br>
&gt; &gt;                        /usr/local/lib64<br>
&gt; &gt;                        /usr/local/oracle/sparcv9<br>
&gt; &gt;                        /usr/local/pgsql/lib/sparcv9<br>
&gt; &gt;                        /lib/sparcv9<br>
&gt; &gt;                        /usr/lib/sparcv9<br>
&gt; &gt;                        /usr/openwin/lib/sparcv9<br>
&gt; &gt;                        /usr/dt/lib/sparcv9<br>
&gt; &gt;                        /usr/X11R6/lib/sparcv9<br>
&gt; &gt;                        /usr/ccs/lib/sparcv9<br>
&gt; &gt;                        /usr/sfw/lib/sparcv9<br>
&gt; &gt;                        /opt/sfw/lib/sparcv9<br>
&gt; &gt;                        /usr/ucblib/sparcv9<br>
&gt; &gt;                        /usr/local/openmpi-1.9_64_cc/lib64<br>
&gt; &gt;                        /home/fd1026/SunOS/sparc/lib<br>
&gt; &gt;     CLASSPATH<br>
&gt; &gt;                        /usr/local/junit4.10<br>
&gt; &gt;                        /usr/local/junit4.10/junit-4.10.jar<br>
&gt; &gt;                        //usr/local/jdk1.7.0_07/j3d/lib/ext/j3dcore.jar<br>
&gt; &gt;                        //usr/local/jdk1.7.0_07/j3d/lib/ext/j3dutils.jar<br>
&gt; &gt;                        //usr/local/jdk1.7.0_07/j3d/lib/ext/vecmath.jar<br>
&gt; &gt;                        /usr/local/javacc-5.0/javacc.jar<br>
&gt; &gt;                        .<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
<br>
</div></div></blockquote></div><br>

