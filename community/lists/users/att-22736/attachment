<div dir="ltr"><div><div><div><div>Dear Jeff,<br><br></div>thanks for your help.<br></div><div><br>Maybe I found a clue: in the ompi_info output is the following<br><br> Package: Open MPI <a href="mailto:damiano@caillou.dicat.unige.it">damiano@caillou.dicat.unige.it</a><br>
                          Distribution<br>                Open MPI: 1.6.4<br>   Open MPI SVN revision: r28081<br>   Open MPI release date: Feb 19, 2013<br>                Open RTE: 1.6.4<br>   Open RTE SVN revision: r28081<br>
   Open RTE release date: Feb 19, 2013<br>                    OPAL: 1.6.4<br>       OPAL SVN revision: r28081<br>       OPAL release date: Feb 19, 2013<br>                 MPI API: 2.1<br>            Ident string: 1.6.4<br>
                  <span style="color:rgb(255,0,0)">Prefix: /home/damiano/OpenFOAM/ThirdParty-2.2.0/platforms/linux64Gcc/openmpi-1.6.3</span><br> Configured architecture: x86_64-unknown-linux-gnu<br>          Configure host: <a href="http://caillou.dicat.unige.it">caillou.dicat.unige.it</a><br>
           Configured by: damiano<br>           Configured on: Mon Sep 30 20:23:06 CEST 2013<br>          Configure host: <a href="http://caillou.dicat.unige.it">caillou.dicat.unige.it</a><br>                Built by: damiano<br>
                Built on: Mon Sep 30 20:33:17 CEST 2013<br>              Built host: <a href="http://caillou.dicat.unige.it">caillou.dicat.unige.it</a><br>              C bindings: yes<br>            C++ bindings: yes<br>
      Fortran77 bindings: yes (all)<br>      Fortran90 bindings: yes<br> Fortran90 bindings size: medium<br>              C compiler: gcc<br>     C compiler absolute: /usr/bin/gcc<br>  C compiler family name: GNU<br>      C compiler version: 4.7.1<br>
            C++ compiler: g++<br>   C++ compiler absolute: /usr/bin/g++<br>      Fortran77 compiler: gfortran<br>  Fortran77 compiler abs: /usr/bin/gfortran<br>      Fortran90 compiler: gfortran<br>  Fortran90 compiler abs: /usr/bin/gfortran<br>
             C profiling: yes<br>           C++ profiling: yes<br>     Fortran77 profiling: yes<br>     Fortran90 profiling: yes<br>          C++ exceptions: no<br>          Thread support: posix (MPI_THREAD_MULTIPLE: no, progress: no)<br>
           Sparse Groups: no<br>  Internal debug support: no<br>  MPI interface warnings: no<br>     MPI parameter check: runtime<br>Memory profiling support: no<br>Memory debugging support: no<br>         libltdl support: yes<br>
   Heterogeneous support: no<br> mpirun default --prefix: no<br>         MPI I/O support: yes<br>       MPI_WTIME support: gettimeofday<br>     Symbol vis. support: yes<br>   Host topology support: yes<br>          MPI extensions: affinity example<br>
   FT Checkpoint support: no (checkpoint thread: no)<br>     VampirTrace support: yes<br>  MPI_MAX_PROCESSOR_NAME: 256<br>    MPI_MAX_ERROR_STRING: 256<br>     MPI_MAX_OBJECT_NAME: 64<br>        MPI_MAX_INFO_KEY: 36<br>        MPI_MAX_INFO_VAL: 256<br>
       MPI_MAX_PORT_NAME: 1024<br>  MPI_MAX_DATAREP_STRING: 128<br>           MCA backtrace: execinfo (MCA v2.0, API v2.0, Component v1.6.4)<br>              MCA memory: linux (MCA v2.0, API v2.0, Component v1.6.4)<br>           MCA paffinity: hwloc (MCA v2.0, API v2.0, Component v1.6.4)<br>
               MCA carto: auto_detect (MCA v2.0, API v2.0, Component<br>                          v1.6.4)<br>               MCA carto: file (MCA v2.0, API v2.0, Component v1.6.4)<br>               MCA shmem: mmap (MCA v2.0, API v2.0, Component v1.6.4)<br>
               MCA shmem: posix (MCA v2.0, API v2.0, Component v1.6.4)<br>               MCA shmem: sysv (MCA v2.0, API v2.0, Component v1.6.4)<br>           MCA maffinity: first_use (MCA v2.0, API v2.0, Component<br>                          v1.6.4)<br>
           MCA maffinity: hwloc (MCA v2.0, API v2.0, Component v1.6.4)<br>               MCA timer: linux (MCA v2.0, API v2.0, Component v1.6.4)<br>         MCA installdirs: env (MCA v2.0, API v2.0, Component v1.6.4)<br>         MCA installdirs: config (MCA v2.0, API v2.0, Component v1.6.4)<br>
             MCA sysinfo: linux (MCA v2.0, API v2.0, Component v1.6.4)<br>               MCA hwloc: hwloc132 (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA dpm: orte (MCA v2.0, API v2.0, Component v1.6.4)<br>
              MCA pubsub: orte (MCA v2.0, API v2.0, Component v1.6.4)<br>           MCA allocator: basic (MCA v2.0, API v2.0, Component v1.6.4)<br>           MCA allocator: bucket (MCA v2.0, API v2.0, Component v1.6.4)<br>
                MCA coll: basic (MCA v2.0, API v2.0, Component v1.6.4)<br>                MCA coll: hierarch (MCA v2.0, API v2.0, Component v1.6.4)<br>                MCA coll: inter (MCA v2.0, API v2.0, Component v1.6.4)<br>
                MCA coll: self (MCA v2.0, API v2.0, Component v1.6.4)<br>                MCA coll: sm (MCA v2.0, API v2.0, Component v1.6.4)<br>                MCA coll: sync (MCA v2.0, API v2.0, Component v1.6.4)<br>                MCA coll: tuned (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA io: romio (MCA v2.0, API v2.0, Component v1.6.4)<br>               MCA mpool: fake (MCA v2.0, API v2.0, Component v1.6.4)<br>               MCA mpool: rdma (MCA v2.0, API v2.0, Component v1.6.4)<br>               MCA mpool: sm (MCA v2.0, API v2.0, Component v1.6.4)<br>
                 MCA pml: bfo (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA pml: csum (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA pml: ob1 (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA pml: v (MCA v2.0, API v2.0, Component v1.6.4)<br>
                 MCA bml: r2 (MCA v2.0, API v2.0, Component v1.6.4)<br>              MCA rcache: vma (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA btl: self (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA btl: sm (MCA v2.0, API v2.0, Component v1.6.4)<br>
                 MCA btl: tcp (MCA v2.0, API v2.0, Component v1.6.4)<br>                MCA topo: unity (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA osc: pt2pt (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA osc: rdma (MCA v2.0, API v2.0, Component v1.6.4)<br>
                 MCA iof: hnp (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA iof: orted (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA iof: tool (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA oob: tcp (MCA v2.0, API v2.0, Component v1.6.4)<br>
                MCA odls: default (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA ras: cm (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA ras: loadleveler (MCA v2.0, API v2.0, Component<br>                          v1.6.4)<br>
                 MCA ras: slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA ras: gridengine (MCA v2.0, API v2.0, Component<br>                          v1.6.3)<br>               MCA rmaps: load_balance (MCA v2.0, API v2.0, Component<br>
                          v1.6.4)<br>               MCA rmaps: rank_file (MCA v2.0, API v2.0, Component<br>                          v1.6.4)<br>               MCA rmaps: resilient (MCA v2.0, API v2.0, Component<br>                          v1.6.4)<br>
               MCA rmaps: round_robin (MCA v2.0, API v2.0, Component<br>                          v1.6.4)<br>               MCA rmaps: seq (MCA v2.0, API v2.0, Component v1.6.4)<br>               MCA rmaps: topo (MCA v2.0, API v2.0, Component v1.6.4)<br>
                 MCA rml: oob (MCA v2.0, API v2.0, Component v1.6.4)<br>              MCA routed: binomial (MCA v2.0, API v2.0, Component v1.6.4)<br>              MCA routed: cm (MCA v2.0, API v2.0, Component v1.6.4)<br>              MCA routed: direct (MCA v2.0, API v2.0, Component v1.6.4)<br>
              MCA routed: linear (MCA v2.0, API v2.0, Component v1.6.4)<br>              MCA routed: radix (MCA v2.0, API v2.0, Component v1.6.4)<br>              MCA routed: slave (MCA v2.0, API v2.0, Component v1.6.4)<br>
                 MCA plm: rsh (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA plm: slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>               MCA filem: rsh (MCA v2.0, API v2.0, Component v1.6.4)<br>              MCA errmgr: default (MCA v2.0, API v2.0, Component v1.6.4)<br>
                 MCA ess: env (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA ess: hnp (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA ess: singleton (MCA v2.0, API v2.0, Component<br>                          v1.6.4)<br>
                 MCA ess: slave (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA ess: slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>                 MCA ess: slurmd (MCA v2.0, API v2.0, Component v1.6.4)<br>
                 MCA ess: tool (MCA v2.0, API v2.0, Component v1.6.4)<br>             MCA grpcomm: bad (MCA v2.0, API v2.0, Component v1.6.4)<br>             MCA grpcomm: basic (MCA v2.0, API v2.0, Component v1.6.4)<br>             MCA grpcomm: hier (MCA v2.0, API v2.0, Component v1.6.4)<br>
            MCA notifier: command (MCA v2.0, API v1.0, Component v1.6.4)<br>            MCA notifier: syslog (MCA v2.0, API v1.0, Component v1.6.4)<br></div><br>Please note the <span style="color:rgb(255,0,0)">Prefix <span style="color:rgb(0,0,0)">line. I&#39;m sure I runned the &#39;./configure&#39; command with the prefix &#39;--prefix=/home/damiano/fortran/openmpi/&#39;, and I checked that that openmpi version is neither on my path or library path!<br>
<br></span></span></div><span style="color:rgb(255,0,0)"><span style="color:rgb(0,0,0)">This is surely a problem, do you know how to fix it?<br><br></span></span></div><span style="color:rgb(255,0,0)"><span style="color:rgb(0,0,0)">Thanks again,<br>
Damiano<br></span></span></div><div class="gmail_extra"><br><br><div class="gmail_quote">2013/9/30 Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span><br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div class="im">On Sep 30, 2013, at 12:16 PM, Gus Correa &lt;<a href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt; wrote:<br>

<br>
&gt; &quot;which mpif90&quot; will tell you what you are actually using.<br>
<br>
</div>This is what I&#39;m guessing is wrong -- if ompi_info is telling you that you have Fortran bindings available, then I&#39;m guessing you&#39;re somehow getting the &quot;wrong&quot; mpif90 (i.e., an mpif90 from an OMPI installation with no Fortran bindings support). Check your PATH.<br>

<span class="HOEnZb"><font color="#888888"><br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
</font></span><div class="HOEnZb"><div class="h5"><br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br><br clear="all"><br>-- <br><div>Damiano Natali</div><div>mail <a href="mailto:damiano.natali@gmail.com" target="_blank">damiano.natali@gmail.com</a></div><div>skype damiano.natali</div>
<div><br></div><div><br></div>
</div>

