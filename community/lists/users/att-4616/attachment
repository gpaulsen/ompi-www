Jeff,&nbsp;<div><br class="webkit-block-placeholder"></div><div>Thanks for the detailed discussion. It certainly makes things a lot clearer, just as I was giving up my hopes for a reply.</div><div><br class="webkit-block-placeholder">
</div><div>The app is fairly heavy on communication (~10k messages per minute) and is also&nbsp;embarrassingly&nbsp;parallel. Taking this into account, I think I&#39;ll readjust my resilience expectations and go with MPI as it will make communications a breeze to deal with.
</div><div><br class="webkit-block-placeholder"></div><div>It does make sense to have the ability to add/remove processes on the go. In a multi-core hardware a scheduler could add more processes to an app as the hardware becomes freed up from other tasks. Of course that would be a problem for apps that require some type of data&nbsp;synchronisation (tightly coupled as you say). It would be nice to have the option of &quot;mpirun -min 4 -max 16&quot; and let the scheduler optimise based on availability.
</div><div><br class="webkit-block-placeholder"></div><div>I&#39;m currently running a test case on two machines with two cores each and, after one day, so far so good. We&#39;ll see how it goes.</div><div><br class="webkit-block-placeholder">
</div><div>Thanks again</div><div>dok<br><br><div class="gmail_quote">On Dec 6, 2007 2:06 PM, Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
It certainly does make sense to use MPI for such a setup. &nbsp;But there<br>are some important things to consider:<br><br>1. MPI, at its heart, is a communications system. &nbsp;There&#39;s lots of<br>other bells and whistles (e.g
., starting up a whole bunch of processes<br>in tandem), but at the core: it&#39;s all about passing messages.<br><br>2. MPI tends to lend itself to a fairly tightly coupled systems. &nbsp;The<br>usual model is that you start all of your parallel processes at the
<br>same time (e.g., &quot;mpirun -np 32 my_application&quot;). &nbsp;The current state<br>of technology is *not* good in terms of fault tolerance -- most MPI&#39;s<br>(Open MPI included) will kill the entire job if any one of those
<br>processes die. &nbsp;This is an important factor for running for weeks,<br>months, or years.<br><br>(lots of good research is ongoing about fault tolerance and MPI, but<br>the existing solutions are still emphasizing tightly-coupled
<br>applications or required a bunch of involvement from the application)<br><br>3. MPI also emphasizes performance: low latency, high bandwidth, good<br>concurrency, etc.<br><br>If you don&#39;t need these things, for example, if your communication
<br>between manager and worker is infrequent, and/or the overall<br>application time is not dominated by communication time, you might be<br>better served for [extremely] long-running applications by using a<br>simple (but resilient) sockets-based communication layer and not using
<br>MPI. &nbsp;I say this mainly because of the fault tolerance issues involved<br>and the natural hardware MTBF values that we see on today&#39;s hardware.<br><br>Hope that helps.<br><div><div></div><div class="Wj3C7c"><br><br>
On Dec 4, 2007, at 1:15 PM, doktora v wrote:<br><br>&gt; Hi, although I did my due diligence on searching for this question,<br>&gt; I apologise if this is a repeat.<br>&gt;<br>&gt; From an architectural point of view does it make sense to use MPI in
<br>&gt; the following scenario (for the purposes of resilience as much as<br>&gt; parallelization):<br>&gt;<br>&gt; Each process is a long-running process (runs non-interrupted for<br>&gt; weeks, months or even years) that collects and crunches some
<br>&gt; streaming data, for example temperature readings, and the data is<br>&gt; replicated to R nodes.<br>&gt;<br>&gt; Because this is a diversion from the normal modus operandi (i.e. all<br>&gt; data is immediately available), is there any obvious MPI issues that
<br>&gt; I am not considering in designing such an application?<br>&gt;<br>&gt; Here is a more detailed description of the app:<br>&gt;<br>&gt; A master receives the data and dispatches it according to some<br>&gt; function such that each tuple is replicated R times to R of the N
<br>&gt; nodes (with R&lt;=N). Suppose that there are K regions from which<br>&gt; temperature readings stream in &nbsp;in the form of &lt;K,T&gt; where K is the<br>&gt; region id and T is the temperature reading. The master sends &lt;K,T&gt;
<br>&gt; to R of the N nodes. These nodes maintain a long-term state of, say,<br>&gt; the min/max readings. If R=N=2, the system is basically duplicated<br>&gt; and if one of the two nodes dies inadvertently, the other one still
<br>&gt; has accounted for all the data.<br>&gt;<br>&gt; Here is some pseudo-code:<br>&gt;<br>&gt; int main(argc, argv)<br>&gt;<br>&gt; int N=10, R=3, K=200;<br>&gt;<br>&gt; Init(argc,argv);<br>&gt; int rank=COMM_WORLD.Get_rank();
<br>&gt; if(rank==0) {<br>&gt; &nbsp; &nbsp; &nbsp;int lastnode = 1;<br>&gt; &nbsp; &nbsp; &nbsp;while(read &lt;k,T&gt; from socket)<br>&gt; &nbsp; &nbsp; &nbsp; &nbsp;for(i in 0:R) COMM_WORLD.Send(&lt;k,T&gt;,1,tuple,++lastnode%N,tag);<br>&gt; } else {<br>&gt; &nbsp; &nbsp; &nbsp; COMM_WORLD.Recv(&lt;k,T&gt;,1,tuple,any,tag,Info);
<br>&gt; &nbsp; &nbsp; &nbsp; &nbsp;process_message(&lt;k,T&gt;);<br>&gt; }<br>&gt;<br>&gt; Many thanks for your time!<br>&gt; Regards<br>&gt; Dok<br></div></div>&gt; _______________________________________________<br>&gt; users mailing list
<br>&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><font color="#888888">
<br><br>--<br>Jeff Squyres<br>Cisco Systems<br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></font></blockquote></div><br></div>

