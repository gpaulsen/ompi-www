<div dir="ltr"><div><div><div><div><div><div><div><div><div><div><div><div>Hello all<br><br></div>I am asking for help for the following situation:<br><br></div>I have two (mostly identical) nodes. Each of them have (completely identical)<br></div>1. qlogic 4x DDR infiniband, AND<br></div>2. Chelsio S310E (T3 chip based) 10GE iWARP cards.<br><br></div>Both are connected back-to-back, without a switch. The connection is physically OK and IP traffic can flow on both of them without issues.<br><br></div>The issue is, I can run MPI programs using the openib BTL using the qlogic card, but not the Chelsio card. Here are the commands:<br><br>[durga@smallMPI ~]$ ibv_devices<br>    device                 node GUID<br>    ------              ----------------<br>    cxgb3_0             00074306cd3b0000      &lt;-- Chelsio<br>    qib0                0011750000ff831d           &lt;-- Qlogic<br><br></div>The following command works:<br><br> mpirun -np 2 --hostfile ~/hostfile -mca btl_openib_if_include qib0 ./osu_acc_latency<br><br></div>And the following do not:<br>mpirun -np 2 --hostfile ~/hostfile -mca btl_openib_if_include cxgb3_0 ./osu_acc_latency<br><br>mpirun -np 2 --hostfile ~/hostfile -mca pml ob1 -mca btl_openib_if_include cxgb3_0 ./osu_acc_latency<br><br>mpirun -np 2 --hostfile ~/hostfile -mca pml ^cm -mca btl_openib_if_include cxgb3_0 ./osu_acc_latency <br><br></div>The error I get is the following (in all of the non-working cases):<br><br>WARNING: The largest queue pair buffer size specified in the<br>btl_openib_receive_queues MCA parameter is smaller than the maximum<br>send size (i.e., the btl_openib_max_send_size MCA parameter), meaning<br>that no queue is large enough to receive the largest possible incoming<br>message fragment.  The OpenFabrics (openib) BTL will therefore be<br>deactivated for this run.<br><br>  Local host: smallMPI<br>  Largest buffer size: 65536<br>  Maximum send fragment size: 131072<br>--------------------------------------------------------------------------<br>--------------------------------------------------------------------------<br>No OpenFabrics connection schemes reported that they were able to be<br>used on a specific port.  As such, the openib BTL (OpenFabrics<br>support) will be disabled for this port.<br><br>  Local host:           bigMPI<br>  Local device:         cxgb3_0<br>  Local port:           1<br>  CPCs attempted:       udcm<br>--------------------------------------------------------------------------<br><br></div>I have a vague understanding of what the message is trying to say, but I do not know which file or configuration parameters to change to fix the situation.<br><br></div>Thanks in advance<br></div>Durga<br><div><div><div><div><div><div><div><br><div><div><div><div><div><div><div><br clear="all"><div><div class="gmail_signature">Life is complex. It has real and imaginary parts.</div></div>
</div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>

