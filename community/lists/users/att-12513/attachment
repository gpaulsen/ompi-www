Thanks a lot for your reply.<br><br>Now the mpiblast run in only one node both inside and outside a torque job. <br><br>How could I setup a hostlist for open mpi? I haven&#39;t found this in the open mpi faq. Thanks.<br><br>
the &quot;ompi_info | grep tm&quot; output is:<br><br><br>              MCA memory: ptmalloc2 (MCA v2.0, API v2.0, Component v1.4.1)<br>                 MCA ras: tm (MCA v2.0, API v2.0, Component v1.4.1)<br>                 MCA plm: tm (MCA v2.0, API v2.0, Component v1.4.1)<br>
<br><br>I also attached the &quot;ompi_info --all&quot; output with this email. Maybe you&#39;d like help me to check it.<br><br>I have set the openmpi/bin to PATH and openmpi/lib to LD_LIBRARY_PATH and I think the mpiblast complier chose the right mpicc.<br>
<br><br>HZ Liu<br><br><br><div class="gmail_quote">2010/4/1 Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<div><font size="2" color="navy" face="Arial">
Are you running your job inside a torque job?  If you don&#39;t, open mpi will not have a hostlist and will assume that it should launch everything on the localhost. <br><br>If you are launching inside a torque job, ensure that ompi was build with torque support properly - run<br>
<br>ompi_info | grep tm<br><br>If you see 1 or more tm plugins listed, ompi has torque support. <br><br>Finally, make sure you&#39;re using the right mpicc and mpirun, etc. 
<br>
<br>-jms
<br>Sent from my PDA.  No type good.</font></div>
<br><div><hr size="2" width="100%" align="center">
<font size="2" face="Tahoma">
<b>From</b>: <a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a> &lt;<a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a>&gt;
<br><b>To</b>: <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;
<br><b>Sent</b>: Thu Apr 01 02:07:08 2010<br><b>Subject</b>: [OMPI users] mpiblast only run in one node
<br></font><br></div><div><div></div><div class="h5">
<span style="font-family: arial,helvetica,sans-serif;">Hi, </span><br style="font-family: arial,helvetica,sans-serif;"><br style="font-family: arial,helvetica,sans-serif;"><span style="font-family: arial,helvetica,sans-serif;">I&#39;ve installed Torque/Maui, Open MPI 1.4.1 and mpiBlast 1.6.0-beta1 in a linux Beowulf culster with 15 nodes (node1~15).</span><br style="font-family: arial,helvetica,sans-serif;">

<br>Open MPI, mpiBlast were installed and my home directory lies in a directory &quot;/data&quot; which was shared by all nodes.<br><br style="font-family: arial,helvetica,sans-serif;"><span style="font-family: arial,helvetica,sans-serif;">Open Mpi was compiled with &quot;--with-tm&quot; to support Torque, and mpiBlast was compiled with</span><code style="font-family: arial,helvetica,sans-serif;"> &quot;--with-mpi&quot; to the directory where Open MPI installed.<br>

<br>When I run mpiBlast by mpirun in command line, like<br><br>node1 $ /data/open-mpi/bin/mpirun -np 34 /data/mpiblast/bin/mpiblast -p blastp -d nr -i test.faa -o test.out<br><br>I noticed all 34 mpiBlast processes run on node1 only.<br>

<br>This would not change if I submit job to Torque.<br><br>I&#39;ve searched the mailing list archives but it seems won&#39;t help.<br><br>How to resolve this problem?<br><br>Any suggestion will be appreciated!<br><br><br>

HZ Liu<br><br><br><br></code>
</div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

