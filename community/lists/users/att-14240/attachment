I don&#39;t know if this will help, but try <br>mpirun --machinefile testfile -np 4 ./test.out<br>for running 4 processes<br><br><div class="gmail_quote">On Mon, Sep 20, 2010 at 3:00 PM, Ethan Deneault <span dir="ltr">&lt;<a href="mailto:edeneault@ut.edu">edeneault@ut.edu</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">All,<br>
<br>
I am running Scientific Linux 5.5, with OpenMPI 1.4 installed into the /usr/lib/openmpi/1.4-gcc/ directory. I know this is typically /opt/openmpi, but Red Hat does things differently. I have my PATH and LD_LIBRARY_PATH set correctly; because the test program does compile and run.<br>


<br>
The cluster consists of 10 Intel Pentium 4 diskless nodes. The master is a AMD x86_64 machine which serves the diskless node images and /home as an NFS mount. I compile all of my programs as 32-bit.<br>
<br>
My code is a simple hello world:<br>
$ more test.f<br>
      program test<br>
<br>
      include &#39;mpif.h&#39;<br>
      integer rank, size, ierror, tag, status(MPI_STATUS_SIZE)<br>
<br>
      call MPI_INIT(ierror)<br>
      call MPI_COMM_SIZE(MPI_COMM_WORLD, size, ierror)<br>
      call MPI_COMM_RANK(MPI_COMM_WORLD, rank, ierror)<br>
      print*, &#39;node&#39;, rank, &#39;: Hello world&#39;<br>
      call MPI_FINALIZE(ierror)<br>
      end<br>
<br>
If I run this program with:<br>
<br>
$ mpirun --machinefile testfile ./test.out<br>
 node           0 : Hello world<br>
 node           2 : Hello world<br>
 node           1 : Hello world<br>
<br>
This is the expected output. Here, testfile contains the master node: &#39;pleiades&#39;, and two slave nodes: &#39;taygeta&#39; and &#39;m43&#39;<br>
<br>
If I add another machine to testfile, say &#39;asterope&#39;, it hangs until I ctrl-c it. I have tried every machine, and as long as I do not include more than 3 hosts, the program will not hang.<br>
<br>
I have run the debug-daemons flag with it as well, and I don&#39;t see what is wrong specifically.<br>
<br>
Working output: pleiades (master) and 2 nodes.<br>
<br>
$ mpirun --debug-daemons --machinefile testfile ./test.out<br>
Daemon was launched on m43 - beginning to initialize<br>
Daemon was launched on taygeta - beginning to initialize<br>
Daemon [[46344,0],2] checking in as pid 2140 on host m43<br>
Daemon [[46344,0],2] not using static ports<br>
[m43:02140] [[46344,0],2] orted: up and running - waiting for commands!<br>
[pleiades:19178] [[46344,0],0] node[0].name pleiades daemon 0 arch ffca0200<br>
[pleiades:19178] [[46344,0],0] node[1].name taygeta daemon 1 arch ffca0200<br>
[pleiades:19178] [[46344,0],0] node[2].name m43 daemon 2 arch ffca0200<br>
[pleiades:19178] [[46344,0],0] orted_cmd: received add_local_procs<br>
[m43:02140] [[46344,0],2] node[0].name pleiades daemon 0 arch ffca0200<br>
[m43:02140] [[46344,0],2] node[1].name taygeta daemon 1 arch ffca0200<br>
[m43:02140] [[46344,0],2] node[2].name m43 daemon 2 arch ffca0200<br>
[m43:02140] [[46344,0],2] orted_cmd: received add_local_procs<br>
Daemon [[46344,0],1] checking in as pid 2317 on host taygeta<br>
Daemon [[46344,0],1] not using static ports<br>
[taygeta:02317] [[46344,0],1] orted: up and running - waiting for commands!<br>
[taygeta:02317] [[46344,0],1] node[0].name pleiades daemon 0 arch ffca0200<br>
[taygeta:02317] [[46344,0],1] node[1].name taygeta daemon 1 arch ffca0200<br>
[taygeta:02317] [[46344,0],1] node[2].name m43 daemon 2 arch ffca0200<br>
[taygeta:02317] [[46344,0],1] orted_cmd: received add_local_procs<br>
[pleiades:19178] [[46344,0],0] orted_recv: received sync+nidmap from local proc [[46344,1],0]<br>
[m43:02140] [[46344,0],2] orted_recv: received sync+nidmap from local proc [[46344,1],2]<br>
[taygeta:02317] [[46344,0],1] orted_recv: received sync+nidmap from local proc [[46344,1],1]<br>
[pleiades:19178] [[46344,0],0] orted_cmd: received collective data cmd<br>
[pleiades:19178] [[46344,0],0] orted_cmd: received collective data cmd<br>
[m43:02140] [[46344,0],2] orted_cmd: received collective data cmd<br>
[taygeta:02317] [[46344,0],1] orted_cmd: received collective data cmd<br>
[pleiades:19178] [[46344,0],0] orted_cmd: received collective data cmd<br>
[pleiades:19178] [[46344,0],0] orted_cmd: received message_local_procs<br>
[taygeta:02317] [[46344,0],1] orted_cmd: received message_local_procs<br>
[m43:02140] [[46344,0],2] orted_cmd: received message_local_procs<br>
[pleiades:19178] [[46344,0],0] orted_cmd: received collective data cmd<br>
[m43:02140] [[46344,0],2] orted_cmd: received collective data cmd<br>
[pleiades:19178] [[46344,0],0] orted_cmd: received collective data cmd<br>
[pleiades:19178] [[46344,0],0] orted_cmd: received collective data cmd<br>
[pleiades:19178] [[46344,0],0] orted_cmd: received message_local_procs<br>
[taygeta:02317] [[46344,0],1] orted_cmd: received collective data cmd<br>
[taygeta:02317] [[46344,0],1] orted_cmd: received message_local_procs<br>
[m43:02140] [[46344,0],2] orted_cmd: received message_local_procs<br>
 node           0 : Hello world<br>
[pleiades:19178] [[46344,0],0] orted_cmd: received collective data cmd<br>
 node           2 : Hello world<br>
 node           1 : Hello world<br>
[pleiades:19178] [[46344,0],0] orted_cmd: received collective data cmd<br>
[pleiades:19178] [[46344,0],0] orted_cmd: received collective data cmd<br>
[pleiades:19178] [[46344,0],0] orted_cmd: received message_local_procs<br>
[taygeta:02317] [[46344,0],1] orted_cmd: received collective data cmd<br>
[taygeta:02317] [[46344,0],1] orted_cmd: received message_local_procs<br>
[m43:02140] [[46344,0],2] orted_cmd: received collective data cmd<br>
[m43:02140] [[46344,0],2] orted_cmd: received message_local_procs<br>
[pleiades:19178] [[46344,0],0] orted_recv: received sync from local proc [[46344,1],0]<br>
[m43:02140] [[46344,0],2] orted_recv: received sync from local proc [[46344,1],2]<br>
[taygeta:02317] [[46344,0],1] orted_recv: received sync from local proc [[46344,1],1]<br>
[pleiades:19178] [[46344,0],0] orted_cmd: received waitpid_fired cmd<br>
[pleiades:19178] [[46344,0],0] orted_cmd: received iof_complete cmd<br>
[m43:02140] [[46344,0],2] orted_cmd: received waitpid_fired cmd<br>
[taygeta:02317] [[46344,0],1] orted_cmd: received waitpid_fired cmd<br>
[m43:02140] [[46344,0],2] orted_cmd: received iof_complete cmd<br>
[taygeta:02317] [[46344,0],1] orted_cmd: received iof_complete cmd<br>
[pleiades:19178] [[46344,0],0] orted_cmd: received exit<br>
[taygeta:02317] [[46344,0],1] orted_cmd: received exit<br>
[taygeta:02317] [[46344,0],1] orted: finalizing<br>
[m43:02140] [[46344,0],2] orted_cmd: received exit<br>
[m43:02140] [[46344,0],2] orted: finalizing<br>
<br>
Not working output: pleiades (master) and 3 nodes:<br>
<br>
$ mpirun --debug-daemons --machinefile testfile ./test.out<br>
Daemon was launched on m43 - beginning to initialize<br>
Daemon was launched on taygeta - beginning to initialize<br>
Daemon was launched on asterope - beginning to initialize<br>
Daemon [[46357,0],2] checking in as pid 2181 on host m43<br>
Daemon [[46357,0],2] not using static ports<br>
[m43:02181] [[46357,0],2] orted: up and running - waiting for commands!<br>
Daemon [[46357,0],1] checking in as pid 2358 on host taygeta<br>
Daemon [[46357,0],1] not using static ports<br>
[taygeta:02358] [[46357,0],1] orted: up and running - waiting for commands!<br>
[pleiades:19191] [[46357,0],0] node[0].name pleiades daemon 0 arch ffca0200<br>
[pleiades:19191] [[46357,0],0] node[1].name taygeta daemon 1 arch ffca0200<br>
[pleiades:19191] [[46357,0],0] node[2].name m43 daemon 2 arch ffca0200<br>
[pleiades:19191] [[46357,0],0] node[3].name asterope daemon 3 arch ffca0200<br>
[pleiades:19191] [[46357,0],0] orted_cmd: received add_local_procs<br>
[taygeta:02358] [[46357,0],1] node[0].name pleiades daemon 0 arch ffca0200<br>
[taygeta:02358] [[46357,0],1] node[1].name taygeta daemon 1 arch ffca0200<br>
[m43:02181] [[46357,0],2] node[0].name pleiades daemon 0 arch ffca0200<br>
[taygeta:02358] [[46357,0],1] node[2].name m43 daemon 2 arch ffca0200<br>
[m43:02181] [[46357,0],2] node[1].name taygeta daemon 1 arch ffca0200<br>
[m43:02181] [[46357,0],2] node[2].name m43 daemon 2 arch ffca0200<br>
[m43:02181] [[46357,0],2] node[3].name asterope daemon 3 arch ffca0200<br>
[m43:02181] [[46357,0],2] orted_cmd: received add_local_procs<br>
[taygeta:02358] [[46357,0],1] node[3].name asterope daemon 3 arch ffca0200<br>
[taygeta:02358] [[46357,0],1] orted_cmd: received add_local_procs<br>
Daemon [[46357,0],3] checking in as pid 1965 on host asterope<br>
Daemon [[46357,0],3] not using static ports<br>
[asterope:01965] [[46357,0],3] orted: up and running - waiting for commands!<br>
[pleiades:19191] [[46357,0],0] orted_recv: received sync+nidmap from local proc [[46357,1],0]<br>
[m43:02181] [[46357,0],2] orted_recv: received sync+nidmap from local proc [[46357,1],2]<br>
[pleiades:19191] [[46357,0],0] orted_cmd: received collective data cmd<br>
[m43:02181] [[46357,0],2] orted_cmd: received collective data cmd<br>
[pleiades:19191] [[46357,0],0] orted_cmd: received collective data cmd<br>
<br>
------------------<br>
The output hangs here.<br>
<br>
After I kill the process, I get the following output:<br>
------------------<br>
<br>
Killed by signal 2.<br>
Killed by signal 2.<br>
--------------------------------------------------------------------------<br>
A daemon (pid 19194) died unexpectedly with status 255 while attempting<br>
to launch so we are aborting.<br>
<br>
There may be more information reported by the environment (see above).<br>
<br>
This may be because the daemon was unable to find all the needed shared<br>
libraries on the remote node. You may set your LD_LIBRARY_PATH to have the<br>
location of the shared libraries on the remote nodes and this will<br>
automatically be forwarded to the remote nodes.<br>
--------------------------------------------------------------------------<br>
mpirun: abort is already in progress...hit ctrl-c again to forcibly terminate<br>
<br>
Killed by signal 2.<br>
--------------------------------------------------------------------------<br>
mpirun noticed that the job aborted, but has no info as to the process<br>
that caused that situation.<br>
--------------------------------------------------------------------------<br>
[pleiades:19191] [[46357,0],0] orted_cmd: received waitpid_fired cmd<br>
[pleiades:19191] [[46357,0],0] orted_cmd: received iof_complete cmd<br>
[pleiades:19191] [[46357,0],0] orted_cmd: received exit<br>
mpirun: clean termination accomplished<br>
<br>
I know that LD_LIBRARY_PATH is -not- to blame. /home/&lt;user&gt; is exported to each machine from the master, and each machine uses the same image (and thus the same paths). If there was a problem with the path, it would not run.<br>


<br>
Any insight would be appreciated.<br>
<br>
Thank you,<br>
Ethan<br>
<br>
<br>
<br>
-- <br>
Dr. Ethan Deneault<br>
Assistant Professor of Physics<br>
SC-234<br>
University of Tampa<br>
Tampa, FL 33615<br>
Office: (813) 257-3555<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br><br clear="all"><br>-- <br>David Zhang<br>University of California, San Diego<br>

