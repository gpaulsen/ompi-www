You're right, I'll try to use netpipes first and then the application.&nbsp; If it doesn't workt I'll send configs and more detailed informations<br><br>Thank you!<br><br><div><span class="gmail_quote">On 3/1/06, <b class="gmail_sendername">
Brian Barrett</b> &lt;<a href="mailto:brbarret@open-mpi.org">brbarret@open-mpi.org</a>&gt; wrote:</span><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Jose -<br><br>I noticed that your output doesn't appear to match what the source<br>code is capable of generating.&nbsp;&nbsp;It's possible that you're running<br>into problems with the code that we can't see because you didn't send
<br>a complete version of the source code.<br><br>You might want to start by running some 3rd party codes that are<br>known to be good, just to make sure that your MPI installation checks<br>out.&nbsp;&nbsp;A good start is NetPIPE, which runs between two peers and gives
<br>latency / bandwidth information.&nbsp;&nbsp;If that runs, then it's time to<br>look at your application.&nbsp;&nbsp;If that doesn't run, then it's time to<br>look at the MPI installation in more detail.&nbsp;&nbsp;In this case, it would<br>be useful to see all of the information requested here:
<br><br>&nbsp;&nbsp; <a href="http://www.open-mpi.org/community/help/">http://www.open-mpi.org/community/help/</a><br><br>as well as from running the mpirun command used to start NetPIPE with<br>the -d option, so something like:<br>
<br>&nbsp;&nbsp; mpirun -np 2 -hostfile foo -d ./NPMpi<br><br>Brian<br><br>On Feb 28, 2006, at 9:29 AM, Jose Pedro Garcia Mahedero wrote:<br><br>&gt; Hello everybody.<br>&gt;<br>&gt; I'm new to MPI and I'm having some problems while runnig a simple
<br>&gt; pingpong program in more than one node.<br>&gt;<br>&gt; 1.- I followed all the instructions and installed open MPI without<br>&gt; problems in&nbsp;&nbsp;a Beowulf cluster.<br>&gt; 2.-&nbsp;&nbsp;Ths cluster is working OK and ssh keys are set for not
<br>&gt; password prompting<br>&gt; 3.- miexec seems to run OK.<br>&gt; 4.- Now I'm using just 2 nodes: I've tried a simple ping-pong<br>&gt; application but my master only sends one request!!<br>&gt; 5.- I reduced the problem by trying to send just two mesages to the
<br>&gt; same node:<br>&gt;<br>&gt; int main(int argc, char **argv){<br>&gt;&nbsp;&nbsp; int myrank;<br>&gt;<br>&gt;&nbsp;&nbsp; /* Initialize MPI */<br>&gt;<br>&gt;&nbsp;&nbsp; MPI_Init(&amp;argc, &amp;argv);<br>&gt;<br>&gt;&nbsp;&nbsp; /* Find out my identity in the default communicator */
<br>&gt;<br>&gt;&nbsp;&nbsp; MPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank);<br>&gt;&nbsp;&nbsp; if (myrank == 0) {<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp; int work = 100;<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp; int count=0;<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp; for (int i =0; i &lt; 10; i++){<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cout &lt;&lt; &quot;MASTER IS SLEEPING...&quot; &lt;&lt; endl;
<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sleep(3);<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cout &lt;&lt; &quot;MASTER AWAKE WILL SEND[&quot;&lt;&lt; count++ &lt;&lt; &quot;]:&quot; &lt;&lt; work<br>&gt; &lt;&lt; endl;<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MPI_Send(&amp;work, 1, MPI_INT, 1, WORKTAG,&nbsp;&nbsp; MPI_COMM_WORLD);
<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp; }<br>&gt;&nbsp;&nbsp; } else {<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int count =0;<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int work;<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MPI_Status status;<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; while (true){<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MPI_Recv(&amp;work, 1, MPI_INT, 0, MPI_ANY_TAG,<br>&gt; MPI_COMM_WORLD, &amp;status);
<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cout &lt;&lt; &quot;SLAVE[&quot; &lt;&lt; myrank &lt;&lt; &quot;] RECEIVED[&quot; &lt;&lt; count++ &lt;&lt;<br>&gt; &quot;]:&quot; &lt;&lt; work &lt;&lt;endl;<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (status.MPI_TAG == DIETAG) {
<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break;<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp; }// while<br>&gt;&nbsp;&nbsp; }<br>&gt;&nbsp;&nbsp; MPI_Finalize();<br>&gt;<br>&gt;<br>&gt;<br>&gt; 6a.- RESULTS&nbsp;&nbsp;(if I put more than one machine in my mpihostsfile),<br>&gt; my master sends the first message&nbsp;&nbsp;and my slave receives it
<br>&gt; perfectly. But my master doesnt send its second .<br>&gt; message:<br>&gt;<br>&gt;<br>&gt;<br>&gt; Here's my output<br>&gt;<br>&gt; MASTER IS SLEEPING...<br>&gt; MASTER AWAKE WILL SEND[0]:100<br>&gt; MASTER IS SLEEPING...
<br>&gt; SLAVE[1] RECEIVED[0]:100MPI_STATUS.MPI_ERROR:0<br>&gt; MASTER AWAKE WILL SEND[1]:100<br>&gt;<br>&gt; 6b.- RESULTS (if I put ONLY&nbsp;&nbsp;1 machine in my mpihostsfile),<br>&gt; everything is OK until iteration 9!!!<br>&gt; MASTER IS SLEEPING...
<br>&gt; MASTER AWAKE WILL SEND[0]:100<br>&gt; MASTER IS SLEEPING...<br>&gt; MASTER AWAKE WILL SEND[1]:100<br>&gt; MASTER IS SLEEPING...<br>&gt; MASTER AWAKE WILL SEND[2]:100<br>&gt; MASTER IS SLEEPING...<br>&gt; MASTER AWAKE WILL SEND[3]:100
<br>&gt; MASTER IS SLEEPING...<br>&gt; MASTER AWAKE WILL SEND[4]:100<br>&gt; MASTER IS SLEEPING...<br>&gt; MASTER AWAKE WILL SEND[5]:100<br>&gt; MASTER IS SLEEPING...<br>&gt; MASTER AWAKE WILL SEND[6]:100<br>&gt; MASTER IS SLEEPING...
<br>&gt; MASTER AWAKE WILL SEND[7]:100<br>&gt; MASTER IS SLEEPING...<br>&gt; MASTER AWAKE WILL SEND[8]:100<br>&gt; MASTER IS SLEEPING...<br>&gt; MASTER AWAKE WILL SEND[9]:100<br>&gt; SLAVE[1] RECEIVED[0]:100MPI_STATUS.MPI_ERROR:0
<br>&gt; SLAVE[1] RECEIVED[1]:100MPI_STATUS.MPI_ERROR:0<br>&gt; SLAVE[1] RECEIVED[2]:100MPI_STATUS.MPI_ERROR:0<br>&gt; SLAVE[1] RECEIVED[3]:100MPI_STATUS.MPI_ERROR:0<br>&gt; SLAVE[1] RECEIVED[4]:100MPI_STATUS.MPI_ERROR:0<br>
&gt; SLAVE[1] RECEIVED[5]:100MPI_STATUS.MPI_ERROR:0<br>&gt; SLAVE[1] RECEIVED[6]:100MPI_STATUS.MPI_ERROR:0<br>&gt; SLAVE[1] RECEIVED[7]:100MPI_STATUS.MPI_ERROR:0<br>&gt; SLAVE[1] RECEIVED[8]:100MPI_STATUS.MPI_ERROR:0<br>&gt; SLAVE[1] RECEIVED[9]:100MPI_STATUS.MPI_ERROR:0
<br>&gt; --------------------------------<br>&gt;<br>&gt; I know this is a lot of text, but I wanted to give a mamixum<br>&gt; detailed question. I've been search in FAQ, but still don't know<br>&gt; what (and why) is going on...
<br>&gt;<br>&gt; Anyone can help please :-)&nbsp;&nbsp;?<br>&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br>--<br>&nbsp;&nbsp; Brian Barrett<br>&nbsp;&nbsp; Open MPI developer<br>&nbsp;&nbsp; <a href="http://www.open-mpi.org/">http://www.open-mpi.org/</a><br><br><br>_______________________________________________
<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div>
<br>

