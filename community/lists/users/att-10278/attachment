<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">Interesting! Well, I always make sure I have my personal OMPI build before any system stuff, and I work exclusively on Mac OS-X:<div><br></div><div><div>rhc$ echo $PATH</div><div>/Library/Frameworks/Python.framework/Versions/Current/bin:/Users/rhc/openmpi/bin:/Users/rhc/bin:/opt/local/bin:/usr/X11R6/bin:/usr/local/bin:/opt/local/bin:/opt/local/sbin:/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/texbin</div><div><br></div><div><div>rhc$ echo $LD_LIBRARY_PATH</div><div>/Users/rhc/openmpi/lib:/Users/rhc/lib:/opt/local/lib:/usr/X11R6/lib:/usr/local/lib:</div><div><br></div><div>Note that I always configure with --prefix=somewhere-in-my-own-dir, never to a system directory. Avoids this kind of confusion.</div><div><br></div><div>What the errors are saying is that we are picking up components from a very old version of OMPI that is distributed by Apple. It may or may not be causing confusion for the system - hard to tell. However, the fact that it is the IO forwarding subsystem that is picking them up, and the fact that you aren't seeing any output from your job, makes me a tad suspicious.</div><div><br></div><div>Can you run other jobs? In other words, do you get stdout/stderr from other programs you run, or does every MPI program hang (even simple ones)? If it is just your program, then it could just be that your application is hanging before any output is generated. Can you have it print something to stderr right when it starts?</div><div><br></div><div><br></div></div><div><div>On Aug 10, 2009, at 8:53 PM, Klymak Jody wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><br><div><div>On 10-Aug-09, at 6:44 PM, Ralph Castain wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div>Check your LD_LIBRARY_PATH - there is an earlier version of OMPI in your path that is interfering with operation (i.e., it comes before your 1.3.3 installation).<br></div></blockquote><div><br></div><div>Hmmmm, The OS X faq says not to do this:</div><div><br></div><div>"<span class="Apple-style-span" style="font-family: verdana, arial, helvetica; font-size: 12px; ">Note that there is no need to add Open MPI's libdir to&nbsp;<code>LD_LIBRARY_PATH</code>; Open MPI's shared library build process automatically uses the "rpath" mechanism to automatically find the correct shared libraries (i.e., the ones associated with this build, vs., for example, the OS X-shipped OMPI shared libraries). Also note that we specifically do&nbsp;<strong>not</strong>&nbsp;recommend adding Open MPI's libdir to&nbsp;<code>DYLD_LIBRARY_PATH</code>."</span></div><div><font class="Apple-style-span" face="verdana, arial, helvetica"><br></font></div><div><font class="Apple-style-span" face="verdana, arial, helvetica"><a href="http://www.open-mpi.org/faq/?category=osx">http://www.open-mpi.org/faq/?category=osx</a></font></div><div><font class="Apple-style-span" face="verdana, arial, helvetica"><br></font></div><div><font class="Apple-style-span" face="verdana, arial, helvetica">Regardless, if I set either, and&nbsp;run&nbsp;ompi_info&nbsp;I&nbsp;still&nbsp;get:</font></div><div><font class="Apple-style-span" face="verdana, arial, helvetica"><br></font></div><div><font class="Apple-style-span" face="verdana, arial, helvetica"><div>[saturna.cluster:94981] mca: base: component_find: iof "mca_iof_proxy" uses an MCA interface that is not recognized (component MCA v1.0.0 != supported MCA v2.0.0) -- ignored</div><div>[saturna.cluster:94981] mca: base: component_find: iof "mca_iof_svc" uses an MCA interface that is not recognized (component MCA v1.0.0 != supported MCA v2.0.0) -- ignored</div><div><br></div><div><div>echo $DYLD_LIBRARY_PATH $LD_LIBRARY_PATH</div><div>/usr/local/openmpi/lib: /usr/local/openmpi/lib:</div><div><br></div><div>So I'm afraid I'm stumped again. &nbsp;I suppose I could go clean out all the libraries in /usr/lib/...</div><div><br></div><div>Thanks again, sorry to be a pain...</div><div><br></div><div>Cheers, &nbsp;Jody</div><div><br></div><div><br></div><div><br></div></div></font></div><div><font class="Apple-style-span" face="verdana, arial, helvetica"><br></font></div><blockquote type="cite"><div><br>On Aug 10, 2009, at 7:38 PM, Klymak Jody wrote:<br><br><blockquote type="cite">So,<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">mpirun --display-allocation -pernode --display-map hostname<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">gives me the output below. &nbsp;Simple jobs seem to run, but the MITgcm does not, either under ssh or torque. &nbsp;It hangs at some early point in execution before anything is written, so its hard for me to tell what the error is. &nbsp;Could these MCA warnings have anything to do with it?<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">I've recompiled the gcm with -L /usr/local/openmpi/lib, so hopefully that catches the right library.<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">Thanks, &nbsp;Jody<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">[xserve02.local:38126] mca: base: component_find: ras "mca_ras_dash_host" uses an MCA interface that is not recogniz<br></blockquote><blockquote type="cite">ed (component MCA v1.0.0 != supported MCA v2.0.0) -- ignored<br></blockquote><blockquote type="cite">[xserve02.local:38126] mca: base: component_find: ras "mca_ras_hostfile" uses an MCA interface that is not recognize<br></blockquote><blockquote type="cite">d (component MCA v1.0.0 != supported MCA v2.0.0) -- ignored<br></blockquote><blockquote type="cite">[xserve02.local:38126] mca: base: component_find: ras "mca_ras_localhost" uses an MCA interface that is not recogniz<br></blockquote><blockquote type="cite">ed (component MCA v1.0.0 != supported MCA v2.0.0) -- ignored<br></blockquote><blockquote type="cite">[xserve02.local:38126] mca: base: component_find: ras "mca_ras_xgrid" uses an MCA interface that is not recognized (<br></blockquote><blockquote type="cite">component MCA v1.0.0 != supported MCA v2.0.0) -- ignored<br></blockquote><blockquote type="cite">[xserve02.local:38126] mca: base: component_find: iof "mca_iof_proxy" uses an MCA interface that is not recognized (<br></blockquote><blockquote type="cite">component MCA v1.0.0 != supported MCA v2.0.0) -- ignored<br></blockquote><blockquote type="cite">[xserve02.local:38126] mca: base: component_find: iof "mca_iof_svc" uses an MCA interface that is not recognized (co<br></blockquote><blockquote type="cite">mponent MCA v1.0.0 != supported MCA v2.0.0) -- ignored<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">====================== &nbsp;&nbsp;ALLOCATED NODES &nbsp;&nbsp;======================<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">Data for node: Name: xserve02.local &nbsp;&nbsp;&nbsp;Num slots: 8 &nbsp;&nbsp;&nbsp;Max slots: 0<br></blockquote><blockquote type="cite">Data for node: Name: xserve01.local &nbsp;&nbsp;&nbsp;Num slots: 8 &nbsp;&nbsp;&nbsp;Max slots: 0<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">=================================================================<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">======================== &nbsp;&nbsp;JOB MAP &nbsp;&nbsp;========================<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">Data for node: Name: xserve02.local &nbsp;&nbsp;&nbsp;Num procs: 1<br></blockquote><blockquote type="cite"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [20967,1] Process rank: 0<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">Data for node: Name: xserve01.local &nbsp;&nbsp;&nbsp;Num procs: 1<br></blockquote><blockquote type="cite"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [20967,1] Process rank: 1<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">=============================================================<br></blockquote><blockquote type="cite">[xserve01.cluster:38518] mca: base: component_find: iof "mca_iof_proxy" uses an MCA interface that is not recognized<br></blockquote><blockquote type="cite">(component MCA v1.0.0 != supported MCA v2.0.0) -- ignored<br></blockquote><blockquote type="cite">[xserve01.cluster:38518] mca: base: component_find: iof "mca_iof_svc" uses an MCA interface that is not recognized (<br></blockquote><blockquote type="cite">component MCA v1.0.0 != supported MCA v2.0.0) -- ignored<br></blockquote><blockquote type="cite">xserve02.local<br></blockquote><blockquote type="cite">xserve01.cluster<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">_______________________________________________<br></blockquote><blockquote type="cite">users mailing list<br></blockquote><blockquote type="cite"><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br></blockquote><blockquote type="cite"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></blockquote></div><br></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
