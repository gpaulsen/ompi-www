However, I thank you Tim, Ralh and Jeff.<div>My sequential application runs in 24s (wall clock time).</div><div>My parallel application runs in 13s with two processes on different nodes.</div><div>With shared memory, when two processes are on the same node, my app runs in 23s.</div>




<div>I&#39;m not understand why.</div><div><br><div class="gmail_quote">2011/3/28 Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span><br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
If your program runs faster across 3 processes, 2 of which are local to each other, with --mca btl tcp,self compared to --mca btl tcp,sm,self, then something is very, very strange.<br>
<br>
Tim cites all kinds of things that can cause slowdowns, but it&#39;s still very, very odd that simply enabling using the shared memory communications channel in Open MPI *slows your overall application down*.<br>
<br>
How much does your application slow down in wall clock time?  Seconds?  Minutes?  Hours?  (anything less than 1 second is in the noise)<br>
<div><div></div><div class="h5"><br>
<br>
<br>
On Mar 27, 2011, at 10:33 AM, Ralph Castain wrote:<br>
<br>
&gt;<br>
&gt; On Mar 27, 2011, at 7:37 AM, Tim Prince wrote:<br>
&gt;<br>
&gt;&gt; On 3/27/2011 2:26 AM, Michele Marena wrote:<br>
&gt;&gt;&gt; Hi,<br>
&gt;&gt;&gt; My application performs good without shared memory utilization, but with<br>
&gt;&gt;&gt; shared memory I get performance worst than without of it.<br>
&gt;&gt;&gt; Do I make a mistake? Don&#39;t I pay attention to something?<br>
&gt;&gt;&gt; I know OpenMPI uses /tmp directory to allocate shared memory and it is<br>
&gt;&gt;&gt; in the local filesystem.<br>
&gt;&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; I guess you mean shared memory message passing.   Among relevant parameters may be the message size where your implementation switches from cached copy to non-temporal (if you are on a platform where that terminology is used).  If built with Intel compilers, for example, the copy may be performed by intel_fast_memcpy, with a default setting which uses non-temporal when the message exceeds about some preset size, e.g. 50% of smallest L2 cache for that architecture.<br>

&gt;&gt; A quick search for past posts seems to indicate that OpenMPI doesn&#39;t itself invoke non-temporal, but there appear to be several useful articles not connected with OpenMPI.<br>
&gt;&gt; In case guesses aren&#39;t sufficient, it&#39;s often necessary to profile (gprof, oprofile, Vtune, ....) to pin this down.<br>
&gt;&gt; If shared message slows your application down, the question is whether this is due to excessive eviction of data from cache; not a simple question, as most recent CPUs have 3 levels of cache, and your application may require more or less data which was in use prior to the message receipt, and may use immediately only a small piece of a large message.<br>

&gt;<br>
&gt; There were several papers published in earlier years about shared memory performance in the 1.2 series.There were known problems with that implementation, which is why it was heavily revised for the 1.3/4 series.<br>

&gt;<br>
&gt; You might also look at the following links, though much of it has been updated to the 1.3/4 series as we don&#39;t really support 1.2 any more:<br>
&gt;<br>
&gt; <a href="http://www.open-mpi.org/faq/?category=sm" target="_blank">http://www.open-mpi.org/faq/?category=sm</a><br>
&gt;<br>
&gt; <a href="http://www.open-mpi.org/faq/?category=perftools" target="_blank">http://www.open-mpi.org/faq/?category=perftools</a><br>
&gt;<br>
&gt;<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; Tim Prince<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
</div></div><font color="#888888">--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to:<br>
<a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
</font><div><div></div><div class="h5"><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div>

