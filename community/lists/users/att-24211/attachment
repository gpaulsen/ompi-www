<div dir="ltr"><div><div><div><div><div>I apologize, but I am now confused. Let me see if I can translate:<br><br></div>* you ran the non-MPI version of the NetPipe benchmark and got 9.5Gps on a 10Gps network<br><br></div>
* you ran iperf and got 9.61Gps - however, this has nothing to do with MPI. Just tests your TCP stack<br><br></div>* you tested your bandwidth program on a 1Gps network and got about 90% efficiency.<br><br></div>Is the above correct? If so, my actual suggestion was to run the MPI version of NetPipe and to use the OSB benchmark program as well. Your program might well be okay, but benchmarking is a hard thing to get right in a parallel world, so you might as well validate it by cross-checking the result.<br>
<br></div>I suggest this mostly because your performance numbers are far worse than anything we&#39;ve measured using those standard benchmarks, and so we should first ensure we aren&#39;t chasing a ghost.<br><br><div><br>
<br></div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Wed, Apr 16, 2014 at 1:41 AM, Muhammad Ansar Javed <span dir="ltr">&lt;<a href="mailto:muhammad.ansar@seecs.edu.pk" target="_blank">muhammad.ansar@seecs.edu.pk</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div>Yes, I have tried NetPipe-Java and iperf for bandwidth and configuration test. NetPipe Java achieves maximum 9.40 Gbps while iperf achieves maximum 9.61 Gbps bandwidth. I have also tested my bandwidth program on 1Gbps Ethernet connection and it achieves 901 Mbps bandwidth. I am using the same program for 10G network benchmarks. Please find attached source file of bandwidth program. <br>

<br></div>As far as --bind-to core is concerned, I think it is working fine. Here is output of --report-bindings switch.<br>[host3:07134] MCW rank 0 bound to socket 0[core 0[hwt 0]]: [B/././.]<br>[host4:10282] MCW rank 1 bound to socket 0[core 0[hwt 0]]: [B/././.]<br>

<br><br>
</div><div class="gmail_extra"><div><div class="h5"><br><br><div class="gmail_quote">On Tue, Apr 15, 2014 at 8:39 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">Have you tried a typical benchmark (e.g., NetPipe or OMB) to ensure the problem isn&#39;t in your program? Outside of that, you might want to explicitly tell it to --bind-to core just to be sure it does so - it&#39;s supposed to do that by default, but might as well be sure. You can check by adding --report-binding to the cmd line.<div>

<br></div><div><br><div><div><div><div>On Apr 14, 2014, at 11:10 PM, Muhammad Ansar Javed &lt;<a href="mailto:muhammad.ansar@seecs.edu.pk" target="_blank">muhammad.ansar@seecs.edu.pk</a>&gt; wrote:</div><br></div>
</div><blockquote type="cite"><div><div><div dir="ltr"><div>Hi,<br>I am trying to benchmark Open MPI performance on 10G Ethernet network between two hosts. The performance numbers of benchmarks are less than expected. The maximum bandwidth achieved by OMPI-C is 5678 Mbps and I was expecting around 9000+ Mbps. Moreover latency is also quite higher than expected, ranging from 37 to 59 us. Here is complete set of numbers.<br>


<br><b>Latency<br>Open MPI C    <br>Size    Time (us)</b><br>1         37.76<br>2         37.75<br>4         37.78<br>8         55.17<br>16       37.89<br>32       39.08<br>64       37.78<br>128     59.46<br>256     39.37<br>


512     40.39<br>1024   47.18<br>2048   47.84<br>    <br><br><b>Bandwidth<br>Open MPI C    <br>Size (Bytes)    Bandwidth (Mbps)</b><br>2048               412.22<br>4096               539.59<br>8192               827.73<br>


16384             1655.35<br>32768             3274.3<br>65536             1995.22<br>131072           3270.84<br>262144           4316.22<br>524288           5019.46<br>1048576         5236.17<br>2097152         5362.61<br>


4194304         5495.2<br>8388608         5565.32<br>16777216       5678.32<br><br><br>My environments consists of two hosts having point-to-point (switch-less) 10Gbps Ethernet connection.  Environment (OS, user, directory structure etc) on both hosts is exactly same. There is no NAS or shared file system between both hosts. Following are configuration and job launching commands that I am using. Moreover, I have attached output of script<span style="font-family:courier new,monospace"> ompi_info --all</span>.<br>


<br>Configuration commmand: <span style="font-family:courier new,monospace">./configure --enable-mpi-java --prefix=/home/mpj/installed/openmpi_installed CC=/usr/bin/gcc --disable-mpi-fortran </span><br><br>Job launching command: <span style="font-family:courier new,monospace">mpirun -np 2 -hostfile machines -npernode 1 ./latency.out<br>


</span><br>Are these numbers okay? If not then please suggest performance tuning steps...<br><br></div>Thanks<br clear="all"><div><br>--<br><div dir="ltr">Ansar Javed<br>HPC Lab<br>SEECS NUST <br>Contact: <a href="tel:%2B92%20334%20438%209394" value="+923344389394" target="_blank">+92 334 438 9394</a><br>


Email: <a href="mailto:muhammad.ansar@seecs.edu.pk" target="_blank">muhammad.ansar@seecs.edu.pk</a><br></div>
</div></div>
</div></div><span>&lt;ompi_info.tar.bz2&gt;</span>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>

</div><br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br><br clear="all"><br></div></div><span class="HOEnZb"><font color="#888888">-- <br>
</font></span><div dir="ltr"><span class="HOEnZb"><font color="#888888">Regards</font></span><div class=""><br><br>Ansar Javed<br>
HPC Lab<br>SEECS NUST <br>Contact: <a href="tel:%2B92%20334%20438%209394" value="+923344389394" target="_blank">+92 334 438 9394</a><br>Email: <a href="mailto:muhammad.ansar@seecs.edu.pk" target="_blank">muhammad.ansar@seecs.edu.pk</a><br>
</div></div>
</div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>

