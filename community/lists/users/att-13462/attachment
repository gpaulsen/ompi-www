<meta http-equiv="content-type" content="text/html; charset=utf-8"><span class="Apple-style-span" style="font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse; ">&quot;But, I still want to find out how to store the result in a data structure with the type TaskPackage because <br>

int type data can only be used to carry integers. Too limited.&quot;</span><div><font class="Apple-style-span" face="arial, sans-serif"><span class="Apple-style-span" style="border-collapse: collapse;"><br></span></font></div>

<div><div><font class="Apple-style-span" face="arial, sans-serif"><span class="Apple-style-span" style="border-collapse: collapse;">you can do that with MPI, all you have to do is declare your own MPI type.  The command is MPI_type_struct.  I haven&#39;t done this personally, but if you go to this link:</span></font></div>

<div><font class="Apple-style-span" face="arial, sans-serif"><span class="Apple-style-span" style="border-collapse: collapse;"><a href="https://computing.llnl.gov/tutorials/mpi/">https://computing.llnl.gov/tutorials/mpi/</a></span></font></div>

<div><font class="Apple-style-span" face="arial, sans-serif">it&#39;ll show you know.  it is also a very good MPI reference.  I visit it often to look up MPI commands and syntax.</font></div><div><font class="Apple-style-span" face="arial, sans-serif"><span class="Apple-style-span" style="border-collapse: collapse; "><br>

</span></font><br><div class="gmail_quote">On Thu, Jul 1, 2010 at 9:09 AM, Jack Bryan <span dir="ltr">&lt;<a href="mailto:dtustudy68@hotmail.com">dtustudy68@hotmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">





<div>
Thanks for all your replies. <br><br>I want to do master-worker asynchronous communication. <br><br>The master needs to distribute tasks to workers and then collect results from them. <br><br>master : <br><br>world.irecv(resultSourceRank, upStreamTaskTag, myResultTaskPackage[iRank][taskCounterT3]);<br>

<br>I got this error &quot;MPI_ERR_TRUNCATE&quot; , because I declared &quot; TaskPackage myResultTaskPackage. &quot;<br><br>It seems that the 2-dimension array cannot be used to receive my defined <br>class package from worker, who sends a TaskPackage to master. <br>

<br>So, I changed it to an int 2-d array to get the result, it works well. <br><br>But, I still want to find out how to store the result in a data structure with the type TaskPackage because <br>int type data can only be used to carry integers. Too limited.<br>

<br>What I want to do is: <br><br>The master can store the results from each worker and then combine them together <br>to form the final result after collecting all results from workers. <br><br>But, if the master has number of tasks that cannot be divided evenly by worker numbers, <br>

each worker may have different number of tasks. <br><br>If we have 11 tasks and 3 workers.<br><br>aveTaskNumPerNode = (11 - 11%3) /3 = 3<br>leftTaskNum = 11%3 =2 = Z<br><br>the master distributes each of left tasks from worker 1 to work Z (Z &lt; totalNumWorkers).<br>

<br>For example, worker 1: 4 tasks, worker 2: 4 task, worker 3: 3 tasks.<br><br>The master tries to distribute tasks evenly so that the difference between workloads of <br>each worker is minimized. <br><br>I am going to use vector&#39;s vector to do the dynamic data storage. <br>

<br>The 2-dimensional data-structure that can store results from workers. <br><br>Each row element of the data-structure has different columns. <br><br>It can be indexed by iterator so that I can find the a specified number worker task result <br>

by searching the data strucutre. <br><br>For example, <br>               column           column <br>                  1                2<br> row 1   (worker1.task1)    (worker1.task4)     <br> row 2   (worker2.task2)     (worker1.task5)   <br>

 row 3   (worker3.task3) <br><br>the data strucutre should remember the location of work ID and the task ID.<br>So that the master can know which task comes from which worker. <br><br>Any help or comment are appreciated. <br>

<div class="im"><br>thanks<br><br>Jack <br><br>June 30   2010<br><br><br><br></div>&gt; Date: Thu, 1 Jul 2010 11:44:19 -0400<br>&gt; From: <a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a><div>

<div></div><div class="h5"><br>&gt; To: <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>&gt; Subject: Re: [OMPI users] Open MPI, Segmentation fault<br>&gt; <br>&gt; Hello Jack, list<br>&gt; <br>

&gt; As others mentioned, this may be a problem with dynamic<br>&gt; memory allocation.<br>&gt; It could also be a violation of statically allocated memory,<br>&gt; I guess.<br>&gt; <br>&gt; You say:<br>&gt; <br>&gt; &gt; My program can run well for 1,2,10 processors, but fail when the<br>

&gt; &gt; number of tasks cannot<br>&gt; &gt; be divided evenly by number of processes.<br>&gt; <br>&gt; Often times, when the division of the number of &quot;tasks&quot;<br>&gt; (or the global problem size) by the number of &quot;processors&quot; is not even, <br>

&gt; one processor gets a lighter/heavier workload then the others,<br>&gt; it also allocates  less/more memory than the others,<br>&gt; and it accesses smaller/larger arrays than the others.<br>&gt; <br>&gt; In general integer division and remainder/module calculations<br>

&gt; are used to control memory allocation, the array sizes, etc,<br>&gt; on different processors.<br>&gt; These formulas tend to use the MPI communicator size<br>&gt; (i.e., effectively the number of processors if you are using <br>

&gt; MPI_COMM_WORLD) to split the workload across the processors.<br>&gt; <br>&gt; I would search for the lines of code where those calculations are done, <br>&gt; and where the arrays are allocated and accessed,<br>&gt; to make sure the algorithm works both when<br>

&gt; they are of the same size<br>&gt; (even workload across the processors),<br>&gt; as when they are of different sizes<br>&gt; (uneven workload across the processors).<br>&gt; You may be violating memory access by a few bytes only, due to a small<br>

&gt; mistake in one of those integer division / remainder/module formulas,<br>&gt; perhaps where an array index upper or lower bound is calculated.<br>&gt; It happened to me before, probably to others too.<br>&gt; <br>&gt; This type of code inspection can be done without a debugger,<br>

&gt; or before you get to the debugger phase.<br>&gt; <br>&gt; I hope this helps,<br>&gt; Gus Correa<br>&gt; ---------------------------------------------------------------------<br>&gt; Gustavo Correa<br>&gt; Lamont-Doherty Earth Observatory - Columbia University<br>

&gt; Palisades, NY, 10964-8000 - USA<br>&gt; ---------------------------------------------------------------------<br>&gt; <br>&gt; &gt; Jeff Squyres wrote:<br>&gt; &gt; Also see <a href="http://www.open-mpi.org/faq/?category=debugging" target="_blank">http://www.open-mpi.org/faq/?category=debugging</a>.<br>

&gt; &gt; <br>&gt; &gt; On Jul 1, 2010, at 3:17 AM, Asad Ali wrote:<br>&gt; &gt; <br>&gt; &gt;&gt; Hi Jack,<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Debugging OpenMPI with traditional debuggers is a pain.<br>&gt; &gt;&gt; &gt;From your error message it sounds that you have some memory allocation problem. Do you use dynamic memory allocation (allocate and then free)?<br>

&gt; &gt;&gt;<br>&gt; &gt;&gt; I use display (printf()) command with MPIrank command. It tells me which thread is giving segmentation fault.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Cheers,<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Asad<br>

&gt; &gt;&gt;<br>&gt; &gt;&gt; On Thu, Jul 1, 2010 at 4:13 PM, Jack Bryan &lt;<a href="mailto:dtustudy68@hotmail.com" target="_blank">dtustudy68@hotmail.com</a>&gt; wrote:<br>&gt; &gt;&gt; thanks<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; I am not familiar with OpenMPI. <br>

&gt; &gt;&gt;<br>&gt; &gt;&gt; Would you please help me with how to ask openMPI to show where the fault occurs ?<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; GNU debuger ?<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Any help is appreciated. <br>

&gt; &gt;&gt;<br>&gt; &gt;&gt; thanks!!!<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Jack <br>&gt; &gt;&gt;<br>&gt; &gt;&gt; June 30  2010<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Date: Wed, 30 Jun 2010 16:13:09 -0400<br>&gt; &gt;&gt; From: <a href="mailto:amjad11@gmail.com" target="_blank">amjad11@gmail.com</a><br>

&gt; &gt;&gt; To: <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>&gt; &gt;&gt; Subject: Re: [OMPI users] Open MPI, Segmentation fault<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Based on my experiences, I would FULLY endorse (100% agree with) David Zhang.<br>

&gt; &gt;&gt; It is usually a coding or typo mistake.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; At first, Ensure that array sizes and dimension are correct.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; I experience that if openmpi is compiled with gnu compilers (not with Intel) then it also point outs the subroutine exactly in which the fault occur. have a try.<br>

&gt; &gt;&gt;<br>&gt; &gt;&gt; best,<br>&gt; &gt;&gt; AA<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;   <br>&gt; &gt;&gt;<br>&gt; &gt;&gt; On Wed, Jun 30, 2010 at 12:43 PM, David Zhang &lt;<a href="mailto:solarbikedz@gmail.com" target="_blank">solarbikedz@gmail.com</a>&gt; wrote:<br>

&gt; &gt;&gt; When I got segmentation faults, it has always been my coding mistakes.  Perhaps your code is not robust against number of processes not divisible by 2?<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; On Wed, Jun 30, 2010 at 8:47 AM, Jack Bryan &lt;<a href="mailto:dtustudy68@hotmail.com" target="_blank">dtustudy68@hotmail.com</a>&gt; wrote:<br>

&gt; &gt;&gt; Dear All,<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; I am using Open MPI, I got the error: <br>&gt; &gt;&gt;<br>&gt; &gt;&gt; n337:37664] *** Process received signal ***<br>&gt; &gt;&gt; [n337:37664] Signal: Segmentation fault (11)<br>

&gt; &gt;&gt; [n337:37664] Signal code: Address not mapped (1)<br>&gt; &gt;&gt; [n337:37664] Failing at address: 0x7fffcfe90000<br>&gt; &gt;&gt; [n337:37664] [ 0] /lib64/libpthread.so.0 [0x3c50e0e4c0]<br>&gt; &gt;&gt; [n337:37664] [ 1] /lustre/home/rhascheduler/RhaScheduler-0.4.1.1/mytest/nmn2 [0x414ed7]<br>

&gt; &gt;&gt; [n337:37664] [ 2] /lib64/libc.so.6(__libc_start_main+0xf4) [0x3c5021d974]<br>&gt; &gt;&gt; [n337:37664] [ 3] /lustre/home/rhascheduler/RhaScheduler-0.4.1.1/mytest/nmn2(__gxx_personality_v0+0x1f1) [0x412139]<br>

&gt; &gt;&gt; [n337:37664] *** End of error message ***<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; After searching answers, it seems that some functions fail. <br>&gt; &gt;&gt;  <br>&gt; &gt;&gt; My program can run well for 1,2,10 processors, but fail when the number of tasks cannot<br>

&gt; &gt;&gt; be divided evenly by number of processes. <br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Any help is appreciated. <br>&gt; &gt;&gt;<br>&gt; &gt;&gt; thanks<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Jack<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; June 30  2010<br>

&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; The New Busy think 9 to 5 is a cute idea. Combine multiple calendars with Hotmail. Get busy.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; _______________________________________________<br>

&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>&gt; &gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>

&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; -- <br>&gt; &gt;&gt; David Zhang<br>&gt; &gt;&gt; University of California, San Diego<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; _______________________________________________<br>

&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>&gt; &gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>

&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Hotmail has tools for the New Busy. Search, chat and e-mail from your inbox. Learn more.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; _______________________________________________<br>

&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>&gt; &gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>

&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; -- <br>&gt; &gt;&gt; &quot;Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write.&quot; - H.G. Wells<br>

&gt; &gt;&gt; _______________________________________________<br>&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>&gt; &gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>

&gt; &gt; <br>&gt; &gt; <br>&gt; <br>&gt; <br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>

 		 	   		  <br></div></div><div class="hm"><hr>The New Busy is not the too busy. Combine all your e-mail accounts with Hotmail. <a href="http://www.windowslive.com/campaign/thenewbusy?tile=multiaccount&amp;ocid=PID28326::T:WLMTAGL:ON:WL:en-US:WM_HMP:042010_4" target="_blank">Get busy.</a></div>

</div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br><br clear="all"><br>-- <br>David Zhang<br>University of California, San Diego<br>


</div></div>

