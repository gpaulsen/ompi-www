Hi all,<br><br>Many many thanks to all of you for your time, sincere help, useful tips and advices. <br>I have solved that problem. I just removed the gcc flag -O3 from my compile script and the error vanished. However the speed of my code is also reduced to 50 iterations/minute from 70 iterations/minute, still not bad.<br>
<br>Is there any alternative to -O3 flag?<br><br>I tried -O2 too but this also gives errors.<br>I know this is now a non-openmpi problem but I know this is the forum of experienced programmers so I hope you people would tolerate me and would help me.<br>
<br>Cheers,<br><br>Asad<br><br><br><div class="gmail_quote">On Wed, Apr 28, 2010 at 4:59 AM, Gus Correa <span dir="ltr">&lt;<a href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Hi Dave<div class="im"><br>
<br>
Dave Love wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Gus Correa &lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt; writes:<br>
<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Or run a serial version on the same set of machines,<br>
compiled in similar ways (compiler version, opt flags, etc)<br>
to the parallel versions, and compare results.<br>
If the results don&#39;t differ, then you can start blaming MPI.<br>
</blockquote>
<br>
That wouldn&#39;t show that there&#39;s actually any OpenMPI-specific problem,<br>
though -- the parallelism potentially introduces indeterminacy.  [I<br>
don&#39;t mean to imply Guy thinks otherwise, or that anyone has enough<br>
information to guess what&#39;s actually happening.]  General discussion of<br>
numerical issues and scientific computing war stories must be way<br>
off-topic here...<br>
<br>
</blockquote>
<br></div>
You are right.<br>
Actually, on a case of an ocean model that I tested this way,<br>
exactly the opposite happened.<br>
The optimization flags were the main cause of changes in the results,<br>
as expected.<br>
Whether the code was compiled and run serial or parallel with MPI<br>
made little difference (although it might).<br>
<br>
Well, IMHO, this is not totally off topic.<br>
I don&#39;t think it hurts the list protocol to have a relatively<br>
lax criterion for what is on or off.<br>
After all, the original question and the following<br>
discussion was about whether MPI (OpenMPI in particular)<br>
can affect or not numerical accuracy.<br>
<br>
Regards,<br><font color="#888888">
Gus Correa</font><div><div></div><div class="h5"><br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br><br clear="all"><br>-- <br>&quot;Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write.&quot; - H.G. Wells<br>

