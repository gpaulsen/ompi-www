<div dir="ltr">Hi Jeff,<div><br></div><div>thanks for the reply.</div><div><br></div><div style>The issue is that when you read or write PCIe_gen 3 dat to a non-local NUMA memory, SandyBridge will use the inter-socket QPIs to get this data across to the other socket. I think there is considerable limitation in PCIe I/O traffic data going over the inter-socket QPI. One way to get around this is for reads to buffer all data into memory space local to the same socket and then transfer them by code across to the other socket&#39;s physical memory. For writes the same approach can be used with intermediary process copying data.</div>
<div style><br></div><div style>I was wondering if OpenMPI does anything special memory mapping to work around this. And if with Ivy Bridge (or Haswell) he situation has improved.</div><div style><br></div><div style>thanks </div>
<div style>Mike</div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Mon, Jul 8, 2013 at 9:57 AM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div class="im">On Jul 6, 2013, at 4:59 PM, Michael Thomadakis &lt;<a href="mailto:drmichaelt7777@gmail.com">drmichaelt7777@gmail.com</a>&gt; wrote:<br>

<br>
&gt; When you stack runs on SandyBridge nodes atached to HCAs ove PCI3 gen 3 do you pay any special attention to the memory buffers according to which socket/memory controller  their physical memory belongs to?<br>
&gt;<br>
&gt; For instance, if the HCA is attached to the PCIgen3 lanes of Socket 1 do you do anything special when the read/write buffers map to physical memory belonging to Socket 2? Or do you7 avoid using buffers mapping ro memory that belongs (is accessible via) the other socket?<br>

<br>
</div>It is not *necessary* to do ensure that buffers are NUMA-local to the PCI device that they are writing to, but it certainly results in lower latency to read/write to PCI devices (regardless of flavor) that are attached to an MPI process&#39; local NUMA node.  The Hardware Locality (hwloc) tool &quot;lstopo&quot; can print a pretty picture of your server to show you where your PCI busses are connected.<br>

<br>
For TCP, Open MPI will use all TCP devices that it finds by default (because it is assumed that latency is so high that NUMA locality doesn&#39;t matter).  The openib (OpenFabrics) transport will use the &quot;closest&quot; HCA ports that it can find to each MPI process.<br>

<br>
In our upcoming Cisco ultra low latency BTL, it defaults to using the closest Cisco VIC ports that it can find for short messages (i.e., to minimize latency), but uses all available VICs for long messages (i.e., to maximize bandwidth).<br>

<div class="im"><br>
&gt; Has this situation improved with Ivy-Brige systems or Haswell?<br>
<br>
</div>It&#39;s the same overall architecture (i.e., NUMA).<br>
<span class="HOEnZb"><font color="#888888"><br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</font></span></blockquote></div><br></div>

