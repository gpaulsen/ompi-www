<html><body><div style="color:#000; background-color:#fff; font-family:times new roman, new york, times, serif;font-size:10pt">Dear MPI people, <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I want to use LogGP model with MPI to find a message with K bytes will take how much time. In this, I need to find Latency L, Overhead o and Gap G. Can somebody tell me how can I measure these three parameters of the underlying network ? and how often should I measure these parameters so that the predication of time for sending a message of K bytes remains accurate.<br><br>regards,<br>Mudassar<br><div><br></div><div style="font-family: times new roman, new york, times, serif; font-size: 10pt;"><div style="font-family: times new roman, new york, times, serif; font-size: 12pt;"><font face="Arial" size="2"><hr
 size="1"><b><span style="font-weight:bold;">From:</span></b> "users-request@open-mpi.org" &lt;users-request@open-mpi.org&gt;<br><b><span style="font-weight: bold;">To:</span></b> users@open-mpi.org<br><b><span style="font-weight: bold;">Sent:</span></b> Wednesday, October 26, 2011 6:00 PM<br><b><span style="font-weight: bold;">Subject:</span></b> users Digest, Vol 2052, Issue 1<br></font><br>Send users mailing list submissions to<br>&nbsp;&nbsp;&nbsp; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><br>To subscribe or unsubscribe via the World Wide Web, visit<br>&nbsp;&nbsp;&nbsp; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>or, via email, send a message with subject or body 'help' to<br>&nbsp;&nbsp;&nbsp; <a ymailto="mailto:users-request@open-mpi.org"
 href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a><br><br>You can reach the person managing the list at<br>&nbsp;&nbsp;&nbsp; <a ymailto="mailto:users-owner@open-mpi.org" href="mailto:users-owner@open-mpi.org">users-owner@open-mpi.org</a><br><br>When replying, please edit your Subject line so it is more specific<br>than "Re: Contents of users digest..."<br><br><br>Today's Topics:<br><br>&nbsp;  1. Re: Problem-Bug with MPI_Intercomm_create() (Ralph Castain)<br>&nbsp;  2. Re: Checkpoint from inside MPI program with OpenMPI 1.4.2 ?<br>&nbsp; &nbsp; &nbsp; (Josh Hursey)<br>&nbsp;  3. Subnet routing (1.2.x) not working in 1.4.3 anymore (Mirco Wahab)<br>&nbsp;  4. Re: mpirun should run with just the localhost&nbsp;&nbsp;&nbsp; interface on<br>&nbsp; &nbsp; &nbsp; win? (MM)<br>&nbsp;  5. Re: Checkpoint from inside MPI program with OpenMPI 1.4.2 ?<br>&nbsp; &nbsp; &nbsp; (Nguyen Toan)<br>&nbsp;  6. Re: exited on signal 11 (Segmentation
 fault).<br>&nbsp; &nbsp; &nbsp; (Mouhamad Al-Sayed-Ali)<br>&nbsp;  7. Changing plm_rsh_agent system wide (Patrick Begou)<br>&nbsp;  8. Re: Checkpoint from inside MPI program with OpenMPI 1.4.2 ?<br>&nbsp; &nbsp; &nbsp; (Josh Hursey)<br>&nbsp;  9. Re: Changing plm_rsh_agent system wide (Ralph Castain)<br>&nbsp; 10. Re: Changing plm_rsh_agent system wide (TERRY DONTJE)<br>&nbsp; 11. Re: Changing plm_rsh_agent system wide (TERRY DONTJE)<br>&nbsp; 12. Re: Changing plm_rsh_agent system wide (Patrick Begou)<br><br><br>----------------------------------------------------------------------<br><br>Message: 1<br>Date: Tue, 25 Oct 2011 10:08:00 -0600<br>From: Ralph Castain &lt;<a ymailto="mailto:rhc@open-mpi.org" href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;<br>Subject: Re: [OMPI users] Problem-Bug with MPI_Intercomm_create()<br>To: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org"
 href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID: &lt;<a ymailto="mailto:30D41149-6683-41C2-ACE0-776C64E5C83C@open-mpi.org" href="mailto:30D41149-6683-41C2-ACE0-776C64E5C83C@open-mpi.org">30D41149-6683-41C2-ACE0-776C64E5C83C@open-mpi.org</a>&gt;<br>Content-Type: text/plain; charset=iso-8859-1<br><br>FWIW: I have tracked this problem down. The fix is a little more complicated then I'd like, so I'm going to have to ping some other folks to ensure we concur on the approach before doing something.<br><br>On Oct 25, 2011, at 8:20 AM, Ralph Castain wrote:<br><br>&gt; I still see it failing the test George provided on the trunk. I'm unaware of anyone looking further into it, though, as the prior discussion seemed to just end.<br>&gt; <br>&gt; On Oct 25, 2011, at 7:01 AM, orel wrote:<br>&gt; <br>&gt;&gt; Dears,<br>&gt;&gt; <br>&gt;&gt; I try from several days to use advanced MPI2 features in the following scenario :<br>&gt;&gt;
 <br>&gt;&gt; 1) a master code A (of size NPA) spawns (MPI_Comm_spawn()) two slave<br>&gt;&gt;&nbsp; &nbsp; codes B (of size NPB) and C (of size NPC), providing intercomms A-B and A-C ;<br>&gt;&gt; 2) i create intracomm AB and AC by merging intercomms ;<br>&gt;&gt; 3) then i create intercomm AB-C by calling MPI_Intercomm_create() by using AC as bridge...<br>&gt;&gt; <br>&gt;&gt;&nbsp;  MPI_Comm intercommABC; A: MPI_Intercomm_create(intracommAB, 0, intracommAC, NPA, TAG,&amp;intercommABC);<br>&gt;&gt; B: MPI_Intercomm_create(intracommAB, 0, MPI_COMM_NULL, 0,TAG,&amp;intercommABC);<br>&gt;&gt; C: MPI_Intercomm_create(intracommC, 0, intracommAC, 0, TAG,&amp;intercommABC);<br>&gt;&gt; <br>&gt;&gt;&nbsp; &nbsp;  In these calls, A0 and C0 play the role of local leader for AB and C respectively.<br>&gt;&gt;&nbsp; &nbsp;  C0 and A0 play the roles of remote leader in bridge intracomm AC.<br>&gt;&gt; <br>&gt;&gt; 3)&nbsp; MPI_Barrier(intercommABC);<br>&gt;&gt;
 4)&nbsp; i merge intercomm AB-C into intracomm ABC$<br>&gt;&gt; 5)&nbsp; MPI_Barrier(intracommABC);<br>&gt;&gt; <br>&gt;&gt; My BUG: These calls success, but when i try to use intracommABC for a collective communication like MPI_Barrier(),<br>&gt;&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; i got the following error :<br>&gt;&gt; <br>&gt;&gt; *** An error occurred in MPI_Barrier<br>&gt;&gt; *** on communicator<br>&gt;&gt; *** MPI_ERR_INTERN: internal error<br>&gt;&gt; *** MPI_ERRORS_ARE_FATAL: your MPI job will now abort<br>&gt;&gt; <br>&gt;&gt; <br>&gt;&gt; I try with OpenMPI trunk, 1.5.3, 1.5.4 and Mpich2-1.4.1p1<br>&gt;&gt; <br>&gt;&gt; My code works perfectly if intracomm A, B and C are obtained by MPI_Comm_split() instead of MPI_Comm_spawn() !!!!<br>&gt;&gt; <br>&gt;&gt; <br>&gt;&gt; I found same problem in a previous thread of the OMPI Users mailing list :<br>&gt;&gt; <br>&gt;&gt; =&gt; <a
 href="http://www.open-mpi.org/community/lists/users/2011/06/16711.php" target="_blank">http://www.open-mpi.org/community/lists/users/2011/06/16711.php</a><br>&gt;&gt; <br>&gt;&gt; Is that bug/problem is currently under investigation ? :-)<br>&gt;&gt; <br>&gt;&gt; i can give detailed code, but the one provided by George Bosilca in this previous thread provides same error...<br>&gt;&gt; <br>&gt;&gt; Thank you to help me...<br>&gt;&gt; <br>&gt;&gt; -- <br>&gt;&gt; Aur?lien Esnard<br>&gt;&gt; University Bordeaux 1 / LaBRI / INRIA (France)<br>&gt;&gt; _______________________________________________<br>&gt;&gt; users mailing list<br>&gt;&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; <br><br><br><br><br>------------------------------<br><br>Message:
 2<br>Date: Tue, 25 Oct 2011 13:25:27 -0500<br>From: Josh Hursey &lt;<a ymailto="mailto:jjhursey@open-mpi.org" href="mailto:jjhursey@open-mpi.org">jjhursey@open-mpi.org</a>&gt;<br>Subject: Re: [OMPI users] Checkpoint from inside MPI program with<br>&nbsp;&nbsp;&nbsp; OpenMPI 1.4.2 ?<br>To: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID:<br>&nbsp;&nbsp;&nbsp; &lt;CAANzjEnOdwva5J4fFBmXtsK6Kj3yGE9j=dKdtaWuZs=<a ymailto="mailto:wHzGbQg@mail.gmail.com" href="mailto:wHzGbQg@mail.gmail.com">wHzGbQg@mail.gmail.com</a>&gt;<br>Content-Type: text/plain; charset=ISO-8859-1<br><br>Open MPI (trunk/1.7 - not 1.4 or 1.5) provides an application level<br>interface to request a checkpoint of an application. This API is<br>defined on the following website:<br>&nbsp; <a href="http://osl.iu.edu/research/ft/ompi-cr/api.php#api-cr_checkpoint"
 target="_blank">http://osl.iu.edu/research/ft/ompi-cr/api.php#api-cr_checkpoint</a><br><br>This will behave the same as if you requested the checkpoint of the<br>job from the command line.<br><br>-- Josh<br><br>On Mon, Oct 24, 2011 at 12:37 PM, Nguyen Toan &lt;<a ymailto="mailto:nguyentoan1508@gmail.com" href="mailto:nguyentoan1508@gmail.com">nguyentoan1508@gmail.com</a>&gt; wrote:<br>&gt; Dear all,<br>&gt; I want to automatically checkpoint an MPI program with OpenMPI ( I'm<br>&gt; currently using 1.4.2 version with BLCR 0.8.2),<br>&gt; not by manually typing ompi-checkpoint command line from another terminal.<br>&gt; So I would like to know if there is a way to call checkpoint function from<br>&gt; inside an MPI program<br>&gt; with OpenMPI or how to do that.<br>&gt; Any ideas are very appreciated.<br>&gt; Regards,<br>&gt; Nguyen Toan<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a
 ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<br><br><br><br>-- <br>Joshua Hursey<br>Postdoctoral Research Associate<br>Oak Ridge National Laboratory<br><a href="http://users.nccs.gov/%7Ejjhursey" target="_blank">http://users.nccs.gov/~jjhursey</a><br><br><br>------------------------------<br><br>Message: 3<br>Date: Tue, 25 Oct 2011 22:15:12 +0200<br>From: Mirco Wahab &lt;<a ymailto="mailto:mirco.wahab@chemie.tu-freiberg.de" href="mailto:mirco.wahab@chemie.tu-freiberg.de">mirco.wahab@chemie.tu-freiberg.de</a>&gt;<br>Subject: [OMPI users] Subnet routing (1.2.x) not working in 1.4.3<br>&nbsp;&nbsp;&nbsp; anymore<br>To: <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Message-ID: &lt;<a
 ymailto="mailto:4EA718D0.5060005@chemie.tu-freiberg.de" href="mailto:4EA718D0.5060005@chemie.tu-freiberg.de">4EA718D0.5060005@chemie.tu-freiberg.de</a>&gt;<br>Content-Type: text/plain; charset=ISO-8859-1; format=flowed<br><br>In the last few years, it has been very simple to<br>set up high-performance (GbE) multiple back-to-back<br>connections between three nodes (triangular topology)<br>or four nodes (tetrahedral topology).<br><br>The only things you had to do was<br>- use 3 (or 4) cheap compute nodes w/Linux and connect<br>&nbsp;  each of them via standard GbE router (onboard GbE NIC)<br>&nbsp;  to a file server,<br>- put 2 (trigonal topol.) or 3 (tetrahedral topol.)<br>&nbsp;  $25 PCIe-GbE-NICs into *each* node,<br>- connect the nodes with 3 (trigonal) or 4 (tetrahedral)<br>&nbsp;  short crossover Cat5e cables,<br>- configure the extra NICs into different subnets<br>&nbsp;  according to their "edge index", eg.<br>&nbsp;  for 3 nodes (node10, node11,
 node12)<br>&nbsp; &nbsp;  node10<br>&nbsp; &nbsp; &nbsp;  onboard NIC: 192.168.0.10 on eth0 (to router/server)<br>&nbsp; &nbsp; &nbsp;  extra NIC: 10.0.1.10 on eth1 (edge 1 to 10.0.1.11)<br>&nbsp; &nbsp; &nbsp;  extra NIC: 10.0.2.10 on eth2 (edge 2 to 10.0.2.12)<br>&nbsp; &nbsp;  node11<br>&nbsp; &nbsp; &nbsp;  onboard NIC: 192.168.0.11 on eth0 (to router/server)<br>&nbsp; &nbsp; &nbsp;  extra NIC: 10.0.1.11 on eth1 (edge 1 to 10.0.1.10)<br>&nbsp; &nbsp; &nbsp;  extra NIC: 10.0.3.11 on eth3 (edge 3 to 10.0.3.12)<br>&nbsp; &nbsp;  node12<br>&nbsp; &nbsp; &nbsp;  onboard NIC: 192.168.0.12 on eth0 (to router/server)<br>&nbsp; &nbsp; &nbsp;  extra NIC: 10.0.2.12 on eth2 (edge 2 to 10.0.2.10)<br>&nbsp; &nbsp; &nbsp;  extra NIC: 10.0.3.12 on eth3 (edge 3 to 10.0.3.11)<br>- that's it. I mean, that *was* it, with 1.2.x.<br><br>OMPI 1.2.x would then ingeniously discover the routable edges<br>and open communication ports accordingly without any
 additional<br>explicit host routing, eg. invoked by<br><br>$&gt; mpirun -np 12 --host c10,c11,c12 --mca btl_tcp_if_exclude lo,eth0&nbsp; my_mpi_app<br><br>and (measured by iftop) saturate the available edges with<br>about 100MB/sec duplex on each of them. It would not stumble<br>on the fact, that some interfaces are not reacheable by<br>every NIC directly. And this was very convenient over the years.<br><br>With 1.4.3 (which comes out of the box) w/actual Linux distributions,<br>this won't work. It would hang and complain after timeout about failed<br>endpoint connects, eg:<br><br>[node12][[52378,1],2][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 10.0.1.11 failed: Connection timed out (110)<br><br>* Can the intelligent behaviour of 1.2.x be "configured back"?<br><br>* How should the topology look like to work with 1,4,x painlessly?<br><br>Thanks &amp;
 regards<br><br>M.<br><br><br><br><br>------------------------------<br><br>Message: 4<br>Date: Tue, 25 Oct 2011 21:33:54 +0100<br>From: "MM" &lt;<a ymailto="mailto:finjulhich@gmail.com" href="mailto:finjulhich@gmail.com">finjulhich@gmail.com</a>&gt;<br>Subject: Re: [OMPI users] mpirun should run with just the localhost<br>&nbsp;&nbsp;&nbsp; interface on win?<br>To: "'openmpi mailing list'" &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID: &lt;00d601cc9355$6af47290$40dd57b0$@com&gt;<br>Content-Type: text/plain;&nbsp;&nbsp;&nbsp; charset="us-ascii"<br><br>-----Original Message-----<br><br>if the interface is down, should localhost still allow mpirun to run mpi<br>processes?<br><br><br><br>------------------------------<br><br>Message: 5<br>Date: Wed, 26 Oct 2011 13:52:17 +0900<br>From: Nguyen Toan &lt;<a ymailto="mailto:nguyentoan1508@gmail.com"
 href="mailto:nguyentoan1508@gmail.com">nguyentoan1508@gmail.com</a>&gt;<br>Subject: Re: [OMPI users] Checkpoint from inside MPI program with<br>&nbsp;&nbsp;&nbsp; OpenMPI 1.4.2 ?<br>To: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID:<br>&nbsp;&nbsp;&nbsp; &lt;CAFiEserJ0U9m9euy1-CA8m=_KihMM5s73qaJiii_N=<a ymailto="mailto:p7f3Kdug@mail.gmail.com" href="mailto:p7f3Kdug@mail.gmail.com">p7f3Kdug@mail.gmail.com</a>&gt;<br>Content-Type: text/plain; charset="iso-8859-1"<br><br>Dear Josh,<br><br>Thank you. I will test the 1.7 trunk as you suggested.<br>Also I want to ask if we can add this interface to OpenMPI 1.4.2,<br>because my applications are mainly involved in this version.<br><br>Regards,<br>Nguyen Toan<br><br>On Wed, Oct 26, 2011 at 3:25 AM, Josh Hursey &lt;<a ymailto="mailto:jjhursey@open-mpi.org" href="mailto:jjhursey@open-mpi.org">jjhursey@open-mpi.org</a>&gt;
 wrote:<br><br>&gt; Open MPI (trunk/1.7 - not 1.4 or 1.5) provides an application level<br>&gt; interface to request a checkpoint of an application. This API is<br>&gt; defined on the following website:<br>&gt;&nbsp; <a href="http://osl.iu.edu/research/ft/ompi-cr/api.php#api-cr_checkpoint" target="_blank">http://osl.iu.edu/research/ft/ompi-cr/api.php#api-cr_checkpoint</a><br>&gt;<br>&gt; This will behave the same as if you requested the checkpoint of the<br>&gt; job from the command line.<br>&gt;<br>&gt; -- Josh<br>&gt;<br>&gt; On Mon, Oct 24, 2011 at 12:37 PM, Nguyen Toan &lt;<a ymailto="mailto:nguyentoan1508@gmail.com" href="mailto:nguyentoan1508@gmail.com">nguyentoan1508@gmail.com</a>&gt;<br>&gt; wrote:<br>&gt; &gt; Dear all,<br>&gt; &gt; I want to automatically checkpoint an MPI program with OpenMPI ( I'm<br>&gt; &gt; currently using 1.4.2 version with BLCR 0.8.2),<br>&gt; &gt; not by manually typing ompi-checkpoint command line from another<br>&gt;
 terminal.<br>&gt; &gt; So I would like to know if there is a way to call checkpoint function<br>&gt; from<br>&gt; &gt; inside an MPI program<br>&gt; &gt; with OpenMPI or how to do that.<br>&gt; &gt; Any ideas are very appreciated.<br>&gt; &gt; Regards,<br>&gt; &gt; Nguyen Toan<br>&gt; &gt; _______________________________________________<br>&gt; &gt; users mailing list<br>&gt; &gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; &gt;<br>&gt;<br>&gt;<br>&gt;<br>&gt; --<br>&gt; Joshua Hursey<br>&gt; Postdoctoral Research Associate<br>&gt; Oak Ridge National Laboratory<br>&gt; <a href="http://users.nccs.gov/%7Ejjhursey" target="_blank">http://users.nccs.gov/~jjhursey</a><br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt;
 <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<br>-------------- next part --------------<br>HTML attachment scrubbed and removed<br><br>------------------------------<br><br>Message: 6<br>Date: Wed, 26 Oct 2011 09:57:38 +0200<br>From: Mouhamad Al-Sayed-Ali &lt;<a ymailto="mailto:Mouhamad.Al-Sayed-Ali@u-bourgogne.fr" href="mailto:Mouhamad.Al-Sayed-Ali@u-bourgogne.fr">Mouhamad.Al-Sayed-Ali@u-bourgogne.fr</a>&gt;<br>Subject: Re: [OMPI users] exited on signal 11 (Segmentation fault).<br>To: Gus Correa &lt;<a ymailto="mailto:gus@ldeo.columbia.edu" href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt;<br>Cc: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID: &lt;<a
 ymailto="mailto:20111026095738.119675e8nwvpxhss@webmail.u-bourgogne.fr" href="mailto:20111026095738.119675e8nwvpxhss@webmail.u-bourgogne.fr">20111026095738.119675e8nwvpxhss@webmail.u-bourgogne.fr</a>&gt;<br>Content-Type: text/plain; charset=ISO-8859-1; DelSp="Yes";<br>&nbsp;&nbsp;&nbsp; format="flowed"<br><br>Hi Gus Correa,<br><br>&nbsp; the output of ulimit -a&nbsp; &nbsp;  is<br><br><br>----<br>file(blocks)&nbsp; &nbsp; &nbsp; &nbsp;  unlimited<br>coredump(blocks)&nbsp; &nbsp;  2048<br>data(kbytes)&nbsp; &nbsp; &nbsp; &nbsp;  unlimited<br>stack(kbytes)&nbsp; &nbsp; &nbsp; &nbsp; 10240<br>lockedmem(kbytes)&nbsp; &nbsp; unlimited<br>memory(kbytes)&nbsp; &nbsp; &nbsp;  unlimited<br>nofiles(descriptors) 1024<br>processes&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 256<br>--------<br><br><br>Thanks<br><br>Mouhamad<br>Gus Correa &lt;<a ymailto="mailto:gus@ldeo.columbia.edu" href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt; a ?crit?:<br><br>&gt;
 Hi Mouhamad<br>&gt;<br>&gt; The locked memory is set to unlimited, but the lines<br>&gt; about the stack are commented out.<br>&gt; Have you tried to add this line:<br>&gt;<br>&gt; *&nbsp;  -&nbsp;  stack&nbsp; &nbsp; &nbsp;  -1<br>&gt;<br>&gt; then run wrf again? [Note no "#" hash character]<br>&gt;<br>&gt; Also, if you login to the compute nodes,<br>&gt; what is the output of 'limit' [csh,tcsh] or 'ulimit -a' [sh,bash]?<br>&gt; This should tell you what limits are actually set.<br>&gt;<br>&gt; I hope this helps,<br>&gt; Gus Correa<br>&gt;<br>&gt; Mouhamad Al-Sayed-Ali wrote:<br>&gt;&gt; Hi all,<br>&gt;&gt;<br>&gt;&gt;&nbsp;  I've checked the "limits.conf", and it contains theses lines<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt; # Jcb 29.06.2007 : pbs wrf (Siji)<br>&gt;&gt; #*&nbsp; &nbsp; &nbsp; hard&nbsp; &nbsp; stack&nbsp;  1000000<br>&gt;&gt; #*&nbsp; &nbsp; &nbsp; soft&nbsp; &nbsp; stack&nbsp;  1000000<br>&gt;&gt;<br>&gt;&gt; # Dr 14.02.2008 : pour
 voltaire mpi<br>&gt;&gt; *&nbsp; &nbsp; &nbsp; hard&nbsp; &nbsp; memlock unlimited<br>&gt;&gt; *&nbsp; &nbsp; &nbsp; soft&nbsp; &nbsp; memlock unlimited<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt; Many thanks for your help<br>&gt;&gt; Mouhamad<br>&gt;&gt;<br>&gt;&gt; Gus Correa &lt;<a ymailto="mailto:gus@ldeo.columbia.edu" href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt; a ?crit :<br>&gt;&gt;<br>&gt;&gt;&gt; Hi Mouhamad, Ralph, Terry<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; Very often big programs like wrf crash with segfault because they<br>&gt;&gt;&gt; can't allocate memory on the stack, and assume the system doesn't<br>&gt;&gt;&gt; impose any limits for it.&nbsp; This has nothing to do with MPI.<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; Mouhamad:&nbsp; Check if your stack size is set to unlimited on all compute<br>&gt;&gt;&gt; nodes.&nbsp; The easy way to get it done<br>&gt;&gt;&gt; is to change /etc/security/limits.conf,<br>&gt;&gt;&gt; where you
 or your system administrator could add these lines:<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; *&nbsp;  -&nbsp;  memlock&nbsp; &nbsp;  -1<br>&gt;&gt;&gt; *&nbsp;  -&nbsp;  stack&nbsp; &nbsp; &nbsp;  -1<br>&gt;&gt;&gt; *&nbsp;  -&nbsp;  nofile&nbsp; &nbsp; &nbsp; 4096<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; My two cents,<br>&gt;&gt;&gt; Gus Correa<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; Ralph Castain wrote:<br>&gt;&gt;&gt;&gt; Looks like you are crashing in wrf - have you asked them for help?<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; On Oct 25, 2011, at 7:53 AM, Mouhamad Al-Sayed-Ali wrote:<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; Hi again,<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; This is exactly the error I have:<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; ----<br>&gt;&gt;&gt;&gt;&gt; taskid: 0 hostname: part034.u-bourgogne.fr<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] *** Process received signal ***<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] Signal: Segmentation fault
 (11)<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] Signal code: Address not mapped (1)<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] Failing at address: 0xfffffffe01eeb340<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] [ 0] /lib64/libpthread.so.0 [0x3612c0de70]<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] [ 1] wrf.exe(__module_ra_rrtm_MOD_taugb3+0x418)&nbsp; <br>&gt;&gt;&gt;&gt;&gt; [0x11cc9d8]<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] [ 2] wrf.exe(__module_ra_rrtm_MOD_gasabs+0x260)&nbsp; <br>&gt;&gt;&gt;&gt;&gt; [0x11cfca0]<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] [ 3] wrf.exe(__module_ra_rrtm_MOD_rrtm+0xb31) [0x11e6e41]<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] [ 4]&nbsp; <br>&gt;&gt;&gt;&gt;&gt; wrf.exe(__module_ra_rrtm_MOD_rrtmlwrad+0x25ec) [0x11e9bcc]<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] [ 5]&nbsp; <br>&gt;&gt;&gt;&gt;&gt; wrf.exe(__module_radiation_driver_MOD_radiation_driver+0xe573)&nbsp; <br>&gt;&gt;&gt;&gt;&gt; [0xcc4ed3]<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] [
 6]&nbsp; <br>&gt;&gt;&gt;&gt;&gt; wrf.exe(__module_first_rk_step_part1_MOD_first_rk_step_part1+0x40c5)&nbsp; <br>&gt;&gt;&gt;&gt;&gt; [0xe0e4f5]<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] [ 7] wrf.exe(solve_em_+0x22e58) [0x9b45c8]<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] [ 8] wrf.exe(solve_interface_+0x80a) [0x902dda]<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] [ 9]&nbsp; <br>&gt;&gt;&gt;&gt;&gt; wrf.exe(__module_integrate_MOD_integrate+0x236) [0x4b2c4a]<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] [10] wrf.exe(__module_wrf_top_MOD_wrf_run+0x24)&nbsp; <br>&gt;&gt;&gt;&gt;&gt; [0x47a924]<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] [11] wrf.exe(main+0x41) [0x4794d1]<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] [12] /lib64/libc.so.6(__libc_start_main+0xf4)&nbsp; <br>&gt;&gt;&gt;&gt;&gt; [0x361201d8b4]<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] [13] wrf.exe [0x4793c9]<br>&gt;&gt;&gt;&gt;&gt; [part034:21443] *** End of error message ***<br>&gt;&gt;&gt;&gt;&gt;
 -------<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; Mouhamad<br>&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>&gt;&gt;&gt;&gt;&gt; users mailing list<br>&gt;&gt;&gt;&gt;&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; _______________________________________________<br>&gt;&gt;&gt;&gt; users mailing list<br>&gt;&gt;&gt;&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;&gt;&gt;<br>&gt;&gt;&gt; _______________________________________________<br>&gt;&gt;&gt;
 users mailing list<br>&gt;&gt;&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;&gt;&gt;<br>&gt;&gt;&gt;<br>&gt;&gt;<br>&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<br>&gt;<br><br><br><br><br>------------------------------<br><br>Message: 7<br>Date: Wed, 26 Oct 2011 11:11:08 +0200<br>From: Patrick Begou &lt;<a ymailto="mailto:Patrick.Begou@hmg.inpg.fr" href="mailto:Patrick.Begou@hmg.inpg.fr">Patrick.Begou@hmg.inpg.fr</a>&gt;<br>Subject: [OMPI users] Changing
 plm_rsh_agent system wide<br>To: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID: &lt;<a ymailto="mailto:4EA7CEAC.3080800@hmg.inpg.fr" href="mailto:4EA7CEAC.3080800@hmg.inpg.fr">4EA7CEAC.3080800@hmg.inpg.fr</a>&gt;<br>Content-Type: text/plain; charset=ISO-8859-15; format=flowed<br><br>I need to change system wide how OpenMPI launch the jobs on the nodes of my cluster.<br><br>Setting:<br>export OMPI_MCA_plm_rsh_agent=oarsh<br><br>works fine but I would like this config to be the default with OpenMPI. I've <br>read several threads (discussions, FAQ) about this but none of the provided <br>solutions seams to work.<br><br>I have two files:<br>/usr/lib/openmpi/1.4-gcc/etc/openmpi-mca-params.conf<br>/usr/lib64/openmpi/1.4-gcc/etc/openmpi-mca-params.conf<br><br>In these files I've set various flavor of the syntax (only one at a time, and <br>the same in each file of
 course!):<br>test 1) plm_rsh_agent = oarsh<br>test 2) pls_rsh_agent = oarsh<br>test 3) orte_rsh_agent = oarsh<br><br>But each time when I run "ompi_info --param plm rsh" I get:<br>MCA plm: parameter "plm_rsh_agent" (current value: "ssh : rsh", data source: <br>default value, synonyms:<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  pls_rsh_agent)<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  The command used to launch executables on remote nodes <br>(typically either "ssh" or "rsh")<br><br>With the exported variable it works fine.<br>Any suggestion ?<br><br>The rpm package of my linux Rocks Cluster provides:<br>&nbsp; &nbsp; Package: Open MPI root@build-x86-64 Distribution<br>&nbsp; &nbsp; Open MPI: 1.4.3<br>&nbsp; &nbsp; Open MPI SVN revision: r23834<br>&nbsp; &nbsp; Open MPI release date: Oct 05, 2010<br><br>Thanks<br><br>Patrick<br><br><br><br>&nbsp;
 --<br>===============================================================<br>|&nbsp; Equipe M.O.S.T.&nbsp; &nbsp; &nbsp; &nbsp;  | <a href="http://most.hmg.inpg.fr" target="_blank">http://most.hmg.inpg.fr</a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |<br>|&nbsp; Patrick BEGOU&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |&nbsp; &nbsp; &nbsp;  ------------&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |<br>|&nbsp; LEGI&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | mailto:<a ymailto="mailto:Patrick.Begou@hmg.inpg.fr" href="mailto:Patrick.Begou@hmg.inpg.fr">Patrick.Begou@hmg.inpg.fr</a> |<br>|&nbsp; BP 53 X&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  | Tel 04 76 82 51 35&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |<br>|&nbsp; 38041 GRENOBLE CEDEX&nbsp; &nbsp; | Fax 04 76 82 52 71&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
 |<br>===============================================================<br><br><br><br>------------------------------<br><br>Message: 8<br>Date: Wed, 26 Oct 2011 07:20:38 -0500<br>From: Josh Hursey &lt;<a ymailto="mailto:jjhursey@open-mpi.org" href="mailto:jjhursey@open-mpi.org">jjhursey@open-mpi.org</a>&gt;<br>Subject: Re: [OMPI users] Checkpoint from inside MPI program with<br>&nbsp;&nbsp;&nbsp; OpenMPI 1.4.2 ?<br>To: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID:<br>&nbsp;&nbsp;&nbsp; &lt;CAANzjEmx=sO_9mtzVM+<a ymailto="mailto:WiPLWFhPSiM6UxeosxNPgdd8QUZObCw@mail.gmail.com" href="mailto:WiPLWFhPSiM6UxeosxNPgdd8QUZObCw@mail.gmail.com">WiPLWFhPSiM6UxeosxNPgdd8QUZObCw@mail.gmail.com</a>&gt;<br>Content-Type: text/plain; charset=ISO-8859-1<br><br>Since this would be a new feature for 1.4, we cannot move it since the<br>1.4 branch is for bug fixes only. However, we may be
 able to add it to<br>1.5. I filed a ticket if you want to track that progress:<br>&nbsp; <a href="https://svn.open-mpi.org/trac/ompi/ticket/2895" target="_blank">https://svn.open-mpi.org/trac/ompi/ticket/2895</a><br><br>-- Josh<br><br><br>On Tue, Oct 25, 2011 at 11:52 PM, Nguyen Toan &lt;<a ymailto="mailto:nguyentoan1508@gmail.com" href="mailto:nguyentoan1508@gmail.com">nguyentoan1508@gmail.com</a>&gt; wrote:<br>&gt; Dear Josh,<br>&gt; Thank you. I will test the 1.7 trunk as you suggested.<br>&gt; Also I want to ask if we can add this interface to OpenMPI 1.4.2,<br>&gt; because my applications are mainly involved in this version.<br>&gt; Regards,<br>&gt; Nguyen Toan<br>&gt; On Wed, Oct 26, 2011 at 3:25 AM, Josh Hursey &lt;<a ymailto="mailto:jjhursey@open-mpi.org" href="mailto:jjhursey@open-mpi.org">jjhursey@open-mpi.org</a>&gt; wrote:<br>&gt;&gt;<br>&gt;&gt; Open MPI (trunk/1.7 - not 1.4 or 1.5) provides an application level<br>&gt;&gt; interface to
 request a checkpoint of an application. This API is<br>&gt;&gt; defined on the following website:<br>&gt;&gt; ?<a href="http://osl.iu.edu/research/ft/ompi-cr/api.php#api-cr_checkpoint" target="_blank">http://osl.iu.edu/research/ft/ompi-cr/api.php#api-cr_checkpoint</a><br>&gt;&gt;<br>&gt;&gt; This will behave the same as if you requested the checkpoint of the<br>&gt;&gt; job from the command line.<br>&gt;&gt;<br>&gt;&gt; -- Josh<br>&gt;&gt;<br>&gt;&gt; On Mon, Oct 24, 2011 at 12:37 PM, Nguyen Toan &lt;<a ymailto="mailto:nguyentoan1508@gmail.com" href="mailto:nguyentoan1508@gmail.com">nguyentoan1508@gmail.com</a>&gt;<br>&gt;&gt; wrote:<br>&gt;&gt; &gt; Dear all,<br>&gt;&gt; &gt; I want to automatically checkpoint an MPI program with OpenMPI ( I'm<br>&gt;&gt; &gt; currently using 1.4.2 version with BLCR 0.8.2),<br>&gt;&gt; &gt; not by manually typing ompi-checkpoint command line from another<br>&gt;&gt; &gt; terminal.<br>&gt;&gt; &gt; So I would like to
 know if there is a way to call checkpoint function<br>&gt;&gt; &gt; from<br>&gt;&gt; &gt; inside an MPI program<br>&gt;&gt; &gt; with OpenMPI or how to do that.<br>&gt;&gt; &gt; Any ideas are very appreciated.<br>&gt;&gt; &gt; Regards,<br>&gt;&gt; &gt; Nguyen Toan<br>&gt;&gt; &gt; _______________________________________________<br>&gt;&gt; &gt; users mailing list<br>&gt;&gt; &gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;&gt; &gt;<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt; --<br>&gt;&gt; Joshua Hursey<br>&gt;&gt; Postdoctoral Research Associate<br>&gt;&gt; Oak Ridge National Laboratory<br>&gt;&gt; <a href="http://users.nccs.gov/%7Ejjhursey" target="_blank">http://users.nccs.gov/~jjhursey</a><br>&gt;&gt;
 _______________________________________________<br>&gt;&gt; users mailing list<br>&gt;&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<br>&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<br><br><br><br>-- <br>Joshua Hursey<br>Postdoctoral Research Associate<br>Oak Ridge National Laboratory<br><a href="http://users.nccs.gov/%7Ejjhursey" target="_blank">http://users.nccs.gov/~jjhursey</a><br><br><br><br>------------------------------<br><br>Message: 9<br>Date: Wed, 26 Oct
 2011 08:44:45 -0600<br>From: Ralph Castain &lt;<a ymailto="mailto:rhc@open-mpi.org" href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;<br>Subject: Re: [OMPI users] Changing plm_rsh_agent system wide<br>To: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID: &lt;<a ymailto="mailto:F188CF99-9A7A-4327-AF9C-51D578CD54C4@open-mpi.org" href="mailto:F188CF99-9A7A-4327-AF9C-51D578CD54C4@open-mpi.org">F188CF99-9A7A-4327-AF9C-51D578CD54C4@open-mpi.org</a>&gt;<br>Content-Type: text/plain; charset=us-ascii<br><br>Did the version you are running get installed in /usr? Sounds like you are picking up a different version when running a command - i.e., that your PATH is finding a different installation than the one in /usr.<br><br><br>On Oct 26, 2011, at 3:11 AM, Patrick Begou wrote:<br><br>&gt; I need to change system wide how OpenMPI launch the jobs on the nodes of my
 cluster.<br>&gt; <br>&gt; Setting:<br>&gt; export OMPI_MCA_plm_rsh_agent=oarsh<br>&gt; <br>&gt; works fine but I would like this config to be the default with OpenMPI. I've read several threads (discussions, FAQ) about this but none of the provided solutions seams to work.<br>&gt; <br>&gt; I have two files:<br>&gt; /usr/lib/openmpi/1.4-gcc/etc/openmpi-mca-params.conf<br>&gt; /usr/lib64/openmpi/1.4-gcc/etc/openmpi-mca-params.conf<br>&gt; <br>&gt; In these files I've set various flavor of the syntax (only one at a time, and the same in each file of course!):<br>&gt; test 1) plm_rsh_agent = oarsh<br>&gt; test 2) pls_rsh_agent = oarsh<br>&gt; test 3) orte_rsh_agent = oarsh<br>&gt; <br>&gt; But each time when I run "ompi_info --param plm rsh" I get:<br>&gt; MCA plm: parameter "plm_rsh_agent" (current value: "ssh : rsh", data source: default value, synonyms:<br>&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; pls_rsh_agent)<br>&gt;&nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; The command used to launch executables on remote nodes (typically either "ssh" or "rsh")<br>&gt; <br>&gt; With the exported variable it works fine.<br>&gt; Any suggestion ?<br>&gt; <br>&gt; The rpm package of my linux Rocks Cluster provides:<br>&gt;&nbsp;  Package: Open MPI root@build-x86-64 Distribution<br>&gt;&nbsp;  Open MPI: 1.4.3<br>&gt;&nbsp;  Open MPI SVN revision: r23834<br>&gt;&nbsp;  Open MPI release date: Oct 05, 2010<br>&gt; <br>&gt; Thanks<br>&gt; <br>&gt; Patrick<br>&gt; <br>&gt; <br>&gt; <br>&gt; --<br>&gt; ===============================================================<br>&gt; |&nbsp; Equipe M.O.S.T.&nbsp; &nbsp; &nbsp; &nbsp;  | <a href="http://most.hmg.inpg.fr" target="_blank">http://most.hmg.inpg.fr</a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |<br>&gt; |&nbsp; Patrick BEGOU&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |&nbsp; &nbsp; &nbsp;  ------------&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp;  |<br>&gt; |&nbsp; LEGI&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | mailto:<a ymailto="mailto:Patrick.Begou@hmg.inpg.fr" href="mailto:Patrick.Begou@hmg.inpg.fr">Patrick.Begou@hmg.inpg.fr</a> |<br>&gt; |&nbsp; BP 53 X&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  | Tel 04 76 82 51 35&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |<br>&gt; |&nbsp; 38041 GRENOBLE CEDEX&nbsp; &nbsp; | Fax 04 76 82 52 71&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |<br>&gt; ===============================================================<br>&gt; <br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
 target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><br><br><br>------------------------------<br><br>Message: 10<br>Date: Wed, 26 Oct 2011 10:49:38 -0400<br>From: TERRY DONTJE &lt;<a ymailto="mailto:terry.dontje@oracle.com" href="mailto:terry.dontje@oracle.com">terry.dontje@oracle.com</a>&gt;<br>Subject: Re: [OMPI users] Changing plm_rsh_agent system wide<br>To: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID: &lt;<a ymailto="mailto:4EA81E02.6080609@oracle.com" href="mailto:4EA81E02.6080609@oracle.com">4EA81E02.6080609@oracle.com</a>&gt;<br>Content-Type: text/plain; charset="iso-8859-1"; Format="flowed"<br><br>I am using prefix configuration so no it does not exist in /usr.<br><br>--td<br><br>On 10/26/2011 10:44 AM, Ralph Castain wrote:<br>&gt; Did the version you are running get installed in /usr? Sounds like you are picking up a different
 version when running a command - i.e., that your PATH is finding a different installation than the one in /usr.<br>&gt;<br>&gt;<br>&gt; On Oct 26, 2011, at 3:11 AM, Patrick Begou wrote:<br>&gt;<br>&gt;&gt; I need to change system wide how OpenMPI launch the jobs on the nodes of my cluster.<br>&gt;&gt;<br>&gt;&gt; Setting:<br>&gt;&gt; export OMPI_MCA_plm_rsh_agent=oarsh<br>&gt;&gt;<br>&gt;&gt; works fine but I would like this config to be the default with OpenMPI. I've read several threads (discussions, FAQ) about this but none of the provided solutions seams to work.<br>&gt;&gt;<br>&gt;&gt; I have two files:<br>&gt;&gt; /usr/lib/openmpi/1.4-gcc/etc/openmpi-mca-params.conf<br>&gt;&gt; /usr/lib64/openmpi/1.4-gcc/etc/openmpi-mca-params.conf<br>&gt;&gt;<br>&gt;&gt; In these files I've set various flavor of the syntax (only one at a time, and the same in each file of course!):<br>&gt;&gt; test 1) plm_rsh_agent = oarsh<br>&gt;&gt; test 2) pls_rsh_agent =
 oarsh<br>&gt;&gt; test 3) orte_rsh_agent = oarsh<br>&gt;&gt;<br>&gt;&gt; But each time when I run "ompi_info --param plm rsh" I get:<br>&gt;&gt; MCA plm: parameter "plm_rsh_agent" (current value: "ssh : rsh", data source: default value, synonyms:<br>&gt;&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  pls_rsh_agent)<br>&gt;&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  The command used to launch executables on remote nodes (typically either "ssh" or "rsh")<br>&gt;&gt;<br>&gt;&gt; With the exported variable it works fine.<br>&gt;&gt; Any suggestion ?<br>&gt;&gt;<br>&gt;&gt; The rpm package of my linux Rocks Cluster provides:<br>&gt;&gt;&nbsp; &nbsp; Package: Open MPI root@build-x86-64 Distribution<br>&gt;&gt;&nbsp; &nbsp; Open MPI: 1.4.3<br>&gt;&gt;&nbsp; &nbsp; Open MPI SVN revision: r23834<br>&gt;&gt;&nbsp; &nbsp; Open MPI release date: Oct 05, 2010<br>&gt;&gt;<br>&gt;&gt; Thanks<br>&gt;&gt;<br>&gt;&gt;
 Patrick<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt; --<br>&gt;&gt; ===============================================================<br>&gt;&gt; |&nbsp; Equipe M.O.S.T.&nbsp; &nbsp; &nbsp; &nbsp;  | <a href="http://most.hmg.inpg.fr" target="_blank">http://most.hmg.inpg.fr</a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |<br>&gt;&gt; |&nbsp; Patrick BEGOU&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |&nbsp; &nbsp; &nbsp;  ------------&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |<br>&gt;&gt; |&nbsp; LEGI&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | mailto:<a ymailto="mailto:Patrick.Begou@hmg.inpg.fr" href="mailto:Patrick.Begou@hmg.inpg.fr">Patrick.Begou@hmg.inpg.fr</a> |<br>&gt;&gt; |&nbsp; BP 53 X&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  | Tel 04 76 82 51 35&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |<br>&gt;&gt; |&nbsp; 38041 GRENOBLE CEDEX&nbsp; &nbsp; | Fax 04 76 82 52 71&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp;  |<br>&gt;&gt; ===============================================================<br>&gt;&gt;<br>&gt;&gt; _______________________________________________<br>&gt;&gt; users mailing list<br>&gt;&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br>-- <br>Oracle<br>Terry D. Dontje | Principal Software Engineer<br>Developer Tools Engineering | +1.781.442.2631<br>Oracle *- Performance Technologies*<br>95 Network Drive, Burlington, MA
 01803<br>Email <a ymailto="mailto:terry.dontje@oracle.com" href="mailto:terry.dontje@oracle.com">terry.dontje@oracle.com</a> &lt;mailto:<a ymailto="mailto:terry.dontje@oracle.com" href="mailto:terry.dontje@oracle.com">terry.dontje@oracle.com</a>&gt;<br><br><br><br>-------------- next part --------------<br>HTML attachment scrubbed and removed<br>-------------- next part --------------<br>A non-text attachment was scrubbed...<br>Name: not available<br>Type: image/gif<br>Size: 2059 bytes<br>Desc: not available<br>URL: &lt;<a href="http://www.open-mpi.org/MailArchives/users/attachments/20111026/2e811d83/attachment.gif" target="_blank">http://www.open-mpi.org/MailArchives/users/attachments/20111026/2e811d83/attachment.gif</a>&gt;<br><br>------------------------------<br><br>Message: 11<br>Date: Wed, 26 Oct 2011 10:51:06 -0400<br>From: TERRY DONTJE &lt;<a ymailto="mailto:terry.dontje@oracle.com"
 href="mailto:terry.dontje@oracle.com">terry.dontje@oracle.com</a>&gt;<br>Subject: Re: [OMPI users] Changing plm_rsh_agent system wide<br>To: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID: &lt;<a ymailto="mailto:4EA81E5A.3030606@oracle.com" href="mailto:4EA81E5A.3030606@oracle.com">4EA81E5A.3030606@oracle.com</a>&gt;<br>Content-Type: text/plain; charset="iso-8859-1"; Format="flowed"<br><br>Sorry please disregard my reply to this email.<br><br>:-)<br><br>--td<br><br>On 10/26/2011 10:44 AM, Ralph Castain wrote:<br>&gt; Did the version you are running get installed in /usr? Sounds like you are picking up a different version when running a command - i.e., that your PATH is finding a different installation than the one in /usr.<br>&gt;<br>&gt;<br>&gt; On Oct 26, 2011, at 3:11 AM, Patrick Begou wrote:<br>&gt;<br>&gt;&gt; I need to change system wide how OpenMPI launch the
 jobs on the nodes of my cluster.<br>&gt;&gt;<br>&gt;&gt; Setting:<br>&gt;&gt; export OMPI_MCA_plm_rsh_agent=oarsh<br>&gt;&gt;<br>&gt;&gt; works fine but I would like this config to be the default with OpenMPI. I've read several threads (discussions, FAQ) about this but none of the provided solutions seams to work.<br>&gt;&gt;<br>&gt;&gt; I have two files:<br>&gt;&gt; /usr/lib/openmpi/1.4-gcc/etc/openmpi-mca-params.conf<br>&gt;&gt; /usr/lib64/openmpi/1.4-gcc/etc/openmpi-mca-params.conf<br>&gt;&gt;<br>&gt;&gt; In these files I've set various flavor of the syntax (only one at a time, and the same in each file of course!):<br>&gt;&gt; test 1) plm_rsh_agent = oarsh<br>&gt;&gt; test 2) pls_rsh_agent = oarsh<br>&gt;&gt; test 3) orte_rsh_agent = oarsh<br>&gt;&gt;<br>&gt;&gt; But each time when I run "ompi_info --param plm rsh" I get:<br>&gt;&gt; MCA plm: parameter "plm_rsh_agent" (current value: "ssh : rsh", data source: default value,
 synonyms:<br>&gt;&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  pls_rsh_agent)<br>&gt;&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  The command used to launch executables on remote nodes (typically either "ssh" or "rsh")<br>&gt;&gt;<br>&gt;&gt; With the exported variable it works fine.<br>&gt;&gt; Any suggestion ?<br>&gt;&gt;<br>&gt;&gt; The rpm package of my linux Rocks Cluster provides:<br>&gt;&gt;&nbsp; &nbsp; Package: Open MPI root@build-x86-64 Distribution<br>&gt;&gt;&nbsp; &nbsp; Open MPI: 1.4.3<br>&gt;&gt;&nbsp; &nbsp; Open MPI SVN revision: r23834<br>&gt;&gt;&nbsp; &nbsp; Open MPI release date: Oct 05, 2010<br>&gt;&gt;<br>&gt;&gt; Thanks<br>&gt;&gt;<br>&gt;&gt; Patrick<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt; --<br>&gt;&gt; ===============================================================<br>&gt;&gt; |&nbsp; Equipe M.O.S.T.&nbsp; &nbsp; &nbsp; &nbsp;  | <a href="http://most.hmg.inpg.fr"
 target="_blank">http://most.hmg.inpg.fr</a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |<br>&gt;&gt; |&nbsp; Patrick BEGOU&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |&nbsp; &nbsp; &nbsp;  ------------&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |<br>&gt;&gt; |&nbsp; LEGI&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | mailto:<a ymailto="mailto:Patrick.Begou@hmg.inpg.fr" href="mailto:Patrick.Begou@hmg.inpg.fr">Patrick.Begou@hmg.inpg.fr</a> |<br>&gt;&gt; |&nbsp; BP 53 X&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  | Tel 04 76 82 51 35&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |<br>&gt;&gt; |&nbsp; 38041 GRENOBLE CEDEX&nbsp; &nbsp; | Fax 04 76 82 52 71&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |<br>&gt;&gt; ===============================================================<br>&gt;&gt;<br>&gt;&gt; _______________________________________________<br>&gt;&gt; users mailing list<br>&gt;&gt; <a
 ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br>-- <br>Oracle<br>Terry D. Dontje | Principal Software Engineer<br>Developer Tools Engineering | +1.781.442.2631<br>Oracle *- Performance Technologies*<br>95 Network Drive, Burlington, MA 01803<br>Email <a ymailto="mailto:terry.dontje@oracle.com" href="mailto:terry.dontje@oracle.com">terry.dontje@oracle.com</a> &lt;mailto:<a ymailto="mailto:terry.dontje@oracle.com"
 href="mailto:terry.dontje@oracle.com">terry.dontje@oracle.com</a>&gt;<br><br><br><br>-------------- next part --------------<br>HTML attachment scrubbed and removed<br>-------------- next part --------------<br>A non-text attachment was scrubbed...<br>Name: not available<br>Type: image/gif<br>Size: 2059 bytes<br>Desc: not available<br>URL: &lt;<a href="http://www.open-mpi.org/MailArchives/users/attachments/20111026/5d399085/attachment.gif" target="_blank">http://www.open-mpi.org/MailArchives/users/attachments/20111026/5d399085/attachment.gif</a>&gt;<br><br>------------------------------<br><br>Message: 12<br>Date: Wed, 26 Oct 2011 17:57:54 +0200<br>From: Patrick Begou &lt;<a ymailto="mailto:Patrick.Begou@hmg.inpg.fr" href="mailto:Patrick.Begou@hmg.inpg.fr">Patrick.Begou@hmg.inpg.fr</a>&gt;<br>Subject: Re: [OMPI users] Changing plm_rsh_agent system wide<br>To: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org"
 href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID: &lt;<a ymailto="mailto:4EA82E02.9020107@hmg.inpg.fr" href="mailto:4EA82E02.9020107@hmg.inpg.fr">4EA82E02.9020107@hmg.inpg.fr</a>&gt;<br>Content-Type: text/plain; charset=ISO-8859-15; format=flowed<br><br>Ralph Castain a ?crit :<br>&gt; Did the version you are running get installed in /usr? Sounds like you are picking up a different version when running a command - i.e., that your PATH is finding a different installation than the one in /usr.<br><br>Right! I'm using OpenMPI with Rocks Cluster distribution. There is:<br>&nbsp; openmpi-1.4-4.el5 rpm installed with<br>/usr/lib*/openmpi/1.4-gcc/etc/openmpi-mca-params.conf<br><br>but there is also&nbsp; rocks-openmpi-1.4.3-1 with<br>/opt/openmpi/etc/openmpi-mca-params.conf<br><br>I never notice this double default install of OpenMPI in this linux distribution.<br>Thanks a lot for the suggestion, I was fixed on a syntax error in my
 config...<br><br>Patrick<br>&gt;<br>&gt;<br>&gt; On Oct 26, 2011, at 3:11 AM, Patrick Begou wrote:<br>&gt;<br>&gt;&gt; I need to change system wide how OpenMPI launch the jobs on the nodes of my cluster.<br>&gt;&gt;<br>&gt;&gt; Setting:<br>&gt;&gt; export OMPI_MCA_plm_rsh_agent=oarsh<br>&gt;&gt;<br>&gt;&gt; works fine but I would like this config to be the default with OpenMPI. I've read several threads (discussions, FAQ) about this but none of the provided solutions seams to work.<br>&gt;&gt;<br>&gt;&gt; I have two files:<br>&gt;&gt; /usr/lib/openmpi/1.4-gcc/etc/openmpi-mca-params.conf<br>&gt;&gt; /usr/lib64/openmpi/1.4-gcc/etc/openmpi-mca-params.conf<br>&gt;&gt;<br>&gt;&gt; In these files I've set various flavor of the syntax (only one at a time, and the same in each file of course!):<br>&gt;&gt; test 1) plm_rsh_agent = oarsh<br>&gt;&gt; test 2) pls_rsh_agent = oarsh<br>&gt;&gt; test 3) orte_rsh_agent = oarsh<br>&gt;&gt;<br>&gt;&gt; But each time when
 I run "ompi_info --param plm rsh" I get:<br>&gt;&gt; MCA plm: parameter "plm_rsh_agent" (current value: "ssh : rsh", data source: default value, synonyms:<br>&gt;&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  pls_rsh_agent)<br>&gt;&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  The command used to launch executables on remote nodes (typically either "ssh" or "rsh")<br>&gt;&gt;<br>&gt;&gt; With the exported variable it works fine.<br>&gt;&gt; Any suggestion ?<br>&gt;&gt;<br>&gt;&gt; The rpm package of my linux Rocks Cluster provides:<br>&gt;&gt;&nbsp; &nbsp; Package: Open MPI root@build-x86-64 Distribution<br>&gt;&gt;&nbsp; &nbsp; Open MPI: 1.4.3<br>&gt;&gt;&nbsp; &nbsp; Open MPI SVN revision: r23834<br>&gt;&gt;&nbsp; &nbsp; Open MPI release date: Oct 05, 2010<br>&gt;&gt;<br>&gt;&gt; Thanks<br>&gt;&gt;<br>&gt;&gt; Patrick<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt; --<br>&gt;&gt;
 ===============================================================<br>&gt;&gt; |&nbsp; Equipe M.O.S.T.&nbsp; &nbsp; &nbsp; &nbsp;  | <a href="http://most.hmg.inpg.fr" target="_blank">http://most.hmg.inpg.fr</a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |<br>&gt;&gt; |&nbsp; Patrick BEGOU&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |&nbsp; &nbsp; &nbsp;  ------------&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |<br>&gt;&gt; |&nbsp; LEGI&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | mailto:<a ymailto="mailto:Patrick.Begou@hmg.inpg.fr" href="mailto:Patrick.Begou@hmg.inpg.fr">Patrick.Begou@hmg.inpg.fr</a> |<br>&gt;&gt; |&nbsp; BP 53 X&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  | Tel 04 76 82 51 35&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |<br>&gt;&gt; |&nbsp; 38041 GRENOBLE CEDEX&nbsp; &nbsp; | Fax 04 76 82 52 71&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |<br>&gt;&gt;
 ===============================================================<br>&gt;&gt;<br>&gt;&gt; _______________________________________________<br>&gt;&gt; users mailing list<br>&gt;&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<br><br><br>-- <br>===============================================================<br>|&nbsp; Equipe M.O.S.T.&nbsp; &nbsp; &nbsp; &nbsp;  | <a href="http://most.hmg.inpg.fr"
 target="_blank">http://most.hmg.inpg.fr</a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |<br>|&nbsp; Patrick BEGOU&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |&nbsp; &nbsp; &nbsp;  ------------&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |<br>|&nbsp; LEGI&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | mailto:<a ymailto="mailto:Patrick.Begou@hmg.inpg.fr" href="mailto:Patrick.Begou@hmg.inpg.fr">Patrick.Begou@hmg.inpg.fr</a> |<br>|&nbsp; BP 53 X&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  | Tel 04 76 82 51 35&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |<br>|&nbsp; 38041 GRENOBLE CEDEX&nbsp; &nbsp; | Fax 04 76 82 52 71&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |<br>===============================================================<br><br><br><br><br><br>------------------------------<br><br>_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org"
 href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br>End of users Digest, Vol 2052, Issue 1<br>**************************************<br><br><br></div></div></div></body></html>
