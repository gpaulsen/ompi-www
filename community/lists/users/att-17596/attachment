<html><body><div style="color:#000; background-color:#fff; font-family:Courier New, courier, monaco, monospace, sans-serif;font-size:10pt"><div id="yiv1423064362"><div><div style="color: rgb(0, 0, 0); background-color: rgb(255, 255, 255); font-size: 10pt; font-family: 'Courier New', courier, monaco, monospace, sans-serif; "><div id="yiv1423064362yui_3_2_0_15_131946841524948"><span id="yiv1423064362yui_3_2_0_15_131946841524983">Hello Community,</span></div><div id="yiv1423064362yui_3_2_0_15_131946841524994"><br><span id="yiv1423064362yui_3_2_0_15_131946841524983"></span></div><div id="yiv1423064362yui_3_2_0_15_131946841524995"><span id="yiv1423064362yui_3_2_0_15_131946841524983">I have been struggling with visual debugging on cluster machines. So far, I tried to work around the problem, or total avoid it, but no more.</span></div><div id="yiv1423064362yui_3_2_0_15_1319468415249122"><br><span id="yiv1423064362yui_3_2_0_15_131946841524983"></span></div><div
 id="yiv1423064362yui_3_2_0_15_1319468415249123"><span id="yiv1423064362yui_3_2_0_15_131946841524983">I have three machines on the cluster: a.s1.s2, </span><span id="yiv1423064362yui_3_2_0_15_131946841524983">b.s1.s2 and </span><span id="yiv1423064362yui_3_2_0_15_131946841524983">c.s1.s2. I do not have admin privileges on any of these
 machines.</span></div><div id="yiv1423064362yui_3_2_0_15_1319468415249230"><br><span id="yiv1423064362yui_3_2_0_15_131946841524983"></span></div><div id="yiv1423064362yui_3_2_0_15_1319468415249231"><span id="yiv1423064362yui_3_2_0_15_131946841524983">Now, I want to run a visual debugger on all of these machines, and have the windows come up. <br></span></div><div id="yiv1423064362yui_3_2_0_15_1319468415249268"><br><span id="yiv1423064362yui_3_2_0_15_131946841524983"></span></div><div id="yiv1423064362yui_3_2_0_15_1319468415249269"><span id="yiv1423064362yui_3_2_0_15_131946841524983">So for from: </span>(<a rel="nofollow" target="_blank" href="http://www.open-mpi.org/faq/?category=running">http://www.open-mpi.org/faq/? category=running</a>)</div><div id="yiv1423064362yui_3_2_0_15_1319468415249278"><br><span id="yiv1423064362yui_3_2_0_15_131946841524983"></span></div><div id="yiv1423064362yui_3_2_0_15_1319468415249279">13. Can I run GUI applications with
 Open MPI?
<span style="color:rgb(51, 51, 255);">
</span>

</div><div style="color:rgb(51, 51, 255);">Yes, but it will depend on your local setup and may require
additional setup.

</div><div style="color:rgb(51, 51, 255);">In short: you will need to have X forwarding enabled from the remote
processes to the display where you want output to appear.  In a secure
environment, you can simply allow all X requests to be shown on the
target display and set the <code>DISPLAY</code> environment variable in all MPI
process' environments to the target display, perhaps something like
this:



</div><table style="color:rgb(51, 51, 255);" border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody><tr>
<td><pre>shell$ hostname
<a rel="nofollow" target="_blank" href="http://my_desktop.secure-cluster.example.com/">my_desktop.secure-cluster. example.com</a>
shell$ xhost +
shell$ mpirun -np 4 -x DISPLAY=<a rel="nofollow" target="_blank" href="http://my_desktop.secure-cluster.example.com/">my_desktop.secure- cluster.example.com</a> a.out
</pre></td>
</tr>
</tbody></table><span style="color:rgb(51, 51, 255);">



</span><div style="color:rgb(51, 51, 255);">However, this technique is not generally suitable for unsecure
environments (because it allows anyone to read and write to your
display).  A slightly more secure way is to only allow X connections
from the nodes where your application will be running:



</div><table id="yiv1423064362yui_3_2_0_15_1319468415249420" style="color:rgb(51, 51, 255);" border="0" cellpadding="5" cellspacing="0" width="100%">
<tbody id="yiv1423064362yui_3_2_0_15_1319468415249419"><tr id="yiv1423064362yui_3_2_0_15_1319468415249418">
<td id="yiv1423064362yui_3_2_0_15_1319468415249417"><pre id="yiv1423064362yui_3_2_0_15_1319468415249416">shell$ hostname
<a rel="nofollow" target="_blank" href="http://my_desktop.secure-cluster.example.com/">my_desktop.secure-cluster. example.com</a>
shell$ xhost +compute1 +compute2 +compute3 +compute4
compute1 being added to access control list
compute2 being added to access control list
compute3 being added to access control list
compute4 being added to access control list
shell$ mpirun -np 4 -x DISPLAY=<a rel="nofollow" target="_blank" href="http://my_desktop.secure-cluster.example.com/">my_desktop.secure- cluster.example.com</a> a.out
</pre></td>
</tr>
</tbody></table><span style="color:rgb(51, 51, 255);">



</span><div style="color:rgb(51, 51, 255);">(assuming that the four nodes you are running on are <code>compute1</code>
through <code>compute4</code>).

</div><div style="color:rgb(51, 51, 255);">Other methods are available, but they involve sophisticated X
forwarding through mpirun and are generally more complicated than
desirable.</div><br>This still gives me "Error: Can't open display:" problem. <br><br>My mpirun shell script contains:<br><br>mpirun-1.4.3 -hostfile hostfile -np 3 -v -nooversubscribe --rankfile rankfile.txt --report-bindings&nbsp; -timestamp-output ./testdisplay-window.sh <br><br><br>where rankfile and hostfile contain <span id="yiv1423064362yui_3_2_0_15_131946841524983">a.s1.s2, </span><span id="yiv1423064362yui_3_2_0_15_131946841524983">b.s1.s2 and </span><span id="yiv1423064362yui_3_2_0_15_131946841524983">c.s1.s2, and are proper.<br><br>The file </span>./testdisplay-window.sh:<br><br><span id="yiv1423064362yui_3_2_0_15_1319468415249437" style="color:rgb(0, 0, 255);">#!/bin/bash</span><br style="color:rgb(0, 0, 255);"><span id="yiv1423064362yui_3_2_0_15_1319468415249444" style="color:rgb(0, 0, 255);">echo "Running xeyes on `hostname`"</span><br style="color:rgb(0, 0, 255);"><span id="yiv1423064362yui_3_2_0_15_1319468415249485" style="color:rgb(0, 0,
 255);">DISPLAY=</span><span style="color:rgb(0, 0, 255);" id="yiv1423064362yui_3_2_0_15_131946841524983">a.s1.s2</span><span style="color:rgb(0, 0, 255);">:11.0</span><br style="color:rgb(0, 0, 255);"><span style="color:rgb(0, 0, 255);">xeyes</span><br style="color:rgb(0, 0, 255);"><span style="color:rgb(0, 0, 255);">exit 0</span><br><span id="yiv1423064362yui_3_2_0_15_131946841524983"><br>I see that my xauth list output already contains entries like:<br><br></span><span id="yiv1423064362yui_3_2_0_15_1319468415249485" style="color:rgb(0, 0, 255);"></span><span style="color:rgb(0, 0, 255);" id="yiv1423064362yui_3_2_0_15_131946841524983">a.s1.s2</span><span id="yiv1423064362yui_3_2_0_15_131946841524983">/unix:12&nbsp; MIT-MAGIC-COOKIE-1&nbsp; aa16a9573f42224d760c7bb618b48a6f<br></span><span id="yiv1423064362yui_3_2_0_15_1319468415249485" style="color:rgb(0, 0, 255);"></span><span style="color:rgb(0, 0, 255);"
 id="yiv1423064362yui_3_2_0_15_131946841524983">a.s1.s2</span><span id="yiv1423064362yui_3_2_0_15_131946841524983">/unix:10&nbsp; MIT-MAGIC-COOKIE-1&nbsp;
 0fb6fe3c2e35676136c8642412fb5809<br></span><span id="yiv1423064362yui_3_2_0_15_1319468415249485" style="color:rgb(0, 0, 255);"></span><span style="color:rgb(0, 0, 255);" id="yiv1423064362yui_3_2_0_15_131946841524983">a.s1.s2</span><span id="yiv1423064362yui_3_2_0_15_131946841524983">/unix:11&nbsp; MIT-MAGIC-COOKIE-1&nbsp; a3a65970b5f545bc750e3520a4e3b872<br><br><br>I seem to have run out of ideas now.<br><br>However, this works prefectly on any of the machines </span><span style="color:rgb(0, 0, 255);" id="yiv1423064362yui_3_2_0_15_131946841524983">a.s1.s2</span><span id="yiv1423064362yui_3_2_0_15_131946841524983">, </span><span style="color:rgb(0, 0, 255);" id="yiv1423064362yui_3_2_0_15_131946841524983">b.s1.s2</span><span id="yiv1423064362yui_3_2_0_15_131946841524983"> or </span><span style="color:rgb(0, 0, 255);" id="yiv1423064362yui_3_2_0_15_131946841524983">c.s1.s2</span><span id="yiv1423064362yui_3_2_0_15_131946841524983">:</span><br><br>(for
 example, running from a.s1.s2):<br><br>ssh b.s1.s2 xeyes<br><br>Can someone help?<br><br><br>Best<br><span id="yiv1423064362yui_3_2_0_15_131946841524983"><br>Devendra Rai<br><br><br></span><br><div><br id="yiv1423064362yui_3_2_0_15_131946841524951"></div><div class="yiv1423064362yui_3_2_0_15_131946841524952" id="yiv1423064362yui_3_2_0_15_131946841524954" style="font-size: 10pt; font-family: 'Courier New', courier, monaco, monospace, sans-serif; "><div id="yiv1423064362yui_3_2_0_15_131946841524976" class="yiv1423064362yui_3_2_0_15_131946841524957" style="font-size: 12pt; font-family: 'times new roman', 'new york', times, serif; "><font id="yiv1423064362yui_3_2_0_15_131946841524975" face="Arial" size="2"><hr id="yiv1423064362yui_3_2_0_15_131946841524982" size="1"><b><span style="font-weight:bold;">From:</span></b> Jeff Squyres &lt;jsquyres@cisco.com&gt;<br><b><span style="font-weight:bold;">To:</span></b> devendra rai &lt;rai.devendra@yahoo.co.uk&gt;;
 Open MPI Users &lt;users@open-mpi.org&gt;<br><b><span style="font-weight:bold;">Sent:</span></b> Friday, 21 October 2011, 13:14<br><b><span style="font-weight:bold;">Subject:</span></b> Re: [OMPI users]
 orte_grpcomm_modex failed<br></font><br>This usually means that you have a Open MPI version mismatch between some of your nodes.&nbsp; Meaning: on some nodes, you're finding version X.Y.Z of Open MPI by default, but on other nodes, you're finding version A.B.C.<br><br><br>On Oct 21, 2011, at 7:00 AM, devendra rai wrote:<br><br>&gt; Hello Community,<br>&gt; <br>&gt; I have been struggling with this error for quite some time:<br>&gt; <br>&gt; It looks like MPI_INIT failed for some reason; your parallel process is<br>&gt; likely to abort.&nbsp; There are many reasons that a parallel process can<br>&gt; fail during MPI_INIT; some of which are due to configuration or environment<br>&gt; problems.&nbsp; This failure appears to be an internal failure; here's some<br>&gt; additional information (which may only be relevant to an Open MPI<br>&gt; developer):<br>&gt; <br>&gt;&nbsp;  orte_grpcomm_modex failed<br>&gt;&nbsp;  --&gt; Returned "Data unpack would read
 past end of buffer" (-26) instead of "Success" (0)<br>&gt; --------------------------------------------------------------------------<br>&gt; --------------------------------------------------------------------------<br>&gt; mpirun has exited due to process rank 1 with PID 18945 on<br>&gt; node tik35x.ethz.ch exiting without calling "finalize". This may<br>&gt; have caused other processes in the application to be<br>&gt; terminated by signals sent by mpirun (as reported here).<br>&gt; <br>&gt; I am running this on a cluster and this has started happening only after a recent rebuild of openmpi-1.4.3. Interestingly, I have the same version of openmpi on my PC, and the same application works fine.<br>&gt; <br>&gt; I have looked into this error on the web, but there is very little discussion, on the causes, or how to correct it. I asked the admin to attempt a re-install of openmpi, but I am not sure whether this will solve the problem.<br>&gt; <br>&gt; Can
 some one please help?<br>&gt; <br>&gt; Thanks a lot.<br>&gt; <br>&gt; Best,<br>&gt; <br>&gt; Devendra Rai<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a rel="nofollow" ymailto="mailto:users@open-mpi.org" target="_blank" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a rel="nofollow" target="_blank" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><br>-- <br>Jeff Squyres<br><a rel="nofollow" ymailto="mailto:jsquyres@cisco.com" target="_blank" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>For corporate legal information go to:<br><a rel="nofollow" target="_blank" href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br><br><br><br></div></div></div></div></div></div></body></html>
