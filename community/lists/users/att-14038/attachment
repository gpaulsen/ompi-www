The value of hdr-&gt;tag seems wrong.<br><br>In ompi/mca/pml/ob1/pml_ob1_hdr.h<br>#define MCA_PML_OB1_HDR_TYPE_MATCH     (MCA_BTL_TAG_PML + 1)<br>#define MCA_PML_OB1_HDR_TYPE_RNDV      (MCA_BTL_TAG_PML + 2)<br>#define MCA_PML_OB1_HDR_TYPE_RGET      (MCA_BTL_TAG_PML + 3)<br>
#define MCA_PML_OB1_HDR_TYPE_ACK       (MCA_BTL_TAG_PML + 4)<br>#define MCA_PML_OB1_HDR_TYPE_NACK      (MCA_BTL_TAG_PML + 5)<br>#define MCA_PML_OB1_HDR_TYPE_FRAG      (MCA_BTL_TAG_PML + 6)<br>#define MCA_PML_OB1_HDR_TYPE_GET       (MCA_BTL_TAG_PML + 7)<br>
#define MCA_PML_OB1_HDR_TYPE_PUT       (MCA_BTL_TAG_PML + 8)<br>#define MCA_PML_OB1_HDR_TYPE_FIN       (MCA_BTL_TAG_PML + 9)<br><br>and in ompi/mca/btl/btl.h <br>#define MCA_BTL_TAG_PML             0x40<br><br>So hdr-&gt;tag should be a value &gt;= 65<br>
Since the tag is incorrect you are not getting the proper callback function pointer and hence the SEGV.<br>I&#39;m not sure at this point as to why you are getting an invalid/corrupt message header ? <br><br>--Nysal<br><br>
<div class="gmail_quote">On Tue, Aug 10, 2010 at 7:45 PM, Eloi Gaudry <span dir="ltr">&lt;<a href="mailto:eg@fft.be">eg@fft.be</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">
Hi,<br>
<br>
sorry, i just forgot to add the values of the function parameters:<br>
(gdb) print reg-&gt;cbdata<br>
$1 = (void *) 0x0<br>
(gdb) print openib_btl-&gt;super<br>
$2 = {btl_component = 0x2b341edd7380, btl_eager_limit = 12288, btl_rndv_eager_limit = 12288, btl_max_send_size = 65536, btl_rdma_pipeline_send_length = 1048576,<br>
  btl_rdma_pipeline_frag_size = 1048576, btl_min_rdma_pipeline_size = 1060864, btl_exclusivity = 1024, btl_latency = 10, btl_bandwidth = 800, btl_flags = 310,<br>
  btl_add_procs = 0x2b341eb8ee47 &lt;mca_btl_openib_add_procs&gt;, btl_del_procs = 0x2b341eb90156 &lt;mca_btl_openib_del_procs&gt;, btl_register = 0, btl_finalize = 0x2b341eb93186 &lt;mca_btl_openib_finalize&gt;,<br>
  btl_alloc = 0x2b341eb90a3e &lt;mca_btl_openib_alloc&gt;, btl_free = 0x2b341eb91400 &lt;mca_btl_openib_free&gt;, btl_prepare_src = 0x2b341eb91813 &lt;mca_btl_openib_prepare_src&gt;,<br>
  btl_prepare_dst = 0x2b341eb91f2e &lt;mca_btl_openib_prepare_dst&gt;, btl_send = 0x2b341eb94517 &lt;mca_btl_openib_send&gt;, btl_sendi = 0x2b341eb9340d &lt;mca_btl_openib_sendi&gt;,<br>
  btl_put = 0x2b341eb94660 &lt;mca_btl_openib_put&gt;, btl_get = 0x2b341eb94c4e &lt;mca_btl_openib_get&gt;, btl_dump = 0x2b341acd45cb &lt;mca_btl_base_dump&gt;, btl_mpool = 0xf3f4110,<br>
  btl_register_error = 0x2b341eb90565 &lt;mca_btl_openib_register_error_cb&gt;, btl_ft_event = 0x2b341eb952e7 &lt;mca_btl_openib_ft_event&gt;}<br>
(gdb) print hdr-&gt;tag<br>
$3 = 0 &#39;\0&#39;<br>
(gdb) print des<br>
$4 = (mca_btl_base_descriptor_t *) 0xf4a6700<br>
(gdb) print reg-&gt;cbfunc<br>
$5 = (mca_btl_base_module_recv_cb_fn_t) 0<br>
<br>
Eloi<br>
<div><div></div><div class="h5"><br>
On Tuesday 10 August 2010 16:04:08 Eloi Gaudry wrote:<br>
&gt; Hi,<br>
&gt;<br>
&gt; Here is the output of a core file generated during a segmentation fault<br>
&gt; observed during a collective call (using openib):<br>
&gt;<br>
&gt; #0  0x0000000000000000 in ?? ()<br>
&gt; (gdb) where<br>
&gt; #0  0x0000000000000000 in ?? ()<br>
&gt; #1  0x00002aedbc4e05f4 in btl_openib_handle_incoming<br>
&gt; (openib_btl=0x1902f9b0, ep=0x1908a1c0, frag=0x190d9700, byte_len=18) at<br>
&gt; btl_openib_component.c:2881 #2  0x00002aedbc4e25e2 in handle_wc<br>
&gt; (device=0x19024ac0, cq=0, wc=0x7ffff279ce90) at<br>
&gt; btl_openib_component.c:3178 #3  0x00002aedbc4e2e9d in poll_device<br>
&gt; (device=0x19024ac0, count=2) at btl_openib_component.c:3318 #4<br>
&gt; 0x00002aedbc4e34b8 in progress_one_device (device=0x19024ac0) at<br>
&gt; btl_openib_component.c:3426 #5  0x00002aedbc4e3561 in<br>
&gt; btl_openib_component_progress () at btl_openib_component.c:3451 #6<br>
&gt; 0x00002aedb8b22ab8 in opal_progress () at runtime/opal_progress.c:207 #7<br>
&gt; 0x00002aedb859f497 in opal_condition_wait (c=0x2aedb888ccc0,<br>
&gt; m=0x2aedb888cd20) at ../opal/threads/condition.h:99 #8  0x00002aedb859fa31<br>
&gt; in ompi_request_default_wait_all (count=2, requests=0x7ffff279d0e0,<br>
&gt; statuses=0x0) at request/req_wait.c:262 #9  0x00002aedbd7559ad in<br>
&gt; ompi_coll_tuned_allreduce_intra_recursivedoubling (sbuf=0x7ffff279d444,<br>
&gt; rbuf=0x7ffff279d440, count=1, dtype=0x6788220, op=0x6787a20,<br>
&gt; comm=0x19d81ff0, module=0x19d82b20) at coll_tuned_allreduce.c:223<br>
&gt; #10 0x00002aedbd7514f7 in ompi_coll_tuned_allreduce_intra_dec_fixed<br>
&gt; (sbuf=0x7ffff279d444, rbuf=0x7ffff279d440, count=1, dtype=0x6788220,<br>
&gt; op=0x6787a20, comm=0x19d81ff0, module=0x19d82b20) at<br>
&gt; coll_tuned_decision_fixed.c:63<br>
&gt; #11 0x00002aedb85c7792 in PMPI_Allreduce (sendbuf=0x7ffff279d444,<br>
&gt; recvbuf=0x7ffff279d440, count=1, datatype=0x6788220, op=0x6787a20,<br>
&gt; comm=0x19d81ff0) at pallreduce.c:102 #12 0x0000000004387dbf in<br>
&gt; FEMTown::MPI::Allreduce (sendbuf=0x7ffff279d444, recvbuf=0x7ffff279d440,<br>
&gt; count=1, datatype=0x6788220, op=0x6787a20, comm=0x19d81ff0) at<br>
&gt; stubs.cpp:626 #13 0x0000000004058be8 in FEMTown::Domain::align (itf=<br>
&gt;             {&lt;FEMTown::Boost::shared_base_ptr&lt;FEMTown::Domain::Interface&gt;&gt;<br>
&gt; = {_vptr.shared_base_ptr = 0x7ffff279d620, ptr_ = {px = 0x199942a4, pn =<br>
&gt; {pi_ = 0x6}}}, &lt;No data fields&gt;}) at interface.cpp:371<br>
&gt; #14 0x00000000040cb858 in FEMTown::Field::detail::align_itfs_and_neighbhors<br>
&gt; (dim=2, set={px = 0x7ffff279d780, pn = {pi_ = 0x2f279d640}},<br>
&gt; check_info=@0x7ffff279d7f0) at check.cpp:63 #15 0x00000000040cbfa8 in<br>
&gt; FEMTown::Field::align_elements (set={px = 0x7ffff279d950, pn = {pi_ =<br>
&gt; 0x66e08d0}}, check_info=@0x7ffff279d7f0) at check.cpp:159 #16<br>
&gt; 0x00000000039acdd4 in PyField_align_elements (self=0x0,<br>
&gt; args=0x2aaab0765050, kwds=0x19d2e950) at check.cpp:31 #17<br>
&gt; 0x0000000001fbf76d in FEMTown::Main::ExErrCatch&lt;_object* (*)(_object*,<br>
&gt; _object*, _object*)&gt;::exec&lt;_object&gt; (this=0x7ffff279dc20, s=0x0,<br>
&gt; po1=0x2aaab0765050, po2=0x19d2e950) at<br>
&gt; /home/qa/svntop/femtown/modules/main/py/exception.hpp:463<br>
&gt; #18 0x00000000039acc82 in PyField_align_elements_ewrap (self=0x0,<br>
&gt; args=0x2aaab0765050, kwds=0x19d2e950) at check.cpp:39 #19<br>
&gt; 0x00000000044093a0 in PyEval_EvalFrameEx (f=0x19b52e90, throwflag=&lt;value<br>
&gt; optimized out&gt;) at Python/ceval.c:3921 #20 0x000000000440aae9 in<br>
&gt; PyEval_EvalCodeEx (co=0x2aaab754ad50, globals=&lt;value optimized out&gt;,<br>
&gt; locals=&lt;value optimized out&gt;, args=0x3, argcount=1, kws=0x19ace4a0,<br>
&gt; kwcount=2, defs=0x2aaab75e4800, defcount=2, closure=0x0) at<br>
&gt; Python/ceval.c:2968<br>
&gt; #21 0x0000000004408f58 in PyEval_EvalFrameEx (f=0x19ace2d0,<br>
&gt; throwflag=&lt;value optimized out&gt;) at Python/ceval.c:3802 #22<br>
&gt; 0x000000000440aae9 in PyEval_EvalCodeEx (co=0x2aaab7550120, globals=&lt;value<br>
&gt; optimized out&gt;, locals=&lt;value optimized out&gt;, args=0x7, argcount=1,<br>
&gt; kws=0x19acc418, kwcount=3, defs=0x2aaab759e958, defcount=6, closure=0x0)<br>
&gt; at Python/ceval.c:2968<br>
&gt; #23 0x0000000004408f58 in PyEval_EvalFrameEx (f=0x19acc1c0,<br>
&gt; throwflag=&lt;value optimized out&gt;) at Python/ceval.c:3802 #24<br>
&gt; 0x000000000440aae9 in PyEval_EvalCodeEx (co=0x2aaab8b5e738, globals=&lt;value<br>
&gt; optimized out&gt;, locals=&lt;value optimized out&gt;, args=0x6, argcount=1,<br>
&gt; kws=0x19abd328, kwcount=5, defs=0x2aaab891b7e8, defcount=3, closure=0x0)<br>
&gt; at Python/ceval.c:2968<br>
&gt; #25 0x0000000004408f58 in PyEval_EvalFrameEx (f=0x19abcea0,<br>
&gt; throwflag=&lt;value optimized out&gt;) at Python/ceval.c:3802 #26<br>
&gt; 0x000000000440aae9 in PyEval_EvalCodeEx (co=0x2aaab3eb4198, globals=&lt;value<br>
&gt; optimized out&gt;, locals=&lt;value optimized out&gt;, args=0xb, argcount=1,<br>
&gt; kws=0x19a89df0, kwcount=10, defs=0x0, defcount=0, closure=0x0) at<br>
&gt; Python/ceval.c:2968<br>
&gt; #27 0x0000000004408f58 in PyEval_EvalFrameEx (f=0x19a89c40,<br>
&gt; throwflag=&lt;value optimized out&gt;) at Python/ceval.c:3802 #28<br>
&gt; 0x000000000440aae9 in PyEval_EvalCodeEx (co=0x2aaab3eb4288, globals=&lt;value<br>
&gt; optimized out&gt;, locals=&lt;value optimized out&gt;, args=0x1, argcount=0,<br>
&gt; kws=0x19a89330, kwcount=0, defs=0x2aaab8b66668, defcount=1, closure=0x0)<br>
&gt; at Python/ceval.c:2968<br>
&gt; #29 0x0000000004408f58 in PyEval_EvalFrameEx (f=0x19a891b0,<br>
&gt; throwflag=&lt;value optimized out&gt;) at Python/ceval.c:3802 #30<br>
&gt; 0x000000000440aae9 in PyEval_EvalCodeEx (co=0x2aaab8b6a738, globals=&lt;value<br>
&gt; optimized out&gt;, locals=&lt;value optimized out&gt;, args=0x0, argcount=0,<br>
&gt; kws=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0) at<br>
&gt; Python/ceval.c:2968<br>
&gt; #31 0x000000000440ac02 in PyEval_EvalCode (co=0x1902f9b0, globals=0x0,<br>
&gt; locals=0x190d9700) at Python/ceval.c:522 #32 0x000000000442853c in<br>
&gt; PyRun_StringFlags (str=0x192fd3d8 &quot;DIRECT.Actran.main()&quot;, start=&lt;value<br>
&gt; optimized out&gt;, globals=0x192213d0, locals=0x192213d0, flags=0x0) at<br>
&gt; Python/pythonrun.c:1335 #33 0x0000000004429690 in PyRun_SimpleStringFlags<br>
&gt; (command=0x192fd3d8 &quot;DIRECT.Actran.main()&quot;, flags=0x0) at<br>
&gt; Python/pythonrun.c:957 #34 0x0000000001fa1cf9 in<br>
&gt; FEMTown::Python::FEMPy::run_application (this=0x7ffff279f650) at<br>
&gt; fempy.cpp:873 #35 0x000000000434ce99 in FEMTown::Main::Batch::run<br>
&gt; (this=0x7ffff279f650) at batch.cpp:374 #36 0x0000000001f9aa25 in main<br>
&gt; (argc=8, argv=0x7ffff279fa48) at main.cpp:10 (gdb) f 1<br>
&gt; #1  0x00002aedbc4e05f4 in btl_openib_handle_incoming<br>
&gt; (openib_btl=0x1902f9b0, ep=0x1908a1c0, frag=0x190d9700, byte_len=18) at<br>
&gt; btl_openib_component.c:2881 2881            reg-&gt;cbfunc(<br>
&gt; &amp;openib_btl-&gt;super, hdr-&gt;tag, des, reg-&gt;cbdata ); Current language:  auto;<br>
&gt; currently c<br>
&gt; (gdb)<br>
&gt; #1  0x00002aedbc4e05f4 in btl_openib_handle_incoming<br>
&gt; (openib_btl=0x1902f9b0, ep=0x1908a1c0, frag=0x190d9700, byte_len=18) at<br>
&gt; btl_openib_component.c:2881 2881            reg-&gt;cbfunc(<br>
&gt; &amp;openib_btl-&gt;super, hdr-&gt;tag, des, reg-&gt;cbdata ); (gdb) l<br>
&gt; 2876<br>
&gt; 2877        if(OPAL_LIKELY(!(is_credit_msg = is_credit_message(frag)))) {<br>
&gt; 2878            /* call registered callback */<br>
&gt; 2879            mca_btl_active_message_callback_t* reg;<br>
&gt; 2880            reg = mca_btl_base_active_message_trigger + hdr-&gt;tag;<br>
&gt; 2881            reg-&gt;cbfunc( &amp;openib_btl-&gt;super, hdr-&gt;tag, des, reg-&gt;cbdata<br>
&gt; ); 2882            if(MCA_BTL_OPENIB_RDMA_FRAG(frag)) {<br>
&gt; 2883                cqp = (hdr-&gt;credits &gt;&gt; 11) &amp; 0x0f;<br>
&gt; 2884                hdr-&gt;credits &amp;= 0x87ff;<br>
&gt; 2885            } else {<br>
&gt;<br>
&gt; Regards,<br>
&gt; Eloi<br>
&gt;<br>
&gt; On Friday 16 July 2010 16:01:02 Eloi Gaudry wrote:<br>
&gt; &gt; Hi Edgar,<br>
&gt; &gt;<br>
&gt; &gt; The only difference I could observed was that the segmentation fault<br>
&gt; &gt; appeared sometimes later during the parallel computation.<br>
&gt; &gt;<br>
&gt; &gt; I&#39;m running out of idea here. I wish I could use the &quot;--mca coll tuned&quot;<br>
&gt; &gt; with &quot;--mca self,sm,tcp&quot; so that I could check that the issue is not<br>
&gt; &gt; somehow limited to the tuned collective routines.<br>
&gt; &gt;<br>
&gt; &gt; Thanks,<br>
&gt; &gt; Eloi<br>
&gt; &gt;<br>
&gt; &gt; On Thursday 15 July 2010 17:24:24 Edgar Gabriel wrote:<br>
&gt; &gt; &gt; On 7/15/2010 10:18 AM, Eloi Gaudry wrote:<br>
&gt; &gt; &gt; &gt; hi edgar,<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; thanks for the tips, I&#39;m gonna try this option as well. the<br>
&gt; &gt; &gt; &gt; segmentation fault i&#39;m observing always happened during a collective<br>
&gt; &gt; &gt; &gt; communication indeed... does it basically switch all collective<br>
&gt; &gt; &gt; &gt; communication to basic mode, right ?<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; sorry for my ignorance, but what&#39;s a NCA ?<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; sorry, I meant to type HCA (InifinBand networking card)<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; Thanks<br>
&gt; &gt; &gt; Edgar<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; thanks,<br>
&gt; &gt; &gt; &gt; éloi<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; On Thursday 15 July 2010 16:20:54 Edgar Gabriel wrote:<br>
&gt; &gt; &gt; &gt;&gt; you could try first to use the algorithms in the basic module, e.g.<br>
&gt; &gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt; mpirun -np x --mca coll basic ./mytest<br>
&gt; &gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt; and see whether this makes a difference. I used to observe sometimes<br>
&gt; &gt; &gt; &gt;&gt; a (similar ?) problem in the openib btl triggered from the tuned<br>
&gt; &gt; &gt; &gt;&gt; collective component, in cases where the ofed libraries were<br>
&gt; &gt; &gt; &gt;&gt; installed but no NCA was found on a node. It used to work however<br>
&gt; &gt; &gt; &gt;&gt; with the basic component.<br>
&gt; &gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt; Thanks<br>
&gt; &gt; &gt; &gt;&gt; Edgar<br>
&gt; &gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt; On 7/15/2010 3:08 AM, Eloi Gaudry wrote:<br>
&gt; &gt; &gt; &gt;&gt;&gt; hi Rolf,<br>
&gt; &gt; &gt; &gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt; unfortunately, i couldn&#39;t get rid of that annoying segmentation<br>
&gt; &gt; &gt; &gt;&gt;&gt; fault when selecting another bcast algorithm. i&#39;m now going to<br>
&gt; &gt; &gt; &gt;&gt;&gt; replace MPI_Bcast with a naive implementation (using MPI_Send and<br>
&gt; &gt; &gt; &gt;&gt;&gt; MPI_Recv) and see if that helps.<br>
&gt; &gt; &gt; &gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt; regards,<br>
&gt; &gt; &gt; &gt;&gt;&gt; éloi<br>
&gt; &gt; &gt; &gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt; On Wednesday 14 July 2010 10:59:53 Eloi Gaudry wrote:<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt; Hi Rolf,<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt; thanks for your input. You&#39;re right, I miss the<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt; coll_tuned_use_dynamic_rules option.<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt; I&#39;ll check if I the segmentation fault disappears when using the<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt; basic bcast linear algorithm using the proper command line you<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt; provided.<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt; Regards,<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt; Eloi<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt; On Tuesday 13 July 2010 20:39:59 Rolf vandeVaart wrote:<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; Hi Eloi:<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; To select the different bcast algorithms, you need to add an<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; extra mca parameter that tells the library to use dynamic<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; selection. --mca coll_tuned_use_dynamic_rules 1<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; One way to make sure you are typing this in correctly is to use<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; it with ompi_info.  Do the following:<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; ompi_info -mca coll_tuned_use_dynamic_rules 1 --param coll<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; You should see lots of output with all the different algorithms<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; that can be selected for the various collectives.<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; Therefore, you need this:<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; --mca coll_tuned_use_dynamic_rules 1 --mca<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; coll_tuned_bcast_algorithm 1<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; Rolf<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; On 07/13/10 11:28, Eloi Gaudry wrote:<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; Hi,<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; I&#39;ve found that &quot;--mca coll_tuned_bcast_algorithm 1&quot; allowed to<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; switch to the basic linear algorithm. Anyway whatever the<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; algorithm used, the segmentation fault remains.<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; Does anyone could give some advice on ways to diagnose the issue<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; I&#39;m facing ?<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; Regards,<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; Eloi<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; On Monday 12 July 2010 10:53:58 Eloi Gaudry wrote:<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; Hi,<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; I&#39;m focusing on the MPI_Bcast routine that seems to randomly<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; segfault when using the openib btl. I&#39;d like to know if there<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; is any way to make OpenMPI switch to a different algorithm<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; than the default one being selected for MPI_Bcast.<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; Thanks for your help,<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; Eloi<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; On Friday 02 July 2010 11:06:52 Eloi Gaudry wrote:<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hi,<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; I&#39;m observing a random segmentation fault during an internode<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; parallel computation involving the openib btl and<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; OpenMPI-1.4.2 (the same issue can be observed with<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; OpenMPI-1.3.3).<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    mpirun (Open MPI) 1.4.2<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    Report bugs to <a href="http://www.open-mpi.org/community/help/" target="_blank">http://www.open-mpi.org/community/help/</a><br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    [pbn08:02624] *** Process received signal ***<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    [pbn08:02624] Signal: Segmentation fault (11)<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    [pbn08:02624] Signal code: Address not mapped (1)<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    [pbn08:02624] Failing at address: (nil)<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    [pbn08:02624] [ 0] /lib64/libpthread.so.0 [0x349540e4c0]<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    [pbn08:02624] *** End of error message ***<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    sh: line 1:  2624 Segmentation fault<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; \/share\/hpc3\/actran_suite\/Actran_11\.0\.rc2\.41872\/RedHatE<br>
</div></div>&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; L\ -5 \/ x 86 _6 4\ /bin\/actranpy_mp<br>
<div class="im">&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#39;--apl=/share/hpc3/actran_suite/Actran_11.0.rc2.41872/RedHatEL<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; -5 /x 86 _ 64 /A c tran_11.0.rc2.41872&#39;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#39;--inputfile=/work/st25652/LSF_130073_0_47696_0/Case1_3Dreal_m<br>
</div>&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 4_ n2 .d a t&#39;<br>
<div><div></div><div class="h5">&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#39;--scratch=/scratch/st25652/LSF_130073_0_47696_0/scratch&#39;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#39;--mem=3200&#39; &#39;--threads=1&#39; &#39;--errorlevel=FATAL&#39; &#39;--t_max=0.1&#39;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#39;--parallel=domain&#39;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; If I choose not to use the openib btl (by using --mca btl<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; self,sm,tcp on the command line, for instance), I don&#39;t<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; encounter any problem and the parallel computation runs<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; flawlessly.<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; I would like to get some help to be able:<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; - to diagnose the issue I&#39;m facing with the openib btl<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; - understand why this issue is observed only when using the<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; openib btl and not when using self,sm,tcp<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Any help would be very much appreciated.<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; The outputs of ompi_info and the configure scripts of OpenMPI<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; are enclosed to this email, and some information on the<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; infiniband drivers as well.<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Here is the command line used when launching a parallel<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; computation<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; using infiniband:<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    path_to_openmpi/bin/mpirun -np $NPROCESS --hostfile<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    host.list --mca<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; btl openib,sm,self,tcp  --display-map --verbose --version<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; --mca mpi_warn_on_fork 0 --mca btl_openib_want_fork_support 0<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; [...]<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; and the command line used if not using infiniband:<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    path_to_openmpi/bin/mpirun -np $NPROCESS --hostfile<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    host.list --mca<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; btl self,sm,tcp  --display-map --verbose --version --mca<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; mpi_warn_on_fork 0 --mca btl_openib_want_fork_support 0 [...]<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Thanks,<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Eloi<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; users mailing list<br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
--<br>
<br>
<br>
Eloi Gaudry<br>
<br>
Free Field Technologies<br>
Company Website: <a href="http://www.fft.be" target="_blank">http://www.fft.be</a><br>
Company Phone:   +32 10 487 959<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

