Hello OMPI Users,<div>I&#39;m using OpenMPI 1.4.1 with gcc 4.4.3 on my x86_64 linux system running the latest Ubuntu 10.04 distro. I don&#39;t seem to be able to run any OpenMPI application. I try running the simplest application, which goes like this</div>
<div><br></div><div><div>#include&lt;mpi.h&gt; </div><div>int main(int argc, char * argv[])</div><div>{</div><div>MPI_Init(NULL, NULL);</div><div>MPI_Finalize();</div><div>}</div></div><div><br></div><div>Compiling it with &quot;mpicc -g test.c&quot;</div>
<div>Running with &quot;mpirun -n 2 -hostfile hosts a.out&quot;</div><div>hosts file contains &quot;localhost slots=2&quot;</div><div>On run, I get this</div><div><br></div><div><br></div><div><div>[starbuck:18829] *** Process received signal ***</div>
<div>[starbuck:18830] *** Process received signal ***</div><div>[starbuck:18830] Signal: Segmentation fault (11)</div><div>[starbuck:18830] Signal code: Address not mapped (1)</div><div>[starbuck:18830] Failing at address: 0x3c</div>
<div>[starbuck:18829] Signal: Segmentation fault (11)</div><div>[starbuck:18829] Signal code: Address not mapped (1)</div><div>[starbuck:18829] Failing at address: 0x3c</div><div>[starbuck:18830] [ 0] /lib/libpthread.so.0(+0xf8f0) [0x7f3b0aae08f0]</div>
<div>[starbuck:18830] [ 1] /usr/local/lib/libmca_common_sm.so.1(+0x1561) [0x7f3b082e8561]</div><div>[starbuck:18830] [ 2] /usr/local/lib/libmca_common_sm.so.1(mca_common_sm_mmap_init+0x6c1) [0x7f3b082e9137]</div><div>[starbuck:18830] [ 3] /usr/lib/openmpi/lib/openmpi/mca_mpool_sm.so(+0x137b) [0x7f3b084ed37b]</div>
<div>[starbuck:18830] [ 4] /usr/lib/libmpi.so.0(mca_mpool_base_module_create+0x7d) [0x7f3b0bacc38d]</div><div>[starbuck:18830] [ 5] /usr/lib/openmpi/lib/openmpi/mca_btl_sm.so(+0x2a38) [0x7f3b06c52a38]</div><div>[starbuck:18830] [ 6] /usr/lib/openmpi/lib/openmpi/mca_bml_r2.so(+0x18e7) [0x7f3b076a48e7]</div>
<div>[starbuck:18830] [ 7] /usr/lib/openmpi/lib/openmpi/mca_pml_ob1.so(+0x258c) [0x7f3b07aae58c]</div><div>[starbuck:18830] [ 8] /usr/lib/libmpi.so.0(+0x392bf) [0x7f3b0ba8b2bf]</div><div>[starbuck:18830] [ 9] /usr/lib/libmpi.so.0(MPI_Init+0x170) [0x7f3b0baac330]</div>
<div>[starbuck:18830] [10] a.out(main+0x22) [0x400866]</div><div>[starbuck:18830] [11] /lib/libc.so.6(__libc_start_main+0xfd) [0x7f3b0a76cc4d]</div><div>[starbuck:18830] [12] a.out() [0x400789]</div><div>[starbuck:18830] *** End of error message ***</div>
<div>[starbuck:18829] [ 0] /lib/libpthread.so.0(+0xf8f0) [0x7fb6efefe8f0]</div><div>[starbuck:18829] [ 1] /usr/local/lib/libmca_common_sm.so.1(+0x1561) [0x7fb6ed706561]</div><div>[starbuck:18829] [ 2] /usr/local/lib/libmca_common_sm.so.1(mca_common_sm_mmap_init+0x6c1) [0x7fb6ed707137]</div>
<div>[starbuck:18829] [ 3] /usr/lib/openmpi/lib/openmpi/mca_mpool_sm.so(+0x137b) [0x7fb6ed90b37b]</div><div>[starbuck:18829] [ 4] /usr/lib/libmpi.so.0(mca_mpool_base_module_create+0x7d) [0x7fb6f0eea38d]</div><div>[starbuck:18829] [ 5] /usr/lib/openmpi/lib/openmpi/mca_btl_sm.so(+0x2a38) [0x7fb6ec070a38]</div>
<div>[starbuck:18829] [ 6] /usr/lib/openmpi/lib/openmpi/mca_bml_r2.so(+0x18e7) [0x7fb6ecac28e7]</div><div>[starbuck:18829] [ 7] /usr/lib/openmpi/lib/openmpi/mca_pml_ob1.so(+0x258c) [0x7fb6ececc58c]</div><div>[starbuck:18829] [ 8] /usr/lib/libmpi.so.0(+0x392bf) [0x7fb6f0ea92bf]</div>
<div>[starbuck:18829] [ 9] /usr/lib/libmpi.so.0(MPI_Init+0x170) [0x7fb6f0eca330]</div><div>[starbuck:18829] [10] a.out(main+0x22) [0x400866]</div><div>[starbuck:18829] [11] /lib/libc.so.6(__libc_start_main+0xfd) [0x7fb6efb8ac4d]</div>
<div>[starbuck:18829] [12] a.out() [0x400789]</div><div>[starbuck:18829] *** End of error message ***</div><div>--------------------------------------------------------------------------</div><div>mpirun noticed that process rank 1 with PID 18830 on node starbuck exited on signal 11 (Segmentation fault).</div>
<div>--------------------------------------------------------------------------</div></div><div><br></div><div>My stack trace from gdb is:</div><div><br></div><div><div>Program received signal SIGSEGV, Segmentation fault.</div>
<div>0x00007ffff43c2561 in opal_list_get_first (list=0x7ffff45c5240)</div><div>    at ../../../../../opal/class/opal_list.h:201</div><div>201         assert(1 == item-&gt;opal_list_item_refcount);</div><div>(gdb) bt</div>
<div>#0  0x00007ffff43c2561 in opal_list_get_first (list=0x7ffff45c5240)</div><div>    at ../../../../../opal/class/opal_list.h:201</div><div>#1  0x00007ffff43c3137 in mca_common_sm_mmap_init (procs=0x673cb0, </div><div>    num_procs=2, size=67113040, </div>
<div>    file_name=0x673c40 &quot;/tmp/openmpi-sessions-srikanth@starbuck_0/1510/1/shared_mem_pool.starbuck&quot;, size_ctl_structure=4176, data_seg_alignment=8)</div><div>    at ../../../../../ompi/mca/common/sm/common_sm_mmap.c:291</div>
<div>#2  0x00007ffff45c737b in mca_mpool_sm_init (resources=&lt;value optimized out&gt;)</div><div>    at ../../../../../../ompi/mca/mpool/sm/mpool_sm_component.c:214</div><div>#3  0x00007ffff7ba638d in mca_mpool_base_module_create ()</div>
<div>   from /usr/lib/libmpi.so.0</div><div>#4  0x00007ffff2d2ca38 in sm_btl_first_time_init (btl=&lt;value optimized out&gt;, </div><div>    nprocs=&lt;value optimized out&gt;, procs=&lt;value optimized out&gt;, </div><div>
    peers=&lt;value optimized out&gt;, reachability=&lt;value optimized out&gt;)</div><div>    at ../../../../../../ompi/mca/btl/sm/btl_sm.c:228</div><div>#5  mca_btl_sm_add_procs (btl=&lt;value optimized out&gt;, </div><div>
    nprocs=&lt;value optimized out&gt;, procs=&lt;value optimized out&gt;, </div><div>    peers=&lt;value optimized out&gt;, reachability=&lt;value optimized out&gt;)</div><div>    at ../../../../../../ompi/mca/btl/sm/btl_sm.c:500</div>
<div>#6  0x00007ffff377e8e7 in mca_bml_r2_add_procs (nprocs=&lt;value optimized out&gt;, </div><div>    procs=0x2, reachable=0x7fffffffdd00)</div><div>    at ../../../../../../ompi/mca/bml/r2/bml_r2.c:206</div><div>#7  0x00007ffff3b8858c in mca_pml_ob1_add_procs (procs=0x678ce0, nprocs=2)</div>
<div>---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit--- </div><div>    at ../../../../../../ompi/mca/pml/ob1/pml_ob1.c:315</div><div>#8  0x00007ffff7b652bf in ?? () from /usr/lib/libmpi.so.0</div><div>#9  0x00007ffff7b86330 in PMPI_Init () from /usr/lib/libmpi.so.0</div>
<div>#10 0x0000000000400866 in main (argc=1, argv=0x7fffffffe008)</div><div>    at test.c:4</div></div><div><br></div><div>I can&#39;t figure out what&#39;s going on here! It says MPI_Init is segfaulting, but I think it is probably some kind of misconfiguration.</div>
<div>I have tried reinstalling the openmpi package. I have an AMD Turion X2 M500(64 bit) processor.</div><div><br></div><div>The interesting thing is, the Segfault occurs only when I try to run multiple processes. With n = 1, it has no problems.</div>
<div>Thanks for any help!</div><div><br>-- <br>Regards,<br>Srikanth Raju<br>
</div>

