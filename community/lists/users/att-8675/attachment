Dear all,<br><br>I&#39;m using Open MPI 1.3.1 and SGE 6.2u2 on CentOS 5.2<br>I have 2 compute nodes for testing, each node has a single quad core CPU.<br><br>Here is my submission script and PE config:<br>$ cat hpl-8cpu.sge<br>
#!/bin/bash<br>#<br>#$ -N HPL_8cpu_IB<br>#$ -pe mpi-fu 8<br>#$ -cwd<br>#$ -j y<br>#$ -S /bin/bash<br>#$ -V<br>#<br>cd /home/admin/hpl-2.0<br># For IB<br>/opt/openmpi-gcc/bin/mpirun -v -np $NSLOTS -machinefile $TMPDIR/machines ./bin/goto-openmpi-gcc/xhpl<br>
<br>I&#39;ve tested the mpirun command can be run correctly in command line.<br><br>$ qconf -sp mpi-fu<br>pe_name            mpi-fu<br>slots              8<br>user_lists         NONE<br>xuser_lists        NONE<br>start_proc_args    /opt/sge/mpi/startmpi.sh -catch_rsh $pe_hostfile<br>
stop_proc_args     /opt/sge/mpi/stopmpi.sh<br>allocation_rule    $fill_up<br>control_slaves     TRUE<br>job_is_first_task  FALSE<br>urgency_slots      min<br>accounting_summary TRUE<br><br><br>I&#39;ve checked the  $TMPDIR/machines after submit, it was correct.<br>
node0002<br>node0002<br>node0002<br>node0002<br>node0001<br>node0001<br>node0001<br>node0001<br><br>However, I found that if I explicitly specify the &quot;-machinefile $TMPDIR/machines&quot;, all 8 mpi processes were spawned within a single node, i.e. node0002.<br>
<br>However, if I omit &quot;-machinefile $TMPDIR/machines&quot; in the line mpirun, i.e.<br>/opt/openmpi-gcc/bin/mpirun -v -np
$NSLOTS ./bin/goto-openmpi-gcc/xhpl<br><br>The mpi processes can start correctly, 4 processes in node0001 and 4 processes in node0002.<br><br>Is this normal behaviour of Open MPI?<br><br>Also, I wondered if I have IB interface, for example, the hostname of IB become node0001-clust and node0002-clust, will Open MPI automatically use the IB interface?<br>
<br>How about if I have 2 IB ports in each node, which IB bonding was done, will Open MPI automatically benefit from the double bandwidth?<br><br>Thanks a lot.<br><br>Best Regards,<br>PN<br><br>

