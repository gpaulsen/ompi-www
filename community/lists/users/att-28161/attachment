Bruce:<div><br></div><div>The ARMCI-MPI (mpi3rma branch) test suite is a good way to determine if an MPI-3 implementation supports passive target RMA properly. I haven&#39;t tested against OpenMPI recently but will add it to <span></span>Travis CI before the year is over. </div><div><br></div><div>Best,</div><div><br></div><div>Jeff<br><br>On Monday, December 14, 2015, Palmer, Bruce J &lt;<a href="mailto:Bruce.Palmer@pnnl.gov">Bruce.Palmer@pnnl.gov</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">





<div lang="EN-US" link="blue" vlink="purple">
<div>
<p class="MsoNormal">I’m trying to get some code working using request-based RMA (MPI_Rput, MPI_Rget, MPI_Raccumulate). My understanding of the MPI 3.0 standard is that after calling MPI_Wait on the request handle, the local buffers should be safe to use. On
 calling MPI_Win_flush_all on the window used for RMA operations, all operations should be completed on the remote processor. Based on this, I would expect that the following program should work:<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">#include &lt;mpi.h&gt;<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">int main(int argc, char *argv[])<u></u><u></u></p>
<p class="MsoNormal">{<u></u><u></u></p>
<p class="MsoNormal">  int bytes = 4096;<u></u><u></u></p>
<p class="MsoNormal">  MPI_Win win;<u></u><u></u></p>
<p class="MsoNormal">  void *buf;<u></u><u></u></p>
<p class="MsoNormal">  <u></u><u></u></p>
<p class="MsoNormal">  MPI_Init(&amp;argc, &amp;argv);<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">  MPI_Alloc_mem(bytes,MPI_INFO_NULL, &amp;win);<u></u><u></u></p>
<p class="MsoNormal">  MPI_Win_create(buf,bytes,1,MPI_INFO_NULL,MPI_COMM_WORLD,&amp;win);<u></u><u></u></p>
<p class="MsoNormal">  MPI_Win_flush_all(win);<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">  MPI_Win_free(&amp;win);<u></u><u></u></p>
<p class="MsoNormal">  MPI_Finalize();<u></u><u></u></p>
<p class="MsoNormal">  return(0);<u></u><u></u></p>
<p class="MsoNormal">}<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">However, with openmpi-1.8.3 I’m seeing a crash <u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">[node302:3689] *** An error occurred in MPI_Win_flush_all<u></u><u></u></p>
<p class="MsoNormal">[node302:3689] *** reported by process [2431516673,0]<u></u><u></u></p>
<p class="MsoNormal">[node302:3689] *** on win rdma window 3<u></u><u></u></p>
<p class="MsoNormal">[node302:3689] *** MPI_ERR_RMA_SYNC: error executing rma sync<u></u><u></u></p>
<p class="MsoNormal">[node302:3689] *** MPI_ERRORS_ARE_FATAL (processes in this win will now abort,<u></u><u></u></p>
<p class="MsoNormal">[node302:3689] ***    and potentially your MPI job)<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">I’m seeing similar failures for mvapich2-2.1 and mpich-3.2. Does anyone know if this stuff is suppose to work? I’ve had pretty good luck using the original RMA calls (MPI_Put, MPI_Get and MPI_Accumulate) with MPI_Lock/MPI_Unlock but the
 request-based calls are mostly a complete failure.<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">Bruce Palmer<u></u><u></u></p>
</div>
</div>

</blockquote></div><br><br>-- <br>Jeff Hammond<br><a href="mailto:jeff.science@gmail.com" target="_blank">jeff.science@gmail.com</a><br><a href="http://jeffhammond.github.io/" target="_blank">http://jeffhammond.github.io/</a><br>

