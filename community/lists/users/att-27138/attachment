
<HTML><BODY>Hello, Alina!<br><br>If I use&nbsp; --map-by node I will get only intranode communications on osu_mbw_mr. I use --map-by core instead.<br><br>I have 2 nodes, each node has 2 sockets with 8 cores per socket.<br><br>When I run osu_mbw_mr on 2 nodes with 32 MPI procs (command see below), I&nbsp; expect to see the unidirectional bandwidth of 4xFDR&nbsp; link as a result&nbsp; of this test.<br><br>With IntelMPI I get 6367 MB/s, <br>With ompi_yalla I get about 3744 MB/s (problem: it is a half of impi result)<br>With openmpi without mxm (ompi_clear) I get 6321 MB/s.<br><br>How can I increase yalla results?<br><br>IntelMPI cmd:<br>/opt/software/intel/impi/4.1.0.030/intel64/bin/mpiexec.hydra&nbsp; -machinefile machines.pYAvuK -n 32 -binding domain=core&nbsp; ../osu_impi/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_mbw_mr -v -r=0<br><br>ompi_yalla cmd:<br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.330-icc-OFED-1.5.4.1-redhat6.2-x86_64/ompi-mellanox-fca-v1.8.5/bin/mpirun&nbsp; -report-bindings -display-map -mca coll_hcoll_enable 1 -x&nbsp; HCOLL_MAIN_IB=mlx4_0:1 -x&nbsp;&nbsp;&nbsp;&nbsp; MXM_IB_PORTS=mlx4_0:1 -x&nbsp; MXM_SHM_KCOPY_MODE=off --mca pml yalla --map-by core --bind-to core&nbsp; --hostfile hostlist&nbsp; ../osu_ompi_hcoll/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_mbw_mr -v&nbsp; -r=0<br><br>ompi_clear cmd:<br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.330-icc-OFED-1.5.4.1-redhat6.2-x86_64/ompi-clear-v1.8.5/bin/mpirun&nbsp; -report-bindings -display-map --hostfile hostlist --map-by core&nbsp; --bind-to core&nbsp; ../osu_ompi_clear/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_mbw_mr -v&nbsp; -r=0<br><br>I have attached output files to this letter:<br>ompi_clear.out, ompi_clear.err - contains ompi_clear results<br>ompi_yalla.out, ompi_yalla.err - contains ompi_yalla results<br>impi.out, impi.err - contains intel MPI results<br><br>Best regards,<br>Timur<br><p><br>Воскресенье,  7 июня 2015, 16:11 +03:00 от Alina Sklarevich &lt;alinas@dev.mellanox.co.il&gt;:<br>
</p><blockquote style="border-left:1px solid #0857A6; margin:10px; padding:0 0 0 10px;">
	<div id="">
	



    











	
	


	
	
	

	

	
	

	

	
	



<div class="js-helper js-readmsg-msg">
	<style type="text/css"></style>
 	<div>
		<base target="_self" href="https://e.mail.ru/">
		
            <div id="style_14336826850000000731_BODY"><div dir="ltr">Hi Timur,<div><br></div><div>After running the&nbsp;osu_mbw_mr benchmark in our lab, we obsereved that the binding policy made a difference on the performance.</div><div>Can you please rerun your ompi tests with the following added to your command line? (one of them in each run)</div><div><br></div><div>1.&nbsp;--map-by node --bind-to socket</div><div>2.&nbsp;--map-by node --bind-to core</div><div><br></div><div>Please attach your results.</div><div><br></div><div>Thank you,</div><div>Alina.</div></div><div><br><div>On Thu, Jun 4, 2015 at 6:53 PM, Timur Ismagilov <span dir="ltr">&lt;<a href="//e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span> wrote:<br><blockquote style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div><p>Hello, Alina.</p><span><p><strong>1.</strong> <span lang="en"><span></span></span>Here is my&nbsp;<br><strong>ompi_yalla&nbsp;command line:</strong></p><p style="margin-left:30px">$HPCX_MPI_DIR/bin/mpirun -mca coll_hcoll_enable 1 -x HCOLL_MAIN_IB=mlx4_0:1 -x MXM_IB_PORTS=mlx4_0:1 -x MXM_SHM_KCOPY_MODE=off --mca pml yalla --hostfile hostlist $@</p><p style="margin-left:30px">echo $HPCX_MPI_DIR <br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.330-icc-OFED-1.5.4.1-redhat6.2-x86_64/<strong>ompi-mellanox-fca-v1.8.5</strong><br>This mpi was configured with: --with-mxm=/path/to/mxm --with-hcoll=/path/to/hcoll --with-platform=contrib/platform/mellanox/optimized --prefix=/path/to/<strong>ompi-mellanox-fca-v1.8.5</strong></p><p><strong><strong>ompi_clear command line:</strong></strong></p><p style="margin-left:30px">HPCX_MPI_DIR/bin/mpirun &nbsp;--hostfile hostlist $@</p><p style="margin-left:30px">echo $HPCX_MPI_DIR <br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.330-icc-OFED-1.5.4.1-redhat6.2-x86_64/<strong>ompi-clear-v1.8.5</strong><br>This mpi was configured with: --with-platform=contrib/platform/mellanox/optimized --prefix=/path/to<strong>/ompi-clear-v1.8.5</strong></p></span><p><strong>2.</strong> When i run osu_mbr_mr with key "-x MXM_TLS=self,shm,rc" . It fails with segmentation fault : <br><strong>stdout</strong> log is in attached file osu_mbr_mr_n-2_ppn-16.out; <br><strong>stderr</strong> log is in attached file osu_mbr_mr_n-2_ppn-16.err;<br><strong>cmd line:</strong><br>$HPCX_MPI_DIR/bin/mpirun -mca coll_hcoll_enable 1 -x HCOLL_MAIN_IB=mlx4_0:1 -x MXM_IB_PORTS=mlx4_0:1 -x MXM_SHM_KCOPY_MODE=off --mca pml yalla -x MXM_TLS=self,shm,rc --hostfile hostlist osu_mbw_mr -v -r=0<br><strong>osu_mbw_mr.c</strong> <br>I have changed WINDOW_SIZES in osu_mbw_mr.c:<br>#define WINDOW_SIZES {8, 16, 32, 64, <strong>128, 256, 512, 1024</strong>}&nbsp; <br><strong>3.</strong> I add results of running osu_mbw_mr with yalla and without hcoll on 32 and 64 nodes (512 and 1024 mpi procs<br>) to <strong>mvs10p_mpi.xls</strong>: list osu_mbr_mr.<br><span lang="en"><span>The results are 20 percents smaller than old results (with hcoll).</span></span></p><span><p><br><br><br>Среда,  3 июня 2015, 10:29 +03:00 от Alina Sklarevich &lt;<a href="//e.mail.ru/compose/?mailto=mailto%3aalinas@dev.mellanox.co.il" target="_blank">alinas@dev.mellanox.co.il</a>&gt;:<br>
</p></span><blockquote style="border-left:1px solid #0857a6;margin:10px;padding:0 0 0 10px">
	<div>
	



    











	
	


	
	
	

	

	
	

	

	
	



<div>
	
 	<div>
		
		
            <div><div dir="ltr">Hello Timur,<div><div><div><br></div><div>I will review your results and try to reproduce them in our lab.<br></div><div><br></div><div>You are using an old OFED -&nbsp;OFED-1.5.4.1 and we suspect that this may be causing the performance issues you are seeing.<br></div><div><br></div><div>In the meantime, could you please:<br></div><div><br></div><div>1. send us the exact command lines that you were running when you got these results?</div><div><br></div><div>2. add the following to the command line that you are running with 'pml yalla' and attach the results?<br></div><div>"-x MXM_TLS=self,shm,rc"</div><div><br></div><div>3. run your command line with yalla and without hcoll?</div><div><br></div><div>Thanks,</div><div>Alina.</div><div><br></div><div><br></div></div></div></div><div><div><div><br><div>On Tue, Jun 2, 2015 at 4:56 PM, Timur Ismagilov <span dir="ltr">&lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span> wrote:<br><blockquote style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div><p>Hi, Mike!<br></p><p>I have impi v 4.1.2 (- impi)<br>I build ompi 1.8.5 with MXM and hcoll (- ompi_yalla)<br>I build ompi 1.8.5 without MXM and hcoll (- ompi_clear)</p><p>I start osu p2p: osu_mbr_mr test with this MPIs.<br></p>You can find the result of benchmark in attached file(mvs10p_mpi.xls: list osu_mbr_mr)<br><p><br>On 64 nodes (and 1024 mpi processes) ompi_yalla get 2 time worse perf than ompi_clear.<br>Is mxm with yalla <span lang="en"><span>reduces</span> <span>performance</span> in p2p <span>compared</span></span> with ompi_clear(and impi)?<br><span lang="en">Am <span>I&nbsp;</span><span>doing something</span> <span>wrong?</span></span><br></p><p>P.S. My colleague Alexander Semenov is in CC</p><p>Best regards,<br>Timur<br><br>Четверг, 28 мая 2015, 20:02 +03:00 от Mike Dubman &lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3amiked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt;:<br>
</p><blockquote style="border-left:1px solid #0857a6;margin:10px;padding:0 0 0 10px">
	<div>
	



    











	
	


	
	
	

	

	
	

	

	
	



<div>
	
 	<div>
		
		
            <div><div dir="ltr">it is not apples-to-apples comparison.<div><br></div><div>yalla/mxm is point-to-point library, it is not collective library.</div><div>collective algorithm happens on top of yalla.</div><div><br></div><div>Intel collective algorithm for a2a is better than OMPI built-in collective algorithm.</div><div><br></div><div>To see benefit of yalla - you should run p2p benchmarks (osu_lat/bw/bibw/mr)</div><div><br></div></div><div><br><div>On Thu, May 28, 2015 at 7:35 PM, Timur Ismagilov <span dir="ltr">&lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span> wrote:<br><blockquote style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div>I compare ompi-1.8.5 (hpcx-1.3.3-icc) with impi v 4.1.4.<br><br>I build ompi with MXM but without HCOLL and without&nbsp; knem (I work on it). Configure options are:<br>&nbsp;./configure&nbsp; --prefix=my_prefix&nbsp;&nbsp; --with-mxm=path/to/hpcx/hpcx-v1.3.330-icc-OFED-1.5.4.1-redhat6.2-x86_64/mxm&nbsp;&nbsp; --with-platform=contrib/platform/mellanox/optimized<br><br>As a result of the IMB-MPI1 Alltoall test, I have got disappointing&nbsp; results: for the most message sizes on 64 nodes and 16 processes per&nbsp; node impi is much (~40%) better.<br><br>You can look at the results in the file "mvs10p_mpi.xlsx", I attach it. System configuration is also there.<br><br>What do you think about? Is there any way to improve ompi yalla performance results?<br><br>I attach the output of&nbsp; "IMB-MPI1 Alltoall" for yalla and impi.<br><br>P.S. My colleague Alexander Semenov is in CC<br><br>Best regards,<br>Timur</div>
</blockquote></div><br><br clear="all"><span><font color="#888888"><div><br></div>-- <br><div><div dir="ltr"><br><div>Kind Regards,</div><div><br></div><div>M.</div></div></div>
</font></span></div>
</div>
            
        
		
	</div>

	
</div>


</div>
</blockquote>
<br>
<br> <br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="https://e.mail.ru/compose/?mailto=mailto%3ausers@open%2dmpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/06/27029.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/06/27029.php</a><br></blockquote></div><br></div>
</div></div></div>
            
        
		
	</div>

	
</div>


</div>
</blockquote>
<br>
<br> <br></div>
</blockquote></div><br></div>
</div>
            
        
		<base target="_self" href="https://e.mail.ru/">
	</div>

	
</div>


</div>
</blockquote>
<br>
<br><br></BODY></HTML>
