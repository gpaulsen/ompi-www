<?
$subject_val = "Re: [OMPI users] reading from file";
include("../../include/msg-header.inc");
?>
<!-- received="Tue May 24 13:41:25 2011" -->
<!-- isoreceived="20110524174125" -->
<!-- sent="Tue, 24 May 2011 23:11:20 +0530" -->
<!-- isosent="20110524174120" -->
<!-- name="sushil samant" -->
<!-- email="solderingmachine_at_[hidden]" -->
<!-- subject="Re: [OMPI users] reading from file" -->
<!-- id="BANLkTikMsd__hA8YCv=_v7fma6+J3qDN_g_at_mail.gmail.com" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="[OMPI users] reading from file" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>Subject:</strong> Re: [OMPI users] reading from file<br>
<strong>From:</strong> sushil samant (<em>solderingmachine_at_[hidden]</em>)<br>
<strong>Date:</strong> 2011-05-24 13:41:20
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16618.php">Faisal: "Re: [OMPI users] openmpi self checkpointing - error while running	example"</a>
<li><strong>Previous message:</strong> <a href="16616.php">Marcus R. Epperson: "Re: [OMPI users] openmpi (1.2.8 or above) and Intel composer XE 2011 (aka 12.0)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
hi rob
<br>
thanks a lot . But if you give some example with .h5 read in c++ or
<br>
fortran, it will help a lot.
<br>
<p>On 5/24/11, users-request_at_[hidden] &lt;users-request_at_[hidden]&gt; wrote:
<br>
<span class="quotelev1">&gt; Send users mailing list submissions to
</span><br>
<span class="quotelev1">&gt; 	users_at_[hidden]
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; To subscribe or unsubscribe via the World Wide Web, visit
</span><br>
<span class="quotelev1">&gt; 	<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1">&gt; or, via email, send a message with subject or body 'help' to
</span><br>
<span class="quotelev1">&gt; 	users-request_at_[hidden]
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; You can reach the person managing the list at
</span><br>
<span class="quotelev1">&gt; 	users-owner_at_[hidden]
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; When replying, please edit your Subject line so it is more specific
</span><br>
<span class="quotelev1">&gt; than &quot;Re: Contents of users digest...&quot;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Today's Topics:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;    1. Invitation to connect on LinkedIn
</span><br>
<span class="quotelev1">&gt;       (Nurul Azri Mohd Radzi via LinkedIn)
</span><br>
<span class="quotelev1">&gt;    2. Re: Invitation to connect on LinkedIn (Jeff Squyres)
</span><br>
<span class="quotelev1">&gt;    3. Re: MPI_COMM_DUP freeze with OpenMPI 1.4.1
</span><br>
<span class="quotelev1">&gt;       (francoise.roch_at_[hidden])
</span><br>
<span class="quotelev1">&gt;    4. Re: users Digest, Vol 1911, Issue 3 (Salvatore Podda)
</span><br>
<span class="quotelev1">&gt;    5. Re: openmpi (1.2.8 or above) and Intel composer	XE	2011 (aka
</span><br>
<span class="quotelev1">&gt;       12.0) (Salvatore Podda)
</span><br>
<span class="quotelev1">&gt;    6. Re: openmpi (1.2.8 or above) and Intel composer XE	2011 (aka
</span><br>
<span class="quotelev1">&gt;       12.0) (Salvatore Podda)
</span><br>
<span class="quotelev1">&gt;    7. Re: btl_openib_cpc_include rdmacm questions (Dave Love)
</span><br>
<span class="quotelev1">&gt;    8. Re: Trouble with MPI-IO (Rob Latham)
</span><br>
<span class="quotelev1">&gt;    9. Re: reading from a file (Rob Latham)
</span><br>
<span class="quotelev1">&gt;   10. Re: Openib with &gt; 32 cores per node (Dave Love)
</span><br>
<span class="quotelev1">&gt;   11. Re: Trouble with MPI-IO (Tom Rosmond)
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ----------------------------------------------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Message: 1
</span><br>
<span class="quotelev1">&gt; Date: Tue, 24 May 2011 00:16:52 +0000 (UTC)
</span><br>
<span class="quotelev1">&gt; From: Nurul Azri Mohd Radzi via LinkedIn &lt;member_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Subject: [OMPI users] Invitation to connect on LinkedIn
</span><br>
<span class="quotelev1">&gt; To: Mohan L &lt;users_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Message-ID:
</span><br>
<span class="quotelev1">&gt; 	&lt;1621713298.532717.1306196212953.JavaMail.app_at_ela4-bed33.prod&gt;
</span><br>
<span class="quotelev1">&gt; Content-Type: text/plain; charset=&quot;utf-8&quot;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; LinkedIn
</span><br>
<span class="quotelev1">&gt; ------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;     Nurul Azri Mohd Radzi requested to add you as a connection on LinkedIn:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Mohan,
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; I'd like to add you to my professional network on LinkedIn.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; - Nurul Azri
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Accept invitation from Nurul Azri Mohd Radzi
</span><br>
<span class="quotelev1">&gt; <a href="http://www.linkedin.com/e/kq0fyp-go23i09i-48/uYFEuWAc-_V_w7MB9hFjx_pd4WRoHI/blk/I47709029_55/pmpxnSRJrSdvj4R5fnhv9ClRsDgZp6lQs6lzoQ5AomZIpn8_djlvej8Mej0TdPh9bPB1pCpRtkFhbPAPcj8OdzoTej8LrCBxbOYWrSlI/EML_comm_afe/">http://www.linkedin.com/e/kq0fyp-go23i09i-48/uYFEuWAc-_V_w7MB9hFjx_pd4WRoHI/blk/I47709029_55/pmpxnSRJrSdvj4R5fnhv9ClRsDgZp6lQs6lzoQ5AomZIpn8_djlvej8Mej0TdPh9bPB1pCpRtkFhbPAPcj8OdzoTej8LrCBxbOYWrSlI/EML_comm_afe/</a>
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; View invitation from Nurul Azri Mohd Radzi
</span><br>
<span class="quotelev1">&gt; <a href="http://www.linkedin.com/e/kq0fyp-go23i09i-48/uYFEuWAc-_V_w7MB9hFjx_pd4WRoHI/blk/I47709029_55/0RdlYVcz0Vc3sTd4ALqnpPbOYWrSlI/svi/">http://www.linkedin.com/e/kq0fyp-go23i09i-48/uYFEuWAc-_V_w7MB9hFjx_pd4WRoHI/blk/I47709029_55/0RdlYVcz0Vc3sTd4ALqnpPbOYWrSlI/svi/</a>
</span><br>
<span class="quotelev1">&gt; ------------------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; DID YOU KNOW you can be the first to know when a trusted member of your
</span><br>
<span class="quotelev1">&gt; network changes jobs? With Network Updates on your LinkedIn home page,
</span><br>
<span class="quotelev1">&gt; you'll be notified as members of your network change their current position.
</span><br>
<span class="quotelev1">&gt; Be the first to know and reach out!
</span><br>
<span class="quotelev1">&gt; <a href="http://www.linkedin.com/">http://www.linkedin.com/</a>
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; --
</span><br>
<span class="quotelev1">&gt; (c) 2011, LinkedIn Corporation
</span><br>
<span class="quotelev1">&gt; -------------- next part --------------
</span><br>
<span class="quotelev1">&gt; HTML attachment scrubbed and removed
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Message: 2
</span><br>
<span class="quotelev1">&gt; Date: Mon, 23 May 2011 20:52:30 -0400
</span><br>
<span class="quotelev1">&gt; From: Jeff Squyres &lt;jsquyres_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Subject: Re: [OMPI users] Invitation to connect on LinkedIn
</span><br>
<span class="quotelev1">&gt; To: Nurul Azri Mohd Radzi &lt;nurulazri_at_[hidden]&gt;,	Open MPI Users
</span><br>
<span class="quotelev1">&gt; 	&lt;users_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Message-ID: &lt;2C83E966-5529-4F59-839E-D25A065796AE_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Content-Type: text/plain; charset=iso-8859-1
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Please do not send such invitations to the Open MPI lists.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; On May 23, 2011, at 8:16 PM, Nurul Azri Mohd Radzi via LinkedIn wrote:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev2">&gt;&gt; LinkedIn
</span><br>
<span class="quotelev2">&gt;&gt; Nurul Azri Mohd Radzi requested to add you as a connection on LinkedIn:
</span><br>
<span class="quotelev2">&gt;&gt; Mohan,
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; I'd like to add you to my professional network on LinkedIn.
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; - Nurul Azri
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Accept
</span><br>
<span class="quotelev2">&gt;&gt; View invitation from Nurul Azri Mohd Radzi
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; DID YOU KNOW you can be the first to know when a trusted member of your
</span><br>
<span class="quotelev2">&gt;&gt; network changes jobs?
</span><br>
<span class="quotelev2">&gt;&gt; With Network Updates on your LinkedIn home page, you'll be notified as
</span><br>
<span class="quotelev2">&gt;&gt; members of your network change their current position. Be the first to
</span><br>
<span class="quotelev2">&gt;&gt; know and reach out!
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; ? 2011, LinkedIn Corporation
</span><br>
<span class="quotelev2">&gt;&gt; _______________________________________________
</span><br>
<span class="quotelev2">&gt;&gt; users mailing list
</span><br>
<span class="quotelev2">&gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev2">&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; --
</span><br>
<span class="quotelev1">&gt; Jeff Squyres
</span><br>
<span class="quotelev1">&gt; jsquyres_at_[hidden]
</span><br>
<span class="quotelev1">&gt; For corporate legal information go to:
</span><br>
<span class="quotelev1">&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a>
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Message: 3
</span><br>
<span class="quotelev1">&gt; Date: Tue, 24 May 2011 10:42:48 +0200
</span><br>
<span class="quotelev1">&gt; From: &quot;francoise.roch_at_[hidden]&quot;
</span><br>
<span class="quotelev1">&gt; 	&lt;francoise.roch_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Subject: Re: [OMPI users] MPI_COMM_DUP freeze with OpenMPI 1.4.1
</span><br>
<span class="quotelev1">&gt; To: Open MPI Users &lt;users_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Message-ID: &lt;4DDB6F88.5060004_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Content-Type: text/plain; charset=us-ascii; format=flowed
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Jeff Squyres wrote:
</span><br>
<span class="quotelev2">&gt;&gt; On May 13, 2011, at 8:31 AM, francoise.roch_at_[hidden] wrote:
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Here is the MUMPS portion of code (in zmumps_part1.F file) where the
</span><br>
<span class="quotelev3">&gt;&gt;&gt; slaves call MPI_COMM_DUP , id%PAR and MASTER are initialized to 0 before
</span><br>
<span class="quotelev3">&gt;&gt;&gt; :
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; CALL MPI_COMM_SIZE(id%COMM, id%NPROCS, IERR )
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; I re-indented so that I could read it better:
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;       CALL MPI_COMM_SIZE(id%COMM, id%NPROCS, IERR )
</span><br>
<span class="quotelev2">&gt;&gt;       IF ( id%PAR .eq. 0 ) THEN
</span><br>
<span class="quotelev2">&gt;&gt;          IF ( id%MYID .eq. MASTER ) THEN
</span><br>
<span class="quotelev2">&gt;&gt;             color = MPI_UNDEFINED
</span><br>
<span class="quotelev2">&gt;&gt;          ELSE
</span><br>
<span class="quotelev2">&gt;&gt;             color = 0
</span><br>
<span class="quotelev2">&gt;&gt;          END IF
</span><br>
<span class="quotelev2">&gt;&gt;          CALL MPI_COMM_SPLIT( id%COMM, color, 0,
</span><br>
<span class="quotelev2">&gt;&gt;          &amp; id%COMM_NODES, IERR )
</span><br>
<span class="quotelev2">&gt;&gt;          id%NSLAVES = id%NPROCS - 1
</span><br>
<span class="quotelev2">&gt;&gt;       ELSE
</span><br>
<span class="quotelev2">&gt;&gt;          CALL MPI_COMM_DUP( id%COMM, id%COMM_NODES, IERR )
</span><br>
<span class="quotelev2">&gt;&gt;          id%NSLAVES = id%NPROCS
</span><br>
<span class="quotelev2">&gt;&gt;       END IF
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;       IF (id%PAR .ne. 0 .or. id%MYID .NE. MASTER) THEN
</span><br>
<span class="quotelev2">&gt;&gt;          CALL MPI_COMM_DUP( id%COMM_NODES, id%COMM_LOAD, IERR
</span><br>
<span class="quotelev2">&gt;&gt;       ENDIF
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; That doesn't look right -- both MPI_COMM_SPLIT and MPI_COMM_DUP are
</span><br>
<span class="quotelev2">&gt;&gt; collective, meaning that all processes in the communicator must call them.
</span><br>
<span class="quotelev2">&gt;&gt; In the first case, only some processes are calling MPI_COMM_SPLIT.  Is
</span><br>
<span class="quotelev2">&gt;&gt; there some other logic that forces the rest of the processes to call
</span><br>
<span class="quotelev2">&gt;&gt; MPI_COMM_SPLIT, too?
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev1">&gt; Actually, we look at the first case, that is id%par = 0. But the
</span><br>
<span class="quotelev1">&gt; MPI_COMM_SPLIT routine is called by all the processes and creates a new
</span><br>
<span class="quotelev1">&gt; communicator named &quot;id%COMM_NODES&quot;. This communicator contains all the
</span><br>
<span class="quotelev1">&gt; slaves, but not the master. The first MPI_COMM_DUP is not executed, the
</span><br>
<span class="quotelev1">&gt; second one is executed on all the slaves nodes (id%MYID .NE. MASTER ),
</span><br>
<span class="quotelev1">&gt; because the communicator is &quot;id%COMM_NODES&quot; and so implies all the
</span><br>
<span class="quotelev1">&gt; processes of this communicator.
</span><br>
<span class="quotelev1">&gt; So it seems correct to me but perhaps I make a mistake because the
</span><br>
<span class="quotelev1">&gt; MPI_COMM_DUP freezes.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Franc,oise
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Message: 4
</span><br>
<span class="quotelev1">&gt; Date: Tue, 24 May 2011 12:46:17 +0200
</span><br>
<span class="quotelev1">&gt; From: Salvatore Podda &lt;salvatore.podda_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Subject: Re: [OMPI users] users Digest, Vol 1911, Issue 3
</span><br>
<span class="quotelev1">&gt; To: gus_at_[hidden]
</span><br>
<span class="quotelev1">&gt; Cc: users open-mpi &lt;users_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Message-ID: &lt;5121958D-8CF0-4386-BB7F-6530865F6D39_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Sorry for the late reply, but, as I just say, we are attempting
</span><br>
<span class="quotelev1">&gt; to recover the full operation of part of our cluster
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Yes, it was a typo, I use to add the &quot;sm&quot; flag to the &quot;--mca btl&quot;
</span><br>
<span class="quotelev1">&gt; option. However I think this is not mandatory, as I suppose
</span><br>
<span class="quotelev1">&gt; openmpi use the the so-called &quot;Law of Least Astonishment&quot;
</span><br>
<span class="quotelev1">&gt; also in this case and adopts &quot;sm&quot; for the intra-node communication
</span><br>
<span class="quotelev1">&gt; or, if you prefer, avoiding to add the sm string does not mean &quot;not use
</span><br>
<span class="quotelev1">&gt; shared memory&quot;.
</span><br>
<span class="quotelev1">&gt; Indeed if  I remove or add this string nothing change, or if
</span><br>
<span class="quotelev1">&gt; I run an mpi job on a single multicore node without this
</span><br>
<span class="quotelev1">&gt; flag all works well.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Thanls
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Salvatore
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; On 20/mag/11, at 20:53, users-request_at_[hidden] wrote:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Message: 1
</span><br>
<span class="quotelev2">&gt;&gt; Date: Fri, 20 May 2011 14:30:13 -0400
</span><br>
<span class="quotelev2">&gt;&gt; From: Gus Correa &lt;gus_at_[hidden]&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Subject: Re: [OMPI users] openmpi (1.2.8 or above) and Intel composer
</span><br>
<span class="quotelev2">&gt;&gt; 	XE	2011 (aka 12.0)
</span><br>
<span class="quotelev2">&gt;&gt; To: Open MPI Users &lt;users_at_[hidden]&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Message-ID: &lt;4DD6B335.2090403_at_[hidden]&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Content-Type: text/plain; charset=us-ascii; format=flowed
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Hi Salvatore
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Just in case ...
</span><br>
<span class="quotelev2">&gt;&gt; You say you have problems when you use &quot;--mca btl openib,self&quot;.
</span><br>
<span class="quotelev2">&gt;&gt; Is this a typo in your email?
</span><br>
<span class="quotelev2">&gt;&gt; I guess this will disable the shared memory btl intra-node,
</span><br>
<span class="quotelev2">&gt;&gt; whereas your other choice &quot;--mca btl_tcp_if_include ib0&quot; will not.
</span><br>
<span class="quotelev2">&gt;&gt; Could this be the problem?
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Here we use &quot;--mca btl openib,self,sm&quot;,
</span><br>
<span class="quotelev2">&gt;&gt; to enable the shared memory btl intra-node as well,
</span><br>
<span class="quotelev2">&gt;&gt; and it works just fine on programs that do use collective calls.
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; My two cents,
</span><br>
<span class="quotelev2">&gt;&gt; Gus Correa
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Salvatore Podda wrote:
</span><br>
<span class="quotelev3">&gt;&gt;&gt; We are still struggling we these problems. Actually the new version
</span><br>
<span class="quotelev3">&gt;&gt;&gt; of
</span><br>
<span class="quotelev3">&gt;&gt;&gt; intel compilers does
</span><br>
<span class="quotelev3">&gt;&gt;&gt; not seem to be the real issue. We clash against the same errors using
</span><br>
<span class="quotelev3">&gt;&gt;&gt; also the `gcc' compilers.
</span><br>
<span class="quotelev3">&gt;&gt;&gt; We succeed in building an openmi-1.2.8 (with different compiler
</span><br>
<span class="quotelev3">&gt;&gt;&gt; flavours) rpm from the installation
</span><br>
<span class="quotelev3">&gt;&gt;&gt; of the cluster section where all seems to work well. We are now
</span><br>
<span class="quotelev3">&gt;&gt;&gt; doing a
</span><br>
<span class="quotelev3">&gt;&gt;&gt; severe IMB benchmark campaign.
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; However, yes this happen only whe we use the --mca btl openib,self,
</span><br>
<span class="quotelev3">&gt;&gt;&gt; on
</span><br>
<span class="quotelev3">&gt;&gt;&gt; the contrary if we use
</span><br>
<span class="quotelev3">&gt;&gt;&gt; --mca btl_tcp_if_include ib0 all works well.
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Yes we can try the flag you suggest. I can check on the FAQ and on
</span><br>
<span class="quotelev3">&gt;&gt;&gt; the
</span><br>
<span class="quotelev3">&gt;&gt;&gt; opem-mpi.org documentation,
</span><br>
<span class="quotelev3">&gt;&gt;&gt; but can you be so kindly to explain the meaning of this flag?
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Thanks
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Salvatore Podda
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; On 20/mag/11, at 03:37, Jeff Squyres wrote:
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Sorry for the late reply.
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Other users have seen something similar but we have never been
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; able to
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; reproduce it.  Is this only when using IB?  If you use &quot;mpirun --mca
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; btl_openib_cpc_if_include rdmacm&quot;, does the problem go away?
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; On May 11, 2011, at 6:00 PM, Marcus R. Epperson wrote:
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; I've seen the same thing when I build openmpi 1.4.3 with Intel 12,
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; but only when I have -O2 or -O3 in CFLAGS. If I drop it down to -O1
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; then the collectives hangs go away. I don't know what, if anything,
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; the higher optimization buys you when compiling openmpi, so I'm not
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; sure if that's an acceptable workaround or not.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; My system is similar to yours - Intel X5570 with QDR Mellanox IB
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; running RHEL 5, Slurm, and these openmpi btls: openib,sm,self. I'm
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; using IMB 3.2.2 with a single iteration of Barrier to reproduce the
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; hang, and it happens 100% of the time for me when I invoke it
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; like this:
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; # salloc -N 9 orterun -n 65 ./IMB-MPI1 -npmin 64 -iter 1 barrier
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; The hang happens on the first Barrier (64 ranks) and each of the
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; participating ranks have this backtrace:
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; __poll (...)
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; poll_dispatch () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; opal_event_loop () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; opal_progress () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ompi_request_default_wait_all () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ompi_coll_tuned_sendrecv_actual () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ompi_coll_tuned_barrier_intra_recursivedoubling () from
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ompi_coll_tuned_barrier_intra_dec_fixed () from
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; PMPI_Barrier () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; IMB_barrier ()
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; IMB_init_buffers_iter ()
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; main ()
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; The one non-participating rank has this backtrace:
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; __poll (...)
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; poll_dispatch () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; opal_event_loop () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; opal_progress () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ompi_request_default_wait_all () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ompi_coll_tuned_sendrecv_actual () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ompi_coll_tuned_barrier_intra_bruck () from [instdir]/lib/
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; libmpi.so.0
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; ompi_coll_tuned_barrier_intra_dec_fixed () from
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; PMPI_Barrier () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; main ()
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; If I use more nodes I can get it to hang with 1ppn, so that seems
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; to
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; rule out the sm btl (or interactions with it) as a culprit at
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; least.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; I can't reproduce this with openmpi 1.5.3, interestingly.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; -Marcus
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; On 05/10/2011 03:37 AM, Salvatore Podda wrote:
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; Dear all,
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; we succeed in building several version of openmpi from 1.2.8 to
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; 1.4.3
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; with Intel composer XE 2011 (aka 12.0).
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; However we found a threshold in the number of cores (depending
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; from the
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; application: IMB, xhpl or user applications
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; and form the number of required cores) above which the application
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; hangs
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; (sort of deadlocks).
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; The building of openmpi with 'gcc' and 'pgi' does not show the
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; same
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; limits.
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; There are any known incompatibilities of openmpi with this
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; version of
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; intel compiilers?
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; The characteristics of our computational infrastructure are:
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; Intel processors E7330, E5345, E5530 e E5620
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; CentOS 5.3, CentOS 5.5.
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; Intel composer XE 2011
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; gcc 4.1.2
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; pgi 10.2-1
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; Regards
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; Salvatore Podda
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; ENEA UTICT-HPC
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; Department for Computer Science Development and ICT
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; Facilities Laboratory for Science and High Performace Computing
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; C.R. Frascati
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; Via E. Fermi, 45
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; PoBox 65
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; 00044 Frascati (Rome)
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; Italy
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; Tel: +39 06 9400 5342
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; Fax: +39 06 9400 5551
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; Fax: +39 06 9400 5735
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; E-mail: salvatore.podda_at_[hidden]
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; Home Page: www.cresco.enea.it
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; users mailing list
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; _______________________________________________
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; users mailing list
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; --
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Jeff Squyres
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; jsquyres_at_[hidden]
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; For corporate legal information go to:
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a>
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; _______________________________________________
</span><br>
<span class="quotelev3">&gt;&gt;&gt; users mailing list
</span><br>
<span class="quotelev3">&gt;&gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev3">&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Message: 5
</span><br>
<span class="quotelev1">&gt; Date: Tue, 24 May 2011 13:29:57 +0200
</span><br>
<span class="quotelev1">&gt; From: Salvatore Podda &lt;salvatore.podda_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Subject: Re: [OMPI users] openmpi (1.2.8 or above) and Intel composer
</span><br>
<span class="quotelev1">&gt; 	XE	2011 (aka 12.0)
</span><br>
<span class="quotelev1">&gt; To: users open-mpi &lt;users_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Message-ID: &lt;99F1D9BD-4921-40C3-B09F-A7D275B4246A_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Apoligize, I forgot to edit the subject line.
</span><br>
<span class="quotelev1">&gt; I send again with the sensible subject.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Salvatore
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Begin forwarded message:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev2">&gt;&gt; From: Salvatore Podda &lt;salvatore.podda_at_[hidden]&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Date: 24 maggio 2011 12:46:17 GMT+02:00
</span><br>
<span class="quotelev2">&gt;&gt; To: gus_at_[hidden]
</span><br>
<span class="quotelev2">&gt;&gt; Cc: users open-mpi &lt;users_at_[hidden]&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Subject: Re: users Digest, Vol 1911, Issue 3
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Sorry for the late reply, but, as I just say, we are attempting
</span><br>
<span class="quotelev2">&gt;&gt; to recover the full operation of part of our cluster
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Yes, it was a typo, I use to add the &quot;sm&quot; flag to the &quot;--mca btl&quot;
</span><br>
<span class="quotelev2">&gt;&gt; option. However I think this is not mandatory, as I suppose
</span><br>
<span class="quotelev2">&gt;&gt; openmpi use the the so-called &quot;Law of Least Astonishment&quot;
</span><br>
<span class="quotelev2">&gt;&gt; also in this case and adopts &quot;sm&quot; for the intra-node communication
</span><br>
<span class="quotelev2">&gt;&gt; or, if you prefer, avoiding to add the sm string does not mean &quot;not
</span><br>
<span class="quotelev2">&gt;&gt; use
</span><br>
<span class="quotelev2">&gt;&gt; shared memory&quot;.
</span><br>
<span class="quotelev2">&gt;&gt; Indeed if  I remove or add this string nothing change, or if
</span><br>
<span class="quotelev2">&gt;&gt; I run an mpi job on a single multicore node without this
</span><br>
<span class="quotelev2">&gt;&gt; flag all works well.
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Thanks
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Salvatore
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; On 20/mag/11, at 20:53, users-request_at_[hidden] wrote:
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Message: 1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Date: Fri, 20 May 2011 14:30:13 -0400
</span><br>
<span class="quotelev3">&gt;&gt;&gt; From: Gus Correa &lt;gus_at_[hidden]&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Subject: Re: [OMPI users] openmpi (1.2.8 or above) and Intel composer
</span><br>
<span class="quotelev3">&gt;&gt;&gt; 	XE	2011 (aka 12.0)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; To: Open MPI Users &lt;users_at_[hidden]&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Message-ID: &lt;4DD6B335.2090403_at_[hidden]&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Content-Type: text/plain; charset=us-ascii; format=flowed
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Hi Salvatore
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Just in case ...
</span><br>
<span class="quotelev3">&gt;&gt;&gt; You say you have problems when you use &quot;--mca btl openib,self&quot;.
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Is this a typo in your email?
</span><br>
<span class="quotelev3">&gt;&gt;&gt; I guess this will disable the shared memory btl intra-node,
</span><br>
<span class="quotelev3">&gt;&gt;&gt; whereas your other choice &quot;--mca btl_tcp_if_include ib0&quot; will not.
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Could this be the problem?
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Here we use &quot;--mca btl openib,self,sm&quot;,
</span><br>
<span class="quotelev3">&gt;&gt;&gt; to enable the shared memory btl intra-node as well,
</span><br>
<span class="quotelev3">&gt;&gt;&gt; and it works just fine on programs that do use collective calls.
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; My two cents,
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Gus Correa
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Salvatore Podda wrote:
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; We are still struggling we these problems. Actually the new
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; version of
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; intel compilers does
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; not seem to be the real issue. We clash against the same errors
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; using
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; also the `gcc' compilers.
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; We succeed in building an openmi-1.2.8 (with different compiler
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; flavours) rpm from the installation
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; of the cluster section where all seems to work well. We are now
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; doing a
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; severe IMB benchmark campaign.
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; However, yes this happen only whe we use the --mca btl
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; openib,self, on
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; the contrary if we use
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; --mca btl_tcp_if_include ib0 all works well.
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Yes we can try the flag you suggest. I can check on the FAQ and on
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; the
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; opem-mpi.org documentation,
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; but can you be so kindly to explain the meaning of this flag?
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Thanks
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Salvatore Podda
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; On 20/mag/11, at 03:37, Jeff Squyres wrote:
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Sorry for the late reply.
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Other users have seen something similar but we have never been
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; able to
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; reproduce it.  Is this only when using IB?  If you use &quot;mpirun --
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; mca
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; btl_openib_cpc_if_include rdmacm&quot;, does the problem go away?
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; On May 11, 2011, at 6:00 PM, Marcus R. Epperson wrote:
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; I've seen the same thing when I build openmpi 1.4.3 with Intel 12,
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; but only when I have -O2 or -O3 in CFLAGS. If I drop it down to -
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; O1
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; then the collectives hangs go away. I don't know what, if
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; anything,
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; the higher optimization buys you when compiling openmpi, so I'm
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; not
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; sure if that's an acceptable workaround or not.
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; My system is similar to yours - Intel X5570 with QDR Mellanox IB
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; running RHEL 5, Slurm, and these openmpi btls: openib,sm,self. I'm
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; using IMB 3.2.2 with a single iteration of Barrier to reproduce
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; the
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; hang, and it happens 100% of the time for me when I invoke it
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; like this:
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; # salloc -N 9 orterun -n 65 ./IMB-MPI1 -npmin 64 -iter 1 barrier
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; The hang happens on the first Barrier (64 ranks) and each of the
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; participating ranks have this backtrace:
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; __poll (...)
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; poll_dispatch () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; opal_event_loop () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; opal_progress () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; ompi_request_default_wait_all () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; ompi_coll_tuned_sendrecv_actual () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; ompi_coll_tuned_barrier_intra_recursivedoubling () from
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; ompi_coll_tuned_barrier_intra_dec_fixed () from
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; PMPI_Barrier () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; IMB_barrier ()
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; IMB_init_buffers_iter ()
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; main ()
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; The one non-participating rank has this backtrace:
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; __poll (...)
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; poll_dispatch () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; opal_event_loop () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; opal_progress () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; ompi_request_default_wait_all () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; ompi_coll_tuned_sendrecv_actual () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; ompi_coll_tuned_barrier_intra_bruck () from [instdir]/lib/
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; libmpi.so.0
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; ompi_coll_tuned_barrier_intra_dec_fixed () from
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; PMPI_Barrier () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; main ()
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; If I use more nodes I can get it to hang with 1ppn, so that
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; seems to
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; rule out the sm btl (or interactions with it) as a culprit at
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; least.
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; I can't reproduce this with openmpi 1.5.3, interestingly.
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; -Marcus
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; On 05/10/2011 03:37 AM, Salvatore Podda wrote:
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Dear all,
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; we succeed in building several version of openmpi from 1.2.8 to
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; 1.4.3
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; with Intel composer XE 2011 (aka 12.0).
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; However we found a threshold in the number of cores (depending
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; from the
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; application: IMB, xhpl or user applications
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; and form the number of required cores) above which the
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; application
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; hangs
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; (sort of deadlocks).
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; The building of openmpi with 'gcc' and 'pgi' does not show the
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; same
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; limits.
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; There are any known incompatibilities of openmpi with this
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; version of
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; intel compiilers?
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; The characteristics of our computational infrastructure are:
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Intel processors E7330, E5345, E5530 e E5620
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; CentOS 5.3, CentOS 5.5.
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Intel composer XE 2011
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; gcc 4.1.2
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; pgi 10.2-1
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Regards
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Salvatore Podda
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; ENEA UTICT-HPC
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Department for Computer Science Development and ICT
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Facilities Laboratory for Science and High Performace Computing
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; C.R. Frascati
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Via E. Fermi, 45
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; PoBox 65
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; 00044 Frascati (Rome)
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Italy
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Tel: +39 06 9400 5342
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Fax: +39 06 9400 5551
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Fax: +39 06 9400 5735
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; E-mail: salvatore.podda_at_[hidden]
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Home Page: www.cresco.enea.it
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; users mailing list
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev3">&gt;&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; users mailing list
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev2">&gt;&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; --
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; Jeff Squyres
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; jsquyres_at_[hidden]
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; For corporate legal information go to:
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a>
</span><br>
<span class="quotelev1">&gt;&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; _______________________________________________
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; users mailing list
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Message: 6
</span><br>
<span class="quotelev1">&gt; Date: Tue, 24 May 2011 14:34:52 +0200
</span><br>
<span class="quotelev1">&gt; From: Salvatore Podda &lt;salvatore.podda_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Subject: Re: [OMPI users] openmpi (1.2.8 or above) and Intel composer
</span><br>
<span class="quotelev1">&gt; 	XE	2011 (aka 12.0)
</span><br>
<span class="quotelev1">&gt; To: Jeff Squyres &lt;jsquyres_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Cc: Giovanni Bracco &lt;giovanni.bracco_at_[hidden]&gt;,	Open MPI Users
</span><br>
<span class="quotelev1">&gt; 	&lt;users_at_[hidden]&gt;,	Agostino Funel &lt;agostino.funel_at_[hidden]&gt;,
</span><br>
<span class="quotelev1">&gt; 	Fiorenzo Ambrosino &lt;fiorenzo.ambrosino_at_[hidden]&gt;,	Guido Guarnieri
</span><br>
<span class="quotelev1">&gt; 	&lt;guido.guarnieri_at_[hidden]&gt;,	Roberto Ciavarella
</span><br>
<span class="quotelev1">&gt; 	&lt;roberto.ciavarella_at_[hidden]&gt;,	Giovanni Ponti &lt;giovanni.ponti_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Message-ID: &lt;CBB46100-8CDF-4826-A95E-CF8E62B3002E_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; OK! I catch the meaning of the &quot;--mca btl_openib_cpc_include rdmacm&quot;
</span><br>
<span class="quotelev1">&gt; parameter.
</span><br>
<span class="quotelev1">&gt; Howerver, as I just said, we are doing, in the meanwhile, several IMB
</span><br>
<span class="quotelev1">&gt; tests on openmpi
</span><br>
<span class="quotelev1">&gt; 1.2.8 and on this (our) version either the RDMA CM support is not
</span><br>
<span class="quotelev1">&gt; implemented or has
</span><br>
<span class="quotelev1">&gt; not been included in the compilation phase
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Salvatore Podda
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; On 20/mag/11, at 03:37, Jeff Squyres wrote:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Sorry for the late reply.
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Other users have seen something similar but we have never been able
</span><br>
<span class="quotelev2">&gt;&gt; to reproduce it.  Is this only when using IB?  If you use &quot;mpirun --
</span><br>
<span class="quotelev2">&gt;&gt; mca btl_openib_cpc_if_include rdmacm&quot;, does the problem go away?
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; On May 11, 2011, at 6:00 PM, Marcus R. Epperson wrote:
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; I've seen the same thing when I build openmpi 1.4.3 with Intel 12,
</span><br>
<span class="quotelev3">&gt;&gt;&gt; but only when I have -O2 or -O3 in CFLAGS. If I drop it down to -O1
</span><br>
<span class="quotelev3">&gt;&gt;&gt; then the collectives hangs go away. I don't know what, if anything,
</span><br>
<span class="quotelev3">&gt;&gt;&gt; the higher optimization buys you when compiling openmpi, so I'm not
</span><br>
<span class="quotelev3">&gt;&gt;&gt; sure if that's an acceptable workaround or not.
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; My system is similar to yours - Intel X5570 with QDR Mellanox IB
</span><br>
<span class="quotelev3">&gt;&gt;&gt; running RHEL 5, Slurm, and these openmpi btls: openib,sm,self. I'm
</span><br>
<span class="quotelev3">&gt;&gt;&gt; using IMB 3.2.2 with a single iteration of Barrier to reproduce the
</span><br>
<span class="quotelev3">&gt;&gt;&gt; hang, and it happens 100% of the time for me when I invoke it like
</span><br>
<span class="quotelev3">&gt;&gt;&gt; this:
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; # salloc -N 9 orterun -n 65 ./IMB-MPI1 -npmin 64 -iter 1 barrier
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; The hang happens on the first Barrier (64 ranks) and each of the
</span><br>
<span class="quotelev3">&gt;&gt;&gt; participating ranks have this backtrace:
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; __poll (...)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; poll_dispatch () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; opal_event_loop () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; opal_progress () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; ompi_request_default_wait_all () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; ompi_coll_tuned_sendrecv_actual () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; ompi_coll_tuned_barrier_intra_recursivedoubling () from [instdir]/
</span><br>
<span class="quotelev3">&gt;&gt;&gt; lib/libmpi.so.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; ompi_coll_tuned_barrier_intra_dec_fixed () from [instdir]/lib/
</span><br>
<span class="quotelev3">&gt;&gt;&gt; libmpi.so.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; PMPI_Barrier () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; IMB_barrier ()
</span><br>
<span class="quotelev3">&gt;&gt;&gt; IMB_init_buffers_iter ()
</span><br>
<span class="quotelev3">&gt;&gt;&gt; main ()
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; The one non-participating rank has this backtrace:
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; __poll (...)
</span><br>
<span class="quotelev3">&gt;&gt;&gt; poll_dispatch () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; opal_event_loop () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; opal_progress () from [instdir]/lib/libopen-pal.so.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; ompi_request_default_wait_all () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; ompi_coll_tuned_sendrecv_actual () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; ompi_coll_tuned_barrier_intra_bruck () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; ompi_coll_tuned_barrier_intra_dec_fixed () from [instdir]/lib/
</span><br>
<span class="quotelev3">&gt;&gt;&gt; libmpi.so.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; PMPI_Barrier () from [instdir]/lib/libmpi.so.0
</span><br>
<span class="quotelev3">&gt;&gt;&gt; main ()
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; If I use more nodes I can get it to hang with 1ppn, so that seems
</span><br>
<span class="quotelev3">&gt;&gt;&gt; to rule out the sm btl (or interactions with it) as a culprit at
</span><br>
<span class="quotelev3">&gt;&gt;&gt; least.
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; I can't reproduce this with openmpi 1.5.3, interestingly.
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; -Marcus
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; On 05/10/2011 03:37 AM, Salvatore Podda wrote:
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Dear all,
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; we succeed in building several version of openmpi from 1.2.8 to
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; 1.4.3
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; with Intel composer XE 2011 (aka 12.0).
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; However we found a threshold in the number of cores (depending
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; from the
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; application: IMB, xhpl or user applications
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; and form the number of required cores) above which the application
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; hangs
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; (sort of deadlocks).
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; The building of openmpi with 'gcc' and 'pgi' does not show the
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; same limits.
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; There are any known incompatibilities of openmpi with this version
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; of
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; intel compiilers?
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; The characteristics of our computational infrastructure are:
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Intel processors E7330, E5345, E5530 e E5620
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; CentOS 5.3, CentOS 5.5.
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Intel composer XE 2011
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; gcc 4.1.2
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; pgi 10.2-1
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Regards
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Salvatore Podda
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; ENEA UTICT-HPC
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Department for Computer Science Development and ICT
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Facilities Laboratory for Science and High Performace Computing
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; C.R. Frascati
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Via E. Fermi, 45
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; PoBox 65
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; 00044 Frascati (Rome)
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Italy
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Tel: +39 06 9400 5342
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Fax: +39 06 9400 5551
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Fax: +39 06 9400 5735
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; E-mail: salvatore.podda_at_[hidden]
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; Home Page: www.cresco.enea.it
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; _______________________________________________
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; users mailing list
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev4">&gt;&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; _______________________________________________
</span><br>
<span class="quotelev3">&gt;&gt;&gt; users mailing list
</span><br>
<span class="quotelev3">&gt;&gt;&gt; users_at_[hidden]
</span><br>
<span class="quotelev3">&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; --
</span><br>
<span class="quotelev2">&gt;&gt; Jeff Squyres
</span><br>
<span class="quotelev2">&gt;&gt; jsquyres_at_[hidden]
</span><br>
<span class="quotelev2">&gt;&gt; For corporate legal information go to:
</span><br>
<span class="quotelev2">&gt;&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a>
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ==================================================
</span><br>
<span class="quotelev1">&gt; Investi nel futuro. Investi nelle nostre ricerche.
</span><br>
<span class="quotelev1">&gt; Destina il 5 x 1000 all'ENEA
</span><br>
<span class="quotelev1">&gt; Cerchiamo:
</span><br>
<span class="quotelev1">&gt; - nuove fonti e nuovi modi per produrre energia pulita e sicura.
</span><br>
<span class="quotelev1">&gt; - modi migliori per utilizzare e risparmiare energia.
</span><br>
<span class="quotelev1">&gt; - metodologie e tecnologie per innovare e rendere piu' competitivo il
</span><br>
<span class="quotelev1">&gt; sistema produttivo nazionale.
</span><br>
<span class="quotelev1">&gt; - metodologie e tecnologie per la salvaguardia e il recupero dell'ambiente e
</span><br>
<span class="quotelev1">&gt; per la tutela della nostra salute e del patrimonio artistico del Paese.
</span><br>
<span class="quotelev1">&gt; Il nostro codice fiscale e': 01320740580
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Message: 7
</span><br>
<span class="quotelev1">&gt; Date: Tue, 24 May 2011 16:09:34 +0100
</span><br>
<span class="quotelev1">&gt; From: Dave Love &lt;d.love_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Subject: Re: [OMPI users] btl_openib_cpc_include rdmacm questions
</span><br>
<span class="quotelev1">&gt; To: Open MPI Users &lt;users_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Message-ID: &lt;877h9gw2xd.fsf_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Content-Type: text/plain; charset=us-ascii
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Brock Palen &lt;brockp_at_[hidden]&gt; writes:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Well I have a new wrench into this situation.
</span><br>
<span class="quotelev2">&gt;&gt; We have a power failure at our datacenter took down our entire system
</span><br>
<span class="quotelev2">&gt;&gt; nodes,switch,sm.
</span><br>
<span class="quotelev2">&gt;&gt; Now I am unable to produce the error with oob default ibflags etc.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; As far as I know, we could still reproduce it.  Mail me if you need an
</span><br>
<span class="quotelev1">&gt; alternative, but we may have trouble getting access to the relevant
</span><br>
<span class="quotelev1">&gt; nodes.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; --
</span><br>
<span class="quotelev1">&gt; Excuse the typping -- I have a broken wrist
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Message: 8
</span><br>
<span class="quotelev1">&gt; Date: Tue, 24 May 2011 10:09:59 -0500
</span><br>
<span class="quotelev1">&gt; From: Rob Latham &lt;robl_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Subject: Re: [OMPI users] Trouble with MPI-IO
</span><br>
<span class="quotelev1">&gt; To: Open MPI Users &lt;users_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Message-ID: &lt;20110524150959.GA8746_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Content-Type: text/plain; charset=us-ascii
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; On Fri, May 20, 2011 at 08:14:07AM -0400, Jeff Squyres wrote:
</span><br>
<span class="quotelev2">&gt;&gt; On May 20, 2011, at 6:23 AM, Jeff Squyres wrote:
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt; &gt; Shouldn't ijlena and ijdisp be 1D arrays, not 2D arrays?
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Ok, if I convert ijlena and ijdisp to 1D arrays, I don't get the compile
</span><br>
<span class="quotelev2">&gt;&gt; error (even though they're allocatable -- so allocate was a red herring,
</span><br>
<span class="quotelev2">&gt;&gt; sorry).  That's all that &quot;use mpi&quot; is complaining about -- that the
</span><br>
<span class="quotelev2">&gt;&gt; function signatures didn't match.
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; use mpi is your friend -- even if you don't use F90 constructs much.
</span><br>
<span class="quotelev2">&gt;&gt; Compile-time checking is Very Good Thing (you were effectively &quot;getting
</span><br>
<span class="quotelev2">&gt;&gt; lucky&quot; by passing in the 2D arrays, I think).
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Attached is my final version.  And with this version, I see the hang when
</span><br>
<span class="quotelev2">&gt;&gt; running it with the &quot;T&quot; parameter.
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; That being said, I'm not an expert on the MPI IO stuff -- your code
</span><br>
<span class="quotelev2">&gt;&gt; *looks* right to me, but I could be missing something subtle in the
</span><br>
<span class="quotelev2">&gt;&gt; interpretation of MPI_FILE_SET_VIEW.  I tried running your code with MPICH
</span><br>
<span class="quotelev2">&gt;&gt; 1.3.2p1 and it also hung.
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Rob (ROMIO guy) -- can you comment this code?  Is it correct?
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; There's a kind of obscure but important rule in MPI-IO: the file view
</span><br>
<span class="quotelev1">&gt; must describe monotonically non-decreasing offsets.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; the T type creates a file type with the following flattened
</span><br>
<span class="quotelev1">&gt; representation (you can kind of think of the flattened representation
</span><br>
<span class="quotelev1">&gt; as a type map, except everything is in terms of bytes):
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; (0, 32), (96, 32), (32, 64)
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; So, 32 bytes at offset 0, 32 bytes at offset 96 and 64 bytes at offset
</span><br>
<span class="quotelev1">&gt; 32.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; That sort of looks like this:
</span><br>
<span class="quotelev1">&gt; |xxxx~~~~~~~~~~~~zzzz~~~~yyyy|
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; But you need the zzzz and yyyy pieces to be swapped in file view.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; It's an annoying part of the standard but as you can see if you
</span><br>
<span class="quotelev1">&gt; violate that ROMIO will go off and spin in an infinite loop looking
</span><br>
<span class="quotelev1">&gt; for the next piece of I/O (which in this case was &quot;behind&quot; the current
</span><br>
<span class="quotelev1">&gt; piece).
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; You can work around this by adjusting your memory datatype: data must
</span><br>
<span class="quotelev1">&gt; be read off of the disk in this monotonically non-decreasing order but
</span><br>
<span class="quotelev1">&gt; it can be jammed into memory any which way you want.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ROMIO should be better about reporting file views that violate this
</span><br>
<span class="quotelev1">&gt; part of the standard.  We report it in a few places but clearly not
</span><br>
<span class="quotelev1">&gt; enough.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ==rob
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; --
</span><br>
<span class="quotelev1">&gt; Rob Latham
</span><br>
<span class="quotelev1">&gt; Mathematics and Computer Science Division
</span><br>
<span class="quotelev1">&gt; Argonne National Lab, IL USA
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Message: 9
</span><br>
<span class="quotelev1">&gt; Date: Tue, 24 May 2011 10:13:18 -0500
</span><br>
<span class="quotelev1">&gt; From: Rob Latham &lt;robl_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Subject: Re: [OMPI users] reading from a file
</span><br>
<span class="quotelev1">&gt; To: Open MPI Users &lt;users_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Message-ID: &lt;20110524151318.GB8746_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Content-Type: text/plain; charset=us-ascii
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; On Sat, May 21, 2011 at 05:15:13PM +0530, sushil samant wrote:
</span><br>
<span class="quotelev2">&gt;&gt; hi all,
</span><br>
<span class="quotelev2">&gt;&gt;  i am a new comer in openmpi programing.i have a txt file containing
</span><br>
<span class="quotelev2">&gt;&gt; seven column each column contains double type data. What i want to do
</span><br>
<span class="quotelev2">&gt;&gt; is to read the file in parallel and find the average value and
</span><br>
<span class="quotelev2">&gt;&gt; standard deviation of each column using c++ and openmpi. If someone
</span><br>
<span class="quotelev2">&gt;&gt; can provide a sample program with explanation it will be very useful.
</span><br>
<span class="quotelev2">&gt;&gt; And if understand it i would like to do it for .h5 file.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; MPI-IO does not do formatted I/O.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; You should just start with the .h5 (HDF5 ? ) file, where decomposing
</span><br>
<span class="quotelev1">&gt; the dataset over N processors will be more straightforward.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ==rob
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; --
</span><br>
<span class="quotelev1">&gt; Rob Latham
</span><br>
<span class="quotelev1">&gt; Mathematics and Computer Science Division
</span><br>
<span class="quotelev1">&gt; Argonne National Lab, IL USA
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Message: 10
</span><br>
<span class="quotelev1">&gt; Date: Tue, 24 May 2011 16:19:59 +0100
</span><br>
<span class="quotelev1">&gt; From: Dave Love &lt;d.love_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Subject: Re: [OMPI users] Openib with &gt; 32 cores per node
</span><br>
<span class="quotelev1">&gt; To: users_at_[hidden]
</span><br>
<span class="quotelev1">&gt; Message-ID: &lt;8762p0w2g0.fsf_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Content-Type: text/plain; charset=us-ascii
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Jeff Squyres &lt;jsquyres_at_[hidden]&gt; writes:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Assuming you built OMPI with PSM support:
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt;     mpirun --mca pml cm --mca mtl psm ....
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; (although probably just the pml/cm setting is sufficient -- the mtl/psm
</span><br>
<span class="quotelev2">&gt;&gt; option will probably happen automatically)
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; For what it's worth, you needn't specify anything to get psm used if
</span><br>
<span class="quotelev1">&gt; it's available
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; --
</span><br>
<span class="quotelev1">&gt; Excuse the typping -- I have a broken wrist
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Message: 11
</span><br>
<span class="quotelev1">&gt; Date: Tue, 24 May 2011 08:31:23 -0700
</span><br>
<span class="quotelev1">&gt; From: Tom Rosmond &lt;rosmond_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Subject: Re: [OMPI users] Trouble with MPI-IO
</span><br>
<span class="quotelev1">&gt; To: Open MPI Users &lt;users_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Message-ID: &lt;1306251083.4275.4.camel_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Content-Type: text/plain
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Rob,
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Thanks for the clarification.  I had seen that point about
</span><br>
<span class="quotelev1">&gt; non-decreasing offsets in the standard and it was just beginning to dawn
</span><br>
<span class="quotelev1">&gt; on me that maybe it was my problem.  I will rethink my mapping strategy
</span><br>
<span class="quotelev1">&gt; to comply with the restriction.  Thanks again.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; T. Rosmond
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; On Tue, 2011-05-24 at 10:09 -0500, Rob Latham wrote:
</span><br>
<span class="quotelev2">&gt;&gt; On Fri, May 20, 2011 at 08:14:07AM -0400, Jeff Squyres wrote:
</span><br>
<span class="quotelev3">&gt;&gt; &gt; On May 20, 2011, at 6:23 AM, Jeff Squyres wrote:
</span><br>
<span class="quotelev3">&gt;&gt; &gt;
</span><br>
<span class="quotelev4">&gt;&gt; &gt; &gt; Shouldn't ijlena and ijdisp be 1D arrays, not 2D arrays?
</span><br>
<span class="quotelev3">&gt;&gt; &gt;
</span><br>
<span class="quotelev3">&gt;&gt; &gt; Ok, if I convert ijlena and ijdisp to 1D arrays, I don't get the compile
</span><br>
<span class="quotelev3">&gt;&gt; &gt; error (even though they're allocatable -- so allocate was a red herring,
</span><br>
<span class="quotelev3">&gt;&gt; &gt; sorry).  That's all that &quot;use mpi&quot; is complaining about -- that the
</span><br>
<span class="quotelev3">&gt;&gt; &gt; function signatures didn't match.
</span><br>
<span class="quotelev3">&gt;&gt; &gt;
</span><br>
<span class="quotelev3">&gt;&gt; &gt; use mpi is your friend -- even if you don't use F90 constructs much.
</span><br>
<span class="quotelev3">&gt;&gt; &gt; Compile-time checking is Very Good Thing (you were effectively &quot;getting
</span><br>
<span class="quotelev3">&gt;&gt; &gt; lucky&quot; by passing in the 2D arrays, I think).
</span><br>
<span class="quotelev3">&gt;&gt; &gt;
</span><br>
<span class="quotelev3">&gt;&gt; &gt; Attached is my final version.  And with this version, I see the hang
</span><br>
<span class="quotelev3">&gt;&gt; &gt; when running it with the &quot;T&quot; parameter.
</span><br>
<span class="quotelev3">&gt;&gt; &gt;
</span><br>
<span class="quotelev3">&gt;&gt; &gt; That being said, I'm not an expert on the MPI IO stuff -- your code
</span><br>
<span class="quotelev3">&gt;&gt; &gt; *looks* right to me, but I could be missing something subtle in the
</span><br>
<span class="quotelev3">&gt;&gt; &gt; interpretation of MPI_FILE_SET_VIEW.  I tried running your code with
</span><br>
<span class="quotelev3">&gt;&gt; &gt; MPICH 1.3.2p1 and it also hung.
</span><br>
<span class="quotelev3">&gt;&gt; &gt;
</span><br>
<span class="quotelev3">&gt;&gt; &gt; Rob (ROMIO guy) -- can you comment this code?  Is it correct?
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; There's a kind of obscure but important rule in MPI-IO: the file view
</span><br>
<span class="quotelev2">&gt;&gt; must describe monotonically non-decreasing offsets.
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; the T type creates a file type with the following flattened
</span><br>
<span class="quotelev2">&gt;&gt; representation (you can kind of think of the flattened representation
</span><br>
<span class="quotelev2">&gt;&gt; as a type map, except everything is in terms of bytes):
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; (0, 32), (96, 32), (32, 64)
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; So, 32 bytes at offset 0, 32 bytes at offset 96 and 64 bytes at offset
</span><br>
<span class="quotelev2">&gt;&gt; 32.
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; That sort of looks like this:
</span><br>
<span class="quotelev2">&gt;&gt; |xxxx~~~~~~~~~~~~zzzz~~~~yyyy|
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; But you need the zzzz and yyyy pieces to be swapped in file view.
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; It's an annoying part of the standard but as you can see if you
</span><br>
<span class="quotelev2">&gt;&gt; violate that ROMIO will go off and spin in an infinite loop looking
</span><br>
<span class="quotelev2">&gt;&gt; for the next piece of I/O (which in this case was &quot;behind&quot; the current
</span><br>
<span class="quotelev2">&gt;&gt; piece).
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; You can work around this by adjusting your memory datatype: data must
</span><br>
<span class="quotelev2">&gt;&gt; be read off of the disk in this monotonically non-decreasing order but
</span><br>
<span class="quotelev2">&gt;&gt; it can be jammed into memory any which way you want.
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; ROMIO should be better about reporting file views that violate this
</span><br>
<span class="quotelev2">&gt;&gt; part of the standard.  We report it in a few places but clearly not
</span><br>
<span class="quotelev2">&gt;&gt; enough.
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; ==rob
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; _______________________________________________
</span><br>
<span class="quotelev1">&gt; users mailing list
</span><br>
<span class="quotelev1">&gt; users_at_[hidden]
</span><br>
<span class="quotelev1">&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; End of users Digest, Vol 1914, Issue 1
</span><br>
<span class="quotelev1">&gt; **************************************
</span><br>
<span class="quotelev1">&gt;
</span><br>
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="16618.php">Faisal: "Re: [OMPI users] openmpi self checkpointing - error while running	example"</a>
<li><strong>Previous message:</strong> <a href="16616.php">Marcus R. Epperson: "Re: [OMPI users] openmpi (1.2.8 or above) and Intel composer XE 2011 (aka 12.0)"</a>
<!-- nextthread="start" -->
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
