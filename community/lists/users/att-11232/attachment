<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">Bingo! This is why we ask for info on how you configure OMPI :-)<div><br></div><div>You need to rebuild OMPI with --enable-heterogeneous. Because there is additional overhead associated with running hetero configurations, and so few people do so, it is disabled by default.</div><div><br></div><div><br></div><div><div><div>On Nov 18, 2009, at 2:55 AM, Laurin Müller wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="MARGIN: 4px 4px 1px; FONT: 10pt Tahoma">
<div>Now i have the same openmpi versions. 1.3.2</div>
<div>&nbsp;</div>
<div>recalulated on both nodes and it works again on each node seperatly:</div>
<div>&nbsp;</div>
<div>node1:</div>
<div>cluster@bioclust:/mnt/projects/PS3Cluster/Benchmark$ mpirun --version<br>mpirun (Open MPI) 1.3.2</div>
<div><a href="mailto:1.3.2cluster@bioclust:/mnt/projects/PS3Cluster/Benchmark$">cluster@bioclust:/mnt/projects/PS3Cluster/Benchmark$</a> mpirun --hostfile /etc/openmpi/openmpi-default-hostfile -np 4 /mnt/projects/PS3Cluster/Benchmark/pi<br>Input number of intervals:<br>20<br>1: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.798498008827023<br>2: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.773339953424083<br>3: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.747089984650041<br>0: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.822248040052981<br>pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.141175986954128<br></div>
<div>node2 (PS3):</div>
<div><a href="mailto:root@kasimir:/mnt/projects/PS3Cluster/Benchmark">root@kasimir:/mnt/projects/PS3Cluster/Benchmark</a># mpirun --version<br>mpirun (Open MPI) 1.3.2</div>
<div>[...]</div>
<div><a href="mailto:root@kasimir:/mnt/projects/PS3Cluster/Benchmark">root@kasimir:/mnt/projects/PS3Cluster/Benchmark</a># mpirun -np 2 pi<br>Input number of intervals:<br>20<br>0: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.595587993477064<br>1: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.545587993477064<br>pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.141175986954128<br></div>
<div>BUT when i start it on node1 with more than 16 processes and hostfile. i get this errors:</div>
<div><a href="mailto:cluster@bioclust:/mnt/projects/PS3Cluster/Benchmark$">cluster@bioclust:/mnt/projects/PS3Cluster/Benchmark$</a> mpirun --hostfile /etc/openmpi/openmpi-default-hostfile -np 17 /mnt/projects/PS3Cluster/Benchmark/pi<br>--------------------------------------------------------------------------<br>This installation of Open MPI was configured without support for<br>heterogeneous architectures, but at least one node in the allocation<br>was detected to have a different architecture. The detected node was:</div>
<div>&nbsp;</div>
<div>Node: bioclust</div>
<div>&nbsp;</div>
<div>In order to operate in a heterogeneous environment, please reconfigure<br>Open MPI with --enable-heterogeneous.<br>--------------------------------------------------------------------------<br>--------------------------------------------------------------------------<br>It looks like MPI_INIT failed for some reason; your parallel process is<br>likely to abort.&nbsp; There are many reasons that a parallel process can<br>fail during MPI_INIT; some of which are due to configuration or environment<br>problems.&nbsp; This failure appears to be an internal failure; here's some<br>additional information (which may only be relevant to an Open MPI<br>developer):</div>
<div>&nbsp;</div>
<div>&nbsp; ompi_proc_set_arch failed<br>&nbsp; --&gt; Returned "Not supported" (-8) instead of "Success" (0)<br>--------------------------------------------------------------------------<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[bioclust:1239] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[bioclust:1240] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[bioclust:1241] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[bioclust:1242] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[bioclust:1244] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[bioclust:1245] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[bioclust:1246] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[bioclust:1247] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[bioclust:1248] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[bioclust:1250] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[bioclust:1251] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[bioclust:1238] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[kasimir:12678] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[bioclust:1243] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[bioclust:1249] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[bioclust:1252] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[bioclust:1253] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>--------------------------------------------------------------------------<br>mpirun has exited due to process rank 16 with PID 12678 on<br>node 10.4.1.23 exiting without calling "finalize". This may<br>have caused other processes in the application to be<br>terminated by signals sent by mpirun (as reported here).<br>--------------------------------------------------------------------------<br>[bioclust:01236] 16 more processes have sent help message help-mpi-runtime / heterogeneous-support-unavailable<br>[bioclust:01236] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages<br>[bioclust:01236] 16 more processes have sent help message help-mpi-runtime / mpi_init:startup:internal-failure<br></div>
<div>&nbsp;</div>
<div>&nbsp;</div>
<div>&nbsp;</div>
<div><br><br>&gt;&gt;&gt; Lenny Verkhovsky &lt;<a href="mailto:lenny.verkhovsky@gmail.com">lenny.verkhovsky@gmail.com</a>&gt; 17.11.2009 16:52 &gt;&gt;&gt;<br></div>
<div dir="ltr">
<div>I noticed that you also have different versions of OMPI. You have 1.3.2 on node1 and 1.3 on node2.</div>
<div>can you try to put same versions of OMPI on both nodes.</div>
<div>can you also try running np 16 on node1 when you try running separately.</div>
<div></div>
<div>Lenny.</div><br>
<div class="gmail_quote">On Tue, Nov 17, 2009 at 5:45 PM, Laurin Müller <span dir="ltr">&lt;<a href="mailto:laurin.mueller@umit.at">laurin.mueller@umit.at</a>&gt;</span> wrote:<br>
<blockquote style="BORDER-LEFT: #ccc 1px solid; MARGIN: 0px 0px 0px 0.8ex; PADDING-LEFT: 1ex" class="gmail_quote">
<div style="FONT-FAMILY: Tahoma, sans-serif; FONT-SIZE: 13px"><br><br>&gt;&gt;&gt; Ralph Castain 11/17/09 4:04 PM &gt;&gt;&gt;
<div class="im"><br>&gt;Your cmd line is telling OMPI to run 17 processes. Since your hostfile indicates that only 16 of them are to &gt;run on 10.4.23.107 (which I assume is your PS3 node?), 1 process is going to be run on 10.4.1.23 (I assume &gt;this is node1?).<br></div>node1 has 16 Cores (4 x AMD Quad Core Processors)<br><br>node2 is the ps3 with two processors (slots)
<div class="im"><br>
<div><br></div>
<div>&gt;I would guess that the executable is compiled to run on the PS3 given your specified path, so I would &gt;expect it to bomb on node1 - which is exactly what appears to be happening.</div></div>
<div>the executable is compiled on each node separately and lies at each node in the same directory
<div class="im"><br>/mnt/projects/PS3Cluster/Benchmark/pi<br></div>on each node different directories are mounted. so there exists a separate executable file compiled at each node.<br><br>in the end i want to ran R on this cluster with Rmpi - as i get a similar problem there i rist wanted to try with an c programm.<br><br>with r happens the same thing it works when i start it on each node but if i want to start more than 16 processes on node one in exits.<br><br></div>
<div>
<div class="h5">
<div><br>
<div>
<div>On Nov 17, 2009, at 1:59 AM, Laurin Müller wrote:</div><br>
<blockquote type="cite">
<div style="MARGIN: 4px 4px 1px; FONT: 10pt Tahoma; font-size-adjust: none; font-stretch: normal">
<div>Hi,</div>
<div></div>
<div>i want to build a cluster with openmpi.</div>
<div></div>
<div>2 nodes:</div>
<div>node 1: 4 x Amd Quad Core, ubuntu 9.04, openmpi 1.3.2</div>
<div>node 2: Sony PS3, ubuntu 9.04, openmpi 1.3</div>
<div></div>
<div>both can connect with ssh to each other and to itself without passwd.</div>
<div></div>
<div>I can run the sample proramm pi.c on both nodes seperatly (see below). But if i try to start it on node1 with --hostfile option to use node 2 "remote" i got this error:</div>
<div></div>
<div><a href="mailto:cluster@bioclust:%7E$" target="_blank">cluster@bioclust:~$</a> mpirun --hostfile /etc/openmpi/openmpi-default-hostfile -np 17 /mnt/projects/PS3Cluster/Benchmark/pi<br>--------------------------------------------------------------------------<br>mpirun noticed that the job aborted, but has no info as to the process<br>that caused that situation.<br>--------------------------------------------------------------------------<br></div>
<div>my hostfile:</div>
<div><a href="mailto:cluster@bioclust:%7E$" target="_blank">cluster@bioclust:~$</a> cat /etc/openmpi/openmpi-default-hostfile</div>
<div>10.4.23.107 slots=16<br>10.4.1.23 slots=2<br></div>
<div>i can see with top that the processors of node2 begin to work shortly, then it apports on node1.</div>
<div></div>
<div>I use this sample/test program:</div>
<div>#include &lt;stdio.h&gt;<br>#include &lt;stdlib.h&gt;</div>
<div>#include "mpi.h"</div>
<div>int main(int argc, char *argv[])<br>{<br>int i, n;<br>double h, pi, x;</div>
<div>int me, nprocs;<br>double piece;</div>
<div>/* --------------------------------------------------- */</div>
<div>MPI_Init (&amp;argc, &amp;argv);</div>
<div>MPI_Comm_size (MPI_COMM_WORLD, &amp;nprocs);<br>MPI_Comm_rank (MPI_COMM_WORLD, &amp;me);</div>
<div>/* --------------------------------------------------- */</div>
<div>if (me == 0)<br>{<br>printf("%s", "Input number of intervals:\n");<br>scanf ("%d", &amp;n);<br>}</div>
<div>/* --------------------------------------------------- */</div>
<div>MPI_Bcast (&amp;n, 1, MPI_INT, 0, MPI_COMM_WORLD);</div>
<div>/* --------------------------------------------------- */</div>
<div>h = 1. / (double) n;</div>
<div>piece = 0.;</div>
<div>for (i=me+1; i &lt;= n; i+=nprocs)<br>{<br>x = (i-1)*h;</div>
<div>piece = piece + ( 4/(1+(x)*(x)) + 4/(1+(x+h)*(x+h))) / 2 * h;<br>}</div>
<div>printf("%d: pi = %25.15f\n", me, piece);</div>
<div>/* --------------------------------------------------- */</div>
<div>MPI_Reduce (&amp;piece, &amp;pi, 1, MPI_DOUBLE,<br>MPI_SUM, 0, MPI_COMM_WORLD);</div>
<div>/* --------------------------------------------------- */</div>
<div>if (me == 0)<br>{<br>printf("pi = %25.15f\n", pi);<br>}</div>
<div>/* --------------------------------------------------- */</div>
<div>MPI_Finalize();</div>
<div>return 0;<br>}<br></div>
<div>it works on each node.</div>
<div>node1:</div>
<div><a href="mailto:cluster@bioclust:%7E$" target="_blank">cluster@bioclust:~$</a> mpirun -np 4 /mnt/projects/PS3Cluster/Benchmark/piInput number of intervals:<br>20<br>0: pi = 0.822248040052981<br>2: pi = 0.773339953424083<br>3: pi = 0.747089984650041<br>1: pi = 0.798498008827023<br>pi = 3.141175986954128</div>
<div></div>
<div>node2:</div>
<div><a href="mailto:cluster@kasimir:%7E$" target="_blank">cluster@kasimir:~$</a> mpirun -np 2 /mnt/projects/PS3Cluster/Benchmark/pi<br>Input number of intervals:<br>5<br>1: pi = 1.267463056905495<br>0: pi = 1.867463056905495<br>pi = 3.134926113810990<br><a href="mailto:cluster@kasimir:%7E$" target="_blank">cluster@kasimir:~$</a> </div>
<div></div>
<div>Thx in advance,</div>
<div>Laurin<br></div>
<div><br></div>
<div></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote></div><br></div></div></div></div><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
