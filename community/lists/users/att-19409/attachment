<html><body><div style="color:#000; background-color:#fff; font-family:times new roman, new york, times, serif;font-size:10pt"><div><span><br></span></div>Running with enabled shared memory gave me the following error.<br><br>mpprun INFO: Starting openmpi run on 2 nodes (16 ranks)...<br>--------------------------------------------------------------------------<br>A requested component was not found, or was unable to be opened.&nbsp; This<br>means that this component is either not installed or is unable to be<br>used on your system (e.g., sometimes this means that shared libraries<br>that the component requires are unable to be found/loaded).&nbsp; Note that<br>Open MPI stopped checking at the first component that it did not find.<br><br>Host:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; n568<br>Framework: btl<br>Component: tcp<br>----------------<br><br>may be it is not installed on our supercomputing center. What do you suggest ?<br><br>best
 regards,<br><div><br></div>  <div style="font-family: times new roman, new york, times, serif; font-size: 10pt;"> <div style="font-family: times new roman, new york, times, serif; font-size: 12pt;"> <div dir="ltr"> <font face="Arial" size="2"> ----- Forwarded Message -----<br>  <b><span style="font-weight:bold;">From:</span></b> Mudassar Majeed &lt;mudassarm30@yahoo.com&gt;<br> <b><span style="font-weight: bold;">To:</span></b> Jeff Squyres &lt;jsquyres@cisco.com&gt; <br> <b><span style="font-weight: bold;">Sent:</span></b> Friday, June 1, 2012 5:03 PM<br> <b><span style="font-weight: bold;">Subject:</span></b> Re: [OMPI users] Intra-node communication<br> </font> </div> <br>
<div id="yiv1912828020"><div><div style="color:#000;background-color:#fff;font-family:times new roman, new york, times, serif;font-size:10pt;"><div>Here is the code, I am taking care of the first message. I start measuring the round trip time from second message. If you see in the code I do 100 hand shakes and find the overall time for them. I have two nodes each having 8 cores ...... first I do exchange of messages between process 1 to process 2 because they are on the same node and measure the time. Then I do messages exchange between process 1 and 12 as they are on different nodes. But the output I got is as follows, <br><br>---------------------------------------------------------------------------------
<br>mpprun INFO: Starting openmpi run on 2 nodes (16 ranks)...
<br>
<br>with-in node: time = 150.663382 secs
<br>across nodes: time = 134.627887 secs
<br>---------------------------------------------------------------------------------
<br></div><div><br></div><div>the code is as follows, <br></div><div><br></div><div>double *buff = NULL;<br>&nbsp;&nbsp;&nbsp; double ex_time = 0.0f;<br>&nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; buff = new double[1000000];<br>&nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; for(i=0;i&lt;1000000;i++)<br>&nbsp;&nbsp;&nbsp; *(buff+i) = 100.5352f;<br>&nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>&nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; int comm_amount = 100;//*(comm + my_rank * N + i);<br>&nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; if(comm_amount &gt; 0)<br>&nbsp;&nbsp;&nbsp; {<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; if(my_rank == 1)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; {<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; for(int j=0;j&lt;comm_amount;j++)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; {<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; if(j&gt;0)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
 {<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; clock_gettime(CLOCK_REALTIME, &amp;stime);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; MPI_Ssend((void*)buff, 1000000, MPI_DOUBLE, 2, 4600, MPI_COMM_WORLD);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; MPI_Recv((void*)buff, 1000000, MPI_DOUBLE, 2, 4600, MPI_COMM_WORLD, &amp;status);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; if(j&gt;0)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; {<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; clock_gettime(CLOCK_REALTIME, &amp;etime);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; ex_time = ex_time + (etime.tv_sec&nbsp; - stime.tv_sec) + 1e-9*(etime.tv_nsec&nbsp; -
 stime.tv_nsec);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; else if(my_rank == 2)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; {&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; for(int j=0;j&lt;comm_amount;j++)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; {<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; if(j&gt;0)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; {<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; clock_gettime(CLOCK_REALTIME, &amp;stime);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; MPI_Recv((void*)buff, 1000000, MPI_DOUBLE, 1, 4600, MPI_COMM_WORLD, &amp;status);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
 &nbsp;&nbsp;&nbsp; MPI_Ssend((void*)buff, 1000000, MPI_DOUBLE, 1, 4600, MPI_COMM_WORLD);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; if(j&gt;0)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; {<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; clock_gettime(CLOCK_REALTIME, &amp;etime);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; ex_time = ex_time + (etime.tv_sec&nbsp; - stime.tv_sec) + 1e-9*(etime.tv_nsec&nbsp; - stime.tv_nsec);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; if(my_rank == 1)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; printf("\nwith-in node: time = %f\n", ex_time);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
 ex_time = 0.0f;<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; if(my_rank == 1)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; {<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; for(int j=0;j&lt;comm_amount;j++)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; {<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; if(j&gt;0)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; {<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; clock_gettime(CLOCK_REALTIME, &amp;stime);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; MPI_Ssend((void*)buff, 1000000, MPI_DOUBLE, 12, 4600, MPI_COMM_WORLD);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; MPI_Recv((void*)buff, 1000000, MPI_DOUBLE, 12, 4600, MPI_COMM_WORLD, &amp;status);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
 &nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; if(j&gt;0)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; {<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; clock_gettime(CLOCK_REALTIME, &amp;etime);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; ex_time = ex_time + (etime.tv_sec&nbsp; - stime.tv_sec) + 1e-9*(etime.tv_nsec&nbsp; - stime.tv_nsec);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; else if(my_rank == 12)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; {&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; for(int j=0;j&lt;comm_amount;j++)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; {<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; if(j&gt;0)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
 {<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; clock_gettime(CLOCK_REALTIME, &amp;stime);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; MPI_Recv((void*)buff, 1000000, MPI_DOUBLE, 1, 4600, MPI_COMM_WORLD, &amp;status);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; MPI_Ssend((void*)buff, 1000000, MPI_DOUBLE, 1, 4600, MPI_COMM_WORLD);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; if(j&gt;0)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; {<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; clock_gettime(CLOCK_REALTIME, &amp;etime);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; ex_time = ex_time + (etime.tv_sec&nbsp; - stime.tv_sec) + 1e-9*(etime.tv_nsec&nbsp; -
 stime.tv_nsec);<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; <br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; if(my_rank == 1)<br>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; printf("\nacross nodes: time = %f\n", ex_time);<br>&nbsp;&nbsp;&nbsp; }</div><div><br></div><div><br></div><div><br></div><div>This time I have added <a rel="nofollow" name="sm-btl"><pre><font color="red">-mca btl self,sm,tcp<br></font></pre></a>may be it will enable the shared memory support. But i had to do with mprun (not mpirun) as I have to submit job and can't use mpirun directly on supercomputer.<br></div><div>thanks for your help,</div><div><br></div><div>best <br></div><div><br></div><div><br></div><div><br></div>  <div style="font-family:times new roman, new york, times, serif;font-size:10pt;"> <div style="font-family:times new roman, new york,
 times, serif;font-size:12pt;"> <div dir="ltr"> <font face="Arial" size="2"> <hr size="1">  <b><span style="font-weight:bold;">From:</span></b> Jeff Squyres &lt;jsquyres@cisco.com&gt;<br> <b><span style="font-weight:bold;">To:</span></b> Open MPI Users &lt;users@open-mpi.org&gt; <br><b><span style="font-weight:bold;">Cc:</span></b> Mudassar Majeed &lt;mudassarm30@yahoo.com&gt; <br> <b><span style="font-weight:bold;">Sent:</span></b> Friday, June 1, 2012 4:52 PM<br> <b><span style="font-weight:bold;">Subject:</span></b> Re: [OMPI users] Intra-node communication<br> </font> </div> <br>
...and exactly how you measured.&nbsp; You might want to run a well-known benchmark, like NetPIPE or the OSU pt2pt benchmarks.<br><br>Note that the *first* send between any given peer pair is likely to be slow because OMPI does a lazy connection scheme (i.e., the connection is made behind the scenes).&nbsp; Subsequent sends are likely faster.&nbsp; Well-known benchmarks do a bunch of warmup sends and then start timing after those are all done.<br><br>Also ensure that you have shared memory support enabled.&nbsp; It is likely to be enabled by default, but if you're seeing different performance than you expect, that's something to check.<br><br><br>On Jun 1, 2012, at 10:44 AM, Jingcha Joba wrote:<br><br>&gt; This should not happen. Typically, Intra node communication latency are way way cheaper than inter node.<br>&gt; Can you please tell us how u ran your application ?<br>&gt; Thanks <br>&gt; <br>&gt; --<br>&gt; Sent from my iPhone<br>&gt; <br>&gt; On Jun
 1, 2012, at 7:34 AM, Mudassar Majeed &lt;<a rel="nofollow" ymailto="mailto:mudassarm30@yahoo.com" target="_blank" href="mailto:mudassarm30@yahoo.com">mudassarm30@yahoo.com</a>&gt; wrote:<br>&gt; <br>&gt;&gt; Dear MPI people, <br>&gt;&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Can someone tell me why MPI_Ssend takes more time when two MPI processes are on same node ...... ?? the same two processes on different nodes take much less time for the same message exchange. I am using a supercomputing center and this happens. I was writing an algorithm to reduce the across node communication. But now I found that across node communication is cheaper than communication within a node (with 8 cores on each node).<br>&gt;&gt; <br>&gt;&gt; best regards,<br>&gt;&gt; <br>&gt;&gt; Mudassar<br>&gt;&gt; _______________________________________________<br>&gt;&gt; users mailing list<br>&gt;&gt; <a
 rel="nofollow" ymailto="mailto:users@open-mpi.org" target="_blank" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a rel="nofollow" ymailto="mailto:users@open-mpi.org" target="_blank" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a rel="nofollow" target="_blank" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><br>-- <br>Jeff Squyres<br><a rel="nofollow" ymailto="mailto:jsquyres@cisco.com" target="_blank" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>For corporate legal information go to: http://www.cisco.com/web/about/doing_business/legal/cri/<br><br><br><br> </div> </div>  </div></div></div><br><br> </div> </div>  </div></body></html>
