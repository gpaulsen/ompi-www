<div dir="ltr">Thankyou Ralph.   i guess the information I did not have in my head was that   core = physical core (not hyperthreaded core)</div><div class="gmail_extra"><br><div class="gmail_quote">On 18 July 2016 at 14:45, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">It sounds like you just want to bind procs to cores since each core is composed of 2 HTs. So a simple “--map-by core --bind-to core&quot; should do the trick.<div><br></div><div>FWIW: the affinity settings are controlled by the bind-to &lt;foo&gt; option. You can use “mpirun -h”  to get the list of supported options and a little explanation:</div><div><br></div><div><dt><b>--bind-to
&lt;foo&gt;</b> </dt>
<dd>Bind processes to the specified object, defaults to <i>core</i>. Supported
options include slot, hwthread, core, l1cache, l2cache, l3cache, socket,
numa, board, and none.  </dd><div><br></div></div><div><a href="https://www.open-mpi.org/doc/current/man1/mpirun.1.php#sect9" target="_blank">https://www.open-mpi.org/doc/current/man1/mpirun.1.php#sect9</a></div><div><br></div><div><br></div><div><br><div><br><div><blockquote type="cite"><div><div class="h5"><div>On Jul 17, 2016, at 11:25 PM, John Hearns &lt;<a href="mailto:hearnsj@googlemail.com" target="_blank">hearnsj@googlemail.com</a>&gt; wrote:</div><br></div></div><div><div><div class="h5"><div dir="ltr">Please can someone point me towards the affinity settings for:<div>OpenMPI 1.10   used with Slurm version 15</div><div><br></div><div>I have some nodes with 2630-v4 processors.</div><div>So 10 cores per socket / 20 hyperthreads</div><div>Hyperthreading is enabled.</div><div>I would like to set affinity for 20 processes per node,<br></div><div>so that the processes are pinned to every second HT core - ie one process per physical thread.</div><div><br></div><div>I&#39;m sure this is quite easy...</div><div><br></div><div>Thankyou</div></div></div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/07/29674.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/07/29674.php</a></div></blockquote></div><br></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/07/29676.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/07/29676.php</a><br></blockquote></div><br></div>

