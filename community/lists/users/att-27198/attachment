<p dir="ltr">Hi Nick</p>
<p dir="ltr">use master not 1.8.x. for cray xc.  also for config do not pay attention to cray/lanl platform files.  just do config.  also if using nativized slurm launch with srun not mpirun.</p>
<p dir="ltr">howard</p>
<p dir="ltr">----------</p>
<p dir="ltr">sent from my smart phonr so no good type.</p>
<p dir="ltr">Howard</p>
<div class="gmail_quote">On Jun 25, 2015 2:56 PM, &quot;Nick Radcliffe&quot; &lt;<a href="mailto:nradclif@cray.com">nradclif@cray.com</a>&gt; wrote:<br type="attribution"><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi,<br>
<br>
I&#39;m trying to build and run Open MPI 1.8.5 with native ugni on a Cray XC. The build works, but I&#39;m getting this error when I run:<br>
<br>
nradclif@kay:/lus/scratch/nradclif&gt; aprun -n 2 -N 1 ./osu_latency<br>
[nid00014:28784] [db_pmi.c:174:pmi_commit_packed] PMI_KVS_Put: Operation failed<br>
[nid00014:28784] [db_pmi.c:457:commit] PMI_KVS_Commit: Operation failed<br>
[nid00012:12788] [db_pmi.c:174:pmi_commit_packed] PMI_KVS_Put: Operation failed<br>
[nid00012:12788] [db_pmi.c:457:commit] PMI_KVS_Commit: Operation failed<br>
# OSU MPI Latency Test<br>
# Size            Latency (us)<br>
osu_latency: btl_ugni_endpoint.c:87: mca_btl_ugni_ep_connect_start: Assertion `0&#39; failed.<br>
[nid00012:12788] *** Process received signal ***<br>
[nid00012:12788] Signal: Aborted (6)<br>
[nid00012:12788] Signal code:  (-6)<br>
[nid00012:12788] [ 0] /lib64/libpthread.so.0(+0xf850)[0x2aaaab42b850]<br>
[nid00012:12788] [ 1] /lib64/libc.so.6(gsignal+0x35)[0x2aaaab66b885]<br>
[nid00012:12788] [ 2] /lib64/libc.so.6(abort+0x181)[0x2aaaab66ce61]<br>
[nid00012:12788] [ 3] /lib64/libc.so.6(__assert_fail+0xf0)[0x2aaaab664740]<br>
[nid00012:12788] [ 4] /lus/scratch/nradclif/openmpi_install/lib/libmpi.so.1(mca_btl_ugni_ep_connect_progress+0x6c9)[0x2aaaaaff9869]<br>
[nid00012:12788] [ 5] /lus/scratch/nradclif/openmpi_install/lib/libmpi.so.1(+0x5ae32)[0x2aaaaaf46e32]<br>
[nid00012:12788] [ 6] /lus/scratch/nradclif/openmpi_install/lib/libmpi.so.1(mca_btl_ugni_sendi+0x8bd)[0x2aaaaaffaf7d]<br>
[nid00012:12788] [ 7] /lus/scratch/nradclif/openmpi_install/lib/libmpi.so.1(+0x1f0c17)[0x2aaaab0dcc17]<br>
[nid00012:12788] [ 8] /lus/scratch/nradclif/openmpi_install/lib/libmpi.so.1(mca_pml_ob1_isend+0xa8)[0x2aaaab0dd488]<br>
[nid00012:12788] [ 9] /lus/scratch/nradclif/openmpi_install/lib/libmpi.so.1(ompi_coll_tuned_barrier_intra_two_procs+0x11b)[0x2aaaab07e84b]<br>
[nid00012:12788] [10] /lus/scratch/nradclif/openmpi_install/lib/libmpi.so.1(PMPI_Barrier+0xb6)[0x2aaaaaf8a7c6]<br>
[nid00012:12788] [11] ./osu_latency[0x401114]<br>
[nid00012:12788] [12] /lib64/libc.so.6(__libc_start_main+0xe6)[0x2aaaab657c36]<br>
[nid00012:12788] [13] ./osu_latency[0x400dd9]<br>
[nid00012:12788] *** End of error message ***<br>
osu_latency: btl_ugni_endpoint.c:87: mca_btl_ugni_ep_connect_start: Assertion `0&#39; failed.<br>
<br>
<br>
Here&#39;s how I build:<br>
<br>
export FC=ftn         (I&#39;m not using Fortran, but the configure fails if it can&#39;t find a Fortran compiler)<br>
./configure --prefix=/lus/scratch/nradclif/openmpi_install --enable-mpi-fortran=none --with-platform=contrib/platform/lanl/cray_xe6/debug-lustre<br>
make install<br>
<br>
I didn&#39;t modify the debug-lustre file, but I did change cray-common to remove the hard-coding, e.g., rather than using the gemini-specific path &quot;with_pmi=/opt/cray/pmi/2.1.4-1.0000.8596.8.9.gem&quot;, I used &quot;with_pmi=/opt/cray/pmi/default&quot;.<br>
<br>
I&#39;ve tried running different executables with different numbers of ranks/nodes, but they all seem to run into problems with PMI_KVS_Put.<br>
<br>
Any ideas what could be going wrong?<br>
<br>
Thanks for any help,<br>
Nick<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/06/27197.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/06/27197.php</a><br>
</blockquote></div>

