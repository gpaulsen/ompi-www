<html><head><meta http-equiv="Content-Type" content="text/html charset=windows-1252"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">I addressed the “not enough slots” problem here: &nbsp;<a href="https://github.com/open-mpi/ompi-release/pull/1163" class="">https://github.com/open-mpi/ompi-release/pull/1163</a><div class=""><br class=""></div><div class="">The multiple slot-list problem is new to me - we’ve never had someone try that before, and I’m not sure how that would work given that the slot-list is an MCA param and can have only one value. Probably something for the future.</div><div class=""><br class=""></div><div class=""><br class=""><div><blockquote type="cite" class=""><div class="">On May 15, 2016, at 7:55 AM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" class="">rhc@open-mpi.org</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><div class="">You are showing different cmd lines then last time :-)<br class=""><br class="">I’ll try to take a look as time permits<br class=""><br class=""><blockquote type="cite" class="">On May 15, 2016, at 7:47 AM, Siegmar Gross &lt;<a href="mailto:Siegmar.Gross@informatik.hs-fulda.de" class="">Siegmar.Gross@informatik.hs-fulda.de</a>&gt; wrote:<br class=""><br class="">Hi Jeff,<br class=""><br class="">today I upgraded to the latest version and I still have<br class="">problems. I compiled with gcc-6.1.0 and I tried to compile<br class="">with Sun C 5.14 beta. Sun C still broke with "unrecognized<br class="">option '-path'" which was reported before, so that I use<br class="">my gcc version. By the way, this problem is solved for<br class="">openmpi-v2.x-dev-1425-ga558e90 and openmpi-dev-4050-g7f65c2b.<br class=""><br class="">loki hello_2 124 ompi_info | grep -e "OPAL repo revision" -e "C compiler absolute"<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;OPAL repo revision: v1.10.2-189-gfc05056<br class=""> &nbsp;&nbsp;&nbsp;C compiler absolute: /usr/local/gcc-6.1.0/bin/gcc<br class="">loki hello_2 125 mpiexec -np 1 --host loki hello_2_mpi : -np 1 --host loki --slot-list 0:0-5,1:0-5 hello_2_slave_mpi<br class="">--------------------------------------------------------------------------<br class="">There are not enough slots available in the system to satisfy the 1 slots<br class="">that were requested by the application:<br class=""> hello_2_slave_mpi<br class=""><br class="">Either request fewer slots for your application, or make more slots available<br class="">for use.<br class="">--------------------------------------------------------------------------<br class=""><br class=""><br class=""><br class="">I get a result, if I add "--slot-list" to the master process<br class="">as well. I changed "-np 2" to "-np 1" for the slave processes<br class="">to show new problems.<br class=""><br class="">loki hello_2 126 mpiexec -np 1 --host loki --slot-list 0:0-5,1:0-5 hello_2_mpi : -np 1 --host loki --slot-list 0:0-5,1:0-5 hello_2_slave_mpi<br class="">Process 0 of 2 running on loki<br class="">Process 1 of 2 running on loki<br class=""><br class="">Now 1 slave tasks are sending greetings.<br class=""><br class="">Greetings from task 1:<br class=""> message type: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3<br class=""> msg length: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;132 characters<br class=""> message:<br class=""> &nbsp;&nbsp;hostname: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loki<br class=""> &nbsp;&nbsp;operating system: &nbsp;Linux<br class=""> &nbsp;&nbsp;release: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.12.55-52.42-default<br class=""> &nbsp;&nbsp;processor: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x86_64<br class=""><br class=""><br class="">Now lets increase the number of slave processes to 2.<br class="">I still get only greetings from one slave process and<br class="">if I increase the number of slave processes to 3, I get<br class="">a segmentation fault. It's nearly the same for<br class="">openmpi-v2.x-dev-1425-ga558e90 (the only difference is<br class="">that the program hangs forever for 3 slave processes<br class="">for my cc and gcc version). Everything works as expected<br class="">for openmpi-dev-4050-g7f65c2b (although it takes very long<br class="">until I get all messages). It even works, if I put<br class="">"--slot-list" only once on the command line as you can see<br class="">below.<br class=""><br class="">loki hello_2 127 mpiexec -np 1 --host loki --slot-list 0:0-5,1:0-5 hello_2_mpi : -np 2 --host loki --slot-list 0:0-5,1:0-5 hello_2_slave_mpi<br class="">Process 0 of 2 running on loki<br class="">Process 1 of 2 running on loki<br class=""><br class="">Now 1 slave tasks are sending greetings.<br class=""><br class="">Greetings from task 1:<br class=""> message type: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3<br class=""> msg length: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;132 characters<br class=""> message:<br class=""> &nbsp;&nbsp;hostname: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loki<br class=""> &nbsp;&nbsp;operating system: &nbsp;Linux<br class=""> &nbsp;&nbsp;release: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.12.55-52.42-default<br class=""> &nbsp;&nbsp;processor: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x86_64<br class=""><br class=""><br class="">loki hello_2 128 mpiexec -np 1 --host loki --slot-list 0:0-5,1:0-5 hello_2_mpi : -np 3 --host loki --slot-list 0:0-5,1:0-5 hello_2_slave_mpi<br class="">[loki:28536] *** Process received signal ***<br class="">[loki:28536] Signal: Segmentation fault (11)<br class="">[loki:28536] Signal code: Address not mapped (1)<br class="">[loki:28536] Failing at address: 0x8<br class="">[loki:28536] [ 0] /lib64/libpthread.so.0(+0xf870)[0x7fd40eb75870]<br class="">[loki:28536] [ 1] /usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_proc_self+0x35)[0x7fd40edd85b0]<br class="">[loki:28536] [ 2] /usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_comm_init+0x68b)[0x7fd40edb7b08]<br class="">[loki:28536] [ 3] /usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_mpi_init+0xa90)[0x7fd40eddde8a]<br class="">[loki:28536] [ 4] /usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(MPI_Init+0x180)[0x7fd40ee1a28e]<br class="">[loki:28536] [ 5] hello_2_slave_mpi[0x400bee]<br class="">[loki:28536] [ 6] *** An error occurred in MPI_Init<br class="">*** on a NULL communicator<br class="">*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br class="">*** &nbsp;&nbsp;&nbsp;and potentially your MPI job)<br class="">[loki:28534] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!<br class="">*** An error occurred in MPI_Init<br class="">*** on a NULL communicator<br class="">*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br class="">*** &nbsp;&nbsp;&nbsp;and potentially your MPI job)<br class="">[loki:28535] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!<br class="">/lib64/libc.so.6(__libc_start_main+0xf5)[0x7fd40e7dfb05]<br class="">[loki:28536] [ 7] hello_2_slave_mpi[0x400fb0]<br class="">[loki:28536] *** End of error message ***<br class="">-------------------------------------------------------<br class="">Primary job &nbsp;terminated normally, but 1 process returned<br class="">a non-zero exit code.. Per user-direction, the job has been aborted.<br class="">-------------------------------------------------------<br class="">--------------------------------------------------------------------------<br class="">mpiexec detected that one or more processes exited with non-zero status, thus causing<br class="">the job to be terminated. The first process to do so was:<br class=""><br class=""> Process name: [[61640,1],0]<br class=""> Exit code: &nbsp;&nbsp;&nbsp;1<br class="">--------------------------------------------------------------------------<br class="">loki hello_2 129<br class=""><br class=""><br class=""><br class="">loki hello_2 114 ompi_info | grep -e "OPAL repo revision" -e "C compiler absolute"<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;OPAL repo revision: dev-4050-g7f65c2b<br class=""> &nbsp;&nbsp;&nbsp;C compiler absolute: /opt/solstudio12.5b/bin/cc<br class="">loki hello_2 115 mpiexec -np 1 --host loki --slot-list 0:0-5,1:0-5 hello_2_mpi : -np 3 --host loki --slot-list 0:0-5,1:0-5 hello_2_slave_mpi<br class="">Process 0 of 4 running on loki<br class="">Process 1 of 4 running on loki<br class="">Process 2 of 4 running on loki<br class="">Process 3 of 4 running on loki<br class="">...<br class=""><br class=""><br class="">It even works, if I put "--slot-list" only once on the command<br class="">line.<br class=""><br class="">loki hello_2 116 mpiexec -np 1 --host loki hello_2_mpi : -np 3 --host loki --slot-list 0:0-5,1:0-5 hello_2_slave_mpi<br class="">Process 1 of 4 running on loki<br class="">Process 2 of 4 running on loki<br class="">Process 0 of 4 running on loki<br class="">Process 3 of 4 running on loki<br class="">...<br class=""><br class=""><br class="">Hopefully you know what happens and why it happens so that<br class="">you can fix the problem for openmpi-1.10.x and openmpi-2.x.<br class="">My three spawn programs work with openmpi-master as well<br class="">while "spawn_master" breaks on both openmpi-1.10.x and<br class="">openmpi-2.x with the same failure as my hello master/slave<br class="">program.<br class=""><br class="">Do you know when the Java problem will be solved?<br class=""><br class=""><br class="">Kind regards<br class=""><br class="">Siegmar<br class=""><br class=""><br class=""><br class="">Am 15.05.2016 um 01:27 schrieb Ralph Castain:<br class=""><blockquote type="cite" class=""><br class=""><blockquote type="cite" class="">On May 7, 2016, at 1:13 AM, Siegmar Gross &lt;<a href="mailto:Siegmar.Gross@informatik.hs-fulda.de" class="">Siegmar.Gross@informatik.hs-fulda.de</a>&gt; wrote:<br class=""><br class="">Hi,<br class=""><br class="">yesterday I installed openmpi-v1.10.2-176-g9d45e07 on my "SUSE Linux<br class="">Enterprise Server 12 (x86_64)" with Sun C 5.13 &nbsp;and gcc-5.3.0. The<br class="">following programs don't run anymore.<br class=""><br class=""><br class="">loki hello_2 112 ompi_info | grep -e "OPAL repo revision" -e "C compiler absolute"<br class=""> &nbsp;&nbsp;&nbsp;OPAL repo revision: v1.10.2-176-g9d45e07<br class=""> &nbsp;&nbsp;C compiler absolute: /opt/solstudio12.4/bin/cc<br class="">loki hello_2 113 mpiexec -np 1 --host loki hello_2_mpi : -np 2 --host loki,loki hello_2_slave_mpi<br class="">--------------------------------------------------------------------------<br class="">There are not enough slots available in the system to satisfy the 2 slots<br class="">that were requested by the application:<br class="">hello_2_slave_mpi<br class=""><br class="">Either request fewer slots for your application, or make more slots available<br class="">for use.<br class="">--------------------------------------------------------------------------<br class="">loki hello_2 114<br class=""><br class=""></blockquote><br class="">The above worked fine for me with:<br class=""><br class="">OPAL repo revision: v1.10.2-182-g52c7573<br class=""><br class="">You might try updating.<br class=""><br class=""><blockquote type="cite" class=""><br class=""><br class="">Everything worked as expected with openmpi-v1.10.0-178-gb80f802.<br class=""><br class="">loki hello_2 114 ompi_info | grep -e "OPAL repo revision" -e "C compiler absolute"<br class=""> &nbsp;&nbsp;&nbsp;OPAL repo revision: v1.10.0-178-gb80f802<br class=""> &nbsp;&nbsp;C compiler absolute: /opt/solstudio12.4/bin/cc<br class="">loki hello_2 115 mpiexec -np 1 --host loki hello_2_mpi : -np 2 --host loki,loki hello_2_slave_mpi<br class="">Process 0 of 3 running on loki<br class="">Process 1 of 3 running on loki<br class="">Process 2 of 3 running on loki<br class=""><br class="">Now 2 slave tasks are sending greetings.<br class=""><br class="">Greetings from task 2:<br class="">message type: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3<br class="">...<br class=""><br class=""><br class="">I have the same problem with openmpi-v2.x-dev-1404-g74d8ea0, if I use<br class="">the following commands.<br class=""><br class="">mpiexec -np 1 --host loki hello_2_mpi : -np 2 --host loki,loki hello_2_slave_mpi<br class="">mpiexec -np 1 --host loki hello_2_mpi : -np 2 --host loki,nfs1 hello_2_slave_mpi<br class="">mpiexec -np 1 --host loki hello_2_mpi : -np 2 --host loki --slot-list 0:0-5,1:0-5 hello_2_slave_mpi<br class=""><br class=""><br class="">I have also the same problem with openmpi-dev-4010-g6c9d65c, if I use<br class="">the following command.<br class=""><br class="">mpiexec -np 1 --host loki hello_2_mpi : -np 2 --host loki,loki hello_2_slave_mpi<br class=""><br class=""><br class="">openmpi-dev-4010-g6c9d65c works as expected with the following commands.<br class=""><br class="">mpiexec -np 1 --host loki hello_2_mpi : -np 2 --host loki,nfs1 hello_2_slave_mpi<br class="">mpiexec -np 1 --host loki hello_2_mpi : -np 2 --host loki --slot-list 0:0-5,1:0-5 hello_2_slave_mpi<br class=""><br class=""><br class="">Has the interface changed so that I'm not allowed to use some of my<br class="">commands any longer? I would be grateful, if somebody can fix the<br class="">problem if it is a problem. Thank you very much for any help in<br class="">advance.<br class=""><br class=""><br class=""><br class="">Kind regards<br class=""><br class="">Siegmar<br class="">&lt;hello_2_mpi.c&gt;&lt;hello_2_slave_mpi.c&gt;_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: https://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2016/05/29126.php<br class=""></blockquote><br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: https://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2016/05/29205.php<br class=""><br class=""></blockquote>&lt;hello_2_mpi.c&gt;&lt;hello_2_slave_mpi.c&gt;_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: https://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2016/05/29206.php<br class=""></blockquote><br class=""></div></div></blockquote></div><br class=""></div></body></html>
