<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><br><div><div>On 10-Aug-09, at 6:44 PM, Ralph Castain wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div>Check your LD_LIBRARY_PATH - there is an earlier version of OMPI in your path that is interfering with operation (i.e., it comes before your 1.3.3 installation).<br></div></blockquote><div><br></div><div>Hmmmm, The OS X faq says not to do this:</div><div><br></div><div>"<span class="Apple-style-span" style="font-family: verdana, arial, helvetica; font-size: 12px; ">Note that there is no need to add Open MPI's libdir to&nbsp;<code>LD_LIBRARY_PATH</code>; Open MPI's shared library build process automatically uses the "rpath" mechanism to automatically find the correct shared libraries (i.e., the ones associated with this build, vs., for example, the OS X-shipped OMPI shared libraries). Also note that we specifically do&nbsp;<strong>not</strong>&nbsp;recommend adding Open MPI's libdir to&nbsp;<code>DYLD_LIBRARY_PATH</code>."</span></div><div><font class="Apple-style-span" face="verdana, arial, helvetica"><br></font></div><div><font class="Apple-style-span" face="verdana, arial, helvetica"><a href="http://www.open-mpi.org/faq/?category=osx">http://www.open-mpi.org/faq/?category=osx</a></font></div><div><font class="Apple-style-span" face="verdana, arial, helvetica"><br></font></div><div><font class="Apple-style-span" face="verdana, arial, helvetica">Regardless, if I set either, and&nbsp;run&nbsp;ompi_info&nbsp;I&nbsp;still&nbsp;get:</font></div><div><font class="Apple-style-span" face="verdana, arial, helvetica"><br></font></div><div><font class="Apple-style-span" face="verdana, arial, helvetica"><div>[saturna.cluster:94981] mca: base: component_find: iof "mca_iof_proxy" uses an MCA interface that is not recognized (component MCA v1.0.0 != supported MCA v2.0.0) -- ignored</div><div>[saturna.cluster:94981] mca: base: component_find: iof "mca_iof_svc" uses an MCA interface that is not recognized (component MCA v1.0.0 != supported MCA v2.0.0) -- ignored</div><div><br></div><div><div>echo $DYLD_LIBRARY_PATH $LD_LIBRARY_PATH</div><div>/usr/local/openmpi/lib: /usr/local/openmpi/lib:</div><div><br></div><div>So I'm afraid I'm stumped again. &nbsp;I suppose I could go clean out all the libraries in /usr/lib/...</div><div><br></div><div>Thanks again, sorry to be a pain...</div><div><br></div><div>Cheers, &nbsp;Jody</div><div><br></div><div><br></div><div><br></div></div></font></div><div><font class="Apple-style-span" face="verdana, arial, helvetica"><br></font></div><blockquote type="cite"><div><br>On Aug 10, 2009, at 7:38 PM, Klymak Jody wrote:<br><br><blockquote type="cite">So,<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">mpirun --display-allocation -pernode --display-map hostname<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">gives me the output below. &nbsp;Simple jobs seem to run, but the MITgcm does not, either under ssh or torque. &nbsp;It hangs at some early point in execution before anything is written, so its hard for me to tell what the error is. &nbsp;Could these MCA warnings have anything to do with it?<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">I've recompiled the gcm with -L /usr/local/openmpi/lib, so hopefully that catches the right library.<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">Thanks, &nbsp;Jody<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">[xserve02.local:38126] mca: base: component_find: ras "mca_ras_dash_host" uses an MCA interface that is not recogniz<br></blockquote><blockquote type="cite">ed (component MCA v1.0.0 != supported MCA v2.0.0) -- ignored<br></blockquote><blockquote type="cite">[xserve02.local:38126] mca: base: component_find: ras "mca_ras_hostfile" uses an MCA interface that is not recognize<br></blockquote><blockquote type="cite">d (component MCA v1.0.0 != supported MCA v2.0.0) -- ignored<br></blockquote><blockquote type="cite">[xserve02.local:38126] mca: base: component_find: ras "mca_ras_localhost" uses an MCA interface that is not recogniz<br></blockquote><blockquote type="cite">ed (component MCA v1.0.0 != supported MCA v2.0.0) -- ignored<br></blockquote><blockquote type="cite">[xserve02.local:38126] mca: base: component_find: ras "mca_ras_xgrid" uses an MCA interface that is not recognized (<br></blockquote><blockquote type="cite">component MCA v1.0.0 != supported MCA v2.0.0) -- ignored<br></blockquote><blockquote type="cite">[xserve02.local:38126] mca: base: component_find: iof "mca_iof_proxy" uses an MCA interface that is not recognized (<br></blockquote><blockquote type="cite">component MCA v1.0.0 != supported MCA v2.0.0) -- ignored<br></blockquote><blockquote type="cite">[xserve02.local:38126] mca: base: component_find: iof "mca_iof_svc" uses an MCA interface that is not recognized (co<br></blockquote><blockquote type="cite">mponent MCA v1.0.0 != supported MCA v2.0.0) -- ignored<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">====================== &nbsp;&nbsp;ALLOCATED NODES &nbsp;&nbsp;======================<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">Data for node: Name: xserve02.local &nbsp;&nbsp;&nbsp;Num slots: 8 &nbsp;&nbsp;&nbsp;Max slots: 0<br></blockquote><blockquote type="cite">Data for node: Name: xserve01.local &nbsp;&nbsp;&nbsp;Num slots: 8 &nbsp;&nbsp;&nbsp;Max slots: 0<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">=================================================================<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">======================== &nbsp;&nbsp;JOB MAP &nbsp;&nbsp;========================<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">Data for node: Name: xserve02.local &nbsp;&nbsp;&nbsp;Num procs: 1<br></blockquote><blockquote type="cite"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [20967,1] Process rank: 0<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">Data for node: Name: xserve01.local &nbsp;&nbsp;&nbsp;Num procs: 1<br></blockquote><blockquote type="cite"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Process OMPI jobid: [20967,1] Process rank: 1<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">=============================================================<br></blockquote><blockquote type="cite">[xserve01.cluster:38518] mca: base: component_find: iof "mca_iof_proxy" uses an MCA interface that is not recognized<br></blockquote><blockquote type="cite">(component MCA v1.0.0 != supported MCA v2.0.0) -- ignored<br></blockquote><blockquote type="cite">[xserve01.cluster:38518] mca: base: component_find: iof "mca_iof_svc" uses an MCA interface that is not recognized (<br></blockquote><blockquote type="cite">component MCA v1.0.0 != supported MCA v2.0.0) -- ignored<br></blockquote><blockquote type="cite">xserve02.local<br></blockquote><blockquote type="cite">xserve01.cluster<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">_______________________________________________<br></blockquote><blockquote type="cite">users mailing list<br></blockquote><blockquote type="cite"><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br></blockquote><blockquote type="cite"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></div></blockquote></div><br></body></html>
