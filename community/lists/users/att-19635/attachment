<HTML><HEAD><META http-equiv="Content-Type" content="text/html; charset=utf-8"></HEAD><BODY><meta http-equiv="Content-Type" content="text/html; charset=utf-8">As far as I know, OMPI combines the fault tolerant features in FT-MPI, LA-MPI and LAM/MPI, is this statement still correct now? Or as you say, OMPI supports checkpoint/restart(like in LAM/MPI)&nbsp;only? I don't know the details of FT-MPI or LA-MPI, aren't they useful or necesarry?<div><br></div><div>In fact, what I really want to know is, suppose I run a job on N processors with OMPI, and one (or some) of these processors crashes, then what would be done by the fault-tolerant&nbsp;mechanism of OMPI? Meanwhile what should the sys-admin do(like restart the crashed node) ?</div><div><br></div><div>In my understanding, after the crash, the sys-admin should restart the crashed node(if it can be restarted), and then do the rollback by some sort of command, while the OMPI would help hang up all the computing process, waiting for rollback command, is this correct?</div><div><br></div><div>thanks again.</div><div><div>&nbsp;<br><br><br>--------- 原始邮件信息 ---------<br><b>发件人:</b> "Open MPI Users" &lt;users@open-mpi.org&gt;<br><b>收件人:</b> "Open MPI Users" &lt;users@open-mpi.org&gt;<br><b>主题:</b> Re: [OMPI users]  2012/06/18 14:35:07 自动保存草稿<br><b>日期:</b> 2012/06/20 01:26:08, Wednesday<br><br>That's a little bit strong - OMPI still supports checkpoint/restart as a fault tolerance mechanism. There really isn't anything the sys admin has to do, though - what is required is that users periodically order their programs to checkpoint so they can be restarted after a failure.<div><br></div><div>Checkpointing is typically done either by the app itself (say, when it reaches some point it feels is a good one to save), or using a script that just orders a checkpoint every so many seconds.</div><div><br></div><div>What we have said is that we don't believe the FT "run thru failure" position pushed by UTK is particularly required at this time. Partly a question of impact vs benefit, mostly due to competing approaches offering equivalent fault recovery capability with less impact. But that's a separate discussion.</div><div><br></div><div><br><div><div>On Jun 19, 2012, at 11:16 AM, George Bosilca wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">It has been clearly stated that the official position pushed forward by a majority of the Open MPI developer community is that fault tolerance is not needed so we (read this as the official version of Open MPI) do not support it.<div><br></div><div>However, a group of researchers have been working toward a version of Open MPI that supports the last fault tolerance proposal submitted for consideration to the MPI Forum. You can access it at&nbsp;<a href="https://bitbucket.org/jjhursey/ompi-ulfm-rts">https://bitbucket.org/jjhursey/ompi-ulfm-rts</a>.</div><div><br></div><div>&nbsp; george.&nbsp;</div><div><div><br><div><div>On Jun 19, 2012, at 09:58 , 陈松 wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><div><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><div>Hi all,</div><div><br></div><div>Can anyone explain me the fault tolerant features in OpenMPI? I've read the FAQs and some papers about this topic listed in <a href="http://open-mpi.org/">open-mpi.org</a>, but still can't figure out when one node of my supercomputer system fails down during computing, what would happen with the fault tolerant mechanism in OpenMPI, and what should we system administrator do after the failure (or before).&nbsp;</div><div><br></div><div>Can anyone help me? My boss want me to deploy OpenMPI in our system cuz he want the fault tolerant feature.</div><div><br></div><div>Thanks very much.</div><div><br><br><br></div><div id="signature_span"><div>---------------</div><div><b><font size="3">CHEN Song</font></b></div><div>R&amp;D Department</div>National Supercomputer Center in Tianjin<div>Binhai New Area, Tianjin, China</div></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote></div><br></div></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div>
<br>
<br>
<hr>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></div></BODY></HTML>