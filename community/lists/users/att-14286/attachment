
<br><font size=2 face="sans-serif">request_1 and request_2 are just local
variable names. </font>
<br>
<br><font size=2 face="sans-serif">The only thing that determines matching
order is CC issue order on the communicator. &nbsp;At each process, some
CC is issued first and some CC is issued second. &nbsp;The first issued
CC at each process will try to match the first issued CC at the other processes.
&nbsp;By this rule,</font>
<br><font size=2 face="sans-serif">rank 0: </font>
<br><font size=2 face="sans-serif">MPI_Ibcast; MPI_Ibcast </font>
<br><font size=2 face="sans-serif">Rank 1;</font>
<br><font size=2 face="sans-serif">MPI_Ibcast; MPI_Ibcast </font>
<br><font size=2 face="sans-serif">is well defined and</font>
<br>
<br><font size=2 face="sans-serif">rank 0: </font>
<br><font size=2 face="sans-serif">MPI_Ibcast; MPI_Ireduce</font>
<br><font size=2 face="sans-serif">Rank 1;</font>
<br><font size=2 face="sans-serif">MPI_Ireducet; MPI_Ibcast </font>
<br><font size=2 face="sans-serif">is incorrect.</font>
<br>
<br><font size=2 face="sans-serif">I do not agree with Jeff on this below.
&nbsp; The Proc 1 case where the MPI_Waits are reversed simply requires
the MPI implementation to make progress on both MPI_Ibcast operations in
the first MPI_Wait. The second MPI_Wait call will simply find that the
first MPI_Ibcast is already done. &nbsp;The second MPI_Wait call becomes,
effectively, a query function.</font>
<br>
<br><tt><font size=2>proc 0:<br>
MPI_IBcast(MPI_COMM_WORLD, request_1) // first Bcast<br>
MPI_IBcast(MPI_COMM_WORLD, request_2) // second Bcast<br>
MPI_Wait(&amp;request_1, ...);<br>
MPI_Wait(&amp;request_2, ...);<br>
<br>
proc 1:<br>
MPI_IBcast(MPI_COMM_WORLD, request_2) // first Bcast<br>
MPI_IBcast(MPI_COMM_WORLD, request_1) // second Bcast<br>
MPI_Wait(&amp;request_1, ...);<br>
MPI_Wait(&amp;request_2, ...);<br>
<br>
That may/will deadlock.</font></tt>
<br>
<br>
<br>
<br>
<br>
<br><font size=2 face="sans-serif">Dick Treumann &nbsp;- &nbsp;MPI Team
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br>
IBM Systems &amp; Technology Group<br>
Dept X2ZA / MS P963 -- 2455 South Road -- Poughkeepsie, NY 12601<br>
Tele (845) 433-7846 &nbsp; &nbsp; &nbsp; &nbsp; Fax (845) 433-8363<br>
</font>
<br>
<br>
<br>
<table width=100%>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">From:</font>
<td><font size=1 face="sans-serif">Jeff Squyres &lt;jsquyres@cisco.com&gt;</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">To:</font>
<td><font size=1 face="sans-serif">Open MPI Users &lt;users@open-mpi.org&gt;</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Date:</font>
<td><font size=1 face="sans-serif">09/23/2010 10:13 AM</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Subject:</font>
<td><font size=1 face="sans-serif">Re: [OMPI users] Question about Asynchronous
collectives</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Sent by:</font>
<td><font size=1 face="sans-serif">users-bounces@open-mpi.org</font></table>
<br>
<hr noshade>
<br>
<br>
<br><tt><font size=2>On Sep 23, 2010, at 10:00 AM, Gabriele Fatigati wrote:<br>
<br>
&gt; to be sure, if i have one processor who does:<br>
&gt; <br>
&gt; MPI_IBcast(MPI_COMM_WORLD, request_1) // first Bcast<br>
&gt; MPI_IBcast(MPI_COMM_WORLD, request_2) // second Bcast<br>
&gt; <br>
&gt; it means that i can't have another process who does the follow:<br>
&gt; <br>
&gt; MPI_IBcast(MPI_COMM_WORLD, request_2) // firt Bcast for another process<br>
&gt; MPI_IBcast(MPI_COMM_WORLD, request_1) // second Bcast for another
process<br>
&gt; <br>
&gt; Because first Bcast of second process matches with first Bcast of
first process, and it's wrong.<br>
<br>
If you did a &quot;waitall&quot; on both requests, it would probably work
because MPI would just &quot;figure it out&quot;. &nbsp;But if you did
something like:<br>
<br>
proc 0:<br>
MPI_IBcast(MPI_COMM_WORLD, request_1) // first Bcast<br>
MPI_IBcast(MPI_COMM_WORLD, request_2) // second Bcast<br>
MPI_Wait(&amp;request_1, ...);<br>
MPI_Wait(&amp;request_2, ...);<br>
<br>
proc 1:<br>
MPI_IBcast(MPI_COMM_WORLD, request_2) // first Bcast<br>
MPI_IBcast(MPI_COMM_WORLD, request_1) // second Bcast<br>
MPI_Wait(&amp;request_1, ...);<br>
MPI_Wait(&amp;request_2, ...);<br>
<br>
That may/will deadlock.<br>
<br>
-- <br>
Jeff Squyres<br>
jsquyres@cisco.com<br>
For corporate legal information go to:<br>
</font></tt><a href=http://www.cisco.com/web/about/doing_business/legal/cri/><tt><font size=2>http://www.cisco.com/web/about/doing_business/legal/cri/</font></tt></a><tt><font size=2><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
users@open-mpi.org<br>
</font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
</font></tt>
<br>
<br>
