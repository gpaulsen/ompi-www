its possible. but Not a novel idea. hehe.<br><br>Its a form of HYBRID programming (distributed shared programming). <br>But it needs to be ensured that whether it is beneficial for a given case/problem/code.<br><br><br><div class="gmail_quote">
On Thu, Jul 22, 2010 at 5:52 PM, Cristobal Navarro <span dir="ltr">&lt;<a href="mailto:axischire@gmail.com">axischire@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
yes,<br>
i was aware of the big difference hehe.<br>
<br>
now that openMP and openMPI is in talk, i&#39;ve alwyas wondered if its a<br>
good idea to model a solution on the following way, using both openMP<br>
and openMPI.<br>
suppose you have n nodes, each node has a quadcore, (so you have n*4 processors)<br>
launch n proceses acorrding to the n nodes available.<br>
set a resource manager like SGE to fill the n*4 slots using round robin.<br>
on each process, make use of the other cores available on the node,<br>
with openMP.<br>
<br>
if this is possible, then on each one could make use fo the shared<br>
memory model locally at each node, evading unnecesary I/O through the<br>
nwetwork, what do you think?<br>
<div><div></div><div class="h5"><br>
<br>
<br>
On Thu, Jul 22, 2010 at 5:27 PM, amjad ali &lt;<a href="mailto:amjad11@gmail.com">amjad11@gmail.com</a>&gt; wrote:<br>
&gt; Hi Cristobal,<br>
&gt;<br>
&gt; Note that the pic in <a href="http://dl.dropbox.com/u/6380744/clusterLibs.png" target="_blank">http://dl.dropbox.com/u/6380744/clusterLibs.png</a><br>
&gt; shows that Scalapack is based on what; it only shows which packages<br>
&gt; Scalapack uses; hence no OpenMP is there.<br>
&gt;<br>
&gt; Also be clear about the difference:<br>
&gt; &quot;OpenMP&quot; is for shared memory parallel programming, while<br>
&gt; &quot;OpenMPI&quot; is an implantation of MPI standard (this list is about OpenMPI<br>
&gt; obviously).<br>
&gt;<br>
&gt; best<br>
&gt; AA.<br>
&gt;<br>
&gt; On Thu, Jul 22, 2010 at 5:06 PM, Cristobal Navarro &lt;<a href="mailto:axischire@gmail.com">axischire@gmail.com</a>&gt;<br>
&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; Thanks<br>
&gt;&gt;<br>
&gt;&gt; im looking at the manual, seems good.<br>
&gt;&gt; i think now the picture is more clear.<br>
&gt;&gt;<br>
&gt;&gt; i have a very custom algorithm, local problem of research,<br>
&gt;&gt; paralelizable, thats where openMPI enters.<br>
&gt;&gt; then, at some point on the program, all the computation traduces to<br>
&gt;&gt; numeric (double) matrix operations, eigenvalues and derivatives. thats<br>
&gt;&gt; where a library like PETSc makes its appearance. or a lower level<br>
&gt;&gt; solution would be GSL and manually implement paralelism with MPI.<br>
&gt;&gt;<br>
&gt;&gt; in case someone chooses, a highlevel library like PETSc and some low<br>
&gt;&gt; level openMPI for its custom algorithms, is there a race for MPI<br>
&gt;&gt; problem?<br>
&gt;&gt;<br>
&gt;&gt; On Thu, Jul 22, 2010 at 3:42 PM, Gus Correa &lt;<a href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt; wrote:<br>
&gt;&gt; &gt; Hi Cristobal<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; You may want to take a look at PETSc,<br>
&gt;&gt; &gt; which has all the machinery for linear algebra that<br>
&gt;&gt; &gt; you need, can easily attach a variety of Linear Algebra packages,<br>
&gt;&gt; &gt; including those in the diagram you sent and more,<br>
&gt;&gt; &gt; builds on top of MPI, and can even build MPI for you, if you prefer.<br>
&gt;&gt; &gt; It has C and Fortran interfaces, and if I remember right,<br>
&gt;&gt; &gt; you can build it alternatively with a C++ interface.<br>
&gt;&gt; &gt; You can choose from real or complex scalars,<br>
&gt;&gt; &gt; depending on your target problem (e.g. if you are going to do<br>
&gt;&gt; &gt; signal/image processing with FFTs, you want complex scalars).<br>
&gt;&gt; &gt; I don&#39;t know if it has high level commands to deal with<br>
&gt;&gt; &gt; data structures (like trees that you mentioned), but it may.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; <a href="http://www.mcs.anl.gov/petsc/petsc-as/" target="_blank">http://www.mcs.anl.gov/petsc/petsc-as/</a><br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; My $0.02<br>
&gt;&gt; &gt; Gus Correa<br>
&gt;&gt; &gt; ---------------------------------------------------------------------<br>
&gt;&gt; &gt; Gustavo Correa<br>
&gt;&gt; &gt; Lamont-Doherty Earth Observatory - Columbia University<br>
&gt;&gt; &gt; Palisades, NY, 10964-8000 - USA<br>
&gt;&gt; &gt; ---------------------------------------------------------------------<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Cristobal Navarro wrote:<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; Hello,<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; i am designing a solution to one of my programs, which mixes some tree<br>
&gt;&gt; &gt;&gt; generation, matrix operatons, eigenvaluies, among other tasks.<br>
&gt;&gt; &gt;&gt; i have to paralellize all of this for a cluster of 4 nodes (32 cores),<br>
&gt;&gt; &gt;&gt; and what i first thought was MPI as a blind choice, but after looking<br>
&gt;&gt; &gt;&gt; at this picture<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; <a href="http://dl.dropbox.com/u/6380744/clusterLibs.png" target="_blank">http://dl.dropbox.com/u/6380744/clusterLibs.png</a> ( On the picture,<br>
&gt;&gt; &gt;&gt; openMP is missing.)<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; i decided to take a break and sit down, think what best suits to my<br>
&gt;&gt; &gt;&gt; needs.<br>
&gt;&gt; &gt;&gt; Adittionally, i am not familiar with Fortran, so i search for C/C++<br>
&gt;&gt; &gt;&gt; libraries.<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; what are your experiences, what aspects of your proyect do you<br>
&gt;&gt; &gt;&gt; consider when choosing, is a good practice to mix these libraries in<br>
&gt;&gt; &gt;&gt; one same proyect?<br>
&gt;&gt; &gt;&gt; _______________________________________________<br>
&gt;&gt; &gt;&gt; users mailing list<br>
&gt;&gt; &gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; &gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; _______________________________________________<br>
&gt;&gt; &gt; users mailing list<br>
&gt;&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; &gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

