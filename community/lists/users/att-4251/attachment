Hi,<br><br>I&#39;ve run into the same problem as discussed in the thread <a href="http://www.open-mpi.org/community/lists/users/2007/07/3655.php">Lev Gelb: &quot;Re: [OMPI users] Recursive use of &quot;orterun&quot; (Ralph H Castain)&quot;
</a><br><br>I am running a parallel python code, then from python I launch a C++ parallel program using the python os.system command, then I come back in python and keep going.<br><br>With LAM/MPI there is no problem with this. 
<br><br>But Open-mpi systematically crashes, because the python os.system command launches the C++ program with the same OMPI_* environment variables as for the Python program. As discussed in the thread, I have tried filtering the OMPI_* variables prior to launching the C++ program with an 
os.execve command, but then it fails to return the hand to python and instead simply terminates when the C++ program ends.<br><br>There is a workaround (<a href="http://thread.gmane.org/gmane.comp.clustering.open-mpi.user/986">
http://thread.gmane.org/gmane.comp.clustering.open-mpi.user/986</a>): create a *.sh file with the following lines:<br><br>--------<br>for i in $(env | grep OMPI_MCA |sed &#39;s/=/ /&#39; | awk &#39;{print $1}&#39;)<br>do<br>
&nbsp;&nbsp; unset $i<br>done<br><br># now the C++ call<br>
mpirun -np 2&nbsp; ./MoM/communicateMeshArrays<br>----------<br><br> and then call the *.sh program through the python os.system command.<br><br>What I would like to know is that if this &quot;problem&quot; will get fixed in open-MPI? Is there another way to elegantly solve this issue? Meanwhile, I will stick to the ugly *.sh hack listed above.
<br><br>Cheers<br><br>Ides<br><br>

