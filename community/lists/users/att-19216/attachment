 I am running MPI and Thrust code on a cluster and measuring time for calculations.<div><br></div><div>My MPI code -   </div><div><div><br></div><div><span style="white-space:pre-wrap">	</span>#include &quot;mpi.h&quot;</div>

<div><span style="white-space:pre-wrap">	</span>#include &lt;stdio.h&gt;</div><div><span style="white-space:pre-wrap">	</span>#include &lt;stdlib.h&gt;</div><div><span style="white-space:pre-wrap">	</span>#include &lt;string.h&gt;</div>

<div><span style="white-space:pre-wrap">	</span>#include &lt;time.h&gt;</div><div><span style="white-space:pre-wrap">	</span>#include &lt;sys/time.h&gt;</div><div><span style="white-space:pre-wrap">	</span>#include &lt;sys/resource.h&gt;</div>

<div><br></div><div><span style="white-space:pre-wrap">	</span>#define  MASTER<span style="white-space:pre-wrap">		</span>0</div><div><span style="white-space:pre-wrap">	</span>#define ARRAYSIZE 20000000</div>
<div><br></div><div><span style="white-space:pre-wrap">	</span>int *masterarray,*onearray,*twoarray,*threearray,*fourarray,*fivearray,*sixarray,*sevenarray,*eightarray,*ninearray;     </div><div>   int main(int argc, char* argv[])</div>

<div><span style="white-space:pre-wrap">	</span>{</div><div><span style="white-space:pre-wrap">	</span>  int   numtasks, taskid,chunksize, namelen; </div><div><span style="white-space:pre-wrap">	</span>  int mysum,one,two,three,four,five,six,seven,eight,nine;</div>

<div><br></div><div><span style="white-space:pre-wrap">	</span>  char myname[MPI_MAX_PROCESSOR_NAME];</div><div><span style="white-space:pre-wrap">	</span>  MPI_Status status;</div><div><span style="white-space:pre-wrap">	</span>  int a,b,c,d,e,f,g,h,i,j;</div>

<div><br></div><div><span style="white-space:pre-wrap">	</span>/***** Initializations *****/</div><div><span style="white-space:pre-wrap">	</span>MPI_Init(&amp;argc, &amp;argv);</div><div><span style="white-space:pre-wrap">	</span>MPI_Comm_size(MPI_COMM_WORLD, &amp;numtasks);</div>

<div><span style="white-space:pre-wrap">	</span>MPI_Comm_rank(MPI_COMM_WORLD,&amp;taskid); </div><div><span style="white-space:pre-wrap">	</span>MPI_Get_processor_name(myname, &amp;namelen);</div><div><span style="white-space:pre-wrap">	</span>printf (&quot;MPI task %d has started on host %s...\n&quot;, taskid, myname);</div>

<div><br></div><div><span style="white-space:pre-wrap">	</span>masterarray= malloc(ARRAYSIZE * sizeof(int));</div><div><span style="white-space:pre-wrap">	</span>onearray= malloc(ARRAYSIZE * sizeof(int));</div>
<div><span style="white-space:pre-wrap">	</span>twoarray= malloc(ARRAYSIZE * sizeof(int));</div><div><span style="white-space:pre-wrap">	</span>threearray= malloc(ARRAYSIZE * sizeof(int));</div><div><span style="white-space:pre-wrap">	</span>fourarray= malloc(ARRAYSIZE * sizeof(int));</div>

<div><span style="white-space:pre-wrap">	</span>fivearray= malloc(ARRAYSIZE * sizeof(int));</div><div><span style="white-space:pre-wrap">	</span>sixarray= malloc(ARRAYSIZE * sizeof(int));</div><div><span style="white-space:pre-wrap">	</span>sevenarray= malloc(ARRAYSIZE * sizeof(int));</div>

<div><span style="white-space:pre-wrap">	</span>eightarray= malloc(ARRAYSIZE * sizeof(int));</div><div><span style="white-space:pre-wrap">	</span>ninearray= malloc(ARRAYSIZE * sizeof(int));<span style="white-space:pre-wrap">	</span></div>

<div><br></div><div><span style="white-space:pre-wrap">	</span>/***** Master task only ******/</div><div><span style="white-space:pre-wrap">	</span>if (taskid == MASTER){</div><div>           for(a=0; a &lt; ARRAYSIZE; a++){</div>

<div>                 masterarray[a] = 1;</div><div>                               </div><div>            }</div><div><span style="white-space:pre-wrap">	</span>   mysum = run_kernel0(masterarray,ARRAYSIZE,taskid, myname);</div>

<div><br></div><div><span style="white-space:pre-wrap">	</span> }  /* end of master section */</div><div><br></div><div><span style="white-space:pre-wrap">	</span>  if (taskid &gt; MASTER) {</div><div>
<br></div><div>             if(taskid == 1){</div><div>                for(b=0;b&lt;ARRAYSIZE;b++){</div><div>                onearray[b] = 1;</div><div>            }</div><div>                 one = run_kernel0(onearray,ARRAYSIZE,taskid, myname);</div>

<div>             }</div><div>             if(taskid == 2){</div><div>                for(c=0;c&lt;ARRAYSIZE;c++){</div><div>                 twoarray[c] = 1;</div><div>            }</div><div>
                 two = run_kernel0(twoarray,ARRAYSIZE,taskid, myname);</div><div>             }</div><div>             if(taskid == 3){</div><div>                 for(d=0;d&lt;ARRAYSIZE;d++){</div><div>
                 threearray[d] = 1;</div><div>                  }</div><div>                  three = run_kernel0(threearray,ARRAYSIZE,taskid, myname);</div><div>             }</div><div><span style="white-space:pre-wrap">	</span>     if(taskid == 4){</div>

<div>                   for(e=0;e &lt; ARRAYSIZE; e++){</div><div>                      fourarray[e] = 1;</div><div>                  }</div><div>                 four = run_kernel0(fourarray,ARRAYSIZE,taskid, myname);</div>

<div>             }</div><div>             if(taskid == 5){</div><div>                for(f=0;f&lt;ARRAYSIZE;f++){</div><div>                  fivearray[f] = 1;</div><div>                  }</div>
<div>                five = run_kernel0(fivearray,ARRAYSIZE,taskid, myname);</div><div>             }</div><div>             if(taskid == 6){</div><div>                  </div><div>                for(g=0;g&lt;ARRAYSIZE;g++){</div>

<div>                 sixarray[g] = 1;</div><div>                }</div><div>                 six = run_kernel0(sixarray,ARRAYSIZE,taskid, myname);</div><div>             }<span style="white-space:pre-wrap">	</span></div>

<div>             if(taskid == 7){</div><div>                    for(h=0;h&lt;ARRAYSIZE;h++){</div><div>                    sevenarray[h] = 1;</div><div>                  }</div><div>                   seven = run_kernel0(sevenarray,ARRAYSIZE,taskid, myname);</div>

<div>             }<span style="white-space:pre-wrap">	</span></div><div>             if(taskid == 8){</div><div><br></div><div>                  for(i=0;i&lt;ARRAYSIZE;i++){</div><div>                  eightarray[i] = 1;</div>

<div>                }</div><div>                   eight = run_kernel0(eightarray,ARRAYSIZE,taskid, myname);</div><div>             }<span style="white-space:pre-wrap">	</span></div><div>             if(taskid == 9){</div>

<div><br></div><div>                   for(j=0;j&lt;ARRAYSIZE;j++){</div><div>                 ninearray[j] = 1;</div><div>                   }</div><div>                   nine = run_kernel0(ninearray,ARRAYSIZE,taskid, myname);</div>

<div>             }<span style="white-space:pre-wrap">	</span></div><div><span style="white-space:pre-wrap">	</span>   }</div><div><span style="white-space:pre-wrap">	</span> MPI_Finalize();</div><div>
<br></div><div><span style="white-space:pre-wrap">	</span>}   </div><div><br></div><div>All the tasks just initialize their own array and then calculate the sum using cuda thrust.</div><div>My CUDA Thrust code - </div>
<div><br></div><div> #include &lt;stdio.h&gt;</div><div><span style="white-space:pre-wrap">	</span>#include &lt;cutil_inline.h&gt;</div><div><span style="white-space:pre-wrap">	</span>#include &lt;cutil.h&gt;</div>
<div><span style="white-space:pre-wrap">	</span>#include &lt;thrust/version.h&gt;</div><div><span style="white-space:pre-wrap">	</span>#include &lt;thrust/generate.h&gt;</div><div><span style="white-space:pre-wrap">	</span>#include &lt;thrust/host_vector.h&gt;</div>

<div><span style="white-space:pre-wrap">	</span>#include &lt;thrust/device_vector.h&gt;</div><div><span style="white-space:pre-wrap">	</span>#include &lt;thrust/functional.h&gt;</div><div><span style="white-space:pre-wrap">	</span>#include &lt;thrust/transform_reduce.h&gt;</div>

<div><span style="white-space:pre-wrap">	</span>#include &lt;time.h&gt;</div><div><span style="white-space:pre-wrap">	</span>#include &lt;sys/time.h&gt;</div><div><span style="white-space:pre-wrap">	</span>#include &lt;sys/resource.h&gt;</div>

<div><br></div><div>  extern &quot;C&quot;</div><div> int run_kernel0( int array[], int nelements, int taskid, char hostname[])</div><div> {</div><div>   </div><div>       float elapsedTime;</div>
<div>        int result = 0;</div><div><span style="white-space:pre-wrap">	</span>int threshold = 25000000;</div><div>        cudaEvent_t start, stop;</div><div><span style="white-space:pre-wrap">	</span>cudaEventCreate(&amp;start);</div>

<div><span style="white-space:pre-wrap">	</span>cudaEventCreate(&amp;stop);</div><div><span style="white-space:pre-wrap">	</span>cudaEventRecord(start, 0);</div><div><span style="white-space:pre-wrap">	</span>thrust::device_vector&lt;int&gt; gpuarray;</div>

<div><span style="white-space:pre-wrap">	</span>int *begin = array;</div><div><span style="white-space:pre-wrap">	</span>int *end = array + nelements;</div><div><span style="white-space:pre-wrap">	</span>while(begin != end)</div>

<div><span style="white-space:pre-wrap">	</span>{</div><div> <span style="white-space:pre-wrap">	</span> int chunk_size = thrust::min(threshold,end - begin);</div><div> <span style="white-space:pre-wrap">	</span> gpuarray.assign(begin, begin + chunk_size); </div>

<div><span style="white-space:pre-wrap">	</span> result += thrust::reduce(gpuarray.begin(), gpuarray.end());</div><div> <span style="white-space:pre-wrap">	</span> begin += chunk_size;</div><div><span style="white-space:pre-wrap">	</span>}</div>

<div>        cudaEventRecord(stop, 0);</div><div>        cudaEventSynchronize(stop);     </div><div><span style="white-space:pre-wrap">	</span>cudaEventElapsedTime(&amp;elapsedTime, start, stop);</div><div>
<span style="white-space:pre-wrap">	</span>cudaEventDestroy(start);</div><div><span style="white-space:pre-wrap">	</span>cudaEventDestroy(stop);</div><div><br></div><div>        printf(&quot; Task %d on has sum (on GPU): %ld Time for the kernel: %f ms \n&quot;, taskid, result, elapsedTime); </div>

<div>     </div><div><span style="white-space:pre-wrap">	</span>return result;</div><div>    }</div><div><br></div><div>I also calculate the sum using CPU and the code is as below - </div><div>
<br></div><div><div>  struct timespec time1, time2, temp_time;</div><div><br></div><div>  clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &amp;time1);</div><div>  int i;</div><div>  int cpu_sum = 0;</div><div>  long diff = 0;</div>

<div><br></div><div>  for (i = 0; i &lt; nelements; i++) {</div><div>    cpu_sum += array[i];</div><div>  }    </div><div>  clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &amp;time2);</div><div>  temp_time.tv_sec = time2.tv_sec - time1.tv_sec;</div>

<div>  temp_time.tv_nsec = time2.tv_nsec - time1.tv_nsec;</div><div>  diff = temp_time.tv_sec * 1000000000 + temp_time.tv_nsec; </div><div>  printf(&quot;Task %d calculated sum: %d using CPU in %lf ms \n&quot;, taskid, cpu_sum, (double) diff/1000000); </div>

<div>  return cpu_sum;</div></div><div><br></div><div>Now when I run the job on cluster with 10 MPI tasks and compare the timings of CPU and GPU, I get weird results where GPU time is much much higher than CPU time. </div>

<div>But the case should be opposite isnt it?</div><br></div><div><div style>The CPU time is almost same for all the task but GPU time increases. </div><div style><br></div><div style>Just wondering what might be the cause of this or are these results correct? Anything wrong with MPI code?</div>
<div style><br></div><div style>My cluster has 3 machines. 4 MPI tasks run on 2 machine and 2 Tasks run on 1 machine. </div><div style>Each machine has 1 GPU - GForce 9500 GT with 512 MB memory. </div><div style><br></div>
<div style>Can anyone please help me with this.?</div></div><div style><br></div><div style>Thanks</div>-- <br><div><span style="font-size:13px;border-collapse:collapse"><font color="#000099" face="verdana, sans-serif"><br>
</font></span></div><div><span style="font-size:13px"><font color="#666666" face="georgia, serif"><br></font></span></div><br>

