<div dir="ltr"><div><div>Hi,<br><br>We appear to have a correctly setup Mellanox IB network (ibdiagnet, ibstat,<br>iblinkinfo, ibqueryerrors(*)).  It&#39;s operating at Rate 40 FDR10.<br><br>But openMPI programs (test and user) that are specifying the &#39;openib,self,sm&#39; <br>paramenters do not seem to be using the IB network according to network-<br>monitoring tools (dstat/tcpdump/ifconfig counters).<br><br>ib0 is the interface to the IB network, em1 is our general network.  It&#39;s just a plain CentOS 6.5 system with openMPI 1.8.1.<br><br>Is anyone able to advise if this is normal behaviour ?<br></div>How can I explictly verify user and tests programs are using the IB network ?<br><br></div>I&#39;ve done alot of google and FAQ searching, but my case does not seem to come up in either.<br><div><div><div><br>(1) no traffic on ib0 at all<br>/usr/local/openmpi-1.8.1/bin/mpirun --mca btl_openib_verbose 1 \<br>--mca btl openib,self,sm -host max140,max141 -n 8 \<br>/usr/lib64/openmpi/bin/mpitests-IMB-MPI1<br><br>Monitoring with below shows no traffic at all on ib0:<br>dstat -n -N ib0,em1,total<br><br>--net/ib0-----net/em1----net/total-<br> recv  send: recv  send: recv  send<br>   0     0 :   0     0 :   0     0 <br>   0     0 : 864B    0 : 864B    0 <br>   0     0 : 452B  832B: 452B  832B<br>   0     0 :3554B  230B:3554B  230B<br><br>Monitoring with the below is showing no traffic at all on ib0:<br>sudo tcpdump -i ib0<br><br>(2) Roughly shared network usage between ib0 and em1<br>/usr/local/openmpi-1.8.1/bin/mpirun --mca btl_openib_verbose 1 \<br>--mca btl tcp,vader,self -host max140,max141 -n 8 \<br>/usr/lib64/openmpi/bin/mpitests-IMB-MPI1<br><br>--net/ib0-----net/em1----net/total-<br> recv  send: recv  send: recv  send<br>   0     0 :   0     0 :   0     0 <br>1061k 1129k:  97M   96M:  98M   97M<br>  46M   45M:9356k   11M:  56M   56M<br>  84M   82M:  12M   12M:  96M   93M<br> 160k  167k:  82M   82M:  82M   82M<br><br>tcpdump -i ib0 # shows lots of network traffic<br>ifconfig ib0 # shows increasing packet counters<br><br>(3) mostly uses ib0, &quot;feels&quot; fast<br>/usr/local/openmpi-1.8.1/bin/mpirun --mca btl_openib_verbose 1 \<br>--mca btl tcp,vader,self --mca btl_tcp_if_include ib0 -host max140,max141 -n 8 <br>/usr/lib64/openmpi/bin/mpitests-IMB-MPI1<br><br>[root@max140.mdc-berlin.net:/home/darnold] $ dstat -n -N ib0,em1,total<br>--net/ib0-----net/em1----net/total-<br> recv  send: recv  send: recv  send<br>   0     0 :   0     0 :   0     0 <br> 506k  538k:2472B 3880B: 508k  542k<br> 371M  362M:5628B   11k: 371M  362M<br>1000M  972M:8517B 5328B:1000M  972M<br>  62M   63M:1248B 1424B:  62M   63M<br><br>tcpdump -i ib0 # shows lots of network traffic<br>ifconfig ib0 # shows increasing packet counters<br><br></div></div></div></div>

