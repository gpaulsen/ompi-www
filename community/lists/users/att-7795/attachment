<div>Hi Gabriele,</div>
<div>it might be that your message size is too large for available memory per node.</div>
<div>I had&nbsp;a&nbsp;problem with IMB when I was not able to run to completion Alltoall on N=128, ppn=8 on our cluster with 16 GB per node. You&#39;d think 16 GB is quite a lot but when you do the maths:<br><span style="FONT-SIZE: 11pt; FONT-FAMILY: &#39;Calibri&#39;,&#39;sans-serif&#39;; mso-ascii-theme-font: minor-latin; mso-fareast-font-family: Calibri; mso-fareast-theme-font: minor-latin; mso-hansi-theme-font: minor-latin; mso-bidi-font-family: &#39;Times New Roman&#39;; mso-bidi-theme-font: minor-bidi; mso-ansi-language: EN-GB; mso-fareast-language: EN-US; mso-bidi-language: AR-SA">2* 4 MB *&nbsp;128 procs * 8 procs/node =&nbsp;8 GB/node plus you need to double because of buffering. I was told&nbsp;by Mellanox (our cards are ConnectX cards) that they introduced XRC in OFED 1.3 in addition to Share Receive Queue which should reduce memory foot print but I have not tested this yet.</span></div>

<div><span style="FONT-SIZE: 11pt; FONT-FAMILY: &#39;Calibri&#39;,&#39;sans-serif&#39;; mso-ascii-theme-font: minor-latin; mso-fareast-font-family: Calibri; mso-fareast-theme-font: minor-latin; mso-hansi-theme-font: minor-latin; mso-bidi-font-family: &#39;Times New Roman&#39;; mso-bidi-theme-font: minor-bidi; mso-ansi-language: EN-GB; mso-fareast-language: EN-US; mso-bidi-language: AR-SA"></span>HTH,</div>

<div>Igor<br></div>
<div class="gmail_quote">2009/1/23 Gabriele Fatigati <span dir="ltr">&lt;<a href="mailto:g.fatigati@cineca.it">g.fatigati@cineca.it</a>&gt;</span><br>
<blockquote class="gmail_quote" style="PADDING-LEFT: 1ex; MARGIN: 0px 0px 0px 0.8ex; BORDER-LEFT: #ccc 1px solid">Hi Igor,<br>My message size is 4096kb and i have 4 procs per core.<br>There isn&#39;t any difference using different algorithms..<br>
<br>2009/1/23 Igor Kozin &lt;<a href="mailto:i.n.kozin@googlemail.com">i.n.kozin@googlemail.com</a>&gt;:<br>
<div>
<div></div>
<div class="Wj3C7c">&gt; what is your message size and the number of cores per node?<br>&gt; is there any difference using different algorithms?<br>&gt;<br>&gt; 2009/1/23 Gabriele Fatigati &lt;<a href="mailto:g.fatigati@cineca.it">g.fatigati@cineca.it</a>&gt;<br>
&gt;&gt;<br>&gt;&gt; Hi Jeff,<br>&gt;&gt; i would like to understand why, if i run over 512 procs or more, my<br>&gt;&gt; code stops over mpi collective, also with little send buffer. All<br>&gt;&gt; processors are locked into call, doing nothing. But, if i add<br>
&gt;&gt; MPI_Barrier &nbsp;after MPI collective, it works! I run over Infiniband<br>&gt;&gt; net.<br>&gt;&gt;<br>&gt;&gt; I know many people with this strange problem, i think there is a<br>&gt;&gt; strange interaction between Infiniband and OpenMPI that causes it.<br>
&gt;&gt;<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt; 2009/1/23 Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;:<br>&gt;&gt; &gt; On Jan 23, 2009, at 6:32 AM, Gabriele Fatigati wrote:<br>&gt;&gt; &gt;<br>
&gt;&gt; &gt;&gt; I&#39;ve noted that OpenMPI has an asynchronous behaviour in the collective<br>&gt;&gt; &gt;&gt; calls.<br>&gt;&gt; &gt;&gt; The processors, doesn&#39;t wait that other procs arrives in the call.<br>&gt;&gt; &gt;<br>
&gt;&gt; &gt; That is correct.<br>&gt;&gt; &gt;<br>&gt;&gt; &gt;&gt; This behaviour sometimes can cause some problems with a lot of<br>&gt;&gt; &gt;&gt; processors in the jobs.<br>&gt;&gt; &gt;<br>&gt;&gt; &gt; Can you describe what exactly you mean? &nbsp;The MPI spec specifically<br>
&gt;&gt; &gt; allows<br>&gt;&gt; &gt; this behavior; OMPI made specific design choices and optimizations to<br>&gt;&gt; &gt; support this behavior. &nbsp;FWIW, I&#39;d be pretty surprised if any optimized<br>&gt;&gt; &gt; MPI<br>
&gt;&gt; &gt; implementation defaults to fully synchronous collective operations.<br>&gt;&gt; &gt;<br>&gt;&gt; &gt;&gt; Is there an OpenMPI parameter to lock all process in the collective<br>&gt;&gt; &gt;&gt; call until is finished? Otherwise &nbsp;i have to insert many MPI_Barrier<br>
&gt;&gt; &gt;&gt; in my code and it is very tedious and strange..<br>&gt;&gt; &gt;<br>&gt;&gt; &gt; As you have notes, MPI_Barrier is the *only* collective operation that<br>&gt;&gt; &gt; MPI<br>&gt;&gt; &gt; guarantees to have any synchronization properties (and it&#39;s a fairly<br>
&gt;&gt; &gt; weak<br>&gt;&gt; &gt; guarantee at that; no process will exit the barrier until every process<br>&gt;&gt; &gt; has<br>&gt;&gt; &gt; entered the barrier -- but there&#39;s no guarantee that all processes leave<br>
&gt;&gt; &gt; the<br>&gt;&gt; &gt; barrier at the same time).<br>&gt;&gt; &gt;<br>&gt;&gt; &gt; Why do you need your processes to exit collective operations at the same<br>&gt;&gt; &gt; time?<br>&gt;&gt; &gt;<br>&gt;&gt; &gt; --<br>
&gt;&gt; &gt; Jeff Squyres<br>&gt;&gt; &gt; Cisco Systems<br>&gt;&gt; &gt;<br>&gt;&gt; &gt; _______________________________________________<br>&gt;&gt; &gt; users mailing list<br>&gt;&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;&gt; &gt;<br>&gt;&gt; &gt;<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt; --<br>
&gt;&gt; Ing. Gabriele Fatigati<br>&gt;&gt;<br>&gt;&gt; Parallel programmer<br>&gt;&gt;<br>&gt;&gt; CINECA Systems &amp; Tecnologies Department<br>&gt;&gt;<br>&gt;&gt; Supercomputing Group<br>&gt;&gt;<br>&gt;&gt; Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>
&gt;&gt;<br>&gt;&gt; <a href="http://www.cineca.it/" target="_blank">www.cineca.it</a> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Tel: &nbsp; +39 051 6171722<br>&gt;&gt;<br>&gt;&gt; g.fatigati [AT] <a href="http://cineca.it/" target="_blank">cineca.it</a><br>
&gt;&gt; _______________________________________________<br>&gt;&gt; users mailing list<br>&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br><br><br><br>--<br>Ing. Gabriele Fatigati<br><br>Parallel programmer<br><br>CINECA Systems &amp; Tecnologies Department<br><br>Supercomputing Group<br><br>Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br><br><a href="http://www.cineca.it/" target="_blank">www.cineca.it</a> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Tel: &nbsp; +39 051 6171722<br>
<br>g.fatigati [AT] <a href="http://cineca.it/" target="_blank">cineca.it</a><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

