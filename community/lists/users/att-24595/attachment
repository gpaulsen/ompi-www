<div dir="ltr">could you please attach output of &quot;ibv_devinfo -v&quot; and &quot;ofed_info -s&quot;<div>Thx</div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Sat, Jun 7, 2014 at 12:53 AM, Tim Miller <span dir="ltr">&lt;<a href="mailto:btamiller@gmail.com" target="_blank">btamiller@gmail.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Hi Josh,<div><br></div><div>I asked one of our more advanced users to add the &quot;-mca <span style="font-family:arial,sans-serif;font-size:13px">btl_openib_if_include mlx4_0:1&quot; argument to his job script. Unfortunately, the same error occurred as before.</span></div>

<div><span style="font-family:arial,sans-serif;font-size:13px"><br></span></div><div><span style="font-family:arial,sans-serif;font-size:13px">We&#39;ll keep digging on our end; if you have any other suggestions, please let us know.</span></div>
<span class="HOEnZb"><font color="#888888">
<div><span style="font-family:arial,sans-serif;font-size:13px"><br></span></div><div><span style="font-family:arial,sans-serif;font-size:13px">Tim</span></div></font></span></div><div class="HOEnZb"><div class="h5"><div class="gmail_extra">
<br><br><div class="gmail_quote">
On Thu, Jun 5, 2014 at 7:32 PM, Tim Miller <span dir="ltr">&lt;<a href="mailto:btamiller@gmail.com" target="_blank">btamiller@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">

<div dir="ltr">Hi Josh,<div><br></div><div>Thanks for attempting to sort this out. In answer to your questions:</div><div><br></div><div>1. Node allocation is done by TORQUE, however we don&#39;t use the TM API to launch jobs (long story). Instead, we just pass a hostfile to mpirun, and mpirun uses the ssh launcher to actually communicate and launch the processes on remote nodes.</div>


<div>2. We have only one port per HCA (the HCA silicon is integrated with the motherboard on most of our nodes, including all that have this issue). They are all configured to use InfiniBand (no IPoIB or other protocols).</div>


<div>3. No, we don&#39;t explicitly ask for a device port pair. We will try your suggestion and report back.</div><div><br></div><div>Thanks again!</div><span><font color="#888888"><div><br></div><div>Tim</div>
</font></span></div><div><div><div class="gmail_extra"><br><br>
<div class="gmail_quote">On Thu, Jun 5, 2014 at 2:22 PM, Joshua Ladd <span dir="ltr">&lt;<a href="mailto:jladd.mlnx@gmail.com" target="_blank">jladd.mlnx@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">


<div dir="ltr"><div><div>Strange indeed. This info (remote adapter info) is passed around in the modex and the struct is locally populated during add procs. <br><br></div>1. How do you launch jobs? Mpirun, srun, or something else?<br>



2. How many active ports do you have on each HCA? Are they all configured to use IB? <br>3. Do you explicitly ask for a device:port pair with the &quot;if include&quot; mca param? If not, can you please add &quot;-mca btl_openib_if_include mlx4_0:1&quot; (assuming you have a ConnectX-3 HCA and port 1 is configured to run over IB.) <br>



<br></div>Josh<br></div><div><div><div class="gmail_extra"><br><br><div class="gmail_quote">On Wed, Jun 4, 2014 at 12:47 PM, Tim Miller <span dir="ltr">&lt;<a href="mailto:btamiller@gmail.com" target="_blank">btamiller@gmail.com</a>&gt;</span> wrote:<br>



<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Hi,<div><br></div><div>I&#39;d like to revive this thread, since I am still periodically getting errors of this type. I have built 1.8.1 with --enable-debug and run with -mca btl_openib_verbose 10. Unfortunately, this doesn&#39;t seem to provide any additional information that I can find useful. I&#39;ve gone ahead and attached a dump of the output under 1.8.1. The key lines are:</div>




<div><br></div><div><div>--------------------------------------------------------------------------</div><div><div>Open MPI detected two different OpenFabrics transport types in the same Infiniband network.</div>
<div>Such mixed network trasport configuration is not supported by Open MPI.</div>
<div><br></div></div><div>  Local host:            w1</div><div><div>  Local adapter:         mlx4_0 (vendor 0x2c9, part ID 26428)</div><div>  Local transport type:  MCA_BTL_OPENIB_TRANSPORT_IB</div><div><br></div>
</div><div>  Remote host:           w16</div><div>
<div>  Remote Adapter:        (vendor 0x2c9, part ID 26428)</div><div>  Remote transport type: MCA_BTL_OPENIB_TRANSPORT_UNKNOWN</div></div><div>-------------------------------------------------------------------------</div>



</div>
<div><br></div><div>Note that the vendor and part IDs are the same. If I immediately run on the same two nodes using MVAPICH2, everything is fine.</div><div><br></div><div>I&#39;m really very befuddled by this. OpenMPI sees that the two cards are the same and made by the same vendor, yet it thinks the transport types are different (and one is unknown). I&#39;m hoping someone with some experience with how the OpenIB BTL works can shed some light on this problem...</div>



<span><font color="#888888">
<div><br></div><div>Tim</div></font></span></div><div><div><div class="gmail_extra"><br><br><div class="gmail_quote">On Fri, May 9, 2014 at 7:39 PM, Joshua Ladd <span dir="ltr">&lt;<a href="mailto:jladd.mlnx@gmail.com" target="_blank">jladd.mlnx@gmail.com</a>&gt;</span> wrote:<br>




<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><br>Just wondering if you&#39;ve tried with the latest stable OMPI, 1.8.1? I&#39;m wondering if this is an issue with the OOB. If you have a debug build, you can run -mca btl_openib_verbose 10 <br>




<br></div>
Josh<br></div><div class="gmail_extra"><br><br><div class="gmail_quote"><div>On Fri, May 9, 2014 at 6:26 PM, Joshua Ladd <span dir="ltr">&lt;<a href="mailto:jladd.mlnx@gmail.com" target="_blank">jladd.mlnx@gmail.com</a>&gt;</span> wrote:<br>





</div><div><div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><div>Hi, Tim<br><br></div>Run &quot;ibstat&quot; on each host:<br><br>1. Make sure the adapters are alive and active.  <br>





<br></div><div>2. Look at the Link Layer settings for host w34. Does it match host w4&#39;s? <br>
</div><div><br><br></div>Josh<br></div><div class="gmail_extra"><br><br><div class="gmail_quote"><div><div>On Fri, May 9, 2014 at 1:18 PM, Tim Miller <span dir="ltr">&lt;<a href="mailto:btamiller@gmail.com" target="_blank">btamiller@gmail.com</a>&gt;</span> wrote:<br>






</div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div><div dir="ltr"><div><div><div><div>Hi All,<br><br></div>We&#39;re using OpenMPI 1.7.3 with Mellanox ConnectX InfiniBand adapters, and periodically our jobs abort at start-up with the following error:<br>






<br>===<br>Open MPI detected two different OpenFabrics transport types in the same Infiniband network.<br>
Such mixed network trasport configuration is not supported by Open MPI.<br><br>  Local host:            w4<br>  Local adapter:         mlx4_0 (vendor 0x2c9, part ID 26428)<br>  Local transport type:  MCA_BTL_OPENIB_TRANSPORT_IB<br>







<br>  Remote host:           w34<br>  Remote Adapter:        (vendor 0x2c9, part ID 26428)<br>  Remote transport type: MCA_BTL_OPENIB_TRANSPORT_UNKNOWN<br>===<br><br></div>I&#39;ve done a bit of googling and not found very much. We do not see this issue when we run with MVAPICH2 on the same sets of nodes.<br>







<br></div>Any advice or thoughts would be very welcome, as I am stumped by what causes this. The nodes are all running Scientific Linux 6 with Mellanox drivers installed via the SL-provided RPMs.<span><font color="#888888"><br>






<br></font></span></div><span><font color="#888888">Tim<br><div>
<div><div><br></div></div></div></font></span></div>
<br></div></div>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>
</blockquote></div></div></div><br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>
</div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>
</div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>
</div></div></blockquote></div><br></div>
</div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>

