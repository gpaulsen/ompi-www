<html><head><meta http-equiv="Content-Type" content="text/html charset=us-ascii"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><br><div><div>On Jun 6, 2013, at 9:29 AM, "Nima Aghajari" &lt;<a href="mailto:greyy@gmx.net">greyy@gmx.net</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div><div style="font-family: Verdana;font-size: 12.0px;">
<div>Hello,</div>

<div>first of all thanks for your reply. I tried specifying the --slot-list option like you proposed. This will unfortunately lead to the same mpirun with 5 cores. Adding another slot-list command for the second program, e.g.</div>

<div>&nbsp; &nbsp;</div>

<div>&nbsp; &nbsp;&nbsp; mpirun -np 4 --slot-list 0-3 prog_1 : -np 1 --slot-list 0 prog_2</div>

<div>&nbsp;</div>

<div>will actually run on only 4 cores, but now it takes more than triple the time as needed before on 5 cores. I suppose there should be some overhead because of the oversubscription but that definitely seems too much. Any suggestions?</div></div></div></blockquote><div><br></div>Not really - oversubscription is pretty painful, depending on what mechanism you use to "idle" prog_2. It could be polling if you aren't careful. You also have to see how you are "idling" prog_1 once it completes its task - again, it could be polling hard if it is in a barrier in MPI_Finalize.</div><div><br></div><div><br></div><div><blockquote type="cite"><div><div style="font-family: Verdana;font-size: 12.0px;">
</div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></body></html>
