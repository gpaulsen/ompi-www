<div dir="ltr"><div>Dear All</div><div> </div><div>I am getting infiniband errors while running mpirun applications on cluster. I get these errors even when I don&#39;t include infiniband usage flags in mpirun command. Please guide</div>
<div> </div><div>mpirun -np 72 -hostfile hostlist ../bin/regcmMPI <a href="http://regcm.in">regcm.in</a></div><div> </div><div>--------------------------------------------------------------------------<br>[[59183,1],24]: A high-performance Open MPI point-to-point messaging module<br>
was unable to find any relevant network interfaces:</div><p>Module: OpenFabrics (openib)<br>  Host: compute-01-10.private.dns.zone</p><p>Another transport will be used instead, although this may result in<br>lower performance.<br>
--------------------------------------------------------------------------<br>--------------------------------------------------------------------------<br>WARNING: There are more than one active ports on host &#39;compute-01-15.private.dns.zone&#39;, but the<br>
default subnet GID prefix was detected on more than one of these<br>ports.  If these ports are connected to different physical IB<br>networks, this configuration will fail in Open MPI.  This version of<br>Open MPI requires that every physically separate IB subnet that is<br>
used between connected MPI processes must have different subnet ID<br>values.</p><p>Please see this FAQ entry for more details:</p><p>  <a href="http://www.open-mpi.org/faq/?category=openfabrics#ofa-default-subnet-gid">http://www.open-mpi.org/faq/?category=openfabrics#ofa-default-subnet-gid</a></p>
<p>NOTE: You can turn off this warning by setting the MCA parameter<br>      btl_openib_warn_default_gid_prefix to 0.<br>--------------------------------------------------------------------------</p><p>  This is RegCM trunk<br>
   SVN Revision: tag 4.3.5.6 compiled at: data : Sep  3 2013  time: 05:10:53</p><p>[<a href="http://pmd.pakmet.com:03309">pmd.pakmet.com:03309</a>] 15 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics<br>
[<a href="http://pmd.pakmet.com:03309">pmd.pakmet.com:03309</a>] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>[<a href="http://pmd.pakmet.com:03309">pmd.pakmet.com:03309</a>] 47 more processes have sent help message help-mpi-btl-openib.txt / default subnet prefix<br>
[compute-01-03.private.dns.zone][[59183,1],1][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.108.10 failed: No route to host (113)<br>[compute-01-03.private.dns.zone][[59183,1],2][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.108.10 failed: No route to host (113)<br>
[compute-01-03.private.dns.zone][[59183,1],5][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.108.10 failed: No route to host (113)<br>[compute-01-03.private.dns.zone][[59183,1],3][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] [compute-01-03.private.dns.zone][[59183,1],0][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.108.10 failed: No route to host (113)<br>
[compute-01-03.private.dns.zone][[59183,1],7][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.108.10 failed: No route to host (113)<br>connect() to 192.168.108.10 failed: No route to host (113)<br>
[compute-01-03.private.dns.zone][[59183,1],6][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.108.10 failed: No route to host (113)<br>[compute-01-03.private.dns.zone][[59183,1],4][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.108.10 failed: No route to host (113)<br clear="all">
<br>Ahsan<br></p>
</div>

