<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
  </head>
  <body text="#000000" bgcolor="#FFFFFF">
    Hello Cristian,<br>
    <br>
    TAU is still under active development and the developers respond
    fairly fast to emails. The latest version, 2.24.1, came out just two
    months ago. Check out
    <a class="moz-txt-link-freetext" href="https://www.cs.uoregon.edu/research/tau/home.php">https://www.cs.uoregon.edu/research/tau/home.php</a> for more
    information.<br>
    <br>
    If you are running in to issues getting the latest version of TAU to
    work with Open MPI 1.8.x, check out the "Contact" page from the
    above URL. The developers are very helpful.<br>
    <br>
    Thanks,<br>
    David<br>
    <br>
    <div class="moz-cite-prefix">On 07/22/2015 02:42 AM, Crisitan RUIZ
      wrote:<br>
    </div>
    <blockquote cite="mid:55AF575F.7010003@inria.fr" type="cite">Thank
      you for your answer Harald
      <br>
      <br>
      Actually I was already using TAU before but it seems that it is
      not maintained any more and there are problems when instrumenting
      applications with the version 1.8.5 of OpenMPI.
      <br>
      <br>
      I was using the OpenMPI 1.6.5 before to test the execution of HPC
      application on Linux containers. I tested the performance of NAS
      benchmarks in three different configurations:
      <br>
      <br>
      - 8 Linux containers on the same machine configured with 2 cores
      <br>
      - 8 physical machines
      <br>
      - 1 physical machine
      <br>
      <br>
      So, as I already described it, each machine counts with 2
      processors (8 cores each). I instrumented and run all NAS
      benchmark in these three configurations and I got the results that
      I attached in this email.
      <br>
      In the table "native" corresponds to using 8 physical machines and
      "SM" corresponds to 1 physical machine using the sm module, time
      is given in miliseconds.
      <br>
      <br>
      What surprise me in the results is that using containers in the
      worse case have equal communication time than just using plain mpi
      processes. Even though the containers use virtual network
      interfaces to communicate between them. Probably this behaviour is
      due to process binding and locality, I wanted to redo the test
      using OpenMPI version 1.8.5 but unfourtunately I couldn't
      sucessfully instrument the applications. I was looking for another
      MPI profiler but I couldn't find any. HPCToolkit looks like it is
      not maintain anymore, Vampir does not maintain any more the tool
      that instrument the application.  I will probably give a try to
      Paraver.
      <br>
      <br>
      <br>
      <br>
      Best regards,
      <br>
      <br>
      Cristian Ruiz
      <br>
      <br>
      <br>
      <br>
      On 07/22/2015 09:44 AM, Harald Servat wrote:
      <br>
      <blockquote type="cite">
        <br>
        Cristian,
        <br>
        <br>
          you might observe super-speedup heres because in 8 nodes you
        have 8 times the cache you have in only 1 node. You can also
        validate that by checking for cache miss activity using the
        tools that I mentioned in my other email.
        <br>
        <br>
        Best regards.
        <br>
        <br>
        On 22/07/15 09:42, Crisitan RUIZ wrote:
        <br>
        <blockquote type="cite">Sorry, I've just discovered that I was
          using the wrong command to run on
          <br>
          8 machines. I have to get rid of the "-np 8"
          <br>
          <br>
          So, I corrected the command and I used:
          <br>
          <br>
          mpirun --machinefile machine_mpi_bug.txt --mca btl self,sm,tcp
          <br>
          --allow-run-as-root mg.C.8
          <br>
          <br>
          And got these results
          <br>
          <br>
          8 cores:
          <br>
          <br>
          Mop/s total     =                 19368.43
          <br>
          <br>
          <br>
          8 machines
          <br>
          <br>
          Mop/s total     =                 96094.35
          <br>
          <br>
          <br>
          Why is the performance of mult-node almost 4 times better than
          <br>
          multi-core? Is this normal behavior?
          <br>
          <br>
          On 07/22/2015 09:19 AM, Crisitan RUIZ wrote:
          <br>
          <blockquote type="cite">
            <br>
             Hello,
            <br>
            <br>
            I'm running OpenMPI 1.8.5 on a cluster with the following
            <br>
            characteristics:
            <br>
            <br>
            Each node is equipped with two Intel Xeon E5-2630v3
            processors (with 8
            <br>
            cores each), 128 GB of RAM and a 10 Gigabit Ethernet
            adapter.
            <br>
            <br>
            When I run the NAS benchmarks using 8 cores in the same
            machine, I'm
            <br>
            getting almost the same performance as using 8 machines
            running a mpi
            <br>
            process per machine.
            <br>
            <br>
            I used the following commands:
            <br>
            <br>
            for running multi-node:
            <br>
            <br>
            mpirun -np 8 --machinefile machine_file.txt --mca btl
            self,sm,tcp
            <br>
            --allow-run-as-root mg.C.8
            <br>
            <br>
            for running in with 8 cores:
            <br>
            <br>
            mpirun -np 8  --mca btl self,sm,tcp --allow-run-as-root
            mg.C.8
            <br>
            <br>
            <br>
            I got the following results:
            <br>
            <br>
            8 cores:
            <br>
            <br>
             Mop/s total     =                 19368.43
            <br>
            <br>
            8 machines:
            <br>
            <br>
            Mop/s total     =                 19326.60
            <br>
            <br>
            <br>
            The results are similar for other benchmarks. Is this
            behavior normal?
            <br>
            I was expecting to see a better behavior using 8 cores given
            that they
            <br>
            use directly the memory to communicate.
            <br>
            <br>
            Thank you,
            <br>
            <br>
            Cristian
            <br>
            _______________________________________________
            <br>
            users mailing list
            <br>
            <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
            <br>
            Subscription:
            <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
            <br>
            Link to this post:
            <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2015/07/27295.php">http://www.open-mpi.org/community/lists/users/2015/07/27295.php</a>
            <br>
          </blockquote>
          <br>
          _______________________________________________
          <br>
          users mailing list
          <br>
          <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
          <br>
          Subscription:
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
          <br>
          Link to this post:
          <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2015/07/27297.php">http://www.open-mpi.org/community/lists/users/2015/07/27297.php</a>
          <br>
        </blockquote>
        <br>
        <br>
        WARNING / LEGAL TEXT: This message is intended only for the use
        of the
        <br>
        individual or entity to which it is addressed and may contain
        <br>
        information which is privileged, confidential, proprietary, or
        exempt
        <br>
        from disclosure under applicable law. If you are not the
        intended
        <br>
        recipient or the person responsible for delivering the message
        to the
        <br>
        intended recipient, you are strictly prohibited from disclosing,
        <br>
        distributing, copying, or in any way using this message. If you
        have
        <br>
        received this communication in error, please notify the sender
        and
        <br>
        destroy and delete any copies you may have received.
        <br>
        <br>
        <a class="moz-txt-link-freetext" href="http://www.bsc.es/disclaimer">http://www.bsc.es/disclaimer</a>
        <br>
        _______________________________________________
        <br>
        users mailing list
        <br>
        <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
        <br>
        Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
        <br>
        Link to this post:
        <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2015/07/27298.php">http://www.open-mpi.org/community/lists/users/2015/07/27298.php</a>
        <br>
      </blockquote>
      <br>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2015/07/27300.php">http://www.open-mpi.org/community/lists/users/2015/07/27300.php</a></pre>
    </blockquote>
    <br>
    <pre class="moz-signature" cols="72">-- 
David Shrader
HPC-3 High Performance Computer Systems
Los Alamos National Lab
Email: dshrader &lt;at&gt; lanl.gov
</pre>
  </body>
</html>

