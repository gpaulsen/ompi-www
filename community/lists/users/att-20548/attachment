<html><head><meta http-equiv="Content-Type" content="text/html charset=iso-8859-1"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><br><div><div>On Oct 26, 2012, at 4:14 AM, Nicolas Deladerriere &lt;<a href="mailto:nicolas.deladerriere@gmail.com">nicolas.deladerriere@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr">Thanks all for your comments<br><br>Ralph<br><br>What I was initially looking at is a tool (or option of orte-clean) that clean up the mess you are talking about, but only the mess that have been created by a single mpirun command. As far I have understood, orte-clean clean all mess on a node associated to all open-mpi process that have run (or are currently running).<br></div></blockquote><div><br></div>That is correct. We could fairly easily modify it to cleanup leftover files from a single mpirun without affecting others. Unfortunately, there really isn't any easy way to tell what processes belong to a specific mpirun, so selectively killing zombies would be very hard to do</div><div><br><blockquote type="cite"><div dir="ltr">
<br>According to Rolph comment, usually, mpirun command does not leave any zombie processes, Hence it seems that the effect of orte-clean is limited. But, since it exists, I was wondering that it is doing usefull stuff ?<br></div></blockquote><div><br></div>It was created during the early years of our work when zombies were frequently occurring. The need for it has declined over the years, but we keep it around because we do still hit problems on occasion - especially during development.</div><div><br><blockquote type="cite"><div dir="ltr">
<br>Cheers,<br>Nicolas<br><br><div class="gmail_quote">2012/10/25 Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span><br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Okay, now I'm confused. If all you want to do is cleanly "kill" a running OMPI job, then why not just issue<br>
<br>
$ kill SIGTERM &lt;pid-for-that-mpirun&gt;<br>
<br>
This will cause mpirun to order the clean termination of all remote procs within that execution, and then cleanly terminate itself. No tool we create could do it any better.<br>
<br>
Is there an issue with doing so?<br>
<br>
orte-clean was intended to cleanup the mess if/when the above method doesn't work - i.e., when you have to "kill SIGKILL mpirun", which forcibly kills mpirun but might leave zombie orteds on the remote nodes.<br>

<div class="HOEnZb"><div class="h5"><br>
<br>
On Oct 24, 2012, at 10:39 AM, Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br>
<br>
&gt; Or perhaps cloned, renamed to orte-kill, and modified to kill a single (or multiple) specific job(s). &nbsp;That would be POSIX-like ("kill" vs. "clean").<br>
&gt;<br>
&gt;<br>
&gt; On Oct 24, 2012, at 1:32 PM, Rolf vandeVaart wrote:<br>
&gt;<br>
&gt;&gt; And just to give a little context, ompi-clean was created initially to "clean" up a node, not for cleaning up a specific job. &nbsp;It was for the case where MPI jobs would leave some files behind or leave some processes running. &nbsp;(I do not believe this happens much at all anymore.) &nbsp;But, as was said, no reason it could not be modified.<br>

&gt;&gt;<br>
&gt;&gt;&gt; -----Original Message-----<br>
&gt;&gt;&gt; From: <a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a> [mailto:<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>]<br>
&gt;&gt;&gt; On Behalf Of Jeff Squyres<br>
&gt;&gt;&gt; Sent: Wednesday, October 24, 2012 12:56 PM<br>
&gt;&gt;&gt; To: Open MPI Users<br>
&gt;&gt;&gt; Subject: Re: [OMPI users] ompi-clean on single executable<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; ...but patches would be greatly appreciated. &nbsp;:-)<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Oct 24, 2012, at 12:24 PM, Ralph Castain wrote:<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; All things are possible, including what you describe. Not sure when we<br>
&gt;&gt;&gt; would get to it, though.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; On Oct 24, 2012, at 4:01 AM, Nicolas Deladerriere<br>
&gt;&gt;&gt; &lt;<a href="mailto:nicolas.deladerriere@gmail.com">nicolas.deladerriere@gmail.com</a>&gt; wrote:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; Reuti,<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; The problem I am facing is a small small part of our production<br>
&gt;&gt;&gt;&gt;&gt; system, and I cannot modify our mpirun submission system. This is why<br>
&gt;&gt;&gt;&gt;&gt; i am looking at solution using only ompi-clean of mpirun command<br>
&gt;&gt;&gt;&gt;&gt; specification.<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; Thanks,<br>
&gt;&gt;&gt;&gt;&gt; Nicolas<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; 2012/10/24, Reuti &lt;<a href="mailto:reuti@staff.uni-marburg.de">reuti@staff.uni-marburg.de</a>&gt;:<br>
&gt;&gt;&gt;&gt;&gt;&gt; Am 24.10.2012 um 11:33 schrieb Nicolas Deladerriere:<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Reuti,<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Thanks for your comments,<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; In our case, we are currently running different mpirun commands on<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; clusters sharing the same frontend. Basically we use a wrapper to<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; run the mpirun command and to run an ompi-clean command to clean<br>
&gt;&gt;&gt; up<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; the mpi job if required.<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Using ompi-clean like this just kills all other mpi jobs running on<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; same frontend. I cannot use queuing system<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; Why? Using it on a single machine was only one possible setup. Its<br>
&gt;&gt;&gt;&gt;&gt;&gt; purpose is to distribute jobs to slave hosts. If you have already<br>
&gt;&gt;&gt;&gt;&gt;&gt; one frontend as login-machine it fits perfect: the qmaster (in case<br>
&gt;&gt;&gt;&gt;&gt;&gt; of SGE) can run there and the execd on the nodes.<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; -- Reuti<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; as you have suggested this<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; is why I was wondering a option or other solution associated to<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; ompi-clean command to avoid this general mpi jobs cleaning.<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Cheers<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Nicolas<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 2012/10/24, Reuti &lt;<a href="mailto:reuti@staff.uni-marburg.de">reuti@staff.uni-marburg.de</a>&gt;:<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hi,<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Am 24.10.2012 um 09:36 schrieb Nicolas Deladerriere:<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; I am having issue running ompi-clean which clean up (this is<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; normal) session associated to a user which means it kills all<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; running jobs assoicated to this session (this is also normal).<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; But I would like to be able to clean up session associated to a<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; job (a not user).<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Here is my point:<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; I am running two executable :<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; % mpirun -np 2 myexec1<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &nbsp; --&gt; run with PID 2399 ...<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; % mpirun -np 2 myexec2<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &nbsp; --&gt; run with PID 2402 ...<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; When I run orte-clean I got this result :<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; % orte-clean -v<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; orte-clean: cleaning session dir tree<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; openmpi-sessions-ndelader@myhost_0<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; orte-clean: killing any lingering procs<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; orte-clean: found potential rogue orterun process<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; (pid=2399,user=ndelader), sending SIGKILL...<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; orte-clean: found potential rogue orterun process<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; (pid=2402,user=ndelader), sending SIGKILL...<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Which means that both jobs have been killed :-( Basically I would<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; like to perform orte-clean using executable name or PID or<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; whatever that identify which job I want to stop an clean. It<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; seems I would need to create an openmpi session per job. Does it<br>
&gt;&gt;&gt; make sense ?<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; And<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; I would like to be able to do something like following command<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; and get following result :<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; % orte-clean -v myexec1<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; orte-clean: cleaning session dir tree<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; openmpi-sessions-ndelader@myhost_0<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; orte-clean: killing any lingering procs<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; orte-clean: found potential rogue orterun process<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; (pid=2399,user=ndelader), sending SIGKILL...<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Does it make sense ? Is there a way to perform this kind of<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; selection in cleaning process ?<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; How many jobs are you starting on how many nodes at one time? This<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; requirement could be a point to start to use a queuing system,<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; where can remove job individually and also serialize your<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; workflow. In fact: we use GridEngine also local on workstations<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; for this purpose.<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; -- Reuti<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; --<br>
&gt;&gt;&gt; Jeff Squyres<br>
&gt;&gt;&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt;&gt;&gt; For corporate legal information go to:<br>
&gt;&gt;&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; -----------------------------------------------------------------------------------<br>
&gt;&gt; This email message is for the sole use of the intended recipient(s) and may contain<br>
&gt;&gt; confidential information. &nbsp;Any unauthorized review, use, disclosure or distribution<br>
&gt;&gt; is prohibited. &nbsp;If you are not the intended recipient, please contact the sender by<br>
&gt;&gt; reply email and destroy all copies of the original message.<br>
&gt;&gt; -----------------------------------------------------------------------------------<br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Jeff Squyres<br>
&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></body></html>
