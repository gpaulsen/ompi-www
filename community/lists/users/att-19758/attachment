<html><body><div style="color:#000; background-color:#fff; font-family:times new roman, new york, times, serif;font-size:12pt"><div><span>Hi.</span></div>
<div><br>
  <span></span></div>
<div><span>To answer the differents remarks :</span></div>
<div><br>
  <span></span></div>
<div><span>1) Code Saturne launch itself embedded python and bash scripts with the mpiexec parameters, but I will test</span></div>
<div><span>your parameter next week and will give you the result of this benchmark.<br>
  </span></div>
<div><br>
  <span></span></div>
<div><span>2) I do not think there is a problem with the load balancing : Code Saturne partitions itself</span></div>
<div><span>the mesh with the reliable and well-known Metis library which is the graph partitioner. So CPU</span></div>
<div><span>are equally busy. <br>
  </span></div>
<div><br>
  <span></span></div>
<div><span>3) CPUs are Xeon which have multithreading capabilities. However I have tested it</span></div>
<div><span>by setting np=24 in the server_priv/nodes file of the PBS server, and compared that</span></div>
<div><span>with a configuration of np=12. The results are very similar : there is no gain of 20% or 30%</span></div>
<div><span><br>
  </span></div>
<div><span> 4) I will examine the hardware options as you have suggested but I will have to convince my</span></div>
<div><span>office for such investissment !</span></div><div><br></div>  <div style="font-family: times new roman, new york, times, serif; font-size: 12pt;"> <div style="font-family: times new roman, new york, times, serif; font-size: 12pt;"> <div dir="ltr"> <font face="Arial" size="2"> <hr size="1">  <b><span style="font-weight:bold;">De&nbsp;:</span></b> Gus Correa &lt;gus@ldeo.columbia.edu&gt;<br> <b><span style="font-weight: bold;">À&nbsp;:</span></b> Open MPI Users &lt;users@open-mpi.org&gt; <br> <b><span style="font-weight: bold;">Envoyé le :</span></b> Mercredi 11 juillet 2012 0h51<br> <b><span style="font-weight: bold;">Objet&nbsp;:</span></b> Re: [OMPI users] Bad parallel scaling using Code Saturne with openmpi<br> </font> </div> <br>On 07/10/2012 05:31 PM, Jeff Squyres wrote:<br>&gt; +1.&nbsp; Also, not all Ethernet switches are created equal --<br>&gt; particularly commodity 1GB Ethernet switches.<br>&gt; I've seen plenty of crappy Ethernet
 switches rated for 1GB<br>&gt; that could not reach that speed when under load.<br>&gt;<br><br>Are you perhaps belittling my dear $43 [brand undisclosed]<br>5-port GigE SoHo switch, that connects my Pentium-III<br>toy cluster, just because it drops a few packages [per microsec]?<br>It looks so good, with all those fiercely blinking green LEDs.<br>Where else could I fool around with cluster setup and test<br>the OpenMPI new releases? :)<br>The production cluster is just too crowded for this,<br>maybe because it has a decent<br>HP GigE switch [IO] and Infiniband [MPI] ...<br><br>Gus<br><br><br>&gt;<br>&gt;<br>&gt; On Jul 10, 2012, at 10:47 AM, Ralph Castain wrote:<br>&gt;<br>&gt;&gt; I suspect it mostly reflects communication patterns. I don't know anything about Saturne, but shared memory is a great deal faster than TCP, so the more processes sharing a node the better. You may also be hitting some natural boundary in your model - perhaps with 8
 processes/node you wind up with more processes that cross the node boundary, further increasing the communication requirement.<br>&gt;&gt;<br>&gt;&gt; Do things continue to get worse if you use all 4 nodes with 6 processes/node?<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt; On Jul 10, 2012, at 7:31 AM, Dugenoux Albert wrote:<br>&gt;&gt;<br>&gt;&gt;&gt; Hi.<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; I have recently built a cluster upon a Dell PowerEdge Server with a Debian 6.0 OS. This server is composed of<br>&gt;&gt;&gt; 4 system board of 2 processors of hexacores. So it gives 12 cores per system board.<br>&gt;&gt;&gt; The boards are linked with a local Gbits switch.<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; In order to parallelize the software Code Saturne, which is a CFD solver, I have configured the cluster<br>&gt;&gt;&gt; such that there are a pbs server/mom on 1 system board and 3 mom and the 3 others cards. So this leads to<br>&gt;&gt;&gt; 48 cores dispatched on 4 nodes of 12
 CPU. Code saturne is compiled with the openmpi 1.6 version.<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; When I launch a simulation using 2 nodes with 12 cores, elapse time is good and network traffic is not full.<br>&gt;&gt;&gt; But when I launch the same simulation using 3 nodes with 8 cores, elapse time is 5 times the previous one.<br>&gt;&gt;&gt; I both cases, I use 24 cores and network seems not to be satured.<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; I have tested several configurations : binaries in local file system or on a NFS. But results are the same.<br>&gt;&gt;&gt; I have visited severals forums (in particular <a href="http://www.open-mpi.org/community/lists/users/2009/08/10394.php" target="_blank">http://www.open-mpi.org/community/lists/users/2009/08/10394.php</a>)<br>&gt;&gt;&gt; and read lots of threads, but as I am not an expert at clusters, I presently do not see where it is wrong !<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; Is it a problem in the configuration of PBS
 (I have installed it from the deb packages), a subtile compilation options<br>&gt;&gt;&gt; of openMPI, or a bad network configuration ?<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; Regards.<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; B. S.<br>&gt;&gt;&gt; _______________________________________________<br>&gt;&gt;&gt; users mailing list<br>&gt;&gt;&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;&gt;<br>&gt;&gt; _______________________________________________<br>&gt;&gt; users mailing list<br>&gt;&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
 target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<br>&gt;<br><br>_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><br> </div> </div>  </div></body></html>
