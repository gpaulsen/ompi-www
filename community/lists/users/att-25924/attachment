<div dir="ltr"><div><div><div>Hi,<br><br></div>This should be fixed in OMPI 1.8.3. Is it possible for you to give 1.8.3 a shot?<br><br></div>Best,<br><br></div>Josh<br></div><div class="gmail_extra"><br><div class="gmail_quote">On Mon, Dec 8, 2014 at 8:43 AM, Götz Waschk <span dir="ltr">&lt;<a href="mailto:goetz.waschk@gmail.com" target="_blank">goetz.waschk@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Dear Open-MPI experts,<br>
<br>
I have updated my little cluster from Scientific Linux 6.5 to 6.6,<br>
this included extensive changes in the Infiniband drivers and a newer<br>
openmpi version (1.8.1). Now I&#39;m getting this message on all nodes<br>
with more than 32 GB of RAM:<br>
<br>
<br>
WARNING: It appears that your OpenFabrics subsystem is configured to only<br>
allow registering part of your physical memory.  This can cause MPI jobs to<br>
run with erratic performance, hang, and/or crash.<br>
<br>
This may be caused by your OpenFabrics vendor limiting the amount of<br>
physical memory that can be registered.  You should investigate the<br>
relevant Linux kernel module parameters that control how much physical<br>
memory can be registered, and increase them to allow registering all<br>
physical memory on your machine.<br>
<br>
See this Open MPI FAQ item for more information on these Linux kernel module<br>
parameters:<br>
<br>
    <a href="http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages" target="_blank">http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages</a><br>
<br>
  Local host:              pax98<br>
  Registerable memory:     32768 MiB<br>
  Total memory:            49106 MiB<br>
<br>
Your MPI job will continue, but may be behave poorly and/or hang.<br>
<br>
<br>
The issue is similar to the one described in a previous thread about<br>
Ubuntu nodes:<br>
<a href="http://www.open-mpi.org/community/lists/users/2014/08/25090.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25090.php</a><br>
But the Infiniband driver is different, the values log_num_mtt and<br>
log_mtts_per_seg both still exist, but they cannot be changed and have<br>
on all configurations the same values:<br>
[pax52] /root # cat /sys/module/mlx4_core/parameters/log_num_mtt<br>
0<br>
[pax52] /root # cat /sys/module/mlx4_core/parameters/log_mtts_per_seg<br>
3<br>
<br>
The kernel changelog says that Red Hat has included this commit:<br>
mlx4: Scale size of MTT table with system RAM (Doug Ledford)<br>
so it should be all fine, the buffers scale automatically, however, as<br>
far as I can see, the wrong value calculated by calculate_max_reg() is<br>
used in the code, so I think I cannot simply ignore the warning. Also,<br>
a user has reported a problem with a job, I cannot confirm that this<br>
is the cause.<br>
<br>
My workaround was to simply load the mlx5_core kernel module, as this<br>
is used by calculate_max_reg() to detect OFED 2.0.<br>
<br>
Regards, Götz Waschk<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/12/25923.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/12/25923.php</a></blockquote></div><br></div>

