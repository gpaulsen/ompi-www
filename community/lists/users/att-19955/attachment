Hi,<br><br>I have a problem with MPI_Senrecv communication where I send columns on edges between processes.<br>For debugging, I show you below a basic example where I initialize a 10x10 matrix (&quot;x0&quot; array) with x_domain=4<br>
and y_domain=4. For the test, I simply initialize the 2D array values with x0[i][j] = i+j . After, in updateBound.c&quot;, I&#39;m <br>using the MPI_Sendrecv functions for the North-South and Est-West process.<br><br>Here&#39;s the main program &quot;example.c&quot; :<br>
<br>-------------------------------------------------------------------------------------------<br><br>#include &lt;stdio.h&gt;<br>#include &lt;stdlib.h&gt;<br>#include &lt;math.h&gt;<br>#include &quot;mpi.h&quot;<br><br>
int main(int argc, char *argv[]) <br>    {<br>      /* size of the discretization */<br><br>      double** x;<br>      double** x0;<br>      <br>      int i,j,k,l;<br>      int nproc;<br>      int ndims; <br>      int S=0, E=1, N=2, W=3;<br>
      int NeighBor[4];<br>      int xcell, ycell, size_tot_x, size_tot_y;<br>      int *xs,*ys,*xe,*ye;<br>      int size_x = 4;<br>      int size_y = 4;<br>      int me;<br>      int x_domains=2;<br>      int y_domains=2;<br>
      <br>      MPI_Comm comm, comm2d;<br>      int dims[2];<br>      int periods[2];<br>      int reorganisation = 0;<br>      int row;<br>      MPI_Datatype column_type;<br><br>      <br>      <br>      size_tot_x=size_x+2*x_domains+2;<br>
      size_tot_y=size_y+2*y_domains+2;<br>      <br>      xcell=(size_x/x_domains);<br>      ycell=(size_y/y_domains);<br><br>      MPI_Init(&amp;argc, &amp;argv);<br>      comm = MPI_COMM_WORLD;<br>      MPI_Comm_size(comm,&amp;nproc);<br>
      MPI_Comm_rank(comm,&amp;me);<br><br>      x = malloc(size_tot_y*sizeof(double*));<br>      x0 = malloc(size_tot_y*sizeof(double*));<br><br><br>      for(j=0;j&lt;=size_tot_y-1;j++) {<br>        x[j] = malloc(size_tot_x*sizeof(double));<br>
        x0[j] = malloc(size_tot_x*sizeof(double));<br>      }<br><br>      xs = malloc(nproc*sizeof(int));<br>      xe = malloc(nproc*sizeof(int));<br>      ys = malloc(nproc*sizeof(int));<br>      ye = malloc(nproc*sizeof(int));<br>
<br>      /* Create 2D cartesian grid */<br>      periods[0] = 0;<br>      periods[1] = 0;<br><br>      ndims = 2;<br>      dims[0]=x_domains;<br>      dims[1]=y_domains;<br><br>      MPI_Cart_create(comm, ndims, dims, periods, reorganisation, &amp;comm2d);<br>
<br>      /* Identify neighbors */<br><br>      NeighBor[0] = MPI_PROC_NULL;<br>      NeighBor[1] = MPI_PROC_NULL;<br>      NeighBor[2] = MPI_PROC_NULL;<br>      NeighBor[3] = MPI_PROC_NULL;<br><br>      /* Left/West and right/Est neigbors */<br>
<br>      MPI_Cart_shift(comm2d,0,1,&amp;NeighBor[W],&amp;NeighBor[E]);<br><br>      /* Bottom/South and Upper/North neigbors */<br><br>      MPI_Cart_shift(comm2d,1,1,&amp;NeighBor[S],&amp;NeighBor[N]);<br><br>      /* coordinates of current cell with me rank */<br>
<br>      xcell=(size_x/x_domains);<br>      ycell=(size_y/y_domains);<br><br>      ys[me]=(y_domains-me%(y_domains)-1)*(ycell+2)+2;<br>      ye[me]=ys[me]+ycell-1;<br><br>      for(i=0;i&lt;=y_domains-1;i++) <br>      {xs[i]=2;}<br>
      <br>      for(i=0;i&lt;=y_domains-1;i++) <br>      {xe[i]=xs[i]+xcell-1;}<br><br>      for(i=1;i&lt;=(x_domains-1);i++)<br>         { for(j=0;j&lt;=(y_domains-1);j++) <br>              {<br>               xs[i*y_domains+j]=xs[(i-1)*y_domains+j]+xcell+2;<br>
               xe[i*y_domains+j]=xs[i*y_domains+j]+xcell-1;<br>              }<br>         }<br>      <br>      for(i=0;i&lt;=size_tot_y-1;i++)<br>          { for(j=0;j&lt;=size_tot_x-1;j++)<br>            { x0[i][j]= i+j;<br>
        //  printf(&quot;%f\n&quot;,x0[i][j]); <br>        }        <br>      }<br>      <br>      /*  Create column data type to communicate with South and North neighbors */<br><br>      MPI_Type_vector( ycell, 1, size_tot_x, MPI_DOUBLE, &amp;column_type);  <br>

      MPI_Type_commit(&amp;column_type);<br>
<br>      updateBound(x0, NeighBor, comm2d, column_type, me, xs, ys, xe, ye,<br>xcell);<br><br>      <br>              for(i=0;i&lt;=size_tot_y-1;i++)<br>           {<br>            free(x[i]);<br>            free(x0[i]);<br>
           }     <br>     <br>        free(x);<br>        free(x0);<br><br>        free(xs);<br>        free(xe);<br>        free(ys);<br>        free(ye);<br>     <br>        MPI_Finalize();<br><br>        return 0;<br>    }        <br>
-------------------------------------------------------------------------------------------<br><br>and the second file &quot;updateBound.c&quot; which sends the columns and rows<br><br><br>-------------------------------------------------------------------------------------------<br>
<br><br>#include &quot;mpi.h&quot;<br>#include &lt;stdio.h&gt;<br><br>/*******************************************************************/<br>/*    Update Bounds of subdomain with me process      */<br>/*******************************************************************/<br>
<br>  void updateBound(double** x,int NeighBor[], MPI_Comm comm2d, MPI_Datatype column_type , int me, int* xs, int* ys, int* xe, int* ye, int xcell) <br>  {<br><br>  int S=0, E=1, N=2, W=3;<br>  int flag;<br>  MPI_Status status;<br>
<br>  int i,j;<br><br>         if(me==0) {printf(&quot;verif_update_before\n&quot;);<br>                    for(i=ys[me]-1;i&lt;=ye[me]+1;i++)<br>                    { for(j=xs[me]-1;j&lt;=xe[me]+1;j++)<br>                      { printf(&quot;%f &quot;,x[i][j]);<br>
                      }<br>                      printf(&quot;\n&quot;);<br>                    }<br>                    printf(&quot;\n&quot;);<br>         } <br><br>/********* North/South communication **********************************/<br>
  flag = 1;<br>  /*Send my boundary to North and receive from South*/<br>  MPI_Sendrecv(&amp;x[ys[me]][xs[me]], xcell, MPI_DOUBLE, NeighBor[N], flag, &amp;x[ye[me]+1][xs[me]], xcell, MPI_DOUBLE, NeighBor[S], flag, comm2d, &amp;status);<br>
<br>  /*Send my boundary to South and receive from North*/<br>  MPI_Sendrecv(&amp;x[ye[me]][xs[me]], xcell, MPI_DOUBLE, NeighBor[S], flag, &amp;x[ys[me]-1][xs[me]], xcell, MPI_DOUBLE, NeighBor[N], flag, comm2d, &amp;status);<br>
<br>/********* Est/West communication ************************************/<br>  flag = 2;<br>  /*Send my boundary to Est and receive from West*/<br>  MPI_Sendrecv(&amp;x[ys[me]][xe[me]], 1, column_type, NeighBor[E], flag, &amp;x[ys[me]][xs[me]-1], 1, column_type, NeighBor[W], flag, comm2d, &amp;status);<br>
<br>  /*Send my boundary to West and receive from Est*/<br>  MPI_Sendrecv(&amp;x[ys[me]][xs[me]], 1, column_type, NeighBor[W], flag, &amp;x[ys[me]][xe[me]+1], 1, column_type, NeighBor[E], flag, comm2d, &amp;status);<br><br>
         if(me==0) {printf(&quot;verif_update_after\n&quot;);<br>                    for(i=ys[me]-1;i&lt;=ye[me]+1;i++)<br>                    { for(j=xs[me]-1;j&lt;=xe[me]+1;j++)<br>                      { printf(&quot;%f &quot;,x[i][j]);<br>
                      }<br>                      printf(&quot;\n&quot;);<br>                    }<br>                    printf(&quot;\n&quot;);<br>         }<br>  }<br><br>------------------------------------------------------------------------------<br>
<br>Running with nproc=4, I print the values of the subarray with rank=0 (so at left bottom of the grid) and I get before and after the <br>bounds udpate  :<br><br>verif_update_before<br>6.000000 7.000000 8.000000 9.000000 <br>
7.000000 8.000000 9.000000 10.000000 <br>8.000000 9.000000 10.000000 11.000000 <br>9.000000 10.000000 11.000000 12.000000 <br><br>verif_update_after<br>6.000000 5.000000 6.000000 9.000000 <br>7.000000 8.000000 9.000000 12.000000 <br>
8.000000 9.000000 10.000000 <b><u>11.000000</u> </b><br>9.000000 10.000000 11.000000 12.000000 <br><br>As you can see, after the udpate, I don&#39;t have the correct value ( in underligned bold : 11.0 ) at the second element <br>
of the column coming from the Est. I expected 13.0 instead of 11.0.<br><br>So there&#39;s a problem with the column datatype which only send the first element of this column.<br><br>In &quot;example.c&quot;, I define the column as following :<br>
<br>      MPI_Type_vector( ycell, 1, size_tot_x, MPI_DOUBLE, &amp;column_type);  <br>
      MPI_Type_commit(&amp;column_type);<br>
<br>However, It seems ok and the computation of begin and end coordinates as a function of rank &quot;me&quot; is also good.<br><br>I make you notice there&#39;s no problem between the exchange of rows between the North and the South, only<br>
between columns.<br><br>If you could help me, I don&#39;t know what to do.<br><br>Regards<br><br>

