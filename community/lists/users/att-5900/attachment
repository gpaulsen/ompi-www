<br><div class="gmail_quote">On Mon, Jun 16, 2008 at 9:46 PM, Brock Palen &lt;<a href="mailto:brockp@umich.edu">brockp@umich.edu</a>&gt; wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<div style="">
Brad just curious.<div><br></div><div>Did you tweak any other values for starting and running a job on such a large system? &nbsp;You say unmodified, &nbsp;but OpenMPI lets you tweak many values at runtime. &nbsp;</div></div></blockquote>
<div><br>Ahh...sorry for the confusion.&nbsp; By &quot;unmodified&quot;, I meant at the source code level.&nbsp; The version of Open MPI used was from a straight checkout of the trunk and did not have any special code modifications for Roadrunner.&nbsp; We certainly did change some of the default run-time settings.<br>
<br></div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div style=""><div></div><div><br></div><div>I would be curious to expand what I know from what you discovered.</div>
</div></blockquote><div><br>The main run-time parameters used are as follows:<br><br>- For scalable process launch<br>&nbsp; routed = binomial<br><br>- To reduce the overhead for IB connections:<br>&nbsp; btl_openib_receive_queues = P,128,64,32,32,32:S,2048,1024,128,32:S,12288,1024,128,32:S,65536,1024,128,32<br>
<br>- Bind processes to computational cores:<br>&nbsp; opal_paffinity_alone = 1<br><br>- Leave buffers pinned and registered<br>&nbsp; mpi_leave_pinned = 1<br><br>--brad<br><br><br></div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<div style=""><div><br><div> <span style="border-collapse: separate; color: rgb(0, 0, 0); font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;"><div>
<br>Brock Palen</div><div><a href="http://www.umich.edu/%7Ebrockp" target="_blank">www.umich.edu/~brockp</a></div><div>Center for Advanced Computing</div><div><a href="mailto:brockp@umich.edu" target="_blank">brockp@umich.edu</a></div>
<div>(734)936-1985</div><br></span><br> </div><br><div><div><div></div><div class="Wj3C7c">On Jun 16, 2008, at 10:12 PM, Brad Benton wrote:<br></div></div><blockquote type="cite"><div><div></div><div class="Wj3C7c">Greetings Open MPI users; we thought you&#39;d be interested in the&nbsp; <br>
following announcement...<br><br>A new supercomputer, powered by Open MPI, has broken the petaflop&nbsp; <br>barrier to become the world&#39;s fastest supercomputer.&nbsp; The&nbsp; <br> &quot;Roadrunner&quot; system was jointly developed by Los Alamos National&nbsp; <br>
Laboratories and IBM.&nbsp; Roadrunner&#39;s design uses a cluster of AMD&nbsp; <br>dual-core processors coupled with computational accelerators based&nbsp; <br> on the IBM Cell Broadband Engine.&nbsp; The cluster consists of 3,060&nbsp; <br>nodes, each of which has 2 dual-core AMD processors and associated&nbsp; <br>
Cell accelerators.&nbsp; The AMD nodes are connected with 4x DDR&nbsp; <br>InfiniBand links.<br> <br>Open MPI was used as the communications library for the 12,240&nbsp; <br>processes comprising the Linpack run which broke the Petaflop&nbsp; <br>
barrier at 1.026 Petaflop/s.&nbsp; The version of Open MPI used in the&nbsp; <br>run-for-record was a pre-release version of the upcoming 1.3&nbsp; <br> release.&nbsp; Enhancements in this release include modifications for&nbsp; <br>efficient, scalable process launch.&nbsp; As such, Open MPI was run&nbsp; <br>
unmodified from a snapshot of the pre-1.3 source base (meaning:&nbsp; <br>there are no Roadrunner-specific enhancements that are unportable to&nbsp; <br> other environments -- all Open MPI users benefit from the&nbsp; <br>scalability and performance improvements contributed by the&nbsp; <br>
Roadrunner project).<br><br>--Brad Benton<br>Open MPI/Roadrunner Team<br></div></div><div style="margin: 0px;">_______________________________________________</div><div style="margin: 0px;">users mailing list</div><div style="margin: 0px;">
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a></div><div style="margin: 0px;"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div>
 </blockquote></div><br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

