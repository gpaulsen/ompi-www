<div dir="ltr"><div><div><div><div><div><div><div><div>Dear users,<br><br></div>     Hello, I&#39;m relatively new to building OpenMPI from scratch, so I&#39;m going to try and provide a lot of information about exactly what I did here. I&#39;m attempting to run the MHD code Flash 4.2.2 on Pleiades (NASA AMES), and also need some python mpi4py functionality and Cuda which ruled out using the pre-installed MPI implementations. My code has been tested and working under a previous build of OpenMPI 1.10.2 on a local cluster at Drexel University that does not have a job manager and that uses a simple Infiniband setup.. Pleiades is a bit more complicated, but I&#39;ve been following the NASA folks setup commands and they claim looking at my job logs from their side that nothing seems wrong communications wise.<br><br></div><div>However, when I run just a vanilla version of Flash 4.2.2 it runs for several steps and then crashes. Here&#39;s the last part of the Flash run output:<br><span style="font-family:times new roman,serif"><br></span><div><font size="1"><span style="font-family:monospace,monospace"> *** Wrote particle file to BB_hdf5_part_0008 ****</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace">      17 1.5956E+11 5.4476E+09  (-5.031E+16,  1.969E+16, -2.188E+15) |  5.448E+09</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace"> *** Wrote plotfile to BB_hdf5_plt_cnt_0009 ****</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace"> WARNING: globalNumParticles = 0!!!</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace">  iteration, no. not moved =            0          69</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace">  iteration, no. not moved =            1          29</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace">  iteration, no. not moved =            2           0</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace"> refined: total leaf blocks =          120</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace"> refined: total blocks =          137</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace">      18 1.7046E+11 5.3814E+09  (-2.516E+16,  2.734E+16, -1.094E+15) |  5.381E+09</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace"> WARNING: globalNumParticles = 0!!!</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace"> *** Wrote particle file to BB_hdf5_part_0009 ****</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace">      19 1.8122E+11 2.9425E+09  (-2.078E+16, -2.516E+16, -3.391E+16) |  2.943E+09</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace"> *** Wrote plotfile to BB_hdf5_plt_cnt_0010 ****</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace"> WARNING: globalNumParticles = 0!!!</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace">  iteration, no. not moved =            0         128</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace">  iteration, no. not moved =            1          25</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace">  iteration, no. not moved =            2           0</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace"> refined: total leaf blocks =          456</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace"> refined: total blocks =          521</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace"> Paramesh error : pe           65  needed full blk      
      1          57  but could not find it or only  found part of it in 
the message buffer.  Contact PARAMESH developers for help.</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace">------------------------------</span></font><font size="1"><span style="font-family:monospace,monospace">------------------------------</span></font><font size="1"><span style="font-family:monospace,monospace">--------------</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace">MPI_ABORT was invoked on rank 65 in communicator MPI COMMUNICATOR 3 SPLIT FROM 0 </span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace">with errorcode 0.</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace"> </span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace">NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace">You may or may not see output from other processes, depending on</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace">exactly when Open MPI kills them.</span></font></div><font size="1"><span style="font-family:monospace,monospace">
</span></font><div><font size="1"><span style="font-family:monospace,monospace">------------------------------</span></font><font size="1"><span style="font-family:monospace,monospace">------------------------------</span></font><font size="1"><span style="font-family:monospace,monospace">--------------</span></font></div><span style="font-family:monospace,monospace">
<font size="1"> Paramesh error : pe           80  needed full blk            1         
 72  but could not find it or only  found part of it in the message 
buffer.  Contact PARAMESH developers for help.</font><br></span><br></div><div>You can see the entire output at:<br><br><a href="https://drive.google.com/file/d/0B7Zx9zNTB3icQWZPTUlhZFQtcWs/view?usp=sharing">https://drive.google.com/file/d/0B7Zx9zNTB3icQWZPTUlhZFQtcWs/view?usp=sharing</a>  <br></div><div><br>Okay, so I built it with (as instructed by NASA HECC):<br></div><br><div><span style="font-family:courier new,courier,monospace">./configure --with-tm=/PBS --with-verbs=/usr --enable-mca-no-build=maffinity-libnuma
 --with-cuda=/nasa/cuda/7.0  --enable-mpi-interface-warning 
--without-slurm --without-loadleveler --enable-mpirun-prefix-by-default </span></div><span style="font-family:courier new,courier,monospace">--enable-btl-openib-failover --prefix=/u/jewall/ompi-1.10.2<br><br><br></span></div><span style="font-family:courier new,courier,monospace"><font face="arial,helvetica,sans-serif">And if I run the ompi_info on 96 cores (the same # I did the job on) I get the following output:<br><br><a href="https://drive.google.com/file/d/0B7Zx9zNTB3icSHNZaEpZZkhPcXc/view?usp=sharing">https://drive.google.com/file/d/0B7Zx9zNTB3icSHNZaEpZZkhPcXc/view?usp=sharing</a><br><br></font></span></div><span style="font-family:courier new,courier,monospace"><font face="arial,helvetica,sans-serif">And the job was run with the following script:<br><font size="1"><span style="font-family:monospace,monospace"><br>#PBS -S /bin/bash<br>#PBS -N cfd<br><br>#PBS -q debug<br>#PBS -l select=8:ncpus=12:model=has<br>#PBS -l walltime=0:30:00<br>#PBS -j oe<br>#PBS -W group_list=g23107<br>#PBS -m e<br><br># Load a compiler you use to build your executable, for example, comp-intel/2015.0.090.<br><br>#source /usr/local/lib/global.profile<br><br>module load git/2.4.5<br>module load szip/2.1/gcc<br>module load cuda/7.0<br>module load gcc/4.9.3<br>module load cmake/<a href="http://2.8.12.1">2.8.12.1</a><br>module load python/2.7.10<br><br># Add your commands here to extend your PATH, etc.<br><br>export MPIHOME=/u/jewall/ompi-1.10.2<br>export MPICC=${MPIHOME}/bin/mpicc<br>export MPIFC=${MPIHOME}/bin/mpif90<br>export MPICXX=${MPIHOME}/bin/mpic++<br>export MPIEXEC=${MPIHOME}/bin/mpiexec<br>export HDF5=/u/jewall/hdf5<br><br>setenv OMPI_MCA_btl_openib_if_include mlx4_0:1<br><br><br>PATH=$PATH:${PYTHONPATH}:$HOME/bin            # Add private commands to PATH<br><br># By default, PBS executes your job from your home directory.<br># However, you can use the environment variable<br># PBS_O_WORKDIR to change to the directory where<br># you submitted your job.<br><br>cd $PBS_O_WORKDIR<br><br>echo ${PBS_NODEFILE}<br>cat ${PBS_NODEFILE} | awk &#39;{print $1}&#39; &gt; &quot;local_host.txt&quot;<br>cat local_host.txt<br><br># use of dplace to pin processes to processors may improve performance<br># Here you request to pin processes to processors 4-11 of each Sandy Bridge node.<br># For other processor types, you may have to pin to different processors.<br><br># The resource request of select=32 and mpiprocs=8 implies<br># that you want to have 256 MPI processes in total.<br># If this is correct, you can omit the -np 256 for mpiexec<br># that you might have used before.<br><br>${MPIEXEC} --mca mpi_warn_on_fork 0 --mca mpi_cuda_support 0 --mca btl self,sm,openib --mca oob_tcp_if_include ib0 -hostfile local_host.txt ./flash4<br><br>#--mca oob_tcp_if_include ib0 # suggested in an OpenMPI forum for Pleiades running<br><br># It is a good practice to write stderr and stdout to a file (ex: output)<br># Otherwise, they will be written to the PBS stderr and stdout in /PBS/spool,<br># which has limited amount  of space. When /PBS/spool is filled up, any job<br># that tries to write to /PBS/spool will die.<br><br># -end of script-</span></font><br><br></font></span></div><span style="font-family:courier new,courier,monospace"><font face="arial,helvetica,sans-serif">Hopefully this is enough information for someone to find an error in how I did things. I also have the outputs of the make, make-test and make-install if anyone would like to see those. :)<br><br></font></span></div><span style="font-family:courier new,courier,monospace"><font face="arial,helvetica,sans-serif">Thanks for the help!<br><br></font></span></div><span style="font-family:courier new,courier,monospace"><font face="arial,helvetica,sans-serif">Cordially,<br><br></font></span></div><span style="font-family:courier new,courier,monospace"><font face="arial,helvetica,sans-serif">Joshua Wall<br></font></span></div>

