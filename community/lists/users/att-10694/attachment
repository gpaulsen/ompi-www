<div>Dear all,</div>
<div> </div>
<div>     The CPMD application which is compiled with OpenMPI-1.3 (Intel 10.1 compilers) on CentOS-4.5, fails only, when a specific node i.e. node-0-2 is involved. But runs well on other nodes.</div>
<div> </div>
<div>      Initially job failed after 5-10 mins (on node-0-2 + some other nodes). After googling error, I added options &quot;-mca btl_openib_ib_min_rnr_timer 25 -mca btl_openib_ib_timeout 20&quot; to mpirun command in the SGE script:</div>

<div> </div>
<div>$ cat cpmdrun.sh<br>#!/bin/bash<br>#$ -N cpmd-acw<br>#$ -S /bin/bash<br>#$ -cwd<br>#$ -e err.$JOB_ID.$JOB_NAME<br>#$ -o out.$JOB_ID.$JOB_NAME<br>#$ -pe ib 32<br>unset SGE_ROOT   </div>
<div>PP_LIBRARY=/home/user1/cpmdrun/wac/prod/PP<br>CPMD=/opt/apps/cpmd/3.11/ompi/SOURCE/cpmd311-ompi-mkl.x<br>MPIRUN=/opt/mpi/openmpi/1.3/intel/bin/mpirun<br>$MPIRUN -np $NSLOTS -hostfile $TMPDIR/machines -mca btl_openib_ib_min_rnr_timer 25 -mca btl_openib_ib_timeout 20 $CPMD <a href="http://wac_md26.in">wac_md26.in</a>  $PP_LIBRARY &gt; wac_md26.out<br>
</div>
<div>After adding these options, job executed for 24+ hours then failed with the same error as earlier. The error is:</div>
<div> </div>
<div>$ cat err.6186.cpmd-acw<br>--------------------------------------------------------------------------<br>The OpenFabrics stack has reported a network error event.  Open MPI<br>will try to continue, but your job may end up failing.</div>

<div>  Local host:        node-0-2.local<br>  MPI process PID:   11840<br>  Error number:      10 (IBV_EVENT_PORT_ERR)</div>
<div>This error may indicate connectivity problems within the fabric;<br>please contact your system administrator.<br>--------------------------------------------------------------------------<br>[node-0-2.local:11836] 7 more processes have sent help message help-mpi-btl-openib.txt / of error event<br>
[node-0-2.local:11836] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>[node-0-2.local:11836] 1 more process has sent help message help-mpi-btl-openib.txt / of error event<br>
[node-0-2.local:11836] 7 more processes have sent help message help-mpi-btl-openib.txt / of error event<br>[node-0-2.local:11836] 1 more process has sent help message help-mpi-btl-openib.txt / of error event<br>[node-0-2.local:11836] 7 more processes have sent help message help-mpi-btl-openib.txt / of error event<br>
[node-0-2.local:11836] 1 more process has sent help message help-mpi-btl-openib.txt / of error event<br>[node-0-2.local:11836] 7 more processes have sent help message help-mpi-btl-openib.txt / of error event<br>[node-0-2.local:11836] 1 more process has sent help message help-mpi-btl-openib.txt / of error event<br>
[node-0-2.local:11836] 7 more processes have sent help message help-mpi-btl-openib.txt / of error event<br>[node-0-2.local:11836] 1 more process has sent help message help-mpi-btl-openib.txt / of error event<br>[node-0-2.local:11836] 15 more processes have sent help message help-mpi-btl-openib.txt / of error event<br>
[node-0-2.local:11836] 16 more processes have sent help message help-mpi-btl-openib.txt / of error event<br>[node-0-2.local:11836] 16 more processes have sent help message help-mpi-btl-openib.txt / of error event<br>[[718,1],20][btl_openib_component.c:2902:handle_wc] from node-0-22.local to: node-0-2 --------------------------------------------------------------------------<br>
The InfiniBand retry count between two MPI processes has been<br>exceeded.  &quot;Retry count&quot; is defined in the InfiniBand spec 1.2<br>(section 12.7.38):</div>
<div>    The total number of times that the sender wishes the receiver to<br>    retry timeout, packet sequence, etc. errors before posting a<br>    completion error.</div>
<div>This error typically means that there is something awry within the<br>InfiniBand fabric itself.  You should note the hosts on which this<br>error has occurred; it has been observed that rebooting or removing a<br>particular host from the job can sometimes resolve this issue.</div>

<div>Two MCA parameters can be used to control Open MPI&#39;s behavior with<br>respect to the retry count:</div>
<div>* btl_openib_ib_retry_count - The number of times the sender will<br>  attempt to retry (defaulted to 7, the maximum value).<br>* btl_openib_ib_timeout - The local ACK timeout parameter (defaulted<br>  to 10).  The actual timeout value used is calculated as:</div>

<div>     4.096 microseconds * (2^btl_openib_ib_timeout)</div>
<div>  See the InfiniBand spec 1.2 (section 12.7.34) for more details.</div>
<div>Below is some information about the host that raised the error and the<br>peer to which it was connected:</div>
<div>  Local host:   node-0-22.local<br>  Local device: mthca0<br>  Peer host:    node-0-2</div>
<div>You may need to consult with your system administrator to get this<br>problem fixed.<br>--------------------------------------------------------------------------<br>error polling LP CQ with status RETRY EXCEEDED ERROR status number 12 for wr_id 66384128 opcode 128 qp_idx 3<br>
--------------------------------------------------------------------------<br>mpirun has exited due to process rank 20 with PID 10425 on<br>node ibc22 exiting without calling &quot;finalize&quot;. This may<br>have caused other processes in the application to be<br>
terminated by signals sent by mpirun (as reported here).<br>--------------------------------------------------------------------------<br>rm: cannot remove `/tmp/6186.1.iblong.q/rsh&#39;: No such file or directory</div>
<div> </div>
<div>The openibd service is running fine:</div>
<div> </div>
<div>$ service openibd status</div>
<div>  HCA driver loaded</div>
<div>Configured devices:<br>ib0</div>
<div>Currently active devices:<br>ib0</div>
<div>The following OFED modules are loaded:</div>
<div>  rdma_ucm<br>  ib_sdp<br>  rdma_cm<br>  ib_addr<br>  ib_ipoib<br>  mlx4_core<br>  mlx4_ib<br>  ib_mthca<br>  ib_uverbs<br>  ib_umad<br>  ib_ucm<br>  ib_sa<br>  ib_cm<br>  ib_mad<br>  ib_core<br></div>
<div>But still the job is failing after hours of running, that to for a particular node. What&#39;s the wrong with node-0-2? How can it be resolved?</div>
<div> </div>
<div>Thanks,</div>
<div>Sangamesh</div>

