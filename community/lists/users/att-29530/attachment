<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    <p>Java uses *many* threads, simply<br>
    </p>
    <p>ls /proc/&lt;pid&gt;/tasks<br>
    </p>
    and you will be amazed at how many threads are used.<br>
    Here is my guess,<br>
    <br>
    <br>
    from the point of view of a given MPI process :<br>
    <br>
    in case 1, the main thread and all the other threads do time
    sharing, so basically, when an other thread is working, the main
    thread is blocked.<br>
    <br>
    in case 2, some parallelism is possible if an other MPI task is
    sleeping : main thread is running, and an other thread is running on
    an other core<br>
    <br>
    in case 3, the main thread can move from on core to an other<br>
    =&gt; cache flush<br>
    =&gt; QPI access if used memory is no more local<br>
    so though there is more opportunity for parallelism, process
    migration can slow down everything<br>
    <br>
    <br>
    bottom line, event with one thread, case 1 and case 2 are quite
    different because Java uses so many threads per process, so i am not
    so surprised with the difference in performance.<br>
    <br>
    if you have any chance, i suggest you write a similar program in C.<br>
    since only a few threads are use per process, i guess case 1 and
    case 2 will become pretty close.<br>
    <br>
    i also suggest that for cases 2 and 3, you bind processes to a
    socket instead of no binding at all<br>
    <br>
    Cheers,<br>
    <br>
    Gilles<br>
    <br>
    <div class="moz-cite-prefix">On 6/23/2016 2:41 PM, Saliya Ekanayake
      wrote:<br>
    </div>
    <blockquote
cite="mid:CA+ssbKViuVJ8chTL3psCkhMsuZuGWSEBA-6znzyVqKg87C4yiA@mail.gmail.com"
      type="cite">
      <div dir="ltr">Thank you, Gilles for the quick response. The code
        comes from a clustering application, bu let me try to explain
        simply what the pattern is. It's a bit long than I expected.
        <div><br>
        </div>
        <div><br>
          <div><br>
          </div>
          <div>The program has the pattern BSP pattern with <i>compute()</i> followed
            by collective <i>allreduce()</i> And it does many
            iterations over these two.</div>
          <div><br>
          </div>
          <div>Each process is a Java process with just the main thread.
            However in Java the process and main thread have their own
            PIDs and act as two LWPs in Linux. </div>
          <div><br>
          </div>
          <div>Now, let's take two binding scenarios. For simplicity,
            I'll assume a node with 2 sockets each with 4-cores. The
            real one I ran has 2 sockets with 12 cores each.</div>
          <div><br>
          </div>
          <div>1. <b>--map-by ppr:8:node:PE=1 --bind-to core</b> results
            in something like below.</div>
          <div><br>
          </div>
          <div><img src="cid:part1.E958B18A.A0E37058@rist.or.jp"
              alt="Inline image 3" height="113" width="555"><br>
          </div>
          <div>where each process is bound to 1 core. The blue dots show
            the main thread in Java. It too is bound to the same core as
            its parent process by default.</div>
          <div><br>
          </div>
          <div>2. <b>--map-by ppr:8:node  --bind-to none </b> This is
            similar to 1, but now processes are not bound (or bound to
            all cores). However, from the program, we <b>explicitly
              bind its main thread to 1 core</b>. It gives something
            like below.</div>
          <div><br>
          </div>
          <div><img src="cid:part2.3A35AF8B.2F3C105E@rist.or.jp"
              alt="Inline image 4" height="119" width="557"><br>
          </div>
          <div>The results we got suggest approach 2 gives better
            communication performance than 1. The btl used is openib.
            Here's a graph showing the variation in timings. It shows
            for other cases that use more than 1 thread to do the
            computation as well. In all patterns communication is done
            through the main thread only.</div>
          <div><br>
          </div>
          <div>What is peculiar is the two points within the dotted
            circle. Intuitively they should overlap as it only has the
            main thread in each Java process and that main is bound to 1
            core. The difference is how the parent process is bound with
            MPI. The red line is for <b>Case 1</b> above and the blue
            is for <b>Case 2</b></div>
          <div><b><br>
            </b></div>
          <div>The green line is when both parent process and threads
            are unbound.</div>
          <div><br>
          </div>
          <div><br>
          </div>
          <div><img src="cid:part3.53992C42.FFAC7AE4@rist.or.jp"
              alt="Inline image 6" height="536" width="562"><br>
          </div>
          <div><br>
          </div>
          <div><br>
          </div>
          <div><i><br>
            </i></div>
          <div><i><br>
            </i></div>
          <div>
            <div><br>
            </div>
            <div><br>
            </div>
          </div>
        </div>
      </div>
      <div class="gmail_extra"><br>
        <div class="gmail_quote">On Thu, Jun 23, 2016 at 12:36 AM,
          Gilles Gouaillardet <span dir="ltr">&lt;<a
              moz-do-not-send="true" href="mailto:gilles@rist.or.jp"
              target="_blank"><a class="moz-txt-link-abbreviated" href="mailto:gilles@rist.or.jp">gilles@rist.or.jp</a></a>&gt;</span> wrote:<br>
          <blockquote class="gmail_quote" style="margin:0 0 0
            .8ex;border-left:1px #ccc solid;padding-left:1ex">
            <div bgcolor="#FFFFFF" text="#000000">
              <p>Can you please provide more details on your config, how
                test are performed and the results ?</p>
              <p><br>
              </p>
              <p>to be fair, you should only compare cases in which mpi
                tasks are bound to the same sockets.</p>
              <p>for example, if socket0 has core[0-7] and socket1 has
                core[8-15]</p>
              <p>it is fair to compare {task0,task1} bound on</p>
              <p>{0,8}, {[0-1],[8-9]}, {[0-7],[8-15]}</p>
              <p>but it is unfair to compare</p>
              <p>{0,1} and {0,8} or {[0-7],[8-15]}</p>
              <p>since {0,1} does not involve traffic on the QPI, but
                {0,8} does.<br>
              </p>
              depending on the btl you are using, it might involve or
              not an other "helper" thread.<br>
              if your task is bound on one core, and assuming there is
              no SMT, then the task and the helper do time sharing.<br>
              but if the task is bound on more than one core, then the
              task and the helper run in parallel.<br>
              <br>
              <br>
              Cheers,<br>
              <br>
              Gilles
              <div>
                <div class="h5"><br>
                  <div>On 6/23/2016 1:21 PM, Saliya Ekanayake wrote:<br>
                  </div>
                </div>
              </div>
              <blockquote type="cite">
                <div>
                  <div class="h5">
                    <div dir="ltr">Hi,
                      <div><br>
                      </div>
                      <div>I am trying to understand this peculiar
                        behavior where the communication time in OpenMPI
                        changes depending on the number of process
                        elements (cores) the process is bound to. </div>
                      <div><br>
                      </div>
                      <div>Is this expected?</div>
                      <div><br>
                      </div>
                      <div>Thank you,<br>
                        saliya</div>
                      <div>
                        <div><br>
                        </div>
                        -- <br>
                        <div data-smartmail="gmail_signature">
                          <div dir="ltr">
                            <div>
                              <div dir="ltr">
                                <div dir="ltr">
                                  <div dir="ltr"><span
                                      style="color:rgb(136,136,136)">Saliya
                                      Ekanayake</span></div>
                                  <div dir="ltr">Ph.D. Candidate |
                                    Research Assistant</div>
                                  <div dir="ltr">School of Informatics
                                    and Computing | Digital Science
                                    Center</div>
                                  <div dir="ltr">Indiana University,
                                    Bloomington<br>
                                    <br>
                                  </div>
                                </div>
                              </div>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                    <br>
                    <fieldset></fieldset>
                    <br>
                  </div>
                </div>
                <pre>_______________________________________________
users mailing list
<a moz-do-not-send="true" href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>
Subscription: <a moz-do-not-send="true" href="https://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a moz-do-not-send="true" href="http://www.open-mpi.org/community/lists/users/2016/06/29523.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/06/29523.php</a></pre>
              </blockquote>
              <br>
            </div>
            <br>
            _______________________________________________<br>
            users mailing list<br>
            <a moz-do-not-send="true" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
            Subscription: <a moz-do-not-send="true"
              href="https://www.open-mpi.org/mailman/listinfo.cgi/users"
              rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
            Link to this post: <a moz-do-not-send="true"
              href="http://www.open-mpi.org/community/lists/users/2016/06/29524.php"
              rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/06/29524.php</a><br>
          </blockquote>
        </div>
        <br>
        <br clear="all">
        <div><br>
        </div>
        -- <br>
        <div class="gmail_signature" data-smartmail="gmail_signature">
          <div dir="ltr">
            <div>
              <div dir="ltr">
                <div dir="ltr">
                  <div dir="ltr"><span style="color:rgb(136,136,136)">Saliya
                      Ekanayake</span></div>
                  <div dir="ltr">Ph.D. Candidate | Research Assistant</div>
                  <div dir="ltr">School of Informatics and Computing |
                    Digital Science Center</div>
                  <div dir="ltr">Indiana University, Bloomington<br>
                    <br>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="https://www.open-mpi.org/mailman/listinfo.cgi/users">https://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/06/29529.php">http://www.open-mpi.org/community/lists/users/2016/06/29529.php</a></pre>
    </blockquote>
    <br>
  </body>
</html>

