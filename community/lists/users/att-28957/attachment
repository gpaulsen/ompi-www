<div dir="ltr"><div><div><div><div><div><div><div>Dear developers<br><br></div>Thank you all for jumping in to help. Here is what I have found so far:<br><br></div>1. Running Netpipe (NPmpi) between the two nodes (in either order) was successful, but following this test, my original code still hung.<br></div>2. Following Gilles&#39;s advice, I then added an MPI_Barrier() at the end of the code, just before MPI_Finalize(), and, to my surprise, the code ran to completion!<br></div>3. Then, I took out the barrier, leaving the code the way it was before, and it still ran to completion!<br></div><div>4. I tried several variations of call sequence, and all of them ran successfully.<br></div><div><br></div>I can&#39;t explain why the runtime behavior seems to depend on the phase of the moon, but, although I cannot prove it, I have a gut feeling there is a bug somewhere in the development branch. I never run into this issue when running the release branch. (I sometimes work as MPI application developer, when I use the release branch, and sometime as MPI developer, when I use the master branch).<br><br></div>Thank you all, again.<br><br></div>Durga<br><div class="gmail_extra"><br clear="all"><div><div class="gmail_signature"><div dir="ltr"><div>1% of the executables have 99% of CPU privilege!<br></div>Userspace code! Unite!! Occupy the kernel!!!<br></div></div></div>
<br><div class="gmail_quote">On Mon, Apr 18, 2016 at 8:04 AM, George Bosilca <span dir="ltr">&lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank">bosilca@icl.utk.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><p dir="ltr">Durga, </p>
<p dir="ltr">Can you run a simple netpipe over TCP using any of the two interfaces you mentioned? </p>
<p dir="ltr">George </p>
<div class="gmail_quote"><div><div class="h5">On Apr 18, 2016 11:08 AM, &quot;Gilles Gouaillardet&quot; &lt;<a href="mailto:gilles.gouaillardet@gmail.com" target="_blank">gilles.gouaillardet@gmail.com</a>&gt; wrote:<br type="attribution"></div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div class="h5">An other test is to swap the hostnames.<br>If the single barrier test fails, this can hint to a firewall.<br><br>Cheers,<br><br>Gilles<br><br>Gilles Gouaillardet &lt;<a href="mailto:gilles@rist.or.jp" target="_blank">gilles@rist.or.jp</a>&gt; wrote:<br>
    sudo make uninstall<br>
    will not remove modules that are no more built<br>
    sudo rm -rf /usr/local/lib/openmpi<br>
    is safe thought<br>
    <br>
    i confirm i did not see any issue on a system with two networks<br>
    <br>
    Cheers,<br>
    <br>
    Gilles<br>
    <br>
    <div>On 4/18/2016 2:53 PM, dpchoudh . wrote:<br>
    </div>
    <blockquote type="cite">
      <div dir="ltr">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    <div>
                      <div>Hello Gilles<br>
                        <br>
                      </div>
                      I did a<br>
                    </div>
                    sudo make uninstall<br>
                  </div>
                  followed by a<br>
                </div>
                sudo make install<br>
              </div>
              on both nodes. But that did not make a difference. I will
              try your tarball build suggestion a bit later.<br>
              <br>
            </div>
            What I find a bit strange is that only I seem to be getting
            into this issue. What could I be doing wrong? Or am I
            discovering an obscure bug?<br>
            <br>
          </div>
          Thanks<br>
        </div>
        Durga<br>
      </div>
      <div class="gmail_extra"><br clear="all">
        <div>
          <div>
            <div dir="ltr">
              <div>1% of the executables have 99% of CPU privilege!<br>
              </div>
              Userspace code! Unite!! Occupy the kernel!!!<br>
            </div>
          </div>
        </div>
        <br>
        <div class="gmail_quote">On Mon, Apr 18, 2016 at 1:21 AM, Gilles
          Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles@rist.or.jp" target="_blank">gilles@rist.or.jp</a>&gt;</span>
          wrote:<br>
          <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
            <div bgcolor="#FFFFFF" text="#000000"> so you might want to<br>
              rm -rf /usr/local/lib/openmpi<br>
              and run<br>
              make install<br>
              again, just to make sure old stuff does not get in the way<br>
              <br>
              Cheers,<br>
              <br>
              Gilles
              <div>
                <div><br>
                  <br>
                  <div>On 4/18/2016 2:12 PM, dpchoudh . wrote:<br>
                  </div>
                </div>
              </div>
              <blockquote type="cite">
                <div>
                  <div>
                    <div dir="ltr">
                      <div>Hello Gilles<br>
                        <br>
                      </div>
                      Thank you very much for your feedback. You are
                      right that my original stack trace was on code
                      that was several weeks behind, but updating it
                      just now did not seem to make a difference: I am
                      copying the stack from the latest code below:<br>
                      <div><br>
                        On the master node:<br>
                        <br>
                        <span style="font-family:monospace,monospace">(gdb)
                          bt<br>
                          #0  0x00007fc0524cbb7d in poll () from
                          /lib64/libc.so.6<br>
                          #1  0x00007fc051e53116 in poll_dispatch
                          (base=0x1aabbe0, tv=0x7fff29fcb240) at
                          poll.c:165<br>
                          #2  0x00007fc051e4adb0 in
                          opal_libevent2022_event_base_loop
                          (base=0x1aabbe0, flags=2) at event.c:1630<br>
                          #3  0x00007fc051de9a00 in opal_progress () at
                          runtime/opal_progress.c:171<br>
                          #4  0x00007fc04ce46b0b in opal_condition_wait
                          (c=0x7fc052d3cde0 &lt;ompi_request_cond&gt;,<br>
                              m=0x7fc052d3cd60
                          &lt;ompi_request_lock&gt;) at
                          ../../../../opal/threads/condition.h:76<br>
                          #5  0x00007fc04ce46cec in
                          ompi_request_wait_completion (req=0x1b7b580)<br>
                              at ../../../../ompi/request/request.h:383<br>
                          #6  0x00007fc04ce48d4f in mca_pml_ob1_send
                          (buf=0x7fff29fcb480, count=4,<br>
                              datatype=0x601080 &lt;ompi_mpi_char&gt;,
                          dst=1, tag=1,
                          sendmode=MCA_PML_BASE_SEND_STANDARD,<br>
                              comm=0x601280 &lt;ompi_mpi_comm_world&gt;)
                          at pml_ob1_isend.c:259<br>
                          #7  0x00007fc052a62d73 in PMPI_Send
                          (buf=0x7fff29fcb480, count=4, type=0x601080
                          &lt;ompi_mpi_char&gt;, dest=1,<br>
                              tag=1, comm=0x601280
                          &lt;ompi_mpi_comm_world&gt;) at psend.c:78<br>
                          #8  0x0000000000400afa in main (argc=1,
                          argv=0x7fff29fcb5e8) at mpitest.c:19<br>
                          (gdb)</span><br>
                        <br>
                      </div>
                      <div>And on the non-master node<br>
                        <br>
                        (gdb) bt<br>
                        #0  0x00007fad2c32148d in nanosleep () from
                        /lib64/libc.so.6<br>
                        #1  0x00007fad2c352014 in usleep () from
                        /lib64/libc.so.6<br>
                        #2  0x00007fad296412de in
                        OPAL_PMIX_PMIX120_PMIx_Fence (procs=0x0,
                        nprocs=0, info=0x0, ninfo=0)<br>
                            at src/client/pmix_client_fence.c:100<br>
                        #3  0x00007fad2960e1a6 in pmix120_fence
                        (procs=0x0, collect_data=0) at
                        pmix120_client.c:258<br>
                        #4  0x00007fad2c89b2da in ompi_mpi_finalize ()
                        at runtime/ompi_mpi_finalize.c:242<br>
                        #5  0x00007fad2c8c5849 in PMPI_Finalize () at
                        pfinalize.c:47<br>
                        #6  0x0000000000400958 in main (argc=1,
                        argv=0x7fff163879c8) at mpitest.c:30<br>
                        (gdb)<br>
                        <br>
                      </div>
                      <div>And my configuration was done as follows:<br>
                        <br>
                         <span style="font-family:monospace,monospace">$
                          ./configure --enable-debug
                          --enable-debug-symbols</span><br>
                        <br>
                      </div>
                      <div>I double checked to ensure that there is not
                        an older installation of OpenMPI that is getting
                        mixed up with the master branch.<br>
                      </div>
                      <div><span style="font-family:monospace,monospace">sudo
                          yum list installed | grep -i mpi</span><br>
                      </div>
                      <div>shows nothing on both nodes, and <span style="font-family:monospace,monospace">pmap
                          -p &lt;pid&gt;</span> shows that all the
                        libraries are coming from <span style="font-family:monospace,monospace">/usr/local/lib</span>,
                        which seems to be correct. I am also quite sure
                        about the firewall issue (that there is none). I
                        will try out your suggestion on installing from
                        a tarball and see how it goes.<br>
                      </div>
                      <div><br>
                      </div>
                      <div>Thanks<br>
                      </div>
                      <div>Durga<br>
                      </div>
                    </div>
                    <div class="gmail_extra"><br clear="all">
                      <div>
                        <div>
                          <div dir="ltr">
                            <div>1% of the executables have 99% of CPU
                              privilege!<br>
                            </div>
                            Userspace code! Unite!! Occupy the kernel!!!<br>
                          </div>
                        </div>
                      </div>
                      <br>
                      <div class="gmail_quote">On Mon, Apr 18, 2016 at
                        12:47 AM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles@rist.or.jp" target="_blank"></a><a href="mailto:gilles@rist.or.jp" target="_blank">gilles@rist.or.jp</a>&gt;</span>
                        wrote:<br>
                        <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
                          <div bgcolor="#FFFFFF" text="#000000"> here is
                            your stack trace<br>
                            <br>
                            <span style="font-family:monospace,monospace"><span>#6 
                                0x00007f72a0d09cd5 in mca_pml_ob1_send
                                (buf=0x7fff81057db0, count=4,<br>
                                    datatype=0x601080
                                &lt;ompi_mpi_char&gt;, dst=1, tag=1,<br>
                                    sendmode=MCA_PML_BASE_SEND_STANDARD,
                                comm=0x601280
                                &lt;ompi_mpi_comm_world&gt;)<br>
                                <br>
                              </span> at line 251<br>
                            </span><br>
                            <br>
                            that would be line 259 in current master,
                            and this file was updated 21 days ago<br>
                            and that suggests your master is not quite
                            up to date.<br>
                            <br>
                            even if the message is sent eagerly, the ob1
                            pml does use an internal request it will
                            wait for.<br>
                            <br>
                            btw, did you configure with
                            --enable-mpi-thread-multiple ?<br>
                            did you configure with
                            --enable-mpirun-prefix-by-default ?<br>
                            did you configure with --disable-dlopen ?<br>
                            <br>
                            at first, i d recommend you download a
                            tarball from <a href="https://www.open-mpi.org/nightly/master" target="_blank">https://www.open-mpi.org/nightly/master</a>,<br>
                            configure &amp;&amp; make &amp;&amp; make
                            install<br>
                            using a new install dir, and check if the
                            issue is still here or not.<br>
                            <br>
                            there could be some side effects if some old
                            modules were not removed and/or if you are<br>
                            not using the modules you expect.<br>
                            /* when it hangs, you can pmap &lt;pid&gt;
                            and check the path of the openmpi libraries
                            are the one you expect */<br>
                            <br>
                            what if you do not send/recv but invoke
                            MPI_Barrier multiple times ?<br>
                            what if you send/recv a one byte message
                            instead ?<br>
                            did you double check there is no firewall
                            running on your nodes ?<br>
                            <br>
                            Cheers,<br>
                            <br>
                            Gilles
                            <div>
                              <div><br>
                                <br>
                                <br>
                                <br>
                                <br>
                                <br>
                                <div>On 4/18/2016 1:06 PM, dpchoudh .
                                  wrote:<br>
                                </div>
                              </div>
                            </div>
                            <blockquote type="cite">
                              <div>
                                <div>
                                  <div dir="ltr">
                                    <div>
                                      <div>
                                        <div>
                                          <div>Thank you for your
                                            suggestion, Ralph. But it
                                            did not make any difference.<br>
                                            <br>
                                          </div>
                                          Let me say that my code is
                                          about a week stale. I just did
                                          a git pull and am building it
                                          right now. The build takes
                                          quite a bit of time, so I
                                          avoid doing that unless there
                                          is a reason. But what I am
                                          trying out is the most basic
                                          functionality, so I&#39;d think a
                                          week or so of lag would not
                                          make a difference.<br>
                                          <br>
                                        </div>
                                        Does the stack trace suggest
                                        something to you? It seems that
                                        the send hangs; but a 4 byte
                                        send should be sent eagerly.<br>
                                        <br>
                                      </div>
                                      Best regards<br>
                                    </div>
                                    &#39;Durga<br>
                                  </div>
                                  <div class="gmail_extra"><br clear="all">
                                    <div>
                                      <div>
                                        <div dir="ltr">
                                          <div>1% of the executables
                                            have 99% of CPU privilege!<br>
                                          </div>
                                          Userspace code! Unite!! Occupy
                                          the kernel!!!<br>
                                        </div>
                                      </div>
                                    </div>
                                    <br>
                                    <div class="gmail_quote">On Sun, Apr
                                      17, 2016 at 11:55 PM, Ralph
                                      Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank"></a><a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span>
                                      wrote:<br>
                                      <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
                                        <div style="word-wrap:break-word">Try
                                          adding -mca oob_tcp_if_include
                                          eno1 to your cmd line and see
                                          if that makes a difference
                                          <div><br>
                                            <div>
                                              <blockquote type="cite">
                                                <div>
                                                  <div>
                                                    <div>On Apr 17,
                                                      2016, at 8:43 PM,
                                                      dpchoudh . &lt;<a href="mailto:dpchoudh@gmail.com" target="_blank"></a><a href="mailto:dpchoudh@gmail.com" target="_blank">dpchoudh@gmail.com</a>&gt;


                                                      wrote:</div>
                                                    <br>
                                                  </div>
                                                </div>
                                                <div>
                                                  <div>
                                                    <div>
                                                      <div dir="ltr">
                                                        <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>Hello
                                                          Gilles and all<br>
                                                          <br>
                                                          </div>
                                                          I am sorry to
                                                          be bugging the
                                                          developers,
                                                          but this issue
                                                          seems to be
                                                          nagging me,
                                                          and I am
                                                          surprised it
                                                          does not seem
                                                          to affect
                                                          anybody else.
                                                          But then
                                                          again, I am
                                                          using the
                                                          master branch,
                                                          and most users
                                                          are probably
                                                          using a
                                                          released
                                                          version.<br>
                                                          <br>
                                                          </div>
                                                          This time I am
                                                          using a
                                                          totally
                                                          different
                                                          cluster. This
                                                          has NO verbs
                                                          capable
                                                          interface;
                                                          just 2
                                                          Ethernet (1 of
                                                          which has no
                                                          IP address and
                                                          hence is
                                                          unusable) plus
                                                          1 proprietary
                                                          interface that
                                                          currently
                                                          supports only
                                                          IP traffic.
                                                          The two IP
                                                          interfaces
                                                          (Ethernet and
                                                          proprietary)
                                                          are on
                                                          different IP
                                                          subnets.<br>
                                                          <br>
                                                          </div>
                                                          My test
                                                          program is as
                                                          follows:<br>
                                                          <br>
                                                          <span style="font-family:monospace,monospace">#include
&lt;stdio.h&gt;<br>
                                                          #include
                                                          &lt;string.h&gt;<br>
                                                          #include
                                                          &quot;mpi.h&quot;<br>
                                                          int main(int
                                                          argc, char
                                                          *argv[])<br>
                                                          {<br>
                                                          char
                                                          host[128];<br>
                                                          int n;<br>
                                                          MPI_Init(&amp;argc,


                                                          &amp;argv);<br>
                                                          MPI_Get_processor_name(host,


                                                          &amp;n);<br>
                                                          printf(&quot;Hello
                                                          from %s\n&quot;,
                                                          host);<br>
                                                          MPI_Comm_size(MPI_COMM_WORLD,


                                                          &amp;n);<br>
                                                          printf(&quot;The
                                                          world has %d
                                                          nodes\n&quot;, n);<br>
                                                          MPI_Comm_rank(MPI_COMM_WORLD,


                                                          &amp;n);<br>
                                                          printf(&quot;My
                                                          rank is
                                                          %d\n&quot;,n);<br>
                                                          //#if 0<br>
                                                          if (n == 0)<br>
                                                          {<br>
                                                          strcpy(host,
                                                          &quot;ha!&quot;);<br>
                                                          MPI_Send(host,
                                                          strlen(host) +
                                                          1, MPI_CHAR,
                                                          1, 1,
                                                          MPI_COMM_WORLD);<br>
                                                          printf(&quot;sent
                                                          %s\n&quot;, host);<br>
                                                          }<br>
                                                          else<br>
                                                          {<br>
                                                          //int len =
                                                          strlen(host) +
                                                          1;<br>
                                                          bzero(host,
                                                          128);<br>
                                                          MPI_Recv(host, 

                                                          4, MPI_CHAR,
                                                          0, 1,
                                                          MPI_COMM_WORLD,
MPI_STATUS_IGNORE);<br>
                                                          printf(&quot;Received

                                                          %s from rank
                                                          0\n&quot;, host);<br>
                                                          }<br>
                                                          //#endif<br>
MPI_Finalize();<br>
                                                          return 0;<br>
                                                          }</span><br>
                                                          <br>
                                                          </div>
                                                          This program,
                                                          when run
                                                          between two
                                                          nodes, hangs.
                                                          The command
                                                          was:<br>
                                                          <span style="font-family:monospace,monospace">[durga@b-1


                                                          ~]$ mpirun -np
                                                          2 -hostfile
                                                          ~/hostfile
                                                          -mca btl
                                                          self,tcp -mca
                                                          pml ob1 -mca
                                                          btl_tcp_if_include
                                                          eno1 ./mpitest<br>
                                                          </span><br>
                                                          And the hang
                                                          is with the
                                                          following
                                                          output: (eno1
                                                          is one of the
                                                          gigEth
                                                          interfaces,
                                                          that takes OOB
                                                          traffic as
                                                          well)<br>
                                                          <br>
                                                          <span style="font-family:monospace,monospace">Hello


                                                          from b-1<br>
                                                          The world has
                                                          2 nodes<br>
                                                          My rank is 0<br>
                                                          Hello from b-2<br>
                                                          The world has
                                                          2 nodes<br>
                                                          My rank is 1</span><br>
                                                          <br>
                                                          </div>
                                                          Note that if I
                                                          uncomment the
                                                          #if 0 - #endif
                                                          (i.e. comment
                                                          out the
                                                          MPI_Send()/MPI_Recv()
                                                          part, the
                                                          program runs
                                                          to completion.
                                                          Also note that
                                                          the printfs
                                                          following
                                                          MPI_Send()/MPI_Recv()
                                                          do not show up
                                                          on console.<br>
                                                          <br>
                                                          </div>
                                                          Upon attaching
                                                          gdb, the stack
                                                          trace from the
                                                          master node is
                                                          as follows:<br>
                                                          <br>
                                                          <span style="font-family:monospace,monospace">Missing


                                                          separate
                                                          debuginfos,
                                                          use:
                                                          debuginfo-install
glibc-2.17-78.el7.x86_64
libpciaccess-0.13.4-2.el7.x86_64<br>
                                                          (gdb) bt<br>
                                                          #0 
                                                          0x00007f72a533eb7d
                                                          in poll ()
                                                          from
                                                          /lib64/libc.so.6<br>
                                                          #1 
                                                          0x00007f72a4cb7146
                                                          in
                                                          poll_dispatch
(base=0xee33d0,
tv=0x7fff81057b70)<br>
                                                              at
                                                          poll.c:165<br>
                                                          #2 
                                                          0x00007f72a4caede0
                                                          in
                                                          opal_libevent2022_event_base_loop
(base=0xee33d0,<br>
                                                              flags=2)
                                                          at
                                                          event.c:1630<br>
                                                          #3 
                                                          0x00007f72a4c4e692
                                                          in
                                                          opal_progress
                                                          () at
                                                          runtime/opal_progress.c:171<br>
                                                          #4 
                                                          0x00007f72a0d07ac1
                                                          in
                                                          opal_condition_wait
                                                          (<br>
                                                             
                                                          c=0x7f72a5bb1e00
                                                          &lt;ompi_request_cond&gt;,


m=0x7f72a5bb1d80
&lt;ompi_request_lock&gt;)<br>
                                                              at
                                                          ../../../../opal/threads/condition.h:76<br>
                                                          #5 
                                                          0x00007f72a0d07ca2
                                                          in
                                                          ompi_request_wait_completion
(req=0x113eb80)<br>
                                                              at
                                                          ../../../../ompi/request/request.h:383<br>
                                                          #6 
                                                          0x00007f72a0d09cd5
                                                          in
                                                          mca_pml_ob1_send
                                                          (buf=0x7fff81057db0,

                                                          count=4,<br>
                                                             
                                                          datatype=0x601080
                                                          &lt;ompi_mpi_char&gt;,

                                                          dst=1, tag=1,<br>
                                                             
                                                          sendmode=MCA_PML_BASE_SEND_STANDARD,
                                                          comm=0x601280
&lt;ompi_mpi_comm_world&gt;)<br>
                                                              at
                                                          pml_ob1_isend.c:251<br>
                                                          #7 
                                                          0x00007f72a58d6be3
                                                          in PMPI_Send
                                                          (buf=0x7fff81057db0,
                                                          count=4,<br>
                                                             
                                                          type=0x601080
                                                          &lt;ompi_mpi_char&gt;,

                                                          dest=1, tag=1,<br>
                                                             
                                                          comm=0x601280
                                                          &lt;ompi_mpi_comm_world&gt;)

                                                          at psend.c:78<br>
                                                          #8 
                                                          0x0000000000400afa
                                                          in main
                                                          (argc=1,
                                                          argv=0x7fff81057f18)
                                                          at
                                                          mpitest.c:19<br>
                                                          (gdb)<br>
                                                          </span><br>
                                                          </div>
                                                          And the
                                                          backtrace on
                                                          the non-master
                                                          node is:<br>
                                                          <br>
                                                          <span style="font-family:monospace,monospace">(gdb)


                                                          bt<br>
                                                          #0 
                                                          0x00007ff3b377e48d
                                                          in nanosleep
                                                          () from
                                                          /lib64/libc.so.6<br>
                                                          #1 
                                                          0x00007ff3b37af014
                                                          in usleep ()
                                                          from
                                                          /lib64/libc.so.6<br>
                                                          #2 
                                                          0x00007ff3b0c922de
                                                          in
                                                          OPAL_PMIX_PMIX120_PMIx_Fence
                                                          (procs=0x0,
                                                          nprocs=0,<br>
                                                              info=0x0,
                                                          ninfo=0) at
                                                          src/client/pmix_client_fence.c:100<br>
                                                          #3 
                                                          0x00007ff3b0c5f1a6
                                                          in
                                                          pmix120_fence
                                                          (procs=0x0,
                                                          collect_data=0)<br>
                                                              at
                                                          pmix120_client.c:258<br>
                                                          #4 
                                                          0x00007ff3b3cf8f4b
                                                          in
                                                          ompi_mpi_finalize
                                                          ()<br>
                                                              at
                                                          runtime/ompi_mpi_finalize.c:242<br>
                                                          #5 
                                                          0x00007ff3b3d23295
                                                          in
                                                          PMPI_Finalize
                                                          () at
                                                          pfinalize.c:47<br>
                                                          #6 
                                                          0x0000000000400958
                                                          in main
                                                          (argc=1,
                                                          argv=0x7fff785e8788)
                                                          at
                                                          mpitest.c:30<br>
                                                          (gdb)</span><br>
                                                          <br>
                                                          </div>
                                                          The hostfile
                                                          is as follows:<br>
                                                          <br>
                                                          <span style="font-family:monospace,monospace">[durga@b-1


                                                          ~]$ cat
                                                          hostfile<br>
                                                          10.4.70.10
                                                          slots=1<br>
                                                          10.4.70.11
                                                          slots=1<br>
                                                          #10.4.70.12
                                                          slots=1</span><br>
                                                          <br>
                                                          </div>
                                                          And the
                                                          ifconfig
                                                          output from
                                                          the master
                                                          node is as
                                                          follows (the
                                                          other node is
                                                          similar; all
                                                          the IP
                                                          interfaces are
                                                          in their
                                                          respective
                                                          subnets) :<br>
                                                          <span style="font-family:monospace,monospace"><br>
                                                          [durga@b-1 ~]$
                                                          ifconfig<br>
                                                          eno1:
                                                          flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; 
                                                          mtu 1500<br>
                                                                  inet
                                                          10.4.70.10 
                                                          netmask
                                                          255.255.255.0 
                                                          broadcast
                                                          10.4.70.255<br>
                                                                  inet6
                                                          fe80::21e:c9ff:fefe:13df 

                                                          prefixlen 64 
                                                          scopeid
                                                          0x20&lt;link&gt;<br>
                                                                  ether
                                                          00:1e:c9:fe:13:df 

                                                          txqueuelen
                                                          1000 
                                                          (Ethernet)<br>
                                                                  RX
                                                          packets 48215 
                                                          bytes 27842846
                                                          (26.5 MiB)<br>
                                                                  RX
                                                          errors 0 
                                                          dropped 0 
                                                          overruns 0 
                                                          frame 0<br>
                                                                  TX
                                                          packets 52746 
                                                          bytes 7817568
                                                          (7.4 MiB)<br>
                                                                  TX
                                                          errors 0 
                                                          dropped 0
                                                          overruns 0 
                                                          carrier 0 
                                                          collisions 0<br>
                                                                  device
                                                          interrupt 16<br>
                                                          <br>
                                                          eno2:
                                                          flags=4099&lt;UP,BROADCAST,MULTICAST&gt; 
                                                          mtu 1500<br>
                                                                  ether
                                                          00:1e:c9:fe:13:e0 

                                                          txqueuelen
                                                          1000 
                                                          (Ethernet)<br>
                                                                  RX
                                                          packets 0 
                                                          bytes 0 (0.0
                                                          B)<br>
                                                                  RX
                                                          errors 0 
                                                          dropped 0 
                                                          overruns 0 
                                                          frame 0<br>
                                                                  TX
                                                          packets 0 
                                                          bytes 0 (0.0
                                                          B)<br>
                                                                  TX
                                                          errors 0 
                                                          dropped 0
                                                          overruns 0 
                                                          carrier 0 
                                                          collisions 0<br>
                                                                  device
                                                          interrupt 17<br>
                                                          <br>
                                                          lf0:
                                                          flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; 
                                                          mtu 2016<br>
                                                                  inet
                                                          192.168.1.2 
                                                          netmask
                                                          255.255.255.0 
                                                          broadcast
                                                          192.168.1.255<br>
                                                                  inet6
                                                          fe80::3002:ff:fe33:3333 

                                                          prefixlen 64 
                                                          scopeid
                                                          0x20&lt;link&gt;<br>
                                                                  ether
                                                          32:02:00:33:33:33 

                                                          txqueuelen
                                                          1000 
                                                          (Ethernet)<br>
                                                                  RX
                                                          packets 10 
                                                          bytes 512
                                                          (512.0 B)<br>
                                                                  RX
                                                          errors 0 
                                                          dropped 0 
                                                          overruns 0 
                                                          frame 0<br>
                                                                  TX
                                                          packets 22 
                                                          bytes 1536
                                                          (1.5 KiB)<br>
                                                                  TX
                                                          errors 0 
                                                          dropped 0
                                                          overruns 0 
                                                          carrier 0 
                                                          collisions 0<br>
                                                          <br>
                                                          lo:
                                                          flags=73&lt;UP,LOOPBACK,RUNNING&gt; 
                                                          mtu 65536<br>
                                                                  inet
                                                          127.0.0.1 
                                                          netmask
                                                          255.0.0.0<br>
                                                                  inet6
                                                          ::1  prefixlen
                                                          128  scopeid
                                                          0x10&lt;host&gt;<br>
                                                                  loop 
                                                          txqueuelen 0 
                                                          (Local
                                                          Loopback)<br>
                                                                  RX
                                                          packets 26 
                                                          bytes 1378
                                                          (1.3 KiB)<br>
                                                                  RX
                                                          errors 0 
                                                          dropped 0 
                                                          overruns 0 
                                                          frame 0<br>
                                                                  TX
                                                          packets 26 
                                                          bytes 1378
                                                          (1.3 KiB)<br>
                                                                  TX
                                                          errors 0 
                                                          dropped 0
                                                          overruns 0 
                                                          carrier 0 
                                                          collisions 0<br>
                                                          </span><br>
                                                          </div>
                                                          Please help me
                                                          with this. I
                                                          am stuck with
                                                          the TCP
                                                          transport,
                                                          which is the
                                                          most basic of
                                                          all
                                                          transports.<br>
                                                          <br>
                                                          </div>
                                                          Thanks in
                                                          advance<br>
                                                        </div>
                                                        Durga<br>
                                                        <div>
                                                          <div>
                                                          <div><br>
                                                          </div>
                                                          </div>
                                                        </div>
                                                      </div>
                                                      <div class="gmail_extra"><br clear="all">
                                                        <div>
                                                          <div>
                                                          <div dir="ltr">
                                                          <div>1% of the
                                                          executables
                                                          have 99% of
                                                          CPU privilege!<br>
                                                          </div>
                                                          Userspace
                                                          code! Unite!!
                                                          Occupy the
                                                          kernel!!!<br>
                                                          </div>
                                                          </div>
                                                        </div>
                                                        <br>
                                                        <div class="gmail_quote">On

                                                          Tue, Apr 12,
                                                          2016 at 9:32
                                                          PM, Gilles
                                                          Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles@rist.or.jp" target="_blank"></a><a href="mailto:gilles@rist.or.jp" target="_blank">gilles@rist.or.jp</a>&gt;</span>
                                                          wrote:<br>
                                                          <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
                                                          <div bgcolor="#FFFFFF" text="#000000">
                                                          This is quite
                                                          unlikely, and
                                                          fwiw, your
                                                          test program
                                                          works for me.<br>
                                                          <br>
                                                          i suggest you
                                                          check your 3
                                                          TCP networks
                                                          are usable,
                                                          for example<br>
                                                          <br>
                                                          $ mpirun -np 2
                                                          -hostfile
                                                          ~/hostfile
                                                          -mca btl
                                                          self,tcp -mca
                                                          pml ob1 --mca
                                                          btl_tcp_if_include

                                                          xxx ./mpitest<br>
                                                          <br>
                                                          in which xxx
                                                          is a [list of]
                                                          interface name
                                                          :<br>
                                                          eth0<br>
                                                          eth1<br>
                                                          ib0<br>
                                                          eth0,eth1<br>
                                                          eth0,ib0<br>
                                                          ...<br>
                                                          eth0,eth1,ib0<br>
                                                          <br>
                                                          and see where
                                                          problem start
                                                          occuring.<br>
                                                          <br>
                                                          btw, are your
                                                          3 interfaces
                                                          in 3 different
                                                          subnet ? is
                                                          routing
                                                          required
                                                          between two
                                                          interfaces of
                                                          the same type
                                                          ?<br>
                                                          <br>
                                                          Cheers,<br>
                                                          <br>
                                                          Gilles
                                                          <div>
                                                          <div><br>
                                                          <div>On
                                                          4/13/2016 7:15
                                                          AM, dpchoudh .
                                                          wrote:<br>
                                                          </div>
                                                          </div>
                                                          </div>
                                                          <blockquote type="cite">
                                                          <div>
                                                          <div>
                                                          <div dir="ltr">
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>Hi all<br>
                                                          <br>
                                                          </div>
                                                          I have
                                                          reported this
                                                          issue before,
                                                          but then had
                                                          brushed it off
                                                          as something
                                                          that was
                                                          caused by my
                                                          modifications
                                                          to the source
                                                          tree. It looks
                                                          like that is
                                                          not the case.<br>
                                                          <br>
                                                          </div>
                                                          Just now, I
                                                          did the
                                                          following:<br>
                                                          <br>
                                                          </div>
                                                          1. Cloned a
                                                          fresh copy
                                                          from master.<br>
                                                          </div>
                                                          2. Configured
                                                          with the
                                                          following
                                                          flags, built
                                                          and installed
                                                          it in my
                                                          two-node
                                                          &quot;cluster&quot;.<br>
                                                          </div>
                                                          --enable-debug
--enable-debug-symbols
--disable-dlopen<br>
                                                          </div>
                                                          3. Compiled
                                                          the following
                                                          program,
                                                          mpitest.c with
                                                          these flags:
                                                          -g3 -Wall
                                                          -Wextra<br>
                                                          </div>
                                                          4. Ran it like
                                                          this:<br>
                                                          [durga@smallMPI

                                                          ~]$ mpirun -np
                                                          2 -hostfile
                                                          ~/hostfile
                                                          -mca btl
                                                          self,tcp -mca
                                                          pml ob1
                                                          ./mpitest<br>
                                                          <br>
                                                          </div>
                                                          With this, the
                                                          code hangs at
                                                          MPI_Barrier()
                                                          on both nodes,
                                                          after
                                                          generating the
                                                          following
                                                          output:<br>
                                                          <br>
                                                          Hello world
                                                          from processor
                                                          smallMPI, rank
                                                          0 out of 2
                                                          processors<br>
                                                          Hello world
                                                          from processor
                                                          bigMPI, rank 1
                                                          out of 2
                                                          processors<br>
                                                          smallMPI sent
                                                          haha!<br>
                                                          bigMPI
                                                          received haha!<br>
                                                          </div>
                                                          &lt;Hangs
                                                          until killed
                                                          by ^C&gt;<br>
                                                          </div>
                                                          Attaching to
                                                          the hung
                                                          process at one
                                                          node gives the
                                                          following
                                                          backtrace:<br>
                                                          <br>
                                                          (gdb) bt<br>
                                                          #0 
                                                          0x00007f55b0f41c3d
                                                          in poll ()
                                                          from
                                                          /lib64/libc.so.6<br>
                                                          #1 
                                                          0x00007f55b03ccde6
                                                          in
                                                          poll_dispatch
                                                          (base=0x70e7b0,


                                                          tv=0x7ffd1bb551c0)

                                                          at poll.c:165<br>
                                                          #2 
                                                          0x00007f55b03c4a90
                                                          in
                                                          opal_libevent2022_event_base_loop
                                                          (base=0x70e7b0,

                                                          flags=2) at
                                                          event.c:1630<br>
                                                          #3 
                                                          0x00007f55b02f0144
                                                          in
                                                          opal_progress
                                                          () at
                                                          runtime/opal_progress.c:171<br>
                                                          #4 
                                                          0x00007f55b14b4d8b
                                                          in
                                                          opal_condition_wait
                                                          (c=0x7f55b19fec40


                                                          &lt;ompi_request_cond&gt;,


                                                          m=0x7f55b19febc0


                                                          &lt;ompi_request_lock&gt;)


                                                          at
                                                          ../opal/threads/condition.h:76<br>
                                                          #5 
                                                          0x00007f55b14b531b
                                                          in
                                                          ompi_request_default_wait_all
                                                          (count=2,
                                                          requests=0x7ffd1bb55370,
                                                          statuses=0x7ffd1bb55340)

                                                          at
                                                          request/req_wait.c:287<br>
                                                          #6 
                                                          0x00007f55b157a225
                                                          in
                                                          ompi_coll_base_sendrecv_zero
                                                          (dest=1,
                                                          stag=-16,
                                                          source=1,
                                                          rtag=-16,
                                                          comm=0x601280
&lt;ompi_mpi_comm_world&gt;)<br>
                                                              at
                                                          base/coll_base_barrier.c:63<br>
                                                          #7 
                                                          0x00007f55b157a92a
                                                          in
                                                          ompi_coll_base_barrier_intra_two_procs
                                                          (comm=0x601280
                                                          &lt;ompi_mpi_comm_world&gt;,


                                                          module=0x7c2630)

                                                          at
                                                          base/coll_base_barrier.c:308<br>
                                                          #8 
                                                          0x00007f55b15aafec
                                                          in
                                                          ompi_coll_tuned_barrier_intra_dec_fixed
                                                          (comm=0x601280
                                                          &lt;ompi_mpi_comm_world&gt;,


                                                          module=0x7c2630)

                                                          at
                                                          coll_tuned_decision_fixed.c:196<br>
                                                          #9 
                                                          0x00007f55b14d36fd
                                                          in
                                                          PMPI_Barrier
                                                          (comm=0x601280
                                                          &lt;ompi_mpi_comm_world&gt;)


                                                          at
                                                          pbarrier.c:63<br>
                                                          #10
                                                          0x0000000000400b0b
                                                          in main
                                                          (argc=1,
                                                          argv=0x7ffd1bb55658)
                                                          at
                                                          mpitest.c:26<br>
                                                          (gdb) <br>
                                                          <br>
                                                          </div>
                                                          Thinking that
                                                          this might be
                                                          a bug in tuned
                                                          collectives,
                                                          since that is
                                                          what the stack
                                                          shows, I ran
                                                          the program
                                                          like this
                                                          (basically
                                                          adding the
                                                          ^tuned part)<br>
                                                          <br>
                                                          [durga@smallMPI

                                                          ~]$ mpirun -np
                                                          2 -hostfile
                                                          ~/hostfile
                                                          -mca btl
                                                          self,tcp -mca
                                                          pml ob1 -mca
                                                          coll ^tuned
                                                          ./mpitest<br>
                                                          <br>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>It still
                                                          hangs, but now
                                                          with a
                                                          different
                                                          stack trace:<br>
                                                          (gdb) bt<br>
                                                          #0 
                                                          0x00007f910d38ac3d
                                                          in poll ()
                                                          from
                                                          /lib64/libc.so.6<br>
                                                          #1 
                                                          0x00007f910c815de6
                                                          in
                                                          poll_dispatch
                                                          (base=0x1a317b0,


                                                          tv=0x7fff43ee3610)

                                                          at poll.c:165<br>
                                                          #2 
                                                          0x00007f910c80da90
                                                          in
                                                          opal_libevent2022_event_base_loop
                                                          (base=0x1a317b0,


                                                          flags=2) at
                                                          event.c:1630<br>
                                                          #3 
                                                          0x00007f910c739144
                                                          in
                                                          opal_progress
                                                          () at
runtime/opal_progress.c:171<br>
                                                          #4 
                                                          0x00007f910db130f7
                                                          in
                                                          opal_condition_wait
                                                          (c=0x7f910de47c40


                                                          &lt;ompi_request_cond&gt;,


m=0x7f910de47bc0
&lt;ompi_request_lock&gt;)<br>
                                                              at
                                                          ../../../../opal/threads/condition.h:76<br>
                                                          #5 
                                                          0x00007f910db132d8
                                                          in
                                                          ompi_request_wait_completion
                                                          (req=0x1b07680)

                                                          at
                                                          ../../../../ompi/request/request.h:383<br>
                                                          #6 
                                                          0x00007f910db1533b
                                                          in
                                                          mca_pml_ob1_send
                                                          (buf=0x0,
                                                          count=0,
                                                          datatype=0x7f910de1e340
                                                          &lt;ompi_mpi_byte&gt;,


                                                          dst=1,
                                                          tag=-16,
                                                          sendmode=MCA_PML_BASE_SEND_STANDARD,
                                                          <br>
                                                             
                                                          comm=0x601280
                                                          &lt;ompi_mpi_comm_world&gt;)


                                                          at
                                                          pml_ob1_isend.c:259<br>
                                                          #7 
                                                          0x00007f910d9c3b38
                                                          in
                                                          ompi_coll_base_barrier_intra_basic_linear
                                                          (comm=0x601280
                                                          &lt;ompi_mpi_comm_world&gt;,


                                                          module=0x1b092c0)

                                                          at
                                                          base/coll_base_barrier.c:368<br>
                                                          #8 
                                                          0x00007f910d91c6fd
                                                          in
                                                          PMPI_Barrier
                                                          (comm=0x601280
                                                          &lt;ompi_mpi_comm_world&gt;)


                                                          at
                                                          pbarrier.c:63<br>
                                                          #9 
                                                          0x0000000000400b0b
                                                          in main
                                                          (argc=1,
                                                          argv=0x7fff43ee3a58)
                                                          at
                                                          mpitest.c:26<br>
                                                          (gdb) <br>
                                                          <br>
                                                          </div>
                                                          <div>
                                                          <div>The
                                                          mpitest.c
                                                          program is as
                                                          follows:<br>
                                                          #include
                                                          &lt;mpi.h&gt;<br>
                                                          #include
                                                          &lt;stdio.h&gt;<br>
                                                          #include
                                                          &lt;string.h&gt;<br>
                                                          <br>
                                                          int main(int
                                                          argc, char**
                                                          argv)<br>
                                                          {<br>
                                                              int
                                                          world_size,
                                                          world_rank,
                                                          name_len;<br>
                                                              char
                                                          hostname[MPI_MAX_PROCESSOR_NAME],
                                                          buf[8];<br>
                                                          <br>
                                                             
                                                          MPI_Init(&amp;argc,
                                                          &amp;argv);<br>
                                                             
                                                          MPI_Comm_size(MPI_COMM_WORLD,
&amp;world_size);<br>
                                                             
                                                          MPI_Comm_rank(MPI_COMM_WORLD,
&amp;world_rank);<br>
                                                             
                                                          MPI_Get_processor_name(hostname,
&amp;name_len);<br>
                                                             
                                                          printf(&quot;Hello
                                                          world from
                                                          processor %s,
                                                          rank %d out of
                                                          %d
                                                          processors\n&quot;,
                                                          hostname,
                                                          world_rank,
                                                          world_size);<br>
                                                              if
                                                          (world_rank ==
                                                          1)<br>
                                                              {<br>
                                                             
                                                          MPI_Recv(buf,
                                                          6, MPI_CHAR,
                                                          0, 99,
                                                          MPI_COMM_WORLD,
MPI_STATUS_IGNORE);<br>
                                                              printf(&quot;%s
                                                          received
                                                          %s\n&quot;,
                                                          hostname,
                                                          buf);<br>
                                                              }<br>
                                                              else<br>
                                                              {<br>
                                                             
                                                          strcpy(buf,
                                                          &quot;haha!&quot;);<br>
                                                             
                                                          MPI_Send(buf,
                                                          6, MPI_CHAR,
                                                          1, 99,
                                                          MPI_COMM_WORLD);<br>
                                                              printf(&quot;%s
                                                          sent %s\n&quot;,
                                                          hostname,
                                                          buf);<br>
                                                              }<br>
                                                             
                                                          MPI_Barrier(MPI_COMM_WORLD);<br>
                                                             
                                                          MPI_Finalize();<br>
                                                              return 0;<br>
                                                          }<br>
                                                          <br>
                                                          </div>
                                                          <div>The
                                                          hostfile is as
                                                          follows:<br>
                                                          10.10.10.10
                                                          slots=1<br>
                                                          10.10.10.11
                                                          slots=1<br>
                                                          <br>
                                                          </div>
                                                          <div>The two
                                                          nodes are
                                                          connected by
                                                          three physical
                                                          and 3 logical
                                                          networks:<br>
                                                          </div>
                                                          <div>Physical:
                                                          Gigabit
                                                          Ethernet, 10G
                                                          iWARP, 20G
                                                          Infiniband<br>
                                                          </div>
                                                          <div>Logical:
                                                          IP (all 3),
                                                          PSM (Qlogic
                                                          Infiniband),
                                                          Verbs (iWARP
                                                          and
                                                          Infiniband)<br>
                                                          <br>
                                                          </div>
                                                          <div>Please
                                                          note again
                                                          that this is a
                                                          fresh, brand
                                                          new clone.<br>
                                                          <br>
                                                          </div>
                                                          <div>Is this a
                                                          bug (perhaps a
                                                          side effect of
                                                          --disable-dlopen)

                                                          or something I
                                                          am doing
                                                          wrong?<br>
                                                          <br>
                                                          </div>
                                                          <div>Thanks<br>
                                                          </div>
                                                          <div>Durga<br>
                                                          </div>
                                                          <div><br clear="all">
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div dir="ltr">
                                                          <div>We learn
                                                          from history
                                                          that we never
                                                          learn from
                                                          history.<br>
                                                          </div>
                                                          </div>
                                                          </div>
                                                          </div>
                                                          </div>
                                                          </div>
                                                          </div>
                                                          </div>
                                                          </div>
                                                          </div>
                                                          </div>
                                                          </div>
                                                          </div>
                                                          </div>
                                                          </div>
                                                          </div>
                                                          </div>
                                                          </div>
                                                          <br>
                                                          <fieldset></fieldset>
                                                          <br>
                                                          </div>
                                                          </div>
                                                          <pre>_______________________________________________
users mailing list
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/04/28930.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/28930.php</a></pre>
                                                          </blockquote>
                                                          <br>
                                                          </div>
                                                          <br>
_______________________________________________<br>
                                                          users mailing
                                                          list<br>
                                                          <a href="mailto:users@open-mpi.org" target="_blank"></a><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
                                                          Subscription:
                                                          <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank"></a><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                                                          Link to this
                                                          post: <a href="http://www.open-mpi.org/community/lists/users/2016/04/28932.php" target="_blank"></a></blockquote></div></div></div></div></div></blockquote></div></div></div></blockquote></div></div></div></div></blockquote></div></blockquote></div></div></div></div></blockquote></div></blockquote></div></div></blockquote><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/04/28951.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/28951.php</a><br>...</blockquote></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/04/28954.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/04/28954.php</a><br></blockquote></div><br></div></div>

