<div dir="ltr"><div>According to <a href="https://svn.open-mpi.org/trac/ompi/milestone/Open%20MPI%201.3">https://svn.open-mpi.org/trac/ompi/milestone/Open%20MPI%201.3</a>&nbsp;very soon, </div>
<div>but you can download trunk version <a href="http://www.open-mpi.org/svn/">http://www.open-mpi.org/svn/</a>&nbsp; and check if it works for you.</div>
<div>&nbsp;</div>
<div>how can you check mapping CPUs&nbsp;by OS , my cat /proc/cpuinfo shows very little info</div>
<div># cat /proc/cpuinfo<br>processor&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 0<br>cpu&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : Cell Broadband Engine, altivec supported<br>clock&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 3200.000000MHz<br>revision&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 48.0 (pvr 0070 3000)</div>
<div>processor&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 1<br>cpu&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : Cell Broadband Engine, altivec supported<br>clock&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 3200.000000MHz<br>revision&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 48.0 (pvr 0070 3000)</div>
<div>processor&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 2<br>cpu&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : Cell Broadband Engine, altivec supported<br>clock&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 3200.000000MHz<br>revision&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 48.0 (pvr 0070 3000)</div>
<div>processor&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 3<br>cpu&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : Cell Broadband Engine, altivec supported<br>clock&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 3200.000000MHz<br>revision&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 48.0 (pvr 0070 3000)</div>
<div>timebase&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 26666666<br>platform&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : Cell<br>machine&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : CHRP IBM,0793-1RZ<br></div>
<div><br><br>&nbsp;</div>
<div class="gmail_quote">On Thu, Oct 23, 2008 at 3:00 PM, Mi Yan <span dir="ltr">&lt;<a href="mailto:miyan@us.ibm.com">miyan@us.ibm.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="PADDING-LEFT: 1ex; MARGIN: 0px 0px 0px 0.8ex; BORDER-LEFT: #ccc 1px solid">
<div>
<p>Hi, Lenny,<br><br>So rank file map will be supported in OpenMPI 1.3? I&#39;m using OpenMPI1.2.6 and did not find parameter &quot;rmaps_rank_file_&quot;. <br>Do you have idea when OpenMPI 1.3 will be available? OpenMPI 1.3 has quite a few features I&#39;m looking for.<br>
<br>Thanks, 
<div class="Ih2E3d"><br>Mi <br><img height="16" alt="Inactive hide details for &quot;Lenny Verkhovsky&quot; &lt;lenny.verkhovsky@gmail.com&gt;" src="cid:2__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" width="16" border="0">&quot;Lenny Verkhovsky&quot; &lt;<a href="mailto:lenny.verkhovsky@gmail.com" target="_blank">lenny.verkhovsky@gmail.com</a>&gt;<br>
<br><br></div>
<table cellspacing="0" cellpadding="0" width="100%" border="0">
<tbody>
<tr valign="top">
<td><div class="Ih2E3d"></div>
<td width="40%">
<ul>
<ul>
<ul>
<ul><b><font size="2">&quot;Lenny Verkhovsky&quot; &lt;<a href="mailto:lenny.verkhovsky@gmail.com" target="_blank">lenny.verkhovsky@gmail.com</a>&gt;</font></b><font size="2"> </font><br><font size="2">Sent by: <a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a></font> 
<p><font size="2">10/23/2008 05:48 AM</font> 
<table border="1">
<tbody>
<tr valign="top">
<td width="168" bgcolor="#ffffff">
<div align="center"><font size="2">Please respond to<br>Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;</font></div></td></tr></tbody></table></p></ul></ul></ul></ul></td>

<td><div></div>
<td width="60%">
<table cellspacing="0" cellpadding="0" width="100%" border="0">
<td><div class="Ih2E3d"></div>
<tbody>
<tr valign="top">
<td width="1%"><img height="1" alt="" src="cid:4__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" width="58" border="0"><br>
<div align="right"><font size="2">To</font></div></td>
<td width="100%"><img height="1" alt="" src="cid:4__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" width="1" border="0"><br><font size="2">&quot;Open MPI Users&quot; &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;</font></td>
</tr>
<tr valign="top">
<td width="1%"><img height="1" alt="" src="cid:4__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" width="58" border="0"><br>
<div align="right"><font size="2">cc</font></div></td>
<td width="100%"><img height="1" alt="" src="cid:4__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" width="1" border="0"><br></td></tr>
<td><div></div>
<tr valign="top">
<td width="1%"><img height="1" alt="" src="cid:4__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" width="58" border="0"><br>
<div align="right"><font size="2">Subject</font></div></td>
<td width="100%"><img height="1" alt="" src="cid:4__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" width="1" border="0"><br><font size="2">Re: [OMPI users] Working with a CellBlade cluster</font></td></tr></td></tbody></td></table>

<table cellspacing="0" cellpadding="0" border="0">
<tbody>
<tr valign="top">
<td width="58"><img height="1" alt="" src="cid:4__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" width="1" border="0"></td>
<td width="336"><img height="1" alt="" src="cid:4__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" width="1" border="0"></td></tr></tbody></table></td></td></td></tr></tbody></table>
<div>
<div></div>
<div class="Wj3C7c"><br><font size="4">Hi,</font><br><font size="4"></font><br><font size="4"></font><br><font size="4">If I understand you correctly the most suitable way to do it is by paffinity that we have in Open MPI 1.3 and the trank.</font><br>
<font size="4">how ever usually OS is distributing processes evenly between sockets by it self.</font><br><font size="4"></font><br><font size="4">There still no formal FAQ due to a multiple reasons but you can read how to use it in the attached scratch ( there were few name changings of the params, so check with ompi_info )</font><br>
<font size="4"></font><br><font size="4">shared memory is used between processes that share same machine, and openib is used between different machines ( hostnames ), no special mca params are needed.</font><br><font size="4"></font><br>
<font size="4">Best Regards</font><br><font size="4">Lenny,</font> </div></div>
<p><font size="4"><br><br></font>
<p>
<div>
<div></div>
<div class="Wj3C7c"><font size="4">On Sun, Oct 19, 2008 at 10:32 AM, Gilbert Grosdidier &lt;</font><a href="mailto:grodid@mail.cern.ch" target="_blank"><u><font color="#0000ff" size="4">grodid@mail.cern.ch</font></u></a><font size="4">&gt; wrote:</font> 
<ul><font size="4">Working with a CellBlade cluster (QS22), the requirement is to have one<br>instance of the executable running on each socket of the blade (there are 2<br>sockets). The application is of the &#39;domain decomposition&#39; type, and each<br>
instance is required to often send/receive data with both the remote blades and<br>the neighbor socket.<br><br>Question is : which specification must be used for the mca btl component<br>to force 1) shmem type messages when communicating with this neighbor socket,<br>
while 2) using openib to communicate with the remote blades ?<br>Is &#39;-mca btl sm,openib,self&#39; suitable for this ?<br><br>Also, which debug flags could be used to crosscheck that the messages are<br>_actually_ going thru the right channel for a given channel, please ?<br>
<br>We are currently using OpenMPI 1.2.5 shipped with RHEL5.2 (ppc64).<br>Which version do you think is currently the most optimised for these<br>processors and problem type ? Should we go towards OpenMPI 1.2.8 instead ?<br>
Or even try some OpenMPI 1.3 nightly build ?<br><br>Thanks in advance for your help, Gilbert.<br><br>_______________________________________________<br>users mailing list</font><u><font color="#0000ff" size="4"><br></font></u><a href="mailto:users@open-mpi.org" target="_blank"><u><font color="#0000ff" size="4">users@open-mpi.org</font></u></a><u><font color="#0000ff" size="4"><br>
</font></u><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank"><u><font color="#0000ff" size="4">http://www.open-mpi.org/mailman/listinfo.cgi/users</font></u></a></ul></div></div>
<div class="Ih2E3d"><i>(See attached file: RANKS_FAQ.doc)</i></div><tt>_______________________________________________ 
<div class="Ih2E3d"><br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br></div></tt>
<div class="Ih2E3d"><tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></tt><br></div>
<p></p>
<p></p>
<p></p></p></p></p></div><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br></div>

