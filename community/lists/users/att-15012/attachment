Hi Gus Correa<br><br>First of all, thanks for your suggestions.<br><br>1) The malloc function do return a non_NULL pointer.<br><br>2) I didn&#39;t tried the MPI_Isend function, actually, The really function I need to use is MPI_Allgatherv(). When I used it, I found this function didn&#39;t work when the the data &gt;= 2GB, then I debugged it and found this function finally call the MPI_Send.<br>
<br>3) I have a large number of data need to train. so transfer the message &gt;= 2GB is neccerary. Although I can divided the data into smaller, but I think the effciency will become lower too.<br><br><br>Regards<br>Xianjun Meng<br>
<br><div class="gmail_quote">2010/12/7 Gus Correa <span dir="ltr">&lt;<a href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt;</span><br><blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">
Hi Xianjun<br>
<br>
Suggestions/Questions:<br>
<br>
1) Did you check if malloc returns a non-NULL pointer?<br>
Your program is assuming this, but it may not be true,<br>
and in this case the problem is not with MPI.<br>
You can print a message and call MPI_Abort if it doesn&#39;t.<br>
<br>
2) Have you tried MPI_Isend/MPI_Irecv?<br>
Or perhaps the buffered cousin MPI_Ibsend?<br>
<br>
3) Why do you want to send these huge messages?<br>
Wouldn&#39;t it be less of a trouble to send several<br>
smaller messages?<br>
<br>
I hope it helps,<br>
Gus Correa<br>
<br>
Xianjun wrote:<br>
<blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;"><div class="im">
<br>
Hi<br>
<br>
Are you running on two processes (mpiexec -n 2)?<br>
Yes<br>
<br>
Have you tried to print Gsize?<br>
Yes, I had checked my codes several times, and I thought the errors came from the OpenMpi. :)<br>
<br>
The command line I used:<br>
&quot;mpirun -hostfile ./Serverlist -np 2 ./test&quot;. The &quot;Serverlist&quot; file include several computers in my network.<br>
<br>
The command line that I used to build the openmpi-1.4.1:<br>
./configure --enable-debug --prefix=/usr/work/openmpi ; make all install;<br>
<br>
What interconnect do you use?<br>
It is normal TCP/IP interconnect with 1GB network card. when I debugged my codes(and the openmpi codes), I found the openMpi do call the &quot;mca_pml_ob1_send_request_start_rdma(...)&quot; function, but I was not quite sure which protocal was used when transfer 2BG data. Do you have any opinions? Thanks<br>

<br>
Best Regards<br>
Xianjun Meng<br>
<br></div>
2010/12/7 Gus Correa &lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a> &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;&gt;<div class="im">
<br>
<br>
    Hi Xianjun<br>
<br>
    Are you running on two processes (mpiexec -n 2)?<br>
    I think this code will deadlock for more than two processes.<br>
    The MPI_Recv won&#39;t have a matching send for rank&gt;1.<br>
<br>
    Also, this is C, not MPI,<br>
    but you may be wrapping into the negative numbers.<br>
    Have you tried to print Gsize?<br>
    It is probably -2147483648 in 32bit and 64bit machines.<br>
<br>
    My two cents.<br>
    Gus Correa<br>
<br>
    Mike Dubman wrote:<br>
<br>
        Hi,<br>
        What interconnect and command line do you use? For InfiniBand<br>
        openib component there is a known issue with large transfers (2GB)<br>
<br>
        <a href="https://svn.open-mpi.org/trac/ompi/ticket/2623" target="_blank">https://svn.open-mpi.org/trac/ompi/ticket/2623</a><br>
<br>
        try disabling memory pinning:<br>
        <a href="http://www.open-mpi.org/faq/?category=openfabrics#large-message-leave-pinned" target="_blank">http://www.open-mpi.org/faq/?category=openfabrics#large-message-leave-pinned</a><br>
<br>
<br>
        regards<br>
        M<br>
<br>
<br>
        2010/12/6 &lt;<a href="mailto:xjun.meng@gmail.com" target="_blank">xjun.meng@gmail.com</a> &lt;mailto:<a href="mailto:xjun.meng@gmail.com" target="_blank">xjun.meng@gmail.com</a>&gt;<br></div>
        &lt;mailto:<a href="mailto:xjun.meng@gmail.com" target="_blank">xjun.meng@gmail.com</a> &lt;mailto:<a href="mailto:xjun.meng@gmail.com" target="_blank">xjun.meng@gmail.com</a>&gt;&gt;&gt;<div class="im"><br>
<br>
<br>
           hi,<br>
<br>
           In my computers(X86-64), the sizeof(int)=4, but the<br>
           sizeof(long)=sizeof(double)=sizeof(size_t)=8. when I checked my<br>
           mpi.h file, I found that the definition about the sizeof(int) is<br>
           correct. meanwhile, I think the mpi.h file was generated<br>
        according<br>
           to my compute environment when I compiled the Openmpi. So, my<br>
        codes<br>
           still don&#39;t work. :(<br>
<br>
           Further, I found when I called the collective routines(such as,<br>
           MPI_Allgatherv(...)) which are implemented by the Point 2 Point<br>
           don&#39;t work either when the data &gt; 2GB.<br>
<br>
           Thanks<br>
           Xianjun<br>
<br>
           2010/12/6 Tim Prince &lt;<a href="mailto:n8tm@aol.com" target="_blank">n8tm@aol.com</a> &lt;mailto:<a href="mailto:n8tm@aol.com" target="_blank">n8tm@aol.com</a>&gt;<br></div>
        &lt;mailto:<a href="mailto:n8tm@aol.com" target="_blank">n8tm@aol.com</a> &lt;mailto:<a href="mailto:n8tm@aol.com" target="_blank">n8tm@aol.com</a>&gt;&gt;&gt;<div><div></div><div class="h5"><br>
<br>
<br>
               On 12/5/2010 7:13 PM, Xianjun wrote:<br>
<br>
                   hi,<br>
<br>
                   I met a question recently when I tested the MPI_send and<br>
                   MPI_Recv<br>
                   functions. When I run the following codes, the processes<br>
                   hanged and I<br>
                   found there was not data transmission in my network<br>
        at all.<br>
<br>
                   BTW: I finished this test on two X86-64 computers<br>
        with 16GB<br>
                   memory and<br>
                   installed Linux.<br>
<br>
                   1 #include &lt;stdio.h&gt;<br>
                   2 #include &lt;mpi.h&gt;<br>
                   3 #include &lt;stdlib.h&gt;<br>
                   4 #include &lt;unistd.h&gt;<br>
                   5<br>
                   6<br>
                   7 int main(int argc, char** argv)<br>
                   8 {<br>
                   9 int localID;<br>
                   10 int numOfPros;<br>
                   11 size_t Gsize = (size_t)2 * 1024 * 1024 * 1024;<br>
                   12<br>
                   13 char* g = (char*)malloc(Gsize);<br>
                   14<br>
                   15 MPI_Init(&amp;argc, &amp;argv);<br>
                   16 MPI_Comm_size(MPI_COMM_WORLD, &amp;numOfPros);<br>
                   17 MPI_Comm_rank(MPI_COMM_WORLD, &amp;localID);<br>
                   18<br>
                   19 MPI_Datatype MPI_Type_lkchar;<br>
                   20 MPI_Type_contiguous(2048, MPI_BYTE, &amp;MPI_Type_lkchar);<br>
                   21 MPI_Type_commit(&amp;MPI_Type_lkchar);<br>
                   22<br>
                   23 if (localID == 0)<br>
                   24 {<br>
                   25 MPI_Send(g, 1024*1024, MPI_Type_lkchar, 1, 1,<br>
                   MPI_COMM_WORLD);<br>
                   26 }<br>
                   27<br>
                   28 if (localID != 0)<br>
                   29 {<br>
                   30 MPI_Status status;<br>
                   31 MPI_Recv(g, 1024*1024, MPI_Type_lkchar, 0, 1, \<br>
                   32 MPI_COMM_WORLD, &amp;status);<br>
                   33 }<br>
                   34<br>
                   35 MPI_Finalize();<br>
                   36<br>
                   37 return 0;<br>
                   38 }<br>
<br>
               You supplied all your constants as 32-bit signed data,<br>
        so, even<br>
               if the count for MPI_Send() and MPI_Recv() were a larger data<br>
               type, you would see this limit. Did you look at your<br>
        &lt;mpi.h&gt; ?<br>
<br>
               --         Tim Prince<br>
<br>
               _______________________________________________<br>
               users mailing list<br>
               <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br></div></div>
        &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;&gt;<div class="im"><br>
<br>
               <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
           _______________________________________________<br>
           users mailing list<br>
           <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br></div>
        &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;&gt;<div class="im"><br>
<br>
           <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
        ------------------------------------------------------------------------<br>
<br>
        _______________________________________________<br>
        users mailing list<br>
        <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
        <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
    _______________________________________________<br>
    users mailing list<br>
    <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
    <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
------------------------------------------------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></blockquote><div><div></div><div class="h5">
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

