If the machine is multi-processor you might want to add the sm btl.&nbsp; That cleared up some similar problems for me, though I don&#39;t use mx so your millage may vary.<br><br><div><span class="gmail_quote">On 7/5/07, <b class="gmail_sendername">
SLIM H.A.</b> &lt;<a href="mailto:h.a.slim@durham.ac.uk">h.a.slim@durham.ac.uk</a>&gt; wrote:</span><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<br>Hello<br><br>I have compiled openmpi-1.2.3 with the --with-mx=&lt;directory&gt;<br>configuration and gcc compiler. On testing with 4-8 slots I get an error<br>message, the mx ports are busy:<br><br>&gt;mpirun --mca btl mx,self -np 4 ./cpi
<br>[node001:10071] mca_btl_mx_init: mx_open_endpoint() failed with<br>status=20<br>[node001:10074] mca_btl_mx_init: mx_open_endpoint() failed with<br>status=20<br>[node001:10073] mca_btl_mx_init: mx_open_endpoint() failed with
<br>status=20<br>------------------------------------------------------------------------<br>--<br>Process 0.1.0 is unable to reach 0.1.1 for MPI communication.<br>If you specified the use of a BTL component, you may have
<br>forgotten a component (such as &quot;self&quot;) in the list of<br>usable components.<br>... snipped<br>It looks like MPI_INIT failed for some reason; your parallel process is<br>likely to abort.&nbsp;&nbsp;There are many reasons that a parallel process can
<br>fail during MPI_INIT; some of which are due to configuration or<br>environment<br>problems.&nbsp;&nbsp;This failure appears to be an internal failure; here&#39;s some<br>additional information (which may only be relevant to an Open MPI
<br>developer):<br><br>&nbsp;&nbsp;PML add procs failed<br>&nbsp;&nbsp;--&gt; Returned &quot;Unreachable&quot; (-12) instead of &quot;Success&quot; (0)<br>------------------------------------------------------------------------<br>--<br>*** An error occurred in MPI_Init
<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (goodbye)<br>mpirun noticed that job rank 0 with PID 10071 on node node001 exited on<br>signal 1 (Hangup).<br><br><br>I would not expect mx messages as communication should not go through
<br>the mx card? (This is a twin dual core&nbsp;&nbsp;shared memory node)<br>The same happens when testing on 2 nodes, using a hostfile.<br>I checked the state of the mx card with mx_endpoint_info and mx_info,<br>they are healthy and free.
<br>What is missing here?<br><br>Thanks<br><br>Henk<br><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

