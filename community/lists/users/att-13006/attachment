Yes,<br>
<br>
it&#39;s correct, but you can use number of MPI_Test fails on MPI_ISend
routines to define a limit. Remember that if you use large buffer (more than eager_limit) send many times in a few time, maybe it&#39;s better to use MPI_Send
routine than MPI_ISend, to avoid too much buffer memory copy.<br><br><div class="gmail_quote">2010/5/11 Gijsbert Wiesenekker <span dir="ltr">&lt;<a href="mailto:gijsbert.wiesenekker@gmail.com">gijsbert.wiesenekker@gmail.com</a>&gt;</span><br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div style="word-wrap: break-word;"><div><div></div><div class="h5"><br><div><div>On May 11, 2010, at 9:29 , Gabriele Fatigati wrote:</div>
<br><blockquote type="cite">Dear Gijsbert,<br><br><br>&gt;Ideally I would like to check how many MPI_Isend messages have not been
processed yet, so that I can stop &gt;sending messages if there are &#39;too
many&#39; waiting. Is there a way to do this?<br><br><br>you can check number of message pending simply using MPI_Test function. It return false if the request of message is in pending status. The difference with MPI_Wait is in the behaviour of these two routines: MPI_Wait is blocking, MPi_Test,  check and returns immediately.<br>

<br>Regards.<br><br><div class="gmail_quote">2010/5/11 Gijsbert Wiesenekker <span dir="ltr">&lt;<a href="mailto:gijsbert.wiesenekker@gmail.com" target="_blank">gijsbert.wiesenekker@gmail.com</a>&gt;</span><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">

An OpenMPI program of mine that uses MPI_Isend and MPI_Irecv crashes after some non-reproducible time my Fedora Linux kernel (invalid opcode), which makes it hard to debug (there is no trace, even with the debug kernel, and if I run it under valgrind it does not crash).<br>


My guess is that the kernel crash is caused by OpenMPI running out if memory because too many MPI_Irecv messages have been sent but not been processed yet.<br>
My questions are:<br>
What does the OpenMPI specification say about the behaviour of MPI_Isend when many messages have been sent but have not been processed yet? Will it fail? Will it block until more memory becomes available (I hope not, because this would cause my program to deadlock)?<br>


Ideally I would like to check how many MPI_Isend messages have not been processed yet, so that I can stop sending messages if there are &#39;too many&#39; waiting. Is there a way to do this?<br>
<br>
Regards,<br>
Gijsbert<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
</blockquote></div><br><br clear="all"><br>-- <br>Ing. Gabriele Fatigati<br><br>Parallel programmer<br><br>CINECA Systems &amp; Tecnologies Department<br><br>Supercomputing Group<br><br>Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>

<br><a href="http://www.cineca.it/" target="_blank">www.cineca.it</a>                    Tel:   +39 051 6171722<br><br>g.fatigati [AT] <a href="http://cineca.it/" target="_blank">cineca.it</a>           <br>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div></div><div>I thought that successful returns from MPI_Wait or MPI_test after an MPI_Isend only meant that the message was received successfully by OpenMPI and that the buffer could be reused, and not that it was successfully received by the other processor. Is that correct?</div>
<div><br></div><font color="#888888"><div>Gijsbert</div><div><br></div></font></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br><br clear="all"><br>-- <br>Ing. Gabriele Fatigati<br><br>Parallel programmer<br>
<br>CINECA Systems &amp; Tecnologies Department<br><br>Supercomputing Group<br><br>Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br><br><a href="http://www.cineca.it">www.cineca.it</a>                    Tel:   +39 051 6171722<br>
<br>g.fatigati [AT] <a href="http://cineca.it">cineca.it</a>           <br>

