<div>But that is what surprises me. Indeed the scenario I described can be implemented using two-sided communication, but it seems not to be possible when using one sided communication.</div>
<div> </div>
<div>Additionally the MPI 2.2. standard describes on page 356 the matching rules for post and start, complete and wait and there it says : &quot;<font face="CMSSBX10" size="3"><font face="CMSSBX10" size="3">MPI</font></font><font face="CMTT10" size="3"><font face="CMTT10" size="3">_</font></font><font face="CMSSBX10" size="3"><font face="CMSSBX10" size="3">WIN</font></font><font face="CMTT10" size="3"><font face="CMTT10" size="3">_</font></font><font face="CMSSBX10" size="3"><font face="CMSSBX10" size="3">COMPLETE(win) </font></font><font face="CMR10" size="3"><font face="CMR10" size="3">initiate a nonblocking send with tag </font></font><font face="CMSS10" size="3"><font face="CMSS10" size="3">tag1 </font></font><font face="CMR10" size="3"><font face="CMR10" size="3">to each process in the group of the preceding start call. No need to wait for the completion of these sends.&quot; </font></font><br>
The wording &#39;nonblocking send&#39; startles me somehow !?</div>
<div> </div>
<div>toon</div>
<div><br> </div>
<div class="gmail_quote">On Thu, Feb 24, 2011 at 2:05 PM, James Dinan <span dir="ltr">&lt;<a href="mailto:dinan@mcs.anl.gov">dinan@mcs.anl.gov</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="PADDING-LEFT: 1ex; MARGIN: 0px 0px 0px 0.8ex; BORDER-LEFT: #ccc 1px solid">Hi Toon,<br><br>Can you use non-blocking send/recv?  It sounds like this will give you the completion semantics you want.<br>
<br>Best,<br> ~Jim. 
<div class="im"><br><br>On 2/24/11 6:07 AM, Toon Knapen wrote:<br></div>
<blockquote class="gmail_quote" style="PADDING-LEFT: 1ex; MARGIN: 0px 0px 0px 0.8ex; BORDER-LEFT: #ccc 1px solid">
<div class="im">In that case, I have a small question concerning design:<br>Suppose task-based parallellism where one node (master) distributes<br>work/tasks to 2 other nodes (slaves) by means of an MPI_Put. The master<br>
allocates 2 buffers locally in which it will store all necessary data<br>that is needed by the slave to perform the task. So I do an MPI_Put on<br>each of my 2 buffers to send each buffer to a specific slave. Now I need<br>
to know when I can reuse one of my buffers to already store the next<br>task (that I will MPI_Put later on). The only way to know this is call<br>MPI_Complete. But since this is blocking and if this buffer is not ready<br>
to be reused yet, I can neither verify if the other buffer is already<br>available to me again (in the same thread).<br>I would very much appreciate input on how to solve such issue !<br>thanks in advance,<br>toon<br>On Tue, Feb 22, 2011 at 7:21 PM, Barrett, Brian W &lt;<a href="mailto:bwbarre@sandia.gov" target="_blank">bwbarre@sandia.gov</a><br>
</div>
<div class="im">&lt;mailto:<a href="mailto:bwbarre@sandia.gov" target="_blank">bwbarre@sandia.gov</a>&gt;&gt; wrote:<br><br>   On Feb 18, 2011, at 8:59 AM, Toon Knapen wrote:<br><br>    &gt; (Probably this issue has been discussed at length before but<br>
   unfortunately I did not find any threads (on this site or anywhere<br>   else) on this topic, if you are able to provide me with links to<br>   earlier discussions on this topic, please do not hesitate)<br>    &gt;<br>
    &gt; Is there an alternative to MPI_Win_complete that does not<br>   &#39;enforce completion of preceding RMS calls at the origin&#39; (as said<br>   on pag 353 of the mpi-2.2 standard) ?<br>    &gt;<br>    &gt; I would like to know if I can reuse the buffer I gave to MPI_Put<br>
   but without blocking on it, if the MPI lib is still using it, I want<br>   to be able to continue (and use another buffer).<br><br><br>   There is not.   MPI_Win_complete is the only way to finish a<br>   MPI_Win_start epoch, and is always blocking until local completion<br>
   of all messages started during the epoch.<br><br>   Brian<br><br>   --<br>     Brian W. Barrett<br>     Dept. 1423: Scalable System Software<br>     Sandia National Laboratories<br><br><br><br>   _______________________________________________<br>
   users mailing list<br></div>   <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt; 
<div class="im"><br>   <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><br><br><br>_______________________________________________<br>
users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></blockquote>
<div>
<div></div>
<div class="h5"><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

