<table cellspacing="0" cellpadding="0" border="0" ><tr><td valign="top" style="font: inherit;">I have had a similar load related problem with Bcast.&nbsp; I don't know what caused it though.&nbsp; With this one, what about the possibility of a buffer overrun or network saturation?<br><br><br>--- On <b>Tue, 24/8/10, Richard Treumann <i>&lt;treumann@us.ibm.com&gt;</i></b> wrote:<br><blockquote style="border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;"><br>From: Richard Treumann &lt;treumann@us.ibm.com&gt;<br>Subject: Re: [OMPI users] IMB-MPI broadcast test stalls for large core counts: debug ideas?<br>To: "Open MPI Users" &lt;users@open-mpi.org&gt;<br>Received: Tuesday, 24 August, 2010, 9:39 AM<br><br><div id="yiv1578815145">
<br><font face="sans-serif" size="2">It is hard to imagine how a total data
load of </font><tt><font size="2">41,943,040 bytes could be a problem. That
is really not much data. By the time the BCAST is done, each task (except
root) will have received a single half meg message form one sender. That
is not much.</font></tt>
<br>
<br><tt><font size="2">IMB does shift the root so some tasks may be in iteration
9 while some are still in iteration 8 or 7 but a 1/2 meg message should
use rendezvous protocol so no message will be injected into the network
until the destination task is ready to receive it.</font></tt>
<br>
<br><tt><font size="2">Any task can be in only one MPI_Bcast at a time so
the total active data cannot ever exceed the 41,943,040 bytes, no matter
how fast the MPI_Bcast loop tries to iterate.</font></tt>
<br>
<br><tt><font size="2">(There are MPI_Bcast algorithms that chunk the data
into smaller messages but even with those algorithms, the total concurrent
load will not exceed 41,943,040 bytes.)</font></tt>
<br>
<br>
<br><font face="sans-serif" size="2">Dick Treumann &nbsp;- &nbsp;MPI Team
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br>
IBM Systems &amp; Technology Group<br>
Dept X2ZA / MS P963 -- 2455 South Road -- Poughkeepsie, NY 12601<br>
Tele (845) 433-7846 &nbsp; &nbsp; &nbsp; &nbsp; Fax (845) 433-8363<br>
</font>
<br>
<br><tt><font size="2">users-bounces@open-mpi.org wrote on 08/23/2010 05:09:56
PM:<br>
<br>
&gt; [image removed] </font></tt>
<br><tt><font size="2">&gt; <br>
&gt; Re: [OMPI users] IMB-MPI broadcast test stalls for large core <br>
&gt; counts: debug ideas?</font></tt>
<br><tt><font size="2">&gt; <br>
&gt; Rahul Nabar </font></tt>
<br><tt><font size="2">&gt; <br>
&gt; to:</font></tt>
<br><tt><font size="2">&gt; <br>
&gt; Open MPI Users</font></tt>
<br><tt><font size="2">&gt; <br>
&gt; 08/23/2010 05:11 PM</font></tt>
<br><tt><font size="2">&gt; <br>
&gt; Sent by:</font></tt>
<br><tt><font size="2">&gt; <br>
&gt; users-bounces@open-mpi.org</font></tt>
<br><tt><font size="2">&gt; <br>
&gt; Please respond to Open MPI Users</font></tt>
<br><tt><font size="2">&gt; <br>
&gt; <br>
</font></tt>
<br><tt><font size="2">&gt; On Sun, Aug 22, 2010 at 9:57 PM, Randolph Pullen
&lt;randolph_pullen@yahoo.com.au<br>
&gt; &gt; wrote:</font></tt>
<br><tt><font size="2">&gt; <br>
&gt; Its a long shot but could it be related to the total data volume ?<br>
&gt; ie&nbsp; 524288 * 80 = 41943040 bytes active in the cluster<br>
&gt; <br>
&gt; Can you exceed this 41943040 data volume with a smaller message <br>
&gt; repeated more often or a larger one less often?</font></tt>
<br><tt><font size="2">&gt; <br>
&gt; <br>
&gt; Not so far, so your diagnosis could be right. The failures have been<br>
&gt; at the following data volumes:<br>
&gt; <br>
&gt; 41.9E6<br>
&gt; 4.1E6<br>
&gt; 8.2E6 <br>
&gt; <br>
&gt; Unfortunately, I'm not sure I can change the repeat rate with the
<br>
&gt; OFED/MPI tests. Can I do that? Didn't see a suitable flag.<br>
&gt; <br>
&gt; In any case, assuming it is related to the total data volume what
<br>
&gt; could be causing such a failure?<br>
&gt; <br>
&gt; -- <br>
&gt; Rahul_______________________________________________<br>
&gt; users mailing list<br>
&gt; users@open-mpi.org<br>
&gt; </font></tt><a rel="nofollow" target="_blank" href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size="2">http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a></div><br>-----Inline Attachment Follows-----<br><br><div class="plainMail">_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="/mc/compose?to=users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></blockquote></td></tr></table><br>



      &nbsp;
