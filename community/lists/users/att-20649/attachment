Yes, we definitely should do so - will put it on the Trac system so it gets done.<br><br>Thanks - and sorry it wasn&#39;t already there.<br><br><div class="gmail_extra"><br><br><div class="gmail_quote">On Wed, Nov 7, 2012 at 4:49 AM, Iliev, Hristo <span dir="ltr">&lt;<a href="mailto:iliev@rz.rwth-aachen.de" target="_blank">iliev@rz.rwth-aachen.de</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hello, Markus,<br>
<br>
The openib BTL component is not thread-safe. It disables itself when the<br>
thread support level is MPI_THREAD_MULTIPLE. See this rant from one of my<br>
colleagues:<br>
<br>
<a href="http://www.open-mpi.org/community/lists/devel/2012/10/11584.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2012/10/11584.php</a><br>
<br>
A message is shown but only if the library was compiled with developer-level<br>
debugging.<br>
<br>
Open MPI guys, could the debug-level message in<br>
btl_openib_component.c:btl_openib_component_init() be replaced by a help<br>
text, e.g. the same way that the help text about the amount of registerable<br>
memory not being enough is shown. Looks like the case of openib being<br>
disabled for no apparent reason when MPI_THREAD_MULTIPLE is in effect is not<br>
isolated to our users only. Or at least could you put somewhere in the FAQ<br>
an explicit statement that openib is not only not thread-safe, but that it<br>
would disable itself in a multithreaded environment.<br>
<br>
Kind regards,<br>
Hristo<br>
--<br>
Hristo Iliev, Ph.D. -- High Performance Computing<br>
RWTH Aachen University, Center for Computing and Communication<br>
Rechen- und Kommunikationszentrum der RWTH Aachen<br>
Seffenter Weg 23,  D 52074  Aachen (Germany)<br>
Tel: <a href="tel:%2B49%20241%2080%2024367" value="+492418024367">+49 241 80 24367</a> -- Fax/UMS: <a href="tel:%2B49%20241%2080%20624367" value="+4924180624367">+49 241 80 624367</a><br>
<div class="HOEnZb"><div class="h5"><br>
&gt; -----Original Message-----<br>
&gt; From: <a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a> [mailto:<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>]<br>
&gt; On Behalf Of Markus Wittmann<br>
&gt; Sent: Wednesday, November 07, 2012 1:14 PM<br>
&gt; To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subject: [OMPI users] Problems with btl openib and MPI_THREAD_MULTIPLE<br>
&gt;<br>
&gt; Hello,<br>
&gt;<br>
&gt; I&#39;ve compiled Open MPI 1.6.3 with --enable-mpi-thread-multiple -with-tm -<br>
&gt; with-openib --enable-opal-multi-threads.<br>
&gt;<br>
&gt; When I use for example the pingpong benchmark from the Intel MPI<br>
&gt; Benchmarks, which call MPI_Init the btl openib is used and everything<br>
works<br>
&gt; fine.<br>
&gt;<br>
&gt; When instead the benchmark calls MPI_Thread_init with<br>
&gt; MPI_THREAD_MULTIPLE as requested threading level the btl openib fails to<br>
&gt; load but gives no further hints for the reason:<br>
&gt;<br>
&gt; mpirun -v -n 2 -npernode 1 -gmca btl_base_verbose 200 ./imb- tm-openmpi-<br>
&gt; ts pingpong<br>
&gt;<br>
&gt; ...<br>
&gt; [l0519:08267] select: initializing btl component openib [l0519:08267]<br>
select:<br>
&gt; init of component openib returned failure [l0519:08267] select: module<br>
&gt; openib unloaded ...<br>
&gt;<br>
&gt; The question is now, is currently just the support for<br>
&gt; MPI_THREADM_MULTIPLE missing in the openib module or are there other<br>
&gt; errors occurring and if so, how to identify them.<br>
&gt;<br>
&gt; Attached ist the config.log from the Open MPI build, the ompi_info output<br>
&gt; and the output of the IMB pingpong bechmarks.<br>
&gt;<br>
&gt; As system used were two nodes with:<br>
&gt;<br>
&gt;   - OpenFabrics 1.5.3<br>
&gt;   - CentOS release 5.8 (Final)<br>
&gt;   - Linux Kernel 2.6.18-308.11.1.el5 x86_64<br>
&gt;   - OpenSM 3.3.3<br>
&gt;<br>
&gt; [l0519] src &gt; ibv_devinfo<br>
&gt; hca_id: mlx4_0<br>
&gt;         transport:                      InfiniBand (0)<br>
&gt;         fw_ver:                         2.7.000<br>
&gt;         node_guid:                      0030:48ff:fff6:31e4<br>
&gt;         sys_image_guid:                 0030:48ff:fff6:31e7<br>
&gt;         vendor_id:                      0x02c9<br>
&gt;         vendor_part_id:                 26428<br>
&gt;         hw_ver:                         0xB0<br>
&gt;         board_id:                       SM_2122000001000<br>
&gt;         phys_port_cnt:                  1<br>
&gt;                 port:   1<br>
&gt;                         state:                  PORT_ACTIVE (4)<br>
&gt;                         max_mtu:                2048 (4)<br>
&gt;                         active_mtu:             2048 (4)<br>
&gt;                         sm_lid:                 48<br>
&gt;                         port_lid:               278<br>
&gt;                         port_lmc:               0x00<br>
&gt;<br>
&gt; Thanks for the help in advance.<br>
&gt;<br>
&gt; Regards,<br>
&gt; Markus<br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Markus Wittmann, HPC Services<br>
&gt; Friedrich-Alexander-Universität Erlangen-Nürnberg Regionales<br>
&gt; Rechenzentrum Erlangen (RRZE) Martensstrasse 1, 91058 Erlangen, Germany<br>
&gt; Tel.: +49 9131 85-20104<br>
&gt; <a href="mailto:markus.wittmann@fau.de">markus.wittmann@fau.de</a><br>
&gt; <a href="http://www.rrze.fau.de/hpc/" target="_blank">http://www.rrze.fau.de/hpc/</a><br>
</div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>

