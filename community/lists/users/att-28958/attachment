<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">Hi Farid<div class=""><br class=""></div><div class="">I’m not sure I understand what you are asking here. If your point is that OMPI isn’t placing and binding procs per the LSF directives, then you are quite correct. The LSF folks never provided that level of integration, nor the info by which we might have derived it (e.g., how the pattern is communicated).</div><div class=""><br class=""></div><div class="">If someone from IBM would like to provide that code, we’d be happy to help answer questions as to how to perform the integration.</div><div class=""><br class=""></div><div class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Apr 18, 2016, at 10:13 AM, Farid Parpia &lt;<a href="mailto:parpia@us.ibm.com" class="">parpia@us.ibm.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><font size="2" face="sans-serif" class="">Greetings!</font><br class=""><br class=""><font size="2" face="sans-serif" class="">The following batch script will successfully
demo the use of LSF's task geometry feature using IBM Parallel Environment:</font><br class=""><font size="2" face="Courier New" class="">#BUB -J "task_geometry"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -n 9</font><br class=""><font size="2" face="Courier New" class="">#BSUB -R "span[ptile=3]"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -network "type=sn_single:mode=us"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -R "affinity[core]"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -e "task_geometry.stderr.%J"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -o "task_geometry.stdout.%J"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -q "normal"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -M "800"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -R "rusage[mem=800]"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -x</font><br class=""><br class=""><font size="2" face="Courier New" class="">export LSB_PJL_TASK_GEOMETRY="{(5)(4,3)(2,1,0)}"</font><br class=""><br class=""><font size="2" face="Courier New" class="">ldd /gpfs/gpfs_stage1/parpia/PE_tests/reporter/bin/reporter_MPI</font><br class=""><br class=""><font size="2" face="Courier New" class="">/gpfs/gpfs_stage1/parpia/PE_tests/reporter/bin/reporter_MPI</font><br class=""><font size="2" face="sans-serif" class="">The </font><font size="2" face="Courier New" class="">reporter_MPI</font><font size="2" face="sans-serif" class="">utility simply reports the hostname and affinitization for each MPI process,
and is what I use to verify that the job is distributed to allocated nodes
and on them with the affinitization expected. &nbsp;Typical output is</font><br class=""><font size="2" face="sans-serif" class="">&nbsp; &nbsp; &nbsp; &nbsp; ,
</font><br class=""><br class=""><font size="2" face="sans-serif" class="">To adapt the above batch script to use
OpenMPI, I modify it to</font><br class=""><font size="2" face="Courier New" class="">#BSUB -J "task_geometry"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -n 9</font><br class=""><font size="2" face="Courier New" class="">#BSUB -R "span[ptile=3]"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -m "p10a30 p10a33 p10a35
p10a55 p10a58"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -R "affinity[core]"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -e "task_geometry.stderr.%J"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -o "task_geometry.stdout.%J"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -q "normal"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -M "800"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -R "rusage[mem=800]"</font><br class=""><font size="2" face="Courier New" class="">#BSUB -x</font><br class=""><br class=""><font size="2" face="Courier New" class="">export PATH=/usr/local/OpenMPI/1.10.2/bin:${PATH}</font><br class=""><font size="2" face="Courier New" class="">export LD_LIBRARY_PATH=/usr/local/OpenMPI/1.10.2/lib:${PATH}</font><br class=""><br class=""><font size="2" face="Courier New" class="">export LSB_PJL_TASK_GEOMETRY="{(5)(4,3)(2,1,0)}"</font><br class=""><br class=""><font size="2" face="Courier New" class="">echo "=== LSB_DJOB_HOSTFILE ==="</font><br class=""><font size="2" face="Courier New" class="">cat ${LSB_DJOB_HOSTFILE}</font><br class=""><font size="2" face="Courier New" class="">echo "=== LSB_AFFINITY_HOSTFILE
==="</font><br class=""><font size="2" face="Courier New" class="">cat ${LSB_AFFINITY_HOSTFILE}</font><br class=""><font size="2" face="Courier New" class="">echo "=== LSB_DJOB_RANKFILE ==="</font><br class=""><font size="2" face="Courier New" class="">cat ${LSB_DJOB_RANKFILE}</font><br class=""><font size="2" face="Courier New" class="">echo "========================="</font><br class=""><br class=""><font size="2" face="Courier New" class="">ldd /gpfs/gpfs_stage1/parpia/OpenMPI_tests/reporter/bin/reporter_MPI</font><br class=""><br class=""><font size="2" face="Courier New" class="">mpirun /gpfs/gpfs_stage1/parpia/OpenMPI_tests/reporter/bin/reporter_MPI</font><br class=""><font size="2" face="sans-serif" class="">There are additional lines of scripting
that I have inserted to help with debugging this failing job. &nbsp;Here
are the output files from the job:</font><br class=""><font size="2" face="sans-serif" class="">&nbsp; &nbsp; &nbsp; &nbsp; ,
</font><br class=""><font size="2" face="sans-serif" class="">If I change the last line of the immediately
above job script to</font><br class=""><font size="2" face="sans-serif" class="">&nbsp; &nbsp; &nbsp; &nbsp; </font><font size="2" face="Courier New" class="">mpirun
-bind-to core:overload-allowed /gpfs/gpfs_stage1/parpia/OpenMPI_tests/reporter/bin/reporter_MPI</font><br class=""><font size="2" face="sans-serif" class="">the job runs through, but the host selection
and affinization is completely wrong (you can extract the relevant information
with </font><font size="2" face="Courier New" class="">grep "can be sched"
*.stdout.* | sort -n -k 9</font><font size="2" face="sans-serif" class="">):</font><br class=""><font size="2" face="sans-serif" class="">&nbsp; &nbsp; &nbsp; &nbsp; ,
</font><br class=""><font size="2" face="sans-serif" class="">OpenMPI 1.10.2 was built using this
script:</font><br class=""><font size="2" face="sans-serif" class="">&nbsp; &nbsp; &nbsp; &nbsp; </font><br class=""><font size="2" face="sans-serif" class="">It was installed with</font><br class=""><font size="2" face="Courier New" class="">make install</font><br class=""><font size="2" face="sans-serif" class="">executed from the top if the build tree.
&nbsp;Here</font><br class=""><font size="2" face="sans-serif" class="">&nbsp; &nbsp; &nbsp; &nbsp; </font><br class=""><font size="2" face="sans-serif" class="">is the output of</font><br class=""><font size="2" face="Courier New" class="">ompi_info --all</font><br class=""><br class=""><font size="2" face="sans-serif" class="">Regards,</font><br class=""><br class=""><font size="2" face="sans-serif" class="">Farid Parpia &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;IBM Corporation: 710-2-RF28, 2455 South Road, Poughkeepsie, NY 12601,
USA; Telephone: (845) 433-8420 = Tie Line 293-8420</font><br class=""><span id="cid:5AC0430D-EDDF-4896-9EED-759D1C9971A1">&lt;task_geometry.stdout.43915.gz&gt;</span><span id="cid:0911A6A5-1878-452E-B767-48E77CAE61A7">&lt;task_geometry.stderr.43915.gz&gt;</span><span id="cid:D55762D9-A90D-4F1E-8969-A1A0EC7C5202">&lt;task_geometry.stderr.43918.gz&gt;</span><span id="cid:B9D52073-ADD9-4B92-B998-0229EDAAD9C4">&lt;task_geometry.stdout.43918.gz&gt;</span><span id="cid:17D931E1-FDE6-42A3-A534-90D05D048974">&lt;task_geometry.stderr.43953.gz&gt;</span><span id="cid:97DD5AD9-6867-44AA-B6A7-9E7CDBC2C5AF">&lt;task_geometry.stdout.43953.gz&gt;</span><span id="cid:0842C830-617D-4252-9A59-CC5203CFC0D7">&lt;build_OpenMPI.sh&gt;</span><span id="cid:82ADA82E-E86E-41AD-AA9E-C0DE6D5BC337">&lt;ompi_info--all.gz&gt;</span>_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2016/04/28955.php</div></blockquote></div><br class=""></div></body></html>
