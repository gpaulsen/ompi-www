<html>
<head>
<style><!--
.hmmessage P
{
margin:0px;
padding:0px
}
body.hmmessage
{
font-size: 10pt;
font-family:Verdana
}
--></style>
</head>
<body class='hmmessage'>
In that case, the way I installed it is not right. I thought that only the HN should be configured with the tm support <br>not the worker nodes; the worker nodes only have the PBS daemon clients - No need for tm support on the worker nodes.<br>&nbsp;<br>When I ran ompi_info | grep tm on the worker nodes, the output is empty.<br><br>The information on the following link has mislead me then:<br>http://www.physics.iitm.ac.in/~sanoop/linux_files/cluster.html <br>(check OpenMPI Configuration section.)<br><br>~Belaid.<br>&gt; Date: Tue, 1 Dec 2009 18:36:15 -0500<br>&gt; From: gus@ldeo.columbia.edu<br>&gt; To: users@open-mpi.org<br>&gt; Subject: Re: [OMPI users] mpirun is using one PBS node only<br>&gt; <br>&gt; Hi Belaid Moa<br>&gt; <br>&gt; The OpenMPI I install and use is on a NFS mounted directory.<br>&gt; Hence, all the nodes see the same version, which has "tm" support.<br>&gt; <br>&gt; After reading your OpenMPI configuration parameters on the headnode<br>&gt; and working nodes (and the difference between them),<br>&gt; I would guess (just a guess) that the problem you see is because your<br>&gt; OpenMPI version on the nodes (probably) do not have Torque support.<br>&gt; <br>&gt; However, you should first verify that this is really the case,<br>&gt; because if the OpenMPI configure script<br>&gt; finds the torque libraries it will (probably) configure and<br>&gt; install OpenMPI with "tm" support, even if you don't ask it<br>&gt; explicitly on the working nodes.<br>&gt; Hence, ssh to WN1 or WN2 and do "ompi_info" to check this out first.<br>&gt; <br>&gt; If there is no Torque on WN1 and WN2 then OpenMPI won't find it<br>&gt; and you won't have "tm" support on the nodes.<br>&gt; <br>&gt; In any case, if OpenMPI "tm" support is missing on WN[1,2},<br>&gt; I would suggest that you reinstall OpenMPI on WN1 and WN2 *with tm support*.<br>&gt; This will require that you have Torque on the working nodes also,<br>&gt; and use the same configure command line that you used on the headnode.<br>&gt; <br>&gt; A low-tech alternative is to copy over your OpenMPI directory tree to <br>&gt; the WN1 and WN2 nodes.<br>&gt; <br>&gt; A yet simpler alternative is to reinstall OpenMPI on the headnode<br>&gt; on a NFS mounted directory (as I do here), then<br>&gt; add the corresponding "bin" path to your PATH,<br>&gt; and the corresponding "lib" path to your LD_LIBRARY_PATH environment<br>&gt; variables.<br>&gt; <br>&gt; Think about maintenance, and upgrades:<br>&gt; On an NFS mounted directory<br>&gt; you need to install only once, whereas the way you have it now you need<br>&gt; to do it N+1 times (or have a mechanism to propagate a single<br>&gt; installation from the head node to the compute nodes).<br>&gt; <br>&gt; NFS is your friend!  :)<br>&gt; <br>&gt; I hope this helps,<br>&gt; Gus Correa<br>&gt; ---------------------------------------------------------------------<br>&gt; Gustavo Correa<br>&gt; Lamont-Doherty Earth Observatory - Columbia University<br>&gt; Palisades, NY, 10964-8000 - USA<br>&gt; ---------------------------------------------------------------------<br>&gt; <br>&gt; <br>&gt; Belaid MOA wrote:<br>&gt; &gt; I tried -bynode option but it did not change anything. I also tried the <br>&gt; &gt; "hostname" name command and<br>&gt; &gt; I keep getting only the name of one node repeated according to the -n <br>&gt; &gt; value.<br>&gt; &gt; <br>&gt; &gt; Just to make sure I did the right installation, here is what I did:<br>&gt; &gt; <br>&gt; &gt; -- On the head node (HN), I installed openMPI using the --with-tm option <br>&gt; &gt; as follows:<br>&gt; &gt; <br>&gt; &gt; ./configure --with-tm=/var/spool/torque --enable-static<br>&gt; &gt; make install all<br>&gt; &gt; <br>&gt; &gt; -- On the worker nodes (WN1 and WN2), I installed openMPI without tm <br>&gt; &gt; option as follows (it is a local installation on each worker node):<br>&gt; &gt; <br>&gt; &gt; ./configure --enable-static<br>&gt; &gt; make install all<br>&gt; &gt; <br>&gt; &gt; Is this correct?<br>&gt; &gt; <br>&gt; &gt; Thanks a lot in advance.<br>&gt; &gt; ~Belaid.<br>&gt; &gt;  &gt; Date: Tue, 1 Dec 2009 17:07:58 -0500<br>&gt; &gt;  &gt; From: gus@ldeo.columbia.edu<br>&gt; &gt;  &gt; To: users@open-mpi.org<br>&gt; &gt;  &gt; Subject: Re: [OMPI users] mpirun is using one PBS node only<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; Hi Belaid Moa<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; Belaid MOA wrote:<br>&gt; &gt;  &gt; &gt; Thanks a lot Gus for you help again. I only have one CPU per node.<br>&gt; &gt;  &gt; &gt; The -n X option (no matter what the value of X is) shows X processes<br>&gt; &gt;  &gt; &gt; running on one node only (the other one is free).<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; So, somehow it is oversubscribing your single processor<br>&gt; &gt;  &gt; on the first node.<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; A simple diagnostic:<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; Have you tried to run "hostname" on the two nodes through Torque/PBS<br>&gt; &gt;  &gt; and mpiexec?<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; [PBS directives, cd $PBS_O_WORKDIR, etc]<br>&gt; &gt;  &gt; ...<br>&gt; &gt;  &gt; /full/path/to/openmpi/bin/mpiexec -n 2 hostname<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; Try also with the -byslot and -bynode options.<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; &gt; If I add the machinefile option with WN1 and WN2 in it, the right<br>&gt; &gt;  &gt; &gt; behavior is manifested. According to the documentation,<br>&gt; &gt;  &gt; &gt; mpirun should get the PBS_NODEFILE automatically from the PBS.<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; Yes, if you compiled OpenMPI you are using with Torque ("tm) support.<br>&gt; &gt;  &gt; Did you?<br>&gt; &gt;  &gt; Make sure the it has tm support.<br>&gt; &gt;  &gt; Run "ompi_info" with full path if needed, to check that.<br>&gt; &gt;  &gt; Are you sure the correct path to what you want is<br>&gt; &gt;  &gt; /usr/local/bin/mpirun ?<br>&gt; &gt;  &gt; Linux distributions, compilers, and other tools come with their<br>&gt; &gt;  &gt; mpiexec and put them in places that you may not suspect, to better<br>&gt; &gt;  &gt; double check you get what you want.<br>&gt; &gt;  &gt; That has been a source of repeated confusion on this and other<br>&gt; &gt;  &gt; mailing lists.<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; Also, make sure that passwordless ssh across the nodes is working.<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; Yet another thing to check, for easy name resolution,<br>&gt; &gt;  &gt; your /etc/hosts file on *all*<br>&gt; &gt;  &gt; nodes including the headnode should<br>&gt; &gt;  &gt; have a list of all nodes and their IP addresses.<br>&gt; &gt;  &gt; Something like this:<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; 127.0.0.1 localhost.localdomain localhost<br>&gt; &gt;  &gt; 192.168.0.1 WN1<br>&gt; &gt;  &gt; 192.168.0.2 WN2<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; (The IPs above are guesswork of mine, you know better which to use.)<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; &gt; So, I do<br>&gt; &gt;  &gt; &gt; not need to use machinefile.<br>&gt; &gt;  &gt; &gt;<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; True assuming the first condition above (OpenMPI *with* "tm" suport).<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; &gt; Any ideas?<br>&gt; &gt;  &gt; &gt;<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; Yes, and I sent it to you on my last email!<br>&gt; &gt;  &gt; Try the "-bynode" option of mpiexec.<br>&gt; &gt;  &gt; ("man mpiexec" is your friend!)<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; &gt; Thanks a lot in advance.<br>&gt; &gt;  &gt; &gt; ~Belaid.<br>&gt; &gt;  &gt; &gt;<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; Best of luck!<br>&gt; &gt;  &gt; Gus Correa<br>&gt; &gt;  &gt; ---------------------------------------------------------------------<br>&gt; &gt;  &gt; Gustavo Correa<br>&gt; &gt;  &gt; Lamont-Doherty Earth Observatory - Columbia University<br>&gt; &gt;  &gt; Palisades, NY, 10964-8000 - USA<br>&gt; &gt;  &gt; ---------------------------------------------------------------------<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; PS - Your web site link to Paul Krugman is out of date.<br>&gt; &gt;  &gt; Here are one to his (active) blog,<br>&gt; &gt;  &gt; and another to his (no longer updated) web page: :)<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; http://krugman.blogs.nytimes.com/<br>&gt; &gt;  &gt; http://www.princeton.edu/~pkrugman/<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; Date: Tue, 1 Dec 2009 15:42:30 -0500<br>&gt; &gt;  &gt; &gt; &gt; From: gus@ldeo.columbia.edu<br>&gt; &gt;  &gt; &gt; &gt; To: users@open-mpi.org<br>&gt; &gt;  &gt; &gt; &gt; Subject: Re: [OMPI users] mpirun is using one PBS node only<br>&gt; &gt;  &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; Hi Belaid Moa<br>&gt; &gt;  &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; Belaid MOA wrote:<br>&gt; &gt;  &gt; &gt; &gt; &gt; Hi everyone,<br>&gt; &gt;  &gt; &gt; &gt; &gt; Here is another elementary question. I tried the following <br>&gt; &gt; steps found<br>&gt; &gt;  &gt; &gt; &gt; &gt; in the FAQ section of www.open-mpi.org with a simple hello world<br>&gt; &gt;  &gt; &gt; example<br>&gt; &gt;  &gt; &gt; &gt; &gt; (with PBS/torque):<br>&gt; &gt;  &gt; &gt; &gt; &gt; $ qsub -l nodes=2 my_script.sh<br>&gt; &gt;  &gt; &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; &gt; my_script.sh is pasted below:<br>&gt; &gt;  &gt; &gt; &gt; &gt; ========================<br>&gt; &gt;  &gt; &gt; &gt; &gt; #!/bin/sh -l<br>&gt; &gt;  &gt; &gt; &gt; &gt; #PBS -N helloTest<br>&gt; &gt;  &gt; &gt; &gt; &gt; #PBS -j eo<br>&gt; &gt;  &gt; &gt; &gt; &gt; echo `cat $PBS_NODEFILE` # shows two nodes: WN1 WN2<br>&gt; &gt;  &gt; &gt; &gt; &gt; cd $PBS_O_WORKDIR<br>&gt; &gt;  &gt; &gt; &gt; &gt; /usr/local/bin/mpirun hello<br>&gt; &gt;  &gt; &gt; &gt; &gt; ========================<br>&gt; &gt;  &gt; &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; &gt; When the job is submitted, only one process is ran. When I add the<br>&gt; &gt;  &gt; &gt; -n 2<br>&gt; &gt;  &gt; &gt; &gt; &gt; option to the mpirun command,<br>&gt; &gt;  &gt; &gt; &gt; &gt; two processes are ran but on one node only.<br>&gt; &gt;  &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; Do you have a single CPU/core per node?<br>&gt; &gt;  &gt; &gt; &gt; Or are they multi-socket/multi-core?<br>&gt; &gt;  &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; Check "man mpiexec" for the options that control on which nodes and<br>&gt; &gt;  &gt; &gt; &gt; slots, etc your program will run.<br>&gt; &gt;  &gt; &gt; &gt; ("Man mpiexec" will tell you more than I possibly can.)<br>&gt; &gt;  &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; The default option is "-byslot",<br>&gt; &gt;  &gt; &gt; &gt; which will use all "slots" (actually cores<br>&gt; &gt;  &gt; &gt; &gt; or CPUs) available on a node before it moves to the next node.<br>&gt; &gt;  &gt; &gt; &gt; Reading your question and your surprise with the result,<br>&gt; &gt;  &gt; &gt; &gt; I would guess what you want is "-bynode" (not the default).<br>&gt; &gt;  &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; Also, if you have more than one CPU/core per node,<br>&gt; &gt;  &gt; &gt; &gt; you need to put this information in your Torque/PBS "nodes" file<br>&gt; &gt;  &gt; &gt; &gt; (and restart your pbs_server daemon).<br>&gt; &gt;  &gt; &gt; &gt; Something like this (for 2 CPUs/cores per node):<br>&gt; &gt;  &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; WN1 np=2<br>&gt; &gt;  &gt; &gt; &gt; WN2 np=2<br>&gt; &gt;  &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; I hope this helps,<br>&gt; &gt;  &gt; &gt; &gt; Gus Correa<br>&gt; &gt;  &gt; &gt; &gt; ---------------------------------------------------------------------<br>&gt; &gt;  &gt; &gt; &gt; Gustavo Correa<br>&gt; &gt;  &gt; &gt; &gt; Lamont-Doherty Earth Observatory - Columbia University<br>&gt; &gt;  &gt; &gt; &gt; Palisades, NY, 10964-8000 - USA<br>&gt; &gt;  &gt; &gt; &gt; ---------------------------------------------------------------------<br>&gt; &gt;  &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; &gt; Note that echo `cat<br>&gt; &gt;  &gt; &gt; &gt; &gt; $PBS_NODEFILE` outputs<br>&gt; &gt;  &gt; &gt; &gt; &gt; the two nodes I am using: WN1 and WN2.<br>&gt; &gt;  &gt; &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; &gt; The output from ompi_info is shown below:<br>&gt; &gt;  &gt; &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; &gt; $ ompi_info | grep tm<br>&gt; &gt;  &gt; &gt; &gt; &gt; MCA memory: ptmalloc2 (MCA v2.0, API v2.0, Component v1.3.3)<br>&gt; &gt;  &gt; &gt; &gt; &gt; MCA ras: tm (MCA v2.0, API v2.0, Component v1.3.3)<br>&gt; &gt;  &gt; &gt; &gt; &gt; MCA plm: tm (MCA v2.0, API v2.0, Component v1.3.3)<br>&gt; &gt;  &gt; &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; &gt; Any help on why openMPI/mpirun is using only one PBS node is very<br>&gt; &gt;  &gt; &gt; &gt; &gt; appreciated.<br>&gt; &gt;  &gt; &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; &gt; Thanks a lot in advance and sorry for bothering you guys with my<br>&gt; &gt;  &gt; &gt; &gt; &gt; elementary questions!<br>&gt; &gt;  &gt; &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; &gt; ~Belaid.<br>&gt; &gt;  &gt; &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; <br>&gt; &gt; ------------------------------------------------------------------------<br>&gt; &gt;  &gt; &gt; &gt; &gt; Windows Live: Keep your friends up to date with what you do online.<br>&gt; &gt;  &gt; &gt; &gt; &gt; &lt;http://go.microsoft.com/?linkid=9691810&gt;<br>&gt; &gt;  &gt; &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; <br>&gt; &gt; ------------------------------------------------------------------------<br>&gt; &gt;  &gt; &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; &gt; _______________________________________________<br>&gt; &gt;  &gt; &gt; &gt; &gt; users mailing list<br>&gt; &gt;  &gt; &gt; &gt; &gt; users@open-mpi.org<br>&gt; &gt;  &gt; &gt; &gt; &gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;  &gt; &gt; &gt;<br>&gt; &gt;  &gt; &gt; &gt; _______________________________________________<br>&gt; &gt;  &gt; &gt; &gt; users mailing list<br>&gt; &gt;  &gt; &gt; &gt; users@open-mpi.org<br>&gt; &gt;  &gt; &gt; &gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;  &gt; &gt;<br>&gt; &gt;  &gt; &gt; <br>&gt; &gt; ------------------------------------------------------------------------<br>&gt; &gt;  &gt; &gt; Windows Live: Keep your friends up to date with what you do online.<br>&gt; &gt;  &gt; &gt; &lt;http://go.microsoft.com/?linkid=9691810&gt;<br>&gt; &gt;  &gt; &gt;<br>&gt; &gt;  &gt; &gt;<br>&gt; &gt;  &gt; &gt; <br>&gt; &gt; ------------------------------------------------------------------------<br>&gt; &gt;  &gt; &gt;<br>&gt; &gt;  &gt; &gt; _______________________________________________<br>&gt; &gt;  &gt; &gt; users mailing list<br>&gt; &gt;  &gt; &gt; users@open-mpi.org<br>&gt; &gt;  &gt; &gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;  &gt;<br>&gt; &gt;  &gt; _______________________________________________<br>&gt; &gt;  &gt; users mailing list<br>&gt; &gt;  &gt; users@open-mpi.org<br>&gt; &gt;  &gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt; <br>&gt; &gt; ------------------------------------------------------------------------<br>&gt; &gt; Get a great deal on Windows 7 and see how it works the way you want. See <br>&gt; &gt; the Windows 7 offers now. &lt;http://go.microsoft.com/?linkid=9691813&gt;<br>&gt; &gt; <br>&gt; &gt; <br>&gt; &gt; ------------------------------------------------------------------------<br>&gt; &gt; <br>&gt; &gt; _______________________________________________<br>&gt; &gt; users mailing list<br>&gt; &gt; users@open-mpi.org<br>&gt; &gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; <br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; users@open-mpi.org<br>&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br> 		 	   		  <br /><hr />Windows Live: Keep your friends up to date <a href='http://go.microsoft.com/?linkid=9691810' target='_new'>with what you do online.</a></body>
</html>
