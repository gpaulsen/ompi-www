<html><head><meta http-equiv="Content-Type" content="text/html charset=iso-8859-1"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">What is "mpiformatdb"? We don't have an MPI database in our system, and I have no idea what that command means<div><br></div><div>As for that error - it means that the identifier we exchange between processes is failing to be recognized. This could mean a couple of things:</div><div><br></div><div>1. the OMPI version on the two ends is different - could be you aren't getting the right paths set on the various machines</div><div><br></div><div>2. the cluster is heterogeneous</div><div><br></div><div>You say you have "virtual nodes" running on various PC's? That would be an unusual setup - VM's can be problematic given the way they handle TCP connections, so that might be another source of the problem if my understanding of your setup is correct. Have you tried running this across the PCs directly - i.e., without any VMs?</div><div><br></div><div><br><div><div>On Apr 3, 2014, at 10:13 AM, Nisha Dhankher -M.Tech(CSE) &lt;<a href="mailto:nishadhankher-coaeseeit@pau.edu">nishadhankher-coaeseeit@pau.edu</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr"><div>i first formatted my database with mpiformatdb command then i ran command :<br>mpirun -np 64 -machinefile mf mpiblast -d all.fas -p blastn -i query.fas -o output.txt<br></div>but
 then it gave this error 113 from some hosts and continue to run for 
other but with no&nbsp; results even after 2 hours lapsed.....on rocks 6.0 
cluster with 12 virtual nodes on pc's ...2 on each using virt-manger , 1
 gb ram to each</div><div class="gmail_extra"><br><br><div class="gmail_quote">On Thu, Apr 3, 2014 at 10:41 PM, Nisha Dhankher -M.Tech(CSE) <span dir="ltr">&lt;<a href="mailto:nishadhankher-coaeseeit@pau.edu" target="_blank">nishadhankher-coaeseeit@pau.edu</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><div>i also made machine file which contain ip adresses of all compute nodes + .ncbirc file for path to mpiblast and shared ,local storage path....<br>
</div>Sir<br></div>I ran the same command of mpirun on my college supercomputer 8 nodes each having 24 processors but it just running....gave no result uptill 3 hours...<br>
</div><div class="HOEnZb"><div class="h5"><div class="gmail_extra"><br><br><div class="gmail_quote">On Thu, Apr 3, 2014 at 10:39 PM, Nisha Dhankher -M.Tech(CSE) <span dir="ltr">&lt;<a href="mailto:nishadhankher-coaeseeit@pau.edu" target="_blank">nishadhankher-coaeseeit@pau.edu</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div>i first formatted my database with mpiformatdb command then i ran command :<br>mpirun -np 64 -machinefile mf mpiblast -d all.fas -p blastn -i query.fas -o output.txt<br>

</div>but then it gave this error 113 from some hosts and continue to run for other but with results even after 2 hours lapsed.....on rocks 6.0 cluster with 12 virtual nodes on pc's ...2 on each using virt-manger , 1 gb ram to each<br>


&nbsp;</div><div class="gmail_extra"><br><br><div class="gmail_quote"><div>On Thu, Apr 3, 2014 at 8:37 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>


</div><div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">I'm having trouble understanding your note, so perhaps I am getting this wrong. Let's see if I can figure out what you said:<div>


<br></div><div>* your perl command fails with "no route to host" - but I don't see any host in your cmd. Maybe I'm just missing something.</div><div><br></div><div>* you tried running a couple of "mpirun", but the mpirun command wasn't recognized? Is that correct?</div>


<div><br></div><div>* you then ran mpiblast and it sounds like it successfully started the processes, but then one aborted? Was there an error message beyond just the -1 return status?</div><div><br></div><div><br></div>

<div>
<div><div><div>On Apr 2, 2014, at 11:17 PM, Nisha Dhankher -M.Tech(CSE) &lt;<a href="mailto:nishadhankher-coaeseeit@pau.edu" target="_blank">nishadhankher-coaeseeit@pau.edu</a>&gt; wrote:</div><br></div><blockquote type="cite">


<div><div dir="ltr">
        
            
<div>
    <h1><a href="http://biosupport.se/questions/696/error-btl_tcp_endpintc-638-connection-failed-due-to-error-113" target="_blank">error btl_tcp_endpint.c: 638 connection failed due to error 113</a></h1><p>In openmpi: this error came when i run my mpiblast program on rocks 
cluster.Connect to hosts failed on ip 10.1.255.236,10.1.255.244 .
And when i run following command
linux_shell$ perl -e 'die$!=113'
this msg comes:  "No route to host at -e line 1."
shell$ mpirun --mca btl ^tcp
shell$ mpirun --mca btl_tcp_if_include eth1,eth2 
shell$ mpirun --mca btl_tcp_if_include 10.1.255.244
was also executed but it did nt recognized these commands....nd 
aborted....
what should i do...?
When i run my mpiblast program for the frst time then it give mpi_abort 
error...bailing out of signal -1 on rank 2 processor...then i removed my
 public ethernet cable....and then give btl_tcp endpint error 113....</p>
</div></div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>


</div><br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div></div><br></div>
</blockquote></div><br></div>
</div></div></blockquote></div><br></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
