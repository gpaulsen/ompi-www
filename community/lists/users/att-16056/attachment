Hi Jeff,<div>I thank you for your help,</div><div>I&#39;ve launched my app with mpiP both when two processes are on different node and when two processes are on the same node.</div><div><br></div><div>The process 0 is the manager (gathers the results only), processes 1 and 2 are  workers (compute).</div>
<div><br></div><div>This is the case processes 1 and 2 are on different nodes (runs in 162s).</div><div><br></div><div><div>---------------------------------------------------------------------------</div><div>@--- MPI Time (seconds) ---------------------------------------------------</div>
<div>---------------------------------------------------------------------------</div><div>Task    AppTime    MPITime     MPI%</div><div>   0        162        162    99.99</div><div>   1        162       30.2    18.66</div>
<div>   2        162       14.7     9.04</div><div>   *        486        207    42.56</div><div><div>---------------------------------------------------------------------------</div><div>@--- Aggregate Time (top twenty, descending, milliseconds) ----------------</div>
<div>---------------------------------------------------------------------------</div><div>Call                 Site       Time    App%    MPI%     COV</div><div>Barrier                 5   1.28e+05   26.24   61.64    0.00</div>
<div>Barrier                14    2.3e+04    4.74   11.13    0.00</div><div>Barrier                 6   2.29e+04    4.72   11.08    0.00</div><div>Barrier                17   1.77e+04    3.65    8.58    1.41</div><div>Recv                    3   1.15e+04    2.37    5.58    0.00</div>
<div>Recv                   30   2.26e+03    0.47    1.09    0.00</div><div>Recv                   12        308    0.06    0.15    0.00</div><div>Recv                   26        286    0.06    0.14    0.00</div><div>Recv                   28        252    0.05    0.12    0.00</div>
<div>Recv                   31        246    0.05    0.12    0.00</div><div>Isend                  35        111    0.02    0.05    0.00</div><div>Isend                  34        108    0.02    0.05    0.00</div><div>Isend                  18        107    0.02    0.05    0.00</div>
<div>Isend                  19        105    0.02    0.05    0.00</div><div>Isend                   9       57.7    0.01    0.03    0.05</div><div>Isend                  32       39.7    0.01    0.02    0.00</div><div>Barrier                25       38.7    0.01    0.02    1.39</div>
<div>Isend                  11       38.6    0.01    0.02    0.00</div><div>Recv                   16       34.1    0.01    0.02    0.00</div><div>Recv                   27       26.5    0.01    0.01    0.00</div><div>---------------------------------------------------------------------------</div>
<div>@--- Aggregate Sent Message Size (top twenty, descending, bytes) ----------</div><div>---------------------------------------------------------------------------</div><div>Call                 Site      Count      Total       Avrg  Sent%</div>
<div>Isend                   9       4096   1.34e+08   3.28e+04  58.73</div><div>Isend                  34       1200   1.85e+07   1.54e+04   8.07</div><div>Isend                  35       1200   1.85e+07   1.54e+04   8.07</div>
<div>Isend                  18       1200   1.85e+07   1.54e+04   8.07</div><div>Isend                  19       1200   1.85e+07   1.54e+04   8.07</div><div>Isend                  32        240   3.69e+06   1.54e+04   1.61</div>
<div>Isend                  11        240   3.69e+06   1.54e+04   1.61</div><div>Isend                  15        180   3.44e+06   1.91e+04   1.51</div><div>Isend                  33         61      2e+06   3.28e+04   0.87</div>
<div>Isend                  10         61      2e+06   3.28e+04   0.87</div><div>Isend                  29         61      2e+06   3.28e+04   0.87</div><div>Isend                  22         61      2e+06   3.28e+04   0.87</div>
<div>Isend                  37        180   1.72e+06   9.57e+03   0.75</div><div>Isend                  24          2         16          8   0.00</div><div>Isend                  20          2         16          8   0.00</div>
<div>Send                    8          1          4          4   0.00</div><div>Send                    1          1          4          4   0.00</div></div><div><br></div><div>The case when processes 1 and 2 are on the same node (runs in 260s).</div>
<div><div>---------------------------------------------------------------------------</div><div>@--- MPI Time (seconds) ---------------------------------------------------</div><div>---------------------------------------------------------------------------</div>
<div>Task    AppTime    MPITime     MPI%</div><div>   0        260        260    99.99</div><div>   1        260       39.7    15.29</div><div>   2        260       26.4    10.17</div><div>   *        779        326    41.82</div>
</div><div><br></div><div><div>---------------------------------------------------------------------------</div><div>@--- Aggregate Time (top twenty, descending, milliseconds) ----------------</div><div>---------------------------------------------------------------------------</div>
<div>Call                 Site       Time    App%    MPI%     COV</div><div>Barrier                 5   2.23e+05   28.64   68.50    0.00</div><div>Barrier                 6   2.49e+04    3.20    7.66    0.00</div><div>Barrier                14   2.31e+04    2.96    7.09    0.00</div>
<div>Recv                   28   1.35e+04    1.73    4.14    0.00</div><div>Recv                   16   1.32e+04    1.70    4.06    0.00</div><div>Barrier                17   1.22e+04    1.56    3.74    1.41</div><div>Recv                    3   1.16e+04    1.48    3.55    0.00</div>
<div>Recv                   26   1.67e+03    0.21    0.51    0.00</div><div>Recv                   30        940    0.12    0.29    0.00</div><div>Recv                   12        674    0.09    0.21    0.00</div><div>Recv                   21        288    0.04    0.09    0.00</div>
<div>Recv                   31        259    0.03    0.08    0.00</div><div>Isend                   9       62.1    0.01    0.02    0.04</div><div>Recv                   27       39.5    0.01    0.01    0.00</div><div>Isend                  35       31.2    0.00    0.01    0.00</div>
<div>Isend                  19         31    0.00    0.01    0.00</div><div>Isend                  34         30    0.00    0.01    0.00</div><div>Isend                  18       29.4    0.00    0.01    0.00</div><div>Isend                  32       14.6    0.00    0.00    0.00</div>
<div>Isend                  11       14.4    0.00    0.00    0.00</div><div>---------------------------------------------------------------------------</div><div>@--- Aggregate Sent Message Size (top twenty, descending, bytes) ----------</div>
<div>---------------------------------------------------------------------------</div><div>Call                 Site      Count      Total       Avrg  Sent%</div><div>Isend                   9       4096   1.34e+08   3.28e+04  58.73</div>
<div>Isend                  34       1200   1.85e+07   1.54e+04   8.07</div><div>Isend                  35       1200   1.85e+07   1.54e+04   8.07</div><div>Isend                  18       1200   1.85e+07   1.54e+04   8.07</div>
<div>Isend                  19       1200   1.85e+07   1.54e+04   8.07</div><div>Isend                  32        240   3.69e+06   1.54e+04   1.61</div><div>Isend                  11        240   3.69e+06   1.54e+04   1.61</div>
<div>Isend                  15        180   3.44e+06   1.91e+04   1.51</div><div>Isend                  33         61      2e+06   3.28e+04   0.87</div><div>Isend                  10         61      2e+06   3.28e+04   0.87</div>
<div>Isend                  29         61      2e+06   3.28e+04   0.87</div><div>Isend                  22         61      2e+06   3.28e+04   0.87</div><div>Isend                  37        180   1.72e+06   9.57e+03   0.75</div>
<div>Isend                  24          2         16          8   0.00</div><div>Isend                  20          2         16          8   0.00</div><div>Send                    8          1          4          4   0.00</div>
<div>Send                    1          1          4          4   0.00</div></div><div><br></div><div>I think there&#39;s a contention problem on the memory bus. If the Shared Memory works correctly. </div><div>However, the message size is 4096 * sizeof(double). Maybe I are wrong in this point. Is the message size too huge for shared memory?</div>
<br><div class="gmail_quote">2011/3/30 Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span><br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
How many messages are you sending, and how large are they?  I.e., if your message passing is tiny, then the network transport may not be the bottleneck here.<br>
<div><div></div><div class="h5"><br>
<br>
On Mar 28, 2011, at 9:41 AM, Michele Marena wrote:<br>
<br>
&gt; I run ompi_info --param btl sm and this is the output<br>
&gt;<br>
&gt;                  MCA btl: parameter &quot;btl_base_debug&quot; (current value: &quot;0&quot;)<br>
&gt;                           If btl_base_debug is 1 standard debug is output, if &gt; 1 verbose debug is output<br>
&gt;                  MCA btl: parameter &quot;btl&quot; (current value: &lt;none&gt;)<br>
&gt;                           Default selection set of components for the btl framework (&lt;none&gt; means &quot;use all components that can be found&quot;)<br>
&gt;                  MCA btl: parameter &quot;btl_base_verbose&quot; (current value: &quot;0&quot;)<br>
&gt;                           Verbosity level for the btl framework (0 = no verbosity)<br>
&gt;                  MCA btl: parameter &quot;btl_sm_free_list_num&quot; (current value: &quot;8&quot;)<br>
&gt;                  MCA btl: parameter &quot;btl_sm_free_list_max&quot; (current value: &quot;-1&quot;)<br>
&gt;                  MCA btl: parameter &quot;btl_sm_free_list_inc&quot; (current value: &quot;64&quot;)<br>
&gt;                  MCA btl: parameter &quot;btl_sm_exclusivity&quot; (current value: &quot;65535&quot;)<br>
&gt;                  MCA btl: parameter &quot;btl_sm_latency&quot; (current value: &quot;100&quot;)<br>
&gt;                  MCA btl: parameter &quot;btl_sm_max_procs&quot; (current value: &quot;-1&quot;)<br>
&gt;                  MCA btl: parameter &quot;btl_sm_sm_extra_procs&quot; (current value: &quot;2&quot;)<br>
&gt;                  MCA btl: parameter &quot;btl_sm_mpool&quot; (current value: &quot;sm&quot;)<br>
&gt;                  MCA btl: parameter &quot;btl_sm_eager_limit&quot; (current value: &quot;4096&quot;)<br>
&gt;                  MCA btl: parameter &quot;btl_sm_max_frag_size&quot; (current value: &quot;32768&quot;)<br>
&gt;                  MCA btl: parameter &quot;btl_sm_size_of_cb_queue&quot; (current value: &quot;128&quot;)<br>
&gt;                  MCA btl: parameter &quot;btl_sm_cb_lazy_free_freq&quot; (current value: &quot;120&quot;)<br>
&gt;                  MCA btl: parameter &quot;btl_sm_priority&quot; (current value: &quot;0&quot;)<br>
&gt;                  MCA btl: parameter &quot;btl_base_warn_component_unused&quot; (current value: &quot;1&quot;)<br>
&gt;                           This parameter is used to turn on warning messages when certain NICs are not used<br>
&gt;<br>
&gt;<br>
&gt; 2011/3/28 Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;<br>
&gt; The fact that this exactly matches the time you measured with shared memory is suspicious. My guess is that you aren&#39;t actually using shared memory at all.<br>
&gt;<br>
&gt; Does your &quot;ompi_info&quot; output show shared memory as being available? Jeff or others may be able to give you some params that would let you check to see if sm is actually being used between those procs.<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; On Mar 28, 2011, at 7:51 AM, Michele Marena wrote:<br>
&gt;<br>
&gt;&gt; What happens with 2 processes on the same node with tcp?<br>
&gt;&gt; With --mca btl self,tcp my app runs in 23s.<br>
&gt;&gt;<br>
&gt;&gt; 2011/3/28 Jeff Squyres (jsquyres) &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>
&gt;&gt; Ah, I didn&#39;t catch before that there were more variables than just tcp vs. shmem.<br>
&gt;&gt;<br>
&gt;&gt; What happens with 2 processes on the same node with tcp?<br>
&gt;&gt;<br>
&gt;&gt; Eg, when both procs are on the same node, are you thrashing caches or memory?<br>
&gt;&gt;<br>
&gt;&gt; Sent from my phone. No type good.<br>
&gt;&gt;<br>
&gt;&gt; On Mar 28, 2011, at 6:27 AM, &quot;Michele Marena&quot; &lt;<a href="mailto:michelemarena@gmail.com">michelemarena@gmail.com</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt;&gt; However, I thank you Tim, Ralh and Jeff.<br>
&gt;&gt;&gt; My sequential application runs in 24s (wall clock time).<br>
&gt;&gt;&gt; My parallel application runs in 13s with two processes on different nodes.<br>
&gt;&gt;&gt; With shared memory, when two processes are on the same node, my app runs in 23s.<br>
&gt;&gt;&gt; I&#39;m not understand why.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; 2011/3/28 Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>
&gt;&gt;&gt; If your program runs faster across 3 processes, 2 of which are local to each other, with --mca btl tcp,self compared to --mca btl tcp,sm,self, then something is very, very strange.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Tim cites all kinds of things that can cause slowdowns, but it&#39;s still very, very odd that simply enabling using the shared memory communications channel in Open MPI *slows your overall application down*.<br>

&gt;&gt;&gt;<br>
&gt;&gt;&gt; How much does your application slow down in wall clock time?  Seconds?  Minutes?  Hours?  (anything less than 1 second is in the noise)<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Mar 27, 2011, at 10:33 AM, Ralph Castain wrote:<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; &gt;<br>
&gt;&gt;&gt; &gt; On Mar 27, 2011, at 7:37 AM, Tim Prince wrote:<br>
&gt;&gt;&gt; &gt;<br>
&gt;&gt;&gt; &gt;&gt; On 3/27/2011 2:26 AM, Michele Marena wrote:<br>
&gt;&gt;&gt; &gt;&gt;&gt; Hi,<br>
&gt;&gt;&gt; &gt;&gt;&gt; My application performs good without shared memory utilization, but with<br>
&gt;&gt;&gt; &gt;&gt;&gt; shared memory I get performance worst than without of it.<br>
&gt;&gt;&gt; &gt;&gt;&gt; Do I make a mistake? Don&#39;t I pay attention to something?<br>
&gt;&gt;&gt; &gt;&gt;&gt; I know OpenMPI uses /tmp directory to allocate shared memory and it is<br>
&gt;&gt;&gt; &gt;&gt;&gt; in the local filesystem.<br>
&gt;&gt;&gt; &gt;&gt;&gt;<br>
&gt;&gt;&gt; &gt;&gt;<br>
&gt;&gt;&gt; &gt;&gt; I guess you mean shared memory message passing.   Among relevant parameters may be the message size where your implementation switches from cached copy to non-temporal (if you are on a platform where that terminology is used).  If built with Intel compilers, for example, the copy may be performed by intel_fast_memcpy, with a default setting which uses non-temporal when the message exceeds about some preset size, e.g. 50% of smallest L2 cache for that architecture.<br>

&gt;&gt;&gt; &gt;&gt; A quick search for past posts seems to indicate that OpenMPI doesn&#39;t itself invoke non-temporal, but there appear to be several useful articles not connected with OpenMPI.<br>
&gt;&gt;&gt; &gt;&gt; In case guesses aren&#39;t sufficient, it&#39;s often necessary to profile (gprof, oprofile, Vtune, ....) to pin this down.<br>
&gt;&gt;&gt; &gt;&gt; If shared message slows your application down, the question is whether this is due to excessive eviction of data from cache; not a simple question, as most recent CPUs have 3 levels of cache, and your application may require more or less data which was in use prior to the message receipt, and may use immediately only a small piece of a large message.<br>

&gt;&gt;&gt; &gt;<br>
&gt;&gt;&gt; &gt; There were several papers published in earlier years about shared memory performance in the 1.2 series.There were known problems with that implementation, which is why it was heavily revised for the 1.3/4 series.<br>

&gt;&gt;&gt; &gt;<br>
&gt;&gt;&gt; &gt; You might also look at the following links, though much of it has been updated to the 1.3/4 series as we don&#39;t really support 1.2 any more:<br>
&gt;&gt;&gt; &gt;<br>
&gt;&gt;&gt; &gt; <a href="http://www.open-mpi.org/faq/?category=sm" target="_blank">http://www.open-mpi.org/faq/?category=sm</a><br>
&gt;&gt;&gt; &gt;<br>
&gt;&gt;&gt; &gt; <a href="http://www.open-mpi.org/faq/?category=perftools" target="_blank">http://www.open-mpi.org/faq/?category=perftools</a><br>
&gt;&gt;&gt; &gt;<br>
&gt;&gt;&gt; &gt;<br>
&gt;&gt;&gt; &gt;&gt;<br>
&gt;&gt;&gt; &gt;&gt; --<br>
&gt;&gt;&gt; &gt;&gt; Tim Prince<br>
&gt;&gt;&gt; &gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; &gt;&gt; users mailing list<br>
&gt;&gt;&gt; &gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; &gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt; &gt;<br>
&gt;&gt;&gt; &gt;<br>
&gt;&gt;&gt; &gt; _______________________________________________<br>
&gt;&gt;&gt; &gt; users mailing list<br>
&gt;&gt;&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; --<br>
&gt;&gt;&gt; Jeff Squyres<br>
&gt;&gt;&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt;&gt;&gt; For corporate legal information go to:<br>
&gt;&gt;&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to:<br>
<a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div>

