<html><head><meta http-equiv="Content-Type" content="text/html charset=windows-1252"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">Thanks!<div><br><div><div>On Jun 17, 2015, at 3:08 PM, Rolf vandeVaart &lt;<a href="mailto:rvandevaart@nvidia.com">rvandevaart@nvidia.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div lang="EN-US" link="blue" vlink="purple" style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><div class="WordSection1" style="page: WordSection1;"><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><span style="font-size: 11pt; font-family: Calibri, sans-serif; color: rgb(31, 73, 125);">There is no short-term plan but we are always looking at ways to improve things so this could be looked at some time in the future.<o:p></o:p></span></div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><span style="font-size: 11pt; font-family: Calibri, sans-serif; color: rgb(31, 73, 125);">&nbsp;</span></div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><span style="font-size: 11pt; font-family: Calibri, sans-serif; color: rgb(31, 73, 125);">Rolf<o:p></o:p></span></div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><span style="font-size: 11pt; font-family: Calibri, sans-serif; color: rgb(31, 73, 125);">&nbsp;</span></div><div style="border-style: none none none solid; border-left-color: blue; border-left-width: 1.5pt; padding: 0in 0in 0in 4pt;"><div><div style="border-style: solid none none; border-top-color: rgb(181, 196, 223); border-top-width: 1pt; padding: 3pt 0in 0in;"><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><b><span style="font-size: 10pt; font-family: Tahoma, sans-serif;">From:</span></b><span style="font-size: 10pt; font-family: Tahoma, sans-serif;"><span class="Apple-converted-space">&nbsp;</span>users [<a href="mailto:users-bounces@open-mpi.org">mailto:users-bounces@open-mpi.org</a>]<span class="Apple-converted-space">&nbsp;</span><b>On Behalf Of<span class="Apple-converted-space">&nbsp;</span></b>Fei Mao<br><b>Sent:</b><span class="Apple-converted-space">&nbsp;</span>Wednesday, June 17, 2015 1:48 PM<br><b>To:</b><span class="Apple-converted-space">&nbsp;</span>Open MPI Users<br><b>Subject:</b><span class="Apple-converted-space">&nbsp;</span>Re: [OMPI users] CUDA-aware MPI_Reduce problem in Openmpi 1.8.5<o:p></o:p></span></div></div></div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><o:p>&nbsp;</o:p></div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;">Hi Rolf,<o:p></o:p></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><o:p>&nbsp;</o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;">Thank you very much for clarifying the problem. Is there any plan to support GPU RDMA for reduction in the future?<o:p></o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><o:p>&nbsp;</o:p></div><div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;">On Jun 17, 2015, at 1:38 PM, Rolf vandeVaart &lt;<a href="mailto:rvandevaart@nvidia.com" style="color: purple; text-decoration: underline;">rvandevaart@nvidia.com</a>&gt; wrote:<o:p></o:p></div></div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><br><br><o:p></o:p></div><div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><span style="font-size: 11pt; font-family: Calibri, sans-serif; color: rgb(31, 73, 125);">Hi Fei:</span><o:p></o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><span style="font-size: 11pt; font-family: Calibri, sans-serif; color: rgb(31, 73, 125);">&nbsp;</span><o:p></o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><span style="font-size: 11pt; font-family: Calibri, sans-serif; color: rgb(31, 73, 125);">The reduction support for CUDA-aware in Open MPI is rather simple.&nbsp; The GPU buffers are copied into temporary host buffers and then the reduction is done with the host buffers.&nbsp; At the completion of the host reduction, the data is copied back into the GPU buffers.&nbsp; So, there is no use of CUDA IPC or GPU Direct RDMA in the reduction.</span><o:p></o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><span style="font-size: 11pt; font-family: Calibri, sans-serif; color: rgb(31, 73, 125);">&nbsp;</span><o:p></o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><span style="font-size: 11pt; font-family: Calibri, sans-serif; color: rgb(31, 73, 125);">Rolf</span><o:p></o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><span style="font-size: 11pt; font-family: Calibri, sans-serif; color: rgb(31, 73, 125);">&nbsp;</span><o:p></o:p></div></div><div style="border-style: none none none solid; border-left-color: blue; border-left-width: 1.5pt; padding: 0in 0in 0in 4pt;"><div><div style="border-style: solid none none; border-top-color: rgb(181, 196, 223); border-top-width: 1pt; padding: 3pt 0in 0in;"><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><b><span style="font-size: 10pt; font-family: Tahoma, sans-serif;">From:</span></b><span class="apple-converted-space"><span style="font-size: 10pt; font-family: Tahoma, sans-serif;">&nbsp;</span></span><span style="font-size: 10pt; font-family: Tahoma, sans-serif;">users [<a href="mailto:users-bounces@open-mpi.org" style="color: purple; text-decoration: underline;">mailto:users-bounces@open-mpi.org</a>]<span class="apple-converted-space">&nbsp;</span><b>On Behalf Of<span class="apple-converted-space">&nbsp;</span></b>Fei Mao<br><b>Sent:</b><span class="apple-converted-space">&nbsp;</span>Wednesday, June 17, 2015 1:08 PM<br><b>To:</b><span class="apple-converted-space">&nbsp;</span><a href="mailto:users@open-mpi.org" style="color: purple; text-decoration: underline;">users@open-mpi.org</a><br><b>Subject:</b><span class="apple-converted-space">&nbsp;</span>[OMPI users] CUDA-aware MPI_Reduce problem in Openmpi 1.8.5</span><o:p></o:p></div></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;">&nbsp;<o:p></o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;">Hi there,<o:p></o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;">&nbsp;<o:p></o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;">I am doing benchmarks on a GPU cluster with two CPU sockets and 4 K80 GPUs each node. Two K80 are connected with CPU socket 0, another two with socket 1. An IB ConnectX-3 (FDR) is also under socket 1. We are using Linux’s OFED, so I know there is no way to do GPU RDMA inter-node communication. I can do intra-node IPC for MPI_Send and MPI_Receive with two K80 (4 GPUs in total) which are connected under same socket (PCI-e switch). So I thought I could do intra-node MPI_Reduce with IPC support in openmpi 1.8.5.<o:p></o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;">&nbsp;<o:p></o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;">The benchmark I was using is osu-micro-benchmarks-4.4.1, and I got the same results when I use two GPU under the same socket or different socket. The result was the same even I used two GPUs in different nodes.&nbsp;<o:p></o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;">&nbsp;<o:p></o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;">Does MPI_Reduce use IPC for intra-node? Should I have to install Mellanox OFED stack to support GPU RDMA reduction on GPUs even they are under with the same PCI-e switch?<o:p></o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;">&nbsp;<o:p></o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;">Thanks,<o:p></o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;">&nbsp;<o:p></o:p></div></div><div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;">Fei Mao<br>High Performance Computing Technical Consultant&nbsp;<o:p></o:p></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;">SHARCNET |&nbsp;<a href="http://www.sharcnet.ca/" style="color: purple; text-decoration: underline;"><span style="color: purple;">http://www.sharcnet.ca</span></a><br>Compute/Calcul Canada |&nbsp;<a href="http://www.computecanada.ca/" style="color: purple; text-decoration: underline;"><span style="color: purple;">http://www.computecanada.ca</span></a><o:p></o:p></div></div></div></div><div><div class="MsoNormal" align="center" style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif; text-align: center;"><span style="font-size: 9pt; font-family: Helvetica, sans-serif;"><hr size="2" width="100%" align="center"></span></div></div><div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><span style="font-size: 9pt; font-family: Helvetica, sans-serif;">This email message is for the sole use of the intended recipient(s) and may contain confidential information.&nbsp; Any unauthorized review, use, disclosure or distribution is prohibited.&nbsp; If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message.<o:p></o:p></span></div></div><div><div class="MsoNormal" align="center" style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif; text-align: center;"><span style="font-size: 9pt; font-family: Helvetica, sans-serif;"><hr size="2" width="100%" align="center"></span></div></div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><span style="font-size: 9pt; font-family: Helvetica, sans-serif;">_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" style="color: purple; text-decoration: underline;"><span style="color: purple;">users@open-mpi.org</span></a><br>Subscription:<span class="apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" style="color: purple; text-decoration: underline;"><span style="color: purple;">http://www.open-mpi.org/mailman/listinfo.cgi/users</span></a><br>Link to this post:<span class="apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2015/06/27147.php" style="color: purple; text-decoration: underline;"><span style="color: purple;">http://www.open-mpi.org/community/lists/users/2015/06/27147.php</span></a><o:p></o:p></span></div></div></div><div style="margin: 0in 0in 0.0001pt; font-size: 12pt; font-family: 'Times New Roman', serif;"><o:p>&nbsp;</o:p></div></div></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" style="color: purple; text-decoration: underline;">users@open-mpi.org</a><br>Subscription:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" style="color: purple; text-decoration: underline;">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2015/06/27151.php" style="color: purple; text-decoration: underline;">http://www.open-mpi.org/community/lists/users/2015/06/27151.php</a></div></blockquote></div><br></div></body></html>
