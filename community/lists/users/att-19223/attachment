Yep you are correct. I did the same and it worked. When I have more than 3 MPI tasks there is lot of overhead on GPU. <div><br></div><div>But for CPU there is not overhead. All three machines have 4 quad core processors with 3.8 GB RAM. </div>
<div><br></div><div>Just wondering why there is no degradation of performance on CPU ?<br><br><div class="gmail_quote">On Tue, May 8, 2012 at 8:21 PM, Rolf vandeVaart <span dir="ltr">&lt;<a href="mailto:rvandevaart@nvidia.com" target="_blank">rvandevaart@nvidia.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div lang="EN-US" link="blue" vlink="purple"><div><p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d">You should be running with one GPU per MPI process.  If I understand correctly, you have a 3 node cluster and each node has a GPU so you should run with np=3.<u></u><u></u></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d">Maybe you can try that and see if your numbers come out better.<u></u><u></u></span></p><p class="MsoNormal">
<span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d"><u></u> <u></u></span></p><p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d"><u></u> <u></u></span></p>
<div style="border:none;border-left:solid blue 1.5pt;padding:0in 0in 0in 4.0pt"><div><div style="border:none;border-top:solid #b5c4df 1.0pt;padding:3.0pt 0in 0in 0in"><p class="MsoNormal"><b><span style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;">From:</span></b><span style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;"> <a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a> [mailto:<a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a>] <b>On Behalf Of </b>Rohan Deshpande<br>
<b>Sent:</b> Monday, May 07, 2012 9:38 PM<br><b>To:</b> Open MPI Users<br><b>Subject:</b> [OMPI users] GPU and CPU timing - OpenMPI and Thrust<u></u><u></u></span></p></div></div><div><div class="h5"><p class="MsoNormal">
<u></u> <u></u></p><p class="MsoNormal"> I am running MPI and Thrust code on a cluster and measuring time for calculations.<u></u><u></u></p><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">My MPI code -   <u></u><u></u></p>
</div><div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">#include &quot;mpi.h&quot;<u></u><u></u></p></div><div><p class="MsoNormal">#include &lt;stdio.h&gt;<u></u><u></u></p></div><div><p class="MsoNormal">
#include &lt;stdlib.h&gt;<u></u><u></u></p></div><div><p class="MsoNormal">#include &lt;string.h&gt;<u></u><u></u></p></div><div><p class="MsoNormal">#include &lt;time.h&gt;<u></u><u></u></p></div><div><p class="MsoNormal">
#include &lt;sys/time.h&gt;<u></u><u></u></p></div><div><p class="MsoNormal">#include &lt;sys/resource.h&gt;<u></u><u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">#define  MASTER 0<u></u><u></u></p>
</div><div><p class="MsoNormal">#define ARRAYSIZE 20000000<u></u><u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">int *masterarray,*onearray,*twoarray,*threearray,*fourarray,*fivearray,*sixarray,*sevenarray,*eightarray,*ninearray;     <u></u><u></u></p>
</div><div><p class="MsoNormal">   int main(int argc, char* argv[])<u></u><u></u></p></div><div><p class="MsoNormal">{<u></u><u></u></p></div><div><p class="MsoNormal">  int   numtasks, taskid,chunksize, namelen; <u></u><u></u></p>
</div><div><p class="MsoNormal">  int mysum,one,two,three,four,five,six,seven,eight,nine;<u></u><u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">  char myname[MPI_MAX_PROCESSOR_NAME];<u></u><u></u></p>
</div><div><p class="MsoNormal">  MPI_Status status;<u></u><u></u></p></div><div><p class="MsoNormal">  int a,b,c,d,e,f,g,h,i,j;<u></u><u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">
/***** Initializations *****/<u></u><u></u></p></div><div><p class="MsoNormal">MPI_Init(&amp;argc, &amp;argv);<u></u><u></u></p></div><div><p class="MsoNormal">MPI_Comm_size(MPI_COMM_WORLD, &amp;numtasks);<u></u><u></u></p>
</div><div><p class="MsoNormal">MPI_Comm_rank(MPI_COMM_WORLD,&amp;taskid); <u></u><u></u></p></div><div><p class="MsoNormal">MPI_Get_processor_name(myname, &amp;namelen);<u></u><u></u></p></div><div><p class="MsoNormal">printf (&quot;MPI task %d has started on host %s...\n&quot;, taskid, myname);<u></u><u></u></p>
</div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">masterarray= malloc(ARRAYSIZE * sizeof(int));<u></u><u></u></p></div><div><p class="MsoNormal">onearray= malloc(ARRAYSIZE * sizeof(int));<u></u><u></u></p>
</div><div><p class="MsoNormal">twoarray= malloc(ARRAYSIZE * sizeof(int));<u></u><u></u></p></div><div><p class="MsoNormal">threearray= malloc(ARRAYSIZE * sizeof(int));<u></u><u></u></p></div><div><p class="MsoNormal">fourarray= malloc(ARRAYSIZE * sizeof(int));<u></u><u></u></p>
</div><div><p class="MsoNormal">fivearray= malloc(ARRAYSIZE * sizeof(int));<u></u><u></u></p></div><div><p class="MsoNormal">sixarray= malloc(ARRAYSIZE * sizeof(int));<u></u><u></u></p></div><div><p class="MsoNormal">sevenarray= malloc(ARRAYSIZE * sizeof(int));<u></u><u></u></p>
</div><div><p class="MsoNormal">eightarray= malloc(ARRAYSIZE * sizeof(int));<u></u><u></u></p></div><div><p class="MsoNormal">ninearray= malloc(ARRAYSIZE * sizeof(int)); <u></u><u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p>
</div><div><p class="MsoNormal">/***** Master task only ******/<u></u><u></u></p></div><div><p class="MsoNormal">if (taskid == MASTER){<u></u><u></u></p></div><div><p class="MsoNormal">           for(a=0; a &lt; ARRAYSIZE; a++){<u></u><u></u></p>
</div><div><p class="MsoNormal">                 masterarray[a] = 1;<u></u><u></u></p></div><div><p class="MsoNormal">                               <u></u><u></u></p></div><div><p class="MsoNormal">            }<u></u><u></u></p>
</div><div><p class="MsoNormal">   mysum = run_kernel0(masterarray,ARRAYSIZE,taskid, myname);<u></u><u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal"> }  /* end of master section */<u></u><u></u></p>
</div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">  if (taskid &gt; MASTER) {<u></u><u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">             if(taskid == 1){<u></u><u></u></p>
</div><div><p class="MsoNormal">                for(b=0;b&lt;ARRAYSIZE;b++){<u></u><u></u></p></div><div><p class="MsoNormal">                onearray[b] = 1;<u></u><u></u></p></div><div><p class="MsoNormal">            }<u></u><u></u></p>
</div><div><p class="MsoNormal">                 one = run_kernel0(onearray,ARRAYSIZE,taskid, myname);<u></u><u></u></p></div><div><p class="MsoNormal">             }<u></u><u></u></p></div><div><p class="MsoNormal">             if(taskid == 2){<u></u><u></u></p>
</div><div><p class="MsoNormal">                for(c=0;c&lt;ARRAYSIZE;c++){<u></u><u></u></p></div><div><p class="MsoNormal">                 twoarray[c] = 1;<u></u><u></u></p></div><div><p class="MsoNormal">            }<u></u><u></u></p>
</div><div><p class="MsoNormal">                 two = run_kernel0(twoarray,ARRAYSIZE,taskid, myname);<u></u><u></u></p></div><div><p class="MsoNormal">             }<u></u><u></u></p></div><div><p class="MsoNormal">             if(taskid == 3){<u></u><u></u></p>
</div><div><p class="MsoNormal">                 for(d=0;d&lt;ARRAYSIZE;d++){<u></u><u></u></p></div><div><p class="MsoNormal">                 threearray[d] = 1;<u></u><u></u></p></div><div><p class="MsoNormal">                  }<u></u><u></u></p>
</div><div><p class="MsoNormal">                  three = run_kernel0(threearray,ARRAYSIZE,taskid, myname);<u></u><u></u></p></div><div><p class="MsoNormal">             }<u></u><u></u></p></div><div><p class="MsoNormal">
     if(taskid == 4){<u></u><u></u></p></div><div><p class="MsoNormal">                   for(e=0;e &lt; ARRAYSIZE; e++){<u></u><u></u></p></div><div><p class="MsoNormal">                      fourarray[e] = 1;<u></u><u></u></p>
</div><div><p class="MsoNormal">                  }<u></u><u></u></p></div><div><p class="MsoNormal">                 four = run_kernel0(fourarray,ARRAYSIZE,taskid, myname);<u></u><u></u></p></div><div><p class="MsoNormal">
             }<u></u><u></u></p></div><div><p class="MsoNormal">             if(taskid == 5){<u></u><u></u></p></div><div><p class="MsoNormal">                for(f=0;f&lt;ARRAYSIZE;f++){<u></u><u></u></p></div><div><p class="MsoNormal">
                  fivearray[f] = 1;<u></u><u></u></p></div><div><p class="MsoNormal">                  }<u></u><u></u></p></div><div><p class="MsoNormal">                five = run_kernel0(fivearray,ARRAYSIZE,taskid, myname);<u></u><u></u></p>
</div><div><p class="MsoNormal">             }<u></u><u></u></p></div><div><p class="MsoNormal">             if(taskid == 6){<u></u><u></u></p></div><div><p class="MsoNormal">                  <u></u><u></u></p></div><div>
<p class="MsoNormal">                for(g=0;g&lt;ARRAYSIZE;g++){<u></u><u></u></p></div><div><p class="MsoNormal">                 sixarray[g] = 1;<u></u><u></u></p></div><div><p class="MsoNormal">                }<u></u><u></u></p>
</div><div><p class="MsoNormal">                 six = run_kernel0(sixarray,ARRAYSIZE,taskid, myname);<u></u><u></u></p></div><div><p class="MsoNormal">             } <u></u><u></u></p></div><div><p class="MsoNormal">             if(taskid == 7){<u></u><u></u></p>
</div><div><p class="MsoNormal">                    for(h=0;h&lt;ARRAYSIZE;h++){<u></u><u></u></p></div><div><p class="MsoNormal">                    sevenarray[h] = 1;<u></u><u></u></p></div><div><p class="MsoNormal">                  }<u></u><u></u></p>
</div><div><p class="MsoNormal">                   seven = run_kernel0(sevenarray,ARRAYSIZE,taskid, myname);<u></u><u></u></p></div><div><p class="MsoNormal">             } <u></u><u></u></p></div><div><p class="MsoNormal">
             if(taskid == 8){<u></u><u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">                  for(i=0;i&lt;ARRAYSIZE;i++){<u></u><u></u></p></div><div><p class="MsoNormal">
                  eightarray[i] = 1;<u></u><u></u></p></div><div><p class="MsoNormal">                }<u></u><u></u></p></div><div><p class="MsoNormal">                   eight = run_kernel0(eightarray,ARRAYSIZE,taskid, myname);<u></u><u></u></p>
</div><div><p class="MsoNormal">             } <u></u><u></u></p></div><div><p class="MsoNormal">             if(taskid == 9){<u></u><u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">
                   for(j=0;j&lt;ARRAYSIZE;j++){<u></u><u></u></p></div><div><p class="MsoNormal">                 ninearray[j] = 1;<u></u><u></u></p></div><div><p class="MsoNormal">                   }<u></u><u></u></p></div>
<div><p class="MsoNormal">                   nine = run_kernel0(ninearray,ARRAYSIZE,taskid, myname);<u></u><u></u></p></div><div><p class="MsoNormal">             } <u></u><u></u></p></div><div><p class="MsoNormal">   }<u></u><u></u></p>
</div><div><p class="MsoNormal"> MPI_Finalize();<u></u><u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">}   <u></u><u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p>
</div><div><p class="MsoNormal">All the tasks just initialize their own array and then calculate the sum using cuda thrust.<u></u><u></u></p></div><div><p class="MsoNormal">My CUDA Thrust code - <u></u><u></u></p></div><div>
<p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal"> #include &lt;stdio.h&gt;<u></u><u></u></p></div><div><p class="MsoNormal">#include &lt;cutil_inline.h&gt;<u></u><u></u></p></div><div><p class="MsoNormal">
#include &lt;cutil.h&gt;<u></u><u></u></p></div><div><p class="MsoNormal">#include &lt;thrust/version.h&gt;<u></u><u></u></p></div><div><p class="MsoNormal">#include &lt;thrust/generate.h&gt;<u></u><u></u></p></div><div><p class="MsoNormal">
#include &lt;thrust/host_vector.h&gt;<u></u><u></u></p></div><div><p class="MsoNormal">#include &lt;thrust/device_vector.h&gt;<u></u><u></u></p></div><div><p class="MsoNormal">#include &lt;thrust/functional.h&gt;<u></u><u></u></p>
</div><div><p class="MsoNormal">#include &lt;thrust/transform_reduce.h&gt;<u></u><u></u></p></div><div><p class="MsoNormal">#include &lt;time.h&gt;<u></u><u></u></p></div><div><p class="MsoNormal">#include &lt;sys/time.h&gt;<u></u><u></u></p>
</div><div><p class="MsoNormal">#include &lt;sys/resource.h&gt;<u></u><u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">  extern &quot;C&quot;<u></u><u></u></p></div><div><p class="MsoNormal">
 int run_kernel0( int array[], int nelements, int taskid, char hostname[])<u></u><u></u></p></div><div><p class="MsoNormal"> {<u></u><u></u></p></div><div><p class="MsoNormal">   <u></u><u></u></p></div><div><p class="MsoNormal">
       float elapsedTime;<u></u><u></u></p></div><div><p class="MsoNormal">        int result = 0;<u></u><u></u></p></div><div><p class="MsoNormal">int threshold = 25000000;<u></u><u></u></p></div><div><p class="MsoNormal">
        cudaEvent_t start, stop;<u></u><u></u></p></div><div><p class="MsoNormal">cudaEventCreate(&amp;start);<u></u><u></u></p></div><div><p class="MsoNormal">cudaEventCreate(&amp;stop);<u></u><u></u></p></div><div><p class="MsoNormal">
cudaEventRecord(start, 0);<u></u><u></u></p></div><div><p class="MsoNormal">thrust::device_vector&lt;int&gt; gpuarray;<u></u><u></u></p></div><div><p class="MsoNormal">int *begin = array;<u></u><u></u></p></div><div><p class="MsoNormal">
int *end = array + nelements;<u></u><u></u></p></div><div><p class="MsoNormal">while(begin != end)<u></u><u></u></p></div><div><p class="MsoNormal">{<u></u><u></u></p></div><div><p class="MsoNormal">   int chunk_size = thrust::min(threshold,end - begin);<u></u><u></u></p>
</div><div><p class="MsoNormal">   gpuarray.assign(begin, begin + chunk_size); <u></u><u></u></p></div><div><p class="MsoNormal"> result += thrust::reduce(gpuarray.begin(), gpuarray.end());<u></u><u></u></p></div><div><p class="MsoNormal">
   begin += chunk_size;<u></u><u></u></p></div><div><p class="MsoNormal">}<u></u><u></u></p></div><div><p class="MsoNormal">        cudaEventRecord(stop, 0);<u></u><u></u></p></div><div><p class="MsoNormal">        cudaEventSynchronize(stop);     <u></u><u></u></p>
</div><div><p class="MsoNormal">cudaEventElapsedTime(&amp;elapsedTime, start, stop);<u></u><u></u></p></div><div><p class="MsoNormal">cudaEventDestroy(start);<u></u><u></u></p></div><div><p class="MsoNormal">cudaEventDestroy(stop);<u></u><u></u></p>
</div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">        printf(&quot; Task %d on has sum (on GPU): %ld Time for the kernel: %f ms \n&quot;, taskid, result, elapsedTime); <u></u><u></u></p>
</div><div><p class="MsoNormal">     <u></u><u></u></p></div><div><p class="MsoNormal">return result;<u></u><u></u></p></div><div><p class="MsoNormal">    }<u></u><u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p>
</div><div><p class="MsoNormal">I also calculate the sum using CPU and the code is as below - <u></u><u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><div><p class="MsoNormal">  struct timespec time1, time2, temp_time;<u></u><u></u></p>
</div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">  clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &amp;time1);<u></u><u></u></p></div><div><p class="MsoNormal">  int i;<u></u><u></u></p></div><div>
<p class="MsoNormal">  int cpu_sum = 0;<u></u><u></u></p></div><div><p class="MsoNormal">  long diff = 0;<u></u><u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">  for (i = 0; i &lt; nelements; i++) {<u></u><u></u></p>
</div><div><p class="MsoNormal">    cpu_sum += array[i];<u></u><u></u></p></div><div><p class="MsoNormal">  }    <u></u><u></u></p></div><div><p class="MsoNormal">  clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &amp;time2);<u></u><u></u></p>
</div><div><p class="MsoNormal">  temp_time.tv_sec = time2.tv_sec - time1.tv_sec;<u></u><u></u></p></div><div><p class="MsoNormal">  temp_time.tv_nsec = time2.tv_nsec - time1.tv_nsec;<u></u><u></u></p></div><div><p class="MsoNormal">
  diff = temp_time.tv_sec * 1000000000 + temp_time.tv_nsec; <u></u><u></u></p></div><div><p class="MsoNormal">  printf(&quot;Task %d calculated sum: %d using CPU in %lf ms \n&quot;, taskid, cpu_sum, (double) diff/1000000); <u></u><u></u></p>
</div><div><p class="MsoNormal">  return cpu_sum;<u></u><u></u></p></div></div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">Now when I run the job on cluster with 10 MPI tasks and compare the timings of CPU and GPU, I get weird results where GPU time is much much higher than CPU time. <u></u><u></u></p>
</div><div><p class="MsoNormal">But the case should be opposite isnt it?<u></u><u></u></p></div><p class="MsoNormal"><u></u> <u></u></p></div><div><div><p class="MsoNormal">The CPU time is almost same for all the task but GPU time increases. <u></u><u></u></p>
</div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">Just wondering what might be the cause of this or are these results correct? Anything wrong with MPI code?<u></u><u></u></p></div><div><p class="MsoNormal">
<u></u> <u></u></p></div><div><p class="MsoNormal">My cluster has 3 machines. 4 MPI tasks run on 2 machine and 2 Tasks run on 1 machine. <u></u><u></u></p></div><div><p class="MsoNormal">Each machine has 1 GPU - GForce 9500 GT with 512 MB memory. <u></u><u></u></p>
</div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">Can anyone please help me with this.?<u></u><u></u></p></div></div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal">
Thanks<u></u><u></u></p></div><p class="MsoNormal">-- <u></u><u></u></p><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p></div><p class="MsoNormal"><u></u> <u></u></p></div></div>
</div></div>
<div>
<hr>
</div>
<div>This email message is for the sole use of the intended recipient(s) and may 
contain confidential information.  Any unauthorized review, use, disclosure 
or distribution is prohibited.  If you are not the intended recipient, 
please contact the sender by reply email and destroy all copies of the original 
message. </div>
<div>
<hr>
</div>
<p></p>
</div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br><div><span style="font-size:13px;border-collapse:collapse"><font color="#000099" face="verdana, sans-serif"><br>
<font>Best Regards,</font></font></span></div><div><span style="font-size:13px;border-collapse:collapse"><font face="verdana, sans-serif"><font color="#000099"><br>ROHAN DESHPANDE</font><font>  </font></font></span></div>
<div><span style="font-size:13px"><br><font color="#666666" face="georgia, serif"><br></font></span></div><br>
</div>

