Sorry Richard,<div><br></div><div>what is <span class="Apple-style-span" style="font-family: sans-serif; border-collapse: collapse; ">CC issue order on the communicator?, in particular, &quot;CC&quot;, what does it mean?</span><br>
<br><div class="gmail_quote">2010/9/23 Richard Treumann <span dir="ltr">&lt;<a href="mailto:treumann@us.ibm.com">treumann@us.ibm.com</a>&gt;</span><br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">

<br><font size="2" face="sans-serif">request_1 and request_2 are just local
variable names. </font>
<br>
<br><font size="2" face="sans-serif">The only thing that determines matching
order is CC issue order on the communicator.  At each process, some
CC is issued first and some CC is issued second.  The first issued
CC at each process will try to match the first issued CC at the other processes.
 By this rule,</font>
<br><font size="2" face="sans-serif">rank 0: </font>
<br><font size="2" face="sans-serif">MPI_Ibcast; MPI_Ibcast </font>
<br><font size="2" face="sans-serif">Rank 1;</font>
<br><font size="2" face="sans-serif">MPI_Ibcast; MPI_Ibcast </font>
<br><font size="2" face="sans-serif">is well defined and</font>
<br>
<br><font size="2" face="sans-serif">rank 0: </font>
<br><font size="2" face="sans-serif">MPI_Ibcast; MPI_Ireduce</font>
<br><font size="2" face="sans-serif">Rank 1;</font>
<br><font size="2" face="sans-serif">MPI_Ireducet; MPI_Ibcast </font>
<br><font size="2" face="sans-serif">is incorrect.</font>
<br>
<br><font size="2" face="sans-serif">I do not agree with Jeff on this below.
  The Proc 1 case where the MPI_Waits are reversed simply requires
the MPI implementation to make progress on both MPI_Ibcast operations in
the first MPI_Wait. The second MPI_Wait call will simply find that the
first MPI_Ibcast is already done.  The second MPI_Wait call becomes,
effectively, a query function.</font>
<br><div class="im">
<br><tt><font size="2">proc 0:<br>
MPI_IBcast(MPI_COMM_WORLD, request_1) // first Bcast<br>
MPI_IBcast(MPI_COMM_WORLD, request_2) // second Bcast<br>
MPI_Wait(&amp;request_1, ...);<br>
MPI_Wait(&amp;request_2, ...);<br>
<br>
proc 1:<br>
MPI_IBcast(MPI_COMM_WORLD, request_2) // first Bcast<br>
MPI_IBcast(MPI_COMM_WORLD, request_1) // second Bcast<br>
MPI_Wait(&amp;request_1, ...);<br>
MPI_Wait(&amp;request_2, ...);<br>
<br>
That may/will deadlock.</font></tt>
<br>
<br>
<br>
<br>
<br>
<br></div><font size="2" face="sans-serif">Dick Treumann  -  MPI Team
          <br>
IBM Systems &amp; Technology Group<br>
Dept X2ZA / MS P963 -- 2455 South Road -- Poughkeepsie, NY 12601<br>
Tele (845) 433-7846         Fax (845) 433-8363<br>
</font>
<br>
<br>
<br>
<table width="100%">
<tbody><tr valign="top">
<td><font size="1" color="#5f5f5f" face="sans-serif">From:</font>
</td><td><div class="im"><font size="1" face="sans-serif">Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</font>
</div></td></tr><tr valign="top">
<td><font size="1" color="#5f5f5f" face="sans-serif">To:</font>
</td><td><font size="1" face="sans-serif">Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;</font>
</td></tr><tr valign="top">
<td><font size="1" color="#5f5f5f" face="sans-serif">Date:</font>
</td><td><font size="1" face="sans-serif">09/23/2010 10:13 AM</font>
</td></tr><tr valign="top">
<td><font size="1" color="#5f5f5f" face="sans-serif">Subject:</font>
</td><td><font size="1" face="sans-serif">Re: [OMPI users] Question about Asynchronous
collectives</font>
</td></tr><tr valign="top">
<td><font size="1" color="#5f5f5f" face="sans-serif">Sent by:</font>
</td><td><font size="1" face="sans-serif"><a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a></font></td></tr></tbody></table>
<br>
<hr noshade><div><div></div><div class="h5">
<br>
<br>
<br><tt><font size="2">On Sep 23, 2010, at 10:00 AM, Gabriele Fatigati wrote:<br>
<br>
&gt; to be sure, if i have one processor who does:<br>
&gt; <br>
&gt; MPI_IBcast(MPI_COMM_WORLD, request_1) // first Bcast<br>
&gt; MPI_IBcast(MPI_COMM_WORLD, request_2) // second Bcast<br>
&gt; <br>
&gt; it means that i can&#39;t have another process who does the follow:<br>
&gt; <br>
&gt; MPI_IBcast(MPI_COMM_WORLD, request_2) // firt Bcast for another process<br>
&gt; MPI_IBcast(MPI_COMM_WORLD, request_1) // second Bcast for another
process<br>
&gt; <br>
&gt; Because first Bcast of second process matches with first Bcast of
first process, and it&#39;s wrong.<br>
<br>
If you did a &quot;waitall&quot; on both requests, it would probably work
because MPI would just &quot;figure it out&quot;.  But if you did
something like:<br>
<br>
proc 0:<br>
MPI_IBcast(MPI_COMM_WORLD, request_1) // first Bcast<br>
MPI_IBcast(MPI_COMM_WORLD, request_2) // second Bcast<br>
MPI_Wait(&amp;request_1, ...);<br>
MPI_Wait(&amp;request_2, ...);<br>
<br>
proc 1:<br>
MPI_IBcast(MPI_COMM_WORLD, request_2) // first Bcast<br>
MPI_IBcast(MPI_COMM_WORLD, request_1) // second Bcast<br>
MPI_Wait(&amp;request_1, ...);<br>
MPI_Wait(&amp;request_2, ...);<br>
<br>
That may/will deadlock.<br>
<br>
-- <br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a><br>
For corporate legal information go to:<br>
</font></tt><a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank"><tt><font size="2">http://www.cisco.com/web/about/doing_business/legal/cri/</font></tt></a><tt><font size="2"><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
</font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank"><tt><font size="2">http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size="2"><br>
</font></tt>
<br>
<br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br><br clear="all"><br>-- <br>Ing. Gabriele Fatigati<br><br>Parallel programmer<br>
<br>CINECA Systems &amp; Tecnologies Department<br><br>Supercomputing Group<br><br>Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br><br><a href="http://www.cineca.it">www.cineca.it</a>                    Tel:   +39 051 6171722<br>
<br>g.fatigati [AT] <a href="http://cineca.it">cineca.it</a>           <br>
</div>

