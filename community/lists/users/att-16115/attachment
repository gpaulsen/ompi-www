<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><br><div><div>On Apr 4, 2011, at 8:42 AM, Nehemiah Dacres wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite">you do realize that this is Sun Cluster Tools branch (it is a branch right? or is it a *port* of openmpi to sun's compilers?) I'm not sure if your changes made it into sunct 8.2.1&nbsp;<br></blockquote><div><br></div>My point was that the error message currently doesn't include the node name - not in the OMPI main code base, nor in the SCT port. So I will add it, which won't help you at the moment.</div><div><br></div><div>Hence my suggestion about using the param :-)</div><div><br></div><div><br><blockquote type="cite"><br><div class="gmail_quote">On Mon, Apr 4, 2011 at 9:34 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;"><div style="word-wrap:break-word">Guess I can/will add the node name to the error message - should have been there before now.<div>
<br></div><div>If it is a debug build, you can add "-mca plm_base_verbose 1" to the cmd line and get output tracing the launch and showing you what nodes are having problems.</div><div><div></div><div class="h5">
<div><br></div><div><br><div><div>On Apr 4, 2011, at 8:24 AM, Nehemiah Dacres wrote:</div><br><blockquote type="cite">I have installed it via a symlink on all of the nodes, I can go 'tentakel which mpirun ' and it finds it' I'll check the library paths but isn't there a way to find out which nodes are returning the error?&nbsp;<br>

<div><br></div><div><br><div class="gmail_quote">On Thu, Mar 31, 2011 at 7:30 AM, Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br>


<blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204, 204, 204);padding-left:1ex">The error message seems to imply that you don't have OMPI installed on all your nodes (because it didn't find /opt/SUNWhpc/HPC8.2.1c/sun/bin/orted on a remote node).<br>





<div><div></div><div><br>
<br>
On Mar 30, 2011, at 4:24 PM, Nehemiah Dacres wrote:<br>
<br>
&gt; I am trying to figure out why my jobs aren't getting distributed and need some help. I have an install of sun cluster tools on Rockscluster 5.2 (essentially centos4u2). this user's account has its home dir shared via nfs. I am getting some strange errors. here's an example run<br>





&gt;<br>
&gt;<br>
&gt; [jian@therock ~]$ /opt/SUNWhpc/HPC8.2.1c/sun/bin/mpirun -np 3 -hostfile list ./job2.sh<br>
&gt; bash: /opt/SUNWhpc/HPC8.2.1c/sun/bin/orted: No such file or directory<br>
&gt; --------------------------------------------------------------------------<br>
&gt; A daemon (pid 20362) died unexpectedly with status 127 while attempting<br>
&gt; to launch so we are aborting.<br>
&gt;<br>
&gt; There may be more information reported by the environment (see above).<br>
&gt;<br>
&gt; This may be because the daemon was unable to find all the needed shared<br>
&gt; libraries on the remote node. You may set your LD_LIBRARY_PATH to have the<br>
&gt; location of the shared libraries on the remote nodes and this will<br>
&gt; automatically be forwarded to the remote nodes.<br>
&gt; --------------------------------------------------------------------------<br>
&gt; --------------------------------------------------------------------------<br>
&gt; mpirun noticed that the job aborted, but has no info as to the process<br>
&gt; that caused that situation.<br>
&gt; --------------------------------------------------------------------------<br>
&gt; mpirun: clean termination accomplished<br>
&gt;<br>
&gt; [jian@therock ~]$ /opt/SUNWhpc/HPC8.2.1c/sun/<br>
&gt; bin/ &nbsp; &nbsp; &nbsp; &nbsp;examples/ &nbsp; instrument/ man/<br>
&gt; etc/ &nbsp; &nbsp; &nbsp; &nbsp;include/ &nbsp; &nbsp;lib/ &nbsp; &nbsp; &nbsp; &nbsp;share/<br>
&gt; [jian@therock ~]$ /opt/SUNWhpc/HPC8.2.1c/sun/bin/orte<br>
&gt; orte-clean &nbsp;orted &nbsp; &nbsp; &nbsp; orte-iof &nbsp; &nbsp;orte-ps &nbsp; &nbsp; orterun<br>
&gt; [jian@therock ~]$ /opt/SUNWhpc/HPC8.2.1c/sun/bin/orted<br>
&gt; [therock.slu.loc:20365] [[INVALID],INVALID] ORTE_ERROR_LOG: Not found in file runtime/orte_init.c at line 125<br>
&gt; --------------------------------------------------------------------------<br>
&gt; It looks like orte_init failed for some reason; your parallel process is<br>
&gt; likely to abort. &nbsp;There are many reasons that a parallel process can<br>
&gt; fail during orte_init; some of which are due to configuration or<br>
&gt; environment problems. &nbsp;This failure appears to be an internal failure;<br>
&gt; here's some additional information (which may only be relevant to an<br>
&gt; Open MPI developer):<br>
&gt;<br>
&gt; &nbsp; orte_ess_base_select failed<br>
&gt; &nbsp; --&gt; Returned value Not found (-13) instead of ORTE_SUCCESS<br>
&gt; --------------------------------------------------------------------------<br>
&gt; [therock.slu.loc:20365] [[INVALID],INVALID] ORTE_ERROR_LOG: Not found in file orted/orted_main.c at line 325<br>
&gt; [jian@therock ~]$<br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Nehemiah I. Dacres<br>
&gt; System Administrator<br>
&gt; Advanced Technology Group Saint Louis University<br>
&gt;<br>
</div></div><div>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
--<br>
</div><font color="#888888">Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a><br>
For corporate legal information go to:<br>
<a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
</font><div><div></div><div><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br><br clear="all"><br>-- <br>Nehemiah I. Dacres<br>System Administrator&nbsp;<div>Advanced Technology Group Saint Louis University</div><br>
</div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br><br clear="all"><br>-- <br>Nehemiah I. Dacres<br>System Administrator&nbsp;<div>
Advanced Technology Group Saint Louis University</div><br>
</blockquote></div><br></body></html>
