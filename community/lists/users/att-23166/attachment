<DIV>&nbsp;I have a&nbsp;server &nbsp;with&nbsp; 12 cores.when I run mpi program with 10 processors.only&nbsp; three processors works.Here are a picture about the problem</DIV>
<DIV><!--StartFragment -->
<DIV>&nbsp;</DIV></DIV>
<DIV>
<DIV><IMG src="cid:40F6DC95@E690AF16.27C0A552.jpg" filesize="33511" naturalW="343" naturalH="139"><BR></DIV>
<DIV>&nbsp;</DIV>
<DIV>why?Is the problem with&nbsp;<SPAN style="WHITE-SPACE: normal; TEXT-TRANSFORM: none; WORD-SPACING: 0px; FLOAT: none; COLOR: rgb(67,67,67); TEXT-ALIGN: left; FONT: bold 12px/19px Tahoma, Arial; DISPLAY: inline !important; LETTER-SPACING: normal; BACKGROUND-COLOR: rgb(242,242,242); TEXT-INDENT: 0px; -webkit-text-stroke-width: 0px">process schedule??&nbsp;&nbsp; </SPAN><BR></DIV>
<DIV style="FONT-SIZE: 12px; FONT-FAMILY: Arial Narrow; PADDING-BOTTOM: 2px; PADDING-TOP: 2px; PADDING-LEFT: 0px; PADDING-RIGHT: 0px">------------------&nbsp;原始邮件&nbsp;------------------</DIV>
<DIV style="FONT-SIZE: 12px; BACKGROUND: #efefef; PADDING-BOTTOM: 8px; PADDING-TOP: 8px; PADDING-LEFT: 8px; PADDING-RIGHT: 8px">
<DIV><B>发件人:</B>&nbsp;"Bruno Coutinho";&lt;coutinho@dcc.ufmg.br&gt;;</DIV>
<DIV><B>发送时间:</B>&nbsp;2013年12月6日(星期五) 晚上11:14</DIV>
<DIV><B>收件人:</B>&nbsp;"Open MPI Users"&lt;users@open-mpi.org&gt;; <WBR></DIV>
<DIV></DIV>
<DIV><B>主题:</B>&nbsp;Re: [OMPI users]回复： can you help me please ?thanks</DIV></DIV>
<DIV><BR></DIV>
<DIV dir=ltr>Probably it was the changing from eager to rendezvous protocols as Jeff said. 
<DIV><BR></DIV>
<DIV>If you don't know what are these, read this:</DIV>
<DIV><A href="https://computing.llnl.gov/tutorials/mpi_performance/#Protocols">https://computing.llnl.gov/tutorials/mpi_performance/#Protocols</A><BR></DIV>
<DIV><A href="http://blogs.cisco.com/performance/what-is-an-mpi-eager-limit/">http://blogs.cisco.com/performance/what-is-an-mpi-eager-limit/</A><BR></DIV>
<DIV><A href="http://blogs.cisco.com/performance/eager-limits-part-2/">http://blogs.cisco.com/performance/eager-limits-part-2/</A><BR></DIV>
<DIV><BR></DIV>
<DIV>You can tune eager limit chaning mca parameters btl_tcp_eager_limit (for tcp), btl_self_eager_limit (comunication fron one process to itself), btl_sm_eager_limit (shared memory) and btl_udapl_eager_limit or btl_openib_eager_limit (if you use infiniband).</DIV>
<DIV class=gmail_extra><BR><BR>
<DIV class=gmail_quote>2013/12/6 Jeff Squyres (jsquyres) <SPAN dir=ltr>&lt;<A href="mailto:jsquyres@cisco.com" target=_blank>jsquyres@cisco.com</A>&gt;</SPAN><BR>
<BLOCKQUOTE class=gmail_quote style="PADDING-LEFT: 1ex; MARGIN: 0px 0px 0px 0.8ex; BORDER-LEFT: #ccc 1px solid">I sent you some further questions yesterday:<BR><BR>&nbsp; &nbsp; <A href="http://www.open-mpi.org/community/lists/users/2013/12/23158.php" target=_blank>http://www.open-mpi.org/community/lists/users/2013/12/23158.php</A><BR>
<DIV class=HOEnZb>
<DIV class=h5><BR><BR>On Dec 6, 2013, at 1:35 AM, 胡杨 &lt;<A href="mailto:781578278@qq.com">781578278@qq.com</A>&gt; wrote:<BR><BR>&gt; Here is &nbsp;my code:<BR>&gt; int*a=(int*)malloc(sizeof(int)*number);<BR>&gt; MPI_Send(a,number, MPI_INT, 1, 1,MPI_COMM_WORLD);<BR>&gt;<BR>&gt; int*b=(int*)malloc(sizeof(int)*number);<BR>&gt; MPI_Recv(b, number, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status);<BR>&gt;<BR>&gt; number &nbsp;here is the size of my array(eg,a or b).<BR>&gt; I &nbsp;have try it on my local compute and my rocks cluster.On rocks cluster, one processor &nbsp;on &nbsp;one frontend node &nbsp;use "MPI_Send" send a message ,other processors on compute nodes use "MPI_Recv" receive message .<BR>&gt; when number is least than 10000,other processors can receive message fast;<BR>&gt; but when &nbsp;number is more than 15000,other processors can receive message slowly<BR>&gt; why?? &nbsp;becesue openmpi API ?? or other &nbsp;problems?<BR>&gt;<BR>&gt; it spends me a few days , I want your help,thanks for all readers. good luck for you<BR>&gt;<BR>&gt;<BR>&gt;<BR>&gt;<BR>&gt; ------------------ 原始邮件 ------------------<BR>&gt; 发件人: "Ralph Castain";&lt;<A href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</A>&gt;;<BR>&gt; 发送时间: 2013年12月5日(星期四) 晚上6:52<BR>&gt; 收件人: "Open MPI Users"&lt;<A href="mailto:users@open-mpi.org">users@open-mpi.org</A>&gt;;<BR>&gt; 主题: Re: [OMPI users] can you help me please ?thanks<BR>&gt;<BR>&gt; You are running 15000 ranks on two nodes?? My best guess is that you are swapping like crazy as your memory footprint problem exceeds available physical memory.<BR>&gt;<BR>&gt;<BR>&gt;<BR>&gt; On Thu, Dec 5, 2013 at 1:04 AM, 胡杨 &lt;<A href="mailto:781578278@qq.com">781578278@qq.com</A>&gt; wrote:<BR>&gt; My ROCKS cluster includes one frontend and two &nbsp;compute nodes.In my program,I have use the openmpi API &nbsp;such as &nbsp;MPI_Send and &nbsp;MPI_Recv . &nbsp;but &nbsp;when I &nbsp;run &nbsp;the progam with 3 processors . one processor &nbsp;send a message ,other receive message .here are some code.<BR>&gt; int*a=(int*)malloc(sizeof(int)*number);<BR>&gt; MPI_Send(a,number, MPI_INT, 1, 1,MPI_COMM_WORLD);<BR>&gt;<BR>&gt; int*b=(int*)malloc(sizeof(int)*number);<BR>&gt; MPI_Recv(b, number, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status);<BR>&gt;<BR>&gt; when number is least than 10000,it runs fast.<BR>&gt; but number is more than 15000,it runs slowly<BR>&gt;<BR>&gt; why?? &nbsp;becesue openmpi API ?? or other &nbsp;problems?<BR>&gt; ------------------ 原始邮件 ------------------<BR>&gt; 发件人: "Ralph Castain";&lt;<A href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</A>&gt;;<BR>&gt; 发送时间: 2013年12月3日(星期二) 中午1:39<BR>&gt; 收件人: "Open MPI Users"&lt;<A href="mailto:users@open-mpi.org">users@open-mpi.org</A>&gt;;<BR>&gt; 主题: Re: [OMPI users] can you help me please ?thanks<BR>&gt;<BR>&gt;<BR>&gt;<BR>&gt;<BR>&gt;<BR>&gt; On Mon, Dec 2, 2013 at 9:23 PM, 胡杨 &lt;<A href="mailto:781578278@qq.com">781578278@qq.com</A>&gt; wrote:<BR>&gt; A simple program at my 4-node ROCKS cluster runs fine with command:<BR>&gt; /opt/openmpi/bin/mpirun -np 4 -machinefile machines ./sort_mpi6<BR>&gt;<BR>&gt;<BR>&gt; Another bigger programs runs fine on the head node only with command:<BR>&gt;<BR>&gt; cd ./sphere; /opt/openmpi/bin/mpirun -np 4 ../bin/sort_mpi6<BR>&gt;<BR>&gt; But with the command:<BR>&gt;<BR>&gt; cd /sphere; /opt/openmpi/bin/mpirun -np 4 -machinefile ../machines<BR>&gt; ../bin/sort_mpi6<BR>&gt;<BR>&gt; It gives output that:<BR>&gt;<BR>&gt; ../bin/sort_mpi6: error while loading shared libraries: libgdal.so.1: cannot open<BR>&gt; shared object file: No such file or directory<BR>&gt; ../bin/sort_mpi6: error while loading shared libraries: libgdal.so.1: cannot open<BR>&gt; shared object file: No such file or directory<BR>&gt; ../bin/sort_mpi6: error while loading shared libraries: libgdal.so.1: cannot open<BR>&gt; shared object file: No such file or directory<BR>&gt;<BR>&gt;<BR>&gt;<BR>&gt; _______________________________________________<BR>&gt; users mailing list<BR>&gt; <A href="mailto:users@open-mpi.org">users@open-mpi.org</A><BR>&gt; <A href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target=_blank>http://www.open-mpi.org/mailman/listinfo.cgi/users</A><BR>&gt;<BR>&gt;<BR>&gt; _______________________________________________<BR>&gt; users mailing list<BR>&gt; <A href="mailto:users@open-mpi.org">users@open-mpi.org</A><BR>&gt; <A href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target=_blank>http://www.open-mpi.org/mailman/listinfo.cgi/users</A><BR>&gt;<BR>&gt; _______________________________________________<BR>&gt; users mailing list<BR>&gt; <A href="mailto:users@open-mpi.org">users@open-mpi.org</A><BR>&gt; <A href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target=_blank>http://www.open-mpi.org/mailman/listinfo.cgi/users</A><BR><BR><BR></DIV></DIV>
<DIV class="im HOEnZb">--<BR>Jeff Squyres<BR><A href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</A><BR>For corporate legal information go to: <A href="http://www.cisco.com/web/about/doing_business/legal/cri/" target=_blank>http://www.cisco.com/web/about/doing_business/legal/cri/</A><BR><BR></DIV>
<DIV class=HOEnZb>
<DIV class=h5>_______________________________________________<BR>users mailing list<BR><A href="mailto:users@open-mpi.org">users@open-mpi.org</A><BR><A href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target=_blank>http://www.open-mpi.org/mailman/listinfo.cgi/users</A></DIV></DIV></BLOCKQUOTE></DIV><BR></DIV></DIV>
<DIV></DIV></DIV>