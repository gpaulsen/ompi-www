<div dir="ltr"><div><div><div><div>Hi All,<br><br></div>We&#39;re using OpenMPI 1.7.3 with Mellanox ConnectX InfiniBand adapters, and periodically our jobs abort at start-up with the following error:<br><br>===<br>Open MPI detected two different OpenFabrics transport types in the same Infiniband network.<br>
Such mixed network trasport configuration is not supported by Open MPI.<br><br>  Local host:            w4<br>  Local adapter:         mlx4_0 (vendor 0x2c9, part ID 26428)<br>  Local transport type:  MCA_BTL_OPENIB_TRANSPORT_IB<br>
<br>  Remote host:           w34<br>  Remote Adapter:        (vendor 0x2c9, part ID 26428)<br>  Remote transport type: MCA_BTL_OPENIB_TRANSPORT_UNKNOWN<br>===<br><br></div>I&#39;ve done a bit of googling and not found very much. We do not see this issue when we run with MVAPICH2 on the same sets of nodes.<br>
<br></div>Any advice or thoughts would be very welcome, as I am stumped by what causes this. The nodes are all running Scientific Linux 6 with Mellanox drivers installed via the SL-provided RPMs.<br><br></div>Tim<br><div>
<div><div><br></div></div></div></div>

