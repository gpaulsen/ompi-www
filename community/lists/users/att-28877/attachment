<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    Hi,<br>
    <br>
    per a previous message, can you give a try to<br>
    mpirun -np 2 -hostfile ~/hostfile -mca btl self,tcp --mca pml ob1
    ./mpitest<br>
    <br>
    if it still hangs, the issue could be OpenMPI think some subnets are
    reachable but they are not.<br>
    <br>
    for diagnostic : <br>
    mpirun --mca btl_base_verbose 100 ...<br>
    <br>
    you can explicitly include/exclude subnets with<br>
    --mca btl_tcp_if_include xxx<br>
    or<br>
    --mca btl_tcp_if_exclude yyy<br>
    <br>
    for example,<br>
    mpirun --mca btl_btp_if_include 192.168.0.0/24 -np 2 -hostfile
    ~/hostfile --mca btl self,tcp --mca pml ob1 ./mpitest<br>
    should do the trick<br>
    <br>
    Cheers,<br>
    <br>
    Gilles<br>
    <br>
    <br>
    <br>
    <div class="moz-cite-prefix">On 4/4/2016 8:32 AM, dpchoudh . wrote:<br>
    </div>
    <blockquote
cite="mid:CAHXxYDg8ePU-hhZ7E9MYM=g0zzq2W8TT3iK7xjuw-i33F5Agdg@mail.gmail.com"
      type="cite">
      <div dir="ltr">
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>Hello all<br>
                    <br>
                  </div>
                  I don't mean to be competing for the 'silliest
                  question of the year award', but I can't figure this
                  out on my own:<br>
                  <br>
                </div>
                My 'cluster' has 2 machines, bigMPI and smallMPI. They
                are connected via several (types of) networks and the
                connectivity is OK.<br>
                <br>
              </div>
              In this setup, the following program hangs after printing<br>
              <br>
              Hello world from processor smallMPI, rank 0 out of 2
              processors<br>
              Hello world from processor bigMPI, rank 1 out of 2
              processors<br>
              smallMPI sent haha!<br>
              <br>
              <br>
            </div>
            Obviously it is hanging at MPI_Recv(). But why? My command
            line is as follows, but this happens if I try openib BTL
            (instead of TCP) as well.<br>
            <br>
            mpirun -np 2 -hostfile ~/hostfile -mca btl self,tcp
            ./mpitest<br>
            <br>
          </div>
          It must be something *really* trivial, but I am drawing a
          blank right now.<br>
          <br>
        </div>
        Please help!<br>
        <br>
        #include &lt;mpi.h&gt;<br>
        #include &lt;stdio.h&gt;<br>
        #include &lt;string.h&gt;<br>
        <br>
        int main(int argc, char** argv)<br>
        {<br>
            int world_size, world_rank, name_len;<br>
            char hostname[MPI_MAX_PROCESSOR_NAME], buf[8];<br>
        <br>
            MPI_Init(&amp;argc, &amp;argv);<br>
            MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);<br>
            MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);<br>
            MPI_Get_processor_name(hostname, &amp;name_len);<br>
            printf("Hello world from processor %s, rank %d out of %d
        processors\n", hostname, world_rank, world_size);<br>
            if (world_rank == 1)<br>
            {<br>
            MPI_Recv(buf, 6, MPI_CHAR, 0, 99, MPI_COMM_WORLD,
        MPI_STATUS_IGNORE);<br>
            printf("%s received %s\n", hostname, buf);<br>
            }<br>
            else<br>
            {<br>
            strcpy(buf, "haha!");<br>
            MPI_Send(buf, 6, MPI_CHAR, 1, 99, MPI_COMM_WORLD);<br>
            printf("%s sent %s\n", hostname, buf);<br>
            }<br>
            MPI_Barrier(MPI_COMM_WORLD);<br>
            MPI_Finalize();<br>
            return 0;<br>
        }<br>
        <br>
        <div>
          <div>
            <div>
              <div>
                <div>
                  <div>
                    <div>
                      <div><br>
                        <br clear="all">
                        <div>
                          <div class="gmail_signature">
                            <div dir="ltr">
                              <div>We learn from history that we never
                                learn from history.<br>
                              </div>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/04/28876.php">http://www.open-mpi.org/community/lists/users/2016/04/28876.php</a></pre>
    </blockquote>
    <br>
  </body>
</html>

