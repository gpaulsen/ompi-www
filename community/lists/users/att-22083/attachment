<html xmlns:v="urn:schemas-microsoft-com:vml" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:w="urn:schemas-microsoft-com:office:word" xmlns:m="http://schemas.microsoft.com/office/2004/12/omml" xmlns="http://www.w3.org/TR/REC-html40">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="Generator" content="Microsoft Word 14 (filtered medium)">
<style><!--
/* Font Definitions */
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:Tahoma;
	panose-1:2 11 6 4 3 5 4 4 2 4;}
/* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin:0in;
	margin-bottom:.0001pt;
	font-size:12.0pt;
	font-family:"Times New Roman","serif";}
a:link, span.MsoHyperlink
	{mso-style-priority:99;
	color:blue;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{mso-style-priority:99;
	color:purple;
	text-decoration:underline;}
span.EmailStyle17
	{mso-style-type:personal-reply;
	font-family:"Calibri","sans-serif";
	color:#1F497D;}
.MsoChpDefault
	{mso-style-type:export-only;
	font-family:"Calibri","sans-serif";}
@page WordSection1
	{size:8.5in 11.0in;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
	{page:WordSection1;}
--></style><!--[if gte mso 9]><xml>
<o:shapedefaults v:ext="edit" spidmax="1026" />
</xml><![endif]--><!--[if gte mso 9]><xml>
<o:shapelayout v:ext="edit">
<o:idmap v:ext="edit" data="1" />
</o:shapelayout></xml><![endif]-->
</head>
<body lang="EN-US" link="blue" vlink="purple">
<div class="WordSection1">
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">The version of mxm is reports as:&nbsp; Version&nbsp;&nbsp;&nbsp;&nbsp; : 1.5.dc8c171<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">The version of OFED reports as:&nbsp; MLNX_OFED_LINUX-2.0-2.0.5:<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D"><o:p>&nbsp;</o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">Here are some revised scaling numbers after configuring OpenMPI to use MXM.&nbsp; I&#8217;m not sure if I posted medium or small case last time, but this is the &#8220;small&#8221; case.&nbsp;
 By the time you get out to 800 cores, each process talks to between 10 to 16 other processes (this is a physical domain decomposition), and the message sizes can be described by saying there is a distribution from 1K bytes up to 10K bytes (25%), 3 times larger
 (50%), and 3 times smaller (25%). On the &#8220;medium&#8221; case, the difference between OpenMPI and MVAPICH is smaller, but OpenMPI is still doing better.<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D"><o:p>&nbsp;</o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D"><o:p>&nbsp;</o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">Scalability - 1 domain per process<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">&nbsp;&nbsp;&nbsp;MPI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # cores&nbsp;&nbsp; Ave. Rate&nbsp;&nbsp; Std. Dev. %&nbsp; # timings&nbsp;&nbsp; Speedup&nbsp;&nbsp;&nbsp; Efficiency<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">================================================================================================<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">MVAPICH&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; 16&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 7.5822&nbsp; |&nbsp;&nbsp; 0.171 % |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; |&nbsp;&nbsp; 16.000&nbsp; |&nbsp; 1.0000<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">MVAPICH&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; 48&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 7.7416&nbsp; |&nbsp;&nbsp; 0.804 % |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; |&nbsp;&nbsp; 47.011&nbsp; |&nbsp; 0.9794<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">MVAPICH&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; 80&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 7.6365&nbsp; |&nbsp;&nbsp; 0.252 % |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; |&nbsp;&nbsp; 79.431&nbsp; |&nbsp; 0.9929<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">MVAPICH&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 160&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 7.4802&nbsp; |&nbsp;&nbsp; 0.887 % |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; |&nbsp; 162.182&nbsp; |&nbsp; 1.0136<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">MVAPICH&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 256&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 7.7930&nbsp; |&nbsp;&nbsp; 1.554 % |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; |&nbsp; 249.073&nbsp; |&nbsp; 0.9729<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">MVAPICH&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 320&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 7.7346&nbsp; |&nbsp;&nbsp; 0.423 % |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; |&nbsp; 313.695&nbsp; |&nbsp; 0.9803<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">MVAPICH&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 480&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 7.9225&nbsp; |&nbsp;&nbsp; 2.594 % |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; |&nbsp; 459.378&nbsp; |&nbsp; 0.9570<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">MVAPICH&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 640&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 8.3111&nbsp; |&nbsp;&nbsp; 2.416 % |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; |&nbsp; 583.866&nbsp; |&nbsp; 0.9123<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">MVAPICH&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 800&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 8.9315&nbsp; |&nbsp;&nbsp; 5.059 % |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; |&nbsp; 679.137&nbsp; |&nbsp; 0.8489<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">OpenMPI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; 16&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 7.5919&nbsp; |&nbsp;&nbsp; 0.879 % |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; |&nbsp;&nbsp; 16.000&nbsp; |&nbsp; 1.0000<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">OpenMPI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; 48&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 7.7469&nbsp; |&nbsp;&nbsp; 0.478 % |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; |&nbsp;&nbsp; 47.040&nbsp; |&nbsp; 0.9800<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">OpenMPI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp; 80&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 7.6654&nbsp; |&nbsp;&nbsp; 0.544 % |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; | &nbsp;&nbsp;79.233&nbsp; |&nbsp; 0.9904<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">OpenMPI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 160&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 7.7252&nbsp; |&nbsp;&nbsp; 2.202 % |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; |&nbsp; 157.239&nbsp; |&nbsp; 0.9827<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">OpenMPI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 256&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 7.7043&nbsp; |&nbsp;&nbsp; 0.563 % |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; |&nbsp; 252.265&nbsp; |&nbsp; 0.9854<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">OpenMPI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 320&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 7.6727&nbsp; |&nbsp;&nbsp; 6.086 % |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; |&nbsp; 316.629&nbsp; |&nbsp; 0.9895<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">OpenMPI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 480&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 7.7016&nbsp; |&nbsp;&nbsp; 0.450 % |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; |&nbsp; 473.163&nbsp; |&nbsp; 0.9858<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">OpenMPI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 640&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 8.0357&nbsp; |&nbsp;&nbsp; 0.572 % |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp; |&nbsp; 604.651&nbsp; |&nbsp; 0.9448<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Courier New&quot;;color:#1F497D">OpenMPI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp; 800&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp; 8.4328&nbsp; |&nbsp;&nbsp; 3.198 % |&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp; |&nbsp; 720.223&nbsp; |&nbsp; 0.9003<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D"><o:p>&nbsp;</o:p></span></p>
<p class="MsoNormal"><b><span style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;">From:</span></b><span style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;"> users-bounces@open-mpi.org [mailto:users-bounces@open-mpi.org]
<b>On Behalf Of </b>Mike Dubman<br>
<b>Sent:</b> Wednesday, June 12, 2013 7:01 AM<br>
<b>To:</b> Open MPI Users<br>
<b>Subject:</b> Re: [OMPI users] EXTERNAL: Re: Need advice on performance problem<o:p></o:p></span></p>
<p class="MsoNormal"><o:p>&nbsp;</o:p></p>
<div>
<p class="MsoNormal">Also, what ofed version (ofed_info -s) and mxm version (rpm -qi mxm) do you use?<o:p></o:p></p>
</div>
<div>
<p class="MsoNormal" style="margin-bottom:12.0pt"><o:p>&nbsp;</o:p></p>
<div>
<p class="MsoNormal">On Wed, Jun 12, 2013 at 3:30 AM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt; wrote:<o:p></o:p></p>
<p class="MsoNormal">Great! Would you mind showing the revised table? I'm curious as to the relative performance.<o:p></o:p></p>
<div>
<div>
<p class="MsoNormal"><br>
<br>
On Jun 11, 2013, at 4:53 PM, <a href="mailto:eblosch@1scom.net">eblosch@1scom.net</a> wrote:<br>
<br>
&gt; Problem solved. I did not configure with --with-mxm=/opt/mellanox/mcm and<br>
&gt; this location was not auto-detected. &nbsp;Once I rebuilt with this option,<br>
&gt; everything worked fine. Scaled better than MVAPICH out to 800. MVAPICH<br>
&gt; configure log showed that it had found this component of the OFED stack.<br>
&gt;<br>
&gt; Ed<br>
&gt;<br>
&gt;<br>
&gt;&gt; If you run at 224 and things look okay, then I would suspect something in<br>
&gt;&gt; the upper level switch that spans cabinets. At that point, I'd have to<br>
&gt;&gt; leave it to Mellanox to advise.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; On Jun 11, 2013, at 6:55 AM, &quot;Blosch, Edwin L&quot; &lt;<a href="mailto:edwin.l.blosch@lmco.com">edwin.l.blosch@lmco.com</a>&gt;<br>
&gt;&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt;&gt; I tried adding &quot;-mca btl openib,sm,self&quot; &nbsp;but it did not make any<br>
&gt;&gt;&gt; difference.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Jesus&#8217; e-mail this morning has got me thinking. &nbsp;In our system, each<br>
&gt;&gt;&gt; cabinet has 224 cores, and we are reaching a different level of the<br>
&gt;&gt;&gt; system architecture when we go beyond 224. &nbsp;I got an additional data<br>
&gt;&gt;&gt; point at 256 and found that performance is already falling off. Perhaps<br>
&gt;&gt;&gt; I did not build OpenMPI properly to support the Mellanox adapters that<br>
&gt;&gt;&gt; are used in the backplane, or I need some configuration setting similar<br>
&gt;&gt;&gt; to FAQ #19 in the Tuning/Openfabrics section.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; From: <a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a> [mailto:<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>] On<br>
&gt;&gt;&gt; Behalf Of Ralph Castain<br>
&gt;&gt;&gt; Sent: Sunday, June 09, 2013 6:48 PM<br>
&gt;&gt;&gt; To: Open MPI Users<br>
&gt;&gt;&gt; Subject: Re: [OMPI users] EXTERNAL: Re: Need advice on performance<br>
&gt;&gt;&gt; problem<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Strange - it looks like a classic oversubscription behavior. Another<br>
&gt;&gt;&gt; possibility is that it isn't using IB for some reason when extended to<br>
&gt;&gt;&gt; the other nodes. What does your cmd line look like? Have you tried<br>
&gt;&gt;&gt; adding &quot;-mca btl openib,sm,self&quot; just to ensure it doesn't use TCP for<br>
&gt;&gt;&gt; some reason?<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Jun 9, 2013, at 4:31 PM, &quot;Blosch, Edwin L&quot; &lt;<a href="mailto:edwin.l.blosch@lmco.com">edwin.l.blosch@lmco.com</a>&gt;<br>
&gt;&gt;&gt; wrote:<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Correct. &nbsp;20 nodes, 8 cores per dual-socket on each node = 360.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; From: <a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a> [mailto:<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>] On<br>
&gt;&gt;&gt; Behalf Of Ralph Castain<br>
&gt;&gt;&gt; Sent: Sunday, June 09, 2013 6:18 PM<br>
&gt;&gt;&gt; To: Open MPI Users<br>
&gt;&gt;&gt; Subject: Re: [OMPI users] EXTERNAL: Re: Need advice on performance<br>
&gt;&gt;&gt; problem<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; So, just to be sure - when you run 320 &quot;cores&quot;, you are running across<br>
&gt;&gt;&gt; 20 nodes?<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Just want to ensure we are using &quot;core&quot; the same way - some people<br>
&gt;&gt;&gt; confuse cores with hyperthreads.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Jun 9, 2013, at 3:50 PM, &quot;Blosch, Edwin L&quot; &lt;<a href="mailto:edwin.l.blosch@lmco.com">edwin.l.blosch@lmco.com</a>&gt;<br>
&gt;&gt;&gt; wrote:<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; 16. &nbsp;dual-socket Xeon, E5-2670.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; I am trying a larger model to see if the performance drop-off happens at<br>
&gt;&gt;&gt; a different number of cores.<br>
&gt;&gt;&gt; Also I&#8217;m running some intermediate core-count sizes to refine the curve<br>
&gt;&gt;&gt; a bit.<br>
&gt;&gt;&gt; I also added mpi_show_mca_params all, and at the same time,<br>
&gt;&gt;&gt; btl_openib_use_eager_rdma 1, just to see if that does anything.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; From: <a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a> [mailto:<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>] On<br>
&gt;&gt;&gt; Behalf Of Ralph Castain<br>
&gt;&gt;&gt; Sent: Sunday, June 09, 2013 5:04 PM<br>
&gt;&gt;&gt; To: Open MPI Users<br>
&gt;&gt;&gt; Subject: EXTERNAL: Re: [OMPI users] Need advice on performance problem<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Looks to me like things are okay thru 160, and then things fall apart<br>
&gt;&gt;&gt; after that point. How many cores are on a node?<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Jun 9, 2013, at 1:59 PM, &quot;Blosch, Edwin L&quot; &lt;<a href="mailto:edwin.l.blosch@lmco.com">edwin.l.blosch@lmco.com</a>&gt;<br>
&gt;&gt;&gt; wrote:<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; I&#8217;m having some trouble getting good scaling with OpenMPI 1.6.4 and I<br>
&gt;&gt;&gt; don&#8217;t know where to start looking. This is an Infiniband FDR network<br>
&gt;&gt;&gt; with Sandy Bridge nodes. &nbsp;I am using affinity (--bind-to-core) but no<br>
&gt;&gt;&gt; other options. As the number of cores goes up, the message sizes are<br>
&gt;&gt;&gt; typically going down. There seem to be lots of options in the FAQ, and I<br>
&gt;&gt;&gt; would welcome any advice on where to start. &nbsp;All these timings are on a<br>
&gt;&gt;&gt; completely empty system except for me.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Thanks<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; &nbsp; &nbsp;MPI &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# cores &nbsp; Ave. Rate &nbsp; Std. Dev. % &nbsp;# timings<br>
&gt;&gt;&gt; Speedup &nbsp; &nbsp;Efficiency<br>
&gt;&gt;&gt; ================================================================================================<br>
&gt;&gt;&gt; MVAPICH &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp; 16 &nbsp; | &nbsp; &nbsp;8.6783 &nbsp;| &nbsp; 0.995 % | &nbsp; &nbsp; &nbsp; 2 &nbsp;|<br>
&gt;&gt;&gt; 16.000 &nbsp;| &nbsp;1.0000<br>
&gt;&gt;&gt; MVAPICH &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp; 48 &nbsp; | &nbsp; &nbsp;8.7665 &nbsp;| &nbsp; 1.937 % | &nbsp; &nbsp; &nbsp; 3 &nbsp;|<br>
&gt;&gt;&gt; 47.517 &nbsp;| &nbsp;0.9899<br>
&gt;&gt;&gt; MVAPICH &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp; 80 &nbsp; | &nbsp; &nbsp;8.8900 &nbsp;| &nbsp; 2.291 % | &nbsp; &nbsp; &nbsp; 3 &nbsp;|<br>
&gt;&gt;&gt; 78.095 &nbsp;| &nbsp;0.9762<br>
&gt;&gt;&gt; MVAPICH &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;160 &nbsp; | &nbsp; &nbsp;8.9897 &nbsp;| &nbsp; 2.409 % | &nbsp; &nbsp; &nbsp; 3 &nbsp;|<br>
&gt;&gt;&gt; 154.457 &nbsp;| &nbsp;0.9654<br>
&gt;&gt;&gt; MVAPICH &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;320 &nbsp; | &nbsp; &nbsp;8.9780 &nbsp;| &nbsp; 2.801 % | &nbsp; &nbsp; &nbsp; 3 &nbsp;|<br>
&gt;&gt;&gt; 309.317 &nbsp;| &nbsp;0.9666<br>
&gt;&gt;&gt; MVAPICH &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;480 &nbsp; | &nbsp; &nbsp;8.9704 &nbsp;| &nbsp; 2.316 % | &nbsp; &nbsp; &nbsp; 3 &nbsp;|<br>
&gt;&gt;&gt; 464.366 &nbsp;| &nbsp;0.9674<br>
&gt;&gt;&gt; MVAPICH &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;640 &nbsp; | &nbsp; &nbsp;9.0792 &nbsp;| &nbsp; 1.138 % | &nbsp; &nbsp; &nbsp; 3 &nbsp;|<br>
&gt;&gt;&gt; 611.739 &nbsp;| &nbsp;0.9558<br>
&gt;&gt;&gt; MVAPICH &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;720 &nbsp; | &nbsp; &nbsp;9.1328 &nbsp;| &nbsp; 1.052 % | &nbsp; &nbsp; &nbsp; 3 &nbsp;|<br>
&gt;&gt;&gt; 684.162 &nbsp;| &nbsp;0.9502<br>
&gt;&gt;&gt; MVAPICH &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;800 &nbsp; | &nbsp; &nbsp;9.1945 &nbsp;| &nbsp; 0.773 % | &nbsp; &nbsp; &nbsp; 3 &nbsp;|<br>
&gt;&gt;&gt; 755.079 &nbsp;| &nbsp;0.9438<br>
&gt;&gt;&gt; OpenMPI &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp; 16 &nbsp; | &nbsp; &nbsp;8.6743 &nbsp;| &nbsp; 2.335 % | &nbsp; &nbsp; &nbsp; 2 &nbsp;|<br>
&gt;&gt;&gt; 16.000 &nbsp;| &nbsp;1.0000<br>
&gt;&gt;&gt; OpenMPI &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp; 48 &nbsp; | &nbsp; &nbsp;8.7826 &nbsp;| &nbsp; 1.605 % | &nbsp; &nbsp; &nbsp; 2 &nbsp;|<br>
&gt;&gt;&gt; 47.408 &nbsp;| &nbsp;0.9877<br>
&gt;&gt;&gt; OpenMPI &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp; 80 &nbsp; | &nbsp; &nbsp;8.8861 &nbsp;| &nbsp; 0.120 % | &nbsp; &nbsp; &nbsp; 2 &nbsp;|<br>
&gt;&gt;&gt; 78.093 &nbsp;| &nbsp;0.9762<br>
&gt;&gt;&gt; OpenMPI &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;160 &nbsp; | &nbsp; &nbsp;8.9774 &nbsp;| &nbsp; 0.785 % | &nbsp; &nbsp; &nbsp; 2 &nbsp;|<br>
&gt;&gt;&gt; 154.598 &nbsp;| &nbsp;0.9662<br>
&gt;&gt;&gt; OpenMPI &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;320 &nbsp; | &nbsp; 12.0585 &nbsp;| &nbsp;16.950 % | &nbsp; &nbsp; &nbsp; 2 &nbsp;|<br>
&gt;&gt;&gt; 230.191 &nbsp;| &nbsp;0.7193<br>
&gt;&gt;&gt; OpenMPI &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;480 &nbsp; | &nbsp; 14.8330 &nbsp;| &nbsp; 1.300 % | &nbsp; &nbsp; &nbsp; 2 &nbsp;|<br>
&gt;&gt;&gt; 280.701 &nbsp;| &nbsp;0.5848<br>
&gt;&gt;&gt; OpenMPI &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;640 &nbsp; | &nbsp; 17.1723 &nbsp;| &nbsp; 2.577 % | &nbsp; &nbsp; &nbsp; 3 &nbsp;|<br>
&gt;&gt;&gt; 323.283 &nbsp;| &nbsp;0.5051<br>
&gt;&gt;&gt; OpenMPI &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;720 &nbsp; | &nbsp; 18.2153 &nbsp;| &nbsp; 2.798 % | &nbsp; &nbsp; &nbsp; 3 &nbsp;|<br>
&gt;&gt;&gt; 342.868 &nbsp;| &nbsp;0.4762<br>
&gt;&gt;&gt; OpenMPI &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;800 &nbsp; | &nbsp; 19.3603 &nbsp;| &nbsp; 2.254 % | &nbsp; &nbsp; &nbsp; 3 &nbsp;|<br>
&gt;&gt;&gt; 358.434 &nbsp;| &nbsp;0.4480<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><o:p></o:p></p>
</div>
</div>
</div>
<p class="MsoNormal"><o:p>&nbsp;</o:p></p>
</div>
</div>
</body>
</html>

