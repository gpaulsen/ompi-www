<html><head><meta http-equiv="Content-Type" content="text/html charset=iso-8859-1"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">Afraid I have no idea, but hopefully someone else here with experience with HDF5 can chime in?<div><br></div><div><br><div><div>On Jan 17, 2014, at 9:03 AM, Ronald Cohen &lt;<a href="mailto:rhcohen@lbl.gov">rhcohen@lbl.gov</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr"><div><div>Still a timely response, thank you.&nbsp;&nbsp;&nbsp; The particular problem I noted hasn't recurred; for reasons I will explain shortly I had to rebuild openmpi again, and this time Sample_mpio.c compiled and ran successfully from the start.<br>
<br>But now my problem is trying to get parallel HDF5 to run.&nbsp; In my first attempt to build HDF5 it failed in the load stage because of unsatisifed externals from openmpi, and I deduced the problem was having built openmpi with --disable-static.&nbsp;&nbsp; So I rebuilt with --enable-static and --disable-dlopen (emulating a successful openmpi + hdf5 combination I had built on Snow Leopard).&nbsp;&nbsp; Once again openmpi passed its make check's, and as noted above the Sample_mpio.c test compiled and ran fine.&nbsp;&nbsp; And the parallel hdf5 configure and make steps ran successfully.&nbsp;&nbsp; But when I ran make check for hdf5, the serial tests passed but none of the parallel tests did.&nbsp; Over a million test failures!&nbsp; Error messages like:<br>
<br>Proc 0: *** MPIO File size range test...<br>--------------------------------<br>MPI_Offset is signed 8 bytes integeral type<br>MPIO GB file write test MPItest.h5<br>MPIO GB file write test MPItest.h5<br>MPIO GB file write test MPItest.h5<br>
MPIO GB file write test MPItest.h5<br>MPIO GB file write test MPItest.h5<br>MPIO GB file write test MPItest.h5<br>MPIO GB file read test MPItest.h5<br>MPIO GB file read test MPItest.h5<br>MPIO GB file read test MPItest.h5<br>
MPIO GB file read test MPItest.h5<br>proc 3: found data error at [2141192192+0], expect -6, got 5<br>proc 3: found data error at [2141192192+1], expect -6, got 5<br><br></div><div>And -- the specific errors reported, which processor, which location, and the total number of errors changes if I rerun make check.<br>
</div><div><br></div>I've sent configure, make and make check logs to the HDF5 help desk but haven't gotten a response.<br></div><div><br></div><div>I am now configuring openmpi (still 1.7.4rc1) with: <br><br>./configure --prefix=/usr/local/openmpi CC=gcc CXX=g++ FC=gfortran F77=gfortran --enable-static --with-pic --disable-dlopen --enable-mpirun-prefix-by-default<br>
<br></div><div>and configuring HDF5 (version 1.8.12) with:<br><br>configure --prefix=/usr/local/hdf5/par CC=mpicc CFLAGS=-fPIC FC=mpif90 FCFLAGS=-fPIC CXX=mpicxx CXXFLAGS=-fPIC --enable-parallel --enable-fortran<br><br></div>
<div>This is the combination that worked for me with Snow Leopard (though it was then earlier versions of both openmpi and hdf5.)<br><br></div><div>If it matters, the gcc is the stock one with Mavericks' XCode, and gfortran is 4.9.0.<br>
</div><div><br></div><div>(I just noticed that the mpi fortran wrapper is now mpifort, but I also see that mpif90 is still there and is a just link to mpifort.)<br><br>Any suggestions?<br></div></div><div class="gmail_extra">
<br><br><div class="gmail_quote">On Fri, Jan 17, 2014 at 8:14 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div style="word-wrap:break-word">sorry for delayed response - just getting back from travel. I don't know why you would get that behavior other than a race condition. Afraid that code path is foreign to me, but perhaps one of the folks in the MPI-IO area can respond<div>
<br></div><div><br><div><div>On Jan 15, 2014, at 4:26 PM, Ronald Cohen &lt;<a href="mailto:rhcohen@lbl.gov" target="_blank">rhcohen@lbl.gov</a>&gt; wrote:</div><br><blockquote type="cite"><div dir="ltr"><div>Update: I reconfigured with enable_io_romio=yes, and this time -- mostly -- the test using Sample_mpio.c&nbsp; passes.&nbsp;&nbsp; Oddly the very first time I tried I got errors:&nbsp; <br>
<br>% mpirun -np 2 sampleio <br>Proc 1: hostname=Ron-Cohen-MBP.local<br>
Testing simple C MPIO program with 2 processes accessing file ./mpitest.data<br>&nbsp;&nbsp;&nbsp; (Filename can be specified via program argument)<br>Proc 0: hostname=Ron-Cohen-MBP.local<br>Proc 1: read data[0:1] got 0, expect 1<br>Proc 1: read data[0:2] got 0, expect 2<br>

Proc 1: read data[0:3] got 0, expect 3<br>Proc 1: read data[0:4] got 0, expect 4<br>Proc 1: read data[0:5] got 0, expect 5<br>Proc 1: read data[0:6] got 0, expect 6<br>Proc 1: read data[0:7] got 0, expect 7<br>Proc 1: read data[0:8] got 0, expect 8<br>

Proc 1: read data[0:9] got 0, expect 9<br>Proc 1: read data[1:0] got 0, expect 10<br>Proc 1: read data[1:1] got 0, expect 11<br>Proc 1: read data[1:2] got 0, expect 12<br>Proc 1: read data[1:3] got 0, expect 13<br>Proc 1: read data[1:4] got 0, expect 14<br>

Proc 1: read data[1:5] got 0, expect 15<br>Proc 1: read data[1:6] got 0, expect 16<br>Proc 1: read data[1:7] got 0, expect 17<br>Proc 1: read data[1:8] got 0, expect 18<br>Proc 1: read data[1:9] got 0, expect 19<br>--------------------------------------------------------------------------<br>

MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD <br>with errorcode 1.<br><br></div>But when I reran the same mpirun command, the test was successful.&nbsp;&nbsp; And deleting the executable and recompiling and then again running the same mpirun command, the test was successful.&nbsp;&nbsp; Can someone explain that?<br>

<br><br></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Wed, Jan 15, 2014 at 1:16 PM, Ronald Cohen <span dir="ltr">&lt;<a href="mailto:rhcohen@lbl.gov" target="_blank">rhcohen@lbl.gov</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div>Aha.&nbsp;&nbsp; I guess I didn't know what the io-romio option does.&nbsp;&nbsp; If you look at my config.log you will see my configure line included --disable-io-romio.&nbsp;&nbsp;&nbsp; Guess I should change --disable to --enable.<br>


<br></div>You seem to imply that the nightly build is stable enough that I should probably switch to that rather than 1.7.4rc1.&nbsp;&nbsp; Am I reading between the lines correctly?<br><div><br></div></div><div class="gmail_extra">


<br><br><div class="gmail_quote">On Wed, Jan 15, 2014 at 10:56 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">


<div dir="ltr">Oh, a word of caution on those config params - you might need to check to ensure I don't disable romio in them. I don't normally build it as I don't use it. Since that is what you are trying to use, just change the "no" to "yes" (or delete that line altogether) and it will build.<div>



<br></div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Wed, Jan 15, 2014 at 10:53 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>



<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">You can find my configure options in the OMPI distribution at contrib/platform/intel/bend/mac. You are welcome to use them - just configure --with-platform=intel/bend/mac<div>



<br></div><div>I work on the developer's trunk, of course, but also run the head of the 1.7.4 branch (essentially the nightly tarball) on a fairly regular basis.</div>
<div><br></div><div>As for the opal_bitmap test: it wouldn't surprise me if that one was stale. I can check on it later tonight, but I'd suspect that the test is bad as we use that class in the code base and haven't seen an issue.</div>




<div><br></div></div><div><div class="gmail_extra"><br><br><div class="gmail_quote">On Wed, Jan 15, 2014 at 10:49 AM, Ronald Cohen <span dir="ltr">&lt;<a href="mailto:rhcohen@lbl.gov" target="_blank">rhcohen@lbl.gov</a>&gt;</span> wrote:<br>




<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><div>Ralph,<br></div><div><br>I just sent out another post with the c file attached.<br><br></div>



If you can get that to work, and even if you can't can you tell me what configure options you use, and what version of open-mpi?&nbsp;&nbsp; Thanks.<br>

<br></div>Ron<br></div><div><div class="gmail_extra"><br><br><div class="gmail_quote">On Wed, Jan 15, 2014 at 10:36 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>





<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">BTW: could you send me your sample test code?</div><div class="gmail_extra"><br><br><div class="gmail_quote">





On Wed, Jan 15, 2014 at 10:34 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">I regularly build on Mavericks and run without problem, though I haven't tried a parallel IO app. I'll give yours a try later, when I get back to my Mac.<div>






<br></div></div><div class="gmail_extra">
<br><br><div class="gmail_quote"><div>On Wed, Jan 15, 2014 at 10:04 AM, Ronald Cohen <span dir="ltr">&lt;<a href="mailto:rhcohen@lbl.gov" target="_blank">rhcohen@lbl.gov</a>&gt;</span> wrote:<br></div>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div>
<div dir="ltr">I have been struggling trying to get a usable build of openmpi on Mac 
OSX Mavericks (10.9.1).&nbsp; I can get openmpi to configure and build 
without error, but have problems after that which depend on the openmpi 
version.&nbsp; <br>
<div><div><div><div><div>

<br>With 1.6.5, make check fails the opal_datatype_test, ddt_test, and 
ddt_raw tests.&nbsp; The various atomic_* tests pass.&nbsp;&nbsp;&nbsp; See checklogs_1.6.5,
 attached as a .gz file.<br><br></div>Following suggestions from openmpi
 discussions I tried openmpi version 1.7.4rc1.&nbsp; In this case make check 
indicates all tests passed.&nbsp; But when I proceeded to try to build a 
parallel code (parallel HDF5) it failed.&nbsp; Following an email exchange 
with the HDF5 support people, they suggested I try to compile and run 
the attached bit of simple code Sample_mpio.c (which they supplied) 
which does not use any HDF5, but just attempts a parallel write to a 
file and parallel read.&nbsp;&nbsp; That test failed when requesting more than 1 
processor -- which they say indicates a failure of the openmpi 
installation.&nbsp;&nbsp; The error message was:<br>

<br>MPI_INIT: argc 1<br>MPI_INIT: argc 1<br>Testing simple C MPIO program with 2 processes accessing file ./mpitest.data<br>&nbsp;&nbsp;&nbsp; (Filename can be specified via program argument)<br>Proc 0: hostname=Ron-Cohen-MBP.local<br>









Proc 1: hostname=Ron-Cohen-MBP.local<br>
MPI_BARRIER[0]: comm MPI_COMM_WORLD<br>MPI_BARRIER[1]: comm MPI_COMM_WORLD<br>Proc 0: MPI_File_open with MPI_MODE_EXCL failed (MPI_ERR_FILE: invalid file)<br>MPI_ABORT[0]: comm MPI_COMM_WORLD errorcode 1<br>MPI_BCAST[1]: buffer 7fff5a483048 count 1 datatype MPI_INT root 0 comm MPI_COMM_WORLD<br>










<br>
I then went back to my openmpi directories and tried running some of the
 individual tests in the test and examples directories.&nbsp; In particular 
in test/class I found one test that seem to not be run as part of make 
check which failed, even with one processor; this is opal_bitmap.&nbsp; Not 
sure if this is because 1.7.4rc1 is incomplete, or there is something 
wrong with the installation, or maybe a 32 vs 64 bit thing?&nbsp;&nbsp; The error 
message is <br>


<br>mpirun detected that one or more processes exited with non-zero status, thus causing the job to be terminated. The first process to do so was:<br><br>&nbsp; Process name: [[48805,1],0]<br>&nbsp; Exit code:&nbsp;&nbsp;&nbsp; 255<br><br></div>











Any suggestions?<br><br></div>More generally has anyone out there gotten
 an openmpi build on Mavericks to work with sufficient success that they
 can get the attached Sample_mpio.c (or better yet, parallel HDF5) to 
build?&nbsp; <br>


<br></div>Details: Running Mac OS X 10.9.1 on a mid-2009 Macbook pro 
with 4 GB memory; tried openmpi 1.6.5 and 1.7.4rc1.&nbsp; Built openmpi 
against the stock gcc that comes with XCode 5.0.2, and gfortran 4.9.0.&nbsp; <br><br></div>

<div>Files attached: config.log.gz, openmpialllog.gz (output of running 
ompi_info --all), checklog2.gz (output of make.check in top openmpi 
directory).&nbsp; <br><br></div>I am not attaching logs of make and 
install since those seem to have been successful, but can generate those
 if that would be helpful.</div>
<br></div>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>
</blockquote></div><br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>
</div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>
</div></blockquote></div><br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>
</blockquote></div><br></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
