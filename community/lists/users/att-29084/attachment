<div dir="ltr"><div>Hi there,</div><div><br></div><div>I am using multiple MPI non-blocking send receives on the GPU buffer followed by a waitall at the end; I also repeat this process multiple times.</div><div><br></div><div>The MPI version that I am using 1.10.2.</div><div><br></div><div>When multiple processes are assigned to a single GPU (or when CUDA IPC is used), I get the following error at the beginning </div><div><br></div><div>The call to cuIpcGetEventHandle failed. This is a unrecoverable error and will<br></div><div><div>cause the program to abort.</div><div>  cuIpcGetEventHandle return value:   1</div></div><div><br></div><div>and this at the end of my benchmark</div><div><br></div><div><div>The call to cuEventDestory failed. This is a unrecoverable error and will</div><div>cause the program to abort.</div><div>  cuEventDestory return value:   400</div><div>Check the cuda.h file for what the return value means.</div></div><div><br></div><div><br></div><div><b>Note1: </b></div><div><b><br></b></div><div>This error doesn&#39;t appear if only one iteration of the non-blocking send/receive call is used (i.e., using MPI_Waitall only once )</div><div><br></div><div>This error doesn&#39;t appear if multiple iterations are used by MPI_Waitall is not included.</div><div><br></div><div><b>Note 2:</b></div><div><b><br></b></div><div>This error doesn&#39;t exist if the buffer is is allocated on the host.</div><div><br></div><div><b>Note 3:</b></div><div><b><br></b></div><div>This error doesn&#39;t exist if cuda_ipc is disabled or OMPI version 1.8.8 is used.</div><div><br></div><div><br></div><div>I&#39;d appreciate if you let me know what causes this issue and how it can be resolved.</div><div><br></div><div>Regards,</div><div>Iman</div></div>

