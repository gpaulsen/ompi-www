<html>
  <head>
    <meta content="text/html; charset=ISO-8859-1"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    Thanks all for your answers, I've added some details about the tests
    I have run.&nbsp; See below.<br>
    <br>
    <br>
    Ralph Castain wrote:
    <blockquote type="cite">
      <meta http-equiv="Content-Type" content="text/html;
        charset=ISO-8859-1">
      Not precisely correct. It depends on the environment.
      <div class=""><br class="">
      </div>
      <div class="">If there is a resource manager allocating nodes, or
        you provide a hostfile that specifies the number of slots on the
        nodes, or you use -host, then we default to no-oversubscribe.</div>
    </blockquote>
    I'm using a batch scheduler (OAR). <br>
    <big><tt># cat /dev/cpuset/oar/begou_7955553/cpuset.cpus</tt><tt><br>
      </tt><tt>4-7</tt><tt><br>
      </tt></big><br>
    So 4 cores allowed. Nodes have two height cores cpus.<br>
    <br>
    Node file contains:<br>
    <big><tt># cat $OAR_NODEFILE</tt><tt><br>
      </tt><tt>frog53</tt><tt><br>
      </tt><tt>frog53</tt><tt><br>
      </tt><tt>frog53</tt><tt><br>
      </tt><tt>frog53</tt><tt><br>
      </tt></big><br>
    <big><tt># mpirun --hostfile $OAR_NODEFILE -bind-to core
        location.exe </tt><tt><br>
      </tt></big>is&nbsp; okay (my test code show one process on each core)<br>
    <big><tt>(process 3) thread is now running on PU logical index 1
        (OS/physical index 5) on system frog53</tt><tt><br>
      </tt><tt>(process 0) thread is now running on PU logical index 3
        (OS/physical index 7) on system frog53</tt><tt><br>
      </tt><tt>(process 1) thread is now running on PU logical index 0
        (OS/physical index 4) on system frog53</tt><tt><br>
      </tt><tt>(process 2) thread is now running on PU logical index 2
        (OS/physical index 6) on system frog53</tt><tt><br>
      </tt><tt><br>
      </tt><tt># mpirun -np 5 --hostfile $OAR_NODEFILE -bind-to core
        location.exe </tt><tt><br>
      </tt></big>oversuscribe with:<br>
    <big><tt>(process 0) thread is now running on PU logical index 3
        (OS/physical index 7) on system frog53</tt><tt><br>
      </tt><tt>(process 1) thread is now running on PU logical index 1
        (OS/physical index 5) on system frog53</tt><tt><br>
      </tt><tt>(<b>process 3</b>) thread is now running on PU logical
        index <b>2 (OS/physical index 6)</b> on system frog53</tt><tt><br>
      </tt><tt>(process 4) thread is now running on PU logical index 0
        (OS/physical index 4) on system frog53</tt><tt><br>
      </tt><tt>(<b>process 2</b>) thread is now running on PU logical
        index <b>2 (OS/physical index 6)</b> on system frog53</tt><tt><br>
      </tt></big>This is not allowed with OpenMPI 1.7.3<br>
    <br>
    I can increase until the maximul core number of this first pocessor
    (8 cores) <br>
    <big><tt># mpirun -np 8 --hostfile $OAR_NODEFILE -bind-to core
        location.exe |grep 'thread is now running on PU'</tt><tt><br>
      </tt><tt>(process 5) thread is now running on PU logical index 1
        (OS/physical index 5) on system frog53</tt><tt><br>
      </tt><tt>(process 7) thread is now running on PU logical index 3
        (OS/physical index 7) on system frog53</tt><tt><br>
      </tt><tt>(process 4) thread is now running on PU logical index 0
        (OS/physical index 4) on system frog53</tt><tt><br>
      </tt><tt>(process 6) thread is now running on PU logical index 2
        (OS/physical index 6) on system frog53</tt><tt><br>
      </tt><tt>(process 2) thread is now running on PU logical index 1
        (OS/physical index 5) on system frog53</tt><tt><br>
      </tt><tt>(process 0) thread is now running on PU logical index 2
        (OS/physical index 6) on system frog53</tt><tt><br>
      </tt><tt>(process 1) thread is now running on PU logical index 0
        (OS/physical index 4) on system frog53</tt><tt><br>
      </tt><tt>(process 3) thread is now running on PU logical index 0
        (OS/physical index 4) on system frog53</tt><tt><br>
      </tt></big><br>
    But I cannot overload more than the 8 cores (max core number of one
    cpu). <br>
    <big><tt># </tt><tt>mpirun -np 9 --hostfile $OAR_NODEFILE -bind-to
        core location.exe</tt><tt><br>
      </tt><tt>A request was made to bind to that would result in
        binding more</tt><tt><br>
      </tt><tt>processes than cpus on a resource:</tt><tt><br>
      </tt><tt><br>
      </tt><tt>&nbsp;&nbsp; Bind to:&nbsp;&nbsp;&nbsp;&nbsp; CORE</tt><tt><br>
      </tt><tt>&nbsp;&nbsp; Node:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; frog53</tt><tt><br>
      </tt><tt>&nbsp;&nbsp; #processes:&nbsp; 2</tt><tt><br>
      </tt><tt>&nbsp;&nbsp; #cpus:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1</tt><tt><br>
      </tt><tt><br>
      </tt><tt>You can override this protection by adding the
        "overload-allowed"</tt><tt><br>
      </tt><tt>option to your binding directive.</tt><tt><br>
      </tt></big><br>
    Now if I add <b>--nooversubscribe</b> the problem doesn't exist
    anymore (no more than 4 processes, one on each core). So looks like
    if default behavior would be a nooversuscribe on cores number of the
    socket ???<br>
    <br>
    Again, with 1.7.3 this problem doesn't occur at all.<br>
    <br>
    Patrick<br>
    <br>
    <br>
    <blockquote type="cite">
      <div class="">
        <div class=""><br class="">
        </div>
        <div class="">If you provide a hostfile that doesn&#8217;t specify
          slots, then we use the number of cores we find on each node,
          and we allow oversubscription.</div>
        <div class=""><br class="">
        </div>
        <div class="">What is being described sounds like more of a bug
          than an intended feature. I&#8217;d need to know more about it,
          though, to be sure. Can you tell me how you are specifying
          this cpuset?</div>
        <div class=""><br class="">
        </div>
        <div class=""><br class="">
          <div>
            <blockquote type="cite" class="">
              <div class="">On Sep 15, 2015, at 4:44 PM, Matt Thompson
                &lt;<a href="mailto:fortran@gmail.com" class="">fortran@gmail.com</a>&gt;
                wrote:</div>
              <br class="Apple-interchange-newline">
              <div class="">
                <div dir="ltr" class="">Looking at the Open MPI 1.10.0
                  man page:
                  <div class=""><br class="">
                  </div>
                  <div class="">&nbsp; <a
                      href="https://www.open-mpi.org/doc/v1.10/man1/mpirun.1.php"
                      class="">https://www.open-mpi.org/doc/v1.10/man1/mpirun.1.php</a><br
                      class="">
                  </div>
                  <div class=""><br class="">
                  </div>
                  <div class="">it looks like perhaps -oversubscribe
                    (which was an option) is now the default behavior.
                    Instead we have:</div>
                  <div class=""><br class="">
                  </div>
                  <div class=""><dt style="font-family: verdana, arial,
                      helvetica; font-size: 12px;" class=""><b class="">-nooversubscribe,
                        --nooversubscribe</b></dt>
                    <dd style="font-family: verdana, arial, helvetica;
                      font-size: 12px;" class="">Do not oversubscribe
                      any nodes; error (without starting any processes)
                      if the requested number of processes would cause
                      oversubscription. This option implicitly sets
                      "max_slots" equal to the "slots" value for each
                      node.</dd>
                  </div>
                  <div class=""><br class="">
                  </div>
                  <div class="">
                    <div class="">It also looks like -map-by has a way
                      to implement it as well (see man page).</div>
                    <div class=""><br class="">
                    </div>
                    <div class="">Thanks for letting me/us know about
                      this. On a system of mine I sort of depend on the
                      -nooversubscribe behavior!</div>
                    <div class=""><br class="">
                    </div>
                    <div class="">Matt</div>
                    <div class="">&nbsp;</div>
                  </div>
                  <div class=""><br class="">
                  </div>
                  <div class="gmail_extra"><br class="">
                    <div class="gmail_quote">On Tue, Sep 15, 2015 at
                      11:17 AM, Patrick Begou <span dir="ltr" class="">&lt;<a
href="mailto:Patrick.Begou@legi.grenoble-inp.fr" target="_blank"
                          class="">Patrick.Begou@legi.grenoble-inp.fr</a>&gt;</span>
                      wrote:<br class="">
                      <blockquote class="gmail_quote" style="margin:0px
                        0px 0px
0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">
                        <div bgcolor="#FFFFFF" text="#000000" class="">
                          Hi,<br class="">
                          <br class="">
                          I'm runing OpenMPI 1.10.0 built with Intel
                          2015 compilers on a Bullx System.<br class="">
                          I've some troubles with the bind-to core
                          option when using cpuset. <br class="">
                          If the cpuset is less than all the cores of a
                          cpu (ex: 4 cores allowed on a 8 cores cpus)
                          OpenMPI 1.10.0 allows to overload these cores&nbsp;
                          until the maximum number of cores of the cpu.<br
                            class="">
                          With this config and because the cpuset only
                          allows 4 cores, I can reach 2 processes/core
                          if I use:<br class="">
                          <br class="">
                          <tt class="">mpirun -np 8 --bind-to core
                            my_application</tt><br class="">
                          <br class="">
                          OpenMPI 1.7.3 doesn't show the problem with
                          the same situation:<br class="">
                          <tt class="">mpirun -np 8 --bind-to-core
                            my_application</tt><br class="">
                          returns:<br class="">
                          <i class="">A request was made to bind to that
                            would result in binding more</i><i class=""><br
                              class="">
                          </i><i class="">processes than cpus on a
                            resource</i><br class="">
                          and that's okay of course.<br class="">
                          <br class="">
                          <br class="">
                          Is there a way to avoid this oveloading with
                          OpenMPI 1.10.0 ?<br class="">
                          <br class="">
                          Thanks<span class=""><font class=""
                              color="#888888"><br class="">
                              <br class="">
                              Patrick<br class="">
                              <br class="">
                              <pre class="" cols="80">-- 
===================================================================
|  Equipe M.O.S.T.         |                                      |
|  Patrick BEGOU           | <a href="mailto:Patrick.Begou@grenoble-inp.fr" target="_blank" class="">mailto:Patrick.Begou@grenoble-inp.fr</a> |
|  LEGI                    |                                      |
|  BP 53 X                 | Tel 04 76 82 51 35                   |
|  38041 GRENOBLE CEDEX    | Fax 04 76 82 52 71                   |
===================================================================
</pre>
                            </font></span></div>
                        <br class="">
                        _______________________________________________<br
                          class="">
                        users mailing list<br class="">
                        <a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br
                          class="">
                        Subscription: <a
                          href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
                          rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br
                          class="">
                        Link to this post: <a
                          href="http://www.open-mpi.org/community/lists/users/2015/09/27575.php"
                          rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/09/27575.php</a><br
                          class="">
                      </blockquote>
                    </div>
                    <br class="">
                    <br class="" clear="all">
                    <div class=""><br class="">
                    </div>
                    -- <br class="">
                    <div class="gmail_signature">
                      <div dir="ltr" class="">
                        <div class="">
                          <div dir="ltr" class="">
                            <div class="">Matt Thompson</div>
                          </div>
                        </div>
                        <blockquote style="margin:0px 0px 0px
                          40px;border:none;padding:0px" class="">
                          <div class="">
                            <div class="">
                              <div class="">Man Among Men</div>
                            </div>
                          </div>
                          <div class="">
                            <div class="">
                              <div class="">Fulcrum of History</div>
                            </div>
                          </div>
                        </blockquote>
                      </div>
                    </div>
                  </div>
                </div>
                _______________________________________________<br
                  class="">
                users mailing list<br class="">
                <a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br
                  class="">
                Subscription:
                <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br
                  class="">
                Link to this post:
                <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2015/09/27579.php">http://www.open-mpi.org/community/lists/users/2015/09/27579.php</a></div>
            </blockquote>
          </div>
          <br class="">
        </div>
      </div>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2015/09/27580.php">http://www.open-mpi.org/community/lists/users/2015/09/27580.php</a></pre>
    </blockquote>
    <br>
    <br>
    <pre class="moz-signature" cols="80">-- 
===================================================================
|  Equipe M.O.S.T.         |                                      |
|  Patrick BEGOU           | <a class="moz-txt-link-freetext" href="mailto:Patrick.Begou@grenoble-inp.fr">mailto:Patrick.Begou@grenoble-inp.fr</a> |
|  LEGI                    |                                      |
|  BP 53 X                 | Tel 04 76 82 51 35                   |
|  38041 GRENOBLE CEDEX    | Fax 04 76 82 52 71                   |
===================================================================
</pre>
  </body>
</html>

