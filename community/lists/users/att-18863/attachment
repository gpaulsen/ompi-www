<html>
<head>
<style><!--
.hmmessage P
{
margin:0px;
padding:0px
}
body.hmmessage
{
font-size: 10pt;
font-family:Tahoma
}
--></style></head>
<body class='hmmessage'><div dir='ltr'>
Hi,<br><br>Thanks that works fine when I submit hello program but when I tried to benchmark the system it look like it does not do anything&nbsp; <br>mpirun -np 8 --hostfile hosts xhpl<br><br>Regards,<br><br><br><div><div id="SkyDrivePlaceholder"></div>&gt; From: reuti@staff.uni-marburg.de<br>&gt; Date: Wed, 28 Mar 2012 17:40:07 +0200<br>&gt; To: users@open-mpi.org<br>&gt; Subject: Re: [OMPI users] Can not run a parallel job on all the nodes in the	cluster<br>&gt; <br>&gt; Am 28.03.2012 um 17:35 schrieb Hameed Alzahrani:<br>&gt; <br>&gt; &gt; Hi,<br>&gt; &gt; <br>&gt; &gt; Is there a specific name or location for the hostfile because I could not figure how to specify the number of processors for each machine in the command line.<br>&gt; <br>&gt; No, just specify the name (or path) to it with:<br>&gt; <br>&gt; --hostfile foobar<br>&gt; <br>&gt; -- Reuti<br>&gt; <br>&gt; <br>&gt; &gt; Regards, <br>&gt; &gt; <br>&gt; &gt; &gt; From: reuti@staff.uni-marburg.de<br>&gt; &gt; &gt; Date: Wed, 28 Mar 2012 17:21:39 +0200<br>&gt; &gt; &gt; To: users@open-mpi.org<br>&gt; &gt; &gt; Subject: Re: [OMPI users] Can not run a parallel job on all the nodes in the	cluster<br>&gt; &gt; &gt; <br>&gt; &gt; &gt; Hi,<br>&gt; &gt; &gt; <br>&gt; &gt; &gt; Am 28.03.2012 um 16:55 schrieb Hameed Alzahrani:<br>&gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; I ran hello program which return the host name when I run it using <br>&gt; &gt; &gt; &gt; mpirun -np 8 hello<br>&gt; &gt; &gt; &gt; all the 8 answer returned from the same machine<br>&gt; &gt; &gt; &gt; when I run it using <br>&gt; &gt; &gt; &gt; mpirun -np 8 --host host1,host2,host3 hello<br>&gt; &gt; &gt; &gt; I got answers from all the machines but it is not from all processors because I have 8 processors host1=4, host2=2, host3=2 the answer was 3 from host1, 3 from host2 and 2 from host3.<br>&gt; &gt; &gt; <br>&gt; &gt; &gt; If you want to specify the number of slots you can put it in a hostfile (otherwise a round robin assignment is just used). I'm not aware that it can be specified on the command line with different values for each machine:<br>&gt; &gt; &gt; <br>&gt; &gt; &gt; host1 slots=4<br>&gt; &gt; &gt; host2 slots=2<br>&gt; &gt; &gt; host3 slots=2<br>&gt; &gt; &gt; <br>&gt; &gt; &gt; -- Reuti<br>&gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; Regards,<br>&gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; From: reuti@staff.uni-marburg.de<br>&gt; &gt; &gt; &gt; &gt; Date: Wed, 28 Mar 2012 16:42:21 +0200<br>&gt; &gt; &gt; &gt; &gt; To: users@open-mpi.org<br>&gt; &gt; &gt; &gt; &gt; Subject: Re: [OMPI users] Can not run a parallel job on all the nodes in the	cluster<br>&gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; Hi,<br>&gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; Am 28.03.2012 um 16:30 schrieb Hameed Alzahrani:<br>&gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; &gt; Hi,<br>&gt; &gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; &gt; I mean the node that I run mpirun command from. I use condor as a scheduler but I need to benchmark the cluster either from condor or directly from open MPI.<br>&gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; I can't say anything regarding the Condor integration of Open MPI, but starting it directly by mpirun and supplying a valid number of ranks and hostfile should start some processes on other machines as requested. Can you run a plain mpihello first and output rank and hostname? Do you have ssh access to all the machines in questions? You have a shared home directory with the applications?<br>&gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; -- Reuti<br>&gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; &gt; when I ran mpirun from a machine and checking the memory status for the three machines that I have it appear that the memory usage increased just in the same machine.<br>&gt; &gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; &gt; Regards,<br>&gt; &gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; From: reuti@staff.uni-marburg.de<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; Date: Wed, 28 Mar 2012 15:12:17 +0200<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; To: users@open-mpi.org<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; Subject: Re: [OMPI users] Can not run a parallel job on all the nodes in the	cluster<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; Hi,<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; Am 27.03.2012 um 23:46 schrieb Hameed Alzahrani:<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; When I run any parallel job I get the answer just from the submitting node<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; what do you mean by submitting node: you use a queuing system - which one?<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; -- Reuti<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; even when I tried to benchmark the cluster using LINPACK but it look like the job just working on the submitting node is there a way to make openMPI send the job equally to all the nodes depending on the number of processor in the current mode even if I specify that the job should use 8 processor it look like openMPI use the submitting node 4 processors instead of using the other processors. I tried also --host but it does not work correctly in benchmarking the cluster so does any one use openMPI in benchmarking a cluster or does any one knows how to make openMPI divids the parallel job equally to every processor on the cluster.<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Regards, <br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; _______________________________________________<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; users mailing list<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; users@open-mpi.org<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; _______________________________________________<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; users mailing list<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; users@open-mpi.org<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt; &gt; &gt; &gt; &gt; _______________________________________________<br>&gt; &gt; &gt; &gt; &gt; &gt; users mailing list<br>&gt; &gt; &gt; &gt; &gt; &gt; users@open-mpi.org<br>&gt; &gt; &gt; &gt; &gt; &gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; <br>&gt; &gt; &gt; &gt; &gt; _______________________________________________<br>&gt; &gt; &gt; &gt; &gt; users mailing list<br>&gt; &gt; &gt; &gt; &gt; users@open-mpi.org<br>&gt; &gt; &gt; &gt; &gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt; &gt; &gt; _______________________________________________<br>&gt; &gt; &gt; &gt; users mailing list<br>&gt; &gt; &gt; &gt; users@open-mpi.org<br>&gt; &gt; &gt; &gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt; &gt; <br>&gt; &gt; &gt; <br>&gt; &gt; &gt; _______________________________________________<br>&gt; &gt; &gt; users mailing list<br>&gt; &gt; &gt; users@open-mpi.org<br>&gt; &gt; &gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt; _______________________________________________<br>&gt; &gt; users mailing list<br>&gt; &gt; users@open-mpi.org<br>&gt; &gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; <br>&gt; <br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; users@open-mpi.org<br>&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br></div> 		 	   		  </div></body>
</html>
