<html><body><div style="color:#000; background-color:#fff; font-family:times new roman, new york, times, serif;font-size:10pt">I have mentioned it two times that by doing this we got good results and the question was just in terms of process migration ...... I have explained the current work as well. <br><br>regards,<br>Mudassar<br><br><div style="font-family: times new roman, new york, times, serif; font-size: 10pt;"><div style="font-family: times new roman, new york, times, serif; font-size: 12pt;"><font face="Arial" size="2"><hr size="1"><b><span style="font-weight:bold;">From:</span></b> "users-request@open-mpi.org" &lt;users-request@open-mpi.org&gt;<br><b><span style="font-weight: bold;">To:</span></b> users@open-mpi.org<br><b><span style="font-weight: bold;">Sent:</span></b> Thursday, November 10, 2011 7:48 PM<br><b><span style="font-weight: bold;">Subject:</span></b> users Digest, Vol 2064, Issue 3<br></font><br>
Send users mailing list submissions to<br>&nbsp;&nbsp;&nbsp; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><br>To subscribe or unsubscribe via the World Wide Web, visit<br>&nbsp;&nbsp;&nbsp; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>or, via email, send a message with subject or body 'help' to<br>&nbsp;&nbsp;&nbsp; <a ymailto="mailto:users-request@open-mpi.org" href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a><br><br>You can reach the person managing the list at<br>&nbsp;&nbsp;&nbsp; <a ymailto="mailto:users-owner@open-mpi.org" href="mailto:users-owner@open-mpi.org">users-owner@open-mpi.org</a><br><br>When replying, please edit your Subject line so it is more specific<br>than "Re: Contents of users digest..."<br><br><br>Today's Topics:<br><br>&nbsp;  1. Re: Process Migration (Jeff Squyres)<br>&nbsp;  2. Re: Problems compiling and running openmpi-1.4.4<br>&nbsp; &nbsp;
 &nbsp; (<a ymailto="mailto:amosleff@gmail.com" href="mailto:amosleff@gmail.com">amosleff@gmail.com</a>)<br><br><br>----------------------------------------------------------------------<br><br>Message: 1<br>Date: Thu, 10 Nov 2011 12:59:17 -0500<br>From: Jeff Squyres &lt;<a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>Subject: Re: [OMPI users] Process Migration<br>To: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID: &lt;<a ymailto="mailto:811FFDFC-C3B6-4BF7-9E53-95C0B572FD74@cisco.com" href="mailto:811FFDFC-C3B6-4BF7-9E53-95C0B572FD74@cisco.com">811FFDFC-C3B6-4BF7-9E53-95C0B572FD74@cisco.com</a>&gt;<br>Content-Type: text/plain; charset=us-ascii<br><br>On Nov 10, 2011, at 11:30 AM, Mudassar Majeed wrote:<br><br>&gt; For example there are 10 nodes, and each node contains 20 cores. We will have 200 cores in total and let
 say there are 2000 MPI processes. We start the application with 10 MPI on each core.<br><br>Is this just to be able to simulate very large MPI jobs, or are you thinking that people will actually run that way (heavily over-subscribing cores)?<br><br>&gt; Let say Comm(Pi, Pj) denotes how much communication Pi and Pj make with each other and let say each process Pi has to communicate with few other processes Pj, Pk, Pl, Pm..... Pz. Secondly let say Load(Pi) denotes the computational load of process Pi. <br><br>Depending on how you define Load(Pi), this really only matters if you're over-subscribing processors.&nbsp; Meaning: if you have only one MPI process per processor core, then Load(Pi) is probably irrelevant (excluding other effects, like cache thrashing, memory and PCI bandwidth usage, etc.).<br><br>Right?<br><br>&gt; Now, we know that sending a message between two nodes is more expensive then sending a message within a node (two processes that
 communicate reside on the cores that exist in the same node). This is true atleast in my supercomputing centers that I use. In my previous work I only consider Load[ ] and not Comm[ ]. In that work, all the MPI processes calculate their new ranks and then call MPI_Comm_split with key = new_rank and color = 0. So all the processes get the new rank and then the actual data is provided to each process for computation. We have found that the total execution time decreases.<br><br>In an oversubscribed case, I'm still not sure how this works.&nbsp; Do you have some MPI processes doing work and some not?&nbsp; (e.g., blocking in sleep() or something)<br><br>I think the reason for my confusion is that MPI processes are generally designed to run 1 per core (or perhaps 1 MPI process per more-than-1-core, if the MPI process is multi-threaded).&nbsp; MPI processes are generally assumed to aggressively use the entire computational resource that is given to them --
 sharing computational resources (e.g., cores) between multiple MPI processes would seem to violate that assumption, and therefore result in bad overall performance.<br><br>I feel like I must be missing something in what you're trying to describe...<br><br>&gt; Now we need to consider the communications as well. We will bring the computational load balance but those MPI which communicate more will be mapped to the same node (not necessarily same cores). I have solved this optimization problem using ILP and that shows good results. But the thing is, in the solution I have found that after applying ILP or my heuristic, the cores (on all nodes) will no longer contain same number of MPI processes (load and communications are balanced instead of count of MPI processes per core). So this means either I use process migration for few processes or I run more than 2000 (means at every core I run few more processes) so that at the end imbalance in the number or MPI
 processes per core can be achieved (to achieve balance in load and communications). I need your suggestions in these regards,<br>&gt; <br>&gt; thanks and best regards,<br>&gt; Mudassar<br>&gt; From: Josh Hursey &lt;<a ymailto="mailto:jjhursey@open-mpi.org" href="mailto:jjhursey@open-mpi.org">jjhursey@open-mpi.org</a>&gt;<br>&gt; To: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>&gt; Cc: Mudassar Majeed &lt;<a ymailto="mailto:mudassarm30@yahoo.com" href="mailto:mudassarm30@yahoo.com">mudassarm30@yahoo.com</a>&gt;<br>&gt; Sent: Thursday, November 10, 2011 5:11 PM<br>&gt; Subject: Re: [OMPI users] Process Migration<br>&gt; <br>&gt; Note that the "migrate me from my current node to node &lt;foo&gt;" scenario<br>&gt; is covered by the migration API exported by the C/R infrastructure, as<br>&gt; I noted earlier.<br>&gt;&nbsp; 
 http://osl.iu.edu/research/ft/ompi-cr/api.php#api-cr_migrate<br>&gt; <br>&gt; The "move rank N to node &lt;foo&gt;" scenario could probably be added as an<br>&gt; extension of this interface (since you can do that via the command<br>&gt; line now) if that is what you are looking for.<br>&gt; <br>&gt; -- Josh<br>&gt; <br>&gt; On Thu, Nov 10, 2011 at 11:03 AM, Ralph Castain &lt;<a ymailto="mailto:rhc@open-mpi.org" href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>&gt; &gt; So what you are looking for is an MPI extension API that let's you say<br>&gt; &gt; "migrate me from my current node to node &lt;foo&gt;"? Or do you have a rank that<br>&gt; &gt; is the "master" that would order "move rank N to node &lt;foo&gt;"?<br>&gt; &gt; Either could be provided, I imagine - just want to ensure I understand what<br>&gt; &gt; you need. Can you pass along a brief description of the syntax and<br>&gt; &gt; functionality you would need?<br>&gt;
 &gt;<br>&gt; &gt; On Nov 10, 2011, at 8:27 AM, Mudassar Majeed wrote:<br>&gt; &gt;<br>&gt; &gt; Thank you for your reply. In our previous publication, we have figured it<br>&gt; &gt; out that run more than one processes on cores and balancing the<br>&gt; &gt; computational load considerably reduces the total execution time. You know<br>&gt; &gt; the MPI_Graph_create function, we created another function MPI_Load_create<br>&gt; &gt; that maps the processes on cores such that balance of computational load can<br>&gt; &gt; be achieved on cores. We were having some issues with increase in<br>&gt; &gt; communication cost due to ranks rearrangements (due to MPI_Comm_split, with<br>&gt; &gt; color=0), so in this research work we will see how can we balance both<br>&gt; &gt; computation load on each core and communication load on each node. Those<br>&gt; &gt; processes that communicate more will reside on the same node keeping the<br>&gt; &gt; computational
 load balance over the cores. I solved this problem using ILP<br>&gt; &gt; but ILP takes time and can't be used in run time so I am thinking about an<br>&gt; &gt; heuristic. That's why I want to see if it is possible to migrate a process<br>&gt; &gt; from one core to another or not. Then I will see how good my heuristic will<br>&gt; &gt; be.<br>&gt; &gt;<br>&gt; &gt; thanks<br>&gt; &gt; Mudassar<br>&gt; &gt;<br>&gt; &gt; ________________________________<br>&gt; &gt; From: Jeff Squyres &lt;<a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>&gt; &gt; To: Mudassar Majeed &lt;<a ymailto="mailto:mudassarm30@yahoo.com" href="mailto:mudassarm30@yahoo.com">mudassarm30@yahoo.com</a>&gt;; Open MPI Users<br>&gt; &gt; &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>&gt; &gt; Cc: Ralph Castain &lt;<a ymailto="mailto:rhc@open-mpi.org"
 href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;<br>&gt; &gt; Sent: Thursday, November 10, 2011 2:19 PM<br>&gt; &gt; Subject: Re: [OMPI users] Process Migration<br>&gt; &gt;<br>&gt; &gt; On Nov 10, 2011, at 8:11 AM, Mudassar Majeed wrote:<br>&gt; &gt;<br>&gt; &gt;&gt; Thank you for your reply. I am implementing a load balancing function for<br>&gt; &gt;&gt; MPI, that will balance the computation load and the communication both at a<br>&gt; &gt;&gt; time. So my algorithm assumes that all the cores may at the end get<br>&gt; &gt;&gt; different number of processes to run.<br>&gt; &gt;<br>&gt; &gt; Are you talking about over-subscribing cores?&nbsp; I.e., putting more than 1 MPI<br>&gt; &gt; process on each core?<br>&gt; &gt;<br>&gt; &gt; In general, that's not a good idea.<br>&gt; &gt;<br>&gt; &gt;&gt; In the beginning (before that function will be called), each core will<br>&gt; &gt;&gt; have equal number of processes. So I am thinking either to
 start more<br>&gt; &gt;&gt; processes on each core (than needed) and run my function for load balancing<br>&gt; &gt;&gt; and then block the remaining processes (on each core). In this way I will be<br>&gt; &gt;&gt; able to achieve different number of processes per core.<br>&gt; &gt;<br>&gt; &gt; Open MPI spins aggressively looking for network progress.&nbsp; For example, if<br>&gt; &gt; you block in an MPI_RECV waiting for a message, Open MPI is actively banging<br>&gt; &gt; on the CPU looking for network progress.&nbsp; Because of this (and other<br>&gt; &gt; reasons), you probably do not want to over-subscribe your processors<br>&gt; &gt; (meaning: you probably don't want to put more than 1 process per core).<br>&gt; &gt;<br>&gt; &gt; --<br>&gt; &gt; Jeff Squyres<br>&gt; &gt; <a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>&gt; &gt; For corporate legal information go to:<br>&gt; &gt;
 http://www.cisco.com/web/about/doing_business/legal/cri/<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt; _______________________________________________<br>&gt; &gt; users mailing list<br>&gt; &gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;<br>&gt; <br>&gt; <br>&gt; <br>&gt; -- <br>&gt; Joshua Hursey<br>&gt; Postdoctoral Research Associate<br>&gt; Oak Ridge National Laboratory<br>&gt; http://users.nccs.gov/~jjhursey<br>&gt; <br>&gt; <br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><br>-- <br>Jeff Squyres<br><a
 ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>For corporate legal information go to:<br><a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br><br><br><br><br>------------------------------<br><br>Message: 2<br>Date: Thu, 10 Nov 2011 13:48:06 -0500<br>From: "<a ymailto="mailto:amosleff@gmail.com" href="mailto:amosleff@gmail.com">amosleff@gmail.com</a>" &lt;<a ymailto="mailto:amosleff@gmail.com" href="mailto:amosleff@gmail.com">amosleff@gmail.com</a>&gt;<br>Subject: Re: [OMPI users] Problems compiling and running openmpi-1.4.4<br>To: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Message-ID:<br>&nbsp;&nbsp;&nbsp; &lt;CAHNB0nPJzkK7Q9jxR8UhDrPO_Vvm+Myjy+<a ymailto="mailto:zZ6LEh6r38k-_-Ag@mail.gmail.com"
 href="mailto:zZ6LEh6r38k-_-Ag@mail.gmail.com">zZ6LEh6r38k-_-Ag@mail.gmail.com</a>&gt;<br>Content-Type: text/plain; charset="iso-8859-1"<br><br>Hi Jeff,<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; In the attached file Compile_out.tar.bz2 I have included the out<br>files for config, make, and install.&nbsp; I also included another copy of the<br>out_test file so that it gives you all of the info that I have.&nbsp; Again your<br>help is much appreciated.<br><br>Amos Leffler<br><br>On Wed, Nov 9, 2011 at 12:23 PM, Jeff Squyres &lt;<a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br><br>&gt; On Nov 9, 2011, at 12:16 PM, <a ymailto="mailto:amosleff@gmail.com" href="mailto:amosleff@gmail.com">amosleff@gmail.com</a> wrote:<br>&gt;<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; The file was the output to the command:<br>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
 &nbsp; &nbsp; &nbsp; "mpicc hello_cc.c -o hello_cc<br>&gt; &gt; and lists files which do not appear to be present.&nbsp; I checked the<br>&gt; permissions and they seem to be correct so I am stumped,&nbsp; I did use the<br>&gt; make and install commands and they seemed to go properly.&nbsp; I have the out<br>&gt; files for the three commands and could send them to you if you want.<br>&gt;<br>&gt; Please send all the information listed here:<br>&gt;<br>&gt;&nbsp; &nbsp; http://www.open-mpi.org/community/help/<br>&gt;<br>&gt; --<br>&gt; Jeff Squyres<br>&gt; <a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>&gt; For corporate legal information go to:<br>&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>&gt;<br>&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a
 ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<br>-------------- next part --------------<br>HTML attachment scrubbed and removed<br>-------------- next part --------------<br>A non-text attachment was scrubbed...<br>Name: out_test~<br>Type: application/octet-stream<br>Size: 581 bytes<br>Desc: not available<br>URL: &lt;http://www.open-mpi.org/MailArchives/users/attachments/20111110/edb0c48e/attachment.obj&gt;<br>-------------- next part --------------<br>A non-text attachment was scrubbed...<br>Name: Compile_out.tar.bz2<br>Type: application/x-bzip2<br>Size: 85433 bytes<br>Desc: not available<br>URL:
 &lt;http://www.open-mpi.org/MailArchives/users/attachments/20111110/edb0c48e/attachment.bz2&gt;<br><br>------------------------------<br><br>_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br>End of users Digest, Vol 2064, Issue 3<br>**************************************<br><br><br></div></div></div></body></html>
