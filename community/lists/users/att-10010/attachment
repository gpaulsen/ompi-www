<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML><HEAD>
<META http-equiv=Content-Type content="text/html; charset=iso-8859-1">
<META content="MSHTML 6.00.2900.2180" name=GENERATOR>
<STYLE></STYLE>
</HEAD>
<BODY bgColor=#ffffff>
<DIV><FONT face=Arial size=2>Hi All,</FONT></DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>We are running open MPI 1.3.2&nbsp;with OFED1.5. we 
have 8 node cluster with 10Gb Iwarp ethernet&nbsp;card. </FONT></DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Node name are as below </FONT><FONT face=Arial 
size=2>n130,n131,n132,n133,n134,n135,n136,n137. Respective 10GB hostname are 
n130x,n131x..... n137x. </FONT></DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>we have /root/mpd.hosts entry like as below: 
</FONT></DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial 
size=2>n130x<BR>n131x<BR>n134x<BR>n135x<BR>n136x<BR>n132x<BR>n133x<BR>n137x</FONT></DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>We are not able to run open mpi with all 8 node. 
</FONT></DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>mpirun -n&nbsp;8 -np&nbsp;8 
-hostfile&nbsp;/root/mpd.hosts -mca btl openib,self,sm --mca 
orte_base_help_aggregate 0 --mca btl_base_verbose 10 --mca btl_openib_verbose 
100 /usr/mpi/gcc/openmpi-1.3.2/tests/IMB-3.1/IMB-MPI1 Barrier</FONT></DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Output: </FONT></DIV>
<DIV><FONT face=Arial 
size=2>=================================================================================</FONT></DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>At least one pair of MPI processes are unable to 
reach each other for<BR>MPI communications.&nbsp; This means that no Open MPI 
device has indicated<BR>that it can be used to communicate between these 
processes.&nbsp; This is<BR>an error; Open MPI requires that all MPI processes 
be able to reach<BR>each other.&nbsp; This error can sometimes be the result of 
forgetting to<BR>specify the "self" BTL.</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; Process 1 ([[33322,1],0]) is on host: 
n130<BR>&nbsp; Process 2 ([[33322,1],5]) is on host: n132x<BR>&nbsp; BTLs 
attempted: openib self sm</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Your MPI job is now going to abort; 
sorry.<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>At 
least one pair of MPI processes are unable to reach each other for<BR>MPI 
communications.&nbsp; This means that no Open MPI device has indicated<BR>that 
it can be used to communicate between these processes.&nbsp; This is<BR>an 
error; Open MPI requires that all MPI processes be able to reach<BR>each 
other.&nbsp; This error can sometimes be the result of forgetting to<BR>specify 
the "self" BTL.</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; Process 1 ([[33322,1],2]) is on host: 
n134<BR>&nbsp; Process 2 ([[33322,1],5]) is on host: n132x<BR>&nbsp; BTLs 
attempted: openib self sm</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Your MPI job is now going to abort; 
sorry.<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>At 
least one pair of MPI processes are unable to reach each other for<BR>MPI 
communications.&nbsp; This means that no Open MPI device has indicated<BR>that 
it can be used to communicate between these processes.&nbsp; This is<BR>an 
error; Open MPI requires that all MPI processes be able to reach<BR>each 
other.&nbsp; This error can sometimes be the result of forgetting to<BR>specify 
the "self" BTL.</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; Process 1 ([[33322,1],5]) is on host: 
n132<BR>&nbsp; Process 2 ([[33322,1],0]) is on host: n130<BR>&nbsp; BTLs 
attempted: openib self sm</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Your MPI job is now going to abort; 
sorry.<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>At 
least one pair of MPI processes are unable to reach each other for<BR>MPI 
communications.&nbsp; This means that no Open MPI device has indicated<BR>that 
it can be used to communicate between these processes.&nbsp; This is<BR>an 
error; Open MPI requires that all MPI processes be able to reach<BR>each 
other.&nbsp; This error can sometimes be the result of forgetting to<BR>specify 
the "self" BTL.</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; Process 1 ([[33322,1],7]) is on host: 
n137<BR>&nbsp; Process 2 ([[33322,1],0]) is on host: n130<BR>&nbsp; BTLs 
attempted: openib self sm</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Your MPI job is now going to abort; 
sorry.<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>At 
least one pair of MPI processes are unable to reach each other for<BR>MPI 
communications.&nbsp; This means that no Open MPI device has indicated<BR>that 
it can be used to communicate between these processes.&nbsp; This is<BR>an 
error; Open MPI requires that all MPI processes be able to reach<BR>each 
other.&nbsp; This error can sometimes be the result of forgetting to<BR>specify 
the "self" BTL.</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; Process 1 ([[33322,1],3]) is on host: 
n135<BR>&nbsp; Process 2 ([[33322,1],5]) is on host: n132x<BR>&nbsp; BTLs 
attempted: openib self sm</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Your MPI job is now going to abort; 
sorry.<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>At 
least one pair of MPI processes are unable to reach each other for<BR>MPI 
communications.&nbsp; This means that no Open MPI device has indicated<BR>that 
it can be used to communicate between these processes.&nbsp; This is<BR>an 
error; Open MPI requires that all MPI processes be able to reach<BR>each 
other.&nbsp; This error can sometimes be the result of forgetting to<BR>specify 
the "self" BTL.</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; Process 1 ([[33322,1],6]) is on host: 
n133<BR>&nbsp; Process 2 ([[33322,1],0]) is on host: n130<BR>&nbsp; BTLs 
attempted: openib self sm</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Your MPI job is now going to abort; 
sorry.<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>At 
least one pair of MPI processes are unable to reach each other for<BR>MPI 
communications.&nbsp; This means that no Open MPI device has indicated<BR>that 
it can be used to communicate between these processes.&nbsp; This is<BR>an 
error; Open MPI requires that all MPI processes be able to reach<BR>each 
other.&nbsp; This error can sometimes be the result of forgetting to<BR>specify 
the "self" BTL.</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; Process 1 ([[33322,1],1]) is on host: 
n131<BR>&nbsp; Process 2 ([[33322,1],5]) is on host: n132x<BR>&nbsp; BTLs 
attempted: openib self sm</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Your MPI job is now going to abort; 
sorry.<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>At 
least one pair of MPI processes are unable to reach each other for<BR>MPI 
communications.&nbsp; This means that no Open MPI device has indicated<BR>that 
it can be used to communicate between these processes.&nbsp; This is<BR>an 
error; Open MPI requires that all MPI processes be able to reach<BR>each 
other.&nbsp; This error can sometimes be the result of forgetting to<BR>specify 
the "self" BTL.</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; Process 1 ([[33322,1],4]) is on host: 
n136<BR>&nbsp; Process 2 ([[33322,1],5]) is on host: n132x<BR>&nbsp; BTLs 
attempted: openib self sm</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Your MPI job is now going to abort; 
sorry.<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>It 
looks like MPI_INIT failed for some reason; your parallel process is<BR>likely 
to abort.&nbsp; There are many reasons that a parallel process can<BR>fail 
during MPI_INIT; some of which are due to configuration or 
environment<BR>problems.&nbsp; This failure appears to be an internal failure; 
here's some<BR>additional information (which may only be relevant to an Open 
MPI<BR>developer):</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; PML add procs failed<BR>&nbsp; --&gt; 
Returned "Unreachable" (-12) instead of "Success" 
(0)<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>It 
looks like MPI_INIT failed for some reason; your parallel process is<BR>likely 
to abort.&nbsp; There are many reasons that a parallel process can<BR>fail 
during MPI_INIT; some of which are due to configuration or 
environment<BR>problems.&nbsp; This failure appears to be an internal failure; 
here's some<BR>additional information (which may only be relevant to an Open 
MPI<BR>developer):</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; PML add procs failed<BR>&nbsp; --&gt; 
Returned "Unreachable" (-12) instead of "Success" 
(0)<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>It 
looks like MPI_INIT failed for some reason; your parallel process is<BR>likely 
to abort.&nbsp; There are many reasons that a parallel process can<BR>fail 
during MPI_INIT; some of which are due to configuration or 
environment<BR>problems.&nbsp; This failure appears to be an internal failure; 
here's some<BR>additional information (which may only be relevant to an Open 
MPI<BR>developer):</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; PML add procs failed<BR>&nbsp; --&gt; 
Returned "Unreachable" (-12) instead of "Success" 
(0)<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>It 
looks like MPI_INIT failed for some reason; your parallel process is<BR>likely 
to abort.&nbsp; There are many reasons that a parallel process can<BR>fail 
during MPI_INIT; some of which are due to configuration or 
environment<BR>problems.&nbsp; This failure appears to be an internal failure; 
here's some<BR>additional information (which may only be relevant to an Open 
MPI<BR>developer):</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; PML add procs failed<BR>&nbsp; --&gt; 
Returned "Unreachable" (-12) instead of "Success" 
(0)<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>It 
looks like MPI_INIT failed for some reason; your parallel process is<BR>likely 
to abort.&nbsp; There are many reasons that a parallel process can<BR>fail 
during MPI_INIT; some of which are due to configuration or 
environment<BR>problems.&nbsp; This failure appears to be an internal failure; 
here's some<BR>additional information (which may only be relevant to an Open 
MPI<BR>developer):</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; PML add procs failed<BR>&nbsp; --&gt; 
Returned "Unreachable" (-12) instead of "Success" 
(0)<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>It 
looks like MPI_INIT failed for some reason; your parallel process is<BR>likely 
to abort.&nbsp; There are many reasons that a parallel process can<BR>fail 
during MPI_INIT; some of which are due to configuration or 
environment<BR>problems.&nbsp; This failure appears to be an internal failure; 
here's some<BR>additional information (which may only be relevant to an Open 
MPI<BR>developer):</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; PML add procs failed<BR>&nbsp; --&gt; 
Returned "Unreachable" (-12) instead of "Success" 
(0)<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>It 
looks like MPI_INIT failed for some reason; your parallel process is<BR>likely 
to abort.&nbsp; There are many reasons that a parallel process can<BR>fail 
during MPI_INIT; some of which are due to configuration or 
environment<BR>problems.&nbsp; This failure appears to be an internal failure; 
here's some<BR>additional information (which may only be relevant to an Open 
MPI<BR>developer):</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; PML add procs failed<BR>&nbsp; --&gt; 
Returned "Unreachable" (-12) instead of "Success" 
(0)<BR>--------------------------------------------------------------------------<BR>*** 
An error occurred in MPI_Init_thread<BR>*** before MPI was initialized<BR>*** 
MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[n134:4888] Abort before 
MPI_INIT completed successfully; not able to guarantee that all other processes 
were killed!<BR>*** An error occurred in MPI_Init_thread<BR>*** before MPI was 
initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>*** An 
error occurred in MPI_Init_thread<BR>*** before MPI was initialized<BR>*** 
MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>*** An error occurred in 
MPI_Init_thread<BR>*** before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL 
(your MPI job will now abort)<BR>*** An error occurred in MPI_Init_thread<BR>*** 
before MPI was initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now 
abort)<BR>*** An error occurred in MPI_Init_thread<BR>*** before MPI was 
initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>*** An 
error occurred in MPI_Init_thread<BR>*** before MPI was initialized<BR>*** 
MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[n137:4890] Abort before 
MPI_INIT completed successfully; not able to guarantee that all other processes 
were killed!<BR>[n135:4883] Abort before MPI_INIT completed successfully; not 
able to guarantee that all other processes were killed!<BR>[n133:4850] Abort 
before MPI_INIT completed successfully; not able to guarantee that all other 
processes were killed!<BR>[n136:4866] Abort before MPI_INIT completed 
successfully; not able to guarantee that all other processes were 
killed!<BR>[n131:4866] Abort before MPI_INIT completed successfully; not able to 
guarantee that all other processes were killed!<BR>[n132:4855] Abort before 
MPI_INIT completed successfully; not able to guarantee that all other processes 
were 
killed!<BR>--------------------------------------------------------------------------<BR>mpirun 
has exited due to process rank 3 with PID 4883 on<BR>node n135x exiting without 
calling "finalize". This may<BR>have caused other processes in the application 
to be<BR>terminated by signals sent by mpirun (as reported 
here).<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>It 
looks like MPI_INIT failed for some reason; your parallel process is<BR>likely 
to abort.&nbsp; There are many reasons that a parallel process can<BR>fail 
during MPI_INIT; some of which are due to configuration or 
environment<BR>problems.&nbsp; This failure appears to be an internal failure; 
here's some<BR>additional information (which may only be relevant to an Open 
MPI<BR>developer):</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; PML add procs failed<BR>&nbsp; --&gt; 
Returned "Unreachable" (-12) instead of "Success" 
(0)<BR>--------------------------------------------------------------------------<BR>*** 
An error occurred in MPI_Init_thread<BR>*** before MPI was initialized<BR>*** 
MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[n130:4885] Abort before 
MPI_INIT completed successfully; not able to guarantee that all other processes 
were killed!<BR></FONT></DIV>
<DIV><FONT face=Arial size=2>
<DIV><FONT face=Arial 
size=2>=================================================================================</FONT></DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV>we are able to run same command on btl with tcp as below for all 8 node 
:</DIV>
<DIV>&nbsp;</DIV>
<DIV>mpirun -n 8 -np 8 -hostfile /root/mpd.hosts&nbsp; -mca btl tcp,self,sm 
--mca orte_base_help_aggregate 0 --mca btl_base_verbose 10 --mca 
btl_openib_verbose 100 /usr/mpi/gcc/openmpi-1.3.2/tests/IMB-3.1/IMB-MPI1 
Barrier</DIV>
<DIV>&nbsp;</DIV></DIV></FONT>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>If we remove n132,n133,n137 node from mpd.hosts 
file then we are able to run open mpi for all remaining 5 node on btl 
openib,sm,self .</FONT></DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>So there is some problem with only n132,n133,n137 
node. we are able to run opnmpi with this 3 node. but when we&nbsp;try to run 
this node with&nbsp;other 5 node or&nbsp;one of the node 
(n130,n131,n134,n135,n136) then we will get&nbsp;below&nbsp;error: </FONT></DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Output :</FONT></DIV>
<DIV><FONT face=Arial size=2>===============</FONT></DIV>
<DIV><FONT face=Arial size=2>At least one pair of MPI processes are unable to 
reach each other for<BR>MPI communications.&nbsp; This means that no Open MPI 
device has indicated<BR>that it can be used to communicate between these 
processes.&nbsp; This is<BR>an error; Open MPI requires that all MPI processes 
be able to reach<BR>each other.&nbsp; This error can sometimes be the result of 
forgetting to<BR>specify the "self" BTL.</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; Process 1 ([[33304,1],1]) is on host: 
n132<BR>&nbsp; Process 2 ([[33304,1],0]) is on host: n130<BR>&nbsp; BTLs 
attempted: openib self sm</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Your MPI job is now going to abort; 
sorry.<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>It 
looks like MPI_INIT failed for some reason; your parallel process is<BR>likely 
to abort.&nbsp; There are many reasons that a parallel process can<BR>fail 
during MPI_INIT; some of which are due to configuration or 
environment<BR>problems.&nbsp; This failure appears to be an internal failure; 
here's some<BR>additional information (which may only be relevant to an Open 
MPI<BR>developer):</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; PML add procs failed<BR>&nbsp; --&gt; 
Returned "Unreachable" (-12) instead of "Success" 
(0)<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>At 
least one pair of MPI processes are unable to reach each other for<BR>MPI 
communications.&nbsp; This means that no Open MPI device has indicated<BR>that 
it can be used to communicate between these processes.&nbsp; This is<BR>an 
error; Open MPI requires that all MPI processes be able to reach<BR>each 
other.&nbsp; This error can sometimes be the result of forgetting to<BR>specify 
the "self" BTL.</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; Process 1 ([[33304,1],0]) is on host: 
n130<BR>&nbsp; Process 2 ([[33304,1],1]) is on host: 100<BR>&nbsp; BTLs 
attempted: openib self sm</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Your MPI job is now going to abort; 
sorry.<BR>--------------------------------------------------------------------------<BR>--------------------------------------------------------------------------<BR>It 
looks like MPI_INIT failed for some reason; your parallel process is<BR>likely 
to abort.&nbsp; There are many reasons that a parallel process can<BR>fail 
during MPI_INIT; some of which are due to configuration or 
environment<BR>problems.&nbsp; This failure appears to be an internal failure; 
here's some<BR>additional information (which may only be relevant to an Open 
MPI<BR>developer):</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>&nbsp; PML add procs failed<BR>&nbsp; --&gt; 
Returned "Unreachable" (-12) instead of "Success" 
(0)<BR>--------------------------------------------------------------------------<BR>*** 
An error occurred in MPI_Init_thread<BR>*** before MPI was initialized<BR>*** 
MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<BR>[n130:4929] Abort before 
MPI_INIT completed successfully; not able to guarantee that all other processes 
were killed!<BR>*** An error occurred in MPI_Init_thread<BR>*** before MPI was 
initialized<BR>*** MPI_ERRORS_ARE_FATAL (your MPI job will now 
abort)<BR>[n132:4963] Abort before MPI_INIT completed successfully; not able to 
guarantee that all other processes were 
killed!<BR>--------------------------------------------------------------------------<BR>mpirun 
has exited due to process rank 0 with PID 4929 on<BR>node n130 exiting without 
calling "finalize". This may<BR>have caused other processes in the application 
to be<BR>terminated by signals sent by mpirun (as reported 
here).<BR>-----------------------------------------------------------</FONT></DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>we are able to run INtel,Mvapich2 MPI on All 8 node 
but we are facing problem for OpenMPI. Can any one help us what the real issue 
with that 3 node.</FONT></DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Find attached Log for detail.</FONT></DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Thanks,<BR>Hardik&nbsp;</FONT></DIV></BODY></HTML>

