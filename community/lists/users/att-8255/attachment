Hello, i ve corrected the syntax and added the flag you suggested, but unfortunately the result doen&#39;t change.<br><br>randori ~ # mpirun --display-map --mca btl tcp,self  -np 2 -host randori,tatami graph <br>[randori:22322]  Map for job: 1    Generated by mapping mode: byslot<br>
     Starting vpid: 0    Vpid range: 2    Num app_contexts: 1<br>
     Data for app_context: index 0    app: graph<br>         Num procs: 2<br>         Argv[0]: graph<br>         Env[0]: OMPI_MCA_btl=tcp,self<br>         Env[1]: OMPI_MCA_rmaps_base_display_map=1<br>         Env[2]: OMPI_MCA_orte_precondition_transports=d45d47f6e1ed0e0b-691fd7f24609dec3<br>

         Env[3]: OMPI_MCA_rds=proxy<br>         Env[4]: OMPI_MCA_ras=proxy<br>         Env[5]: OMPI_MCA_rmaps=proxy<br>         Env[6]: OMPI_MCA_pls=proxy<br>         Env[7]: OMPI_MCA_rmgr=proxy<br>         Working dir: /root (user: 0)<br>

         Num maps: 1<br>         Data for app_context_map: Type: 1    Data: randori,tatami<br>     Num elements in nodes list: 2<br>     Mapped node:<br>         Cell: 0    Nodename: randori    Launch id: -1    Username: NULL<br>

         Daemon name:<br>             Data type: ORTE_PROCESS_NAME    Data Value: NULL<br>         Oversubscribed: False    Num elements in procs list: 1<br>         Mapped proc:<br>             Proc Name:<br>             Data type: ORTE_PROCESS_NAME    Data Value: [0,1,0]<br>

             Proc Rank: 0    Proc PID: 0    App_context index: 0<br><br>     Mapped node:<br>         Cell: 0    Nodename: tatami    Launch id: -1    Username: NULL<br>         Daemon name:<br>             Data type: ORTE_PROCESS_NAME    Data Value: NULL<br>

         Oversubscribed: False    Num elements in procs list: 1<br>         Mapped proc:<br>             Proc Name:<br>             Data type: ORTE_PROCESS_NAME    Data Value: [0,1,1]<br>             Proc Rank: 1    Proc PID: 0    App_context index: 0<br>

Master thread reporting<br>matrix size 33554432 kB, time is in [us]<br><br>(and then it just hangs)<br><br>Vittorio<br><br><div class="gmail_quote">On Fri, Feb 27, 2009 at 6:00 PM,  <span dir="ltr">&lt;<a href="mailto:users-request@open-mpi.org" target="_blank">users-request@open-mpi.org</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><br>
Date: Fri, 27 Feb 2009 08:22:17 -0700<br>
From: Ralph Castain &lt;<a href="mailto:rhc@lanl.gov" target="_blank">rhc@lanl.gov</a>&gt;<br>
Subject: Re: [OMPI users] TCP instead of openIB doesn&#39;t work<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
Message-ID: &lt;<a href="mailto:E3C4683C-1F97-4558-AB68-006E39A8334B@lanl.gov" target="_blank">E3C4683C-1F97-4558-AB68-006E39A8334B@lanl.gov</a>&gt;<br>
Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes<br>
<br>
I&#39;m not entirely sure what is causing the problem here, but one thing<br>
does stand out. You have specified two -host options for the same<br>
application - this is not our normal syntax. The usual way of<br>
specifying this would be:<br>
<br>
mpirun  --mca btl tcp,self  -np 2 -host randori,tatami hostname<br>
<br>
I&#39;m not entirely sure what OMPI does when it gets two separate -host<br>
arguments - could be equivalent to the above syntax, but could also<br>
cause some unusual behavior.<br>
<br>
Could you retry your job with the revised syntax? Also, could you add<br>
--display-map to your mpirun cmd line? This will tell us where OMPI<br>
thinks the procs are going, and a little info about how it interpreted<br>
your cmd line.<br>
<br>
Thanks<br>
Ralph<br>
<br>
<br>
On Feb 27, 2009, at 8:00 AM, Vittorio Giovara wrote:<br>
<br>
&gt; Hello, i&#39;m posting here another problem of my installation<br>
&gt; I wanted to benchmark the differences between tcp and openib transport<br>
&gt;<br>
&gt; if i run a simple non mpi application i get<br>
&gt; randori ~ # mpirun  --mca btl tcp,self  -np 2 -host randori -host<br>
&gt; tatami hostname<br>
&gt; randori<br>
&gt; tatami<br>
&gt;<br>
&gt; but as soon as i switch to my benchmark program i have<br>
&gt; mpirun  --mca btl tcp,self  -np 2 -host randori -host tatami graph<br>
&gt; Master thread reporting<br>
&gt; matrix size 33554432 kB, time is in [us]<br>
&gt;<br>
&gt; and instead of starting the send/receive functions it just hangs<br>
&gt; there; i also checked the transmitted packets with wireshark but<br>
&gt; after the handshake no more packets are exchanged<br>
&gt;<br>
&gt; I read in the archives that there were some problems in this area<br>
&gt; and so i tried what was suggested in previous emails<br>
&gt;<br>
&gt; mpirun --mca btl ^openib  -np 2 -host randori -host tatami graph<br>
&gt; mpirun --mca pml ob1  --mca btl tcp,self  -np 2 -host randori -host<br>
&gt; tatami graph<br>
&gt;<br>
&gt; gives exactly the same output as before (no mpisend/receive)<br>
&gt; while the next commands gives something more interesting<br>
&gt;<br>
&gt; mpirun --mca pml cm  --mca btl tcp,self  -np 2 -host randori -host<br>
&gt; tatami graph<br>
&gt; --------------------------------------------------------------------------<br>
&gt; No available pml components were found!<br>
&gt;<br>
&gt; This means that there are no components of this type installed on your<br>
&gt; system or all the components reported that they could not be used.<br>
&gt;<br>
&gt; This is a fatal error; your MPI process is likely to abort.  Check the<br>
&gt; output of the &quot;ompi_info&quot; command and ensure that components of this<br>
&gt; type are available on your system.  You may also wish to check the<br>
&gt; value of the &quot;component_path&quot; MCA parameter and ensure that it has at<br>
&gt; least one directory that contains valid MCA components.<br>
&gt;<br>
&gt; --------------------------------------------------------------------------<br>
&gt; [tatami:06619] PML cm cannot be selected<br>
&gt; mpirun noticed that job rank 0 with PID 6710 on node randori exited<br>
&gt; on signal 15 (Terminated).<br>
&gt;<br>
&gt; which is not possible as if i do ompi_info --param all there is the<br>
&gt; CM pml component<br>
&gt;<br>
&gt;                  MCA pml: cm (MCA v1.0, API v1.0, Component v1.2.8)<br>
&gt;                  MCA pml: ob1 (MCA v1.0, API v1.0, Component v1.2.8)<br>
&gt;<br>
&gt;<br>
&gt; my test program is quite simple, just a couple of MPI_Send and<br>
&gt; MPI_Recv (just after the signature)<br>
&gt; do you have any ideas that might help me?<br>
&gt; thanks a lot<br>
&gt; Vittorio<br>
&gt;<br>
&gt; ========================<br>
&gt; #include &quot;mpi.h&quot;<br>
&gt; #include &lt;stdio.h&gt;<br>
&gt; #include &lt;stdlib.h&gt;<br>
&gt; #include &lt;string.h&gt;<br>
&gt; #include &lt;math.h&gt;<br>
&gt;<br>
&gt; #define M_COL 4096<br>
&gt; #define M_ROW 524288<br>
&gt; #define NUM_MSG 25<br>
&gt;<br>
&gt; unsigned long int  gigamatrix[M_ROW][M_COL];<br>
&gt;<br>
&gt; int main (int argc, char *argv[]) {<br>
&gt;     int numtasks, rank, dest, source, rc, tmp, count, tag=1;<br>
&gt;     unsigned long int  exp, exchanged;<br>
&gt;     unsigned long int i, j, e;<br>
&gt;     unsigned long matsize;<br>
&gt;     MPI_Status Stat;<br>
&gt;     struct timeval timing_start, timing_end;<br>
&gt;     double inittime = 0;<br>
&gt;     long int totaltime = 0;<br>
&gt;<br>
&gt;     MPI_Init (&amp;argc, &amp;argv);<br>
&gt;     MPI_Comm_size (MPI_COMM_WORLD, &amp;numtasks);<br>
&gt;     MPI_Comm_rank (MPI_COMM_WORLD, &amp;rank);<br>
&gt;<br>
&gt;<br>
&gt;     if (rank == 0) {<br>
&gt;         fprintf (stderr, &quot;Master thread reporting\n&quot;, numtasks - 1);<br>
&gt;         matsize = (long) M_COL * M_ROW / 64;<br>
&gt;         fprintf (stderr, &quot;matrix size %d kB, time is in [us]\n&quot;,<br>
&gt; matsize);<br>
&gt;<br>
&gt;         source = 1;<br>
&gt;         dest = 1;<br>
&gt;<br>
&gt;         /*warm up phase*/<br>
&gt;         rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);<br>
&gt;         rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag,<br>
&gt; MPI_COMM_WORLD, &amp;Stat);<br>
&gt;         rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);<br>
&gt;         rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);<br>
&gt;         rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag,<br>
&gt; MPI_COMM_WORLD, &amp;Stat);<br>
&gt;         rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);<br>
&gt;<br>
&gt;         for (e = 0; e &lt; NUM_MSG; e++) {<br>
&gt;             exp = pow (2, e);<br>
&gt;             exchanged = 64 * exp;<br>
&gt;<br>
&gt;             /*timing of ops*/<br>
&gt;             gettimeofday (&amp;timing_start, NULL);<br>
&gt;             rc = MPI_Send (&amp;gigamatrix[0], exchanged,<br>
&gt; MPI_UNSIGNED_LONG, dest, tag, MPI_COMM_WORLD);<br>
&gt;             rc = MPI_Recv (&amp;gigamatrix[0], exchanged,<br>
&gt; MPI_UNSIGNED_LONG, source, tag, MPI_COMM_WORLD, &amp;Stat);<br>
&gt;             gettimeofday (&amp;timing_end, NULL);<br>
&gt;<br>
&gt;             totaltime = (timing_end.tv_sec - timing_start.tv_sec) *<br>
&gt; 1000000 + (timing_end.tv_usec - timing_start.tv_usec);<br>
&gt;             memset (&amp;timing_start, 0, sizeof(struct timeval));<br>
&gt;             memset (&amp;timing_end, 0, sizeof(struct timeval));<br>
&gt;             fprintf (stdout, &quot;%d kB\t%d\n&quot;, exp, totaltime);<br>
&gt;         }<br>
&gt;<br>
&gt;         fprintf(stderr, &quot;task complete\n&quot;);<br>
&gt;<br>
&gt;     } else {<br>
&gt;         if (rank &gt;= 1) {<br>
&gt;             dest = 0;<br>
&gt;             source = 0;<br>
&gt;<br>
&gt;             rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag,<br>
&gt; MPI_COMM_WORLD, &amp;Stat);<br>
&gt;             rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag,<br>
&gt; MPI_COMM_WORLD);<br>
&gt;             rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag,<br>
&gt; MPI_COMM_WORLD, &amp;Stat);<br>
&gt;             rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag,<br>
&gt; MPI_COMM_WORLD, &amp;Stat);<br>
&gt;             rc = MPI_Send (&amp;tmp, 1, MPI_INT, dest, tag,<br>
&gt; MPI_COMM_WORLD);<br>
&gt;             rc = MPI_Recv (&amp;tmp, 1, MPI_INT, source, tag,<br>
&gt; MPI_COMM_WORLD, &amp;Stat);<br>
&gt;<br>
&gt;             for (e = 0; e &lt; NUM_MSG; e++) {<br>
&gt;                 exp = pow (2, e);<br>
&gt;                 exchanged = 64 * exp;<br>
&gt;<br>
&gt;                 rc = MPI_Recv (&amp;gigamatrix[0], (unsigned)<br>
&gt; exchanged, MPI_UNSIGNED_LONG, source, tag, MPI_COMM_WORLD, &amp;Stat);<br>
&gt;                 rc = MPI_Send (&amp;gigamatrix[0], (unsigned)<br>
&gt; exchanged, MPI_UNSIGNED_LONG, dest, tag, MPI_COMM_WORLD);<br>
&gt;<br>
&gt;             }<br>
&gt;         }<br>
&gt;     }<br>
&gt;<br>
&gt;     MPI_Finalize ();<br>
&gt;<br>
&gt;     return 0;<br>
&gt; }<br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

