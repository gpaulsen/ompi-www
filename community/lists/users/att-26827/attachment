<div dir="ltr">I went over the code and in fact I think it is correct as is. The length is for the local representation, which indeed uses pointers to datatype structures. On the opposite, the total_pack_size represents the amount of space we would need to store the data in a format that can be sent to another peer, in which case handling pointers is pointless and we fall back to int.<div><br></div><div>However, I think we are counting twice the space needed for predefined data. I&#39;ll push a patch shortly.<br><div><br></div><div>  George.</div><div><br></div></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Thu, Apr 30, 2015 at 3:33 PM, George Bosilca <span dir="ltr">&lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank">bosilca@icl.utk.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">In the packed representation we store not MPI_Datatypes but a handcrafted id for each one. The 2 codes should have been in sync. I&#39;m looking at another issue right now, and I&#39;ll come back to this one right after.<div><br></div><div>Thanks for paying attention to the code.</div><span class="HOEnZb"><font color="#888888"><div>  George.</div></font></span></div><div class="HOEnZb"><div class="h5"><div class="gmail_extra"><br><div class="gmail_quote">On Thu, Apr 30, 2015 at 3:13 PM, Satish Balay <span dir="ltr">&lt;<a href="mailto:balay@mcs.anl.gov" target="_blank">balay@mcs.anl.gov</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Thanks for checking and getting a more appropriate fix in.<br>
<br>
I&#39;ve just tried this out - and the PETSc test code runs fine with it.<br>
<br>
BTW: There is one inconsistancy in ompi/datatype/ompi_datatype_args.c<br>
[that I noticed] - that you might want to check.<br>
Perhaps the second line should be  &quot;(DC) * sizeof(MPI_Datatype)&quot;?<br>
<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
        int length = sizeof(ompi_datatype_args_t) + (IC) * sizeof(int) + \<br>
            (AC) * sizeof(OPAL_PTRDIFF_TYPE) + (DC) * sizeof(MPI_Datatype); \<br>
<br>
<br>
       pArgs-&gt;total_pack_size = (4 + (IC)) * sizeof(int) +             \<br>
            (AC) * sizeof(OPAL_PTRDIFF_TYPE) + (DC) * sizeof(int);      \<br>
&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;<br>
<br>
Satish<br>
<br>
<br>
On Thu, 30 Apr 2015, Matthew Knepley wrote:<br>
<span><br>
&gt; On Fri, May 1, 2015 at 4:55 AM, Jeff Squyres (jsquyres) &lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;<br>
&gt; wrote:<br>
&gt;<br>
&gt; &gt; Thank you!<br>
&gt; &gt;<br>
&gt; &gt; George reviewed your patch and adjusted it a bit.  We applied it to master<br>
&gt; &gt; and it&#39;s pending to the release series (v1.8.x).<br>
&gt; &gt;<br>
&gt;<br>
</span>&gt; Was this identified by IBM?<br>
&gt;<br>
&gt;<br>
&gt; <a href="https://github.com/open-mpi/ompi/commit/015d3f56cf749ee5ad9ea4428d2f5da72f9bbe08" target="_blank">https://github.com/open-mpi/ompi/commit/015d3f56cf749ee5ad9ea4428d2f5da72f9bbe08</a><br>
&gt;<br>
&gt;      Matt<br>
<div><div>&gt;<br>
&gt;<br>
&gt; &gt; Would you mind testing a nightly master snapshot?  It should be in<br>
&gt; &gt; tonight&#39;s build:<br>
&gt; &gt;<br>
&gt; &gt;     <a href="http://www.open-mpi.org/nightly/master/" target="_blank">http://www.open-mpi.org/nightly/master/</a><br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; &gt; On Apr 30, 2015, at 12:50 AM, Satish Balay &lt;<a href="mailto:balay@mcs.anl.gov" target="_blank">balay@mcs.anl.gov</a>&gt; wrote:<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; OpenMPI developers,<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; We&#39;ve had issues (memory errors) with OpenMPI - and code in PETSc<br>
&gt; &gt; &gt; library that uses MPI_Win_fence().<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; Vagrind shows memory corruption deep inside OpenMPI function stack.<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; I&#39;m attaching a potential patch that appears to fix this issue for us.<br>
&gt; &gt; &gt; [the corresponding valgrind trace is listed in the patch header]<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; Perhaps there is a more appropriate fix for this memory corruption. Could<br>
&gt; &gt; &gt; you check on this?<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; [Sorry I don&#39;t have a pure MPI test code to demonstrate this error -<br>
&gt; &gt; &gt; but a PETSc test example code consistantly reproduces this issue]<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; Thanks,<br>
&gt; &gt; &gt; Satish&lt;openmpi-1.8.4.patch&gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; --<br>
&gt; &gt; Jeff Squyres<br>
&gt; &gt; <a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a><br>
&gt; &gt; For corporate legal information go to:<br>
&gt; &gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/04/26823.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/04/26823.php</a><br>
</blockquote></div><br></div>
</div></div></blockquote></div><br></div>

