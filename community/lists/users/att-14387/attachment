<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><br><div><div>On Oct 4, 2010, at 1:48 PM, Storm Zhang wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><meta charset="utf-8"><span class="Apple-style-span" style="font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse; ">Thanks a lot, Ralgh. As I said, I also tried to use SGE(also showing 1024 available for parallel tasks) which only assign 34-38 compute nodes which only has 272-304 real cores for 500 procs running. The running time is consistent with 100 procs and not a lot fluctuations due to the number of machines' changing.</span></blockquote><div><br></div>Afraid I don't understand your statement. If you have 500 procs running on &lt; 500 cores, then the performance relative to a high-performance job (#procs &lt;= #cores) will be worse. We deliberately dial down the performance when oversubscribed to ensure that procs "play nice" in situations where the node is oversubscribed.</div><div><br><blockquote type="cite"><span class="Apple-style-span" style="font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse; ">&nbsp;So I guess it is not related to hyperthreading. Correct me if I'm wrong.</span></blockquote><div><br></div>Has nothing to do with hyperthreading - OMPI has no knowledge of hyperthreads at this time.</div><div><br><blockquote type="cite"><div>
<span class="Apple-style-span" style="font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse; "><br></span></div><div><span class="Apple-style-span" style="font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse; ">BTW, how to bind the proc to the core? I tried --bind-to-core or -bind-to-core but neither works. Is it for OpenMP, not for OpenMPI?&nbsp;</span></div></blockquote><div><br></div>Those should work. You might try --report-bindings to see what OMPI thought it did.</div><div><br><blockquote type="cite"><div><div>
<span class="Apple-style-span" style="font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse; "><div><br></div><div>Linbao</div></span><br><br><div class="gmail_quote">On Mon, Oct 4, 2010 at 12:27 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;"><div style="word-wrap:break-word">Some of what you are seeing is the natural result of context switching....some thoughts regarding the results:<div>
<br></div><div>1. You didn't bind your procs to cores when running with #procs &lt; #cores, so you're performance in those scenarios will also be less than max.&nbsp;</div><div><br></div><div>2. Once the number of procs exceeds the number of cores, you guarantee a lot of context switching, so performance will definitely take a hit.<div>
<br></div><div>3. Sometime in the not-too-distant-future, OMPI will (hopefully) become hyperthread aware. For now, we don't see them as separate processing units. So as far as OMPI is concerned, you only have 512 computing units to work with, not 1024.</div>
<div><br></div><div>Bottom line is that you are running oversubscribed, so OMPI turns down your performance so that the machine doesn't hemorrhage as it context switches.</div><div><div></div><div class="h5"><div><br>
</div><div><br><div><div>On Oct 4, 2010, at 11:06 AM, Doug Reeder wrote:</div><br><blockquote type="cite"><div style="word-wrap:break-word">In my experience hyperthreading can't really deliver two cores worth of processing simultaneously for processes expecting sole use of a core. Since you really have 512 cores I'm not surprised that you see a performance hit when requesting &gt; 512 compute units. We should really get input from a hyperthreading expert, preferably form intel.<div>
<br></div><div>Doug Reeder<br><div><div>On Oct 4, 2010, at 9:53 AM, Storm Zhang wrote:</div><br><blockquote type="cite"><span style="font-family:arial, sans-serif;font-size:13px;border-collapse:collapse"><div><span style="font-family:arial, sans-serif;font-size:13px;border-collapse:collapse">We have 64 compute nodes which are dual qual-core and hyperthreaded CPUs. So we have 1024 compute units shown in the ROCKS 5.3 system. I'm trying to scatter an array from the master node to the compute nodes using mpiCC and mpirun using C++.&nbsp;</span></div>

<div><span style="font-family:arial, sans-serif;font-size:13px;border-collapse:collapse"><br></span></div><div><span style="font-family:arial, sans-serif;font-size:13px;border-collapse:collapse">Here is my test:</span></div>

<div><span style="font-family:arial, sans-serif;font-size:13px;border-collapse:collapse"><br></span></div><div><span style="font-family:arial, sans-serif;font-size:13px;border-collapse:collapse">The array size is 18KB * Number of compute nodes and is scattered to the compute nodes 5000 times repeatly.&nbsp;</span></div>

<div><span style="font-family:arial, sans-serif;font-size:13px;border-collapse:collapse"><br></span></div><div><span style="font-family:arial, sans-serif;font-size:13px;border-collapse:collapse">The average running time(seconds):</span></div>

<div><span style="font-family:arial, sans-serif;font-size:13px;border-collapse:collapse"><br></span></div><div><span style="font-family:arial, sans-serif;font-size:13px;border-collapse:collapse">100 nodes: 170,</span></div>

<div>400 nodes: 690,</div><div>500 nodes: 855,</div><div>600 nodes: 2550,</div><div>700 nodes: 2720,</div><div>800 nodes: 2900,</div><div><br></div><div>There is a big jump of running time from 500 nodes to 600 nodes.&nbsp;<span style="font-size:small">Don't know what's the problem.&nbsp;</span></div>

<div>Tried both in OMPI 1.3.2 and OMPI 1.4.2. Running time is a little faster for all the tests in 1.4.2 but the jump still exists.&nbsp;</div><div>Tried using either Bcast function or simply Send/Recv which give very close results.&nbsp;</div>

<div>Tried both in running it directly or using SGE and got the same results.</div><div><span style="font-size:small"><br></span></div><div><span style="font-size:small"><span style="font-size:13px"><div>
<span style="font-size:small">The code and ompi_info are attached to this email. The direct running command is :</span></div><div><span style="font-size:small"><div>
/opt/openmpi/bin/mpirun --mca btl_tcp_if_include eth0 --machinefile ../machines -np 600 scatttest</div><div><br></div><div>The ifconfig of head node for eth0 is:</div><div><div>eth0 &nbsp; &nbsp; &nbsp;Link encap:Ethernet &nbsp;HWaddr 00:26:B9:56:8B:44 &nbsp;</div>

<div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;inet addr:192.168.1.1 &nbsp;Bcast:192.168.1.255 &nbsp;Mask:255.255.255.0</div><div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;inet6 addr: fe80::226:b9ff:fe56:8b44/64 Scope:Link</div><div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;UP BROADCAST RUNNING MULTICAST &nbsp;MTU:1500 &nbsp;Metric:1</div>

<div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RX packets:1096060373 errors:0 dropped:2512622 overruns:0 frame:0</div><div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;TX packets:513387679 errors:0 dropped:0 overruns:0 carrier:0</div><div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;collisions:0 txqueuelen:1000&nbsp;</div><div>

&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RX bytes:832328807459 (775.1 GiB) &nbsp;TX bytes:250824621959 (233.5 GiB)</div><div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Interrupt:106 Memory:d6000000-d6012800&nbsp;</div></div><div><br></div><div>A typical ifconfig of a compute node is:</div><div>

<div>eth0 &nbsp; &nbsp; &nbsp;Link encap:Ethernet &nbsp;HWaddr 00:21:9B:9A:15:AC &nbsp;</div><div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;inet addr:192.168.1.253 &nbsp;Bcast:192.168.1.255 &nbsp;Mask:255.255.255.0</div><div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;inet6 addr: fe80::221:9bff:fe9a:15ac/64 Scope:Link</div>

<div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;UP BROADCAST RUNNING MULTICAST &nbsp;MTU:1500 &nbsp;Metric:1</div><div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RX packets:362716422 errors:0 dropped:0 overruns:0 frame:0</div><div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;TX packets:349967746 errors:0 dropped:0 overruns:0 carrier:0</div>

<div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;collisions:0 txqueuelen:1000&nbsp;</div><div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RX bytes:139699954685 (130.1 GiB) &nbsp;TX bytes:338207741480 (314.9 GiB)</div><div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Interrupt:82 Memory:d6000000-d6012800&nbsp;</div></div><div><br></div></span></div>

</span></span></div><div><span style="font-size:small"><br></span></div><div><span style="font-size:small">Does anyone help me out of this? It bothers me a lot.</span></div>
<div><span style="font-size:small"><br></span></div><div><span style="font-size:small">Thank you very much.</span></div><div><span style="font-size:small"><br>
</span></div><div><span style="font-size:small">Linbao</span></div></span>
<span>&lt;scatttest.cpp&gt;</span><span>&lt;ompi_info&gt;</span>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div></div></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote></div><br></body></html>
