Does it means we have to split the MPI_Get to many 2GB parts?<br><br>I have a MPI programm which first serialize a object, sending to other process. The char array after serialize is just below 2GB now, but the data is increasing.<br>

<br>One method is to build a large type with MPI_Type_vector, align the char array to the upper bound. Send and Recv using the created large size type. I think this is better than split send and recv.<br><br>Is there any graceful methods to avoid the problem? Or, I think, using size_t(or ssize_t) as the length parameters is more reasonable in new mpi implementation?<br>

<br clear="all">                                                      Changsheng Jiang<br>
<br><br><div class="gmail_quote">On Thu, Jul 8, 2010 at 01:39, Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">

Sorry for the delay in replying.  :-(<br>
<br>
It&#39;s because for a 32 bit signed int, at 2GB, the value turns negative.<br>
<div><div></div><div class="h5"><br>
<br>
On Jun 29, 2010, at 1:46 PM, Price, Brian M (N-KCI) wrote:<br>
<br>
&gt; OpenMPI version: 1.3.3<br>
&gt;<br>
&gt; Platform: IBM P5<br>
&gt;<br>
&gt; Built OpenMPI 64-bit (i.e., CFLAGS=-q64, CXXFLAGS=-q64, -FFLAGS=-q64, -FCFLAGS=-q64)<br>
&gt;<br>
&gt; FORTRAN 90 test program:<br>
&gt; -          Create a large array (3.6 GB of 32-bit INTs)<br>
&gt; -          Initialize MPI<br>
&gt; -          Create a large window to encompass large array (3.6 GB)<br>
&gt; -          Have PE 0 get 1 32-bit INT from PE1<br>
&gt; o   Lock the window<br>
&gt; o   MPI_GET<br>
&gt; o   Unlock the window<br>
&gt; -          Free the window<br>
&gt; -          Finalize MPI<br>
&gt;<br>
&gt; Built FORTRAN 90 test program 64-bit using OpenMPI wrapper compiler (mpif90 –q64).<br>
&gt;<br>
&gt; Why would this MPI_GET work fine with displacements all the way up to just under 2 GB, and then fail as soon as the displacement hits 2 GB?<br>
&gt;<br>
&gt; The MPI_GET succeeds with a displacement of 2147483644 (4 bytes less than 2 GB).<br>
&gt;<br>
&gt; I get a segmentation fault (address not mapped) when the displacement is 2147483648 (2 GB) or larger.<br>
&gt;<br>
&gt; Thanks.<br>
&gt;<br>
</div></div>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to:<br>
<a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br>

