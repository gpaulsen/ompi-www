<br clear="all">Hi, <br><br>I am trying to distribute large amount of data using MPI. <br><br>When I exceed the certain data size the segmentation fault occurs. <br><br>Here is my code, <br>   <br>     <br>#include &quot;mpi.h&quot;
<br>#include &lt;stdio.h&gt;<br>#include &lt;stdlib.h&gt;<br>#include &lt;string.h&gt;
<br>#define  ARRAYSIZE    2000000
<br>#define  MASTER        0
<br> <br>int  data[ARRAYSIZE];<br> <br> <br>int main(int argc, char* argv[])<br>{
<br>int   numtasks, taskid, rc, dest, offset, i, j, tag1, tag2, source, chunksize, namelen; 
<br>int mysum, sum;
<br>int update(int myoffset, int chunk, int myid);<br>char myname[MPI_MAX_PROCESSOR_NAME];
<br>MPI_Status status;<br>double start, stop, time;<br>double totaltime;<br>FILE *fp;<br>char line[128];<br>char element;<br>int n;<br>int k=0;<br><br> <br> <br>/***** Initializations *****/
<br>MPI_Init(&amp;argc, &amp;argv);
<br>MPI_Comm_size(MPI_COMM_WORLD, &amp;numtasks);
<br>MPI_Comm_rank(MPI_COMM_WORLD,&amp;taskid); <br>MPI_Get_processor_name(myname, &amp;namelen);
<br>printf (&quot;MPI task %d has started on host %s...\n&quot;, taskid, myname);
<br>chunksize = (ARRAYSIZE / numtasks);
<br>tag2 = 1;
<br>tag1 = 2;<br> <br> <br>/***** Master task only ******/
<br>if (taskid == MASTER){<br><br>   /* Initialize the array */
<br>  sum = 0;
<br>  for(i=0; i&lt;ARRAYSIZE; i++) {
<br>    data[i] =  i * 1 ;
<br>    sum = sum + data[i];
<br>    }
<br>  printf(&quot;Initialized array sum = %d\n&quot;,sum); <br> <br>  /* Send each task its portion of the array - master keeps 1st part */
<br>  offset = chunksize;
<br>  for (dest=1; dest&lt;numtasks; dest++) {
<br>    MPI_Send(&amp;offset, 1, MPI_INT, dest, tag1, MPI_COMM_WORLD);
<br>    MPI_Send(&amp;data[offset], chunksize, MPI_INT, dest, tag2, MPI_COMM_WORLD);
<br>    printf(&quot;Sent %d elements to task %d offset= %d\n&quot;,chunksize,dest,offset);
<br>    offset = offset + chunksize;
<br>    }
<br> <br>  /* Master does its part of the work */
<br>  offset = 0;
<br>  mysum = update(offset, chunksize, taskid);
<br> <br>  /* Wait to receive results from each task */
<br>  for (i=1; i&lt;numtasks; i++) {
<br>    source = i;
<br>    MPI_Recv(&amp;offset, 1, MPI_INT, source, tag1, MPI_COMM_WORLD, &amp;status);
<br>    MPI_Recv(&amp;data[offset], chunksize, MPI_INT, source, tag2,
<br>      MPI_COMM_WORLD, &amp;status);
<br>    }
<br> <br>  /* Get final sum and print sample results */  
<br>  MPI_Reduce(&amp;mysum, &amp;sum, 1, MPI_INT, MPI_SUM, MASTER, MPI_COMM_WORLD);
<br> /* printf(&quot;Sample results: \n&quot;);
<br>  offset = 0;
<br>  for (i=0; i&lt;numtasks; i++) {
<br>    for (j=0; j&lt;5; j++) 
<br>      printf(&quot;  %d&quot;,data[offset+j]);ARRAYSIZE
<br>    printf(&quot;\n&quot;);
<br>    offset = offset + chunksize;
<br>    }*/
<br>  printf(&quot;\n*** Final sum= %d ***\n&quot;,sum);
<br> <br>  }  /* end of master section */
<br> <br> <br>#include &lt;stdlib.h&gt;
<br>/***** Non-master tasks only *****/
<br> <br>if (taskid &gt; MASTER) {
<br> <br>  /* Receive my portion of array from the master task */<br>  start= MPI_Wtime();
<br>  source = MASTER;
<br>  MPI_Recv(&amp;offset, 1, MPI_INT, source, tag1, MPI_COMM_WORLD, &amp;status);
<br>  MPI_Recv(&amp;data[offset], chunksize, MPI_INT, source, tag2,MPI_COMM_WORLD, &amp;status);<br> <br>  mysum = update(offset, chunksize, taskid);<br>  stop = MPI_Wtime();<br>  time = stop -start;<br>  printf(&quot;time taken by process %d to recieve elements and caluclate own sum is = %lf seconds \n&quot;, taskid, time);<br>
  totaltime = totaltime + time;
<br> <br>  /* Send my results back to the master task */
<br>  dest = MASTER;
<br>  MPI_Send(&amp;offset, 1, MPI_INT, dest, tag1, MPI_COMM_WORLD);
<br>  MPI_Send(&amp;data[offset], chunksize, MPI_INT, MASTER, tag2, MPI_COMM_WORLD);
<br> <br>  MPI_Reduce(&amp;mysum, &amp;sum, 1, MPI_INT, MPI_SUM, MASTER, MPI_COMM_WORLD);
<br> <br>  } /* end of non-master */
<br> <br>// printf(&quot;Total time taken for distribution is -  %lf  seconds&quot;, totaltime);
<br>MPI_Finalize();
<br> <br>}   /* end of main */
<br> <br> <br>int update(int myoffset, int chunk, int myid) {
<br>  int i,j; 
<br>  int mysum;<br>  int mydata[myoffset+chunk];
<br>  /* Perform addition to each of my array elements and keep my sum */
<br>  mysum = 0;<br> /*  printf(&quot;task %d has elements:&quot;,myid);<br>  for(j = myoffset; j&lt;myoffset+chunk; j++){<br>      printf(&quot;\t%d&quot;, data[j]);<br>  }<br> printf(&quot;\n&quot;);*/
<br>  for(i=myoffset; i &lt; myoffset + chunk; i++) {<br>    
<br>    //data[i] = data[i] + i;
<br>    mysum = mysum + data[i];
<br>    }
<br>  printf(&quot;Task %d has sum = %d\n&quot;,myid,mysum);
<br>  return(mysum);
<br>}
<br><br><br>When I run it with ARRAYSIZE = 2000000

The program works fine. But when I increase the size ARRAYSIZE = 20000000. The program ends with segmentation fault.<br>I am running it on a cluster (machine 4 is master, machine 5,6 are slaves)  and np=20<br><br>MPI task 0 has started on host machine4<br>
MPI task 2 has started on host machine4<br>MPI task 3 has started on host machine4<br>MPI task 14 has started on host machine4<br>MPI task 8 has started on host machine6<br>MPI task 10 has started on host machine6<br>MPI task 13 has started on host machine4<br>
MPI task 4 has started on host machine5<br>MPI task 6 has started on host machine5<br>MPI task 7 has started on host machine5<br>MPI task 16 has started on host machine5<br>MPI task 11 has started on host machine6<br>MPI task 12 has started on host machine4<br>
MPI task 5 has started on hostmachine5<br>MPI task 17 has started on host machine5<br>MPI task 18 has started on host machine5<br>MPI task 15 has started on host machine4<br>MPI task 19 has started on host machine5<br>MPI task 1 has started on host machine4<br>
MPI task 9 has started on host machine6<br>Initialized array sum = 542894464<br>Sent 1000000 elements to task 1 offset= 1000000<br>Task 1 has sum = 1055913696<br>time taken by process 1 to recieve elements and caluclate own sum is = 0.249345 seconds <br>
Sent 1000000 elements to task 2 offset= 2000000<br>Sent 1000000 elements to task 3 offset= 3000000<br>Task 2 has sum = 328533728<br>time taken by process 2 to recieve elements and caluclate own sum is = 0.274285 seconds <br>
Sent 1000000 elements to task 4 offset= 4000000<br>--------------------------------------------------------------------------<br>mpirun noticed that process rank 3 with PID 5695 on node machine4 exited on signal 11 (Segmentation fault).<br>
<br>Any idea what could be wrong here?<br><br> <br>-- <br><div><span style="font-size:13px;border-collapse:collapse"><font color="#000099" face="verdana, sans-serif"><br><font>Best Regards,</font></font></span></div><div>
<span style="font-size:13px;border-collapse:collapse"><font face="verdana, sans-serif"><font color="#000099"><br>ROHAN DESHPANDE</font><font>  </font></font></span></div><div><span style="font-size:13px"><br><font color="#666666" face="georgia, serif"><br>
</font></span></div><br>

