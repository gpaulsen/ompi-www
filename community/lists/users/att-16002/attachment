No, I can&#39;t. I&#39;m not a administrator of the cluster and I&#39;m not the owner.<br><br><div class="gmail_quote">2011/3/26 Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;</span><br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;"><div style="word-wrap:break-word">Can you update to a more recent version? That version is several years out-of-date - we don&#39;t even really support it any more.<div>
<div></div><div class="h5"><div><br></div><div><br><div><div>On Mar 26, 2011, at 1:04 PM, Michele Marena wrote:</div><br><blockquote type="cite">Yes, the syntax is wrong in the email, but I write it correctly when I launch mpirun. When some communicating processes are on the same node the application don&#39;t terminate, otherwise the application terminate and its results are correct. My OpenMPI version is 1.2.7.<br>

<br><div class="gmail_quote">2011/3/26 Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span><br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">

<div><br>
On Mar 26, 2011, at 11:34 AM, Michele Marena wrote:<br>
<br>
&gt; Hi,<br>
&gt; I&#39;ve a problem with shared memory. When my application runs using pure message passing (one process for node), it terminates and returns correct results. When 2 processes share a node and use shared memory for exchanges messages, my application don&#39;t terminate. At shell I write &quot;mpirun -nolocal --mca self,sm,tcp ... (forces interface to eth0)... -np (number of processes) executable arguments&quot;.<br>


<br>
</div>That&#39;s not right. If you want the apps to use shared memory, you don&#39;t have to do anything. However, if you do want to specify, then the correct syntax is<br>
<br>
mpirun -mca btl self,sm,tcp<br>
<div><br>
<br>
&gt;<br>
&gt; I hope you help me.<br>
&gt; I thank you.<br>
</div>&gt; Michele _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

