<div dir="ltr">Ralph, thanks for the quick reply. Is cross-job fast transport like InfiniBand supported? <div><br></div><div>Louis <br><br><div class="gmail_quote"><div dir="ltr">On Tue, Jun 14, 2016 at 3:53 PM Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">Nope - we don’t currently support cross-job shared memory operations. Nathan has talked about doing so for vader, but not at this time.<div><br></div><div><br><div><blockquote type="cite"></blockquote></div></div></div><div style="word-wrap:break-word"><div><div><blockquote type="cite"><div>On Jun 14, 2016, at 12:38 PM, Louis Williams &lt;<a href="mailto:louis.williams@gatech.edu" target="_blank">louis.williams@gatech.edu</a>&gt; wrote:</div><br></blockquote></div></div></div><div style="word-wrap:break-word"><div><div><blockquote type="cite"><div><div dir="ltr"><span style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">Hi,</span><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px"><br></div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">I am attempting to use the sm and vader BTLs between a client and server process, but it seems impossible to use fast transports (i.e. not TCP) between two independent groups started with two separate mpirun invocations. Am I correct, or is there a way to communicate using shared memory between a client and server like this? <span style="line-height:1.5">It seems this might be the case: </span><a href="https://github.com/open-mpi/ompi/blob/master/ompi/dpm/dpm.c#L495" style="line-height:1.5" target="_blank">https://github.com/open-mpi/ompi/blob/master/ompi/dpm/dpm.c#L495</a></div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px"><br></div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px"><span style="line-height:1.5">The server calls MPI::COMM_WORLD.Accept() and the client calls MPI::COMM_WORLD.Connect(). </span><span style="line-height:1.5">Each program is started with &quot;mpirun --np 1 --mca btl self,sm,vader &lt;exectuable&gt;&quot; where the executable is either the client or server program. </span><span style="line-height:1.5">When no BTL is specified, both establish a TCP connection just fine.</span><span style="line-height:1.5"> </span><span style="line-height:1.5">But when the sm and vader BTLs are specified, immediately after the Connect() call, both client and server exit with the message, copied at the end. It seems as though intergroup communication can&#39;t use fast transport and only uses TCP. </span></div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px"><span style="line-height:1.5"><br></span></div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">Also, as expected, running the Accept() and Connect() calls within a single program with &quot;mpirun -np 2 --mca btl self,sm,vader ...&quot; uses shared memory as transport.</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px"><br></div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">$&gt; mpirun --ompi-server &quot;3414491136.0;tcp://<a href="http://10.4.131.16:49775/" target="_blank">10.4.131.16:49775</a>&quot; -np 1 --mca btl self,vader ./server</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px"><br></div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">At least one pair of MPI processes are unable to reach each other for</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">MPI communications.  This means that no Open MPI device has indicated</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">that it can be used to communicate between these processes.  This is</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">an error; Open MPI requires that all MPI processes be able to reach</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">each other.  This error can sometimes be the result of forgetting to</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">specify the &quot;self&quot; BTL.</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px"><br></div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">  Process 1 ([[50012,1],0]) is on host: MacBook-Pro-80</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">  Process 2 ([[50010,1],0]) is on host: MacBook-Pro-80</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">  BTLs attempted: self</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px"><br></div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">Your MPI job is now going to abort; sorry.</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">--------------------------------------------------------------------------</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">[MacBook-Pro-80.local:57315] [[50012,1],0] ORTE_ERROR_LOG: Unreachable in file dpm_orte.c at line 523</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">[MacBook-Pro-80:57315] *** An error occurred in MPI_Comm_accept</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">[MacBook-Pro-80:57315] *** reported by process [7572553729,4294967296]</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">[MacBook-Pro-80:57315] *** on communicator MPI_COMM_WORLD</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">[MacBook-Pro-80:57315] *** MPI_ERR_INTERN: internal error</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">[MacBook-Pro-80:57315] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">[MacBook-Pro-80:57315] ***    and potentially your MPI job)</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">-------------------------------------------------------</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">Primary job  terminated normally, but 1 process returned</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">a non-zero exit code.. Per user-direction, the job has been aborted.</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">-------------------------------------------------------</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">--------------------------------------------------------------------------</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">mpirun detected that one or more processes exited with non-zero status, thus causing</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">the job to be terminated. The first process to do so was:</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px"><br></div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">  Process name: [[50012,1],0]</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">  Exit code:    17</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">-------------------------------------------------------------------------- </div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px"><br></div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">Thanks,</div><div style="color:rgb(33,33,33);font-family:&quot;helvetica neue&quot;,helvetica,arial,sans-serif;font-size:13px">Louis</div></div></div></blockquote></div></div></div><div style="word-wrap:break-word"><div><div><blockquote type="cite"><div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/06/29441.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/06/29441.php</a></div></blockquote></div><br></div></div>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/06/29442.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/06/29442.php</a></blockquote></div></div></div>

