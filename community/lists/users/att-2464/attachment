Hi.<br><br>&nbsp; I am having some problems in integrating OpenMPI 1.2b2 with SGE. <br><br>&nbsp; I running the DLPOLY3 code made with pathscale 2.5 compiler suite, the OS is Red Hat EL4, and network is Gigabit.<br><br>&nbsp; When I run interactively (mpirun -np 64 --hostfile ./nodes16_slots4.txt&nbsp; (...)/DLPOLY.Y, everything goes fine. But when I use SGE I got the following error:
<br>&nbsp;&nbsp;&nbsp; Signal:7 info.si_errno:0(Success) si_code:2() <br>&nbsp;&nbsp;&nbsp; Failing at addr:0x4a2823<br>&nbsp;&nbsp; (...)<br>&nbsp; [node023:07187] mca_btl_tcp_frag_send: writev failed with errno=104<br>&nbsp; [node067:06766] mca_btl_tcp_frag_send: writev failed with errno=104
<br>&nbsp; [node023:07185] mca_btl_tcp_frag_send: writev failed with errno=104<br>&nbsp; [node067:06764] mca_btl_tcp_frag_send: writev failed with errno=104<br>I configured de PE as suggest by the list[1], except for the &quot;allocation_rule&quot; that I changed to &quot;$fill_up&quot; , like O. Letho[2].
<br><br>&nbsp; The ompi_info reports the gridengine correctly<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [ocf@master TEST2]$ ompi_info | grep gridengine<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MCA ras: gridengine (MCA v1.0, API v1.3, Component v1.2)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; MCA pls: gridengine (MCA 
v1.0, API v1.3, Component v1.2)<br>and the queue has the PE<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [ocf@master TEST2]$ qconf -sq ocf.q | grep pe_list<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pe_list&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mpich-uni mpich-multi openmp<br><br>&nbsp; Does anyone has/had similar problems with SGE?
<br><br>&nbsp; Thanks for your attention.<br><br>Marcelo Garcia<br>[1] <a href="http://www.open-mpi.org/faq/?category=running#run-n1ge-or-sge">http://www.open-mpi.org/faq/?category=running#run-n1ge-or-sge</a><br>[2] <a href="http://staff.csc.fi/~oplehto/openmpi-gridengine/">
http://staff.csc.fi/~oplehto/openmpi-gridengine/</a><br>=========== PE openmp ===========================================<br>[ocf@master TEST2]$ qconf -sp openmp<br>pe_name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; openmp<br>slots&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 300<br>user_lists&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NONE
<br>xuser_lists&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NONE<br>start_proc_args&nbsp;&nbsp; /bin/true<br>stop_proc_args&nbsp;&nbsp;&nbsp; /bin/true<br>allocation_rule&nbsp;&nbsp; $fill_up<br>control_slaves&nbsp;&nbsp;&nbsp; TRUE<br>job_is_first_task FALSE<br>urgency_slots&nbsp;&nbsp;&nbsp;&nbsp; min<br>=========== PE openmp ===========================================
<br>
<br>=========== submission script&nbsp; ===========================================<br>[ocf@master TEST2]$ more test2.sh <br>#!/bin/bash<br>#$ -S /bin/bash<br>#$ -N DLPOLY2<br>#$ -q ocf.q<br>#$ -cwd<br>#$ -o dlpoly.o<br>#$ -e 
dlpoly.e<br>#$ -pe openmp 64<br>#$ -V<br><br># This does not make difference, Allways aborts.<br>export PATH=/home/ocf/ompi/bin:${PATH}<br>export LD_LIBRARY_PATH=/home/ocf/ompi/lib:${LD_LIBRARY_PATH}<br><br>DLPOLY_TEST=/home/ocf/SRIFBENCH/DLPOLY3/data/TEST2
<br>MPIRUN=/home/ocf/ompi/bin/mpirun<br><br>cd ${DLPOLY_TEST}<br>${MPIRUN} -np $NSLOTS /home/ocf/SRIFBENCH/DLPOLY3/execute/DLPOLY.Y<br>=========== submission script&nbsp; ===========================================<br>
<br>

