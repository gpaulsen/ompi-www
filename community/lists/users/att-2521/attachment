<div>Hi Ralph and Jeff,</div>
<div>&nbsp;</div>
<div>Thanks a lot for the advice. Our cluster setup is pretty&nbsp;&quot;limited&quot;, it is an OpenSSI cluster with several P3 and P4 machines connected through Ethernet. So I guess we won&#39;t be able to take full speed advantage of the current OMPI implementation. 
</div>
<div>&nbsp;</div>
<div>So far I narrowed down&nbsp;the problem (oh well, kind of solved) to share memory permission for normal user. Pretty sure the machine got enough memory since starting two processes works fine for root. I list the test cases below: 
<br>&nbsp;</div>
<div>*** As root</div>
<div>&nbsp;</div>
<div>$ mpirun --mca btl sm --np 1 tut01<br>oceanus:Hello world from 0</div>
<div>&nbsp;</div>
<div>$ mpirun --mca btl sm --np 2 tut01<br>oceanus:Hello world from 1<br>oceanus:Hello world from 0</div>
<div>&nbsp;</div>
<div>*** As normal user</div>
<div>&nbsp;</div>
<div>$ mpirun --mca btl sm --np 1 tut01<br>oceanus:Hello world from 0</div>
<div>&nbsp;</div>
<div>$ mpirun --mca btl sm --np 2 tut01<br>[oceanus:126207] mca_common_sm_mmap_init: ftruncate failed with errno=13<br>[oceanus:126207] mca_mpool_sm_init: unable to create shared memory mapping (<a>/tmp/openmpi-sessions-eddie@localhost_0/default-universe-126204/1/shared_mem_pool.localhost 
</a>).....<br>&nbsp;</div>
<div>$ free -m<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; used&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; free&nbsp;&nbsp;&nbsp;&nbsp; shared&nbsp;&nbsp;&nbsp; buffers&nbsp;&nbsp;&nbsp;&nbsp; cached<br>Mem:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 499&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 491&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 179&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 37<br>-/+ buffers/cache:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 274&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 224<br>Swap:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1027&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1027 
</div>
<div>&nbsp;</div>
<div>$ echo &quot;Hello World&quot; &gt; /tmp/eddie.txt | ll /tmp/eddie.txt<br>-rw-rw-r--&nbsp; 1 eddie eddie 12 Jan 22 11:58 /tmp/eddie.txt<br>&nbsp;</div>
<div>&nbsp;</div>
<div>Not so sure why normal user can&#39;t create shared memory for two processes, it is very strange.</div>
<div>&nbsp;</div>
<div>Cheers,</div>
<div>&nbsp;</div>
<div>Eddie.</div><br><br>
<div><span class="gmail_quote">On 1/19/07, <b class="gmail_sendername">Ralph Castain</b> &lt;<a href="mailto:rhc@lanl.gov">rhc@lanl.gov</a>&gt; wrote:</span>
<blockquote class="gmail_quote" style="PADDING-LEFT: 1ex; MARGIN: 0px 0px 0px 0.8ex; BORDER-LEFT: #ccc 1px solid">
<div><font face="Verdana, Helvetica, Arial"><span style="FONT-SIZE: 12px">What that parameter does is turn "off" all of the transports except tcp – so the problem you're seeing goes away because we no longer try to create the shared memory file. This will somewhat hurt your performance, but it will work.
<br><br>Alternatively, you could use "--mca btl ^sm", which would allow you to use whatever high speed interconnects are on your system while still turning "off" the shared memory file.<br><br>I'm not sure why your tmp directory is getting its permissions wrong. It sounds like there is something in your environment that is doing something unexpected. You might just write a script and execute it that creates a file and lists its permissions – would be interesting to see if the user or access permissions are different than what you would normally expect.
<br><br>Ralph 
<div><span class="e" id="q_1103886fe2df2c54_1"><br><br><br>On 1/18/07 8:30 PM, &quot;eddie168&quot; &lt;<a onclick="return top.js.OpenExtLink(window,event,this)" href="mailto:eddie168+ompi_user@gmail.com" target="_blank">
eddie168+ompi_user@gmail.com</a>&gt; wrote:<br><br></span></div></span></font>
<blockquote>
<div><span class="e" id="q_1103886fe2df2c54_3"><font face="Verdana, Helvetica, Arial"><span style="FONT-SIZE: 12px">Just to answer my own question, after I explicitly specify the &quot;--mca btl tcp&quot; parameter, the program works. So I will need to issue command like this:
<br>&nbsp;<br>$ mpirun --mca btl tcp -np 2 tut01<br>oceanus:Hello world from 0<br>oceanus:Hello world from 1<br>&nbsp;<br>Regards,<br>&nbsp;<br>Eddie.<br><br>&nbsp;<br>On 1/18/07, <b>eddie168</b> &lt;<a onclick="return top.js.OpenExtLink(window,event,this)" href="mailto:eddie168+ompi_user@gmail.com" target="_blank">
eddie168+ompi_user@gmail.com</a>&gt; wrote: <br></span></font></span></div>
<blockquote><font face="Verdana, Helvetica, Arial"><span style="FONT-SIZE: 12px">
<div><span class="e" id="q_1103886fe2df2c54_5">Hi Ralph and Brian,<br>&nbsp;<br>Thanks for the advice, I have checked the permission to /tmp<br>&nbsp;<br>drwxrwxrwt &nbsp;&nbsp;19 root &nbsp;root &nbsp;4096 Jan 18 11:38 tmp<br>&nbsp;<br>which I think there shouldn&#39;t be any problem to create files there, so option (a) still not work for me.
<br>&nbsp;<br>I tried option (b) which set --tmpdir on command line and run as normal user, it works for -np 1, however it gives the same error for -np 2.<br>&nbsp;<br>Option (c) also tested by setting &quot;OMPI_MCA_tmpdir_base = /home2/mpi_tut/tmp&quot; in &quot;~/.openmpi/mca-
params.conf&quot;, however error still occurred.<br>&nbsp;<br>I included the debug output of what I ran (with IP masked), I noticed that the optional tmp directory set in the beginning of the process, however it changed back to &quot;/tmp&quot; after executing orted. Could the error I got related to SSH setting? 
<br>&nbsp;<br>Many thanks,<br>&nbsp;<br>Eddie.<br>&nbsp;<br><br><br>[eddie@oceanus:~/home2/mpi_tut]$ mpirun -d --tmpdir /home2/mpi_tut/tmp -np 2 tut01<br>[oceanus:129119] [0,0,0] setting up session dir with<br>[oceanus:129119] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tmpdir /home2/mpi_tut/tmp
<br>[oceanus:129119] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;universe default-universe <br>[oceanus:129119] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user eddie<br>[oceanus:129119] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;host oceanus<br>[oceanus:129119] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;jobid 0<br>[oceanus:129119] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;procid 0<br>[oceanus:129119] procdir: /home2/mpi_tut/tmp/openmpi-
sessions-eddie@oceanus_0/default-universe/0/0 <br>[oceanus:129119] jobdir: /home2/mpi_tut/tmp/openmpi-sessions-eddie@oceanus_0/default-universe/0<br>[oceanus:129119] unidir: /home2/mpi_tut/tmp/openmpi-sessions-eddie@oceanus
_0/default-universe<br>[oceanus:129119] top: openmpi-sessions-eddie@oceanus_0<br>[oceanus:129119] tmp: /home2/mpi_tut/tmp<br>[oceanus:129119] [0,0,0] contact_file /home2/mpi_tut/tmp/openmpi-sessions-eddie@oceanus_0/default-universe/universe-
setup.txt <br>[oceanus:129119] [0,0,0] wrote setup file<br>[oceanus:129119] pls:rsh: local csh: 0, local bash: 1<br>[oceanus:129119] pls:rsh: assuming same remote shell as local shell <br>[oceanus:129119] pls:rsh: remote csh: 0, remote bash: 1 
<br>[oceanus:129119] pls:rsh: final template argv:<br>[oceanus:129119] pls:rsh: &nbsp;&nbsp;&nbsp;&nbsp;/usr/bin/ssh &lt;template&gt; orted --debug --bootproxy 1 --name &lt;template&gt; --num_procs 2 --vpid_start 0 --nodename &lt;template&gt; --universe 
eddie@oceanus:default-universe --nsreplica &quot;0.0.0;tcp://xxx.xxx.xxx.xxx:52428&quot; --gprreplica &quot; 0.0.0;tcp://xxx.xxx.xxx.xxx:52428&quot; --mpi-call-yield 0<br>[oceanus:129119] pls:rsh: launching on node localhost
<br>[oceanus:129119] pls:rsh: oversubscribed -- setting mpi_yield_when_idle to 1 (1 2)<br>[oceanus:129119] pls:rsh: localhost is a LOCAL node <br>[oceanus:129119] pls:rsh: changing to directory /home/eddie <br></span></div>
[oceanus:129119] pls:rsh: executing: orted --debug --bootproxy 1 --name 0.0.1 --num_procs 2 --vpid_start 0 --nodename localhost --universe eddie@oceanus:default-universe &nbsp;<a onclick="return top.js.OpenExtLink(window,event,this)" href="mailto:eddie@oceanus:default-universe" target="_blank">
&lt;mailto:eddie@oceanus:default-universe&gt;</a> --nsreplica &quot;0.0.0;tcp://xxx.xxx.xxx.xxx:52428&quot; --gprreplica &quot;0.0.0;tcp://xxx.xxx.xxx.xxx:52428&quot; --mpi-call-yield 1 
<div><span class="e" id="q_1103886fe2df2c54_7"><br>[oceanus:129120] [0,0,1] setting up session dir with <br>[oceanus:129120] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;universe default-universe <br>[oceanus:129120] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user eddie<br>[oceanus:129120] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;host localhost
<br>[oceanus:129120] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;jobid 0<br>[oceanus:129120] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;procid 1<br>[oceanus:129120] procdir: /tmp/openmpi-sessions-eddie@localhost_0/default-universe/0/1 <br>[oceanus:129120] jobdir: /tmp/openmpi-sessions-eddie@localhost
_0/default-universe/0<br>[oceanus:129120] unidir: /tmp/openmpi-sessions-eddie@localhost_0/default-universe <br>[oceanus:129120] top: openmpi-sessions-eddie@localhost_0<br>[oceanus:129120] tmp: /tmp <br>[oceanus:129121] [0,1,0] setting up session dir with
<br>[oceanus:129121] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;universe default-universe<br>[oceanus:129121] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user eddie<br>[oceanus:129121] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;host localhost <br>[oceanus:129121] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;jobid 1 <br>[oceanus:129121] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;procid 0<br>[oceanus:129121] procdir: /tmp/openmpi-
sessions-eddie@localhost_0/default-universe/1/0<br>[oceanus:129121] jobdir: /tmp/openmpi-sessions-eddie@localhost_0/default-universe/1 <br>[oceanus:129121] unidir: /tmp/openmpi-sessions-eddie@localhost_0/default-universe<br>
[oceanus:129121] top: openmpi-sessions-eddie@localhost_0<br>[oceanus:129121] tmp: /tmp <br>[oceanus:129122] [0,1,1] setting up session dir with<br>[oceanus:129122] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;universe default-universe<br>[oceanus:129122] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user eddie
<br>[oceanus:129122] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;host localhost <br>[oceanus:129122] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;jobid 1 <br>[oceanus:129122] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;procid 1<br>[oceanus:129122] procdir: /tmp/openmpi-sessions-eddie@localhost_0/default-universe/1/1<br>[oceanus:129122] jobdir: /tmp/openmpi-
sessions-eddie@localhost_0/default-universe/1 <br>[oceanus:129122] unidir: /tmp/openmpi-sessions-eddie@localhost_0/default-universe<br>[oceanus:129122] top: openmpi-sessions-eddie@localhost_0<br>[oceanus:129122] tmp: /tmp 
<br>[oceanus:129119] spawn: in job_state_callback(jobid = 1, state = 0x4)<br>[oceanus:129119] Info: Setting up debugger process table for applications<br>&nbsp;&nbsp;MPIR_being_debugged = 0<br>&nbsp;&nbsp;MPIR_debug_gate = 0<br>&nbsp;&nbsp;MPIR_debug_state = 1 
<br>&nbsp;&nbsp;MPIR_acquired_pre_main = 0<br>&nbsp;&nbsp;MPIR_i_am_starter = 0<br>&nbsp;&nbsp;MPIR_proctable_size = 2<br>&nbsp;&nbsp;MPIR_proctable:<br>&nbsp;&nbsp;&nbsp;&nbsp;(i, host, exe, pid) = (0, localhost, tut01, 129121) <br>&nbsp;&nbsp;&nbsp;&nbsp;(i, host, exe, pid) = (1, localhost, tut01, 129122) 
<br>[oceanus:129121] mca_common_sm_mmap_init: ftruncate failed with errno=13<br>[oceanus:129121] mca_mpool_sm_init: unable to create shared memory mapping (/tmp/openmpi-sessions-eddie@localhost_0/default-universe/1/shared_mem_pool.localhost )
<br>--------------------------------------------------------------------------<br>It looks like MPI_INIT failed for some reason; your parallel process is<br>likely to abort. &nbsp;There are many reasons that a parallel process can 
<br>fail during MPI_INIT; some of which are due to configuration or environment<br>problems. &nbsp;This failure appears to be an internal failure; here&#39;s some<br>additional information (which may only be relevant to an Open MPI 
<br>developer):<br><br>&nbsp;&nbsp;PML add procs failed<br>&nbsp;&nbsp;--&gt; Returned &quot;Out of resource&quot; (-2) instead of &quot;Success&quot; (0)<br>--------------------------------------------------------------------------<br>*** An error occurred in MPI_Init 
<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (goodbye)<br>[oceanus:129120] sess_dir_finalize: found proc session dir empty - deleting<br>[oceanus:129120] sess_dir_finalize: job session dir not empty - leaving 
<br>[oceanus:129120] sess_dir_finalize: found proc session dir empty - deleting<br>[oceanus:129120] sess_dir_finalize: found job session dir empty - deleting<br>[oceanus:129120] sess_dir_finalize: univ session dir not empty - leaving 
<br>[oceanus:129120] orted: job_state_callback(jobid = 1, state = ORTE_PROC_STATE_TERMINATED)<br>[oceanus:129120] sess_dir_finalize: job session dir not empty - leaving<br>[oceanus:129120] sess_dir_finalize: found proc session dir empty - deleting 
<br>[oceanus:129120] sess_dir_finalize: found job session dir empty - deleting<br>[oceanus:129120] sess_dir_finalize: found univ session dir empty - deleting<br>[oceanus:129120] sess_dir_finalize: found top session dir empty - deleting 
<br>[eddie@oceanus:~/home2/mpi_tut]$<br><br>&nbsp;<br>On 1/18/07, <b>Ralph H Castain</b> &lt;<a onclick="return top.js.OpenExtLink(window,event,this)" href="mailto:rhc@lanl.gov" target="_blank">rhc@lanl.gov</a>&gt; wrote: &nbsp;<br>
</span></div></span></font>
<blockquote><font face="Verdana, Helvetica, Arial"><span style="FONT-SIZE: 12px">
<div><span class="e" id="q_1103886fe2df2c54_9">Hi Eddie<br><br>Open MPI needs to create a temporary file system – what we call our &quot;session directory&quot; - where it stores things like the shared memory file. From this output, it appears that your /tmp directory is &quot;locked&quot; to root access only. 
<br><br>You have three options for resolving this problem:<br><br>(a) you could make /tmp accessible to general users;<br><br>(b) you could use the —tmpdir xxx command line option to point Open MPI at another directory that is accessible to the user (for example, you could use a &quot;tmp&quot; directory under the user&#39;s home directory); or 
<br><br>(c) you could set an MCA parameter OMPI_MCA_tmpdir_base to identify a directory we can use instead of /tmp.<br><br>&nbsp;If you select options (b) or (c), the only requirement is that this location must be accessible on every node being used. Let me be clear on this: the tmp directory 
<b>must not</b> be NSF mounted and therefore shared across all nodes. However, each node must be able to access a location of the given name – that location should be strictly local to each node.<br><br>Hope that helps<br>
Ralph <br><br><br><br><br></span></div>
<div><span class="e" id="q_1103886fe2df2c54_10">On 1/17/07 12:25 AM, &quot;eddie168&quot; &lt; <a onclick="return top.js.OpenExtLink(window,event,this)" href="mailto:eddie168+ompi_user@gmail.com" target="_blank">eddie168+ompi_user@gmail.com
</a> <a onclick="return top.js.OpenExtLink(window,event,this)" href="mailto:eddie168+ompi_user@gmail.com" target="_blank">&lt;mailto:eddie168+ompi_user@gmail.com&gt;</a> &gt; wrote:<br><br></span></div></span></font>
<div><span class="e" id="q_1103886fe2df2c54_12">
<blockquote><font face="Verdana, Helvetica, Arial"><span style="FONT-SIZE: 12px">Dear all,<br>&nbsp;<br>I have recently installed OpenMPI 1.1.2 on a OpenSSI cluster running Fedora core 3. I tested a simple hello world mpi program (attached) and it runs ok as root. However, if I run the same program under normal user, it gives the following error: 
<br>&nbsp;<br></span></font><span style="FONT-SIZE: 12px"><font face="Courier New">[eddie@oceanus:~/home2/mpi_tut]$ mpirun -np 2 tut01<br>[oceanus:125089] mca_common_sm_mmap_init: ftruncate failed with errno=13<br>[oceanus:125089] mca_mpool_sm_init: unable to create shared memory mapping ( /tmp/openmpi- 
sessions-eddie@localhost_0/default-universe/1/shared_mem_pool.localhost)<br>-------------------------------------------------------------------------- <br>It looks like MPI_INIT failed for some reason; your parallel process is 
<br>likely to abort. &nbsp;There are many reasons that a parallel process can<br>fail during MPI_INIT; some of which are due to configuration or environment <br>problems. &nbsp;This failure appears to be an internal failure; here&#39;s some 
<br>additional information (which may only be relevant to an Open MPI<br>developer):<br>&nbsp;&nbsp;PML add procs failed<br>&nbsp;&nbsp;--&gt; Returned &quot;Out of resource&quot; (-2) instead of &quot;Success&quot; (0)<br>-------------------------------------------------------------------------- 
<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (goodbye)<br>[eddie@oceanus:~/home2/mpi_tut]$<br></font><font face="Verdana, Helvetica, Arial"><br>Am I need to give certain permission to the user in order to oversubscribe processes?
<br><br>Thanks in advance,<br><br>Eddie.<br><br>&nbsp;<br><br>
<hr align="center" width="95%" size="3">
</font><font face="Monaco, Courier New">_______________________________________________<br>users mailing list<br><a onclick="return top.js.OpenExtLink(window,event,this)" href="mailto:users@open-mpi.org" target="_blank">
users@open-mpi.org</a><br><a onclick="return top.js.OpenExtLink(window,event,this)" href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></font>
</span></blockquote><span style="FONT-SIZE: 12px"><font face="Monaco, Courier New"><br></font><font face="Verdana, Helvetica, Arial"><br>_______________________________________________<br>users mailing list<br><a onclick="return top.js.OpenExtLink(window,event,this)" href="mailto:users@open-mpi.org" target="_blank">
users@open-mpi.org</a><br><a onclick="return top.js.OpenExtLink(window,event,this)" href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br></font>
</span></span></div></blockquote><span style="FONT-SIZE: 12px"><font face="Verdana, Helvetica, Arial"><br></font></span></blockquote><span style="FONT-SIZE: 12px"><font face="Verdana, Helvetica, Arial"><br><br>
<hr align="center" width="95%" size="3">
</font></span><span class="q"><font size="2"><font face="Monaco, Courier New"><span style="FONT-SIZE: 10px">_______________________________________________<br>users mailing list<br><a onclick="return top.js.OpenExtLink(window,event,this)" href="mailto:users@open-mpi.org" target="_blank">
users@open-mpi.org</a><br><a onclick="return top.js.OpenExtLink(window,event,this)" href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></span>
</font></font></span></blockquote><font size="2"><font face="Monaco, Courier New"><span style="FONT-SIZE: 10px"><br></span></font></font></div><br>_______________________________________________<br>users mailing list<br><a onclick="return top.js.OpenExtLink(window,event,this)" href="mailto:users@open-mpi.org">
users@open-mpi.org</a><br><a onclick="return top.js.OpenExtLink(window,event,this)" href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br></blockquote>
</div><br>

