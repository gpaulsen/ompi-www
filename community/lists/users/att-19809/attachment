hello 
<span style="color:rgb(34,34,34);font-family:arial,sans-serif;font-size:12.499999046325684px;background-color:rgb(255,255,255)">Hristo</span> <div><br></div><div>Thank you for your reply. I was able to understand some parts of your response, but still had some doubts due to my lack of knowledge about the way memory is allocated.</div>
<div><br></div><div>I have created a small sample program and the resulting output which will help me  pin point my question.</div><div>The program is : </div><div><br></div><div><div style="color:rgb(34,34,34);font-family:arial,sans-serif;font-size:12.499999046325684px;background-color:rgb(255,255,255)">
       <span style="white-space:pre-wrap"><b>program test
	include&#39;mpif.h&#39;
	
	integer a,b,c(10),ierr,id,datatype,size(3),type(3),i,status
	
	integer(kind=MPI_ADDRESS_KIND) add(3)</b></span></div><div style="color:rgb(34,34,34);font-family:arial,sans-serif;font-size:12.499999046325684px;background-color:rgb(255,255,255)"><span style="white-space:pre-wrap"><b>
	call MPI_INIT(ierr)
	call MPI_COMM_RANK(MPI_COMM_WORLD,id,ierr)
	call MPI_GET_ADDRESS(a,add(1),ierr)
	write(*,*) &#39;address of a ,id &#39;, add(1), id
	call MPI_GET_ADDRESS(b,add(2),ierr)
	write(*,*) &#39;address of b,id &#39;, add(2), id 
	call MPI_GET_ADDRESS(c,add(3),ierr)
	write(*,*) &#39;address of c,id &#39;, add(3), id

	add(3)=add(3)-add(1)
	add(2)=add(2)-add(1)
	add(1)=add(1)-add(1)
	
	size(1)=1
	size(2)=1
	size(3)=10
	type(1)=MPI_INTEGER
	type(2)=MPI_INTEGER
	type(3)=MPI_INTEGER
	call MPI_TYPE_CREATE_STRUCT(3,size,add,type,datatype,ierr)
	call MPI_TYPE_COMMIT(datatype,ierr)
	
	write(*,*) &#39;datatype ,id&#39;, datatype , id
	write(*,*) &#39; relative add1 &#39;,add(1), &#39;id&#39;,id
	write(*,*) &#39; relative add2 &#39;,add(2), &#39;id&#39;,id
	write(*,*) &#39; relative add3 &#39;,add(3), &#39;id&#39;,id
	if(id==0) then
	a = 1000
	b=2000
	do i=1,10
	c(i)=i
	end do
	c(10)=700
	c(1)=600
	end if</b></span></div><div style="color:rgb(34,34,34);font-family:arial,sans-serif;font-size:12.499999046325684px;background-color:rgb(255,255,255)"><span style="white-space:pre-wrap"><b><br></b></span></div><div style="color:rgb(34,34,34);font-family:arial,sans-serif;font-size:12.499999046325684px;background-color:rgb(255,255,255)">
<span style="white-space:pre-wrap"><b>        if(id==0) then
	call MPI_SEND(a,1,datatype,1,8,MPI_COMM_WORLD,ierr)
	end if

	if(id==1) then
	call MPI_RECV(a,1,datatype,0,8,MPI_COMM_WORLD,status,ierr)
	write(*,*) &#39;id =&#39;,id
	write(*,*) &#39;a=&#39; , a
	write(*,*) &#39;b=&#39; , b
	do i=1,10
	write(*,*) &#39;c(&#39;,i,&#39;)=&#39;,c(i)
	end do
	end if
	
	call MPI_FINALIZE(ierr)
	end</b></span>   
</div><div style="color:rgb(34,34,34);font-family:arial,sans-serif;font-size:12.499999046325684px;background-color:rgb(255,255,255)"><span style="white-space:pre-wrap"><b><br></b></span></div><div style="color:rgb(34,34,34);font-family:arial,sans-serif;font-size:12.499999046325684px;background-color:rgb(255,255,255)">
<span style="white-space:pre-wrap"><b> </b></span></div><div style="color:rgb(34,34,34);font-family:arial,sans-serif;font-size:12.499999046325684px;background-color:rgb(255,255,255)"><div><span style="white-space:pre-wrap">the output is <b>:</b></span></div>
<div><span style="white-space:pre-wrap"><b><br>
</b></span></div><div><div><b> address of a ,id        140736841025492           0</b></div><div><b> address of b,id        140736841025496            0</b></div><div><b> address of c,id                        6994640            0</b></div>
<div><b> datatype ,id                                         58           0</b></div><div><b>  relative add1                                      0   id      0</b></div><div><b>  relative add2                                      4   id      0</b></div>
<div><b>  relative add3         -140736834030852   id      0</b></div><div><b> address of a ,id        140736078234324           1</b></div><div><b> address of b,id         140736078234328           1</b></div><div><b> address of c,id                         6994640           1</b></div>
<div><b> datatype ,id                                         58           1</b></div><div><b>  relative add1                                     0  id        1</b></div><div><b>  relative add2                                     4 id         1</b></div>
<div><b>  relative add3       -140736071239684 id          1</b></div><div><b> id =           1</b></div><div><b> a=        1000</b></div><div><b> b=        2000</b></div><div><b> c( 1 )=         600</b></div><div><b> c( 2 )=           2</b></div>
<div><b> c( 3 )=           3</b></div><div><b> c( 4 )=           4</b></div><div><b> c(5 )=            5</b></div><div><b> c( 6 )=           6</b></div><div><b> c( 7 )=           7</b></div><div><b> c( 8 )=           8</b></div>
<div><b> c(9 )=            9</b></div><div><b> c(10 )=         700</b></div></div></div><div style="color:rgb(34,34,34);font-family:arial,sans-serif;font-size:12.499999046325684px;background-color:rgb(255,255,255)"><span style="white-space:pre-wrap"><b><br>
</b></span></div><div><br></div><div>As I had mentioned that the smaller address(of array c) is same for both the processors. However the larger ones(of &#39;a&#39; and &#39;b&#39; ) are different. This gets explained by what you had mentioned.</div>
<div><br></div><div>So the relative address of the array &#39;c &#39; with respect to &#39;a&#39; is  different for both the processors . The way I am passing data should not work(specifically the passing of array &#39;c&#39;) but still everything is correctly sent from processor 0 to 1. I have noticed that  this way of sending non contiguous data is common but I am confused why this works.</div>
<div style="color:rgb(34,34,34);font-family:arial,sans-serif;font-size:12.499999046325684px;background-color:rgb(255,255,255)"><br></div><div style="background-color:rgb(255,255,255)"><span style="color:rgb(34,34,34);font-family:arial,sans-serif;font-size:12.499999046325684px">thanks</span></div>
<div style="color:rgb(34,34,34);font-family:arial,sans-serif;font-size:12.499999046325684px;background-color:rgb(255,255,255)">priyesh</div><div class="gmail_quote">On Mon, Jul 23, 2012 at 12:00 PM,  <span dir="ltr">&lt;<a href="mailto:users-request@open-mpi.org" target="_blank">users-request@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Send users mailing list submissions to<br>
        <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<br>
To subscribe or unsubscribe via the World Wide Web, visit<br>
        <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
or, via email, send a message with subject or body &#39;help&#39; to<br>
        <a href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a><br>
<br>
You can reach the person managing the list at<br>
        <a href="mailto:users-owner@open-mpi.org">users-owner@open-mpi.org</a><br>
<br>
When replying, please edit your Subject line so it is more specific<br>
than &quot;Re: Contents of users digest...&quot;<br>
<br>
<br>
Today&#39;s Topics:<br>
<br>
   1. Efficient polling for both incoming messages and  request<br>
      completion (Geoffrey Irving)<br>
   2. checkpoint problem (=?gb2312?B?s8LLyQ==?=)<br>
   3. Re: checkpoint problem (Reuti)<br>
   4. Re: Re :Re:  OpenMP and OpenMPI Issue (Paul Kapinos)<br>
   5. Re: issue with addresses (Iliev, Hristo)<br>
<br>
<br>
----------------------------------------------------------------------<br>
<br>
Message: 1<br>
Date: Sun, 22 Jul 2012 15:01:09 -0700<br>
From: Geoffrey Irving &lt;<a href="mailto:irving@naml.us">irving@naml.us</a>&gt;<br>
Subject: [OMPI users] Efficient polling for both incoming messages and<br>
        request completion<br>
To: users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Message-ID:<br>
        &lt;CAJ1ofpdNxSVD=_<a href="mailto:FFN1j3kN9KTzjgJehB0XJF3EyL76ajwvDN2Q@mail.gmail.com">FFN1j3kN9KTzjgJehB0XJF3EyL76ajwvDN2Q@mail.gmail.com</a>&gt;<br>
Content-Type: text/plain; charset=ISO-8859-1<br>
<br>
Hello,<br>
<br>
Is it possible to efficiently poll for both incoming messages and<br>
request completion using only one thread?  As far as I know, busy<br>
waiting with alternate MPI_Iprobe and MPI_Testsome calls is the only<br>
way to do this.  Is that approach dangerous to do performance-wise?<br>
<br>
Background: my application is memory constrained, so when requests<br>
complete I may suddenly be able to schedule new computation.  At the<br>
same time, I need to be responding to a variety of asynchronous<br>
messages from unknown processors with unknown message sizes, which as<br>
far as I know I can&#39;t turn into a request to poll on.<br>
<br>
Thanks,<br>
Geoffrey<br>
<br>
<br>
------------------------------<br>
<br>
Message: 2<br>
Date: Mon, 23 Jul 2012 16:02:03 +0800<br>
From: &quot;=?gb2312?B?s8LLyQ==?=&quot; &lt;<a href="mailto:chensong@nscc-tj.gov.cn">chensong@nscc-tj.gov.cn</a>&gt;<br>
Subject: [OMPI users] checkpoint problem<br>
To: &quot;Open MPI Users&quot; &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Message-ID: &lt;<a href="mailto:4b55b3e5fc79bad3009c21962e84892c@nscc-tj.gov.cn">4b55b3e5fc79bad3009c21962e84892c@nscc-tj.gov.cn</a>&gt;<br>
Content-Type: text/plain; charset=&quot;gb2312&quot;<br>
<br>
&amp;nbsp;Hi all,How can I create ckpt files regularly? I mean, do checkpoint every 100 seconds. Is there any options to do this? Or I have to write a script myself?THANKS,---------------CHEN SongR&amp;amp;D DepartmentNational Supercomputer Center in TianjinBinhai New Area, Tianjin, China<br>

-------------- next part --------------<br>
HTML attachment scrubbed and removed<br>
<br>
------------------------------<br>
<br>
Message: 3<br>
Date: Mon, 23 Jul 2012 12:15:49 +0200<br>
From: Reuti &lt;<a href="mailto:reuti@staff.uni-marburg.de">reuti@staff.uni-marburg.de</a>&gt;<br>
Subject: Re: [OMPI users] checkpoint problem<br>
To: ?? &lt;<a href="mailto:chensong@nscc-tj.gov.cn">chensong@nscc-tj.gov.cn</a>&gt;,       Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Message-ID:<br>
        &lt;<a href="mailto:623C01F7-8D8C-4DCF-AA47-2C3EDED2811F@staff.uni-marburg.de">623C01F7-8D8C-4DCF-AA47-2C3EDED2811F@staff.uni-marburg.de</a>&gt;<br>
Content-Type: text/plain; charset=GB2312<br>
<br>
Am 23.07.2012 um 10:02 schrieb ????:<br>
<br>
&gt; How can I create ckpt files regularly? I mean, do checkpoint every 100 seconds. Is there any options to do this? Or I have to write a script myself?<br>
<br>
Yes, or use a queuing system which supports creation of a checkpoint in fixed time intervals.<br>
<br>
-- Reuti<br>
<br>
<br>
&gt; THANKS,<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; ---------------<br>
&gt; CHEN Song<br>
&gt; R&amp;D Department<br>
&gt; National Supercomputer Center in Tianjin<br>
&gt; Binhai New Area, Tianjin, China<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
<br>
------------------------------<br>
<br>
Message: 4<br>
Date: Mon, 23 Jul 2012 12:26:24 +0200<br>
From: Paul Kapinos &lt;<a href="mailto:kapinos@rz.rwth-aachen.de">kapinos@rz.rwth-aachen.de</a>&gt;<br>
Subject: Re: [OMPI users] Re :Re:  OpenMP and OpenMPI Issue<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Message-ID: &lt;<a href="mailto:500D26D0.4070704@rz.rwth-aachen.de">500D26D0.4070704@rz.rwth-aachen.de</a>&gt;<br>
Content-Type: text/plain; charset=&quot;iso-8859-1&quot;; Format=&quot;flowed&quot;<br>
<br>
Jack,<br>
note that support for THREAD_MULTIPLE is available in [newer] versions of open<br>
MPI, but disabled by default. You have to enable it by configuring, in 1.6:<br>
<br>
   --enable-mpi-thread-multiple<br>
                           Enable MPI_THREAD_MULTIPLE support (default:<br>
                           disabled)<br>
<br>
You may check the available threading supprt level by using the attaches program.<br>
<br>
<br>
On 07/20/12 19:33, Jack Galloway wrote:<br>
&gt; This is an old thread, and I&#39;m curious if there is support now for this?  I have<br>
&gt; a large code that I&#39;m running, a hybrid MPI/OpenMP code, that is having trouble<br>
&gt; over our infiniband network.  I&#39;m running a fairly large problem (uses about<br>
&gt; 18GB), and part way in, I get the following errors:<br>
<br>
You say &quot;big footprint&quot;? I hear a bell ringing...<br>
<a href="http://www.open-mpi.org/faq/?category=openfabrics#ib-low-reg-mem" target="_blank">http://www.open-mpi.org/faq/?category=openfabrics#ib-low-reg-mem</a><br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
--<br>
Dipl.-Inform. Paul Kapinos   -   High Performance Computing,<br>
RWTH Aachen University, Center for Computing and Communication<br>
Seffenter Weg 23,  D 52074  Aachen (Germany)<br>
Tel: <a href="tel:%2B49%20241%2F80-24915" value="+492418024915">+49 241/80-24915</a><br>
-------------- next part --------------<br>
A non-text attachment was scrubbed...<br>
Name: mpi_threading_support.f<br>
Type: text/x-fortran<br>
Size: 411 bytes<br>
Desc: not available<br>
URL: &lt;<a href="http://www.open-mpi.org/MailArchives/users/attachments/20120723/1f30ae61/attachment.bin" target="_blank">http://www.open-mpi.org/MailArchives/users/attachments/20120723/1f30ae61/attachment.bin</a>&gt;<br>

-------------- next part --------------<br>
A non-text attachment was scrubbed...<br>
Name: smime.p7s<br>
Type: application/pkcs7-signature<br>
Size: 4471 bytes<br>
Desc: S/MIME Cryptographic Signature<br>
URL: &lt;<a href="http://www.open-mpi.org/MailArchives/users/attachments/20120723/1f30ae61/attachment-0001.bin" target="_blank">http://www.open-mpi.org/MailArchives/users/attachments/20120723/1f30ae61/attachment-0001.bin</a>&gt;<br>

<br>
------------------------------<br>
<br>
Message: 5<br>
Date: Mon, 23 Jul 2012 11:18:32 +0000<br>
From: &quot;Iliev, Hristo&quot; &lt;<a href="mailto:iliev@rz.rwth-aachen.de">iliev@rz.rwth-aachen.de</a>&gt;<br>
Subject: Re: [OMPI users] issue with addresses<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Message-ID:<br>
        &lt;<a href="mailto:FDAA43115FAF4A4F88865097FC2C3CC9030E21BF@rz-mbx2.win.rz.rwth-aachen.de">FDAA43115FAF4A4F88865097FC2C3CC9030E21BF@rz-mbx2.win.rz.rwth-aachen.de</a>&gt;<br>
<br>
Content-Type: text/plain; charset=&quot;iso-8859-1&quot;<br>
<br>
Hello,<br>
<br>
Placement of data in memory is highly implementation dependent. I assume you<br>
are running on Linux. This OS? libc (glibc) provides two different methods<br>
for dynamic allocation of memory ? heap allocation and anonymous mappings.<br>
Heap allocation is used for small data up to MMAP_TRESHOLD bytes in length<br>
(128 KiB by default, controllable by calls to ?mallopt(3)?). Such<br>
allocations end up at predictable memory addresses as long as all processes<br>
in your MPI job allocate memory following exactly the same pattern. For<br>
larger memory blocks malloc() uses private anonymous mappings which might<br>
end at different locations in the virtual address space depending on how it<br>
is being used.<br>
<br>
What this has to do with your Fortran code? Fortran runtimes use malloc()<br>
behind the scenes to allocate automatic heap arrays as well as ALLOCATABLE<br>
ones. Small arrays are allocated on the stack usually and will mostly have<br>
the same addresses unless some stack placement randomisation is in effect.<br>
<br>
Hope that helps.<br>
<br>
Kind regards,<br>
Hristo<br>
<br>
&gt; From: <a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a> [mailto:<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>] On<br>
Behalf Of Priyesh Srivastava<br>
&gt; Sent: Saturday, July 21, 2012 10:00 PM<br>
&gt; To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subject: [OMPI users] issue with addresses<br>
&gt;<br>
&gt; Hello?<br>
&gt;<br>
&gt; I am working on a mpi program. I have been printing?the?addresses of<br>
different variables and arrays using the MPI_GET_ADDRESS command. What I<br>
have &gt; noticed is that all the processors are giving the same address of a<br>
particular variable as long as the address is less than 2 GB size. When the<br>
address of a &gt; variable/ array?is?more than 2GB size different processors<br>
are giving different addresses for the same variable. (I am working on a 64<br>
bit system and am using &gt; the new MPI Functions and MPI_ADDRESS_KIND<br>
integers for getting?the?addresses).<br>
&gt;<br>
&gt; my question is that should?all?the processors give the same address for<br>
same variables? If so then why is this not happening for variables with<br>
larger addresses.<br>
&gt;<br>
&gt;<br>
&gt; thanks<br>
&gt; priyesh<br>
<br>
--<br>
Hristo Iliev, Ph.D. -- High Performance Computing<br>
RWTH Aachen University, Center for Computing and Communication<br>
Rechen- und Kommunikationszentrum der RWTH Aachen<br>
Seffenter Weg 23,  D 52074  Aachen (Germany)<br>
Tel: <a href="tel:%2B49%20241%2080%2024367" value="+492418024367">+49 241 80 24367</a> -- Fax/UMS: <a href="tel:%2B49%20241%2080%20624367" value="+4924180624367">+49 241 80 624367</a><br>
-------------- next part --------------<br>
A non-text attachment was scrubbed...<br>
Name: smime.p7s<br>
Type: application/pkcs7-signature<br>
Size: 5494 bytes<br>
Desc: not available<br>
URL: &lt;<a href="http://www.open-mpi.org/MailArchives/users/attachments/20120723/abceb9c3/attachment.bin" target="_blank">http://www.open-mpi.org/MailArchives/users/attachments/20120723/abceb9c3/attachment.bin</a>&gt;<br>

<br>
------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
End of users Digest, Vol 2304, Issue 1<br>
**************************************<br>
</blockquote></div><br></div>

