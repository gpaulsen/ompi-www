Hi,<div>      I tried executing those tests with the other devices like tcp instead of ib with the same open-mpi 1.4.3.. It went fine but it took time to execute, when i tried to execute the same test on the customized OFED ,tests are hanging at the same message size..</div>
<div><br></div><div>Can u please tel me, what could me the possible issue over there, so that you can narrow down the issue..</div><div>i.e.. Do i have to move to open-mpi 1.5 tree or there is a issue with the customized OFED ( in RDMA scenario&#39;s or anything (if u can specify)).<br>
<br><div class="gmail_quote">On Thu, Mar 1, 2012 at 1:45 AM, Jeffrey Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div class="im">On Feb 29, 2012, at 2:57 PM, Jingcha Joba wrote:<br>
<br>
&gt; So if I understand correctly, if a message size is smaller than it will use the MPI way (non-RDMA, 2 way communication), if its larger, then it would use the Open Fabrics, by using the ibverbs (and ofed stack) instead of using the MPI&#39;s stack?<br>

<br>
</div>Er... no.<br>
<br>
So let&#39;s talk MPI-over-OpenFabrics-verbs specifically.<br>
<br>
All MPI communication calls will use verbs under the covers.  They may use verbs send/receive semantics in some cases, and RDMA semantics in other cases.  &quot;It depends&quot; -- on a lot of things, actually.  It&#39;s hard to come up with a good rule of thumb for when it uses one or the other; this is one of the reasons that the openib BTL code is so complex.  :-)<br>

<br>
The main points here are:<br>
<br>
1. you can trust the openib BTL to do the Best thing possible to get the message to the other side.  Regardless of whether that message is an MPI_SEND or an MPI_PUT (for example).<br>
<br>
2. MPI_PUT does not necessarily == verbs RDMA write (and likewise, MPI_GET does not necessarily == verbs RDMA read).<br>
<div class="im"><br>
&gt; If so, could that be the reason why the MPI_Put &quot;hangs&quot; when sending a message more than 512KB (or may be 1MB)?<br>
<br>
</div>No.  I&#39;m guessing that there&#39;s some kind of bug in the MPI_PUT implementation.<br>
<div class="im"><br>
&gt; Also is there a way to know if for a particular MPI call, OF uses send/recv or RDMA exchange?<br>
<br>
</div>Not really.<br>
<br>
More specifically: all things being equal, you don&#39;t care which is used.  You just want your message to get to the receiver/target as fast as possible.  One of the main ideas of MPI is to hide those kinds of details from the user.  I.e., you call MPI_SEND.  A miracle occurs.  The message is received on the other side.<br>

<br>
:-)<br>
<div><div></div><div class="h5"><br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br><br clear="all"><div><br></div>-- <br><span style="color:rgb(51,51,255)">Thanks &amp; Regards,<br>D.Venkateswara Rao,</span><br style="color:rgb(51,51,255)"><span style="color:rgb(51,51,255)">Software Engineer,One Convergence Devices Pvt Ltd.,<br>
Jubille Hills,Hyderabad.<br></span><br>
</div>

