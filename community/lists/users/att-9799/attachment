<div dir="ltr"><div>Hi, </div>
<div>I am not an HPL expert, but this might help.</div>
<div> </div>
<div>1.   rankfile mapper is avaliale only from Open MPI 1.3 version, if you are using Open MPI 1.2.8 try -mca mpi_paffinity_alone 1 </div>
<div>2.   if you are using Open MPI 1.3 you dont have to use mpi_leave_pinned 1 , since it&#39;s a default value</div>
<div> </div>
<div>Lenny.</div>
<div> </div>
<div class="gmail_quote">On Thu, Jul 2, 2009 at 4:47 PM, Swamy Kandadai <span dir="ltr">&lt;<a href="mailto:swamy@us.ibm.com">swamy@us.ibm.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="PADDING-LEFT: 1ex; MARGIN: 0px 0px 0px 0.8ex; BORDER-LEFT: #ccc 1px solid">
<div>
<p>Jeff:<br><br>I am running on a 2.66 GHz Nehalem node. On this node, the turbo mode and hyperthreading are enabled.<br>When I run LINPACK with Intel MPI, I get 82.68 GFlops without much trouble.<br><br>When I ran with OpenMPI (I have OpenMPI 1.2.8 but my colleague was using 1.3.2). I was using the same MKL libraries both with OpenMPI and<br>
Intel MPI. But with OpenMPI, the best I got so far is 80.22 GFlops and I could never achieve close to what I am getting with Intel MPI.<br>Here are muy options with OpenMPI:<br><br>mpirun -n 8 --machinefile hf --mca rmaps_rank_file_path rankfile --mca coll_sm_info_num_procs 8 --mca btl self,sm -mca mpi_leave_pinned 1 ./xhpl_ompi<br>
<br>Here is my rankfile:<br><br>at rankfile<br>rank 0=i02n05 slot=0<br>rank 1=i02n05 slot=1<br>rank 2=i02n05 slot=2<br>rank 3=i02n05 slot=3<br>rank 4=i02n05 slot=4<br>rank 5=i02n05 slot=5<br>rank 6=i02n05 slot=6<br>rank 7=i02n05 slot=7<br>
<br>In this case the physical cores are 0-7 while the additional logical processors with hyperthreading are 8-15.<br>With &quot;top&quot; command, I could see all the 8 tasks are running on 8 different physical cores. I did not see<br>
2 MPI tasks running on the same physical core. Also, the program is not paging as the problem size<br>fits in the meory.<br><br>Do you have any ideas how I can improve the performance so that it matches with Intel MPI performance?<br>
Any suggestions will be greatly appreciated.<br><br>Thanks<br>Swamy Kandadai<br><br><br>Dr. Swamy N. Kandadai<br>IBM Senior Certified Executive IT Specialist<br>STG WW Modular Systems Benchmark Center<br>STG WW HPC and BI CoC Benchmark Center<br>
Phone:( 845) 433 -8429 (8-293) Fax:(845)432-9789<br><a href="mailto:swamy@us.ibm.com" target="_blank">swamy@us.ibm.com</a><br><a href="http://w3.ibm.com/sales/systems/benchmarks" target="_blank">http://w3.ibm.com/sales/systems/benchmarks</a><br>
<br><br><br><br></p></div><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br></div>

