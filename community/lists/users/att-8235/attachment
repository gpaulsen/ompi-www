Hello, and thanks for both replies,<br><br>I&#39;ve tried to run non-mpi program but i still measured some latency time before starting, something around 2 seconds this time.<br>SSH should be properly configured, in fact i can login to both machines without password; openmpi and mvapich use ssh as default.<br>
<br>i&#39;ve tried these commands<br>mpirun  --mca btl ^sm  -np 2 -host node0 -host node1 ./graph<br>mpirun  --mca btl openib,self  -np 2 -host node0 -host node1 ./graph<br><br>and, apart a slight performance increase in the ^sm benchmark, the latency time is the same<br>
this is really strange, but i can&#39;t figure out the source!<br>do you have any other ideas?<br>thanks<br>Vittorio<br><br>Date: Wed, 25 Feb 2009 20:20:51 -0500<br>
From: Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>
Subject: Re: [OMPI users] 3.5 seconds before application launches<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Message-ID: &lt;<a href="mailto:86D3B246-1866-4B84-B05C-4D13659F8F1C@cisco.com">86D3B246-1866-4B84-B05C-4D13659F8F1C@cisco.com</a>&gt;<br>
Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes<br>
<br>
Dorian raises a good point.<br>
<br>
You might want to try some simple tests of launching non-MPI codes<br>
(e.g., hostname, uptime, etc.) and see how they fare.  Those will more<br>
accurately depict OMPI&#39;s launching speeds.  Getting through MPI_INIT<br>
is another matter (although on 2 nodes, the startup should be pretty<br>
darn fast).<br>
<br>
Two other things that *may* impact you:<br>
<br>
1. Is your ssh speed between the machines slow?  OMPI uses ssh by<br>
default, but will fall back to rsh (or you can force rsh if you<br>
want).  MVAPICH may use rsh by default...?  (I don&#39;t actually know)<br>
<br>
2. OMPI may be spending time creating shared memory files.  You can<br>
disable OMPI&#39;s use of shared memory by running with:<br>
<br>
     mpirun --mca btl ^sm ...<br>
<br>
Meaning &quot;use anything except the &#39;sm&#39; (shared memory) transport for<br>
MPI messages&quot;.<br>
<br>
<br>
On Feb 25, 2009, at 4:01 PM, doriankrause wrote:<br>
<br>
&gt; Vittorio wrote:<br>
&gt;&gt; Hi!<br>
&gt;&gt; I&#39;m using OpenMPI 1.3 on two nodes connected with Infiniband; i&#39;m<br>
&gt;&gt; using<br>
&gt;&gt; Gentoo Linux x86_64.<br>
&gt;&gt;<br>
&gt;&gt; I&#39;ve noticed that before any application starts there is a variable<br>
&gt;&gt; amount<br>
&gt;&gt; of time (around 3.5 seconds) in which the terminal just hangs with<br>
&gt;&gt; no output<br>
&gt;&gt; and then the application starts and works well.<br>
&gt;&gt;<br>
&gt;&gt; I imagined that there might have been some initialization routine<br>
&gt;&gt; somewhere<br>
&gt;&gt; in the Infiniband layer or in the software stack, but as i<br>
&gt;&gt; continued my<br>
&gt;&gt; tests i observed that this &quot;latency&quot; time is not present in other MPI<br>
&gt;&gt; implementations (like mvapich2) where my application starts<br>
&gt;&gt; immediately (but<br>
&gt;&gt; performs worse).<br>
&gt;&gt;<br>
&gt;&gt; Is my MPI configuration/installation broken or is this expected<br>
&gt;&gt; behaviour?<br>
&gt;&gt;<br>
&gt;<br>
&gt; Hi,<br>
&gt;<br>
&gt; I&#39;m not really qualified to answer this question, but I know that in<br>
&gt; contrast<br>
&gt; to other MPI implementations (MPICH) the modular structure of Open<br>
&gt; MPI is based<br>
&gt; on shared libs that are dlopened at the startup. As symbol<br>
&gt; relocation can be<br>
&gt; costly this might be a reason why the startup time is higher.<br>
&gt;<br>
&gt; Have you checked wether this is an mpiexec start issue or the<br>
&gt; MPI_Init call?<br>
&gt;<br>
&gt; Regards,<br>
&gt; Dorian<br>
&gt;<br>
&gt;&gt; thanks a lot!<br>
&gt;&gt; Vittorio<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; ------------------------------<div id=":3l" class="ArwC7c ckChnd">------------------------------------------<br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
Cisco Systems<br>
</div>

