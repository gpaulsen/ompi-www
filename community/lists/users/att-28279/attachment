<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    Thomas,<br>
    <br>
    i did double check this and<br>
    - there is no problem with MPI_Isend/MPI_Irecv (datatypes are
    correctly retained/released, and this part is well "hidden" inside
    some macros)<br>
    - there is no such thing with libnbc (and hence the bug). depending
    on the collective and the algo that will be chosed (depends on
    communicator and message size) you may or may not hit the bug.<br>
    <br>
    i opened <a class="moz-txt-link-freetext" href="https://github.com/open-mpi/ompi/issues/1304">https://github.com/open-mpi/ompi/issues/1304</a> in order to
    track this issue, and will start making a proof of concept from now.<br>
    <br>
    Cheers,<br>
    <br>
    Gilles<br>
    <br>
    <br>
    <div class="moz-cite-prefix">On 1/13/2016 11:00 PM, Gilles
      Gouaillardet wrote:<br>
    </div>
    <blockquote
cite="mid:CAAkFZ5vc09dZaGjjs4cN3ctSi=VWYuFRE58mZTJ7ZTEzFEvSUg@mail.gmail.com"
      type="cite">Thomas,
      <div><br>
      </div>
      <div>thanks for the report,</div>
      <div><br>
      </div>
      <div>at first glance, libnbc (the default module that implements
        non blocking collective) does not retain/release datatypes, that
        is why you ran into this kind of trouble.</div>
      <div><br>
      </div>
      <div>I quickly checked the code, and it seems this kind of
        mechanism is also missing for MPI_Isend/MPI_Irecv ...</div>
      <div><br>
      </div>
      <div>I will investigate this further</div>
      <div><br>
      </div>
      <div>Cheers,</div>
      <div><br>
      </div>
      <div>Gilles<br>
        <br>
        On Wednesday, January 13, 2016, Thomas Ponweiser &lt;<a
          moz-do-not-send="true"
          href="javascript:_e(%7B%7D,'cvml','Thomas.Ponweiser@risc-software.at');"
          target="_blank"><a class="moz-txt-link-abbreviated" href="mailto:Thomas.Ponweiser@risc-software.at">Thomas.Ponweiser@risc-software.at</a></a>&gt;
        wrote:<br>
        <blockquote class="gmail_quote" style="margin:0 0 0
          .8ex;border-left:1px #ccc solid;padding-left:1ex">Dear friends
          of Open MPI,<br>
          <br>
          I am currently facing a problem in connection with MPI_Ibcast
          and MPI_Type_free. I've been able to isolate the problem in a
          minimalistic test program which I attached.<br>
          <br>
          Maybe some of you can tell me what I am doing wrong or confirm
          that this might be a bug in Open MPI (I am using version
          1.10.1).<br>
          <br>
          Here is what I am doing:<br>
          1) I have two struct types, foo_type and bar_type, as follows:<br>
          <br>
          typedef struct<br>
          {<br>
             int v[6];<br>
             long l;<br>
          } foo_type;<br>
          <br>
          typedef struct<br>
          {<br>
             int v[3];<br>
             foo_type foo;<br>
          } bar_type;<br>
          <br>
          2) I am creating corresponding MPI types (foo_mpitype and
          bar_mpitype) with MPI_Type_create_struct.<br>
          <br>
          3) I am freeing foo_mpitype.<br>
          <br>
          4) I am broadcasting a variable of type bar_type with
          MPI_Ibcast (using count = 1 and datatype = bar_mpitype).<br>
          <br>
          5) I am freeing bar_mpitype.<br>
          <br>
          6) I am waiting for the completion of step 4) with MPI_Wait.<br>
          <br>
          In step 6) I get a segmentation fault within MPI_Wait, but
          only if the MPI job is larger than 4 processes.<br>
          <br>
          Testing with MPICH 3.2, the program seems to work just fine.<br>
          <br>
          I found out that swapping the steps 5) and 6) helps. But I
          think this should not make any difference, according to the
          description of MPI_Type_free at <a moz-do-not-send="true"
            href="http://www.mpi-forum.org/docs/mpi-1.1/mpi-11-html/node58.html"
            target="_blank">http://www.mpi-forum.org/docs/mpi-1.1/mpi-11-html/node58.html</a>:
          "Any communication that is currently using this datatype will
          complete normally." And: " Freeing a datatype does not affect
          any other datatype that was built from the freed datatype."<br>
          <br>
          (In fact, doing the same thing, that is MPI_IBcast followed by
          MPI_Type_free followed by MPI_Wait, with foo_type and
          foo_mpitype seems to work fine).<br>
          <br>
          Thanks in advance for your help,<br>
          <br>
          kind regards,<br>
          Thomas<br>
          <br>
        </blockquote>
      </div>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/01/28265.php">http://www.open-mpi.org/community/lists/users/2016/01/28265.php</a></pre>
    </blockquote>
    <br>
  </body>
</html>

