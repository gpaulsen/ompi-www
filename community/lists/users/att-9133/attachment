<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">Hi,<div><br></div><div>Yes I'm using ethernet connections. Doing as you suggest removes the errors generated by running the small test program, but still doesn't allow programs (including the small test program) to execute on any node other than the one launching mpirun. If I try to do that, the command hangs until I interrupt it, whereupon it gives the same timeout errors. It seems that there must be some problem with the setup of my Open MPI installation. Do you agree, and do you have any idea what it is? Also, is there a global settings file so I can instruct Open MPI to always only try ethernet?</div><div><br></div><div>Cheers,</div><div><br></div><div>Hugh</div><div><br><div><div>On 28 Apr 2009, at 20:12, Ralph Castain wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite">In this instance, OMPI is complaining that you are attempting to use Infiniband, but no suitable devices are found.<br><br>I assume you have Ethernet between your nodes? Can you run this with the following added to your mpirun cmd line:<br> <br>-mca btl tcp,self<br><br>That will cause OMPI to ignore the Infiniband subsystem and attempt to run via TCP over any available Ethernet.<br><br><br><br><div class="gmail_quote">On Tue, Apr 28, 2009 at 12:16 PM, Hugh Dickinson <span dir="ltr">&lt;<a href="mailto:h.j.dickinson@durham.ac.uk">h.j.dickinson@durham.ac.uk</a>></span> wrote:<br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">Many thanks for your help nonetheless.<br><font color="#888888"> <br> Hugh</font><div><div></div><div class="h5"><br> <br> On 28 Apr 2009, at 17:23, jody wrote:<br> <br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> Hi Hugh<br> <br> I'm sorry, but i must admit that i have never encountered these messages,<br> and i don't know what their cause exactly is.<br> <br> Perhaps one of the developers can give an explanation?<br> <br> Jody<br> <br> On Tue, Apr 28, 2009 at 5:52 PM, Hugh Dickinson<br> &lt;<a href="mailto:h.j.dickinson@durham.ac.uk" target="_blank">h.j.dickinson@durham.ac.uk</a>> wrote:<br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> Hi again,<br> <br> I tried a simple mpi c++ program:<br> <br> --<br> #include &lt;iostream><br> #include &lt;mpi.h><br> <br> using namespace MPI;<br> using namespace std;<br> <br> int main(int argc, char* argv[]) {<br> &nbsp;int rank,size;<br> &nbsp;Init(argc,argv);<br> &nbsp;rank=COMM_WORLD.Get_rank();<br> &nbsp;size=COMM_WORLD.Get_size();<br> &nbsp;cout &lt;&lt; "P:" &lt;&lt; rank &lt;&lt; " out of " &lt;&lt; size &lt;&lt; endl;<br> &nbsp;Finalize();<br> }<br> --<br> It didn't work over all the nodes, again same problem - the system seems to<br> hang. However, by &nbsp;forcing mpirun to use only the node on which I'm<br> launching mpirun I get some more error messages<br> <br> --<br> libibverbs: Fatal: couldn't read uverbs ABI version.<br> libibverbs: Fatal: couldn't read uverbs ABI version.<br> --------------------------------------------------------------------------<br> [0,1,0]: OpenIB on host gamma2 was unable to find any HCAs.<br> Another transport will be used instead, although this may result in<br> lower performance.<br> --------------------------------------------------------------------------<br> --------------------------------------------------------------------------<br> [0,1,1]: OpenIB on host gamma2 was unable to find any HCAs.<br> Another transport will be used instead, although this may result in<br> lower performance.<br> --------------------------------------------------------------------------<br> --------------------------------------------------------------------------<br> [0,1,1]: uDAPL on host gamma2 was unable to find any NICs.<br> Another transport will be used instead, although this may result in<br> lower performance.<br> --------------------------------------------------------------------------<br> --------------------------------------------------------------------------<br> [0,1,0]: uDAPL on host gamma2 was unable to find any NICs.<br> Another transport will be used instead, although this may result in<br> lower performance.<br> --------------------------------------------------------------------------<br> --<br> <br> However, as before the program does work in this special case, and I get:<br> --<br> P:0 out of 2<br> P:1 out of 2<br> --<br> <br> Do these errors indicate a problem with the Open MPI installation?<br> <br> Hugh<br> <br> On 28 Apr 2009, at 16:36, Hugh Dickinson wrote:<br> <br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> Hi Jody,<br> <br> I can paswordlessly ssh between all nodes (to and from)<br> Almost none of these mpirun commands work. The only working case is if<br> nodenameX is the node from which you are running the command. I don't know<br> if this gives you extra diagnostic information, but if I explicitly set the<br> wrong prefix (using --prefix), then I get errors from all the nodes telling<br> me the daemon would not start. I don't get these errors normally. It seems<br> to me that the communication is working okay, at least in the outwards<br> direction (and from all nodes). Could this be a problem with forwarding of<br> standard output? If I were to try a simple hello world program, is this more<br> likely to work, or am I just adding another layer of complexity?<br> <br> Cheers,<br> <br> Hugh<br> <br> On 28 Apr 2009, at 15:55, jody wrote:<br> <br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> Hi Hugh<br> You're right, there is no initialization command (like lamboot) &nbsp;you<br> have to call.<br> <br> I don't really know why your sewtup doesn't work, so i'm making some<br> more "blind shots"<br> <br> can you do passwordless ssh from between any two of your nodes?<br> <br> does<br> &nbsp;mpirun -np 1 --host nodenameX uptime<br> work for every X when called from any of your nodes?<br> <br> Have you tried<br> &nbsp;mpirun -np 2 --host nodename1,nodename2 &nbsp;uptime<br> (i.e. not using the host file)<br> <br> Jody<br> <br> On Tue, Apr 28, 2009 at 4:37 PM, Hugh Dickinson<br> &lt;<a href="mailto:h.j.dickinson@durham.ac.uk" target="_blank">h.j.dickinson@durham.ac.uk</a>> wrote:<br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> <br> Hi Jody,<br> <br> The node names are exactly the same. I wanted to avoid updating the<br> version<br> because I'm not the system administrator, and it could take some time<br> before<br> it gets done. If it's likely to fix the problem though I'll try it. I'm<br> assuming that I don't have to do something analogous to the old<br> "lamboot"<br> command to initialise Open MPI on all the nodes. I've seen no<br> documentation<br> anywhere that says I should.<br> <br> Cheers,<br> <br> Hugh<br> <br> On 28 Apr 2009, at 15:28, jody wrote:<br> <br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> Hi Hugh<br> <br> Again, just to make sure, are the hostnames in your host file<br> well-known?<br> I.e. when you say you can do<br> &nbsp;ssh nodename uptime<br> do you use exactly the same nodename in your host file?<br> (I'm trying to eliminate all non-Open-MPI error sources,<br> because with your setup it should basically work.)<br> <br> One more point to consider is to &nbsp;update to Open-MPI 1.3.<br> I don't think your OPen-MPI version is the cause of your trouble,<br> but there have been quite some changes since v1.2.5<br> <br> Jody<br> <br> On Tue, Apr 28, 2009 at 3:22 PM, Hugh Dickinson<br> &lt;<a href="mailto:h.j.dickinson@durham.ac.uk" target="_blank">h.j.dickinson@durham.ac.uk</a>> wrote:<br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> <br> Hi Jody,<br> <br> Indeed, all the nodes are running the same version of Open MPI.<br> Perhaps I<br> was incorrect to describe the cluster as heterogeneous. In fact, all<br> the<br> nodes run the same operating system (Scientific Linux 5.2), it's only<br> the<br> hardware that's different and even then they're all i386 or i686. I'm<br> also<br> attaching the output of ompi_info --all as I've seen it's suggested in<br> the<br> mailing list instructions.<br> <br> Cheers,<br> <br> Hugh<br> <br> Hi Hugh<br> <br> Just to make sure:<br> You have installed Open-MPI on all your nodes?<br> Same version everywhere?<br> <br> Jody<br> <br> On Tue, Apr 28, 2009 at 12:57 PM, Hugh Dickinson<br> &lt;h.j.dickinson_at_[hidden]> wrote:<br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> <br> Hi all,<br> <br> First of all let me make it perfectly clear that I'm a complete<br> beginner<br> as<br> far as MPI is concerned, so this may well be a trivial problem!<br> <br> I've tried to set up Open MPI to use SSH to communicate between nodes<br> on<br> a<br> heterogeneous cluster. I've set up passwordless SSH and it seems to<br> be<br> working fine. For example by hand I can do:<br> <br> ssh nodename uptime<br> <br> and it returns the appropriate information for each node.<br> I then tried running a non-MPI program on all the nodes at the same<br> time:<br> <br> mpirun -np 10 --hostfile hostfile uptime<br> <br> Where hostfile is a list of the 10 cluster node names with slots=1<br> after<br> each one i.e<br> <br> nodename1 slots=1<br> nodename2 slots=2<br> etc...<br> <br> Nothing happens! The process just seems to hang. If I interrupt the<br> process<br> with Ctrl-C I get:<br> <br> "<br> <br> mpirun: killing job...<br> <br> [<a href="http://gamma2.phyastcl.dur.ac.uk:18124" target="_blank">gamma2.phyastcl.dur.ac.uk:18124</a>] [0,0,0] ORTE_ERROR_LOG: Timeout in<br> file<br> base/pls_base_orted_cmds.c at line 275<br> [<a href="http://gamma2.phyastcl.dur.ac.uk:18124" target="_blank">gamma2.phyastcl.dur.ac.uk:18124</a>] [0,0,0] ORTE_ERROR_LOG: Timeout in<br> file<br> pls_rsh_module.c at line 1166<br> <br> <br> --------------------------------------------------------------------------<br> WARNING: mpirun has exited before it received notification that all<br> started processes had terminated. &nbsp;You should double check and ensure<br> that there are no runaway processes still executing.<br> <br> <br> --------------------------------------------------------------------------<br> <br> "<br> <br> If, instead of using the hostfile, I specify on the command line the<br> host<br> from which I'm running mpirun, e.g.:<br> <br> mpirun -np 1 --host nodename uptime<br> <br> then it works (i.e. if it doesn't need to communicate with other<br> nodes).<br> Do<br> I need to tell Open MPI it should be using SSH to communicate? If so,<br> how<br> do<br> I do this? To be honest I think it's trying to do so, because before<br> I<br> set<br> up passwordless SSH it challenged me for lots of passwords.<br> <br> I'm running Open MPI 1.2.5 installed with Scientific Linux 5.2. Let<br> me<br> reiterate, it's very likely that I've done something stupid, so all<br> suggestions are welcome.<br> <br> Cheers,<br> <br> Hugh<br> <br> _______________________________________________<br> users mailing list<br> users_at_[hidden]<br> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> <br> </blockquote> <br> _______________________________________________<br> users mailing list<br> <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> <br> </blockquote> <br> _______________________________________________<br> users mailing list<br> <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> </blockquote> <br> _______________________________________________<br> users mailing list<br> <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> <br> </blockquote> <br> _______________________________________________<br> users mailing list<br> <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> </blockquote> <br> _______________________________________________<br> users mailing list<br> <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> </blockquote> <br> _______________________________________________<br> users mailing list<br> <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> <br> </blockquote> <br> _______________________________________________<br> users mailing list<br> <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> </blockquote> <br> _______________________________________________<br> users mailing list<br> <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> </div></div></blockquote></div><br> _______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
