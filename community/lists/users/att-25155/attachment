<div dir="ltr">Hi Reuti,<div><br>Thanks a lot for your help. </div><div><br></div><div>The &#39;Openmp&#39; PE in our clusters has the allocation rule &#39;pe_slots&#39;. But I guess I can only use limited slots for my job under this PE...<br>
<br>The command &#39;qacct -j jobID&#39; gives the information below. It turns out the job might exceed its memory allocation. After setting a larger h_vmem (5G), it works now.<br>
<br><span style="color:rgb(102,102,102)"><i>$ qacct -j jobID</i></span></div><div><span style="color:rgb(102,102,102)"><i>...</i></span></div><div><div><span style="color:rgb(102,102,102)"><i>failed       100 : assumedly after job</i></span></div>

<div><span style="color:rgb(102,102,102)"><i>exit_status  137</i></span></div><div><span style="color:rgb(102,102,102)"><i>...</i></span></div><div><span style="color:rgb(102,102,102)"><i>maxvmem      4.003G</i></span><br>

</div><div class="gmail_extra">
<br></div><div class="gmail_extra">However, in this case, the number of slots my job can use is still limited. For example, in one cluster, the job can run for a few seconds with 10 slots. Then the job state (qstat) becomes &#39;dr&#39; and it is deleted by the shell without any error messages. In another cluster, an error message below will appear if I require more than 8 slots.</div>
<div class="gmail_extra">
<br><span style="color:rgb(102,102,102)"><i>[cl093:30366] mca_btl_mx_init: mx_open_endpoint() failed with status=20<br>--------------------------------------------------------------------------<br>[0,3,0]: Myrinet/MX on host cl093 was unable to find any endpoints.<br>


Another transport will be used instead, although this may result in<br>lower performance.</i></span><br><div>
<br></div>Anyway, it can temporarily work with 8 slots now, 
especially when these 8 slots are on the same machine coincidentally, 
which allows a large virtual memory limit. It would be better if it can be run with more slots to save computational time.<br><br></div><div class="gmail_extra">Best regards,<br></div><div class="gmail_extra">Pengcheng<br>
</div><div class="gmail_extra"><br></div><div class="gmail_extra"><br><div class="gmail_quote">On Mon, Aug 25, 2014 at 1:00 PM,  <span dir="ltr">&lt;<a href="mailto:users-request@open-mpi.org" target="_blank">users-request@open-mpi.org</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid;border-left-color:rgb(204,204,204);padding-left:1ex">
Send users mailing list submissions to<br>
        <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<br>
To subscribe or unsubscribe via the World Wide Web, visit<br>
        <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
or, via email, send a message with subject or body &#39;help&#39; to<br>
        <a href="mailto:users-request@open-mpi.org" target="_blank">users-request@open-mpi.org</a><br>
<br>
You can reach the person managing the list at<br>
        <a href="mailto:users-owner@open-mpi.org" target="_blank">users-owner@open-mpi.org</a><br>
<br>
When replying, please edit your Subject line so it is more specific<br>
than &quot;Re: Contents of users digest...&quot;<br>
<br>
<br>
Today&#39;s Topics:<br>
<br>
   1. Re: Running a hybrid MPI+openMP program (Reuti)<br>
   2. Re: A daemon on node cl231 failed to start as expected<br>
      (Pengcheng) (Pengcheng Wang)<br>
   3. Re: A daemon on node cl231 failed to start as expected<br>
      (Pengcheng) (Reuti)<br>
<br>
<br>
----------------------------------------------------------------------<br>
<br>
Message: 1<br>
Date: Mon, 25 Aug 2014 11:51:35 +0200<br>
From: Reuti &lt;<a href="mailto:reuti@staff.uni-marburg.de" target="_blank">reuti@staff.uni-marburg.de</a>&gt;<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
Subject: Re: [OMPI users] Running a hybrid MPI+openMP program<br>
Message-ID:<br>
        &lt;<a href="mailto:9EAE85F0-5479-45AF-A8F1-14519216B48C@staff.uni-marburg.de" target="_blank">9EAE85F0-5479-45AF-A8F1-14519216B48C@staff.uni-marburg.de</a>&gt;<br>
Content-Type: text/plain; charset=us-ascii<br>
<br>
Am 21.08.2014 um 16:50 schrieb Reuti:<br>
<br>
&gt; Am 21.08.2014 um 16:00 schrieb Ralph Castain:<br>
&gt;<br>
&gt;&gt;<br>
&gt;&gt; On Aug 21, 2014, at 6:54 AM, Reuti &lt;<a href="mailto:reuti@staff.uni-marburg.de" target="_blank">reuti@staff.uni-marburg.de</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt;&gt; Am 21.08.2014 um 15:45 schrieb Ralph Castain:<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; On Aug 21, 2014, at 2:51 AM, Reuti &lt;<a href="mailto:reuti@staff.uni-marburg.de" target="_blank">reuti@staff.uni-marburg.de</a>&gt; wrote:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; Am 20.08.2014 um 23:16 schrieb Ralph Castain:<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; On Aug 20, 2014, at 11:16 AM, Reuti &lt;<a href="mailto:reuti@staff.uni-marburg.de" target="_blank">reuti@staff.uni-marburg.de</a>&gt; wrote:<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Am 20.08.2014 um 19:05 schrieb Ralph Castain:<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &lt;snip&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Aha, this is quite interesting - how do you do this: scanning the /proc/&lt;pid&gt;/status or alike? What happens if you don&#39;t find enough free cores as they are used up by other applications already?<br>



&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Remember, when you use mpirun to launch, we launch our own daemons using the native launcher (e.g., qsub). So the external RM will bind our daemons to the specified cores on each node. We use hwloc to determine what cores our daemons are bound to, and then bind our own child processes to cores within that range.<br>



&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Thx for reminding me of this. Indeed, I mixed up two different aspects in this discussion.<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; a) What will happen in case no binding was done by the RM (hence Open MPI could use all cores) and two Open MPI jobs (or something completely different besides one Open MPI job) are running on the same node (due to the Tight Integration with two different Open MPI directories in /tmp and two `orted`, unique for each job)? Will the second Open MPI job know what the first Open MPI job used up already? Or will both use the same set of cores as &quot;-bind-to none&quot; can&#39;t be set in the given `mpiexec` command because of &quot;-map-by slot:pe=$OMP_NUM_THREADS&quot; was used - which triggers &quot;-bind-to core&quot; indispensable and can&#39;t be switched off? I see the same cores being used for both jobs.<br>



&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; Yeah, each mpirun executes completely independently of the other, so they have no idea what the other is doing. So the cores will be overloaded. Multi-pe&#39;s requires bind-to-core otherwise there is no way to implement the request<br>



&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; Yep, and so it&#39;s no option in a mixed cluster. Why would it hurt to allow &quot;-bind-to none&quot; here?<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Guess I&#39;m confused here - what does pe=N mean if we bind-to none?? If you are running on a mixed cluster and don&#39;t want binding, then just say bind-to none and leave the pe argument out entirely as it wouldn&#39;t mean anything unless you are bound<br>



&gt;&gt;&gt;<br>
&gt;&gt;&gt; I would mean: divide the overall number of slots/cores in the machinefile by N (i.e. $OMP_NUM_THREADS).<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; - Request made to the queuing system: I need 80 cores in total.<br>
&gt;&gt;&gt; - The machinefile will contain 80 cores<br>
&gt;&gt;&gt; - Open MPI will divide it by N, i.e. 8 here<br>
&gt;&gt;&gt; - Open MPI will start only 10 processes, one on each node<br>
&gt;&gt;&gt; - The application will use 8 threads per started MPI process<br>
&gt;&gt;<br>
&gt;&gt; I see - so you were talking about the case where the user doesn&#39;t provide the -np N option<br>
&gt;<br>
&gt; Yes. Even if -np is specified: AFAICS Open MPI fills up the given slots in the machinefile from the beginning (first nodes get all the processes, remaining nodes are free). Making it in a round-robin way would work better for this case.<br>



<br>
Could this be an option which include all cases:<br>
<br>
&gt;&gt; and we need to compute the number of procs to start. Okay, the change you requested below will fix that one too. I can make that easily enough.<br>
&gt;<br>
&gt; Therefore I wanted to start a discussion about it (at that time I wasn&#39;t aware of the &quot;-map-by slot:pe=N&quot; option), as I have no final syntax which would cover all cases. Someone may want the binding by the &quot;-map-by slot:pe=N&quot;. How can this be specified, while keeping an easy tight-integration for users who don&#39;t want any binding at all.<br>



&gt;<br>
&gt; The boundary conditions are:<br>
&gt;<br>
&gt; - the job is running inside a queuingsystem<br>
&gt; - the user requests the overall amount of slots to the queuingsystem<br>
&gt; - hence the machinefile has entries for all slots<br>
&gt; - the user sets OMP_NUM_THREADS<br>
<br>
- $OMP_NUM_THREADS set<br>
- -np set on the command line<br>
<br>
=&gt; change from fill-up to one per OMP_NUM_THREADS on each machine (including more than one)<br>
<br>
Using both as a trigger, it wouldn&#39;t touch case 2), which I don&#39;t want to be removed of course (as dividing all the time if $OMP_NUM_THREADS is used would do)<br>
<br>
-- Reuti<br>
<br>
<br>
&gt; case 1) no interest in any binding, other jobs may exist on the nodes<br>
&gt;<br>
&gt; case 2) user wants binding: i.e. $OMP_NUM_THREADS cores assigned to each MPI process, maybe with &quot;-map-by slot:pe=N&quot;<br>
&gt;<br>
&gt; In both cases only (overall amount of slots) / ($OMP_NUM_THREADS) MPI processes should be started, not (overall amount of slots) processes AFAICS.<br>
&gt;<br>
&gt; -- Reuti<br>
&gt;<br>
&gt;<br>
&gt;&gt;&gt; -- Reuti<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Altering the machinefile instead: the processes are not bound to any core, and the OS takes care of a proper assignment.<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; Here the ordinary user has to mangle the hostfile, this is not good (but allows several jobs per node as the OS shift the processes around). Could/should it be put into the &quot;gridengine&quot; module in OpenMPI, to divide the slot count per node automatically when $OMP_NUM_THREADS is found, or generate an error if it&#39;s not divisible?<br>



&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Sure, that could be done - but it will only have if OMP_NUM_THREADS is set when someone spins off threads. So far as I know, that&#39;s only used for OpenMP - so we&#39;d get a little help, but it wouldn&#39;t be full coverage.<br>



&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; ===<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; If the cores we are bound to are the same on each node, then we will do this with no further instruction. However, if the cores are different on the individual nodes, then you need to add --hetero-nodes to your command line (as the nodes appear to be heterogeneous to us).<br>



&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; b) Aha, it&#39;s not about different type CPU types, but also same CPU type but different allocations between the nodes? It&#39;s not in the `mpiexec` man-page of 1.8.1 though. I&#39;ll have a look at it.<br>



&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; I tried:<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; $ qsub -binding linear:2:0 -pe smp2 8 -masterq parallel@node01 -q parallel@node0[1-4] test_openmpi.sh<br>
&gt;&gt;&gt;&gt;&gt; Your job 247109 (&quot;test_openmpi.sh&quot;) has been submitted<br>
&gt;&gt;&gt;&gt;&gt; $ qsub -binding linear:2:1 -pe smp2 8 -masterq parallel@node01 -q parallel@node0[1-4] test_openmpi.sh<br>
&gt;&gt;&gt;&gt;&gt; Your job 247110 (&quot;test_openmpi.sh&quot;) has been submitted<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; Getting on node03:<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; 6733 ?        Sl     0:00  \_ sge_shepherd-247109 -bg<br>
&gt;&gt;&gt;&gt;&gt; 6734 ?        SNs    0:00  |   \_ /usr/sge/utilbin/lx24-amd64/qrsh_starter /var/spool/sge/node03/active_jobs/247109.1/1.node03<br>
&gt;&gt;&gt;&gt;&gt; 6741 ?        SN     0:00  |       \_ orted -mca orte_hetero_nodes 1 -mca ess env -mca orte_ess_jobid 1493303296 -mca orte_ess_vpid<br>
&gt;&gt;&gt;&gt;&gt; 6742 ?        RNl    0:31  |           \_ ./mpihello<br>
&gt;&gt;&gt;&gt;&gt; 6745 ?        Sl     0:00  \_ sge_shepherd-247110 -bg<br>
&gt;&gt;&gt;&gt;&gt; 6746 ?        SNs    0:00      \_ /usr/sge/utilbin/lx24-amd64/qrsh_starter /var/spool/sge/node03/active_jobs/247110.1/1.node03<br>
&gt;&gt;&gt;&gt;&gt; 6753 ?        SN     0:00          \_ orted -mca orte_hetero_nodes 1 -mca ess env -mca orte_ess_jobid 1506607104 -mca orte_ess_vpid<br>
&gt;&gt;&gt;&gt;&gt; 6754 ?        RNl    0:25              \_ ./mpihello<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; reuti@node03:~&gt; cat /proc/6741/status | grep Cpus_<br>
&gt;&gt;&gt;&gt;&gt; Cpus_allowed:     00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003<br>
&gt;&gt;&gt;&gt;&gt; Cpus_allowed_list:        0-1<br>
&gt;&gt;&gt;&gt;&gt; reuti@node03:~&gt; cat /proc/6753/status | grep Cpus_<br>
&gt;&gt;&gt;&gt;&gt; Cpus_allowed:     00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000030<br>
&gt;&gt;&gt;&gt;&gt; Cpus_allowed_list:        4-5<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; Hence, &quot;orted&quot; got two cores assigned for each of them. But:<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; reuti@node03:~&gt; cat /proc/6742/status | grep Cpus_<br>
&gt;&gt;&gt;&gt;&gt; Cpus_allowed:     00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003<br>
&gt;&gt;&gt;&gt;&gt; Cpus_allowed_list:        0-1<br>
&gt;&gt;&gt;&gt;&gt; reuti@node03:~&gt; cat /proc/6754/status | grep Cpus_<br>
&gt;&gt;&gt;&gt;&gt; Cpus_allowed:     00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000003<br>
&gt;&gt;&gt;&gt;&gt; Cpus_allowed_list:        0-1<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; What I see here (and in `top` + pressing &quot;1&quot;) that only two cores are used, and Open MPI assigns 0-1 to both jobs. The information in &quot;status&quot; is not the one OpenMPI gets from hwloc?<br>



&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; -- Reuti<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; The man page is probably a little out-of-date in this area - but yes, --hetero-nodes is required for *any* difference in the way the nodes appear to us (cpus, slot assignments, etc.). The 1.9 series may remove that requirement - still looking at it.<br>



&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; So it is up to the RM to set the constraint - we just live within it.<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Fine.<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; -- Reuti<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25097.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25097.php</a><br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt;&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt;&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25098.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25098.php</a><br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25106.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25106.php</a><br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25111.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25111.php</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt;&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25112.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25112.php</a><br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25113.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25113.php</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25114.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25114.php</a><br>
<br>
<br>
<br>
------------------------------<br>
<br>
Message: 2<br>
Date: Mon, 25 Aug 2014 08:23:47 -0300<br>
From: Pengcheng Wang &lt;<a href="mailto:wpc302@gmail.com" target="_blank">wpc302@gmail.com</a>&gt;<br>
To: <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subject: Re: [OMPI users] A daemon on node cl231 failed to start as<br>
        expected        (Pengcheng)<br>
Message-ID:<br>
        &lt;CAPdTcQhcsLhRoeowmC9RwhYGB2--JL0Zo2Ccj=<a href="mailto:haD1S8L9LFGw@mail.gmail.com" target="_blank">haD1S8L9LFGw@mail.gmail.com</a>&gt;<br>
Content-Type: text/plain; charset=&quot;utf-8&quot;<br>
<br>
Hi Reuti,<br>
<br>
A simple hello_world program works without the h_vmem limit. Honestly, I am<br>
not familiar with Open MPI. The command qconf -spl and qconf -sp ompi give<br>
the information below. But strangely, it begins to work after I insert *unset<br>
SGE_ROOT* in my job script. I don&#39;t know why.<br>
<br>
However, it still cannot work smoothly through 60hrs I setup. After running<br>
for about two hours, it stops without any error messages. Is this related<br>
to the h_vemem limit?<br>
<br>
*$ qconf -spl*<br>
16per<br>
1per<br>
2per<br>
4per<br>
hadoop<br>
make<br>
ompi<br>
openmp<br>
<br>
*$ qconf -sp ompi*<br>
pe_name           ompi<br>
slots             9999<br>
user_lists        NONE<br>
xuser_lists       NONE<br>
start_proc_args   /bin/true<br>
stop_proc_args    /bin/true<br>
allocation_rule   $fill_up<br>
control_slaves    TRUE<br>
job_is_first_task FALSE<br>
urgency_slots     min<br>
<br>
SGE version: 6.1u6<br>
Open MPI version: 1.2.9<br>
<br>
*Job script updated:*<br>
#$ -S /bin/bash<br>
#$ -N couple<br>
#$ -cwd<br>
#$ -j y<br>
#$ -R y<br>
#$ -l h_rt=62:00:00<br>
#$ -l h_vmem=2G<br>
#$ -o couple.out<br>
#$ -e couple.err<br>
#$ -pe ompi* 8<br>
*unset SGE_ROOT*<br>
   ./app<br>
<br>
Thanks,<br>
Pengcheng<br>
<br>
On Sun, Aug 24, 2014 at 1:00 PM, &lt;<a href="mailto:users-request@open-mpi.org" target="_blank">users-request@open-mpi.org</a>&gt; wrote:<br>
<br>
&gt; Send users mailing list submissions to<br>
&gt;         <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt;<br>
&gt; To subscribe or unsubscribe via the World Wide Web, visit<br>
&gt;         <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; or, via email, send a message with subject or body &#39;help&#39; to<br>
&gt;         <a href="mailto:users-request@open-mpi.org" target="_blank">users-request@open-mpi.org</a><br>
&gt;<br>
&gt; You can reach the person managing the list at<br>
&gt;         <a href="mailto:users-owner@open-mpi.org" target="_blank">users-owner@open-mpi.org</a><br>
&gt;<br>
&gt; When replying, please edit your Subject line so it is more specific<br>
&gt; than &quot;Re: Contents of users digest...&quot;<br>
&gt;<br>
&gt;<br>
&gt; Today&#39;s Topics:<br>
&gt;<br>
&gt;    1. Re: A daemon on node cl231 failed to start as expected (Reuti)<br>
&gt;<br>
&gt;<br>
&gt; ----------------------------------------------------------------------<br>
&gt;<br>
&gt; Message: 1<br>
&gt; Date: Sat, 23 Aug 2014 18:49:38 +0200<br>
&gt; From: Reuti &lt;<a href="mailto:reuti@staff.uni-marburg.de" target="_blank">reuti@staff.uni-marburg.de</a>&gt;<br>
&gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
&gt; Subject: Re: [OMPI users] A daemon on node cl231 failed to start as<br>
&gt;         expected<br>
&gt; Message-ID:<br>
&gt;         &lt;<a href="mailto:8F21A4D9-9E8D-4E20-9AE6-04A495A33DC9@staff.uni-marburg.de" target="_blank">8F21A4D9-9E8D-4E20-9AE6-04A495A33DC9@staff.uni-marburg.de</a>&gt;<br>
&gt; Content-Type: text/plain; charset=windows-1252<br>
&gt;<br>
&gt; Hi,<br>
&gt;<br>
&gt; Am 23.08.2014 um 16:09 schrieb Pengcheng Wang:<br>
&gt;<br>
&gt; &gt; I need to run a single driver program that only require one proc with<br>
&gt; the command mpirun -np 1 ./app or ./app. But it will schedule the launch of<br>
&gt; other executable files including parallel and sequential computing. So I<br>
&gt; require more than one proc to run it. It can be run smoothly as an<br>
&gt; interactive job with the command below.<br>
&gt; &gt;<br>
&gt; &gt; qrsh -cwd -pe &quot;ompi*&quot; 6 -l h_rt=00:30:00,test=true ./app<br>
&gt; &gt;<br>
&gt; &gt; But after I submitted the job, a strange error occurred and it<br>
&gt; stopped... Please find the job script and error message below:<br>
&gt; &gt;<br>
&gt; &gt; ? job submission script:<br>
&gt; &gt; #$ -S /bin/bash<br>
&gt; &gt; #$ -N couple<br>
&gt; &gt; #$ -cwd<br>
&gt; &gt; #$ -j y<br>
&gt; &gt; #$ -l h_rt=05:00:00<br>
&gt; &gt; #$ -l h_vmem=2G<br>
&gt;<br>
&gt; Is a simple hello_world program listing the threads working? Does it work<br>
&gt; without the h_vmem limit?<br>
&gt;<br>
&gt;<br>
&gt; &gt; #$ -o couple.out<br>
&gt; &gt; #$ -pe ompi*  6<br>
&gt;<br>
&gt; Which PEs can be addressed here? What are their allocation rules (looks<br>
&gt; like you need &quot;$pe_slots&quot;).<br>
&gt;<br>
&gt; What version of SGE?<br>
&gt; What version of Open MPI?<br>
&gt; Compiled with --with-sge?<br>
&gt;<br>
&gt; For me it&#39;s working in either way.<br>
&gt;<br>
&gt; -- Reuti<br>
&gt;<br>
&gt;<br>
&gt; &gt;     ./app<br>
&gt; &gt;<br>
&gt; &gt; error message:<br>
&gt; &gt; error: executing task of job 6777095 failed:<br>
&gt; &gt; [cl231:23777] ERROR: A daemon on node cl231 failed to start as expected.<br>
&gt; &gt; [cl231:23777] ERROR: There may be more information available from<br>
&gt; &gt; [cl231:23777] ERROR: the &#39;qstat -t&#39; command on the Grid Engine tasks.<br>
&gt; &gt; [cl231:23777] ERROR: If the problem persists, please restart the<br>
&gt; &gt; [cl231:23777] ERROR: Grid Engine PE job<br>
&gt; &gt; [cl231:23777] ERROR: The daemon exited unexpectedly with status 1.<br>
&gt; &gt;<br>
&gt; &gt; Thanks for any help!<br>
&gt; &gt;<br>
&gt; &gt; Pengcheng<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; &gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; &gt; Link to this post:<br>
&gt; <a href="http://www.open-mpi.org/community/lists/users/2014/08/25141.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25141.php</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; ------------------------------<br>
&gt;<br>
&gt; Subject: Digest Footer<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt; ------------------------------<br>
&gt;<br>
&gt; End of users Digest, Vol 2966, Issue 1<br>
&gt; **************************************<br>
-------------- next part --------------<br>
HTML attachment scrubbed and removed<br>
<br>
------------------------------<br>
<br>
Message: 3<br>
Date: Mon, 25 Aug 2014 14:16:35 +0200<br>
From: Reuti &lt;<a href="mailto:reuti@staff.uni-marburg.de" target="_blank">reuti@staff.uni-marburg.de</a>&gt;<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
Subject: Re: [OMPI users] A daemon on node cl231 failed to start as<br>
        expected        (Pengcheng)<br>
Message-ID:<br>
        &lt;<a href="mailto:E4E52447-76DB-4564-B20A-CB42EEB349E7@staff.uni-marburg.de" target="_blank">E4E52447-76DB-4564-B20A-CB42EEB349E7@staff.uni-marburg.de</a>&gt;<br>
Content-Type: text/plain; charset=us-ascii<br>
<br>
Am 25.08.2014 um 13:23 schrieb Pengcheng Wang:<br>
<br>
&gt; Hi Reuti,<br>
&gt;<br>
&gt; A simple hello_world program works without the h_vmem limit. Honestly, I am not familiar with Open MPI. The command qconf -spl and qconf -sp ompi give the information below.<br>
<br>
Thx.<br>
<br>
<br>
&gt; But strangely, it begins to work after I insert unset SGE_ROOT in my job script. I don&#39;t know why.<br>
<br>
Unsetting this variable will make Open MPI unaware that it runs under SGE. Hence it will use `ssh` to reach other machines. These `ssh` calls will have no memory or time limit set then.<br>
<br>
As you run a singleton this shouldn&#39;t matter though. But: when you want to start additional threads (according to your &quot;#$ -pe ompi*  6&quot;) you should use a PE with allocation rule &quot;$pe_slots&quot; so that all slots which SGE grants to your task are on one and the same machine.<br>



<br>
SGE will multiply the limit with the number of slots, but only with the count granted on the master node of the parallel job (resp. for each slave). How the other treads or tasks started is something you might look at.<br>



<br>
<br>
&gt; However, it still cannot work smoothly through 60hrs I setup. After running for about two hours, it stops without any error messages. Is this related to the h_vemem limit?<br>
<br>
You can have a look in $SGE_ROOT/spool/&lt;exechost&gt;/messages (resp. your actual location of the spool directories) whether any limit was passed and triggered an abortion of the job (for all granted machines for this job). Also `qacct -j &lt;job_id&gt;` might give some hint whether the was an exitcode of 137 due to a kill -9.<br>



<br>
<br>
&gt; $ qconf -spl<br>
&gt; 16per<br>
&gt; 1per<br>
&gt; 2per<br>
&gt; 4per<br>
&gt; hadoop<br>
&gt; make<br>
&gt; ompi<br>
&gt; openmp<br>
&gt;<br>
&gt; $ qconf -sp ompi<br>
&gt; pe_name           ompi<br>
&gt; slots             9999<br>
&gt; user_lists        NONE<br>
&gt; xuser_lists       NONE<br>
&gt; start_proc_args   /bin/true<br>
&gt; stop_proc_args    /bin/true<br>
&gt; allocation_rule   $fill_up<br>
<br>
This will allow to collect the slots from several machines, not necessarily all will be on one and the same machine where the jobscript runs.<br>
<br>
<br>
&gt; control_slaves    TRUE<br>
&gt; job_is_first_task FALSE<br>
&gt; urgency_slots     min<br>
&gt;<br>
&gt; SGE version: 6.1u6<br>
&gt; Open MPI version: 1.2.9<br>
<br>
Both are really old versions. I fear I can&#39;t help much here as many things changed compared to the actual version 1.8.1 of Open MPI, while also SGE&#39;s latest version is 6.2u5 with SoGE being now at 8.1.7.<br>
<br>
-- Reuti<br>
<br>
<br>
&gt; Job script updated:<br>
&gt; #$ -S /bin/bash<br>
&gt; #$ -N couple<br>
&gt; #$ -cwd<br>
&gt; #$ -j y<br>
&gt; #$ -R y<br>
&gt; #$ -l h_rt=62:00:00<br>
&gt; #$ -l h_vmem=2G<br>
&gt; #$ -o couple.out<br>
&gt; #$ -e couple.err<br>
&gt; #$ -pe ompi* 8<br>
&gt; unset SGE_ROOT<br>
&gt;    ./app<br>
&gt;<br>
&gt; Thanks,<br>
&gt; Pengcheng<br>
&gt;<br>
&gt; On Sun, Aug 24, 2014 at 1:00 PM, &lt;<a href="mailto:users-request@open-mpi.org" target="_blank">users-request@open-mpi.org</a>&gt; wrote:<br>
&gt; Send users mailing list submissions to<br>
&gt;         <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt;<br>
&gt; To subscribe or unsubscribe via the World Wide Web, visit<br>
&gt;         <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; or, via email, send a message with subject or body &#39;help&#39; to<br>
&gt;         <a href="mailto:users-request@open-mpi.org" target="_blank">users-request@open-mpi.org</a><br>
&gt;<br>
&gt; You can reach the person managing the list at<br>
&gt;         <a href="mailto:users-owner@open-mpi.org" target="_blank">users-owner@open-mpi.org</a><br>
&gt;<br>
&gt; When replying, please edit your Subject line so it is more specific<br>
&gt; than &quot;Re: Contents of users digest...&quot;<br>
&gt;<br>
&gt;<br>
&gt; Today&#39;s Topics:<br>
&gt;<br>
&gt;    1. Re: A daemon on node cl231 failed to start as expected (Reuti)<br>
&gt;<br>
&gt;<br>
&gt; ----------------------------------------------------------------------<br>
&gt;<br>
&gt; Message: 1<br>
&gt; Date: Sat, 23 Aug 2014 18:49:38 +0200<br>
&gt; From: Reuti &lt;<a href="mailto:reuti@staff.uni-marburg.de" target="_blank">reuti@staff.uni-marburg.de</a>&gt;<br>
&gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
&gt; Subject: Re: [OMPI users] A daemon on node cl231 failed to start as<br>
&gt;         expected<br>
&gt; Message-ID:<br>
&gt;         &lt;<a href="mailto:8F21A4D9-9E8D-4E20-9AE6-04A495A33DC9@staff.uni-marburg.de" target="_blank">8F21A4D9-9E8D-4E20-9AE6-04A495A33DC9@staff.uni-marburg.de</a>&gt;<br>
&gt; Content-Type: text/plain; charset=windows-1252<br>
&gt;<br>
&gt; Hi,<br>
&gt;<br>
&gt; Am 23.08.2014 um 16:09 schrieb Pengcheng Wang:<br>
&gt;<br>
&gt; &gt; I need to run a single driver program that only require one proc with the command mpirun -np 1 ./app or ./app. But it will schedule the launch of other executable files including parallel and sequential computing. So I require more than one proc to run it. It can be run smoothly as an interactive job with the command below.<br>



&gt; &gt;<br>
&gt; &gt; qrsh -cwd -pe &quot;ompi*&quot; 6 -l h_rt=00:30:00,test=true ./app<br>
&gt; &gt;<br>
&gt; &gt; But after I submitted the job, a strange error occurred and it stopped... Please find the job script and error message below:<br>
&gt; &gt;<br>
&gt; &gt; ? job submission script:<br>
&gt; &gt; #$ -S /bin/bash<br>
&gt; &gt; #$ -N couple<br>
&gt; &gt; #$ -cwd<br>
&gt; &gt; #$ -j y<br>
&gt; &gt; #$ -l h_rt=05:00:00<br>
&gt; &gt; #$ -l h_vmem=2G<br>
&gt;<br>
&gt; Is a simple hello_world program listing the threads working? Does it work without the h_vmem limit?<br>
&gt;<br>
&gt;<br>
&gt; &gt; #$ -o couple.out<br>
&gt; &gt; #$ -pe ompi*  6<br>
&gt;<br>
&gt; Which PEs can be addressed here? What are their allocation rules (looks like you need &quot;$pe_slots&quot;).<br>
&gt;<br>
&gt; What version of SGE?<br>
&gt; What version of Open MPI?<br>
&gt; Compiled with --with-sge?<br>
&gt;<br>
&gt; For me it&#39;s working in either way.<br>
&gt;<br>
&gt; -- Reuti<br>
&gt;<br>
&gt;<br>
&gt; &gt;     ./app<br>
&gt; &gt;<br>
&gt; &gt; error message:<br>
&gt; &gt; error: executing task of job 6777095 failed:<br>
&gt; &gt; [cl231:23777] ERROR: A daemon on node cl231 failed to start as expected.<br>
&gt; &gt; [cl231:23777] ERROR: There may be more information available from<br>
&gt; &gt; [cl231:23777] ERROR: the &#39;qstat -t&#39; command on the Grid Engine tasks.<br>
&gt; &gt; [cl231:23777] ERROR: If the problem persists, please restart the<br>
&gt; &gt; [cl231:23777] ERROR: Grid Engine PE job<br>
&gt; &gt; [cl231:23777] ERROR: The daemon exited unexpectedly with status 1.<br>
&gt; &gt;<br>
&gt; &gt; Thanks for any help!<br>
&gt; &gt;<br>
&gt; &gt; Pengcheng<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; &gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; &gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25141.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25141.php</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; ------------------------------<br>
&gt;<br>
&gt; Subject: Digest Footer<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt; ------------------------------<br>
&gt;<br>
&gt; End of users Digest, Vol 2966, Issue 1<br>
&gt; **************************************<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25144.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25144.php</a><br>
<br>
<br>
<br>
------------------------------<br>
<br>
Subject: Digest Footer<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
------------------------------<br>
<br>
End of users Digest, Vol 2967, Issue 1<br>
**************************************<br>
</blockquote></div><br>
</div></div></div>

