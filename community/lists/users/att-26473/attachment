<div dir="ltr">Thomas,<div><br></div><div>IWhat exactly is &#39;<span style="font-size:12.8000001907349px">local_tlr_</span><span style="font-size:12.8000001907349px">lookup(1)%wtlr&#39;?</span></div><div><span style="font-size:12.8000001907349px"><br></span></div><div><span style="font-size:12.8000001907349px">I think the problem is that your MPI derived datatype use the pointer to the allocatable arrays instead of using the pointer to the first element of these arrays. As an example instead of doing</span></div><div><span style="font-size:12.8000001907349px">call mpi_get_address(local_tlr_</span><span style="font-size:12.8000001907349px">lookup(1)%wtlr,       offsets(3), ierr)</span><span style="font-size:12.8000001907349px"><br></span></div><div><span style="font-size:12.8000001907349px">I would go for</span></div><div><span style="font-size:12.8000001907349px">call mpi_get_address(local_tlr_</span><span style="font-size:12.8000001907349px">lookup(1)%</span><span style="font-size:12.8000001907349px">TLR(1,1)</span><span style="font-size:12.8000001907349px">,       offsets(3), ierr)</span><span style="font-size:12.8000001907349px"><br></span></div><div><span style="font-size:12.8000001907349px"><br></span></div><div><span style="font-size:12.8000001907349px">Then you don&#39;t have to subtract offset(1) from the other, but instead you can go for absolute addressing. Unfortunately this approach is not compatible with sending multiple structures (aka. using a count != 1), simply because each struct might have different displacements for the internal allocatable arrays.</span></div><div><span style="font-size:12.8000001907349px"><br></span></div><div><span style="font-size:12.8000001907349px">  George.</span></div><div><span style="font-size:12.8000001907349px"><br></span></div><div><span style="font-size:12.8000001907349px"><br></span></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Sun, Mar 15, 2015 at 3:45 PM, Thomas Markovich <span dir="ltr">&lt;<a href="mailto:thomasmarkovich@gmail.com" target="_blank">thomasmarkovich@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div>Hi All,</div><div><br></div><div>I&#39;m trying to parallelize my code by distributing the computation of various elements of a lookup table and then sync that lookup table across all nodes. To make the code easier to read, and to keep track of everything easier, I&#39;ve decided to use a derived data type in fortran defined as follows:</div><div>    type tlr_lut</div><div>        sequence</div><div>        integer p</div><div>        integer q</div><div>        real(dp), dimension(3, 3) :: TLR</div><div>        real(dp), dimension(:, :, :, :) :: dTLRdr</div><div>        real(dp), dimension(3, 3, 3, 3) :: dTLRdh</div><div>        integer unique_ind</div><div>    end type tlr_lut</div><div><br></div><div>and this works quite well in serial. I just have to allocate dTLRdr at run time. This is because TLR should be size 3x3xNx3, where N is a constant known at run time but not compile time. I&#39;ve tried to create a custom data type to tell open-mpi what the size should be but I&#39;m at a loss for how to deal with the allocatable array. I&#39;ve tried something like this:</div><div><br></div><div>type(tlr_lut), dimension(:), allocatable :: tlr_lookup, temp_tlr_lookup</div><div>type(tlr_lut), dimension(:), allocatable :: local_tlr_lookup</div><div>integer :: datatype, oldtypes(6), blockcounts(6) </div><div>INTEGER(KIND=MPI_ADDRESS_KIND) :: offsets(6)</div><div>integer :: numtasks, rank, i,  ierr </div><div>integer :: n, status(mpi_status_size)</div><div><br></div><div>do i = 1, num_pairs, 1</div><div>    p = unique_pairs(i)%p</div><div>    q = unique_pairs(i)%q</div><div>    cpuid = unique_pairs(i)%cpu</div><div>    if(cpuid.eq.me_image) then</div><div>        TLR = 0.0_DP</div><div>        dTLRdr = 0.0_DP</div><div>        dTLRdh = 0.0_DP</div><div>        call mbdvdw_TLR(p, q, TLR, dTLRdr, dTLRdh)</div><div>        if(.not.allocated(local_tlr_lookup(counter)%dTLRdr)) allocate(local_tlr_lookup(counter)%dTLRdr(3, 3, nat, 3))</div><div>        local_tlr_lookup(counter)%p = p</div><div>        local_tlr_lookup(counter)%q = q</div><div>        local_tlr_lookup(counter)%TLR(:, :) = TLR(:, :)</div><div>        local_tlr_lookup(counter)%dTLRdr(:,:,:,:) = dTLRdR(:,:,:,:)</div><div>        local_tlr_lookup(counter)%dTLRdh(:,:,:,:) = dTLRdh(:,:,:,:)</div><div>    end if</div><div>end do</div><div><br></div><div>call mpi_get_address(local_tlr_lookup(1)%p,          offsets(1), ierr)</div><div>call mpi_get_address(local_tlr_lookup(1)%q,          offsets(2), ierr)</div><div>call mpi_get_address(local_tlr_lookup(1)%wtlr,       offsets(3), ierr)</div><div>call mpi_get_address(local_tlr_lookup(1)%wdtlrdr,    offsets(4), ierr)</div><div>call mpi_get_address(local_tlr_lookup(1)%wdtlrdh,    offsets(5), ierr)</div><div>call mpi_get_address(local_tlr_lookup(1)%unique_ind, offsets(6), ierr)</div><div><br></div><div>do i = 2, size(offsets)</div><div>  offsets(i) = offsets(i) - offsets(1)</div><div>end do</div><div>offsets(1) = 0</div><div><br></div><div>oldtypes = (/mpi_integer, mpi_integer, mpi_real, mpi_real, mpi_real, mpi_integer/)</div><div>blockcounts = (/1, 1, 3*3, 3*3*nat*3, 3*3*3*3, 1/)</div><div><br></div><div>But it didn&#39;t seem to work and I&#39;m sorta at a loss. Any suggestions?</div><div><br></div><div>Best,</div><div>Thomas</div></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/03/26472.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/03/26472.php</a><br></blockquote></div><br></div>

