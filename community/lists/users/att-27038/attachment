
<HTML><BODY><p>1. Here is my&nbsp;<br><strong>ompi_yalla&nbsp;command line:</strong></p><p style="margin-left: 30px;">$HPCX_MPI_DIR/bin/mpirun -mca coll_hcoll_enable 1 -x HCOLL_MAIN_IB=mlx4_0:1 -x MXM_IB_PORTS=mlx4_0:1 -x MXM_SHM_KCOPY_MODE=off --mca pml yalla --hostfile hostlist $@<br></p><p style="margin-left: 30px;">echo $HPCX_MPI_DIR <br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.330-icc-OFED-1.5.4.1-redhat6.2-x86_64/<strong>ompi-mellanox-fca-v1.8.5</strong><br>This mpi was configured with: --with-mxm=/path/to/mxm --with-hcoll=/path/to/hcoll --with-platform=contrib/platform/mellanox/optimized --prefix=/path/to/<strong>ompi-mellanox-fca-v1.8.5</strong></p><p style=""><strong><strong>ompi_clear command line:</strong></strong></p><p style="margin-left: 30px;">HPCX_MPI_DIR/bin/mpirun &nbsp;--hostfile hostlist $@<br></p><p style="margin-left: 30px;">echo $HPCX_MPI_DIR <br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.330-icc-OFED-1.5.4.1-redhat6.2-x86_64/<strong>ompi-clear-v1.8.5</strong><br>This mpi was configured with: --with-platform=contrib/platform/mellanox/optimized --prefix=/path/to<strong>/ompi-clear-v1.8.5</strong></p><p>2. I will run ompi_yalla with&nbsp;"-x MXM_TLS=self,shm,rc" and I will send you results in a few days.</p><p>3. I have alredy run ompi_yalla without hcoll &nbsp;in IMB_alltoall test. hcoll&nbsp;provides a performance boost about 10%. You can find this results in&nbsp;mvs10p_mpi.xls: list IMB_MPI1 Alltoall.</p><p><br><br>Среда,  3 июня 2015, 10:29 +03:00 от Alina Sklarevich &lt;alinas@dev.mellanox.co.il&gt;:<br>
</p><blockquote style="border-left:1px solid #0857A6; margin:10px; padding:0 0 0 10px;">
	<div id="">
	



    











	
	


	
	
	

	

	
	

	

	
	



<div class="js-helper js-readmsg-msg">
	<style type="text/css"></style>
 	<div>
		<base target="_self" href="https://e.mail.ru/">
		
            <div id="style_14333165760000000011_BODY"><div dir="ltr">Hello Timur,<div><br></div><div>I will review your results and try to reproduce them in our lab.<br></div><div><br></div><div>You are using an old OFED -&nbsp;OFED-1.5.4.1 and we suspect that this may be causing the performance issues you are seeing.<br></div><div><br></div><div>In the meantime, could you please:<br></div><div><br></div><div>1. send us the exact command lines that you were running when you got these results?</div><div><br></div><div>2. add the following to the command line that you are running with 'pml yalla' and attach the results?<br></div><div>"-x MXM_TLS=self,shm,rc"</div><div><br></div><div>3. run your command line with yalla and without hcoll?</div><div><br></div><div>Thanks,</div><div>Alina.</div><div><br></div><div><br></div></div><div><br><div>On Tue, Jun 2, 2015 at 4:56 PM, Timur Ismagilov <span dir="ltr">&lt;<a href="//e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span> wrote:<br><blockquote style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div><p>Hi, Mike!<br></p><p>I have impi v 4.1.2 (- impi)<br>I build ompi 1.8.5 with MXM and hcoll (- ompi_yalla)<br>I build ompi 1.8.5 without MXM and hcoll (- ompi_clear)</p><p>I start osu p2p: osu_mbr_mr test with this MPIs.<br></p>You can find the result of benchmark in attached file(mvs10p_mpi.xls: list osu_mbr_mr)<br><p><br>On 64 nodes (and 1024 mpi processes) ompi_yalla get 2 time worse perf than ompi_clear.<br>Is mxm with yalla <span lang="en"><span>reduces</span> <span>performance</span> in p2p <span>compared</span></span> with ompi_clear(and impi)?<br><span lang="en">Am <span>I&nbsp;</span><span>doing something</span> <span>wrong?</span></span><br></p><p>P.S. My colleague Alexander Semenov is in CC</p><p>Best regards,<br>Timur<br><br>Четверг, 28 мая 2015, 20:02 +03:00 от Mike Dubman &lt;<a href="//e.mail.ru/compose/?mailto=mailto%3amiked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt;:<br>
</p><blockquote style="border-left:1px solid #0857a6;margin:10px;padding:0 0 0 10px">
	<div>
	



    











	
	


	
	
	

	

	
	

	

	
	



<div>
	
 	<div>
		
		
            <div><div dir="ltr">it is not apples-to-apples comparison.<div><br></div><div>yalla/mxm is point-to-point library, it is not collective library.</div><div>collective algorithm happens on top of yalla.</div><div><br></div><div>Intel collective algorithm for a2a is better than OMPI built-in collective algorithm.</div><div><br></div><div>To see benefit of yalla - you should run p2p benchmarks (osu_lat/bw/bibw/mr)</div><div><br></div></div><div><br><div>On Thu, May 28, 2015 at 7:35 PM, Timur Ismagilov <span dir="ltr">&lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span> wrote:<br><blockquote style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div>I compare ompi-1.8.5 (hpcx-1.3.3-icc) with impi v 4.1.4.<br><br>I build ompi with MXM but without HCOLL and without&nbsp; knem (I work on it). Configure options are:<br>&nbsp;./configure&nbsp; --prefix=my_prefix&nbsp;&nbsp; --with-mxm=path/to/hpcx/hpcx-v1.3.330-icc-OFED-1.5.4.1-redhat6.2-x86_64/mxm&nbsp;&nbsp; --with-platform=contrib/platform/mellanox/optimized<br><br>As a result of the IMB-MPI1 Alltoall test, I have got disappointing&nbsp; results: for the most message sizes on 64 nodes and 16 processes per&nbsp; node impi is much (~40%) better.<br><br>You can look at the results in the file "mvs10p_mpi.xlsx", I attach it. System configuration is also there.<br><br>What do you think about? Is there any way to improve ompi yalla performance results?<br><br>I attach the output of&nbsp; "IMB-MPI1 Alltoall" for yalla and impi.<br><br>P.S. My colleague Alexander Semenov is in CC<br><br>Best regards,<br>Timur</div>
</blockquote></div><br><br clear="all"><span><font color="#888888"><div><br></div>-- <br><div><div dir="ltr"><br><div>Kind Regards,</div><div><br></div><div>M.</div></div></div>
</font></span></div>
</div>
            
        
		
	</div>

	
</div>


</div>
</blockquote>
<br>
<br> <br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="//e.mail.ru/compose/?mailto=mailto%3ausers@open%2dmpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/06/27029.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/06/27029.php</a><br></blockquote></div><br></div>
</div>
            
        
		<base target="_self" href="https://e.mail.ru/">
	</div>

	
</div>


</div>
</blockquote>
<br>
<br><br></BODY></HTML>
