<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">Dear Open MPI experts,<div><br></div><div>I have a problem that is related to the integration of OpenMPI, slurm and PMI interface. I spent some time today with a colleague of mine trying to figure out why we were not able to obtain all H5 profile files (generated by&nbsp;acct_gather_profile) using Open MPI. When I say "all" I mean if I run using 8 nodes (e.g. tesla[121-128]) then I always systematically miss the file related to the first one (the first node in the allocation list, in this case tesla121).</div><div><br></div><div>By comparing which processes are spawn on the compute nodes, I discovered that mpirun running on tesla121 calls srun only to spawn remotely new MPI processes to the other 7 nodes (maybe this is obvious, for me it was not)...</div><div><br></div><div><font face="Andale Mono">fs395 &nbsp; &nbsp; &nbsp;617 &nbsp;0.0 &nbsp;0.0 106200 &nbsp;1504 ? &nbsp; &nbsp; &nbsp; &nbsp;S &nbsp; &nbsp;22:41 &nbsp; 0:00 /bin/bash /var/spool/slurm-test/slurmd/job390044/slurm_script<br>fs395 &nbsp; &nbsp; &nbsp;629 &nbsp;0.1 &nbsp;0.0 194552 &nbsp;5288 ? &nbsp; &nbsp; &nbsp; &nbsp;Sl &nbsp; 22:41 &nbsp; 0:00 &nbsp;\_ mpirun -bind-to socket --map-by ppr:1:socket --host tesla121,tesla122,tesla123,tesla124,tesla125,tesla126,tes<br>fs395 &nbsp; &nbsp; &nbsp;632 &nbsp;0.0 &nbsp;0.0 659740 &nbsp;9148 ? &nbsp; &nbsp; &nbsp; &nbsp;Sl &nbsp; 22:41 &nbsp; 0:00 &nbsp;| &nbsp; \_ srun --ntasks-per-node=1 --kill-on-bad-exit --cpu_bind=none --nodes=7 --nodelist=tesla122,tesla123,tesla1<br>fs395 &nbsp; &nbsp; &nbsp;633 &nbsp;0.0 &nbsp;0.0 &nbsp;55544 &nbsp;1072 ? &nbsp; &nbsp; &nbsp; &nbsp;S &nbsp; &nbsp;22:41 &nbsp; 0:00 &nbsp;| &nbsp; | &nbsp; \_ srun --ntasks-per-node=1 --kill-on-bad-exit --cpu_bind=none --nodes=7 --nodelist=tesla122,tesla123,te<br>fs395 &nbsp; &nbsp; &nbsp;651 &nbsp;0.0 &nbsp;0.0 106072 &nbsp;1392 ? &nbsp; &nbsp; &nbsp; &nbsp;S &nbsp; &nbsp;22:41 &nbsp; 0:00 &nbsp;| &nbsp; \_ /bin/bash ./run_linpack ./xhpl<br>fs395 &nbsp; &nbsp; &nbsp;654 &nbsp;295 35.5 120113412 23289280 ? &nbsp;RLl &nbsp;22:41 &nbsp; 3:12 &nbsp;| &nbsp; | &nbsp; \_ ./xhpl<br>fs395 &nbsp; &nbsp; &nbsp;652 &nbsp;0.0 &nbsp;0.0 106072 &nbsp;1396 ? &nbsp; &nbsp; &nbsp; &nbsp;S &nbsp; &nbsp;22:41 &nbsp; 0:00 &nbsp;| &nbsp; \_ /bin/bash ./run_linpack ./xhpl<br>fs395 &nbsp; &nbsp; &nbsp;656 &nbsp;307 35.5 120070332 23267728 ? &nbsp;RLl &nbsp;22:41 &nbsp; 3:19 &nbsp;| &nbsp; &nbsp; &nbsp; \_ ./xhpl</font></div><div><br></div><div><br></div><div>The "xhpl" processes allocated on the first node of a job are not called by srun and because of this the SLURM profile plugin is not activated on the node!!! As result I always miss the first node profile information. Intel MPI does not have this behavior, mpiexec.hydra uses srun on the first node.&nbsp;</div><div><br></div><div>I got to the conclusion that SLURM is configured properly, something is wrong in the way I lunch Open MPI using mpirun. If I disable SLURM support and I revert back to rsh (--mca plm rsh) everything work but there is not profiling because the SLURM plug-in is not activated. During the configure step, Open MPI 1.8.1 detects slurm and libmpi/libpmi2 correctly. Honestly, I would prefer to avoid to use srun as job luncher if possible...</div><div><br></div><div>Any suggestion to get this sorted out is really appreciated!</div><div><br></div><div>Best Regards,</div><div>Filippo<br><div><br class="webkit-block-placeholder"></div><div apple-content-edited="true">--<br>Mr. Filippo SPIGA, M.Sc.<br><a href="http://filippospiga.info">http://filippospiga.info</a>&nbsp;~ skype: filippo.spiga<br><br>«Nobody will drive us out of Cantor's paradise.» ~ David Hilbert<br><br>*****<br>Disclaimer: "Please note this message and any attachments are CONFIDENTIAL and may be privileged or otherwise protected from disclosure. The contents are not to be disclosed to anyone other than the addressee. Unauthorized recipients are requested to&nbsp;preserve this confidentiality and to advise the sender immediately of any error in transmission."<br><br></div><br></div></body></html>
