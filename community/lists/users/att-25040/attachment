<div dir="ltr"><div>But OMPI 1.8.x does run the ring_c program successfully on your compute node, right? The error only happens on the front-end login node if I understood you correctly.<br><br></div>Josh <br></div><div class="gmail_extra">
<br><br><div class="gmail_quote">On Fri, Aug 15, 2014 at 5:20 PM, Maxime Boissonneault <span dir="ltr">&lt;<a href="mailto:maxime.boissonneault@calculquebec.ca" target="_blank">maxime.boissonneault@calculquebec.ca</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
  
    
  
  <div bgcolor="#FFFFFF" text="#000000">
    <div>Here are the requested files. <br>
      <br>
      In the archive, you will find the output of configure, make, make
      install as well as the config.log, the environment when running
      ring_c and the ompi_info --all.<br>
      <br>
      Just for a reminder, the ring_c example compiled and ran, but
      produced no output when running and exited with code 65.<br>
      <br>
      Thanks,<br>
      <br>
      Maxime<br>
      <br>
      Le 2014-08-14 15:26, Joshua Ladd a écrit :<br>
    </div>
    <blockquote type="cite"><div><div class="h5">
      <div dir="ltr">
        <div>One more, Maxime, can you please make sure you&#39;ve covered
          everything here:<br>
          <br>
          <span dir="ltr"><a href="http://www.open-mpi.org/community/help/" target="_blank">http://www.open-mpi.org/community/help/</a><br>
            <br>
          </span></div>
        <span dir="ltr">Josh<br>
        </span></div>
      <div class="gmail_extra"><br>
        <br>
        <div class="gmail_quote">On Thu, Aug 14, 2014 at 3:18 PM, Joshua
          Ladd <span dir="ltr">&lt;<a href="mailto:jladd.mlnx@gmail.com" target="_blank">jladd.mlnx@gmail.com</a>&gt;</span>
          wrote:<br>
          <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
            <div dir="ltr">
              <div>And maybe include your LD_LIBRARY_PATH<br>
                <br>
              </div>
              Josh<br>
            </div>
            <div>
              <div>
                <div class="gmail_extra"><br>
                  <br>
                  <div class="gmail_quote">On Thu, Aug 14, 2014 at 3:16
                    PM, Joshua Ladd <span dir="ltr">&lt;<a href="mailto:jladd.mlnx@gmail.com" target="_blank">jladd.mlnx@gmail.com</a>&gt;</span>
                    wrote:<br>
                    <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
                      <div dir="ltr">
                        <div>Can you try to run the example code
                          &quot;ring_c&quot; across nodes?<br>
                          <br>
                        </div>
                        Josh<br>
                      </div>
                      <div class="gmail_extra">
                        <br>
                        <br>
                        <div class="gmail_quote">
                          <div>
                            <div>On Thu, Aug 14, 2014 at 3:14 PM, Maxime
                              Boissonneault <span dir="ltr">&lt;<a href="mailto:maxime.boissonneault@calculquebec.ca" target="_blank">maxime.boissonneault@calculquebec.ca</a>&gt;</span>
                              wrote:<br>
                            </div>
                          </div>
                          <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
                            <div>
                              <div>
                                <div bgcolor="#FFFFFF" text="#000000">
                                  <div>Yes,<br>
                                    Everything has been built with GCC
                                    4.8.x, although x might have changed
                                    between the OpenMPI 1.8.1 build and
                                    the gromacs build. For OpenMPI
                                    1.8.2rc4 however, it was the exact
                                    same compiler for everything.<br>
                                    <br>
                                    Maxime<br>
                                    <br>
                                    Le 2014-08-14 14:57, Joshua Ladd a
                                    écrit :<br>
                                  </div>
                                  <blockquote type="cite">
                                    <div>
                                      <div>
                                        <div dir="ltr">
                                          <div>Hmmm...weird. Seems like
                                            maybe a mismatch between
                                            libraries. Did you build
                                            OMPI with the same compiler
                                            as you did GROMACS/Charm++?<br>
                                            <br>
                                            I&#39;m stealing this suggestion
                                            from an old Gromacs forum
                                            with essentially the same
                                            symptom:<br>
                                            <br>
                                            &quot;Did you compile Open MPI
                                            and Gromacs with the same
                                            compiler (i.e. both gcc and
                                            the same version)? You write
                                            you tried different OpenMPI
                                            versions and different GCC
                                            versions but it is unclear
                                            whether those match. Can you
                                            provide more detail how you
                                            compiled (including all
                                            options you specified)? Have
                                            you tested any other MPI
                                            program linked against those
                                            Open MPI versions? Please
                                            make sure (e.g. with ldd)
                                            that the MPI and pthread
                                            library you compiled against
                                            is also used for execution.
                                            If you compiled and run on
                                            different hosts, check
                                            whether the error still
                                            occurs when executing on the
                                            build host.&quot;<br>
                                            <br>
                                            <a href="http://redmine.gromacs.org/issues/1025" target="_blank">http://redmine.gromacs.org/issues/1025</a><br>
                                          </div>
                                          <div><br>
                                          </div>
                                          <div>Josh</div>
                                          <div><br>
                                            <br>
                                          </div>
                                        </div>
                                        <div class="gmail_extra"><br>
                                          <br>
                                          <div class="gmail_quote">On
                                            Thu, Aug 14, 2014 at 2:40
                                            PM, Maxime Boissonneault <span dir="ltr">&lt;<a href="mailto:maxime.boissonneault@calculquebec.ca" target="_blank">maxime.boissonneault@calculquebec.ca</a>&gt;</span>
                                            wrote:<br>
                                            <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
                                              <div bgcolor="#FFFFFF" text="#000000">
                                                <div>I just tried
                                                  Gromacs with two
                                                  nodes. It crashes, but
                                                  with a different
                                                  error. I get<br>
                                                  [gpu-k20-13:142156]
                                                  *** Process received
                                                  signal ***<br>
                                                  [gpu-k20-13:142156]
                                                  Signal: Segmentation
                                                  fault (11)<br>
                                                  [gpu-k20-13:142156]
                                                  Signal code: Address
                                                  not mapped (1)<br>
                                                  [gpu-k20-13:142156]
                                                  Failing at address:
                                                  0x8<br>
                                                  [gpu-k20-13:142156] [
                                                  0]
                                                  /lib64/libpthread.so.0(+0xf710)[0x2ac5d070c710]<br>
                                                  [gpu-k20-13:142156] [
                                                  1]
                                                  /usr/lib64/nvidia/libcuda.so.1(+0x263acf)[0x2ac5ddfbcacf]<br>
                                                  [gpu-k20-13:142156] [
                                                  2]
                                                  /usr/lib64/nvidia/libcuda.so.1(+0x229a83)[0x2ac5ddf82a83]<br>
                                                  [gpu-k20-13:142156] [
                                                  3]
                                                  /usr/lib64/nvidia/libcuda.so.1(+0x15b2da)[0x2ac5ddeb42da]<br>
                                                  [gpu-k20-13:142156] [
                                                  4]
                                                  /usr/lib64/nvidia/libcuda.so.1(cuInit+0x43)[0x2ac5ddea0933]<br>
                                                  [gpu-k20-13:142156] [
                                                  5]
/software-gpu/cuda/6.0.37/lib64/libcudart.so.6.0(+0x15965)[0x2ac5d0930965]<br>
                                                  [gpu-k20-13:142156] [
                                                  6]
/software-gpu/cuda/6.0.37/lib64/libcudart.so.6.0(+0x15a0a)[0x2ac5d0930a0a]<br>
                                                  [gpu-k20-13:142156] [
                                                  7]
/software-gpu/cuda/6.0.37/lib64/libcudart.so.6.0(+0x15a3b)[0x2ac5d0930a3b]<br>
                                                  [gpu-k20-13:142156] [
                                                  8]
/software-gpu/cuda/6.0.37/lib64/libcudart.so.6.0(cudaDriverGetVersion+0x4a)[0x2ac5d094602a]<br>
                                                  [gpu-k20-13:142156] [
                                                  9]
/software-gpu/apps/gromacs/4.6.5_gcc/lib/libgmxmpi.so.8(gmx_print_version_info_gpu+0x55)[0x2ac5cf9a90b5]<br>
                                                  [gpu-k20-13:142156]
                                                  [10]
/software-gpu/apps/gromacs/4.6.5_gcc/lib/libgmxmpi.so.8(gmx_log_open+0x17e)[0x2ac5cf54b9be]<br>
                                                  [gpu-k20-13:142156]
                                                  [11]
                                                  mdrunmpi(cmain+0x1cdb)[0x43b4bb]<br>
                                                  [gpu-k20-13:142156]
                                                  [12]
                                                  /lib64/libc.so.6(__libc_start_main+0xfd)[0x2ac5d1534d1d]<br>
                                                  [gpu-k20-13:142156]
                                                  [13]
                                                  mdrunmpi[0x407be1]<br>
                                                  [gpu-k20-13:142156]
                                                  *** End of error
                                                  message ***<br>
--------------------------------------------------------------------------<br>
                                                  mpiexec noticed that
                                                  process rank 0 with
                                                  PID 142156 on node
                                                  gpu-k20-13 exited on
                                                  signal 11
                                                  (Segmentation fault).<br>
--------------------------------------------------------------------------<br>
                                                  <br>
                                                  <br>
                                                  <br>
                                                  We do not have
                                                  MPI_THREAD_MULTIPLE
                                                  enabled in our build,
                                                  so Charm++ cannot be
                                                  using this level of
                                                  threading. The
                                                  configure line for
                                                  OpenMPI was<br>
                                                  ./configure
                                                  --prefix=$PREFIX \<br>
                                                        --with-threads
                                                  --with-verbs=yes
                                                  --enable-shared
                                                  --enable-static \<br>
                                                       
                                                  --with-io-romio-flags=&quot;--with-file-system=nfs+lustre&quot;
                                                  \<br>
                                                        
                                                  --without-loadleveler
                                                  --without-slurm
                                                  --with-tm \<br>
                                                        
                                                  --with-cuda=$(dirname
                                                  $(dirname $(which
                                                  nvcc)))<br>
                                                  <br>
                                                  Maxime<br>
                                                  <br>
                                                  <br>
                                                  Le 2014-08-14 14:20,
                                                  Joshua Ladd a écrit :<br>
                                                </div>
                                                <blockquote type="cite">
                                                  <div>
                                                    <div>
                                                      <div dir="ltr">
                                                        <div>What about
                                                          between nodes?
                                                          Since this is
                                                          coming from
                                                          the OpenIB
                                                          BTL, would be
                                                          good to check
                                                          this. <br>
                                                          <br>
                                                          Do you know
                                                          what the MPI
                                                          thread level
                                                          is set to when
                                                          used with the
                                                          Charm++
                                                          runtime? Is it
                                                          MPI_THREAD_MULTIPLE?
                                                          The OpenIB BTL
                                                          is not thread
                                                          safe.<br>
                                                          <br>
                                                        </div>
                                                        Josh <br>
                                                      </div>
                                                      <div class="gmail_extra"><br>
                                                        <br>
                                                        <div class="gmail_quote">On
                                                          Thu, Aug 14,
                                                          2014 at 2:17
                                                          PM, Maxime
                                                          Boissonneault
                                                          <span dir="ltr">&lt;<a href="mailto:maxime.boissonneault@calculquebec.ca" target="_blank">maxime.boissonneault@calculquebec.ca</a>&gt;</span>
                                                          wrote:<br>
                                                          <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
                                                          <div bgcolor="#FFFFFF" text="#000000">
                                                          <div>Hi,<br>
                                                          I ran gromacs
                                                          successfully
                                                          with OpenMPI
                                                          1.8.1 and Cuda
                                                          6.0.37 on a
                                                          single node,
                                                          with 8 ranks
                                                          and multiple
                                                          OpenMP
                                                          threads.<br>
                                                          <br>
                                                          Maxime<br>
                                                          <br>
                                                          <br>
                                                          Le 2014-08-14
                                                          14:15, Joshua
                                                          Ladd a écrit :<br>
                                                          </div>
                                                          <blockquote type="cite">
                                                          <div>
                                                          <div>
                                                          <div dir="ltr">
                                                          <div>
                                                          <div>Hi,
                                                          Maxime<br>
                                                          <br>
                                                          </div>
                                                          Just curious,
                                                          are you able
                                                          to run a
                                                          vanilla MPI
                                                          program? Can
                                                          you try one
                                                          one of the
                                                          example
                                                          programs in
                                                          the &quot;examples&quot;
                                                          subdirectory.
                                                          Looks like a
                                                          threading
                                                          issue to me.<br>
                                                          <br>
                                                          </div>
                                                          <div>Thanks,<br>
                                                          <br>
                                                          </div>
                                                          Josh<br>
                                                          <div> <br>
                                                          </div>
                                                          </div>
                                                          <br>
                                                          <fieldset></fieldset>
                                                          <br>
                                                          </div>
                                                          </div>
                                                          <pre><div>_______________________________________________
users mailing list
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25023.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25023.php</a></pre>
                                                          </blockquote>
                                                          <br>
                                                          <br>
                                                          </div>
                                                          <br>
_______________________________________________<br>
                                                          users mailing
                                                          list<br>
                                                          <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
                                                          Subscription:
                                                          <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                                                          Link to this
                                                          post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25024.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25024.php</a><br>

                                                          </blockquote>
                                                        </div>
                                                        <br>
                                                      </div>
                                                      <br>
                                                      <fieldset></fieldset>
                                                      <br>
                                                    </div>
                                                  </div>
                                                  <pre><div><div>_______________________________________________
users mailing list
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25025.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25025.php</a></pre>
                                                </blockquote>
                                                <div> <br>
                                                  <br>
                                                  <pre cols="72">-- 
---------------------------------
Maxime Boissonneault
Analyste de calcul - Calcul Québec, Université Laval
Ph. D. en physique</pre>
                                                </div>
                                              </div>
                                              <br>
_______________________________________________<br>
                                              users mailing list<br>
                                              <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
                                              Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                                              Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25026.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25026.php</a><br>

                                            </blockquote>
                                          </div>
                                          <br>
                                        </div>
                                        <br>
                                        <fieldset></fieldset>
                                        <br>
                                      </div>
                                    </div>
                                    <pre><div><div>_______________________________________________
users mailing list
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25027.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25027.php</a></pre>
                                  </blockquote>
                                  <div> <br>
                                    <br>
                                    <pre cols="72">-- 
---------------------------------
Maxime Boissonneault
Analyste de calcul - Calcul Québec, Université Laval
Ph. D. en physique</pre>
                                  </div>
                                </div>
                                <br>
_______________________________________________<br>
                                users mailing list<br>
                                <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
                                Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                              </div>
                            </div>
                            Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25028.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25028.php</a><br>
                          </blockquote>
                        </div>
                        <br>
                      </div>
                    </blockquote>
                  </div>
                  <br>
                </div>
              </div>
            </div>
          </blockquote>
        </div>
        <br>
      </div>
      <br>
      <fieldset></fieldset>
      <br>
      </div></div><pre><div><div class="h5">_______________________________________________
users mailing list
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25031.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25031.php</a></pre>
    </blockquote><div class="">
    <br>
    <br>
    <pre cols="72">-- 
---------------------------------
Maxime Boissonneault
Analyste de calcul - Calcul Québec, Université Laval
Ph. D. en physique</pre>
  </div></div>

<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25039.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25039.php</a><br></blockquote></div><br></div>

