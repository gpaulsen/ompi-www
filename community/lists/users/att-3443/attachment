<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta content="text/html;charset=ISO-8859-1" http-equiv="Content-Type">
</head>
<body bgcolor="#ffffff" text="#000000">
Glad to contribute Victor!<br>
<br>
I am running on a home workstation that uses an AMD 3800 cpu attached
to 2 gigs of ram.<br>
My timings for FT were 175 secs with one core and 110 on two cores with
-O3 and -mtune=amd64 as tuning options.<br>
<br>
Brock, Terry and Jeff are all exactly correct in their comments
regarding benchmarks. There are simply too many variables to contend
with. In addition, one and two core runs on a single workstation
probably isn't the best evaluation of OpenMPI. As you expand to more
devices and generate bigger problems (HPL or HPCC for example), a
better overall picture will emerge.<br>
<br>
<br>
<div class="moz-signature"><small><small style="font-family: Axaxax;"><big>Jeff
F. Pummill</big><br>
Senior Linux Cluster Administrator<br>
University of Arkansas<br>
<br>
</small></small></div>
<br>
<br>
victor marian wrote:
<blockquote cite="mid:303303.56641.qm@web55814.mail.re3.yahoo.com"
 type="cite">
  <pre wrap="">  Thank you everybody for the advices.
  I ran the NAS benchmark class B and it runs in 181
seconds on one core and in 90 seconds on two cores, so
it scales almost perfectly.
  What were your timings, Jeff, and what processor do
you exactly have?
  Mine is a Pentium D at 2.8GHz.

                                         Victor


--- Jeff Pummill <a class="moz-txt-link-rfc2396E" href="mailto:jpummil@uark.edu">&lt;jpummil@uark.edu&gt;</a> wrote:

  </pre>
  <blockquote type="cite">
    <pre wrap="">Victor,

Build the FT benchmark and build it as a class B
problem. This will run 
in the 1-2 minute range instead of 2-4 seconds the
CG class A benchmark 
does.


Jeff F. Pummill
Senior Linux Cluster Administrator
University of Arkansas



Terry Frankcombe wrote:
    </pre>
    <blockquote type="cite">
      <pre wrap="">Hi Victor

I'd suggest 3 seconds of CPU time is far, far to
      </pre>
    </blockquote>
    <pre wrap="">small a problem to do
    </pre>
    <blockquote type="cite">
      <pre wrap="">scaling tests with.  Even with only 2 CPUs, I
      </pre>
    </blockquote>
    <pre wrap="">wouldn't go below 100
    </pre>
    <blockquote type="cite">
      <pre wrap="">times that.


On Mon, 2007-06-11 at 01:10 -0700, victor marian
      </pre>
    </blockquote>
    <pre wrap="">wrote:
    </pre>
    <blockquote type="cite">
      <pre wrap="">  
      </pre>
      <blockquote type="cite">
        <pre wrap="">Hi Jeff

I ran the NAS Parallel Bechmark and it gives for
        </pre>
      </blockquote>
    </blockquote>
    <pre wrap="">me
    </pre>
  </blockquote>
  <pre wrap=""><!---->-bash%/export/home/vmarian/fortran/benchmarks/NPB3.2/NPB3.2-MPI/bin$
  </pre>
  <blockquote type="cite">
    <blockquote type="cite">
      <blockquote type="cite">
        <pre wrap="">mpirun -np 1 cg.A.1

        </pre>
      </blockquote>
    </blockquote>
  </blockquote>
  <pre wrap=""><!---->--------------------------------------------------------------------------
  </pre>
  <blockquote type="cite">
    <blockquote type="cite">
      <blockquote type="cite">
        <pre wrap="">[0,1,0]: uDAPL on host SERVSOLARIS was unable to
        </pre>
      </blockquote>
    </blockquote>
    <pre wrap="">find
    </pre>
    <blockquote type="cite">
      <blockquote type="cite">
        <pre wrap="">any NICs.
Another transport will be used instead, although
        </pre>
      </blockquote>
    </blockquote>
    <pre wrap="">this
    </pre>
    <blockquote type="cite">
      <blockquote type="cite">
        <pre wrap="">may result in
lower performance.

        </pre>
      </blockquote>
    </blockquote>
  </blockquote>
  <pre wrap=""><!---->--------------------------------------------------------------------------
  </pre>
  <blockquote type="cite">
    <blockquote type="cite">
      <blockquote type="cite">
        <pre wrap=""> NAS Parallel Benchmarks 3.2 -- CG Benchmark

 Size:      14000
 Iterations:    15
 Number of active processes:     1
 Number of nonzeroes per row:       11
 Eigenvalue shift: .200E+02
 Benchmark completed
 VERIFICATION SUCCESSFUL
 Zeta is      0.171302350540E+02
 Error is     0.512264003323E-13


 CG Benchmark Completed.
 Class           =                        A
 Size            =                    14000
 Iterations      =                       15
 Time in seconds =                     3.02
 Total processes =                        1
 Compiled procs  =                        1
 Mop/s total     =                   495.93
 Mop/s/process   =                   495.93
 Operation type  =           floating point
 Verification    =               SUCCESSFUL
 Version         =                      3.2
 Compile date    =              11 Jun 2007



        </pre>
      </blockquote>
    </blockquote>
  </blockquote>
  <pre wrap=""><!---->-bash%/export/home/vmarian/fortran/benchmarks/NPB3.2/NPB3.2-MPI/bin$
  </pre>
  <blockquote type="cite">
    <blockquote type="cite">
      <blockquote type="cite">
        <pre wrap="">mpirun -np 2 cg.A.2

        </pre>
      </blockquote>
    </blockquote>
  </blockquote>
  <pre wrap=""><!---->--------------------------------------------------------------------------
  </pre>
  <blockquote type="cite">
    <blockquote type="cite">
      <blockquote type="cite">
        <pre wrap="">[0,1,0]: uDAPL on host SERVSOLARIS was unable to
        </pre>
      </blockquote>
    </blockquote>
    <pre wrap="">find
    </pre>
    <blockquote type="cite">
      <blockquote type="cite">
        <pre wrap="">any NICs.
Another transport will be used instead, although
        </pre>
      </blockquote>
    </blockquote>
    <pre wrap="">this
    </pre>
    <blockquote type="cite">
      <blockquote type="cite">
        <pre wrap="">may result in
lower performance.

        </pre>
      </blockquote>
    </blockquote>
  </blockquote>
  <pre wrap=""><!---->--------------------------------------------------------------------------
  </pre>
  <pre wrap=""><!---->--------------------------------------------------------------------------
  </pre>
  <blockquote type="cite">
    <blockquote type="cite">
      <blockquote type="cite">
        <pre wrap="">[0,1,1]: uDAPL on host SERVSOLARIS was unable to
        </pre>
      </blockquote>
    </blockquote>
    <pre wrap="">find
    </pre>
    <blockquote type="cite">
      <blockquote type="cite">
        <pre wrap="">any NICs.
Another transport will be used instead, although
        </pre>
      </blockquote>
    </blockquote>
    <pre wrap="">this
    </pre>
    <blockquote type="cite">
      <blockquote type="cite">
        <pre wrap="">may result in
lower performance.

        </pre>
      </blockquote>
    </blockquote>
  </blockquote>
  <pre wrap=""><!---->--------------------------------------------------------------------------
  </pre>
  <blockquote type="cite">
    <blockquote type="cite">
      <blockquote type="cite">
        <pre wrap="">
 NAS Parallel Benchmarks 3.2 -- CG Benchmark

 Size:      14000
 Iterations:    15
 Number of active processes:     2
 Number of nonzeroes per row:       11
 Eigenvalue shift: .200E+02

 Benchmark completed
 VERIFICATION SUCCESSFUL
 Zeta is      0.171302350540E+02
 Error is     0.522633719989E-13


 CG Benchmark Completed.
 Class           =                        A
 Size            =                    14000
 Iterations      =                       15
 Time in seconds =                     2.47
 Total processes =                        2
 Compiled procs  =                        2
 Mop/s total     =                   606.32
 Mop/s/process   =                   303.16
 Operation type  =           floating point
 Verification    =               SUCCESSFUL
 Version         =                      3.2
 Compile date    =              11 Jun 2007


    You can remark that the scalling is not so
        </pre>
      </blockquote>
    </blockquote>
    <pre wrap="">good
    </pre>
    <blockquote type="cite">
      <blockquote type="cite">
        <pre wrap="">like yours. Maibe I am having comunications
        </pre>
      </blockquote>
    </blockquote>
    <pre wrap="">problems
    </pre>
    <blockquote type="cite">
      <blockquote type="cite">
        <pre wrap="">between processors.
   You can also remark that I am faster on one
        </pre>
      </blockquote>
    </blockquote>
    <pre wrap="">process
    </pre>
    <blockquote type="cite">
      <blockquote type="cite">
        <pre wrap="">concared to your processor.

                                       Victor





--- Jeff Pummill <a class="moz-txt-link-rfc2396E" href="mailto:jpummil@uark.edu">&lt;jpummil@uark.edu&gt;</a> wrote:

    
        </pre>
        <blockquote type="cite">
          <pre wrap="">Perfect! Thanks Jeff!

The NAS Parallel Benchmark on a dual core AMD
machine now returns this...
[jpummil@localhost bin]$ mpirun -np 1 cg.A.1
NAS Parallel Benchmarks 3.2 -- CG Benchmark
CG Benchmark Completed.
 Class           =                        A
 Size            =                    14000
 Iterations      =                       15
 Time in seconds =                     4.75
 Total processes =                        1
 Compiled procs  =                        1
 Mop/s total     =                   315.32

...and...

[jpummil@localhost bin]$ mpirun -np 2 cg.A.2
NAS Parallel Benchmarks 3.2 -- CG Benchmark
 CG Benchmark Completed.
 Class           =                        A
 Size            =                    14000
 Iterations      =                       15
 Time in seconds =                     2.48
 Total processes =                        2
 Compiled procs  =                        2
 Mop/s total     =                   604.46

Not quite linear, but one must account for all
          </pre>
        </blockquote>
      </blockquote>
    </blockquote>
    <pre wrap="">of
    </pre>
    <blockquote type="cite">
      <blockquote type="cite">
        <blockquote type="cite">
          <pre wrap="">the OS traffic that 
one core or the other must deal with.


Jeff F. Pummill
Senior Linux Cluster Administrator
University of Arkansas
Fayetteville, Arkansas 72701
(479) 575 - 4590
<a class="moz-txt-link-freetext" href="http://hpc.uark.edu">http://hpc.uark.edu</a>

"A supercomputer is a device for turning
compute-bound
problems into I/O-bound problems." -Seymour Cray


Jeff Squyres wrote:
      
          </pre>
          <blockquote type="cite">
            <pre wrap="">Just remove the -L and -l arguments -- OMPI's
        
            </pre>
          </blockquote>
          <pre wrap="">"mpif90" (and other  
      
          </pre>
          <blockquote type="cite">
            <pre wrap="">wrapper compilers) will do all that magic for
            </pre>
          </blockquote>
        </blockquote>
      </blockquote>
    </blockquote>
    <pre wrap="">you.

    </pre>
  </blockquote>
  <pre wrap=""><!---->=== message truncated ===&gt;
_______________________________________________
  </pre>
  <blockquote type="cite">
    <pre wrap="">users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
    </pre>
  </blockquote>
  <pre wrap=""><!---->


       
____________________________________________________________________________________
Be a better Globetrotter. Get better travel answers from someone who knows. Yahoo! Answers - Check it out.
<a class="moz-txt-link-freetext" href="http://answers.yahoo.com/dir/?link=list&sid=396545469">http://answers.yahoo.com/dir/?link=list&amp;sid=396545469</a>
_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
  </pre>
</blockquote>
</body>
</html>

