<div dir="ltr">Dear All,<div style>I have performed some tests and I finally run successfully mpiexec without simlinks. As Thomas said my error was the LD_LIBRARY_PATH setting. The correct setup is the following:</div><div style>
<br></div><div style><div>source /home/stefano/opt/intel/2013.4.183/bin/compilervars.sh intel64</div><div>export MPI=/home/stefano/opt/mpi/openmpi/1.6.4/intel</div><div>export PATH=${MPI}/bin:$PATH</div><div>export LD_LIBRARY_PATH=<b>/home/stefano/opt/intel/2013.4.183/lib</b>:${MPI}/lib/openmpi:${MPI}/lib:$LD_LIBRARY_PATH</div>
<div>export LD_RUN_PATH=${MPI}/lib/openmpi:${MPI}/lib:$LD_RUN_PATH</div><div><br></div><div style>Using the above setting mpiexec (orted) finds all its shared library also with remote node runs. My previous setups were wrong because:</div>
<div style><br></div><div style>1) in the first test I have forgot <b>/home/stefano/opt/intel/2013.4.183/lib</b> in the LD_LIBRARY_PATH;</div><div style>2) in the second test I have used <b>/home/stefano/opt/intel/2013.4.183/lib/intel64</b> in the LD_LIBRARY_PATH.</div>
<div style><br></div><div style>It seems that the source of <b>compilervars.sh</b> does not set the correct LD_LIBRARY_PATH.</div><div style><br></div><div style>Thanks you for all suggestions,</div><div style>sincerely</div>
<div><br></div></div></div><div class="gmail_extra"><br clear="all"><div><div dir="ltr">Stefano Zaghi<br>Ph.D. Aerospace Engineer,<br>Research Scientist, Dept. of Computational Hydrodynamics at <a href="http://www.insean.cnr.it/en/content/cnr-insean" target="_blank"><b><font color="#6fa8dc">CNR-INSEAN</font></b></a> <br>
The Italian Ship Model Basin<br>(+39) 06.50299297 (Office)<div><div>My codes:</div><div><a href="https://github.com/szaghi/OFF" target="_blank"><b><font color="#6fa8dc">OFF</font></b></a>, Open source Finite volumes Fluid dynamics code</div>
<div><a href="https://github.com/szaghi/Lib_VTK_IO" target="_blank"><b><font color="#6fa8dc">Lib_VTK_IO</font></b></a>, a Fortran library to write and read data conforming the VTK standard</div><div><a href="https://github.com/szaghi/IR_Precision" target="_blank"><b><font color="#6fa8dc">IR_Precision</font></b></a>, a Fortran (standard 2003) module to develop portable codes</div>
</div></div></div>
<br><br><div class="gmail_quote">2013/6/21 Stefano Zaghi <span dir="ltr">&lt;<a href="mailto:stefano.zaghi@gmail.com" target="_blank">stefano.zaghi@gmail.com</a>&gt;</span><br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div dir="ltr">Dear All,<div>I have compiled OpenMPI 1.6.4 with Intel Composer_xe_2013.4.183. </div><div><br></div><div>My configure is:</div><div><br></div><div>./configure --prefix=/home/stefano/opt/mpi/openmpi/1.6.4/intel CC=icc CXX=icpc F77=ifort FC=ifort</div>

<div><br></div><div>Intel Composer has been installed in: </div><div><br></div><div>/home/stefano/opt/intel/2013.4.183/composer_xe_2013.4.183<br></div><div><br></div><div>Into the .bashrc and .profile in all nodes there is:</div>

<div><br></div><div><div>source /home/stefano/opt/intel/2013.4.183/bin/compilervars.sh intel64</div><div>export MPI=/home/stefano/opt/mpi/openmpi/1.6.4/intel<br></div><div>export PATH=${MPI}/bin:$PATH</div><div>
export LD_LIBRARY_PATH=${MPI}/lib/openmpi:${MPI}/lib:$LD_LIBRARY_PATH</div><div>export LD_RUN_PATH=${MPI}/lib/openmpi:${MPI}/lib:$LD_RUN_PATH</div></div><div><br></div><div>If I run parallel job into each single node (e.g. mpirun -np 8 myprog) all works well. However, when I tried to run parallel job in more nodes of the cluster (remote runs) like the following:</div>

<div><br></div><div><div>mpirun -np 16 --bynode --machinefile nodi.txt -x LD_LIBRARY_PATH -x LD_RUN_PATH myprog </div><div><br></div><div><div>I got the following error:</div></div><div><br></div><div>/home/stefano/opt/mpi/openmpi/1.6.4/intel/bin/orted: error while loading shared libraries: libimf.so: cannot open shared object file: No such file or directory</div>

</div><div><br></div><div>I have read many FAQs and online resources, all indicating LD_LIBRARY_PATH as the possible problem (wrong setting). However I am not able to figure out what is going wrong, the LD_LIBRARY_PATH seems to set right in all nodes.</div>

<div><br></div><div>It is worth noting that in the same cluster I have successful installed OpenMPI 1.4.3 with Intel Composer_xe_2011_sp1.6.233 following exactly the same procedure.</div><div><br></div><div>
Thank you in advance for all suggestion,</div><div>sincerely</div><div><br clear="all"><div><div dir="ltr">Stefano Zaghi<br>Ph.D. Aerospace Engineer,<br>Research Scientist, Dept. of Computational Hydrodynamics at <a href="http://www.insean.cnr.it/en/content/cnr-insean" target="_blank"><b><font color="#6fa8dc">CNR-INSEAN</font></b></a> <br>

The Italian Ship Model Basin<br>(+39) 06.50299297 (Office)<div><div>My codes:</div><div><a href="https://github.com/szaghi/OFF" target="_blank"><b><font color="#6fa8dc">OFF</font></b></a>, Open source Finite volumes Fluid dynamics code</div>

<div><a href="https://github.com/szaghi/Lib_VTK_IO" target="_blank"><b><font color="#6fa8dc">Lib_VTK_IO</font></b></a>, a Fortran library to write and read data conforming the VTK standard</div><div><a href="https://github.com/szaghi/IR_Precision" target="_blank"><b><font color="#6fa8dc">IR_Precision</font></b></a>, a Fortran (standard 2003) module to develop portable codes</div>

</div></div></div>
</div></div>
</blockquote></div><br></div>

