<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    <p>Siegmar,</p>
    <p>i was unable to reproduce the issue with one solaris 11 x86_64 VM
      and one linux x86_64 VM</p>
    <p><br>
    </p>
    <p>what is the minimal configuration you need to reproduce the issue
      ?</p>
    <p>are you able to reproduce the issue with only x86_64 nodes ?</p>
    <p>i was under the impression that solaris vs linux is the issue,
      but is big vs little endian instead ?</p>
    <p><br>
    </p>
    <p>Cheers,</p>
    <p><br>
    </p>
    <p>Gilles<br>
    </p>
    <br>
    <div class="moz-cite-prefix">On 5/5/2016 9:13 PM, Siegmar Gross
      wrote:<br>
    </div>
    <blockquote
      cite="mid:78571a89-7e80-9c60-5d31-6cb8dde7a0b1@informatik.hs-fulda.de"
      type="cite">Hi Gilles,
      <br>
      <br>
      is the following output helpful to find the error? I've put
      <br>
      another output below the output from gdb, which shows that
      <br>
      things are a little bit "random" if I use only 3+2 or 4+1
      <br>
      Sparc machines.
      <br>
      <br>
      <br>
      tyr spawn 127 /usr/local/gdb-7.6.1_64_gcc/bin/gdb mpiexec
      <br>
      GNU gdb (GDB) 7.6.1
      <br>
      Copyright (C) 2013 Free Software Foundation, Inc.
      <br>
      License GPLv3+: GNU GPL version 3 or later
      <a class="moz-txt-link-rfc2396E" href="http://gnu.org/licenses/gpl.html">&lt;http://gnu.org/licenses/gpl.html&gt;</a>
      <br>
      This is free software: you are free to change and redistribute it.
      <br>
      There is NO WARRANTY, to the extent permitted by law.  Type "show
      copying"
      <br>
      and "show warranty" for details.
      <br>
      This GDB was configured as "sparc-sun-solaris2.10".
      <br>
      For bug reporting instructions, please see:
      <br>
      <a class="moz-txt-link-rfc2396E" href="http://www.gnu.org/software/gdb/bugs/">&lt;http://www.gnu.org/software/gdb/bugs/&gt;</a>...
      <br>
      Reading symbols from
      /export2/prog/SunOS_sparc/openmpi-1.10.3_64_cc/bin/orterun...done.
      <br>
      (gdb) set args -np 1 --host tyr,sunpc1,linpc1,ruester
      spawn_multiple_master
      <br>
      (gdb) run
      <br>
      Starting program: /usr/local/openmpi-1.10.3_64_cc/bin/mpiexec -np
      1 --host tyr,sunpc1,linpc1,ruester spawn_multiple_master
      <br>
      [Thread debugging using libthread_db enabled]
      <br>
      [New Thread 1 (LWP 1)]
      <br>
      [New LWP    2        ]
      <br>
      <br>
      Parent process 0 running on tyr.informatik.hs-fulda.de
      <br>
        I create 3 slave processes.
      <br>
      <br>
      Assertion failed: OPAL_OBJ_MAGIC_ID == ((opal_object_t *)
      (proc_pointer))-&gt;obj_magic_id, file
      ../../openmpi-v1.10.2-163-g42da15d/ompi/group/group_init.c, line
      215, function ompi_group_increment_proc_count
      <br>
      [ruester:17809] *** Process received signal ***
      <br>
      [ruester:17809] Signal: Abort (6)
      <br>
      [ruester:17809] Signal code:  (-1)
      <br>
/usr/local/openmpi-1.10.3_64_cc/lib64/libopen-pal.so.13.0.2:opal_backtrace_print+0x1c
      <br>
/usr/local/openmpi-1.10.3_64_cc/lib64/libopen-pal.so.13.0.2:0x1b10f0
      <br>
      /lib/sparcv9/libc.so.1:0xd8c28
      <br>
      /lib/sparcv9/libc.so.1:0xcc79c
      <br>
      /lib/sparcv9/libc.so.1:0xcc9a8
      <br>
      /lib/sparcv9/libc.so.1:__lwp_kill+0x8 [ Signal 2091943080 (?)]
      <br>
      /lib/sparcv9/libc.so.1:abort+0xd0
      <br>
      /lib/sparcv9/libc.so.1:_assert_c99+0x78
      <br>
/usr/local/openmpi-1.10.3_64_cc/lib64/libmpi.so.12.0.3:ompi_group_increment_proc_count+0x10c
      <br>
/usr/local/openmpi-1.10.3_64_cc/lib64/openmpi/mca_dpm_orte.so:0xe758
      <br>
/usr/local/openmpi-1.10.3_64_cc/lib64/openmpi/mca_dpm_orte.so:0x113d4
      <br>
/usr/local/openmpi-1.10.3_64_cc/lib64/libmpi.so.12.0.3:ompi_mpi_init+0x188c
      <br>
/usr/local/openmpi-1.10.3_64_cc/lib64/libmpi.so.12.0.3:MPI_Init+0x26c
      <br>
      /home/fd1026/SunOS/sparc/bin/spawn_slave:main+0x18
      <br>
      /home/fd1026/SunOS/sparc/bin/spawn_slave:_start+0x108
      <br>
      [ruester:17809] *** End of error message ***
      <br>
--------------------------------------------------------------------------
      <br>
      mpiexec noticed that process rank 2 with PID 0 on node ruester
      exited on signal 6 (Abort).
      <br>
--------------------------------------------------------------------------
      <br>
      [LWP    2         exited]
      <br>
      [New Thread 2        ]
      <br>
      [Switching to Thread 1 (LWP 1)]
      <br>
      sol_thread_fetch_registers: td_ta_map_id2thr: no thread can be
      found to satisfy query
      <br>
      (gdb) bt
      <br>
      #0  0xffffffff7f6173d0 in rtld_db_dlactivity () from
      /usr/lib/sparcv9/ld.so.1
      <br>
      #1  0xffffffff7f6175a8 in rd_event () from
      /usr/lib/sparcv9/ld.so.1
      <br>
      #2  0xffffffff7f618950 in lm_delete () from
      /usr/lib/sparcv9/ld.so.1
      <br>
      #3  0xffffffff7f6226bc in remove_so () from
      /usr/lib/sparcv9/ld.so.1
      <br>
      #4  0xffffffff7f624574 in remove_hdl () from
      /usr/lib/sparcv9/ld.so.1
      <br>
      #5  0xffffffff7f61d97c in dlclose_core () from
      /usr/lib/sparcv9/ld.so.1
      <br>
      #6  0xffffffff7f61d9d4 in dlclose_intn () from
      /usr/lib/sparcv9/ld.so.1
      <br>
      #7  0xffffffff7f61db0c in dlclose () from /usr/lib/sparcv9/ld.so.1
      <br>
      #8  0xffffffff7e5f9718 in dlopen_close (handle=0x100)
      <br>
          at
../../../../../openmpi-v1.10.2-163-g42da15d/opal/mca/dl/dlopen/dl_dlopen_module.c:144<br>
      #9  0xffffffff7e5f364c in opal_dl_close
      (handle=0xffffff7d700200ff)
      <br>
          at
../../../../openmpi-v1.10.2-163-g42da15d/opal/mca/dl/base/dl_base_fns.c:53<br>
      #10 0xffffffff7e546714 in ri_destructor (obj=0x1200)
      <br>
          at
../../../../openmpi-v1.10.2-163-g42da15d/opal/mca/base/mca_base_component_repository.c:357<br>
      #11 0xffffffff7e543840 in opal_obj_run_destructors
      (object=0xffffff7f607a6cff)
      <br>
          at
      ../../../../openmpi-v1.10.2-163-g42da15d/opal/class/opal_object.h:451
      <br>
      #12 0xffffffff7e545f54 in mca_base_component_repository_release
      (component=0xffffff7c801df0ff)
      <br>
          at
../../../../openmpi-v1.10.2-163-g42da15d/opal/mca/base/mca_base_component_repository.c:223<br>
      #13 0xffffffff7e54d0d8 in mca_base_component_unload
      (component=0xffffff7d00003000, output_id=-1610596097)
      <br>
          at
../../../../openmpi-v1.10.2-163-g42da15d/opal/mca/base/mca_base_components_close.c:47<br>
      #14 0xffffffff7e54d17c in mca_base_component_close
      (component=0x100, output_id=-1878702080)
      <br>
          at
../../../../openmpi-v1.10.2-163-g42da15d/opal/mca/base/mca_base_components_close.c:60<br>
      #15 0xffffffff7e54d28c in mca_base_components_close
      (output_id=1942099968, components=0xff,
      <br>
          skip=0xffffff7f61c5a800)
      <br>
          at
../../../../openmpi-v1.10.2-163-g42da15d/opal/mca/base/mca_base_components_close.c:86<br>
      #16 0xffffffff7e54d1cc in mca_base_framework_components_close
      (framework=0x1000000ff, skip=0x10018ebb000)
      <br>
          at
../../../../openmpi-v1.10.2-163-g42da15d/opal/mca/base/mca_base_components_close.c:68<br>
      #17 0xffffffff7ee4db88 in orte_oob_base_close ()
      <br>
          at
../../../../openmpi-v1.10.2-163-g42da15d/orte/mca/oob/base/oob_base_frame.c:94<br>
      #18 0xffffffff7e580054 in mca_base_framework_close
      (framework=0xffffff0000004fff)
      <br>
          at
../../../../openmpi-v1.10.2-163-g42da15d/opal/mca/base/mca_base_framework.c:198<br>
      #19 0xffffffff7c514cdc in rte_finalize ()
      <br>
          at
../../../../../openmpi-v1.10.2-163-g42da15d/orte/mca/ess/hnp/ess_hnp_module.c:882<br>
      #20 0xffffffff7ec5c414 in orte_finalize () at
      ../../openmpi-v1.10.2-163-g42da15d/orte/runtime/orte_finalize.c:65
      <br>
      #21 0x000000010000eb24 in orterun (argc=1423033599,
      argv=0xffffff7fffce41ff)
      <br>
          at
../../../../openmpi-v1.10.2-163-g42da15d/orte/tools/orterun/orterun.c:1151<br>
      #22 0x0000000100004d4c in main (argc=416477439,
      argv=0xffffff7fffd7f000)
      <br>
          at
      ../../../../openmpi-v1.10.2-163-g42da15d/orte/tools/orterun/main.c:13
      <br>
      (gdb)
      <br>
      <br>
      <br>
      <br>
      <br>
      tyr spawn 145 mpiexec -np 1 --host ruester,ruester,ruester,tyr,tyr
      spawn_multiple_master
      <br>
      <br>
      Parent process 0 running on ruester.informatik.hs-fulda.de
      <br>
        I create 3 slave processes.
      <br>
      <br>
      Assertion failed: OPAL_OBJ_MAGIC_ID == ((opal_object_t *)
      (proc_pointer))-&gt;obj_magic_id, file
      ../../openmpi-v1.10.2-163-g42da15d/ompi/group/group_init.c, line
      215, function ompi_group_increment_proc_count
      <br>
      [ruester:18238] *** Process received signal ***
      <br>
      [ruester:18238] Signal: Abort (6)
      <br>
      [ruester:18238] Signal code:  (-1)
      <br>
/usr/local/openmpi-1.10.3_64_cc/lib64/libopen-pal.so.13.0.2:opal_backtrace_print+0x1c
      <br>
/usr/local/openmpi-1.10.3_64_cc/lib64/libopen-pal.so.13.0.2:0x1b10f0
      <br>
      /lib/sparcv9/libc.so.1:0xd8c28
      <br>
      ------------------------------------------------------------
      <br>
      A process or daemon was unable to complete a TCP connection
      <br>
      to another process:
      <br>
        Local host:    ruester
      <br>
        Remote host:   ruester
      <br>
      This is usually caused by a firewall on the remote host. Please
      <br>
      check that any firewall (e.g., iptables) has been disabled and
      <br>
      try again.
      <br>
      ------------------------------------------------------------
      <br>
      /lib/sparcv9/libc.so.1:0xcc79c
      <br>
      /lib/sparcv9/libc.so.1:0xcc9a8
      <br>
      /lib/sparcv9/libc.so.1:__lwp_kill+0x8 [ Signal 2091943080 (?)]
      <br>
      /lib/sparcv9/libc.so.1:abort+0xd0
      <br>
      /lib/sparcv9/libc.so.1:_assert_c99+0x78
      <br>
/usr/local/openmpi-1.10.3_64_cc/lib64/libmpi.so.12.0.3:ompi_group_increment_proc_count+0x10c
      <br>
/usr/local/openmpi-1.10.3_64_cc/lib64/openmpi/mca_dpm_orte.so:0xe758
      <br>
/usr/local/openmpi-1.10.3_64_cc/lib64/libmpi.so.12.0.3:MPI_Comm_spawn_multiple+0x8f4
      <br>
      /home/fd1026/SunOS/sparc/bin/spawn_multiple_master:main+0x188
      <br>
      /home/fd1026/SunOS/sparc/bin/spawn_multiple_master:_start+0x108
      <br>
      [ruester:18238] *** End of error message ***
      <br>
--------------------------------------------------------------------------
      <br>
      mpiexec noticed that process rank 0 with PID 0 on node ruester
      exited on signal 6 (Abort).
      <br>
--------------------------------------------------------------------------
      <br>
      <br>
      <br>
      <br>
      tyr spawn 146 mpiexec -np 1 --host
      ruester,ruester,ruester,ruester,tyr spawn_multiple_master
      <br>
      <br>
      Parent process 0 running on ruester.informatik.hs-fulda.de
      <br>
        I create 3 slave processes.
      <br>
      <br>
      Parent process 0: tasks in MPI_COMM_WORLD:                    1
      <br>
                        tasks in COMM_CHILD_PROCESSES local group:  1
      <br>
                        tasks in COMM_CHILD_PROCESSES remote group: 3
      <br>
      <br>
      Slave process 2 of 3 running on ruester.informatik.hs-fulda.de
      <br>
      Slave process 0 of 3 running on ruester.informatik.hs-fulda.de
      <br>
      spawn_slave 0: argv[0]: spawn_slave
      <br>
      Slave process 1 of 3 running on ruester.informatik.hs-fulda.de
      <br>
      spawn_slave 1: argv[0]: spawn_slave
      <br>
      spawn_slave 1: argv[1]: program type 2
      <br>
      spawn_slave 1: argv[2]: another parameter
      <br>
      spawn_slave 2: argv[0]: spawn_slave
      <br>
      spawn_slave 2: argv[1]: program type 2
      <br>
      spawn_slave 2: argv[2]: another parameter
      <br>
      spawn_slave 0: argv[1]: program type 1
      <br>
      tyr spawn 147
      <br>
      <br>
      <br>
      <br>
      Hopefully you can sort these things out. I've no idea what happens
      <br>
      and why I get different outputs, if I use different sets of the
      <br>
      same machines.
      <br>
      <br>
      <br>
      Kind regards
      <br>
      <br>
      Siegmar
      <br>
      <br>
      <br>
      <br>
      Am 05.05.2016 um 11:13 schrieb Gilles Gouaillardet:
      <br>
      <blockquote type="cite">Siegmar,
        <br>
        <br>
        is this Solaris 10 specific (e.g. Solaris 11 works fine)
        <br>
        <br>
        ( I only have a x86_64 vm with Solaris 11 and sun compilers ...)
        <br>
        <br>
        Cheers,
        <br>
        <br>
        Gilles
        <br>
        <br>
        On Thursday, May 5, 2016, Siegmar Gross
        &lt;<a class="moz-txt-link-abbreviated" href="mailto:siegmar.gross@informatik.hs-fulda.de">siegmar.gross@informatik.hs-fulda.de</a>
        <a class="moz-txt-link-rfc2396E" href="mailto:siegmar.gross@informatik.hs-fulda.de">&lt;mailto:siegmar.gross@informatik.hs-fulda.de&gt;</a>&gt; wrote:
        <br>
        <br>
            Hi Ralph and Gilles,
        <br>
        <br>
            Am 04.05.2016 um 20:02 schrieb rhc54:
        <br>
        <br>
                @ggouaillardet <a class="moz-txt-link-rfc2396E" href="https://github.com/ggouaillardet">&lt;https://github.com/ggouaillardet&gt;</a>
        Where does this stand?
        <br>
        <br>
                
        <br>
                You are receiving this because you were mentioned.
        <br>
                Reply to this email directly or view it on GitHub
        <br>
               
<a class="moz-txt-link-rfc2396E" href="https://github.com/open-mpi/ompi/issues/1569#issuecomment-216950103">&lt;https://github.com/open-mpi/ompi/issues/1569#issuecomment-216950103&gt;</a><br>
        <br>
        <br>
            With my last installed version of openmpi-v1.10.x all of my
        <br>
            spawn programs fail on Solaris Sparc and x86_64 with the
        same
        <br>
            error for both compilers (gcc-5.1.0 and Sun C 5.13).
        Everything
        <br>
            works as expected on Linux. Tomorrow I'm back in my office
        and
        <br>
            I can try to build and test the latest version.
        <br>
        <br>
            sunpc1 fd1026 108 ompi_info | grep -e "OPAL repo" -e "C
        compiler absolute"
        <br>
                  OPAL repo revision: v1.10.2-163-g42da15d
        <br>
                 C compiler absolute: /opt/solstudio12.4/bin/cc
        <br>
            sunpc1 fd1026 114 mpiexec -np 1 --host
        sunpc1,sunpc1,sunpc1,sunpc1,sunpc1 spawnmaster
        <br>
            [sunpc1:00957] *** Process received signal ***
        <br>
            [sunpc1:00957] Signal: Segmentation Fault (11)
        <br>
            [sunpc1:00957] Signal code: Address not mapped (1)
        <br>
            [sunpc1:00957] Failing at address: 0
        <br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libopen-pal.so.20.0.0:opalbacktrace_print+0x2d<br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libopen-pal.so.20.0.0:0x2383c<br>
            /lib/amd64/libc.so.1:0xdd6b6
        <br>
            /lib/amd64/libc.so.1:0xd1f82
        <br>
            /lib/amd64/libc.so.1:strlen+0x30 [ Signal 11 (SEGV)]
        <br>
            /lib/amd64/libc.so.1:vsnprintf+0x51
        <br>
            /lib/amd64/libc.so.1:vasprintf+0x49
        <br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libopen-pal.so.20.0.0:opalshow_help_vstring+0x83<br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libopen-rte.so.20.0.0:orteshow_help+0xd6<br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libmpi.so.20.0.0:ompi_mpi_nit+0x1010<br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libmpi.so.20.0.0:PMPI_Init0x9d<br>
            /home/fd1026/SunOS/x86_64/bin/spawn_master:main+0x21
        <br>
            [sunpc1:00957] *** End of error message ***
        <br>
           
--------------------------------------------------------------------------<br>
            mpiexec noticed that process rank 0 with PID 957 on node
        sunpc1 exited on signa 11 (Segmentation Fault).
        <br>
           
--------------------------------------------------------------------------<br>
        <br>
        <br>
        <br>
            sunpc1 fd1026 115 mpiexec -np 1 --host
        sunpc1,sunpc1,sunpc1,sunpc1,sunpc1 spawnmultiple_master
        <br>
            [sunpc1:00960] *** Process received signal ***
        <br>
            [sunpc1:00960] Signal: Segmentation Fault (11)
        <br>
            [sunpc1:00960] Signal code: Address not mapped (1)
        <br>
            [sunpc1:00960] Failing at address: 0
        <br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libopen-pal.so.20.0.0:opalbacktrace_print+0x2d<br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libopen-pal.so.20.0.0:0x2383c<br>
            /lib/amd64/libc.so.1:0xdd6b6
        <br>
            /lib/amd64/libc.so.1:0xd1f82
        <br>
            /lib/amd64/libc.so.1:strlen+0x30 [ Signal 11 (SEGV)]
        <br>
            /lib/amd64/libc.so.1:vsnprintf+0x51
        <br>
            /lib/amd64/libc.so.1:vasprintf+0x49
        <br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libopen-pal.so.20.0.0:opalshow_help_vstring+0x83<br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libopen-rte.so.20.0.0:orteshow_help+0xd6<br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libmpi.so.20.0.0:ompi_mpi_nit+0x1010<br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libmpi.so.20.0.0:PMPI_Init0x9d<br>
           
        /home/fd1026/SunOS/x86_64/bin/spawn_multiple_master:main+0x5d
        <br>
            [sunpc1:00960] *** End of error message ***
        <br>
           
--------------------------------------------------------------------------<br>
            mpiexec noticed that process rank 0 with PID 960 on node
        sunpc1 exited on signa 11 (Segmentation Fault).
        <br>
           
--------------------------------------------------------------------------<br>
        <br>
        <br>
        <br>
            sunpc1 fd1026 116 mpiexec -np 1 --host
        sunpc1,sunpc1,sunpc1,sunpc1,sunpc1 spawnintra_comm
        <br>
            [sunpc1:00963] *** Process received signal ***
        <br>
            [sunpc1:00963] Signal: Segmentation Fault (11)
        <br>
            [sunpc1:00963] Signal code: Address not mapped (1)
        <br>
            [sunpc1:00963] Failing at address: 0
        <br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libopen-pal.so.20.0.0:opalbacktrace_print+0x2d<br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libopen-pal.so.20.0.0:0x2383c<br>
            /lib/amd64/libc.so.1:0xdd6b6
        <br>
            /lib/amd64/libc.so.1:0xd1f82
        <br>
            /lib/amd64/libc.so.1:strlen+0x30 [ Signal 11 (SEGV)]
        <br>
            /lib/amd64/libc.so.1:vsnprintf+0x51
        <br>
            /lib/amd64/libc.so.1:vasprintf+0x49
        <br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libopen-pal.so.20.0.0:opalshow_help_vstring+0x83<br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libopen-rte.so.20.0.0:orteshow_help+0xd6<br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libmpi.so.20.0.0:ompi_mpi_nit+0x1010<br>
           
/export2/prog/SunOS_x86_64/openmpi-2.0.0_64_cc/lib64/libmpi.so.20.0.0:PMPI_Init0x9d<br>
            /home/fd1026/SunOS/x86_64/bin/spawn_intra_comm:main+0x23
        <br>
            [sunpc1:00963] *** End of error message ***
        <br>
           
--------------------------------------------------------------------------<br>
            mpiexec noticed that process rank 0 with PID 963 on node
        sunpc1 exited on signa 11 (Segmentation Fault).
        <br>
           
--------------------------------------------------------------------------<br>
            sunpc1 fd1026 117
        <br>
        <br>
        <br>
            Kind regards
        <br>
        <br>
            Siegmar
        <br>
            _______________________________________________
        <br>
            users mailing list
        <br>
            <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
        <br>
            Subscription:
        <a class="moz-txt-link-freetext" href="https://www.open-mpi.org/mailman/listinfo.cgi/users">https://www.open-mpi.org/mailman/listinfo.cgi/users</a>
        <br>
            Link to this post:
        <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/05/29090.php">http://www.open-mpi.org/community/lists/users/2016/05/29090.php</a>
        <br>
        <br>
        <br>
        <br>
        _______________________________________________
        <br>
        users mailing list
        <br>
        <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
        <br>
        Subscription:
        <a class="moz-txt-link-freetext" href="https://www.open-mpi.org/mailman/listinfo.cgi/users">https://www.open-mpi.org/mailman/listinfo.cgi/users</a>
        <br>
        Link to this post:
        <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/05/29092.php">http://www.open-mpi.org/community/lists/users/2016/05/29092.php</a>
        <br>
        <br>
      </blockquote>
      <br>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="https://www.open-mpi.org/mailman/listinfo.cgi/users">https://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/05/29096.php">http://www.open-mpi.org/community/lists/users/2016/05/29096.php</a></pre>
    </blockquote>
    <br>
  </body>
</html>

