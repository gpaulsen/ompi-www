at first I recommend you test 7 cases<div>- one network only (3 cases)</div><div>- two networks ony (3 cases)</div><div>- three networks (1 case)</div><div><br></div><div>and see when things hang</div><div><br></div><div>you might also want to </div><div>mpirun --mca oob_tcp_if_include <a href="http://10.1.10.0/24">10.1.10.0/24</a> ...</div><div>to ensure no hang will happen in oob</div><div><br></div><div>as usual, double check no firewall is running, and your hosts can ping each other </div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles</div><div><br>On Saturday, May 14, 2016, dpchoudh . &lt;<a href="mailto:dpchoudh@gmail.com">dpchoudh@gmail.com</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><div><div><div>Dear developers<br><br></div>I have been observing this issue all along on the master branch, but have been brushing off as something to do with my installation.<br><br></div>Right now, I just downloaded a fresh checkout (via git pull), built and installed it (after deleting /usr/local/lib/openmpi/) and I can reproduce the hang 100% of the time.<br><br></div>Description of the setup:<br><br></div><div>1. Two x86_64 boxes (dual xeons, 6 core each)<br></div><div>2. Four network interfaces, 3 running IP:<br></div><div><div><div><div><div>    Broadcom GbE (IP <span style="font-family:monospace,monospace">10.01.10.X/24</span>) BW 1 Gbps<br></div><div>    Chelsio iWARP (IP <span style="font-family:monospace,monospace">10.10.10.X/24</span>) BW 10 Gbps<br></div><div>    Qlogic Infiniband (IP <span style="font-family:monospace,monospace">10.01.11.X/24</span>) BW 20Gbps<br></div><div>    LSI logic Fibre channel (not running IP, I don&#39;t think this matters)<br><br></div><div>All of the NICs have their link UP. All the NICs are in separate IP subnets, connected back to back.<br><br></div><div>With this, the following command hangs:<br></div><div>(The hostfile is this:<br><span style="font-family:monospace,monospace">10.10.10.10 slots=1<br>10.10.10.11 slots=1</span><br><br></div><div><span style="font-family:monospace,monospace">[durga@smallMPI ~]$ mpirun -np 2 -hostfile ~/hostfile -mca btl self,tcp -mca pml ob1 ./mpitest</span><br><br></div><div>with the following output:<br><br><span style="font-family:monospace,monospace">Hello world from processor smallMPI, rank 0 out of 2 processors<br>Hello world from processor bigMPI, rank 1 out of 2 processors<br>smallMPI sent haha!, rank 0<br>bigMPI received haha!, rank 1</span><br><br></div><div>The stack trace at rank 0 is:<br><br><span style="font-family:monospace,monospace">(gdb) bt<br>#0  0x00007f9cb844769d in poll () from /lib64/libc.so.6<br>#1  0x00007f9cb79354d6 in poll_dispatch (base=0xddb540, tv=0x7ffc065d01b0) at poll.c:165<br>#2  0x00007f9cb792d180 in opal_libevent2022_event_base_loop (base=0xddb540, flags=2) at event.c:1630<br>#3  0x00007f9cb7851e74 in opal_progress () at runtime/opal_progress.c:171<br>#4  0x00007f9cb89bc47d in opal_condition_wait (c=0x7f9cb8f37c40 &lt;ompi_request_cond&gt;, m=0x7f9cb8f37bc0 &lt;ompi_request_lock&gt;) at ../opal/threads/condition.h:76<br>#5  0x00007f9cb89bcadf in ompi_request_default_wait_all (count=2, requests=0x7ffc065d0360, statuses=0x7ffc065d0330) at request/req_wait.c:287<br>#6  0x00007f9cb8a95469 in ompi_coll_base_sendrecv_zero (dest=1, stag=-16, source=1, rtag=-16, comm=0x601280 &lt;ompi_mpi_comm_world&gt;)<br>    at base/coll_base_barrier.c:63<br>#7  0x00007f9cb8a95b86 in ompi_coll_base_barrier_intra_two_procs (comm=0x601280 &lt;ompi_mpi_comm_world&gt;, module=0xeb4a00) at base/coll_base_barrier.c:313<br>#8  0x00007f9cb8ac6d1c in ompi_coll_tuned_barrier_intra_dec_fixed (comm=0x601280 &lt;ompi_mpi_comm_world&gt;, module=0xeb4a00) at coll_tuned_decision_fixed.c:196<br>#9  0x00007f9cb89dc689 in PMPI_Barrier (comm=0x601280 &lt;ompi_mpi_comm_world&gt;) at pbarrier.c:63<br>#10 0x0000000000400b11 in main (argc=1, argv=0x7ffc065d0648) at mpitest.c:27<br></span><br></div><div>and at rank 1 is:<br><br><span style="font-family:monospace,monospace">(gdb) bt<br>#0  0x00007f1101e7d69d in poll () from /lib64/libc.so.6<br>#1  0x00007f110136b4d6 in poll_dispatch (base=0x1d54540, tv=0x7ffd73013710) at poll.c:165<br>#2  0x00007f1101363180 in opal_libevent2022_event_base_loop (base=0x1d54540, flags=2) at event.c:1630<br>#3  0x00007f1101287e74 in opal_progress () at runtime/opal_progress.c:171<br>#4  0x00007f11023f247d in opal_condition_wait (c=0x7f110296dc40 &lt;ompi_request_cond&gt;, m=0x7f110296dbc0 &lt;ompi_request_lock&gt;) at ../opal/threads/condition.h:76<br>#5  0x00007f11023f2adf in ompi_request_default_wait_all (count=2, requests=0x7ffd730138c0, statuses=0x7ffd73013890) at request/req_wait.c:287<br>#6  0x00007f11024cb469 in ompi_coll_base_sendrecv_zero (dest=0, stag=-16, source=0, rtag=-16, comm=0x601280 &lt;ompi_mpi_comm_world&gt;)<br>    at base/coll_base_barrier.c:63<br>#7  0x00007f11024cbb86 in ompi_coll_base_barrier_intra_two_procs (comm=0x601280 &lt;ompi_mpi_comm_world&gt;, module=0x1e2ebc0) at base/coll_base_barrier.c:313<br>#8  0x00007f11024cde3c in ompi_coll_tuned_barrier_intra_dec_fixed (comm=0x601280 &lt;ompi_mpi_comm_world&gt;, module=0x1e2ebc0) at coll_tuned_decision_fixed.c:196<br>#9  0x00007f1102412689 in PMPI_Barrier (comm=0x601280 &lt;ompi_mpi_comm_world&gt;) at pbarrier.c:63<br>#10 0x0000000000400b11 in main (argc=1, argv=0x7ffd73013ba8) at mpitest.c:27</span><br><br></div><div>The code for the test program is:<br><br><span style="font-family:monospace,monospace">#include &lt;mpi.h&gt;<br>#include &lt;stdio.h&gt;<br>#include &lt;string.h&gt;<br>#include &lt;stdlib.h&gt;<br><br>int main(int argc, char *argv[])<br>{<br>    int world_size, world_rank, name_len;<br>    char hostname[MPI_MAX_PROCESSOR_NAME], buf[8];<br><br>    MPI_Init(&amp;argc, &amp;argv);<br>    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);<br>    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);<br>    MPI_Get_processor_name(hostname, &amp;name_len);<br>    printf(&quot;Hello world from processor %s, rank %d out of %d processors\n&quot;, hostname, world_rank, world_size);<br>    if (world_rank == 1)<br>    {<br>    MPI_Recv(buf, 6, MPI_CHAR, 0, 99, MPI_COMM_WORLD, MPI_STATUS_IGNORE);<br>    printf(&quot;%s received %s, rank %d\n&quot;, hostname, buf, world_rank);<br>    }<br>    else<br>    {<br>    strcpy(buf, &quot;haha!&quot;); <br>    MPI_Send(buf, 6, MPI_CHAR, 1, 99, MPI_COMM_WORLD);<br>    printf(&quot;%s sent %s, rank %d\n&quot;, hostname, buf, world_rank);<br>    }<br>    MPI_Barrier(MPI_COMM_WORLD);<br>    MPI_Finalize();<br>    return 0;<br>}</span><br><br></div><div>I have a strong feeling that there is an issue in this kind of situation. I&#39;ll be more than happy to run further tests if someone asks me to.<br><br></div><div>Thank you<br></div><div>Durga<br></div><div><br clear="all"><div><div><div dir="ltr"><div><div dir="ltr">The surgeon general advises you to eat right, exercise regularly and quit ageing.</div></div></div></div></div>
</div></div></div></div></div></div>
</blockquote></div>

