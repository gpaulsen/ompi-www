<html><head><meta http-equiv="Content-Type" content="text/html charset=iso-8859-1"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">Interesting data. Couple of quick points that might help:<div><br></div><div>option B is equivalent to --map-by node --bind-to none. When you bind to every core on the node, we don't bind you at all since "bind to all" is exactly equivalent to "bind to none". So it will definitely run slower as the threads run across the different NUMA regions on the node.</div><div><br></div><div>You might also want to try --map-by socket, with no binding directive. This would map one process to each socket, binding it to the socket - which is similar to what your option A actually accomplished. The only difference is that the procs that share a node will differ in rank by 1, whereas option A would have those procs differ in rank by N. Depending on your communication pattern, this could make a big difference.</div><div><br></div><div>Map-by socket is typically the fastest performance for threaded apps. You generally don't want P=1 unless you have a *lot* of threads in the process as it removes any use of shared memory, and so messaging will run slower - and you want the ranks that share a node to be the ones that most frequently communicate to each other, if you can identify them.</div><div><br></div><div>HTH</div><div>Ralph</div><div><br></div><div><div><div>On Apr 10, 2014, at 5:59 PM, Saliya Ekanayake &lt;<a href="mailto:esaliya@gmail.com">esaliya@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr"><div style="font-family:arial,sans-serif;font-size:13px">Hi,</div><div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">I am evaluating the performance of a clustering program written in Java with MPI+threads and would like to get some insight in solving a peculiar case. I've attached a performance graph to explain this.<br>
</div><div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">In essence the tests were carried out as TxPxN, where T is threads per process, P is processes per node, and N is number of nodes. I noticed an inefficiency with Tx<b>1</b>xN cases in general (tall bars in graph).</div>
<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">To elaborate a bit further,&nbsp;</div><div style="font-family:arial,sans-serif;font-size:13px">1. each node has 2 sockets with 4 cores each (totaling 8 cores)&nbsp;</div>
<div style="font-family:arial,sans-serif;font-size:13px">2. used OpenMPI 1.7.5rc5 (later tested with 1.8 and observed the same)</div><div style="font-family:arial,sans-serif;font-size:13px">3. with options</div><div style="font-family:arial,sans-serif;font-size:13px">
&nbsp; &nbsp; &nbsp;A.) --map-by node:PE=4 and --bind-to core</div><div style="font-family:arial,sans-serif;font-size:13px">&nbsp; &nbsp; &nbsp;B.) --map-by node:PE=8 and --bind-to-core</div><div style="font-family:arial,sans-serif;font-size:13px">&nbsp; &nbsp; &nbsp;C.) --map-by socket and --bind-to none</div>
<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">Timing of A,B,C came out as A &lt; B &lt; C, so used results from option A for Tx<b>1</b>xN in the graph.&nbsp;</div>
<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">Could you please give some suggestion that may help to speed up these Tx<b>1</b>xN cases? Also, I expected B to perform better than A as threads could utilize all 8 cores, but it wasn't the case.</div>
<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">Thank you,</div><div style="font-family:arial,sans-serif;font-size:13px">Saliya</div><div style="font-family:arial,sans-serif;font-size:13px">
<br></div><div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px"><span>&lt;image.png&gt;</span></div>
<div><br></div>-- <br><div dir="ltr"><span style="color:rgb(136,136,136)">Saliya Ekanayake <a href="mailto:esaliya@gmail.com" target="_blank">esaliya@gmail.com</a></span><span style="color:rgb(136,136,136)">&nbsp;</span><br style="color:rgb(136,136,136)">
<span style="color:rgb(136,136,136)">Cell 812-391-4914 Home 812-961-6383</span><br style="color:rgb(136,136,136)"><font color="#888888"><a href="http://saliya.org/" target="_blank">http://saliya.org</a></font></div>
</div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
