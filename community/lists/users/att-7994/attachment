I&#39;m trying to run a small &quot;proof of concept&quot; program using OpenMPI 1.3. &nbsp;I am using Solaris 8 with Sparc processors across 2 nodes. &nbsp;It appears that the MPI_Reduce function is hanging. &nbsp;If I run the same program with only 4 instances on 1 node , or 2 instances on 2 nodes, it works fine. &nbsp;The problem is visible with 4 instances on 2 nodes.<div>
<br></div><div>First, I had some issues while compiling OpenMPI. &nbsp;I did resolve my compile-time issues, so I would like to share with you my fixes. &nbsp;I believe that my compile-time issues are related to running an older version of Solaris, and probably not due to any major issue in OpenMPI. &nbsp;These fixes are not related to my problem, but thought you might need to see this in case it provides insight onto what my problem is.</div>
<div><br></div><div>1)&nbsp;./opal/mca/paffinity/solaris/paffinity_solaris_module.c</div><div>_SC_CPUID_MAX was undefined. &nbsp;I made the following change to 2 locations in the source:</div><div><div>&nbsp;&nbsp; &nbsp;cpuid_max = 7; /* sysconf(_SC_CPUID_MAX); */ &nbsp;/* Running on 8 CPU nodes */</div>
<div><br></div><div>2)&nbsp;./ompi/contrib/vt/vt/vtlib/vt_iowrap.c</div><div>vfscan was undefined. &nbsp;I had to comment out the following code (it appears that fscanf() was not required anyway):</div><div><br></div><div>a)&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;/* #include &lt;stdint.h&gt; */</div>
<div>b)&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;/* &nbsp; &nbsp; &nbsp;VT_IOWRAP_INIT_FUNC(fscanf); */</div><div>c) &nbsp;I commented out the entire fscanf() function</div><div><br></div><div><br></div><div>Now, I seem to be stuck on a run-time issue. &nbsp;I wrote a program (located in the attached bz2 file) called sieve.c which calculates prime numbers using the sieve algorithm (copied the code from somewhere). &nbsp;When I run the program on a local node only with 4 threads it works fine. &nbsp;If I run the program with 2 threads on 2 nodes, it also works fine. &nbsp;If I run the program with 4 threads on 2 nodes, it hangs. &nbsp;I made the following observations:</div>
<div><br></div><div>1) It is&nbsp;definitely&nbsp;hanging during the call to MPI_Reduce().&nbsp;</div><div><br></div><div>2) Some instances do exit MPI_Reduce(), while other instances enter but never exit this function.</div><div><br></div>
<div>3) If I added the following code right before calling MPI_Reduce(), the problem went away. &nbsp;It appears that by delaying the destination instance of the reduce operation from making the call, it seems to work. &nbsp;However, I do realize this is a&nbsp;kludge&nbsp;and that it is no&nbsp;guarantee&nbsp;that it will work all the time.</div>
<div><br></div><div><div>&nbsp;&nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);</div><div>&nbsp;&nbsp; &nbsp;&nbsp;if(!id) sleep(1);</div><div><br></div><div>4) If I changed the MPI_Reduce() to an MPI_Allreduce(), the sieve program also works with 4 instances across 2 nodes.</div>
<div><br></div><div>I did search your archives, and found someone else with a similar issue, but I didn&#39;t see any response.</div><div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href="http://www.open-mpi.org/community/lists/users/2008/07/6157.php">http://www.open-mpi.org/community/lists/users/2008/07/6157.php</a></div>
<div><br></div><div>My PATH includes:</div><div>&nbsp;&nbsp; &nbsp;&nbsp;/home/username/mpi/openmpi-1.3.local/bin</div><div><br></div><div>My LD_LIBRARY_PATH includes:</div><div>&nbsp;&nbsp; &nbsp;&nbsp;/home/username/mpi/openmpi-1.3.local/lib</div><div><br></div>
<div>I used the following in my configure parameters:</div><div>./configure --prefix=/home/username/mpi/openmpi-1.3.local --disable-mpi-f77 --disable-mpi-f90 CFLAGS=-xarch=v8plus CXXFLAGS=-xarch=v8plus<br></div><div><br></div>
<div>I compiled the program with:</div><div>mpicc -g -o sieve sieve.c<br></div><div><br></div><div>I ran the program with:</div><div>mpirun -np 4 -H node1,node2 sieve 100<br></div><div><br></div><div><br></div><div>Please let me know if you need any additional information. &nbsp;And thanks in advance for any help you can provide.</div>
<div><br></div><div>Thanks,</div><div>Brian</div><div><br></div><div><br></div></div></div>

