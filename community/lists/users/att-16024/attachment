<html>
<head>
<style><!--
.hmmessage P
{
margin:0px;
padding:0px
}
body.hmmessage
{
font-size: 10pt;
font-family:Tahoma
}
--></style>
</head>
<body class='hmmessage'>
<font class="Apple-style-span" face="Tahoma" size="2">Hi,&nbsp;</font><div style="font-family: Tahoma; font-size: 10pt; "><br></div><div><font class="Apple-style-span" face="Tahoma" size="2">The job queue has a time budget, which has been set in my job&nbsp;script.</font></div><div style="font-family: Tahoma; font-size: 10pt; "><br></div><div style="font-family: Tahoma; font-size: 10pt; ">For example, my current job queue is 24 hours.&nbsp;</div><div style="font-family: Tahoma; font-size: 10pt; "><br></div><div style="font-family: Tahoma; font-size: 10pt; ">But, my program got SIGKILL (signal 9) within not more than 2 hours since it began to run.&nbsp;</div><div style="font-family: Tahoma; font-size: 10pt; "><br></div><div style="font-family: Tahoma; font-size: 10pt; ">Are there other possible settings that I need to consider ?&nbsp;</div><div style="font-family: Tahoma; font-size: 10pt; "><br></div><div style="font-family: Tahoma; font-size: 10pt; ">thanks</div><div style="font-family: Tahoma; font-size: 10pt; "><br></div><div style="font-family: Tahoma; font-size: 10pt; ">Jack</div><div style="font-family: Tahoma; font-size: 10pt; "><br><br>&gt; From: jsquyres@cisco.com<br>&gt; Date: Sun, 27 Mar 2011 20:29:11 -0400<br>&gt; To: users@open-mpi.org<br>&gt; Subject: Re: [OMPI users] OMPI error terminate w/o reasons<br>&gt; <br>&gt; +1 on what Ralph is saying.<br>&gt; <br>&gt; You need to talk to your local administrators and ask them why Torque is killing your job.  Perhaps you're submitting to a queue that only allows jobs to run for a few seconds, or something like that.<br>&gt; <br>&gt; <br>&gt; On Mar 27, 2011, at 3:08 PM, Ralph Castain wrote:<br>&gt; <br>&gt; &gt; It means that Torque is unhappy with your job - either you are running longer than it permits, or you exceeded some other system limit.<br>&gt; &gt; <br>&gt; &gt; Talk to your sys admin about imposed limits. Usually, there are flags you can provide to your job submission that allow you to change limits for your program.<br>&gt; &gt; <br>&gt; &gt; <br>&gt; &gt; On Mar 27, 2011, at 12:59 PM, Jack Bryan wrote:<br>&gt; &gt; <br>&gt; &gt;&gt; Hi, I have figured out how to run the command. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; OMPI_RANKFILE=$HOME/$PBS_JOBID.ranks<br>&gt; &gt;&gt; <br>&gt; &gt;&gt;  mpirun -np 200  -rf $OMPI_RANKFILE --mca btl self,sm,openib -output-filename 700g200i200p14ye  ./myapplication <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Each process print out to a distinct file.<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; But, the program is terminated by the error :<br>&gt; &gt;&gt; ---------------------------------------------------------------------------------------------------------------------<br>&gt; &gt;&gt; =&gt;&gt; PBS: job killed: node 18 (n314) requested job terminate, 'EOF' (code 1099) - received SISTER_EOF attempting to communicate with sister MOM's<br>&gt; &gt;&gt; mpirun: Forwarding signal 10 to job<br>&gt; &gt;&gt; mpirun: killing job...<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; --------------------------------------------------------------------------<br>&gt; &gt;&gt; mpirun was unable to cleanly terminate the daemons on the nodes shown<br>&gt; &gt;&gt; below. Additional manual cleanup may be required - please refer to<br>&gt; &gt;&gt; the "orte-clean" tool for assistance.<br>&gt; &gt;&gt; --------------------------------------------------------------------------<br>&gt; &gt;&gt;         n341<br>&gt; &gt;&gt;         n338<br>&gt; &gt;&gt;         n337<br>&gt; &gt;&gt;         n336<br>&gt; &gt;&gt;         n335<br>&gt; &gt;&gt;         n334<br>&gt; &gt;&gt;         n333<br>&gt; &gt;&gt;         n332<br>&gt; &gt;&gt;         n331<br>&gt; &gt;&gt;         n329<br>&gt; &gt;&gt;         n328<br>&gt; &gt;&gt;         n326<br>&gt; &gt;&gt;         n324<br>&gt; &gt;&gt;         n321<br>&gt; &gt;&gt;         n318<br>&gt; &gt;&gt;         n316<br>&gt; &gt;&gt;         n315<br>&gt; &gt;&gt;         n314<br>&gt; &gt;&gt;         n313<br>&gt; &gt;&gt;         n312<br>&gt; &gt;&gt;         n309<br>&gt; &gt;&gt;         n308<br>&gt; &gt;&gt;         n306<br>&gt; &gt;&gt;         n305<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; --------------------------------------------------------------------<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; After searching, I find that the error is probably related to the highly frequent I/O activities. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; I have also run valgrind to do mem check in  order to find the possible reason for the original <br>&gt; &gt;&gt; signal 9 (SIGKILL) problem. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; mpirun -np 200 -rf $OMPI_RANKFILE --mca btl self,sm,openib  /usr/bin/valgrind --tool=memcheck --error-limit=no --leak-check=yes --log-file=nsga2b_g700_pop200_p200_valg_cystorm_mpi.log  ./myapplication <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; But, I got the similar error as the above. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; What does the error mean ?   <br>&gt; &gt;&gt; I cannot change the file system of the cluster. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; I only want to find a way to find the bug, which only appears in the case that the problem size is very large. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; But, I am stucked by the SIGKILL and then the above MOM_SISTER issues now. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Any help is really appreciated. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; thanks<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Jack <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; --------------------------------------------------------------------------------------------------------<br>&gt; &gt;&gt; From: rhc@open-mpi.org<br>&gt; &gt;&gt; Date: Sat, 26 Mar 2011 20:47:19 -0600<br>&gt; &gt;&gt; To: users@open-mpi.org<br>&gt; &gt;&gt; Subject: Re: [OMPI users] OMPI error terminate w/o reasons<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; That command line cannot possibly work. Both the -rf and --output-filename options require arguments.<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; PLEASE read the documentation? mpirun -h, or "man mpirun" will tell you how to correctly use these options.<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; On Mar 26, 2011, at 6:35 PM, Jack Bryan wrote:<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Hi, I used : <br>&gt; &gt;&gt; <br>&gt; &gt;&gt;  mpirun -np 200 -rf  --output-filename /mypath/myapplication<br>&gt; &gt;&gt; But, no files are printed out.<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Can "--debug" option help me hear ? <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; When I tried :<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; -bash-3.2$ mpirun -debug<br>&gt; &gt;&gt; --------------------------------------------------------------------------<br>&gt; &gt;&gt; A suitable debugger could not be found in your PATH.  Check the values<br>&gt; &gt;&gt; specified in the orte_base_user_debugger MCA parameter for the list of<br>&gt; &gt;&gt; debuggers that was searched.<br>&gt; &gt;&gt; --------------------------------------------------------------------------<br>&gt; &gt;&gt; Any help is really appreciated. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; thanks<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; From: rhc@open-mpi.org<br>&gt; &gt;&gt; Date: Sat, 26 Mar 2011 15:45:39 -0600<br>&gt; &gt;&gt; To: users@open-mpi.org<br>&gt; &gt;&gt; Subject: Re: [OMPI users] OMPI error terminate w/o reasons<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; If you use that mpirun option, mpirun will place the output from each rank into a -separate- file for you. Give it:<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; mpirun --output-filename /myhome/debug/run01<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; and in /myhome/debug, you will find files:<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; run01.0<br>&gt; &gt;&gt; run01.1<br>&gt; &gt;&gt; ...<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; each with the output from the indicated rank.<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; On Mar 26, 2011, at 3:41 PM, Jack Bryan wrote:<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; The cluster can print out all output into one file. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; But, checking them for bugs is very hard. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; The cluster also print out possible error messages into one file. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; But, sometimes the error file is empty , sometimes it is signal 9.<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; If I only run dummy tasks on worker nodes, no errors. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; If I run real task, sometimes processes are terminated w/o any errors before the program normally exit.<br>&gt; &gt;&gt; Sometimes, the program get signal 9 but no other error messages. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; It is weird. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Any help is really appreciated. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Jack<br>&gt; &gt;&gt; From: rhc@open-mpi.org<br>&gt; &gt;&gt; Date: Sat, 26 Mar 2011 15:18:53 -0600<br>&gt; &gt;&gt; To: users@open-mpi.org<br>&gt; &gt;&gt; Subject: Re: [OMPI users] OMPI error terminate w/o reasons<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; I don't know, but Ashley may be able to help - or you can see his web site for instructions.<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Alternatively, since you can put print statements into your code, have you considered using mpirun's option to direct output from each rank into its own file? Look at "mpirun -h" for the options.<br>&gt; &gt;&gt; <br>&gt; &gt;&gt;    -output-filename|--output-filename &lt;arg0&gt;  <br>&gt; &gt;&gt;                          Redirect output from application processes into<br>&gt; &gt;&gt;                          filename.rank<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; On Mar 26, 2011, at 2:48 PM, Jack Bryan wrote:<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Is it possible to enable padb to print out the stack trace and other program execute information into a file ?<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; I can run the program in gdb as this: <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; mpirun -np 200 -e gdb ./myapplication <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; How to make gdb print out the debug information to a file ? <br>&gt; &gt;&gt; So that I can check it when the program is terminated. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; thanks<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Jack<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; From: rhc@open-mpi.org<br>&gt; &gt;&gt; Date: Sat, 26 Mar 2011 13:56:13 -0600<br>&gt; &gt;&gt; To: users@open-mpi.org<br>&gt; &gt;&gt; Subject: Re: [OMPI users] OMPI error terminate w/o reasons<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; You don't need to install anything on a system folder - you can just install it in your home directory, assuming that is accessible on the remote nodes.<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; As for the script - unless you can somehow modify it to allow you to run under a debugger, I am afraid you are completely out of luck.<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; On Mar 26, 2011, at 12:54 PM, Jack Bryan wrote:<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Hi, <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; I am working on a cluster, where I am not allowed to install software on system folder. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; My Open MPI is 1.3.4. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; I have a very quick of the padb on http://padb.pittman.org.uk/ . <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Does it require some software install on the cluster in order to use it ? <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; I cannot use command-line to run job on the lcuster , but only script.<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; thanks<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; From: rhc@open-mpi.org<br>&gt; &gt;&gt; Date: Sat, 26 Mar 2011 12:12:11 -0600<br>&gt; &gt;&gt; To: users@open-mpi.org<br>&gt; &gt;&gt; Subject: Re: [OMPI users] OMPI error terminate w/o reasons<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Have you tried a parallel debugger such as padb?<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; On Mar 26, 2011, at 10:34 AM, Jack Bryan wrote:<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Hi, <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; I have tried this. But, the printout from 200 parallel processes make it <br>&gt; &gt;&gt; very hard to locate the possible bug. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; They may not stop at the same point when the program got signal 9.<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; So, even though I can figure out the print out statements from all<br>&gt; &gt;&gt; 200 processes, so many different locations where the processes<br>&gt; &gt;&gt; are stopped make it harder to find out some hints about the bug. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Are there some other programming tricks, which can help me <br>&gt; &gt;&gt; narrow down to the doubt points ASAP.<br>&gt; &gt;&gt; Any help is appreciated. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Jack<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; From: rhc@open-mpi.org<br>&gt; &gt;&gt; Date: Sat, 26 Mar 2011 07:53:40 -0600<br>&gt; &gt;&gt; To: users@open-mpi.org<br>&gt; &gt;&gt; Subject: Re: [OMPI users] OMPI error terminate w/o reasons<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Try adding some print statements so you can see where the error occurs.<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; On Mar 25, 2011, at 11:49 PM, Jack Bryan wrote:<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Hi , All: <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; I running a Open MPI (1.3.4) program by 200 parallel processes. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; But, the program is terminated with <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; --------------------------------------------------------------------------<br>&gt; &gt;&gt; mpirun noticed that process rank 0 with PID 77967 on node n342 exited on signal 9 (Killed).<br>&gt; &gt;&gt; --------------------------------------------------------------------------<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; After searching, the signal 9 means: <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; the process is currently in an unworkable state and should be terminated with extreme prejudice<br>&gt; &gt;&gt; <br>&gt; &gt;&gt;  If a process does not respond to any other termination signals, sending it a SIGKILL signal will almost always cause it to go away.<br>&gt; &gt;&gt; <br>&gt; &gt;&gt;  The system will generate SIGKILL for a process itself under some unusual conditions where the program cannot possibly continue to run (even to run a signal handler).<br>&gt; &gt;&gt;  <br>&gt; &gt;&gt; But, the error message does not indicate any possible reasons for the termination. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; There is a FOR loop in the main() program, if the loop number is small (&lt; 200), the program works well, <br>&gt; &gt;&gt; but if it becomes lager and larger, the program will got SIGKILL. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; The cluster where I am running the MPI program does not allow running debug tools. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; If I run it on a workstation, it will take a very very long time (for &gt; 200 loops) in order to <br>&gt; &gt;&gt; get the error occur again. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; What can I do to find the possible bugs ? <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Any help is really appreciated. <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; thanks<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Jack<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; _______________________________________________<br>&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; users@open-mpi.org<br>&gt; &gt;&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; _______________________________________________ users mailing list users@open-mpi.org http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt; _______________________________________________<br>&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; users@open-mpi.org<br>&gt; &gt;&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; _______________________________________________ users mailing list users@open-mpi.org http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt; _______________________________________________<br>&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; users@open-mpi.org<br>&gt; &gt;&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; _______________________________________________ users mailing list users@open-mpi.org http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt; _______________________________________________<br>&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; users@open-mpi.org<br>&gt; &gt;&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; _______________________________________________ users mailing list users@open-mpi.org http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt; _______________________________________________<br>&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; users@open-mpi.org<br>&gt; &gt;&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; _______________________________________________ users mailing list users@open-mpi.org http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt; _______________________________________________<br>&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; users@open-mpi.org<br>&gt; &gt;&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; _______________________________________________ users mailing list users@open-mpi.org http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt; _______________________________________________<br>&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; users@open-mpi.org<br>&gt; &gt;&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt; <br>&gt; &gt; _______________________________________________<br>&gt; &gt; users mailing list<br>&gt; &gt; users@open-mpi.org<br>&gt; &gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; <br>&gt; <br>&gt; -- <br>&gt; Jeff Squyres<br>&gt; jsquyres@cisco.com<br>&gt; For corporate legal information go to:<br>&gt; http://www.cisco.com/web/about/doing_business/legal/cri/<br>&gt; <br>&gt; <br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; users@open-mpi.org<br>&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br></div> 		 	   		  </body>
</html>
