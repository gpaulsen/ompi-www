Diego, <div><br></div><div>your code snippet does MPI_Waitall(2,...)</div><div>but the error is about MPI_Waitall(3,...)</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<br><br>On Friday, January 29, 2016, Diego Avesani &lt;<a href="mailto:diego.avesani@gmail.com">diego.avesani@gmail.com</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div>Dear all, </div><div><br></div><div>I have created a program in fortran and OpenMPI, I test it on my laptop and it works.</div><div>I would like to use it on a cluster that has, unfortunately, intel MPI.<br></div><div><br></div><div>The program crushes on the cluster and I get the following error:</div><div><br></div><div><div><i>Fatal error in MPI_Waitall: Invalid MPI_Request, error stack:</i></div><div><i>MPI_Waitall(271): MPI_Waitall(count=3, req_array=0x7445f0, status_array=0x744600) failed</i></div><div><i>MPI_Waitall(119): The supplied request in array element 2 was invalid (kind=0)</i></div></div><div><i><br></i></div><div>Do OpenMPI and MPI have some difference that I do not know?</div><div><br></div><div>this is my code</div><div><br></div><div><div> REQUEST = MPI_REQUEST_NULL</div><div> !send data share with left</div><div> IF(MPIdata%rank.NE.0)THEN</div><div>    MsgLength = MPIdata%imaxN</div><div>    DO icount=1,MPIdata%imaxN</div><div>            iNode = MPIdata%nodeFromUp(icount)</div><div>            send_messageL(icount) = R1(iNode)</div><div>    ENDDO</div><div>    CALL MPI_ISEND(send_messageL, MsgLength, MPIdata%AUTO_COMP, MPIdata%rank-1, MPIdata%rank, MPI_COMM_WORLD, REQUEST(1), MPIdata%iErr)</div><div> ENDIF</div><div> !</div><div> !recive message FROM RIGHT CPU</div><div> IF(MPIdata%rank.NE.MPIdata%nCPU-1)THEN</div><div>    MsgLength = MPIdata%imaxN</div><div>    CALL MPI_IRECV(recv_messageR, MsgLength, MPIdata%AUTO_COMP, MPIdata%rank+1, MPIdata%rank+1, MPI_COMM_WORLD, REQUEST(2), MPIdata%iErr)</div><div> ENDIF</div><div> CALL MPI_WAITALL(2,REQUEST,send_status_list,MPIdata%iErr)</div><div> IF(MPIdata%rank.NE.MPIdata%nCPU-1)THEN</div><div>    DO i=1,MPIdata%imaxN</div><div>       iNode=MPIdata%nodeList2Up(i)</div><div>       R1(iNode)=recv_messageR(i)</div><div>    ENDDO</div><div> ENDIF</div></div><div><br></div><div>Thank a lot your help</div><div><br></div><div><br></div><div><br></div><div><div>Diego<br><br></div></div>
</div>
</blockquote></div>

