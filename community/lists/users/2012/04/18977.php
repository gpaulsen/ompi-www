<?
$subject_val = "Re: [OMPI users] sge tight intregration leads to bad allocation";
include("../../include/msg-header.inc");
?>
<!-- received="Tue Apr 10 05:42:25 2012" -->
<!-- isoreceived="20120410094225" -->
<!-- sent="Tue, 10 Apr 2012 11:42:20 +0200" -->
<!-- isosent="20120410094220" -->
<!-- name="Eloi Gaudry" -->
<!-- email="eloi.gaudry_at_[hidden]" -->
<!-- subject="Re: [OMPI users] sge tight intregration leads to bad allocation" -->
<!-- id="zarafa.4f84007c.57f3.3dafed3e642894cf_at_mail.fft" -->
<!-- charset="us-ascii" -->
<!-- inreplyto="D97E3572-87D1-4133-8B62-7CF7140C7B5A_at_staff.uni-marburg.de" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>Subject:</strong> Re: [OMPI users] sge tight intregration leads to bad allocation<br>
<strong>From:</strong> Eloi Gaudry (<em>eloi.gaudry_at_[hidden]</em>)<br>
<strong>Date:</strong> 2012-04-10 05:42:20
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18978.php">Ralph Castain: "Re: [OMPI users] sge tight intregration leads to bad allocation"</a>
<li><strong>Previous message:</strong> <a href="18976.php">Shiqing Fan: "Re: [OMPI users] Myid changes to 0 after using a mpi_recv"</a>
<li><strong>In reply to:</strong> <a href="18975.php">Reuti: "Re: [OMPI users] sge tight intregration leads to bad allocation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18978.php">Ralph Castain: "Re: [OMPI users] sge tight intregration leads to bad allocation"</a>
<li><strong>Reply:</strong> <a href="18978.php">Ralph Castain: "Re: [OMPI users] sge tight intregration leads to bad allocation"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
Thx. This is the allocation which is also confirmed by the Open MPI output.
<br>
[eg: ] exactly, but not the one used afterwards by openmpi
<br>
<p>- The application was compiled with the same version of Open MPI?
<br>
[eg: ] yes, version 1.4.4 for all
<br>
<p>- Does the application start something on its own besides the tasks granted by mpiexec/orterun?
<br>
[eg: ] no
<br>
<p>You want 12 ranks in total, and to barney.fft and carl.fft there are also &quot;-mca orte_ess_num_procs 3 &quot; given in to the qrsh_starter. In total I count only 10 ranks in this example given - 4+4+2 - do you observe the same?
<br>
[eg: ] i don't know why the -mca orte_ess_num_procs 3 is added here...
<br>
In the &quot;Map generated by mapping policy&quot; output in my last email, I see that 4 processes were started on each node (barney, carl and charlie), but yes, in the ps -elf output, two of them are missing for one node (barney)... sorry about that, a bad copy/paste. Here is the actual output for this node:
<br>
2048 ?        Sl     3:33 /opt/sge/bin/lx-amd64/sge_execd
<br>
27502 ?        Sl     0:00  \_ sge_shepherd-1416 -bg
<br>
27503 ?        Ss     0:00      \_ /opt/sge/utilbin/lx-amd64/qrsh_starter /opt/sge/default/spool/barney/active_jobs/1416.1/1.barney
<br>
27510 ?        S      0:00          \_ bash -c  PATH=/opt/openmpi-1.4.4/bin:$PATH ; export PATH ; LD_LIBRARY_PATH=/opt/openmpi-1.4.4/lib:$LD_LIBRARY_PATH ; export LD_LIBRARY_PATH ;  /opt/openmpi-1.4.4/bin/orted -mca ess env -mca orte_ess_jobid 3800367104 -mca orte_ess_vpid 1 -mca orte_ess_num_procs 3 --hnp-uri &quot;3800367104.0;tcp://192.168.0.20:57233&quot; --mca pls_gridengine_verbose 1 --mca ras_gridengine_show_jobid 1 --mca ras_gridengine_verbose 1
<br>
27511 ?        S      0:00              \_ /opt/openmpi-1.4.4/bin/orted -mca ess env -mca orte_ess_jobid 3800367104 -mca orte_ess_vpid 1 -mca orte_ess_num_procs 3 --hnp-uri 3800367104.0;tcp://192.168.0.20:57233 --mca pls_gridengine_verbose 1 --mca ras_gridengine_show_jobid 1 --mca ras_gridengine_verbose 1
<br>
27512 ?        Rl    12:54                  \_ /opt/fft/actran_product/Actran_13.0.b.57333/bin/actranpy_mp --apl=/opt/fft/actran_product/Actran_13.0.b.57333 -e radiation -m 10000 --parallel=frequency --scratch=/scratch/cluster/1416 --inputfile=/home/jj/Projects/Toyota/REFERENCE_JPC/semi_green_PML_06/semi_green_coarse.edat
<br>
27513 ?        Rl    12:54                  \_ /opt/fft/actran_product/Actran_13.0.b.57333/bin/actranpy_mp --apl=/opt/fft/actran_product/Actran_13.0.b.57333 -e radiation -m 10000 --parallel=frequency --scratch=/scratch/cluster/1416 --inputfile=/home/jj/Projects/Toyota/REFERENCE_JPC/semi_green_PML_06/semi_green_coarse.edat
<br>
27514 ?        Rl    12:54                  \_ /opt/fft/actran_product/Actran_13.0.b.57333/bin/actranpy_mp --apl=/opt/fft/actran_product/Actran_13.0.b.57333 -e radiation -m 10000 --parallel=frequency --scratch=/scratch/cluster/1416 --inputfile=/home/jj/Projects/Toyota/REFERENCE_JPC/semi_green_PML_06/semi_green_coarse.edat
<br>
27515 ?        Rl    12:53                  \_ /opt/fft/actran_product/Actran_13.0.b.57333/bin/actranpy_mp --apl=/opt/fft/actran_product/Actran_13.0.b.57333 -e radiation -m 10000 --parallel=frequency --scratch=/scratch/cluster/1416 --inputfile=/home/jj/Projects/Toyota/REFERENCE_JPC/semi_green_PML_06/semi_green_coarse.edat
<br>
<p>It looks like Open MPI is doing the right thing, but the applications decided to start in a different allocation.
<br>
[eg: ] if the &quot;Map generated by mapping policy&quot; is different than the sge allocation, then openmpi is not doing the right thing, don't you think ?
<br>
<p>Does the application use OpenMP in addition or other kinds of threads? The suffix &quot;_mp&quot; in the name &quot;actranpy_mp&quot; makes me suspicious about it.
<br>
[eg: ] no, the suffix _mp stands for &quot;parallel&quot;.
<br>
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18978.php">Ralph Castain: "Re: [OMPI users] sge tight intregration leads to bad allocation"</a>
<li><strong>Previous message:</strong> <a href="18976.php">Shiqing Fan: "Re: [OMPI users] Myid changes to 0 after using a mpi_recv"</a>
<li><strong>In reply to:</strong> <a href="18975.php">Reuti: "Re: [OMPI users] sge tight intregration leads to bad allocation"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18978.php">Ralph Castain: "Re: [OMPI users] sge tight intregration leads to bad allocation"</a>
<li><strong>Reply:</strong> <a href="18978.php">Ralph Castain: "Re: [OMPI users] sge tight intregration leads to bad allocation"</a>
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
