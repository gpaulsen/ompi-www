<?
$subject_val = "[OMPI users] sge tight intregration leads to bad allocation";
include("../../include/msg-header.inc");
?>
<!-- received="Tue Apr  3 09:23:24 2012" -->
<!-- isoreceived="20120403132324" -->
<!-- sent="Tue, 3 Apr 2012 15:23:17 +0200" -->
<!-- isosent="20120403132317" -->
<!-- name="Eloi Gaudry" -->
<!-- email="eloi.gaudry_at_[hidden]" -->
<!-- subject="[OMPI users] sge tight intregration leads to bad allocation" -->
<!-- id="zarafa.4f7af9c5.6495.62b21fa141df3b31_at_mail.fft" -->
<!-- charset="us-ascii" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>Subject:</strong> [OMPI users] sge tight intregration leads to bad allocation<br>
<strong>From:</strong> Eloi Gaudry (<em>eloi.gaudry_at_[hidden]</em>)<br>
<strong>Date:</strong> 2012-04-03 09:23:17
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18918.php">Tom Bryan: "Re: [OMPI users] sge tight intregration leads to bad allocation"</a>
<li><strong>Previous message:</strong> <a href="18916.php">Ralph Castain: "Re: [OMPI users] Error with multiple MPI runs inside one Slurm allocation (with QLogic PSM)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18918.php">Tom Bryan: "Re: [OMPI users] sge tight intregration leads to bad allocation"</a>
<li><strong>Reply:</strong> <a href="18918.php">Tom Bryan: "Re: [OMPI users] sge tight intregration leads to bad allocation"</a>
<li><strong>Reply:</strong> <a href="18919.php">Ralph Castain: "Re: [OMPI users] sge tight intregration leads to bad allocation"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
Hi,

 
I've observed a strange behavior during rank allocation on a distributed run schedule and submitted using Sge (Son of Grid Egine 8.0.0d) and OpenMPI-1.4.4.

Briefly, there is a one-slot difference between allocated rank/slot for Sge and OpenMPI. The issue here is that one node becomes oversubscribed at runtime.

 
Here is the output of the allocation done for gridengine:

 
======================   ALLOCATED NODES   ======================

 
Data for node: Name: barney                 Launch id: -1      Arch: ffc91200   State: 2

               Num boards: 1  Num sockets/board: 2  Num cores/socket: 2

               Daemon: [[22904,0],0]  Daemon launched: True

               Num slots: 1      Slots in use: 0

               Num slots allocated: 1   Max slots: 0

               Username on node: NULL

               Num procs: 0     Next node_rank: 0

Data for node: Name: carl.fft                  Launch id: -1      Arch: 0  State: 2

               Num boards: 1  Num sockets/board: 2  Num cores/socket: 2

               Daemon: Not defined   Daemon launched: False

               Num slots: 1      Slots in use: 0

               Num slots allocated: 1   Max slots: 0

               Username on node: NULL

               Num procs: 0     Next node_rank: 0

Data for node: Name: charlie.fft                            Launch id: -1      Arch: 0  State: 2

               Num boards: 1  Num sockets/board: 2  Num cores/socket: 2

               Daemon: Not defined   Daemon launched: False

               Num slots: 2      Slots in use: 0

               Num slots allocated: 2   Max slots: 0

               Username on node: NULL

               Num procs: 0     Next node_rank: 0

 
 
And here is the allocation finally used:

=================================================================

 
Map generated by mapping policy: 0200

               Npernode: 0      Oversubscribe allowed: TRUE   CPU Lists: FALSE

               Num new daemons: 2  New daemon starting vpid 1

               Num nodes: 3

 
Data for node: Name: barney                 Launch id: -1      Arch: ffc91200   State: 2

               Num boards: 1  Num sockets/board: 2  Num cores/socket: 2

               Daemon: [[22904,0],0]  Daemon launched: True

               Num slots: 1      Slots in use: 2

               Num slots allocated: 1   Max slots: 0

               Username on node: NULL

               Num procs: 2     Next node_rank: 2

               Data for proc: [[22904,1],0]

                              Pid: 0     Local rank: 0       Node rank: 0

                              State: 0                App_context: 0                Slot list: NULL

               Data for proc: [[22904,1],3]

                              Pid: 0     Local rank: 1       Node rank: 1

                              State: 0                App_context: 0                Slot list: NULL

 
Data for node: Name: carl.fft                  Launch id: -1      Arch: 0  State: 2

               Num boards: 1  Num sockets/board: 2  Num cores/socket: 2

               Daemon: [[22904,0],1]  Daemon launched: False

               Num slots: 1      Slots in use: 1

               Num slots allocated: 1   Max slots: 0

               Username on node: NULL

               Num procs: 1     Next node_rank: 1

               Data for proc: [[22904,1],1]

                              Pid: 0     Local rank: 0       Node rank: 0

                              State: 0                App_context: 0                Slot list: NULL

 
Data for node: Name: charlie.fft                            Launch id: -1      Arch: 0  State: 2

               Num boards: 1  Num sockets/board: 2  Num cores/socket: 2

               Daemon: [[22904,0],2]  Daemon launched: False

               Num slots: 2      Slots in use: 1

               Num slots allocated: 2   Max slots: 0

               Username on node: NULL

               Num procs: 1     Next node_rank: 1

               Data for proc: [[22904,1],2]

                              Pid: 0     Local rank: 0       Node rank: 0

                              State: 0                App_context: 0                Slot list: NULL

 
Has anyone already encounter the same behavior ?

Is there a simple fix than not using the tight integration mode between Sge and OpenMPI ?

 
Eloi

 

<br>
<p>
<p><hr>
<ul>
<li>text/html attachment: <a href="http://www.open-mpi.org/community/lists/users/att-18917/attachment">attachment</a>
</ul>
<!-- attachment="attachment" -->
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="18918.php">Tom Bryan: "Re: [OMPI users] sge tight intregration leads to bad allocation"</a>
<li><strong>Previous message:</strong> <a href="18916.php">Ralph Castain: "Re: [OMPI users] Error with multiple MPI runs inside one Slurm allocation (with QLogic PSM)"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="18918.php">Tom Bryan: "Re: [OMPI users] sge tight intregration leads to bad allocation"</a>
<li><strong>Reply:</strong> <a href="18918.php">Tom Bryan: "Re: [OMPI users] sge tight intregration leads to bad allocation"</a>
<li><strong>Reply:</strong> <a href="18919.php">Ralph Castain: "Re: [OMPI users] sge tight intregration leads to bad allocation"</a>
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
