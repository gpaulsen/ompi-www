<html><body>
<p>Hi, Lenny,<br>
<br>
        So rank file map will be supported in OpenMPI 1.3?    I'm using OpenMPI1.2.6 and did not find parameter &quot;rmaps_rank_file_&quot;.  <br>
       Do you have idea when OpenMPI 1.3 will be available?    OpenMPI 1.3 has quite a few features I'm looking for.<br>
<br>
Thanks,<br>
Mi <br>
<img width="16" height="16" src="cid:2__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" border="0" alt="Inactive hide details for &quot;Lenny Verkhovsky&quot; &lt;lenny.verkhovsky@gmail.com&gt;">&quot;Lenny Verkhovsky&quot; &lt;lenny.verkhovsky@gmail.com&gt;<br>
<br>
<br>

<table width="100%" border="0" cellspacing="0" cellpadding="0">
<tr valign="top"><td style="background-image:url(cid:3__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com); background-repeat: no-repeat; " width="40%">
<ul>
<ul>
<ul>
<ul><b><font size="2">&quot;Lenny Verkhovsky&quot; &lt;lenny.verkhovsky@gmail.com&gt;</font></b><font size="2"> </font><br>
<font size="2">Sent by: users-bounces@open-mpi.org</font>
<p><font size="2">10/23/2008 05:48 AM</font>
<table border="1">
<tr valign="top"><td width="168" bgcolor="#FFFFFF"><div align="center"><font size="2">Please respond to<br>
Open MPI Users &lt;users@open-mpi.org&gt;</font></div></td></tr>
</table>
</ul>
</ul>
</ul>
</ul>
</td><td width="60%">
<table width="100%" border="0" cellspacing="0" cellpadding="0">
<tr valign="top"><td width="1%"><img width="58" height="1" src="cid:4__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<div align="right"><font size="2">To</font></div></td><td width="100%"><img width="1" height="1" src="cid:4__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<font size="2">&quot;Open MPI Users&quot; &lt;users@open-mpi.org&gt;</font></td></tr>

<tr valign="top"><td width="1%"><img width="58" height="1" src="cid:4__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<div align="right"><font size="2">cc</font></div></td><td width="100%"><img width="1" height="1" src="cid:4__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
</td></tr>

<tr valign="top"><td width="1%"><img width="58" height="1" src="cid:4__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<div align="right"><font size="2">Subject</font></div></td><td width="100%"><img width="1" height="1" src="cid:4__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<font size="2">Re: [OMPI users] Working with a CellBlade cluster</font></td></tr>
</table>

<table border="0" cellspacing="0" cellpadding="0">
<tr valign="top"><td width="58"><img width="1" height="1" src="cid:4__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" border="0" alt=""></td><td width="336"><img width="1" height="1" src="cid:4__=0ABBFE78DFD5666F8f9e8a93df938@us.ibm.com" border="0" alt=""></td></tr>
</table>
</td></tr>
</table>
<br>
<font size="4">Hi,</font><br>
<font size="4"> </font><br>
<font size="4"> </font><br>
<font size="4">If I understand you correctly the most suitable way to do it is by paffinity that we have in Open MPI 1.3 and the trank.</font><br>
<font size="4">how ever usually OS is distributing processes evenly between sockets by it self.</font><br>
<font size="4"> </font><br>
<font size="4">There still no formal FAQ due to a multiple reasons but you can read how to use it in the attached scratch ( there were few name changings of the params, so check with ompi_info )</font><br>
<font size="4"> </font><br>
<font size="4">shared memory is used between processes that share same machine, and openib is used between different machines ( hostnames ), no special mca params are needed.</font><br>
<font size="4"> </font><br>
<font size="4">Best Regards</font><br>
<font size="4">Lenny,</font>
<p><font size="4"><br>
<br>
 </font>
<p><font size="4">On Sun, Oct 19, 2008 at 10:32 AM, Gilbert Grosdidier &lt;</font><a href="mailto:grodid@mail.cern.ch"><u><font size="4" color="#0000FF">grodid@mail.cern.ch</font></u></a><font size="4">&gt; wrote:</font>
<ul><font size="4"> Working with a CellBlade cluster (QS22), the requirement is to have one<br>
instance of the executable running on each socket of the blade (there are 2<br>
sockets). The application is of the 'domain decomposition' type, and each<br>
instance is required to often send/receive data with both the remote blades and<br>
the neighbor socket.<br>
<br>
 Question is : which specification must be used for the mca btl component<br>
to force 1) shmem type messages when communicating with this neighbor socket,<br>
while 2) using openib to communicate with the remote blades ?<br>
Is '-mca btl sm,openib,self' suitable for this ?<br>
<br>
 Also, which debug flags could be used to crosscheck that the messages are<br>
_actually_ going thru the right channel for a given channel, please ?<br>
<br>
 We are currently using OpenMPI 1.2.5 shipped with RHEL5.2 (ppc64).<br>
Which version do you think is currently the most optimised for these<br>
processors and problem type ? Should we go towards OpenMPI 1.2.8 instead ?<br>
Or even try some OpenMPI 1.3 nightly build ?<br>
<br>
 Thanks in advance for your help,                  Gilbert.<br>
<br>
_______________________________________________<br>
users mailing list</font><u><font size="4" color="#0000FF"><br>
</font></u><a href="mailto:users@open-mpi.org"><u><font size="4" color="#0000FF">users@open-mpi.org</font></u></a><u><font size="4" color="#0000FF"><br>
</font></u><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank"><u><font size="4" color="#0000FF">http://www.open-mpi.org/mailman/listinfo.cgi/users</font></u></a></ul>
<i>(See attached file: RANKS_FAQ.doc)</i><tt>_______________________________________________<br>
users mailing list<br>
users@open-mpi.org<br>
</tt><tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></tt><br>
</body></html>

