<div dir="ltr">I have try if another mpi process is running in the node already the process run <div><br><div>$ricardo$ /opt/openmpi/bin/mpirun  --mca plm_rsh_no_tree_spawn 1 -mca plm_base_verbose 10 -host nexus16 ompi_info</div>
<div><div>[nexus10.nlroc:27397] mca: base: components_register: registering plm components</div><div>[nexus10.nlroc:27397] mca: base: components_register: found loaded component isolated</div><div>[nexus10.nlroc:27397] mca: base: components_register: component isolated has no register or open function</div>
<div>[nexus10.nlroc:27397] mca: base: components_register: found loaded component rsh</div><div>[nexus10.nlroc:27397] mca: base: components_register: component rsh register function successful</div><div>[nexus10.nlroc:27397] mca: base: components_register: found loaded component slurm</div>
<div>[nexus10.nlroc:27397] mca: base: components_register: component slurm register function successful</div><div>[nexus10.nlroc:27397] mca: base: components_open: opening plm components</div><div>[nexus10.nlroc:27397] mca: base: components_open: found loaded component isolated</div>
<div>[nexus10.nlroc:27397] mca: base: components_open: component isolated open function successful</div><div>[nexus10.nlroc:27397] mca: base: components_open: found loaded component rsh</div><div>[nexus10.nlroc:27397] mca: base: components_open: component rsh open function successful</div>
<div>[nexus10.nlroc:27397] mca: base: components_open: found loaded component slurm</div><div>[nexus10.nlroc:27397] mca: base: components_open: component slurm open function successful</div><div>[nexus10.nlroc:27397] mca:base:select: Auto-selecting plm components</div>
<div>[nexus10.nlroc:27397] mca:base:select:(  plm) Querying component [isolated]</div><div>[nexus10.nlroc:27397] mca:base:select:(  plm) Query of component [isolated] set priority to 0</div><div>[nexus10.nlroc:27397] mca:base:select:(  plm) Querying component [rsh]</div>
<div>[nexus10.nlroc:27397] mca:base:select:(  plm) Query of component [rsh] set priority to 10</div><div>[nexus10.nlroc:27397] mca:base:select:(  plm) Querying component [slurm]</div><div>[nexus10.nlroc:27397] mca:base:select:(  plm) Skipping component [slurm]. Query failed to return a module</div>
<div>[nexus10.nlroc:27397] mca:base:select:(  plm) Selected component [rsh]</div><div>[nexus10.nlroc:27397] mca: base: close: component isolated closed</div><div>[nexus10.nlroc:27397] mca: base: close: unloading component isolated</div>
<div>[nexus10.nlroc:27397] mca: base: close: component slurm closed</div><div>[nexus10.nlroc:27397] mca: base: close: unloading component slurm</div><div>[nexus10.nlroc:27397] [[52326,0],0] plm:base:receive update proc state command from [[52326,0],1]</div>
<div>[nexus10.nlroc:27397] [[52326,0],0] plm:base:receive got update_proc_state for job [52326,1]</div><div>[nexus16.nlroc:59687] mca: base: components_register: registering plm components</div><div>[nexus16.nlroc:59687] mca: base: components_register: found loaded component isolated</div>
<div>[nexus16.nlroc:59687] mca: base: components_register: component isolated has no register or open function</div><div>[nexus16.nlroc:59687] mca: base: components_register: found loaded component rsh</div><div>[nexus16.nlroc:59687] mca: base: components_register: component rsh register function successful</div>
<div>[nexus16.nlroc:59687] mca: base: components_register: found loaded component slurm</div><div>[nexus16.nlroc:59687] mca: base: components_register: component slurm register function successful</div></div><div><div>                 Package: Open MPI XXXX@nexus10.nlroc Distribution</div>
<div>                Open MPI: 1.8.1</div><div>  Open MPI repo revision: r31483</div><div>   Open MPI release date: Apr 22, 2014</div><div>                Open RTE: 1.8.1</div></div><div>…</div><div><br></div><div>but if the compute node has not a mpi process running in it it already hangs as</div>
<div><br></div><div><div>/opt/openmpi/bin/mpirun  --mca plm_rsh_no_tree_spawn 1 -mca plm_base_verbose 10 -host nexus17 ompi_info</div><div>[nexus10.nlroc:27438] mca: base: components_register: registering plm components</div>
<div>[nexus10.nlroc:27438] mca: base: components_register: found loaded component isolated</div><div>[nexus10.nlroc:27438] mca: base: components_register: component isolated has no register or open function</div><div>[nexus10.nlroc:27438] mca: base: components_register: found loaded component rsh</div>
<div>[nexus10.nlroc:27438] mca: base: components_register: component rsh register function successful</div><div>[nexus10.nlroc:27438] mca: base: components_register: found loaded component slurm</div><div>[nexus10.nlroc:27438] mca: base: components_register: component slurm register function successful</div>
<div>[nexus10.nlroc:27438] mca: base: components_open: opening plm components</div><div>[nexus10.nlroc:27438] mca: base: components_open: found loaded component isolated</div><div>[nexus10.nlroc:27438] mca: base: components_open: component isolated open function successful</div>
<div>[nexus10.nlroc:27438] mca: base: components_open: found loaded component rsh</div><div>[nexus10.nlroc:27438] mca: base: components_open: component rsh open function successful</div><div>[nexus10.nlroc:27438] mca: base: components_open: found loaded component slurm</div>
<div>[nexus10.nlroc:27438] mca: base: components_open: component slurm open function successful</div><div>[nexus10.nlroc:27438] mca:base:select: Auto-selecting plm components</div><div>[nexus10.nlroc:27438] mca:base:select:(  plm) Querying component [isolated]</div>
<div>[nexus10.nlroc:27438] mca:base:select:(  plm) Query of component [isolated] set priority to 0</div><div>[nexus10.nlroc:27438] mca:base:select:(  plm) Querying component [rsh]</div><div>[nexus10.nlroc:27438] mca:base:select:(  plm) Query of component [rsh] set priority to 10</div>
<div>[nexus10.nlroc:27438] mca:base:select:(  plm) Querying component [slurm]</div><div>[nexus10.nlroc:27438] mca:base:select:(  plm) Skipping component [slurm]. Query failed to return a module</div><div>[nexus10.nlroc:27438] mca:base:select:(  plm) Selected component [rsh]</div>
<div>[nexus10.nlroc:27438] mca: base: close: component isolated closed</div><div>[nexus10.nlroc:27438] mca: base: close: unloading component isolated</div><div>[nexus10.nlroc:27438] mca: base: close: component slurm closed</div>
<div>[nexus10.nlroc:27438] mca: base: close: unloading component slurm</div></div><div><br></div><div>and  it stop there</div></div><div><br></div><div><br></div></div><div class="gmail_extra"><br><br><div class="gmail_quote">
On Mon, Jul 14, 2014 at 8:56 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div style="word-wrap:break-word">Hmmm...no, it worked just fine for me. It sounds like something else is going on.<div><br></div><div>Try configuring OMPI with --enable-debug, and then add -mca plm_base_verbose 10 to get a better sense of what is going on.</div>
<div><div class="h5"><div><br></div><div><br><div><div>On Jul 14, 2014, at 10:27 AM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt; wrote:</div><br><blockquote type="cite"><div style="word-wrap:break-word">
I confess I haven&#39;t tested no_tree_spawn in ages, so it is quite possible it has suffered bit rot. I can try to take a look at it in a bit<div><br><div><br><div><div>On Jul 14, 2014, at 10:13 AM, Ricardo Fernández-Perea &lt;<a href="mailto:rfernandezperea@gmail.com" target="_blank">rfernandezperea@gmail.com</a>&gt; wrote:</div>
<br><blockquote type="cite"><div dir="ltr"><div>Thank you for the fast answer </div><div><br></div>While that resolve my problem with cross ssh authentication   a command as<div><br><div>/opt/openmpi/bin/mpirun  --mca mtl mx --mca pml cm --mca plm_rsh_no_tree_spawn 1 -hostfile hostfile ompi_info<br>

</div></div><div><br></div><div>just hung with no output and although there is a ssh connexion no orte program is initiated in the destination nodes</div><div><br></div><div>and while </div><div><br></div><div>/opt/openmpi/bin/mpirun  -host host18 ompi_info<br>

</div><div><br></div><div>works</div><div><br></div><div>/opt/openmpi/bin/mpirun  --mca plm_rsh_no_tree_spawn 1 -host host18 ompi_info<br></div><div><br></div><div>hangs, is there some condition in the use of this parameter.</div>

<div><br></div><div>Yours truly</div><div><br></div><div>Ricardo </div><div><br></div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Mon, Jul 14, 2014 at 6:35 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">During the 1.7 series and for all follow-on series, OMPI changed to a mode where it launches a daemon on all allocated nodes at the startup of mpirun. This allows us to determine the hardware topology of the nodes and take that into account when mapping. You can override that behavior by either adding --novm to your cmd line (which will impact your mapping/binding options), or by specifying the hosts to use by editing your hostfile, or adding --host host1,host2 to your cmd line<br>


<br>
The rsh launcher defaults to a tree-based pattern, thus requiring that we be able to ssh from one compute node to another. You can change that to a less scalable direct mode by adding<br>
<br>
--mca plm_rsh_no_tree_spawn 1<br>
<br>
to the cmd line<br>
<div><div><br>
<br>
On Jul 14, 2014, at 9:21 AM, Ricardo Fernández-Perea &lt;<a href="mailto:rfernandezperea@gmail.com" target="_blank">rfernandezperea@gmail.com</a>&gt; wrote:<br>
<br>
&gt; I&#39;m trying to update to openMPI 1.8.1 thru ssh  and Myrinet<br>
&gt;<br>
&gt; running a command as<br>
&gt;<br>
&gt; /opt/openmpi/bin/mpirun --verbose --mca mtl mx --mca pml cm  -hostfile hostfile -np 16<br>
&gt;<br>
&gt; when the hostfile contain only two nodes as<br>
&gt;<br>
&gt; host1 slots=8 max-slots=8<br>
&gt; host2 slots=8 max-slots=8<br>
&gt;<br>
&gt; it runs perfectly but when the hostfile has a third node as<br>
&gt;<br>
&gt;<br>
&gt; host1 slots=8 max-slots=8<br>
&gt; host2 slots=8 max-slots=8<br>
&gt; host3 slots=8 max-slots=8<br>
&gt;<br>
&gt; it try to establish an ssh connection between  the running hosts1 and host3 that should not run any process  that fails hanging the process without signaling.<br>
&gt;<br>
&gt;<br>
&gt; my ompi_info is as follow<br>
&gt;<br>
&gt;                 Package: Open MPI XXX Distribution<br>
&gt;                 Open MPI: 1.8.1<br>
&gt;   Open MPI repo revision: r31483<br>
&gt;    Open MPI release date: Apr 22, 2014<br>
&gt;                 Open RTE: 1.8.1<br>
&gt;   Open RTE repo revision: r31483<br>
&gt;    Open RTE release date: Apr 22, 2014<br>
&gt;                     OPAL: 1.8.1<br>
&gt;       OPAL repo revision: r31483<br>
&gt;        OPAL release date: Apr 22, 2014<br>
&gt;                  MPI API: 3.0<br>
&gt;             Ident string: 1.8.1<br>
&gt;                   Prefix: /opt/openmpi<br>
&gt;  Configured architecture: x86_64-apple-darwin9.8.0<br>
&gt;           Configure host: XXXX<br>
&gt;            Configured by: XXXX<br>
&gt;            Configured on: Thu Jun 12 10:37:33 CEST 2014<br>
&gt;           Configure host: XXXX<br>
&gt;                 Built by: XXXX<br>
&gt;                 Built on: Thu Jun 12 11:13:16 CEST 2014<br>
&gt;               Built host: XXXX<br>
&gt;               C bindings: yes<br>
&gt;             C++ bindings: yes<br>
&gt;              Fort mpif.h: yes (single underscore)<br>
&gt;             Fort use mpi: yes (full: ignore TKR)<br>
&gt;        Fort use mpi size: deprecated-ompi-info-value<br>
&gt;         Fort use mpi_f08: yes<br>
&gt;  Fort mpi_f08 compliance: The mpi_f08 module is available, but due to<br>
&gt;                           limitations in the ifort compiler, does not support<br>
&gt;                           the following: array subsections, direct passthru<br>
&gt;                           (where possible) to underlying Open MPI&#39;s C<br>
&gt;                           functionality<br>
&gt;   Fort mpi_f08 subarrays: no<br>
&gt;            Java bindings: no<br>
&gt;   Wrapper compiler rpath: unnecessary<br>
&gt;               C compiler: icc<br>
&gt;      C compiler absolute: /opt/intel/Compiler/11.1/080/bin/intel64/icc<br>
&gt;   C compiler family name: INTEL<br>
&gt;       C compiler version: 1110.20091130<br>
&gt;             C++ compiler: icpc<br>
&gt;    C++ compiler absolute: /opt/intel/Compiler/11.1/080/bin/intel64/icpc<br>
&gt;            Fort compiler: ifort<br>
&gt;        Fort compiler abs: /opt/intel/Compiler/11.1/080/bin/intel64/ifort<br>
&gt;          Fort ignore TKR: yes (!DEC$ ATTRIBUTES NO_ARG_CHECK ::)<br>
&gt;    Fort 08 assumed shape: no<br>
&gt;       Fort optional args: yes<br>
&gt;       Fort BIND(C) (all): yes<br>
&gt;       Fort ISO_C_BINDING: yes<br>
&gt;  Fort SUBROUTINE BIND(C): yes<br>
&gt;        Fort TYPE,BIND(C): yes<br>
&gt;  Fort T,BIND(C,name=&quot;a&quot;): yes<br>
&gt;             Fort PRIVATE: yes<br>
&gt;           Fort PROTECTED: yes<br>
&gt;            Fort ABSTRACT: yes<br>
&gt;        Fort ASYNCHRONOUS: yes<br>
&gt;           Fort PROCEDURE: yes<br>
&gt;  Fort f08 using wrappers: yes<br>
&gt;              C profiling: yes<br>
&gt;            C++ profiling: yes<br>
&gt;    Fort mpif.h profiling: yes<br>
&gt;   Fort use mpi profiling: yes<br>
&gt;    Fort use mpi_f08 prof: yes<br>
&gt;           C++ exceptions: no<br>
&gt;           Thread support: posix (MPI_THREAD_MULTIPLE: no, OPAL support: yes,<br>
&gt;                           OMPI progress: no, ORTE progress: yes, Event lib:<br>
&gt;                           yes)<br>
&gt;            Sparse Groups: no<br>
&gt;   Internal debug support: no<br>
&gt;   MPI interface warnings: yes<br>
&gt;      MPI parameter check: runtime<br>
&gt; Memory profiling support: no<br>
&gt; Memory debugging support: no<br>
&gt;          libltdl support: yes<br>
&gt;    Heterogeneous support: no<br>
&gt;  mpirun default --prefix: no<br>
&gt;          MPI I/O support: yes<br>
&gt;        MPI_WTIME support: gettimeofday<br>
&gt;      Symbol vis. support: yes<br>
&gt;    Host topology support: yes<br>
&gt;           MPI extensions:<br>
&gt;    FT Checkpoint support: no (checkpoint thread: no)<br>
&gt;    C/R Enabled Debugging: no<br>
&gt;      VampirTrace support: yes<br>
&gt;   MPI_MAX_PROCESSOR_NAME: 256<br>
&gt;     MPI_MAX_ERROR_STRING: 256<br>
&gt;      MPI_MAX_OBJECT_NAME: 64<br>
&gt;         MPI_MAX_INFO_KEY: 36<br>
&gt;         MPI_MAX_INFO_VAL: 256<br>
&gt;        MPI_MAX_PORT_NAME: 1024<br>
&gt;   MPI_MAX_DATAREP_STRING: 128<br>
&gt;<br>
&gt;<br>
</div></div>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/07/24764.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/07/24764.php</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/07/24765.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/07/24765.php</a><br>
</blockquote></div><br></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/07/24766.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/07/24766.php</a></blockquote></div><br></div></div></div></blockquote>
</div><br></div></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/07/24768.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/07/24768.php</a><br></blockquote></div><br></div>

