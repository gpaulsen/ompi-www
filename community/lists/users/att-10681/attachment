Hi Josh,<br><br>It is good to hear from you that work is in progress towards resiliency of Open-MPI. I was and I am waiting for this capability in Open-MPI. I have almost finished my development work and waiting for this to happen so that I can test my programs. It will be good if you can tell how long it will take to make Open-MPI a resilient impementation. Here by resiliency I mean abnormal termination or intentionally killing a process should not cause any(parent or sibling) process to be terminated, given that processes are connected.<br>
<br>thanks.<br><br>Regards,<br><br><div class="gmail_quote">On Mon, Aug 3, 2009 at 8:37 PM, Josh Hursey <span dir="ltr">&lt;<a href="mailto:jjhursey@open-mpi.org">jjhursey@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Task-farm or manager/worker recovery models typically depend on intercommunicators (i.e., from MPI_Comm_spawn) and a resilient MPI implementation. William Gropp and Ewing Lusk have a paper entitled &quot;Fault Tolerance in MPI Programs&quot; that outlines how an application might take advantage of these features in order to recover from process failure.<br>

<br>
However, these techniques strongly depend upon resilient MPI implementations, and behaviors that, some may argue, are non-standard. Unfortunately there are not many MPI implementations that are sufficiently resilient in the face of process failure to support failure in task-farm scenarios. Though Open MPI supports the current MPI 2.1 standard, it is not as resilient to process failure as it could be.<br>

<br>
There are a number of people working on improving the resiliency of Open MPI in the face of network and process failure (including myself). We have started to move some of the resiliency work into the Open MPI trunk. Resiliency in Open MPI has been improving over the past few months, but I would not assess it as ready quite yet. Most of the work has focused on the runtime level (ORTE), and there are still some MPI level (OMPI) issues that need to be worked out.<br>

<br>
With all of that being said, I would try some of the techniques presented in the Gropp/Lusk paper in your application. Then test it with Open MPI and let us know how it goes.<br>
<br>
Best,<br><font color="#888888">
Josh</font><div><div></div><div class="h5"><br>
<br>
On Aug 3, 2009, at 10:30 AM, Durga Choudhury wrote:<br>
<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Is that kind of approach possible within an MPI framework? Perhaps a<br>
grid approach would be better. More experienced people, speak up,<br>
please?<br>
(The reason I say that is that I too am interested in the solution of<br>
that kind of problem, where an individual blade of a blade server<br>
fails and correcting for that failure on the fly is better than taking<br>
checkpoints and restarting the whole process excluding the failed<br>
blade.<br>
<br>
Durga<br>
<br>
On Mon, Aug 3, 2009 at 9:21 AM, jody&lt;<a href="mailto:jody.xha@gmail.com" target="_blank">jody.xha@gmail.com</a>&gt; wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Hi<br>
<br>
I guess &quot;task-farming&quot; could give you a certain amount of the kind of<br>
fault-tolerance you want.<br>
(i.e. a master process distributes tasks to idle slave processors -<br>
however, this will only work<br>
if the slave processes don&#39;t need to communicate with each other)<br>
<br>
Jody<br>
<br>
<br>
On Mon, Aug 3, 2009 at 1:24 PM, vipin kumar&lt;<a href="mailto:vipinkumar41@gmail.com" target="_blank">vipinkumar41@gmail.com</a>&gt; wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Hi all,<br>
<br>
Thanks Durga for your reply.<br>
<br>
Jeff, once you wrote code for Mandelbrot set to demonstrate fault tolerance<br>
in LAM-MPI. i. e. killing any slave process doesn&#39;t<br>
affect others. Exact behaviour I am looking for in Open MPI. I attempted,<br>
but no luck. Can you please tell how to write such programs in Open MPI.<br>
<br>
Thanks in advance.<br>
<br>
Regards,<br>
On Thu, Jul 9, 2009 at 8:30 PM, Durga Choudhury &lt;<a href="mailto:dpchoudh@gmail.com" target="_blank">dpchoudh@gmail.com</a>&gt; wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<br>
Although I have perhaps the least experience on the topic in this<br>
list, I will take a shot; more experienced people, please correct me:<br>
<br>
MPI standards specify communication mechanism, not fault tolerance at<br>
any level. You may achieve network tolerance at the IP level by<br>
implementing &#39;equal cost multipath&#39; routes (which means two equally<br>
capable NIC cards connecting to the same destination and modifying the<br>
kernel routing table to use both cards; the kernel will dynamically<br>
load balance.). At the MAC level, you can achieve the same effect by<br>
trunking multiple network cards.<br>
<br>
You can achieve process level fault tolerance by a checkpointing<br>
scheme such as BLCR, which has been tested to work with OpenMPI (and<br>
other processes as well)<br>
<br>
Durga<br>
<br>
On Thu, Jul 9, 2009 at 4:57 AM, vipin kumar&lt;<a href="mailto:vipinkumar41@gmail.com" target="_blank">vipinkumar41@gmail.com</a>&gt; wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<br>
Hi all,<br>
<br>
I want to know whether open mpi supports Network and process fault<br>
tolerance<br>
or not? If there is any example demonstrating these features that will<br>
be<br>
best.<br>
<br>
Regards,<br>
--<br>
Vipin K.<br>
Research Engineer,<br>
C-DOTB, India<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
</blockquote>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote>
<br>
<br>
<br>
--<br>
Vipin K.<br>
Research Engineer,<br>
C-DOTB, India<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
</blockquote>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
</blockquote>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br><br clear="all"><br>-- <br>Vipin K.<br>Research Engineer,<br>C-DOTB, India<br>

