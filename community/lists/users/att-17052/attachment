The job was dispatched by SGE scheduler but the mpi hello binary never gets executed on compute nodes.  It appears that the OpenMPI orted is waiting for something as shown below:<br><br>Master node:<br><br> 4465 ?        Sl     0:05 /usr/local/sge/latest/bin/lx26-amd64/sge_execd<br>
 4677 ?        S      0:00  \_ sge_shepherd-296 -bg<br> 4679 ?        Ss     0:00      \_ /bin/bash ./sge_jsb.sh<br> 4681 ?        S      0:00          \_ mpirun -np 3097 ./hello_openmpi.exe<br> 4682 ?        Sl     0:02              \_ /usr/local/sge/latest/bin/lx26-amd64/qrsh -inherit -nostdin -V x-01-00-a  orted -mca ess env -mca orte_ess_jobid 662831104 -mca orte_ess_vpid 1 -mca orte_ess_num_procs 130 --hnp-uri &quot;662831104.0;tcp://xxx.xx.4.8:39025&quot;<br>
 4683 ?        Sl     0:01              \_ /usr/local/sge/latest/bin/lx26-amd64/qrsh -inherit -nostdin -V x-01-06-b  orted -mca ess env -mca orte_ess_jobid 662831104 -mca orte_ess_vpid 2 -mca orte_ess_num_procs 130 --hnp-uri &quot;662831104.0;tcp://xxx.xx.4.8:39025&quot;<br>
... &lt;cut the ramining process&gt; ...<br><br>===<br><br>A client compute node:<br><br> 6290 ?        Sl     0:05 /usr/local/sge/latest/bin/lx26-amd64/sge_execd<br> 6793 ?        Sl     0:00  \_ sge_shepherd-296 -bg<br>
 6794 ?        Ss     0:00      \_ /usr/local/sge/latest/utilbin/lx26-amd64/qrsh_starter /var/spool/sge/62u5/x-01-00-a/active_jobs/296.1/1.x-01-00-a<br> 6801 ?        S      0:00          \_ orted -mca ess env -mca orte_ess_jobid 662831104 -mca orte_ess_vpid 1 -mca orte_ess_num_procs 130 --hnp-uri 662831104.0;tcp://xxx.xx.4.8:39025<br>
<br>- Chansup<br><br><div class="gmail_quote">On Wed, Aug 10, 2011 at 4:19 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">
<div style="word-wrap: break-word;">When you say &quot;stuck&quot;, what actually happens?<div><div></div><div class="h5"><div><br></div><div>On Aug 10, 2011, at 2:09 PM, CB wrote:</div><div><div><br><blockquote type="cite">
Now I was able to run MPI hello world example up to 3096 processes across 129 nodes (24 cores per node).<br>However, it seems to get stuck with 3097 processes.<br><br>Any suggestions for troubleshooting?<br><br>Thanks,<br>

- Chansup<br><br><br><div class="gmail_quote">On Tue, Aug 9, 2011 at 2:02 PM, CB <span dir="ltr">&lt;<a href="mailto:cbalways@gmail.com" target="_blank">cbalways@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">

Hi Ralph,<br><br>Yes, you are right. Those nodes were still pointing to an old version.<br>I&#39;ll check the installation on all nodes and try to run it again.<br><br>Thanks,<br>- Chansup<div><div></div><div><br>
<br><div class="gmail_quote">On Tue, Aug 9, 2011 at 1:48 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">That error makes no sense - line 335 is just a variable declaration. Sure you are not picking up a different version on that node?<br>



<div><div></div><div><br>
<br>
On Aug 9, 2011, at 11:37 AM, CB wrote:<br>
<br>
&gt; Hi,<br>
&gt;<br>
&gt; Currently I&#39;m having trouble to scale an MPI job beyond a certain limit.<br>
&gt; So I&#39;m running an MPI hello example to test beyond 1024 but it failed with the following error with 2048 processes.<br>
&gt; It worked fine with 1024 processes.  I have enough file descriptor limit (65536) defined for each process.<br>
&gt;<br>
&gt; I appreciate if anyone gives me any suggestions.<br>
&gt; I&#39;m running (Open MPI) 1.4.3<br>
&gt;<br>
&gt; [x-01-06-a:25989] [[37568,0],69] ORTE_ERROR_LOG: Data unpack had inadequate space in file base/odls_base_default_fns.c at line 335<br>
&gt; [x-01-06-b:09532] [[37568,0],74] ORTE_ERROR_LOG: Data unpack had inadequate space in file base/odls_base_default_fns.c at line 335<br>
&gt; --------------------------------------------------------------------------<br>
&gt; mpirun noticed that the job aborted, but has no info as to the process<br>
&gt; that caused that situation.<br>
&gt; --------------------------------------------------------------------------<br>
&gt; [x-03-20-b:23316] *** Process received signal ***<br>
&gt; [x-03-20-b:23316] Signal: Segmentation fault (11)<br>
&gt; [x-03-20-b:23316] Signal code: Address not mapped (1)<br>
&gt; [x-03-20-b:23316] Failing at address: 0x6c<br>
&gt; [x-03-20-b:23316] [ 0] /lib64/libpthread.so.0 [0x310860ee90]<br>
&gt; [x-03-20-b:23316] [ 1] /usr/local/MPI/openmpi-1.4.3/lib/libopen-rte.so.0(orte_plm_base_app_report_launch+0x230) [0x7f0dbe0c5010]<br>
&gt; [x-03-20-b:23316] [ 2] /usr/local/MPI/openmpi-1.4.3/lib/libopen-pal.so.0 [0x7f0dbde5c8f8]<br>
&gt; [x-03-20-b:23316] [ 3] mpirun [0x403bbe]<br>
&gt; [x-03-20-b:23316] [ 4] /usr/local/MPI/openmpi-1.4.3/lib/libopen-pal.so.0 [0x7f0dbde5c8f8]<br>
&gt; [x-03-20-b:23316] [ 5] /usr/local/MPI/openmpi-1.4.3/lib/libopen-pal.so.0(opal_progress+0x99) [0x7f0dbde50e49]<br>
&gt; [x-03-20-b:23316] [ 6] /usr/local/MPI/openmpi-1.4.3/lib/libopen-rte.so.0(orte_trigger_event+0x42) [0x7f0dbe0a7ca2]<br>
&gt; [x-03-20-b:23316] [ 7] /usr/local/MPI/openmpi-1.4.3/lib/libopen-rte.so.0(orte_plm_base_app_report_launch+0x22d) [0x7f0dbe0c500d]<br>
&gt; [x-03-20-b:23316] [ 8] /usr/local/MPI/openmpi-1.4.3/lib/libopen-pal.so.0 [0x7f0dbde5c8f8]<br>
&gt; [x-03-20-b:23316] [ 9] /usr/local/MPI/openmpi-1.4.3/lib/libopen-pal.so.0(opal_progress+0x99) [0x7f0dbde50e49]<br>
&gt; [x-03-20-b:23316] [10] /usr/local/MPI/openmpi-1.4.3/lib/libopen-rte.so.0(orte_plm_base_launch_apps+0x23d) [0x7f0dbe0c5ddd]<br>
&gt; [x-03-20-b:23316] [11] /usr/local/MPI/openmpi-1.4.3/lib/openmpi/mca_plm_rsh.so [0x7f0dbd41d679]<br>
&gt; [x-03-20-b:23316] [12] mpirun [0x40373f]<br>
&gt; [x-03-20-b:23316] [13] mpirun [0x402a1c]<br>
&gt; [x-03-20-b:23316] [14] /lib64/libc.so.6(__libc_start_main+0xfd) [0x3107e1ea2d]<br>
&gt; [x-03-20-b:23316] [15] mpirun [0x402939]<br>
&gt; [x-03-20-b:23316] *** End of error message ***<br>
&gt; [x-01-06-a:25989] [[37568,0],69]-[[37568,0],0] mca_oob_tcp_msg_recv: readv failed: Connection reset by peer (104)<br>
&gt; [x-01-06-b:09532] [[37568,0],74]-[[37568,0],0] mca_oob_tcp_msg_recv: readv failed: Connection reset by peer (104)<br>
&gt; ./sge_jsb.sh: line 9: 23316 Segmentation fault      (core dumped) mpirun -np $NSLOTS ./hello_openmpi.exe<br>
&gt;<br>
&gt;<br>
</div></div>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br>
</div></div></blockquote></div><br>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

