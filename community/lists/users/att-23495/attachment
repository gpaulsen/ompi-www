<div dir="ltr">Sorry typo. I have dual X5660 not X5560. <a href="http://ark.intel.com/products/47921/Intel-Xeon-Processor-X5660-12M-Cache-2_80-GHz-6_40-GTs-Intel-QPI?q=x5660">http://ark.intel.com/products/47921/Intel-Xeon-Processor-X5660-12M-Cache-2_80-GHz-6_40-GTs-Intel-QPI?q=x5660</a></div>
<div class="gmail_extra"><br><br><div class="gmail_quote">On 29 January 2014 21:02, Reuti <span dir="ltr">&lt;<a href="mailto:reuti@staff.uni-marburg.de" target="_blank">reuti@staff.uni-marburg.de</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div class="im">Quoting Victor &lt;<a href="mailto:victor.major@gmail.com" target="_blank">victor.major@gmail.com</a>&gt;:<br>

<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Thanks for the reply Reuti,<br>
<br>
There are two machines: Node1 with 12 physical cores (dual 6 core Xeon) and<br>
</blockquote>
<br></div>
Do you have this CPU?<br>
<br>
<a href="http://ark.intel.com/de/products/37109/Intel-Xeon-Processor-X5560-8M-Cache-2_80-GHz-6_40-GTs-Intel-QPI" target="_blank">http://ark.intel.com/de/<u></u>products/37109/Intel-Xeon-<u></u>Processor-X5560-8M-Cache-2_80-<u></u>GHz-6_40-GTs-Intel-QPI</a><span class="HOEnZb"><font color="#888888"><br>

<br>
-- Reuti</font></span><div class="HOEnZb"><div class="h5"><br>
<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Node2 with 4 physical cores (i5-2400).<br>
<br>
Regarding scaling on the single 12 core node, not it is also not linear. In<br>
fact it is downright strange. I do not remember the numbers right now but<br>
10 jobs are faster than 11 and 12 are the fastest with peak performance of<br>
approximately 66 Msu/s which is also far from triple the 4 core<br>
performance. This odd non-linear behaviour also happens at the lower job<br>
counts on that 12 core node. I understand the decrease in scaling with<br>
increase in core count on the single node as the memory bandwidth is an<br>
issue.<br>
<br>
On the 4 core machine the scaling is progressive, ie. every additional job<br>
brings an increase in performance. Single core delivers 8.1 Msu/s while 4<br>
cores deliver 30.8 Msu/s. This is almost linear.<br>
<br>
Since my original email I have also installed Open-MX and recompiled<br>
OpenMPI to use it. This has resulted in approximately 10% better<br>
performance using the existing GbE hardware.<br>
<br>
<br>
On 29 January 2014 19:40, Reuti &lt;<a href="mailto:reuti@staff.uni-marburg.de" target="_blank">reuti@staff.uni-marburg.de</a>&gt; wrote:<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Am 29.01.2014 um 03:00 schrieb Victor:<br>
<br>
&gt; I am running a CFD simulation benchmark cavity3d available within<br>
<a href="http://www.palabos.org/images/palabos_releases/palabos-v1.4r1.tgz" target="_blank">http://www.palabos.org/images/<u></u>palabos_releases/palabos-v1.<u></u>4r1.tgz</a><br>
&gt;<br>
&gt; It is a parallel friendly Lattice Botlzmann solver library.<br>
&gt;<br>
&gt; Palabos provides benchmark results for the cavity3d on several different<br>
platforms and variables here:<br>
<a href="http://wiki.palabos.org/plb_wiki:benchmark:cavity_n400" target="_blank">http://wiki.palabos.org/plb_<u></u>wiki:benchmark:cavity_n400</a><br>
&gt;<br>
&gt; The problem that I have is that the benchmark performance on my cluster<br>
does not scale even close to a linear scale.<br>
&gt;<br>
&gt; My cluster configuration:<br>
&gt;<br>
&gt; Node1: Dual Xeon 5560 48 Gb RAM<br>
&gt; Node2: i5-2400 24 Gb RAM<br>
&gt;<br>
&gt; Gigabit ethernet connection on eth0<br>
&gt;<br>
&gt; OpenMPI 1.6.5 on Ubuntu 12.04.3<br>
&gt;<br>
&gt;<br>
&gt; Hostfile:<br>
&gt;<br>
&gt; Node1 -slots=4 -max-slots=4<br>
&gt; Node2 -slots=4 -max-slots=4<br>
&gt;<br>
&gt; MPI command: mpirun --mca btl_tcp_if_include eth0 --hostfile<br>
/home/mpiuser/.mpi_hostfile -np 8 ./cavity3d 400<br>
&gt;<br>
&gt; Problem:<br>
&gt;<br>
&gt; cavity3d 400<br>
&gt;<br>
&gt; When I run mpirun -np 4 on Node1 I get 35.7615 Mega site updates per<br>
second<br>
&gt; When I run mpirun -np 4 on Node2 I get 30.7972 Mega site updates per<br>
second<br>
&gt; When I run mpirun --mca btl_tcp_if_include eth0 --hostfile<br>
/home/mpiuser/.mpi_hostfile -np 8 ./cavity3d 400 I get  47.3538 Mega site<br>
updates per second<br>
&gt;<br>
&gt; I understand that there are latencies with GbE and that there is MPI<br>
overhead, but this performance scaling still seems very poor. Are my<br>
expectations of scaling naive, or is there actually something wrong and<br>
fixable that will improve the scaling? Optimistically I would like each<br>
node to add to the cluster performance, not slow it down.<br>
&gt;<br>
&gt; Things get even worse if I run asymmetric number of mpi jobs in each<br>
node. For instance running -np 12 on Node1<br>
<br>
Isn&#39;t this overloading the machine with only 8 real cores in total?<br>
<br>
<br>
&gt; is significantly faster than running -np 16 across Node1 and Node2, thus<br>
adding Node2 actually slows down the performance.<br>
<br>
The i5-2400 has only 4 cores and no threads.<br>
<br>
It depends on the algorithm how much data has to be exchanged between the<br>
processes, and this can indeed be worse when used across a network.<br>
<br>
Also: is the algorithm scaling linear when used on node1 only with 8<br>
cores? When it&#39;s &quot;35.7615 &quot; with 4 cores, what result do you get with 8<br>
cores on this machine.<br>
<br>
-- Reuti<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
<br>
</blockquote></blockquote>
<br>
<br>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div>

