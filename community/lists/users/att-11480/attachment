<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">Thanks Ashley, &nbsp;I'll try your tool..<div><br></div><div>I would think that this is an error in the programs I am trying to use, too, but this is a problem with 2 different programs, written by 2 different groups.. One of them might be bad, but both.. seems unlikely.&nbsp;</div><div><br></div><div><div>Interestingly the results for the connectivity_c test that is included with OMPI... works fine with -np &lt;8. For -np &gt;8 it works some of the time, other times it HANGS. I have got to believe that this is a big clue!! Also, when it hangs, sometimes I get the message "mpirun was unable to cleanly terminate the daemons on the nodes shown below" Note that NO nodes are shown below. &nbsp; Once, I got -np 250 to pass the connectivity test, but I was not able to replicate this reliable, so I'm not sure if it was a fluke, or what. &nbsp;Here is a like to a screenshop of TOP when connectivity_c is hung with -np 14.. I see that 2 processes are only at 50% CPU usage.. Hmmmm &nbsp;</div><div><br></div><div><a href="http://picasaweb.google.com/lh/photo/87zVEucBNFaQ0TieNVZtdw?authkey=Gv1sRgCLKokNOVqo7BYw&amp;feat=directlink" target="_blank">http://picasaweb.google.com/<wbr>lh/photo/<wbr>87zVEucBNFaQ0TieNVZtdw?<wbr>authkey=Gv1sRgCLKokNOVqo7BYw&amp;<wbr>feat=directlink</a></div><div><br></div><div>The other tests, ring_c, hello_c, as well as the cxx versions of these guys with with all values of -np.</div><div><br></div></div><div>Unfortunately, I could not get valgrind to work...</div><div><br></div><div>Thanks, Matt</div><div><br></div><div><br></div><div><br><div><div><div>On Dec 9, 2009, at 2:37 AM, Ashley Pittman wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div>On Tue, 2009-12-08 at 08:30 -0800, Matthew MacManes wrote:<br><blockquote type="cite">There are 8 physical cores, or 16 with hyperthreading enabled. <br></blockquote><br>That should be meaty enough.<br><br><blockquote type="cite">1st of all, let me say that when I specify that -np is less than 4<br></blockquote><blockquote type="cite">processors (1, 2, or 3), both programs seem to work as expected. Also,<br></blockquote><blockquote type="cite">the non-mpi version of each of them works fine.<br></blockquote><br>Presumably the non-mpi version is serial however? this this doesn't mean<br>the program is bug-free or that the parallel version isn't broken.<br>There are any number of apps that don't work above N processes, in fact<br>probably all programs break for some value of N, it's normally a little<br>higher then 3 however.<br><br><blockquote type="cite">Thus, I am pretty sure that this is a problem with MPI rather that<br></blockquote><blockquote type="cite">with the program code or something else. &nbsp;<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">What happens is simply that the program hangs..<br></blockquote><br>I presume you mean here the output stops? &nbsp;The program continues to use<br>CPU cycles but no longer appears to make any progress?<br><br>I'm of the opinion that this is most likely a error in your program, I<br>would start by using either valgrind or padb.<br><br>You can run the app under valgrind using the following mpirun options,<br>this will give you four files named v.log.0 to v.log.3 which you can<br>check for errors in the normal way. &nbsp;The "--mca btl tcp,self" option<br>will disable shared memory which can create false positives.<br><br>mpirun -n 4 --mca btl tcp,self valgrind --log-file=v.log.%<br>q{OMPI_COMM_WORLD_RANK} &lt;app&gt;<br><br>Alternatively you can run the application, wait for it to hang and then<br>in another window run my tool, padb, which will show you the MPI message<br>queues and stack traces which should show you where it's hung,<br>instructions and sample output are on this page.<br><br><a href="http://padb.pittman.org.uk/full-report.html">http://padb.pittman.org.uk/full-report.html</a><br><br><blockquote type="cite">There are no error messages, and there is no clue from anything else<br></blockquote><blockquote type="cite">(system working fine otherwise- no RAM issues, etc). It does not hang<br></blockquote><blockquote type="cite">at the same place everytime, sometimes in the very beginning, sometime<br></blockquote><blockquote type="cite">near the middle.. &nbsp;<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">Could this an issue with hyperthreading? A conflict with something?<br></blockquote><br>Unlikely, if there was a problem in OMPI running more than 3 processes<br>it would have been found by now. &nbsp;I regularly run 8 process applications<br>on my dual-core netbook alongside all my desktop processes without<br>issue, it runs fine, a little slowly but fine.<br><br>All this talk about binding and affinity won't help either, process<br>binding is about squeezing the last 15% of performance out of a system<br>and making performance reproducible, it has no bearing on correctness or<br>scalability. &nbsp;If you're not running on a dedicated machine which with<br>firefox running I guess you aren't then there would be a good case for<br>leaving it off anyway.<br><br>Ashley,<br><br>-- <br><br>Ashley Pittman, Bath, UK.<br><br>Padb - A parallel job inspection tool for cluster computing<br>http://padb.pittman.org.uk<br><br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></div></blockquote></div><br><div>
<span class="Apple-style-span" style="border-collapse: separate; color: rgb(0, 0, 0); font-family: Optima; font-size: medium; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; "><span class="Apple-style-span" style="border-collapse: separate; color: rgb(0, 0, 0); font-family: Optima; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; "><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><div><div>_________________________________<br>Matthew MacManes<br>PhD Candidate<br>University of California- Berkeley<br>Museum of Vertebrate Zoology<br>Phone: 510-495-5833<br>Lab Website:&nbsp;<a href="http://ib.berkeley.edu/labs/lacey">http://ib.berkeley.edu/labs/lacey</a><br>Personal Website: <a href="http://macmanes.com/">http://macmanes.com/</a></div><div><br></div></div><br></div></span><br class="Apple-interchange-newline"></span><br class="Apple-interchange-newline">
</div>
<br></div></div></body></html>
