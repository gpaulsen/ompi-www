<html xmlns:v="urn:schemas-microsoft-com:vml" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:w="urn:schemas-microsoft-com:office:word" xmlns:m="http://schemas.microsoft.com/office/2004/12/omml" xmlns="http://www.w3.org/TR/REC-html40">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="Generator" content="Microsoft Word 14 (filtered medium)">
<style><!--
/* Font Definitions */
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:Tahoma;
	panose-1:2 11 6 4 3 5 4 4 2 4;}
/* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin:0in;
	margin-bottom:.0001pt;
	font-size:12.0pt;
	font-family:"Times New Roman","serif";}
a:link, span.MsoHyperlink
	{mso-style-priority:99;
	color:blue;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{mso-style-priority:99;
	color:purple;
	text-decoration:underline;}
p.MsoAcetate, li.MsoAcetate, div.MsoAcetate
	{mso-style-priority:99;
	mso-style-link:"Balloon Text Char";
	margin:0in;
	margin-bottom:.0001pt;
	font-size:8.0pt;
	font-family:"Tahoma","sans-serif";}
span.EmailStyle17
	{mso-style-type:personal-reply;
	font-family:"Calibri","sans-serif";
	color:#1F497D;}
span.BalloonTextChar
	{mso-style-name:"Balloon Text Char";
	mso-style-priority:99;
	mso-style-link:"Balloon Text";
	font-family:"Tahoma","sans-serif";}
.MsoChpDefault
	{mso-style-type:export-only;
	font-family:"Calibri","sans-serif";}
@page WordSection1
	{size:8.5in 11.0in;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
	{page:WordSection1;}
--></style><!--[if gte mso 9]><xml>
<o:shapedefaults v:ext="edit" spidmax="1026" />
</xml><![endif]--><!--[if gte mso 9]><xml>
<o:shapelayout v:ext="edit">
<o:idmap v:ext="edit" data="1" />
</o:shapelayout></xml><![endif]-->
</head>
<body lang="EN-US" link="blue" vlink="purple">
<div class="WordSection1">
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D">Hi Geoff:<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D"><o:p>&nbsp;</o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D">Our original implementation used cuMemcpy for copying GPU memory into and out of host memory.&nbsp; However, what we learned is that the cuMemcpy causes a synchronization
 for all work on the GPU.&nbsp; This means that one could not overlap very well running a kernel and doing communication.&nbsp; So, now we create an internal stream and then use that along with cuMemcpyAsync/cuStreamSynchronize for doing the copy.<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D"><o:p>&nbsp;</o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D">In turns out in Jeremia’s case, he wanted to have a long running kernel and he wanted the MPI_Send/MPI_Recv to happen at the same time.&nbsp; With the use of cuMemcpy,
 the MPI library was waiting for his kernel to complete before doing the cuMemcpy.<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D"><o:p>&nbsp;</o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D">Rolf<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D"><o:p>&nbsp;</o:p></span></p>
<div style="border:none;border-left:solid blue 1.5pt;padding:0in 0in 0in 4.0pt">
<div>
<div style="border:none;border-top:solid #B5C4DF 1.0pt;padding:3.0pt 0in 0in 0in">
<p class="MsoNormal"><b><span style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;">From:</span></b><span style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;"> users [mailto:users-bounces@open-mpi.org]
<b>On Behalf Of </b>Geoffrey Paulsen<br>
<b>Sent:</b> Wednesday, August 12, 2015 12:55 PM<br>
<b>To:</b> users@open-mpi.org<br>
<b>Cc:</b> users@open-mpi.org; Sameh S Sharkawi<br>
<b>Subject:</b> Re: [OMPI users] CUDA Buffers: Enforce asynchronous memcpy's<o:p></o:p></span></p>
</div>
</div>
<p class="MsoNormal"><o:p>&nbsp;</o:p></p>
<div>
<div>
<p class="MsoNormal"><span style="font-size:10.5pt;font-family:&quot;Arial&quot;,&quot;sans-serif&quot;">I'm confused why this application needs an asynchronous cuMemcpyAsync()in a blocking MPI call.&nbsp;&nbsp; Rolf could you please explain?<br>
<br>
And how does is a call to cuMemcpyAsync() followed by a syncronization any different than a cuMemcpy() in this use case?
<o:p></o:p></span></p>
<div>
<p class="MsoNormal"><span style="font-size:10.5pt;font-family:&quot;Arial&quot;,&quot;sans-serif&quot;">&nbsp;<o:p></o:p></span></p>
</div>
<div>
<p class="MsoNormal"><span style="font-size:10.5pt;font-family:&quot;Arial&quot;,&quot;sans-serif&quot;">I would still expect that if the MPI_Send / Recv call issued the cuMemcpyAsync() that it would be MPI's responsibility to issue the synchronization call as well.<o:p></o:p></span></p>
</div>
<p class="MsoNormal"><span style="font-size:10.5pt;font-family:&quot;Arial&quot;,&quot;sans-serif&quot;">&nbsp;
<o:p></o:p></span></p>
<div>
<p class="MsoNormal"><span style="font-size:10.5pt;font-family:&quot;Arial&quot;,&quot;sans-serif&quot;">&nbsp;<o:p></o:p></span></p>
</div>
<div>
<p class="MsoNormal"><span style="font-size:10.5pt;font-family:&quot;Arial&quot;,&quot;sans-serif&quot;"><br>
---<br>
Geoffrey Paulsen<br>
Software Engineer, IBM Platform MPI<br>
IBM Platform-MPI<br>
Phone: 720-349-2832<br>
Email: <a href="mailto:gpaulsen@us.ibm.com">gpaulsen@us.ibm.com</a><br>
<a href="http://www.ibm.com">www.ibm.com</a> <o:p></o:p></span></p>
<div>
<p class="MsoNormal"><span style="font-size:10.5pt;font-family:&quot;Arial&quot;,&quot;sans-serif&quot;">&nbsp;<o:p></o:p></span></p>
</div>
<div>
<p class="MsoNormal"><span style="font-size:10.5pt;font-family:&quot;Arial&quot;,&quot;sans-serif&quot;">&nbsp;<o:p></o:p></span></p>
</div>
<blockquote style="border:none;border-left:solid #AAAAAA 1.5pt;padding:0in 0in 0in 4.0pt;margin-left:3.75pt;margin-top:5.0pt;margin-bottom:5.0pt">
<p class="MsoNormal"><span style="font-size:10.5pt;font-family:&quot;Arial&quot;,&quot;sans-serif&quot;">----- Original message -----<br>
From: Rolf vandeVaart &lt;<a href="mailto:rvandevaart@nvidia.com">rvandevaart@nvidia.com</a>&gt;<br>
Sent by: &quot;users&quot; &lt;<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>&gt;<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Cc:<br>
Subject: Re: [OMPI users] CUDA Buffers: Enforce asynchronous memcpy's<br>
Date: Tue, Aug 11, 2015 1:45 PM<br>
&nbsp; <o:p></o:p></span></p>
<div>
<p class="MsoNormal"><span style="font-size:10.0pt;font-family:&quot;Courier New&quot;">I talked with Jeremia off list and we figured out what was going on. &nbsp;There is the ability to use the cuMemcpyAsync/cuStreamSynchronize rather than the cuMemcpy but it was never made
 the default for Open MPI 1.8 series. &nbsp;So, to get that behavior you need the following:<br>
<br>
--mca mpi_common_cuda_cumemcpy_async 1<br>
<br>
It is too late to change this in 1.8 but it will be made the default behavior in 1.10 and all future versions. &nbsp;In addition, he is right about not being able to see these variables in the Open MPI 1.8 series. &nbsp;This was a bug and it has been fixed in Open MPI
 v2.0.0. &nbsp;Currently, there are no plans to bring that back into 1.10.<br>
<br>
Rolf<br>
<br>
&gt;-----Original Message-----<br>
&gt;From: users [<a href="mailto:users-bounces@open-mpi.org" target="_blank">mailto:users-bounces@open-mpi.org</a>] On Behalf Of Jeremia Bär<br>
&gt;Sent: Tuesday, August 11, 2015 9:17 AM<br>
&gt;To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;Subject: [OMPI users] CUDA Buffers: Enforce asynchronous memcpy's<br>
&gt;<br>
&gt;Hi!<br>
&gt;<br>
&gt;In my current application, MPI_Send/MPI_Recv hangs when using buffers in<br>
&gt;GPU device memory of a Nvidia GPU. I realized this is due to the fact that<br>
&gt;OpenMPI uses the synchronous cuMempcy rather than the asynchornous<br>
&gt;cuMemcpyAsync (see stacktrace at the bottom). However, in my application,<br>
&gt;synchronous copies cannot be used.<br>
&gt;<br>
&gt;I scanned through the source and saw support for async memcpy's are<br>
&gt;available. It's controlled by 'mca_common_cuda_cumemcpy_async' in<br>
&gt;./ompi/mca/common/cuda/common_cuda.c<br>
&gt;However, I can't find a way to enable it. It's not exposed in 'ompi_info' (but<br>
&gt;registered?). How can I enforce the use of cuMemcpyAsync in OpenMPI?<br>
&gt;Version used is OpenMPI 1.8.5.<br>
&gt;<br>
&gt;Thank you,<br>
&gt;Jeremia<br>
&gt;<br>
&gt;(gdb) bt<br>
&gt;#0 &nbsp;0x00002aaaaaaaba11 in clock_gettime ()<br>
&gt;#1 &nbsp;0x00000039e5803e46 in clock_gettime () from /lib64/librt.so.1<br>
&gt;#2 &nbsp;0x00002aaaab58a7ae in ?? () from /usr/lib64/libcuda.so.1<br>
&gt;#3 &nbsp;0x00002aaaaaf41dfb in ?? () from /usr/lib64/libcuda.so.1<br>
&gt;#4 &nbsp;0x00002aaaaaf1f623 in ?? () from /usr/lib64/libcuda.so.1<br>
&gt;#5 &nbsp;0x00002aaaaaf17361 in ?? () from /usr/lib64/libcuda.so.1<br>
&gt;#6 &nbsp;0x00002aaaaaf180b6 in ?? () from /usr/lib64/libcuda.so.1<br>
&gt;#7 &nbsp;0x00002aaaaae860c2 in ?? () from /usr/lib64/libcuda.so.1<br>
&gt;#8 &nbsp;0x00002aaaaae8621a in ?? () from /usr/lib64/libcuda.so.1<br>
&gt;#9 &nbsp;0x00002aaaaae69d85 in cuMemcpy () from /usr/lib64/libcuda.so.1<br>
&gt;#10 0x00002aaaaf0a7dea in mca_common_cuda_cu_memcpy () from<br>
&gt;/home/jbaer/local_root/opt/openmpi_from_src_1.8.5/lib/libmca_common_c<br>
&gt;uda.so.1<br>
&gt;#11 0x00002aaaac992544 in opal_cuda_memcpy () from<br>
&gt;/home/jbaer/local_root/opt/openmpi_from_src_1.8.5/lib/libopen-pal.so.6<br>
&gt;#12 0x00002aaaac98adf7 in opal_convertor_pack () from<br>
&gt;/home/jbaer/local_root/opt/openmpi_from_src_1.8.5/lib/libopen-pal.so.6<br>
&gt;#13 0x00002aaab167c611 in mca_pml_ob1_send_request_start_copy () from<br>
&gt;/home/jbaer/local_root/opt/openmpi_from_src_1.8.5/lib/openmpi/mca_pm<br>
&gt;l_ob1.so<br>
&gt;#14 0x00002aaab167353f in mca_pml_ob1_send () from<br>
&gt;/home/jbaer/local_root/opt/openmpi_from_src_1.8.5/lib/openmpi/mca_pm<br>
&gt;l_ob1.so<br>
&gt;#15 0x00002aaaabf4f322 in PMPI_Send () from<br>
&gt;/users/jbaer/local_root/opt/openmpi_from_src_1.8.5/lib/libmpi.so.1<br>
&gt;<br>
&gt;_______________________________________________<br>
&gt;users mailing list<br>
&gt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;Link to this post: <a href="http://www.open-" target="_blank">http://www.open-</a><br>
&gt;mpi.org/community/lists/users/2015/08/27424.php<br>
-----------------------------------------------------------------------------------<br>
This email message is for the sole use of the intended recipient(s) and may contain<br>
confidential information. &nbsp;Any unauthorized review, use, disclosure or distribution<br>
is prohibited. &nbsp;If you are not the intended recipient, please contact the sender by<br>
reply email and destroy all copies of the original message.<br>
-----------------------------------------------------------------------------------<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/08/27431.php" target="_blank">
http://www.open-mpi.org/community/lists/users/2015/08/27431.php</a></span><span style="font-size:10.5pt;font-family:&quot;Arial&quot;,&quot;sans-serif&quot;"><br>
&nbsp;<o:p></o:p></span></p>
</div>
</blockquote>
</div>
</div>
</div>
<p class="MsoNormal"><o:p>&nbsp;</o:p></p>
</div>
</div>
</body>
</html>
