Greetings,<br><br>I was hoping someone could help me with the following situation.&nbsp;&nbsp;I have a program which has no MPI support that I&#39;d like to run &quot;in parallel&quot; by running a portion of my total task on N CPUs of a PBS/Maui/Open-MPI cluster.&nbsp;&nbsp;(The algorithm is such that there is no real need for MPI, I am just as well-off running N processes on N CPUs as I would be adding MPI support to my program and then running on N CPUs.)
<br><br>So it&#39;s easy enough to set up a Perl script to submit N jobs to the queue to run on N nodes.&nbsp;&nbsp;But, my cluster has two CPUs per node, and I am not RAM-limited, so I&#39;d like to run two serial jobs per node, one on each node CPU.&nbsp;&nbsp;From what my admin tells me, I must use the mpiexec command to run my program so that the scheduler knows to run my program on the nodes which it has assigned to me.
<br><br>In my PBS script (this is one of N/2 similar scripts),<br><br>#!/bin/bash<br>#PBS -l nodes=1:ppn=2<br>#PBS -l walltime=1:00:00:00<br>mpiexec -pernode program-executable&lt;inputfile1&gt;outputfile1<br>mpiexec -pernode program-executable&lt;inputfile2&gt;outputfile2
<br><br>does not have the desired effect.&nbsp; It appears that (1) the second process waits for the first to finish, and (2) MPI or the scheduler (I can&#39;t tell which) tries to re-start the program a few times (you can see this in the output files).&nbsp;&nbsp;Adding an ampersand to the first mpiexec line appears to cause mpiexec to crash and the job does not run at all.&nbsp; Using:
<br><br>mpiexec -np 1 program-executable&lt;inputfile&gt;outputfile<br><br>avoids the strange re-start problem I mentioned above, but of course does not use both CPUs on a node.<br><br><br>Maybe I am making a simple mistake, but I am quite new to cluster computing...&nbsp; Any help you can offer is greatly appreciated!
<br><br><br>Thanks,<br><br>--John Borchardt<br><br><br><br>

