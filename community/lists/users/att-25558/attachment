<div dir="ltr">HI, Thanks for your reply:)<div>I really run an MPI program (compile with OpenMPI and run with &quot;mpirun -n 8 ......&quot;). My OpenMPI version is 1.8.3 and my program is Gromacs. BTW, what is <span style="font-family:arial,sans-serif;font-size:13px">OSHMEM</span><span style="font-family:arial,sans-serif;font-size:13px"> ?</span></div><div><span style="font-family:arial,sans-serif;font-size:13px"><br></span></div><div><span style="font-family:arial,sans-serif;font-size:13px">Best</span></div><div><span style="font-family:arial,sans-serif;font-size:13px">Vincent</span></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Thu, Oct 23, 2014 at 12:21 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">From your error message, I gather you are not running an MPI program, but rather an OSHMEM one? Otherwise, I find the message strange as it only would be emitted from an OSHMEM program.<div><br></div><div>What version of OMPI are you trying to use?</div><div><br></div><div><div><blockquote type="cite"><div><div class="h5"><div>On Oct 22, 2014, at 7:12 PM, Vinson Leung &lt;<a href="mailto:lwhvinson1990@gmail.com" target="_blank">lwhvinson1990@gmail.com</a>&gt; wrote:</div><br></div></div><div><div><div class="h5"><div dir="ltr"><span style="font-family:arial,sans-serif;font-size:13px">Thanks for your reply:)</span><div style="font-family:arial,sans-serif;font-size:13px">Follow your advice I tried to set the TMPDIR to /var/tmp and /dev/shm and even reset to /tmp (I get the system permission), the problem still occur (CPU utilization still lower than 20%). I have no idea why and ready to give up OpenMPI instead of using other MPI library.</div><div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">--------Old Message-------------</div><div style="font-family:arial,sans-serif;font-size:13px"><br>Date: Tue, 21 Oct 2014 22:21:31 -0400<br>From: Brock Palen &lt;<a href="mailto:brockp@umich.edu" target="_blank">brockp@umich.edu</a>&gt;<br>To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>Subject: Re: [OMPI users] low CPU utilization with OpenMPI<br>Message-ID: &lt;<a href="mailto:CC54135D-0CFE-440A-8DF2-06B587E172D2@umich.edu" target="_blank">CC54135D-0CFE-440A-8DF2-06B587E172D2@umich.edu</a>&gt;<br>Content-Type: text/plain; charset=us-ascii<br><br>Doing special files on NFS can be weird,  try the other /tmp/ locations:<br><br>/var/tmp/<br>/dev/shm  (ram disk careful!)<br><br>Brock Palen<br><a href="http://www.umich.edu/~brockp" target="_blank">www.umich.edu/~brockp</a><br>CAEN Advanced Computing<br>XSEDE Campus Champion<br><a href="mailto:brockp@umich.edu" target="_blank">brockp@umich.edu</a><br>(734)936-1985<br><br><br><br>&gt; On Oct 21, 2014, at 10:18 PM, Vinson Leung &lt;<a href="mailto:lwhvinson1990@gmail.com" target="_blank">lwhvinson1990@gmail.com</a>&gt; wrote:<br>&gt;<br>&gt; Because of permission reason (OpenMPI can not write temporary file to the default /tmp directory), I change the TMPDIR to my local directory (export TMPDIR=/home/user/tmp ) and then the MPI program can run. But the CPU utilization is very low under 20% (8 MPI rank running in Intel Xeon 8-core CPU).<br>&gt;<br>&gt; And I also got some message when I run with OpenMPI:<br>&gt; [cn3:28072] 9 more processes have sent help message help-opal-shmem-mmap.txt / mmap on nfs<br>&gt; [cn3:28072] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>&gt;<br>&gt; Any idea?<br>&gt; Thanks<br>&gt;<br>&gt; VIncent<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/10/25548.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/10/25548.php</a></div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/10/25555.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/10/25555.php</a></div></blockquote></div><br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/10/25556.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/10/25556.php</a><br></blockquote></div><br></div>

