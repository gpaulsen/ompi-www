
<br><font size=2 face="sans-serif">As Rob mentions</font>
<br>
<br><font size=2 face="sans-serif">There are three capabilities to consider:</font>
<br>
<br><font size=2 face="sans-serif">1) The process (or processes) that will
do the I/O are members of the file handle's hidden communicator and the
call is collective</font>
<br>
<br><font size=2 face="sans-serif">2)) The process (or processes) that
will do the I/O are members of the file handle's hidden communicator but
the call is non-collective and made by a remote rank</font>
<br>
<br><font size=2 face="sans-serif">3) The process (or processes) that will
do the I/O are not members. &nbsp;The MPI_COMM_SELF mention would probably
be this second case.</font>
<br>
<br><font size=2 face="sans-serif">Number 2 &amp; 3 are harder but still
an implementation option. &nbsp;The standard does not require or prohibit
them.</font>
<br>
<br>
<br><font size=2 face="sans-serif">Dick Treumann &nbsp;- &nbsp;MPI Team
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br>
IBM Systems &amp; Technology Group<br>
Dept X2ZA / MS P963 -- 2455 South Road -- Poughkeepsie, NY 12601<br>
Tele (845) 433-7846 &nbsp; &nbsp; &nbsp; &nbsp; Fax (845) 433-8363<br>
</font>
<br>
<br>
<br>
<table width=100%>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">From:</font>
<td><font size=1 face="sans-serif">Rob Latham &lt;robl@mcs.anl.gov&gt;</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">To:</font>
<td><font size=1 face="sans-serif">Open MPI Users &lt;users@open-mpi.org&gt;</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Date:</font>
<td><font size=1 face="sans-serif">10/19/2010 02:47 PM</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Subject:</font>
<td><font size=1 face="sans-serif">Re: [OMPI users] a question about [MPI]IO
on systems &nbsp; &nbsp; &nbsp; &nbsp;without network filesystem</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Sent by:</font>
<td><font size=1 face="sans-serif">users-bounces@open-mpi.org</font></table>
<br>
<hr noshade>
<br>
<br>
<br><tt><font size=2>On Thu, Sep 30, 2010 at 09:00:31AM -0400, Richard
Treumann wrote:<br>
&gt; It is possible for MPI-IO to be implemented in a way that lets a single
<br>
&gt; process or the set of process on a node act as the disk i/O agents
for the <br>
&gt; entire job but someone else will need to tell you if OpenMPI can do
this, <br>
&gt; I think OpenMPI built on the ROMIO MPI-IO implementation and based
on my <br>
&gt; outdated knowledge of ROMIO, I would be a bit surprised if it has
his <br>
&gt; option.<br>
<br>
SURPRISE!!! &nbsp;ROMIO has been able to do this since about 2002 (It was<br>
my first ROMIO project when I came to Argonne).<br>
<br>
now, if you do independent i/o or you do i/o on comm_self, then ROMIO<br>
can't really do anything for you. &nbsp;<br>
<br>
But... <br>
- if you use collective I/O <br>
- and you set the &quot;cb_config_list&quot; to contain the machine name
of the<br>
 &nbsp;one node with a disk (or if everyone has a disk, pick one to be
the<br>
 &nbsp;master)<br>
- and you set &quot;romio_no_indep_rw&quot; to &quot;enable&quot;<br>
<br>
then two things will happen. &nbsp;first, ROMIO will enter &quot;deferred
open&quot;<br>
mode, meaning only the designated I/O aggregators will open the file.<br>
second, your collective MPI_File_*_all calls will all go through the<br>
one node you gave in the cb_config_list.<br>
<br>
Try it and if it does/doesn't work, I'd like to hear. &nbsp;<br>
<br>
==rob<br>
<br>
-- <br>
Rob Latham<br>
Mathematics and Computer Science Division<br>
Argonne National Lab, IL USA<br>
_______________________________________________<br>
users mailing list<br>
users@open-mpi.org<br>
</font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
</font></tt>
<br>
<br>
