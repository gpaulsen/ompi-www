<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><br><div><div>On Jul 30, 2009, at 4:36 PM, Adams, Brian M wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"> <div style="WORD-WRAP: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space"> <div dir="ltr" align="left"><font face="Arial" color="#0000ff" size="2">I found the manual pages for mpirun and orte_hosts, which have a pretty thorough description of these features.&nbsp; Let me know if there's anything else I should check out.</font></div> <div><font face="Arial" color="#0000ff" size="2"></font>&nbsp;</div> <div dir="ltr" align="left"><font face="Arial" color="#0000ff" size="2">My quick impression is that this will meet at least 90% of user needs out of the box as most (all?) users will run with number of job processors that divides into PPN or is a multiple of PPN.&nbsp; </font></div> <div><br><font face="Arial" color="#0000ff" size="2">The docs imply that the relative indexing is relative to nodes as opposed to slots.&nbsp; If so, it's easy to cover my cases 1 and 2 with modulo arithmetic right on the command line, but the edge case 3 might require creation of host files or a maps file, specifying specific nodes and slots.&nbsp; I'm still trying to fully wrap my head around the way the scheduling works and the nodes/slots distinction, so have to think on this.&nbsp; For example, in a PBS/Torque environment, I guess each processor gets an entry as a "node" in the $PBS_NODEFILE, so maybe this won't be an issue for many of our machines.</font></div></div></blockquote><div><br></div>Just as any FYI. The PBS_NODEFILE contains one entry for each -slot- on a node. So if you have 4 slots on a node, the node's name appears in the file 4 times (repeated sequentially). When we read it to compute #slots available, we collapse that info. If you give it to us as the sequential mapper file, we will map ranks n to n+4 on that node.</div><div><br><blockquote type="cite"><div style="WORD-WRAP: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space"> <div><font face="Arial" color="#0000ff" size="2"></font>&nbsp;</div> <div dir="ltr" align="left"><font face="Arial" color="#0000ff" size="2">Another potential gotcha is the scheduler driving my M scheduler processes ideally runs asynchronously (since not all compute jobs take equal time as parameters vary), so initially jobs 0--M could be on resource blocks 0--M in order, but if job 2 finishes first, I'd want to put job M+1 on block 2.&nbsp; So I might end up needing some glue code anyway for the dynamic scheduling case.&nbsp; What's in ORTE today will<span class="157093522-30072009"> likely</span>&nbsp;work great in the static scheduling case.</font></div></div></blockquote><div><br></div>I'll give some more thought to the dynamic scheduling case. Like I said, this has come up here at LANL too, so it is a common problem between us. Something else you might want to consider in the interim. There is a "tools" interface in the ORTE library that provides simple commands to (a) find out all the mpirun instances currently running, and (b) what nodes and processes they are running. It isn't quite as clean for what you are trying to do since it was intended mostly for determining status, but you could parse the info to get what you need.</div><div><br></div><div>Might make that glue easier, at the least.</div><div><br></div><div>I'll get back to you with some ideas on how ORTE could do this natively. Part of the issue, of course, is figuring out a cmd syntax that isn't too burdensome :-)</div><div><br><blockquote type="cite"><div style="WORD-WRAP: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space"> <div><font face="Arial" color="#0000ff" size="2"></font>&nbsp;</div> <div dir="ltr" align="left"><font face="Arial" color="#0000ff" size="2">Brian</font></div> <div><font face="Arial" color="#0000ff" size="2"></font>&nbsp;</div><br> <blockquote dir="ltr" style="PADDING-LEFT: 5px; MARGIN-LEFT: 5px; BORDER-LEFT: #0000ff 2px solid; MARGIN-RIGHT: 0px">  <div class="OutlookMessageHeader" lang="en-us" dir="ltr" align="left">  <hr tabindex="-1">  <font face="Tahoma" size="2"><b>From:</b> users-bounces@open-mpi.org   [<a href="mailto:users-bounces@open-mpi.org">mailto:users-bounces@open-mpi.org</a>] <b>On Behalf Of </b>Ralph   Castain<br><b>Sent:</b> Thursday, July 30, 2009 2:08 PM<br><b>To:</b> Open MPI   Users<br><b>Subject:</b> Re: [OMPI users] Multiple mpiexec's within a job   (schedule within a scheduled machinefile/job allocation)<br></font><br></div>  <div></div>Let me know how it goes, if you don't mind. It would be nice to   know if we actually met your needs, or if a tweak might help make it easier.&nbsp;  <div><br></div>  <div>Thanks</div>  <div>Ralph</div>  <div><br>  <div>  <div>On Jul 30, 2009, at 1:36 PM, Adams, Brian M wrote:</div><br class="Apple-interchange-newline">  <blockquote type="cite">    <div style="WORD-WRAP: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space">    <div dir="ltr" align="left"><span class="915373419-30072009"><font face="Arial" color="#0000ff" size="2">Thanks Ralph, I wasn't aware of the relative indexing     or sequential mapper capabilities.&nbsp; I will check those out and report     back if I still have a feature request. -- Brian</font></span></div><br>    <blockquote style="PADDING-LEFT: 5px; MARGIN-LEFT: 5px; BORDER-LEFT: #0000ff 2px solid; MARGIN-RIGHT: 0px">      <div class="OutlookMessageHeader" lang="en-us" dir="ltr" align="left">      <hr tabindex="-1">      <font face="Tahoma" size="2"><b>From:</b> <a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a> [<a href="mailto:users-bounces@open-mpi.org">mailto:users-bounces@open-mpi.org</a>]       <b>On Behalf Of </b>Ralph Castain<br><b>Sent:</b> Thursday, July 30, 2009       12:26 PM<br><b>To:</b> Open MPI Users<br><b>Subject:</b> Re: [OMPI users]       Multiple mpiexec's within a job (schedule within a scheduled       machinefile/job allocation)<br></font><br></div>      <div></div><br>      <div>      <div>On Jul 30, 2009, at 11:49 AM, Adams, Brian M wrote:</div><br class="Apple-interchange-newline">      <blockquote type="cite">        <div>Apologies if I'm being confusing; I'm probably trying to get at         atypical use cases. &nbsp;M and N &nbsp;need not correspond to the         number of nodes/ppn nor ppn/nodes available. &nbsp;By node vs. slot         doesn't much matter, as long as in the end I don't oversubscribe any         node. &nbsp;By slot might be good for efficiency in some apps, but I         can't make a general case for it.<br><br>I think what you proposed         offers some help in the case where N is an integer multiple of the         number of available nodes, but perhaps not in other cases. &nbsp;I must         be missing something here, so instead of being fully general, perhaps         consider a &nbsp;specific case. &nbsp;Suppose we have 4 nodes, 8 ppn (32         slots is I think the ompi language). &nbsp;I might want to schedule, for         example<br><br>1. M=2 simultaneous N=16 processor jobs: Here I believe         what you suggested will work since N is a multiple of the available         number of nodes. &nbsp;I could use either npernode 4 or just bynode and         I think get the same result: an even distribution of tasks.         &nbsp;(similar applies to, e.g., 8x4, 4x8)<br></div></blockquote>      <div><br></div>Yes, agreed</div>      <div><br>      <blockquote type="cite">        <div><br>2. M=16 simultaneous N=2 processor jobs: it seems if I use         bynode or npernode, I would end up with 16 processes on each of the         first two nodes (similar applies to, e.g., 32x1 or 10x3).         &nbsp;Scheduling many small jobs is a common problem for       us.<br></div></blockquote></div>      <div>      <blockquote type="cite">        <div><font class="Apple-style-span" color="#000000"><br></font>3. M=3         simultaneous, N=10 processor jobs: I think we'd end up with this         distribution (where A-D are nodes and 0-2 jobs)<br><br>A 0 0 0 1 1 1 2 2         2<br>B 0 0 0 1 1 1 2 2 2<br>C 0 0 &nbsp;&nbsp;1 1 &nbsp;&nbsp;2 2<br>D 0         0 &nbsp;&nbsp;1 1 &nbsp;&nbsp;2 2<br><br>where A and B are         over-subscribed and there are more than the two unused slots I'd expect         in the whole allocation.<br><br>Again, I can manage all these via a         script that partitions the machine files, just wondering which scenarios         OpenMPI can manage.<br><br></div></blockquote><br>      <div>Have you looked at the relative indexing in 1.3.3? You could specify       any of these in relative index terms, and have one "hostfile" that would       support 16x2 operations. This would work then for any allocation.</div>      <div><br></div>      <div>Your launch script could even just do it, something like this:</div>      <div><br></div>      <div>mpirun -n 2 -host +n0:1,+n1:1 app</div>      <div>mpirun -n 2 -host +n0:2,+n1:2 app</div>      <div><br></div>      <div>etc. Obviously, you could compute the relative indexing and just       stick it in as required.</div>      <div><br></div>      <div>Likewise, you could use the new "seq" (sequential) mapper to achieve       any desired layout, again utilizing relative indexing to avoid having to       create a special hostfile for each run.</div>      <div><br></div>      <div>Note that in all cases, you can specify a -n N that will tell OMPI to       only execute N processes, regardless of what is in the sequential mapper       file or -host.</div>      <div><br></div>      <div>If none of those work well, please let me know. I'm happy to create       the required capability as I'm sure LANL will use it too (know of several       similar cases here, but the current options seem okay for them).</div>      <div><br>      <blockquote type="cite"></blockquote></div>      <blockquote type="cite">        <div>Thanks!<br>Brian<br><br>        <blockquote type="cite">-----Original Message-----<br></blockquote>        <blockquote type="cite">From: <a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>           <br></blockquote>        <blockquote type="cite">[<a href="mailto:users-bounces@open-mpi.org">mailto:users-bounces@open-mpi.org</a>]           On Behalf Of Ralph Castain<br></blockquote>        <blockquote type="cite">Sent: Wednesday, July 29, 2009 4:19         PM<br></blockquote>        <blockquote type="cite">To: Open MPI Users<br></blockquote>        <blockquote type="cite">Subject: Re: [OMPI users] Multiple mpiexec's           within a job <br></blockquote>        <blockquote type="cite">(schedule within a scheduled machinefile/job           allocation)<br></blockquote>        <blockquote type="cite"><br></blockquote>        <blockquote type="cite">Oh my - that does take me back a long way!           :-)<br></blockquote>        <blockquote type="cite"><br></blockquote>        <blockquote type="cite">Do you need these processes to be mapped           byslot (i.e., do you <br></blockquote>        <blockquote type="cite">care if the process ranks are sharing nodes)?           If not, why not <br></blockquote>        <blockquote type="cite">add "-bynode" to your cmd line?<br></blockquote>        <blockquote type="cite"><br></blockquote>        <blockquote type="cite">Alternatively, given the mapping you want,           just do<br></blockquote>        <blockquote type="cite"><br></blockquote>        <blockquote type="cite">mpirun -npernode 1         application.exe<br></blockquote>        <blockquote type="cite"><br></blockquote>        <blockquote type="cite">This would launch one copy on each of your N           nodes. So if you <br></blockquote>        <blockquote type="cite">fork M times, you'll wind up with the exact           pattern you <br></blockquote>        <blockquote type="cite">wanted. And, as each one exits, you could           immediately launch <br></blockquote>        <blockquote type="cite">a replacement without worrying about           oversubscription.<br></blockquote>        <blockquote type="cite"><br></blockquote>        <blockquote type="cite">Does that help?<br></blockquote>        <blockquote type="cite">Ralph<br></blockquote>        <blockquote type="cite"><br></blockquote>        <blockquote type="cite">PS. we dropped that "persistent" operation -           caused way too <br></blockquote>        <blockquote type="cite">many problems with cleanup and other things.           :-)<br></blockquote>        <blockquote type="cite"><br></blockquote>        <blockquote type="cite">On Jul 29, 2009, at 3:46 PM, Adams, Brian M           wrote:<br></blockquote>        <blockquote type="cite"><br></blockquote>        <blockquote type="cite">          <blockquote type="cite">Hi Ralph (all),<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite"><br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">I'm resurrecting this 2006 thread for a             status check. &nbsp;The <br></blockquote></blockquote>        <blockquote type="cite">new 1.3.x <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">machinefile behavior is great (thanks!) -- I             can use <br></blockquote></blockquote>        <blockquote type="cite">machinefiles to <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">manage multiple simultaneous mpiruns within             a single torque<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">allocation (where the hosts are a subset of             $PBS_NODEFILE). &nbsp;&nbsp;<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">However, this requires some careful             management of machinefiles.<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite"><br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">I'm curious if OpenMPI now directly supports             the behavior I need, <br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">described in general in the quote below.             &nbsp;Specifically, <br></blockquote></blockquote>        <blockquote type="cite">given a single <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">PBS/Torque allocation of M*N processors, I             will run a <br></blockquote></blockquote>        <blockquote type="cite">serial program <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">that will fork M times. &nbsp;Each of the M             forked processes<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">calls 'mpirun -np N application.exe' and             blocks until completion. &nbsp;&nbsp;<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">This seems akin to the case you described of             "mpiruns executed in <br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">separate         windows/prompts."<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite"><br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">What I'd like to see is the M processes             "tiled" across the <br></blockquote></blockquote>        <blockquote type="cite">available <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">slots, so all M*N processors are used.             &nbsp;What I see instead <br></blockquote></blockquote>        <blockquote type="cite">appears at <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">face value to be the first N resources being             oversubscribed M times.<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite"><br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">Also, when one of the forked processes             returns, I'd like to <br></blockquote></blockquote>        <blockquote type="cite">be able to <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">spawn another and have its mpirun schedule             on the resources <br></blockquote></blockquote>        <blockquote type="cite">freed by <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">the previous one that exited. &nbsp;Is any             of this possible?<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite"><br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">I tried starting an orted (1.3.3, roughly as             you suggested <br></blockquote></blockquote>        <blockquote type="cite">below), but <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">got this error:<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite"><br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">orted         --daemonize<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">[gy8:25871] [[INVALID],INVALID]             ORTE_ERROR_LOG: Not found in file <br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">runtime/orte_init.c at line           125<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite"><br></blockquote></blockquote>        <blockquote type="cite">----------------------------------------------------------------------<br></blockquote>        <blockquote type="cite">          <blockquote type="cite">---- It looks like orte_init failed for some             reason; your parallel <br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">process is likely to abort. &nbsp;There are             many reasons that a parallel <br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">process can fail during orte_init; some of             which are due to <br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">configuration or environment problems.             &nbsp;This failure <br></blockquote></blockquote>        <blockquote type="cite">appears to be an <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">internal failure; here's some additional             information (which <br></blockquote></blockquote>        <blockquote type="cite">may only <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">be relevant to an Open MPI           developer):<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite"><br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">orte_ess_base_select         failed<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">--&gt; Returned value Not found (-13)             instead of ORTE_SUCCESS<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite"><br></blockquote></blockquote>        <blockquote type="cite">----------------------------------------------------------------------<br></blockquote>        <blockquote type="cite">          <blockquote type="cite">---- [gy8:25871] [[INVALID],INVALID]             ORTE_ERROR_LOG: Not <br></blockquote></blockquote>        <blockquote type="cite">found in file <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">orted/orted_main.c at line         323<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite"><br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">I spared the debugging info as I'm not even             sure this is a correct <br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">invocation...<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite"><br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">Thanks for any suggestions you can           offer!<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">Brian<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">----------<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">Brian M. Adams, PhD (<a href="mailto:briadam@sandia.gov">briadam@sandia.gov</a>)             Optimization and <br></blockquote></blockquote>        <blockquote type="cite">Uncertainty <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">Quantification Sandia National Laboratories,             Albuquerque, NM <br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite"><a href="http://www.sandia.gov/~briadam">http://www.sandia.gov/~briadam</a><br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite"><br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite"><br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">From: Ralph Castain             (rhc_at_[hidden])<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">Date: 2006-12-12           00:46:59<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">Hi         Chris<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">Some of this is doable with today's               code....and one of these <br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">behaviors is not.           :-(<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">Open MPI/OpenRTE can be run in               "persistent" mode - this allows         <br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">multiple jobs to share the same               allocation. This works much as you         <br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">describe (syntax is slightly different,               of<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">course!) - the first mpirun will map using               whatever mode was <br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">requested, then the next mpirun will map               starting from where the <br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">first one left           off.<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">I *believe* you can run each mpirun in the               background.<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">However, I don't know if this has really               been tested enough to <br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">support such a claim. All testing that I               know about to-date has <br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">executed mpirun in the foreground - thus,               your example <br></blockquote></blockquote></blockquote>        <blockquote type="cite">would execute <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">sequentially instead of in             parallel.<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">I know people have tested multiple               mpirun's operating in parallel         <br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">within a single allocation (i.e.,               persistent mode) where <br></blockquote></blockquote></blockquote>        <blockquote type="cite">the mpiruns <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">are executed in separate               windows/prompts.<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">So I suspect you could do something like               you describe - <br></blockquote></blockquote></blockquote>        <blockquote type="cite">just haven't <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">personally verified           it.<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">Where we definitely differ is that Open               MPI/RTE will *not* block <br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">until resources are freed up from the               prior mpiruns.<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">Instead, we will attempt to execute each               mpirun immediately - and <br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">will error out the one(s) that try to               execute without sufficient <br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">resources. I imagine we could provide the               kind of "flow <br></blockquote></blockquote></blockquote>        <blockquote type="cite">control" you <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">describe, but I'm not sure when that might               happen.<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">I am (in my copious free time...haha)               working on an "orteboot" <br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">program that will startup a virtual               machine to make the persistent         <br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">mode of operation a little easier. For               now, though, you <br></blockquote></blockquote></blockquote>        <blockquote type="cite">can do it by:<br></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">1. starting up the "server" using the               following command:<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">orted --seed --persistent --scope public               [--universe foo]<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">2. do your mpirun commands. They will               automagically find <br></blockquote></blockquote></blockquote>        <blockquote type="cite">the "server" <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">and connect to it. If you specified a               universe name when <br></blockquote></blockquote></blockquote>        <blockquote type="cite">starting the <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">server, then you must specify the same               universe name on <br></blockquote></blockquote></blockquote>        <blockquote type="cite">your mpirun <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">commands.<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">When you are done, you will have to               (unfortunately) <br></blockquote></blockquote></blockquote>        <blockquote type="cite">manually "kill" <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">the server and remove its session               directory. I have a <br></blockquote></blockquote></blockquote>        <blockquote type="cite">program called <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">"ortehalt"<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">in the trunk that will do this cleanly for               you, but it <br></blockquote></blockquote></blockquote>        <blockquote type="cite">isn't yet in <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">the release distributions. You are welcome               to use it, <br></blockquote></blockquote></blockquote>        <blockquote type="cite">though, if you <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">are working with the trunk - I can't               promise it is <br></blockquote></blockquote></blockquote>        <blockquote type="cite">bulletproof yet, <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">but it seems to be           working.<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">Ralph<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">On 12/11/06 8:07 PM, "Maestas, Christopher               Daniel"<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">&lt;cdmaest_at_[hidden]&gt;<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">wrote:<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">Hello,<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">Sometimes we have users that like to do                 from within a single job         <br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">(think schedule within an job scheduler                 allocation):<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">"mpiexec -n X             myprog"<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">"mpiexec -n Y             myprog2"<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">Does mpiexec within Open MPI keep track                 of the node list         it<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">is         using<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">if it binds to a particular                 scheduler?<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">For example with 4 nodes (2ppn               SMP):<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">"mpiexec -n 2             myprog"<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">"mpiexec -n 2             myprog2"<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">"mpiexec -n 1             myprog3"<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">And assume this is by-slot allocation we                 would have the         following<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">allocation:<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">node1 - processor1 -               myprog<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">- processor2 -             myprog<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">node2 - processor1 -               myprog2<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">- processor2 -             myprog2<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">And for a by-node               allocation:<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">node1 - processor1 -               myprog<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">- processor2 -             myprog2<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">node2 - processor1 -               myprog<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">- processor2 -             myprog2<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">I think this is possible using ssh cause                 it shouldn't <br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">really matter <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">how many times it spawns, but with                 something like torque         it<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">would         get<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">restricted to a max process launch of 4.                 We would want the third         <br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">mpiexec to block processes and                 eventually be run on the first           <br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">available node allocation that frees up                 from myprog or <br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">myprog2 ....<br></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">For example for torque, we had to add                 the following to <br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">osc mpiexec:<br></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">---<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">Finally, since only one mpiexec can be                 the master at a<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">time, if         your<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">code setup requires that mpiexec exit to                 get a result, you<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">can start         a<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">"dummy"<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">mpiexec first in your               batch<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">job:<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">mpiexec           -server<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">It runs no tasks itself but handles the                 connections of<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">other         transient<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">mpiexec             clients.<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">It will shut down cleanly when the batch                 job exits or you<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">may kill         the<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">server             explicitly.<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">If the server is killed with SIGTERM (or                 HUP or INT), it <br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">will exit <br></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">with a status of zero if there were no                 clients connected         at<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">the         time.<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">If there were still clients using the                 server, the         server<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">will kill         all<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">their tasks, disconnect from the                 clients, and exit with status           1.<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">---<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">So a user           ran:<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">mpiexec           -server<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">mpiexec -n 2             myprog<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">mpiexec -n 2             myprog2<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">And the server kept track of the                 allocation ... I         would<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">think that         the<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">orted could do             this?<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">Sorry if this sounds confusing ... But                 I'm sure it will<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">clear up         with<br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">any further responses I make. :-)                 -cdm<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite"><br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">_______________________________________________<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">users mailing             list<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite">users_at_[hidden]<br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite">              <blockquote type="cite"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">            <blockquote type="cite"><br></blockquote></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite"><br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">_______________________________________________<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite">users mailing list<br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite"><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br></blockquote></blockquote>        <blockquote type="cite">          <blockquote type="cite"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></blockquote>        <blockquote type="cite"><br></blockquote>        <blockquote type="cite">_______________________________________________<br></blockquote>        <blockquote type="cite">users mailing list<br></blockquote>        <blockquote type="cite"><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br></blockquote>        <blockquote type="cite"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote>        <blockquote type="cite"><br></blockquote>        <blockquote type="cite"><br></blockquote><br>_______________________________________________<br>users         mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></blockquote></div><br></blockquote></div>_______________________________________________<br>users     mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote></div><br></div></blockquote></div> _______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></body></html>
