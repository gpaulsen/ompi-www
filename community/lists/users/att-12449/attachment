hi<div>I solve this problem, some previous versions of directories in the cluster are not removed, after I remove them, it works fine. thank you</div><div><br></div><div>cheers</div><div>fengguang<br><br><div class="gmail_quote">
On Mon, Mar 29, 2010 at 11:47 AM, Josh Hursey <span dir="ltr">&lt;<a href="mailto:jjhursey@open-mpi.org">jjhursey@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
Does this happen when you run without &#39;-am ft-enable-cr&#39; (so a no-C/R run)?<br>
<br>
This will help us determine if your problem is with the C/R work or with the ORTE runtime. I suspect that there is something odd with your system that is confusing the runtime (so not a C/R problem).<br>
<br>
Have you made sure to remove the previous versions of Open MPI from all machines on your cluster, before installing the new version? Sometimes problems like this come up because of mismatches in Open MPI versions on a machine.<br>

<br>
-- Josh<div><div></div><div class="h5"><br>
<br>
On Mar 23, 2010, at 5:42 PM, fengguang tian wrote:<br>
<br>
</div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div></div><div class="h5">
I met the same problem with this link:<a href="http://www.open-mpi.org/community/lists/users/2009/12/11374.php" target="_blank">http://www.open-mpi.org/community/lists/users/2009/12/11374.php</a><br>
<br>
in the link, they give a solution that use v1.4 open mpi instead of v1.3 open mpi. but, I am using v1.7a1r22794 open mpi, and met the same problem.<br>
here is what I have done:<br>
my cluster composed of two machines:nimbus(master) and nimbus1(slave), when I run mpirun -np 40 -am ft-enable-cr --hostfile .mpihostfile myapplication<br>
on the nimbus, and it doesn&#39;t work, it shows:<br>
<br>
[nimbus1:21387] opal_os_dirpath_create: Error: Unable to create the sub-directory (/tmp/openmpi-sessions-mpiu@nimbus1_0/59759) of (/tmp/openmpi-sessions-mpiu@nimbus1_0/59759/0/1), mkdir failed [1]<br>
[nimbus1:21387] [[59759,0],1] ORTE_ERROR_LOG: Error in file util/session_dir.c at line 106<br>
[nimbus1:21387] [[59759,0],1] ORTE_ERROR_LOG: Error in file util/session_dir.c at line 399<br>
[nimbus1:21387] [[59759,0],1] ORTE_ERROR_LOG: Error in file base/ess_base_std_orted.c at line 301<br>
[nimbus1:21387] [[59759,0],1] ORTE_ERROR_LOG: A message is attempting to be sent to a process whose contact information is unknown in file rml_oob_send.c at line 104<br>
[nimbus1:21387] [[59759,0],1] could not get route to [[INVALID],INVALID]<br>
[nimbus1:21387] [[59759,0],1] ORTE_ERROR_LOG: A message is attempting to be sent to a process whose contact information is unknown in file util/show_help.c at line 602<br>
[nimbus1:21387] [[59759,0],1] ORTE_ERROR_LOG: Error in file ess_env_module.c at line 143<br>
[nimbus1:21387] [[59759,0],1] ORTE_ERROR_LOG: A message is attempting to be sent to a process whose contact information is unknown in file rml_oob_send.c at line 104<br>
[nimbus1:21387] [[59759,0],1] could not get route to [[INVALID],INVALID]<br>
[nimbus1:21387] [[59759,0],1] ORTE_ERROR_LOG: A message is attempting to be sent to a process whose contact information is unknown in file util/show_help.c at line 602<br>
[nimbus1:21387] [[59759,0],1] ORTE_ERROR_LOG: Error in file runtime/orte_init.c at line 129<br>
[nimbus1:21387] [[59759,0],1] ORTE_ERROR_LOG: A message is attempting to be sent to a process whose contact information is unknown in file rml_oob_send.c at line 104<br>
[nimbus1:21387] [[59759,0],1] could not get route to [[INVALID],INVALID]<br>
[nimbus1:21387] [[59759,0],1] ORTE_ERROR_LOG: A message is attempting to be sent to a process whose contact information is unknown in file util/show_help.c at line 602<br>
[nimbus1:21387] [[59759,0],1] ORTE_ERROR_LOG: Error in file orted/orted_main.c at line 355<br>
--------------------------------------------------------------------------<br>
A daemon (pid 10737) died unexpectedly with status 255 while attempting<br>
to launch so we are aborting.<br>
<br>
There may be more information reported by the environment (see above).<br>
<br>
This may be because the daemon was unable to find all the needed shared<br>
libraries on the remote node. You may set your LD_LIBRARY_PATH to have the<br>
location of the shared libraries on the remote nodes and this will<br>
automatically be forwarded to the remote nodes.<br>
--------------------------------------------------------------------------<br>
--------------------------------------------------------------------------<br>
mpirun noticed that the job aborted, but has no info as to the process<br>
that caused that situation.<br>
--------------------------------------------------------------------------<br>
<br>
<br>
cheers<br>
fengguang<br></div></div>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br></div>

