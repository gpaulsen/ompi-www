Hello Ryan,<br><br>I have been running a similar heterogeneous setup in my lab; i.e., a mix of ppc64 and x86_64 systems connected by ethernet and InfiniBand.&nbsp; In trying to replicate your problem, what I see is that it is not an issue of processor heterogeneity, but rather an issue with heterogeneous transports.&nbsp; Can you remove the openib specifier from the btl lists in the appfile and try again?&nbsp; I.e., force all inter-system communications over ethernet?&nbsp; For me, that works.&nbsp; But, if I mix systems with IB with systems without IB, I, too, see a hang...even if the processor architectures are the same.&nbsp; If you could confirm that your case is the same, then we can make sure we&#39;re only chasing one problem and not two.<br>
<br>Thanks,<br>--Brad<br><br>Brad Benton<br>IBM<br><div style="margin-left: 40px;"><br></div><br><br><br><div class="gmail_quote">On Thu, May 1, 2008 at 11:02 AM, Ryan Buckley ; 21426 &lt;<a href="mailto:rbuckley@mc.com">rbuckley@mc.com</a>&gt; wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">Hello,<br>
<br>
I am trying to run a simple Hello World MPI application in a<br>
heterogeneous environment. &nbsp;The machines include 1 x86 machine with a<br>
standard 1Gb ethernet connection and 2 ppc machines with standard 1Gb<br>
ethernet as well as a 10Gb ethernet (Infiniband) switch between the two.<br>
The Hello World program is the same hello_c.c that is included in the<br>
examples directory of the Open MPI installation.<br>
<br>
The goal is that I would like to run heterogeneous applications between<br>
the three aforementioned machines in the following manner:<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp;The x86 machine will use tcp to communicate to the 2 ppc machines,<br>
while the ppc machines will communicate with one another via the 10GbE.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;x86 &lt;--tcp--&gt; ppc_1<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;x86 &lt;--tcp--&gt; ppc_2<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ppc1 &lt;--openib--&gt; ppc_2<br>
<br>
I am currently using a machfile set up as follows,<br>
<br>
# cat machfile<br>
&lt;ppc_host_1&gt;<br>
&lt;ppc_host_2&gt;<br>
&lt;x86_host&gt;<br>
<br>
In addition I am using an appfile set up as follows,<br>
<br>
# cat appfile<br>
-np 1 --hostfile machfile --host &lt;ppc_host_1&gt; --mca btl<br>
sm,self,tcp,openib /path/to/ppc/openmpi-1.2.5/examples/hello<br>
-np 1 --hostfile machfile --host &lt;ppc_host_2&gt; --mca btl<br>
sm,self,tcp,openib /path/to/ppc/openmpi-1.2.5/examples/hello<br>
-np 1 --hostfile machfile --host &lt;x86_host&gt; --mca btl<br>
sm,self,tcp /path/to/x86/openmpi-1.2.5/examples/hello<br>
<br>
I am running on the command line via<br>
<br>
# mpirun --app appfile<br>
<br>
I&#39;ve also attached the output from &#39;ompi_info --all&#39; from all machines.<br>
<br>
Any suggestions would be much appreciated.<br>
<br>
Thanks,<br>
<font color="#888888"><br>
Ryan<br>
<br>
</font><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

