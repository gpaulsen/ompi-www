On Sat, Jun 11, 2011 at 5:17 PM, Ole Kliemann <span dir="ltr">&lt;<a href="mailto:ole-ompi-2011@mail.plastictree.net">ole-ompi-2011@mail.plastictree.net</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
<div class="im">On Sat, Jun 11, 2011 at 07:24:24AM -0600, Ralph Castain wrote:<br>
&gt; Oh my - that is such an old version! Any reason for using it instead of something more recent?<br>
<br>
</div>I&#39;m using the cluster of the university where I work und I&#39;m not the<br>
admin. So I&#39;m going with what is installed there.<br></blockquote><div><br></div><div>Provided your account is available over all the nodes of the cluster (commonly through a shared filesystem (e.g. NFS)),</div><div>
you can easily install and use a more recent version of OpenMPI.</div><div><br></div><div>mkdir -p ${HOME}/ompi-1.5.3 &amp;&amp; ./configure --prefix=${HOME}/ompi-1.5.3</div><div>make </div><div>make install</div><div><br>
</div><div>You should not forget to modify your &quot;PATH&quot; and &quot;LD_LIBRARY_PATH&quot; environment variables in your &quot;.bash_profile&quot;.</div><div> </div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">

<br>
It&#39;s the first time I&#39;m using MPI. Before I complain to the admins about<br>
old versions or anything else, I&#39;d like to check whether my code<br>
actually should be okay in regard to MPI specifications.<br>
<div><div class="h5"><br>
&gt; On Jun 11, 2011, at 8:43 AM, Ole Kliemann wrote:<br>
&gt;<br>
&gt; &gt; Hi everyone!<br>
&gt; &gt;<br>
&gt; &gt; I&#39;m trying to use MPI on a cluster running OpenMPI 1.2.4 and starting<br>
&gt; &gt; processes through PBSPro_11.0.2.110766. I&#39;ve been running into a couple<br>
&gt; &gt; of performance and deadlock problems and like to check whether I&#39;m<br>
&gt; &gt; making a mistake.<br>
&gt; &gt;<br>
&gt; &gt; One of the deadlocks I managed to boil down to the attached example. I<br>
&gt; &gt; run it on 8 cores. It usually deadlocks with all except one process<br>
&gt; &gt; showing<br>
&gt; &gt;<br>
&gt; &gt;     start barrier<br>
&gt; &gt;<br>
&gt; &gt; as last output.<br>
&gt; &gt;<br>
&gt; &gt; The one process out of order shows:<br>
&gt; &gt;<br>
&gt; &gt;     start getting local<br>
&gt; &gt;<br>
&gt; &gt; My question at this point is simply whether this is expected behaviour<br>
&gt; &gt; of OpenMPI.<br>
&gt; &gt;<br>
&gt; &gt; Thanks in advance!<br>
&gt; &gt; Ole<br>
&gt; &gt; &lt;mpi_barrier.cc&gt;_______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote><br>

