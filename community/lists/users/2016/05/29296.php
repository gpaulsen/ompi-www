<?
$subject_val = "Re: [OMPI users] users Digest, Vol 3510, Issue 2";
include("../../include/msg-header.inc");
?>
<!-- received="Mon May 23 20:02:14 2016" -->
<!-- isoreceived="20160524000214" -->
<!-- sent="Tue, 24 May 2016 09:02:10 +0900" -->
<!-- isosent="20160524000210" -->
<!-- name="Gilles Gouaillardet" -->
<!-- email="gilles_at_[hidden]" -->
<!-- subject="Re: [OMPI users] users Digest, Vol 3510, Issue 2" -->
<!-- id="fbc5e77e-e701-ca69-b9fb-fc44bf5f3add_at_rist.or.jp" -->
<!-- charset="windows-1252" -->
<!-- inreplyto="1659377143.2420437.1464047087622.JavaMail.yahoo_at_mail.yahoo.com" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>Subject:</strong> Re: [OMPI users] users Digest, Vol 3510, Issue 2<br>
<strong>From:</strong> Gilles Gouaillardet (<em>gilles_at_[hidden]</em>)<br>
<strong>Date:</strong> 2016-05-23 20:02:10
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="29297.php">Megdich Islem: "Re: [OMPI users] users Digest, Vol 3510, Issue 2"</a>
<li><strong>Previous message:</strong> <a href="29295.php">Megdich Islem: "Re: [OMPI users] users Digest, Vol 3510, Issue 2"</a>
<li><strong>In reply to:</strong> <a href="29295.php">Megdich Islem: "Re: [OMPI users] users Digest, Vol 3510, Issue 2"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="29297.php">Megdich Islem: "Re: [OMPI users] users Digest, Vol 3510, Issue 2"</a>
<li><strong>Reply:</strong> <a href="29297.php">Megdich Islem: "Re: [OMPI users] users Digest, Vol 3510, Issue 2"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
what do you mean by coupling ?
<br>
<p>does Empire and OpenFoam communicate via MPI ?
<br>
<p>wouldn't it be much easier if you rebuild OpenFoam with mpich or intelmpi ?
<br>
<p><p>Cheers,
<br>
<p><p>Gilles
<br>
<p><p>On 5/24/2016 8:44 AM, Megdich Islem wrote:
<br>
<span class="quotelev1">&gt; &quot;Open MPI does not work when MPICH or intel MPI are installed&quot;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Thank you for your suggestion. But I need to run OpenFoam and Empire 
</span><br>
<span class="quotelev1">&gt; at the same time. In fact, Empire couples OpenFoam with another software.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Is there any solution for this case ?
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Regards,
</span><br>
<span class="quotelev1">&gt; Islem
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Le Lundi 23 mai 2016 17h00, &quot;users-request_at_[hidden]&quot; 
</span><br>
<span class="quotelev1">&gt; &lt;users-request_at_[hidden]&gt; a &#233;crit :
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Send users mailing list submissions to
</span><br>
<span class="quotelev1">&gt; users_at_[hidden]
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; To subscribe or unsubscribe via the World Wide Web, visit
</span><br>
<span class="quotelev1">&gt; <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users">https://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1">&gt; or, via email, send a message with subject or body 'help' to
</span><br>
<span class="quotelev1">&gt; users-request_at_[hidden]
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; You can reach the person managing the list at
</span><br>
<span class="quotelev1">&gt; users-owner_at_[hidden]
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; When replying, please edit your Subject line so it is more specific
</span><br>
<span class="quotelev1">&gt; than &quot;Re: Contents of users digest...&quot;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Today's Topics:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;   1. Re: Open MPI does not work when MPICH or intel MPI are
</span><br>
<span class="quotelev1">&gt;       installed (Andy Riebs)
</span><br>
<span class="quotelev1">&gt;   2. segmentation fault for slot-list and openmpi-1.10.3rc2
</span><br>
<span class="quotelev1">&gt;       (Siegmar Gross)
</span><br>
<span class="quotelev1">&gt;   3. Re: problem about mpirun on two nodes (Jeff Squyres (jsquyres))
</span><br>
<span class="quotelev1">&gt;   4. Re: Open MPI does not work when MPICH or intel MPI are
</span><br>
<span class="quotelev1">&gt;       installed (Gilles Gouaillardet)
</span><br>
<span class="quotelev1">&gt;   5. Re: segmentation fault for slot-list and openmpi-1.10.3rc2
</span><br>
<span class="quotelev1">&gt;       (Ralph Castain)
</span><br>
<span class="quotelev1">&gt;   6. mpirun java (Claudio Stamile)
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ----------------------------------------------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; [Message discarded by content filter]
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Message: 2
</span><br>
<span class="quotelev1">&gt; Date: Mon, 23 May 2016 15:26:52 +0200
</span><br>
<span class="quotelev1">&gt; From: Siegmar Gross &lt;siegmar.gross_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; To: Open MPI Users &lt;users_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Subject: [OMPI users] segmentation fault for slot-list and
</span><br>
<span class="quotelev1">&gt;     openmpi-1.10.3rc2
</span><br>
<span class="quotelev1">&gt; Message-ID:
</span><br>
<span class="quotelev1">&gt;     &lt;241613b1-ada6-292f-eeb9-722fc8fa2f94_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Content-Type: text/plain; charset=utf-8; format=flowed
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Hi,
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; I installed openmpi-1.10.3rc2 on my &quot;SUSE Linux Enterprise Server
</span><br>
<span class="quotelev1">&gt; 12 (x86_64)&quot; with Sun C 5.13  and gcc-6.1.0. Unfortunately I get
</span><br>
<span class="quotelev1">&gt; a segmentation fault for &quot;--slot-list&quot; for one of my small programs.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; loki spawn 119 ompi_info | grep -e &quot;OPAL repo revision:&quot; -e &quot;C 
</span><br>
<span class="quotelev1">&gt; compiler absolute:&quot;
</span><br>
<span class="quotelev1">&gt;       OPAL repo revision: v1.10.2-201-gd23dda8
</span><br>
<span class="quotelev1">&gt;       C compiler absolute: /usr/local/gcc-6.1.0/bin/gcc
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; loki spawn 120 mpiexec -np 1 --host loki,loki,loki,loki,loki spawn_master
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Parent process 0 running on loki
</span><br>
<span class="quotelev1">&gt;   I create 4 slave processes
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Parent process 0: tasks in MPI_COMM_WORLD:       1
</span><br>
<span class="quotelev1">&gt;                   tasks in COMM_CHILD_PROCESSES local group:  1
</span><br>
<span class="quotelev1">&gt;                   tasks in COMM_CHILD_PROCESSES remote group: 4
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Slave process 0 of 4 running on loki
</span><br>
<span class="quotelev1">&gt; Slave process 1 of 4 running on loki
</span><br>
<span class="quotelev1">&gt; Slave process 2 of 4 running on loki
</span><br>
<span class="quotelev1">&gt; spawn_slave 2: argv[0]: spawn_slave
</span><br>
<span class="quotelev1">&gt; Slave process 3 of 4 running on loki
</span><br>
<span class="quotelev1">&gt; spawn_slave 0: argv[0]: spawn_slave
</span><br>
<span class="quotelev1">&gt; spawn_slave 1: argv[0]: spawn_slave
</span><br>
<span class="quotelev1">&gt; spawn_slave 3: argv[0]: spawn_slave
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; loki spawn 121 mpiexec -np 1 --host loki --slot-list 0:0-5,1:0-5 
</span><br>
<span class="quotelev1">&gt; spawn_master
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Parent process 0 running on loki
</span><br>
<span class="quotelev1">&gt;   I create 4 slave processes
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; [loki:17326] *** Process received signal ***
</span><br>
<span class="quotelev1">&gt; [loki:17326] Signal: Segmentation fault (11)
</span><br>
<span class="quotelev1">&gt; [loki:17326] Signal code: Address not mapped (1)
</span><br>
<span class="quotelev1">&gt; [loki:17326] Failing at address: 0x8
</span><br>
<span class="quotelev1">&gt; [loki:17326] [ 0] /lib64/libpthread.so.0(+0xf870)[0x7f4e469b3870]
</span><br>
<span class="quotelev1">&gt; [loki:17326] [ 1] *** An error occurred in MPI_Init
</span><br>
<span class="quotelev1">&gt; *** on a NULL communicator
</span><br>
<span class="quotelev1">&gt; *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
</span><br>
<span class="quotelev1">&gt; ***    and potentially your MPI job)
</span><br>
<span class="quotelev1">&gt; [loki:17324] Local abort before MPI_INIT completed successfully; not 
</span><br>
<span class="quotelev1">&gt; able to
</span><br>
<span class="quotelev1">&gt; aggregate error messages, and not able to guarantee that all other 
</span><br>
<span class="quotelev1">&gt; processes
</span><br>
<span class="quotelev1">&gt; were killed!
</span><br>
<span class="quotelev1">&gt; /usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_proc_self+0x35)[0x7f4e46c165b0]
</span><br>
<span class="quotelev1">&gt; [loki:17326] [ 2]
</span><br>
<span class="quotelev1">&gt; /usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_comm_init+0x68b)[0x7f4e46bf5b08]
</span><br>
<span class="quotelev1">&gt; [loki:17326] [ 3] *** An error occurred in MPI_Init
</span><br>
<span class="quotelev1">&gt; *** on a NULL communicator
</span><br>
<span class="quotelev1">&gt; *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
</span><br>
<span class="quotelev1">&gt; ***    and potentially your MPI job)
</span><br>
<span class="quotelev1">&gt; [loki:17325] Local abort before MPI_INIT completed successfully; not 
</span><br>
<span class="quotelev1">&gt; able to
</span><br>
<span class="quotelev1">&gt; aggregate error messages, and not able to guarantee that all other 
</span><br>
<span class="quotelev1">&gt; processes
</span><br>
<span class="quotelev1">&gt; were killed!
</span><br>
<span class="quotelev1">&gt; /usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_mpi_init+0xa90)[0x7f4e46c1be8a]
</span><br>
<span class="quotelev1">&gt; [loki:17326] [ 4]
</span><br>
<span class="quotelev1">&gt; /usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(MPI_Init+0x180)[0x7f4e46c5828e]
</span><br>
<span class="quotelev1">&gt; [loki:17326] [ 5] spawn_slave[0x40097e]
</span><br>
<span class="quotelev1">&gt; [loki:17326] [ 6] /lib64/libc.so.6(__libc_start_main+0xf5)[0x7f4e4661db05]
</span><br>
<span class="quotelev1">&gt; [loki:17326] [ 7] spawn_slave[0x400a54]
</span><br>
<span class="quotelev1">&gt; [loki:17326] *** End of error message ***
</span><br>
<span class="quotelev1">&gt; -------------------------------------------------------
</span><br>
<span class="quotelev1">&gt; Child job 2 terminated normally, but 1 process returned
</span><br>
<span class="quotelev1">&gt; a non-zero exit code.. Per user-direction, the job has been aborted.
</span><br>
<span class="quotelev1">&gt; -------------------------------------------------------
</span><br>
<span class="quotelev1">&gt; --------------------------------------------------------------------------
</span><br>
<span class="quotelev1">&gt; mpiexec detected that one or more processes exited with non-zero 
</span><br>
<span class="quotelev1">&gt; status, thus
</span><br>
<span class="quotelev1">&gt; causing
</span><br>
<span class="quotelev1">&gt; the job to be terminated. The first process to do so was:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;   Process name: [[56340,2],0]
</span><br>
<span class="quotelev1">&gt;   Exit code:    1
</span><br>
<span class="quotelev1">&gt; --------------------------------------------------------------------------
</span><br>
<span class="quotelev1">&gt; loki spawn 122
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; I would be grateful, if somebody can fix the problem. Thank you
</span><br>
<span class="quotelev1">&gt; very much for any help in advance.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Kind regards
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Siegmar
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Message: 3
</span><br>
<span class="quotelev1">&gt; Date: Mon, 23 May 2016 14:13:11 +0000
</span><br>
<span class="quotelev1">&gt; From: &quot;Jeff Squyres (jsquyres)&quot; &lt;jsquyres_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; To: &quot;Open MPI User's List&quot; &lt;users_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Subject: Re: [OMPI users] problem about mpirun on two nodes
</span><br>
<span class="quotelev1">&gt; Message-ID: &lt;B2033C1D-8AA4-4823-B984-92756DC1E756_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Content-Type: text/plain; charset=&quot;us-ascii&quot;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; On May 21, 2016, at 11:31 PM, douraku_at_[hidden] wrote:
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; I encountered a problem about mpirun and SSH when using OMPI 1.10.0 
</span><br>
<span class="quotelev1">&gt; compiled with gcc, running on centos7.2.
</span><br>
<span class="quotelev2">&gt; &gt; When I execute mpirun on my 2 node cluster, I get the following 
</span><br>
<span class="quotelev1">&gt; errors pasted below.
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; [douraku_at_master home]$ mpirun -np 12 a.out
</span><br>
<span class="quotelev2">&gt; &gt; Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; This is the key right here: you got a permission denied error when you 
</span><br>
<span class="quotelev1">&gt; (assumedly) tried to execute on the remote server.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Triple check your ssh settings to ensure that you can run on the 
</span><br>
<span class="quotelev1">&gt; remote server(s) without a password or interactive passphrase entry.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; -- 
</span><br>
<span class="quotelev1">&gt; Jeff Squyres
</span><br>
<span class="quotelev1">&gt; jsquyres_at_[hidden]
</span><br>
<span class="quotelev1">&gt; For corporate legal information go to: 
</span><br>
<span class="quotelev1">&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a>
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Message: 4
</span><br>
<span class="quotelev1">&gt; Date: Mon, 23 May 2016 23:31:30 +0900
</span><br>
<span class="quotelev1">&gt; From: Gilles Gouaillardet &lt;gilles.gouaillardet_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; To: Open MPI Users &lt;users_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Subject: Re: [OMPI users] Open MPI does not work when MPICH or intel
</span><br>
<span class="quotelev1">&gt;     MPI are    installed
</span><br>
<span class="quotelev1">&gt; Message-ID:
</span><br>
<span class="quotelev1">&gt;     &lt;CAAkFZ5u86Q0ev=ospehnKvd0kumYBoeMD8WF=J+TaDUH3xYecQ_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Content-Type: text/plain; charset=&quot;utf-8&quot;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; modules are way more friendly that manually setting and exporting your
</span><br>
<span class="quotelev1">&gt; environment.
</span><br>
<span class="quotelev1">&gt; the issue here is you are setting your environment in your .bashrc, and
</span><br>
<span class="quotelev1">&gt; that cannot work if your account is used with various MPI implementations.
</span><br>
<span class="quotelev1">&gt; (unless your .bashrc checks a third party variable to select the
</span><br>
<span class="quotelev1">&gt; appropriate mpi, in this case, simply extend the logic to select openmpi)
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; if you configure'd with --enable-mpirun-prefix-by-default, you should not
</span><br>
<span class="quotelev1">&gt; need anything in your environment.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Cheers,
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Gilles
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; On Monday, May 23, 2016, Andy Riebs &lt;andy.riebs_at_[hidden]&gt; wrote:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev2">&gt; &gt; Hi,
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; The short answer: Environment module files are probably the best 
</span><br>
<span class="quotelev1">&gt; solution
</span><br>
<span class="quotelev2">&gt; &gt; for your problem.
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; The long answer: See
</span><br>
<span class="quotelev2">&gt; &gt; &lt;<a href="http://www.admin-magazine.com/HPC/Articles/Environment-Modules">http://www.admin-magazine.com/HPC/Articles/Environment-Modules</a>&gt;
</span><br>
<span class="quotelev2">&gt; &gt; &lt;<a href="http://www.admin-magazine.com/HPC/Articles/Environment-Modules">http://www.admin-magazine.com/HPC/Articles/Environment-Modules</a>&gt;, which
</span><br>
<span class="quotelev2">&gt; &gt; pretty much addresses your question.
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; Andy
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; On 05/23/2016 07:40 AM, Megdich Islem wrote:
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; Hi,
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; I am using 2 software, one is called Open Foam and the other called 
</span><br>
<span class="quotelev1">&gt; EMPIRE
</span><br>
<span class="quotelev2">&gt; &gt; that need to run together at the same time.
</span><br>
<span class="quotelev2">&gt; &gt; Open Foam uses  Open MPI implementation and EMPIRE uses either MPICH or
</span><br>
<span class="quotelev2">&gt; &gt; intel mpi.
</span><br>
<span class="quotelev2">&gt; &gt; The version of Open MPI that comes with Open Foam is 1.6.5.
</span><br>
<span class="quotelev2">&gt; &gt; I am using Intel (R) MPI Library for linux * OS, version 5.1.3 and MPICH
</span><br>
<span class="quotelev2">&gt; &gt; 3.0.4.
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; My problem is when I have the environment variables of  either mpich or
</span><br>
<span class="quotelev2">&gt; &gt; Intel MPI  sourced to bashrc, I fail to run a case of Open Foam with
</span><br>
<span class="quotelev2">&gt; &gt; parallel processing ( You find attached a picture of the error I got )
</span><br>
<span class="quotelev2">&gt; &gt; This is an example of a command line I use to run Open Foam
</span><br>
<span class="quotelev2">&gt; &gt; mpirun -np 4 interFoam -parallel
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; Once I keep the environment variable of OpenFoam only, the parallel
</span><br>
<span class="quotelev2">&gt; &gt; processing works without any problem, so I won't be able to run EMPIRE.
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; I am sourcing the environment variables in this way:
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; For Open Foam:
</span><br>
<span class="quotelev2">&gt; &gt; source /opt/openfoam30/etc/bashrc
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; For MPICH 3.0.4
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; export PATH=/home/islem/Desktop/mpich/bin:$PATH
</span><br>
<span class="quotelev2">&gt; &gt; export LD_LIBRARY_PATH=&quot;/home/islem/Desktop/mpich/lib/:$LD_LIBRARY_PATH&quot;
</span><br>
<span class="quotelev2">&gt; &gt; export MPICH_F90=gfortran
</span><br>
<span class="quotelev2">&gt; &gt; export MPICH_CC=/opt/intel/bin/icc
</span><br>
<span class="quotelev2">&gt; &gt; export MPICH_CXX=/opt/intel/bin/icpc
</span><br>
<span class="quotelev2">&gt; &gt; export MPICH-LINK_CXX=&quot;-L/home/islem/Desktop/mpich/lib/ -Wl,-rpath
</span><br>
<span class="quotelev2">&gt; &gt; -Wl,/home/islem/Desktop/mpich/lib -lmpichcxx -lmpich -lopa -lmpl -lrt
</span><br>
<span class="quotelev2">&gt; &gt; -lpthread&quot;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; For intel
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; export PATH=$PATH:/opt/intel/bin/
</span><br>
<span class="quotelev2">&gt; &gt; LD_LIBRARY_PATH=&quot;/opt/intel/lib/intel64:$LD_LIBRARY_PATH&quot;
</span><br>
<span class="quotelev2">&gt; &gt; export LD_LIBRARY_PATH
</span><br>
<span class="quotelev2">&gt; &gt; source
</span><br>
<span class="quotelev2">&gt; &gt; 
</span><br>
<span class="quotelev1">&gt; /opt/intel/compilers_and_libraries_2016.3.210/linux/mpi/intel64/bin/mpivars.sh
</span><br>
<span class="quotelev2">&gt; &gt; intel64
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; If Only Open Foam is sourced, mpirun --version gives OPEN MPI (1.6.5)
</span><br>
<span class="quotelev2">&gt; &gt; If Open Foam and MPICH are sourced, mpirun --version gives mpich 3.0.1
</span><br>
<span class="quotelev2">&gt; &gt; If Open Foam and intel MPI are sourced, mpirun --version gives intel (R)
</span><br>
<span class="quotelev2">&gt; &gt; MPI libarary for linux, version 5.1.3
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; My question is why I can't have two MPI implementation installed and
</span><br>
<span class="quotelev2">&gt; &gt; sourced together. How can I solve the problem ?
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; Regards,
</span><br>
<span class="quotelev2">&gt; &gt; Islem Megdiche
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; _______________________________________________
</span><br>
<span class="quotelev2">&gt; &gt; users mailing listusers_at_[hidden] 
</span><br>
<span class="quotelev1">&gt; &lt;javascript:_e(%7B%7D,'cvml','users_at_[hidden]');&gt;
</span><br>
<span class="quotelev2">&gt; &gt; Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users">https://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev2">&gt; &gt; Link to this post: 
</span><br>
<span class="quotelev1">&gt; <a href="http://www.open-mpi.org/community/lists/users/2016/05/29279.php">http://www.open-mpi.org/community/lists/users/2016/05/29279.php</a>
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev1">&gt; -------------- next part --------------
</span><br>
<span class="quotelev1">&gt; HTML attachment scrubbed and removed
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Message: 5
</span><br>
<span class="quotelev1">&gt; Date: Mon, 23 May 2016 08:45:53 -0700
</span><br>
<span class="quotelev1">&gt; From: Ralph Castain &lt;rhc_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; To: Open MPI Users &lt;users_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Subject: Re: [OMPI users] segmentation fault for slot-list and
</span><br>
<span class="quotelev1">&gt;     openmpi-1.10.3rc2
</span><br>
<span class="quotelev1">&gt; Message-ID: &lt;73195D72-CEA7-4AFC-9527-8F725C8B1FA1_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Content-Type: text/plain; charset=&quot;utf-8&quot;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; I cannot replicate the problem - both scenarios work fine for me. I?m 
</span><br>
<span class="quotelev1">&gt; not convinced your test code is correct, however, as you call 
</span><br>
<span class="quotelev1">&gt; Comm_free the inter-communicator but didn?t call Comm_disconnect. 
</span><br>
<span class="quotelev1">&gt; Checkout the attached for a correct code and see if it works for you.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; FWIW: I don?t know how many cores you have on your sockets, but if you 
</span><br>
<span class="quotelev1">&gt; have 6 cores/socket, then your slot-list is equivalent to ??bind-to 
</span><br>
<span class="quotelev1">&gt; none? as the slot-list applies to every process being launched
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; -------------- next part --------------
</span><br>
<span class="quotelev1">&gt; A non-text attachment was scrubbed...
</span><br>
<span class="quotelev1">&gt; Name: simple_spawn.c
</span><br>
<span class="quotelev1">&gt; Type: application/octet-stream
</span><br>
<span class="quotelev1">&gt; Size: 1926 bytes
</span><br>
<span class="quotelev1">&gt; Desc: not available
</span><br>
<span class="quotelev1">&gt; URL: 
</span><br>
<span class="quotelev1">&gt; &lt;<a href="http://www.open-mpi.org/MailArchives/users/attachments/20160523/7554b3ec/attachment.obj">http://www.open-mpi.org/MailArchives/users/attachments/20160523/7554b3ec/attachment.obj</a>&gt;
</span><br>
<span class="quotelev1">&gt; -------------- next part --------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev2">&gt; &gt; On May 23, 2016, at 6:26 AM, Siegmar Gross 
</span><br>
<span class="quotelev1">&gt; &lt;Siegmar.Gross_at_[hidden]&gt; wrote:
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; Hi,
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; I installed openmpi-1.10.3rc2 on my &quot;SUSE Linux Enterprise Server
</span><br>
<span class="quotelev2">&gt; &gt; 12 (x86_64)&quot; with Sun C 5.13  and gcc-6.1.0. Unfortunately I get
</span><br>
<span class="quotelev2">&gt; &gt; a segmentation fault for &quot;--slot-list&quot; for one of my small programs.
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; loki spawn 119 ompi_info | grep -e &quot;OPAL repo revision:&quot; -e &quot;C 
</span><br>
<span class="quotelev1">&gt; compiler absolute:&quot;
</span><br>
<span class="quotelev2">&gt; &gt;      OPAL repo revision: v1.10.2-201-gd23dda8
</span><br>
<span class="quotelev2">&gt; &gt;    C compiler absolute: /usr/local/gcc-6.1.0/bin/gcc
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; loki spawn 120 mpiexec -np 1 --host loki,loki,loki,loki,loki 
</span><br>
<span class="quotelev1">&gt; spawn_master
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; Parent process 0 running on loki
</span><br>
<span class="quotelev2">&gt; &gt;  I create 4 slave processes
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; Parent process 0: tasks in MPI_COMM_WORLD:           1
</span><br>
<span class="quotelev2">&gt; &gt;                  tasks in COMM_CHILD_PROCESSES local group:  1
</span><br>
<span class="quotelev2">&gt; &gt;                  tasks in COMM_CHILD_PROCESSES remote group: 4
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; Slave process 0 of 4 running on loki
</span><br>
<span class="quotelev2">&gt; &gt; Slave process 1 of 4 running on loki
</span><br>
<span class="quotelev2">&gt; &gt; Slave process 2 of 4 running on loki
</span><br>
<span class="quotelev2">&gt; &gt; spawn_slave 2: argv[0]: spawn_slave
</span><br>
<span class="quotelev2">&gt; &gt; Slave process 3 of 4 running on loki
</span><br>
<span class="quotelev2">&gt; &gt; spawn_slave 0: argv[0]: spawn_slave
</span><br>
<span class="quotelev2">&gt; &gt; spawn_slave 1: argv[0]: spawn_slave
</span><br>
<span class="quotelev2">&gt; &gt; spawn_slave 3: argv[0]: spawn_slave
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; loki spawn 121 mpiexec -np 1 --host loki --slot-list 0:0-5,1:0-5 
</span><br>
<span class="quotelev1">&gt; spawn_master
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; Parent process 0 running on loki
</span><br>
<span class="quotelev2">&gt; &gt;  I create 4 slave processes
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; [loki:17326] *** Process received signal ***
</span><br>
<span class="quotelev2">&gt; &gt; [loki:17326] Signal: Segmentation fault (11)
</span><br>
<span class="quotelev2">&gt; &gt; [loki:17326] Signal code: Address not mapped (1)
</span><br>
<span class="quotelev2">&gt; &gt; [loki:17326] Failing at address: 0x8
</span><br>
<span class="quotelev2">&gt; &gt; [loki:17326] [ 0] /lib64/libpthread.so.0(+0xf870)[0x7f4e469b3870]
</span><br>
<span class="quotelev2">&gt; &gt; [loki:17326] [ 1] *** An error occurred in MPI_Init
</span><br>
<span class="quotelev2">&gt; &gt; *** on a NULL communicator
</span><br>
<span class="quotelev2">&gt; &gt; *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
</span><br>
<span class="quotelev2">&gt; &gt; ***    and potentially your MPI job)
</span><br>
<span class="quotelev2">&gt; &gt; [loki:17324] Local abort before MPI_INIT completed successfully; not 
</span><br>
<span class="quotelev1">&gt; able to aggregate error messages, and not able to guarantee that all 
</span><br>
<span class="quotelev1">&gt; other processes were killed!
</span><br>
<span class="quotelev2">&gt; &gt; 
</span><br>
<span class="quotelev1">&gt; /usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_proc_self+0x35)[0x7f4e46c165b0]
</span><br>
<span class="quotelev2">&gt; &gt; [loki:17326] [ 2] 
</span><br>
<span class="quotelev1">&gt; /usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_comm_init+0x68b)[0x7f4e46bf5b08]
</span><br>
<span class="quotelev2">&gt; &gt; [loki:17326] [ 3] *** An error occurred in MPI_Init
</span><br>
<span class="quotelev2">&gt; &gt; *** on a NULL communicator
</span><br>
<span class="quotelev2">&gt; &gt; *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
</span><br>
<span class="quotelev2">&gt; &gt; ***    and potentially your MPI job)
</span><br>
<span class="quotelev2">&gt; &gt; [loki:17325] Local abort before MPI_INIT completed successfully; not 
</span><br>
<span class="quotelev1">&gt; able to aggregate error messages, and not able to guarantee that all 
</span><br>
<span class="quotelev1">&gt; other processes were killed!
</span><br>
<span class="quotelev2">&gt; &gt; 
</span><br>
<span class="quotelev1">&gt; /usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(ompi_mpi_init+0xa90)[0x7f4e46c1be8a]
</span><br>
<span class="quotelev2">&gt; &gt; [loki:17326] [ 4] 
</span><br>
<span class="quotelev1">&gt; /usr/local/openmpi-1.10.3_64_gcc/lib64/libmpi.so.12(MPI_Init+0x180)[0x7f4e46c5828e]
</span><br>
<span class="quotelev2">&gt; &gt; [loki:17326] [ 5] spawn_slave[0x40097e]
</span><br>
<span class="quotelev2">&gt; &gt; [loki:17326] [ 6] 
</span><br>
<span class="quotelev1">&gt; /lib64/libc.so.6(__libc_start_main+0xf5)[0x7f4e4661db05]
</span><br>
<span class="quotelev2">&gt; &gt; [loki:17326] [ 7] spawn_slave[0x400a54]
</span><br>
<span class="quotelev2">&gt; &gt; [loki:17326] *** End of error message ***
</span><br>
<span class="quotelev2">&gt; &gt; -------------------------------------------------------
</span><br>
<span class="quotelev2">&gt; &gt; Child job 2 terminated normally, but 1 process returned
</span><br>
<span class="quotelev2">&gt; &gt; a non-zero exit code.. Per user-direction, the job has been aborted.
</span><br>
<span class="quotelev2">&gt; &gt; -------------------------------------------------------
</span><br>
<span class="quotelev2">&gt; &gt; 
</span><br>
<span class="quotelev1">&gt; --------------------------------------------------------------------------
</span><br>
<span class="quotelev2">&gt; &gt; mpiexec detected that one or more processes exited with non-zero 
</span><br>
<span class="quotelev1">&gt; status, thus causing
</span><br>
<span class="quotelev2">&gt; &gt; the job to be terminated. The first process to do so was:
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;  Process name: [[56340,2],0]
</span><br>
<span class="quotelev2">&gt; &gt;  Exit code:    1
</span><br>
<span class="quotelev2">&gt; &gt; 
</span><br>
<span class="quotelev1">&gt; --------------------------------------------------------------------------
</span><br>
<span class="quotelev2">&gt; &gt; loki spawn 122
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; I would be grateful, if somebody can fix the problem. Thank you
</span><br>
<span class="quotelev2">&gt; &gt; very much for any help in advance.
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; Kind regards
</span><br>
<span class="quotelev2">&gt; &gt;
</span><br>
<span class="quotelev2">&gt; &gt; Siegmar
</span><br>
<span class="quotelev2">&gt; &gt; _______________________________________________
</span><br>
<span class="quotelev2">&gt; &gt; users mailing list
</span><br>
<span class="quotelev2">&gt; &gt; users_at_[hidden]
</span><br>
<span class="quotelev2">&gt; &gt; Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users">https://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev2">&gt; &gt; Link to this post: 
</span><br>
<span class="quotelev1">&gt; <a href="http://www.open-mpi.org/community/lists/users/2016/05/29281.php">http://www.open-mpi.org/community/lists/users/2016/05/29281.php</a>
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Message: 6
</span><br>
<span class="quotelev1">&gt; Date: Mon, 23 May 2016 17:47:53 +0200
</span><br>
<span class="quotelev1">&gt; From: Claudio Stamile &lt;claudiostamile_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; To: users_at_[hidden]
</span><br>
<span class="quotelev1">&gt; Subject: [OMPI users] mpirun java
</span><br>
<span class="quotelev1">&gt; Message-ID:
</span><br>
<span class="quotelev1">&gt;     &lt;CAAdD79zz1wAonmr5hOd3Jp51QEDUhmP5WW8Tp7EuJLDfsHeFxw_at_[hidden]&gt;
</span><br>
<span class="quotelev1">&gt; Content-Type: text/plain; charset=&quot;utf-8&quot;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Dear all,
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; I'm using openmpi for Java.
</span><br>
<span class="quotelev1">&gt; I've a problem when I try to use more option parameters in my java 
</span><br>
<span class="quotelev1">&gt; command.
</span><br>
<span class="quotelev1">&gt; More in detail I run mpirun as follow:
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; mpirun -n 5 java -cp path1:path2 -Djava.library.path=pathLibs
</span><br>
<span class="quotelev1">&gt; classification.MyClass
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; It seems that the option &quot;-Djava.library.path&quot; is ignored when i execute
</span><br>
<span class="quotelev1">&gt; the command.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Is it normal ?
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Do you know how to solve this problem ?
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Thank you.
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Best,
</span><br>
<span class="quotelev1">&gt; Claudio
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; -- 
</span><br>
<span class="quotelev1">&gt; C.
</span><br>
<span class="quotelev1">&gt; -------------- next part --------------
</span><br>
<span class="quotelev1">&gt; HTML attachment scrubbed and removed
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; Subject: Digest Footer
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; _______________________________________________
</span><br>
<span class="quotelev1">&gt; users mailing list
</span><br>
<span class="quotelev1">&gt; users_at_[hidden]
</span><br>
<span class="quotelev1">&gt; <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users">https://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; ------------------------------
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; End of users Digest, Vol 3510, Issue 2
</span><br>
<span class="quotelev1">&gt; **************************************
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt;
</span><br>
<span class="quotelev1">&gt; _______________________________________________
</span><br>
<span class="quotelev1">&gt; users mailing list
</span><br>
<span class="quotelev1">&gt; users_at_[hidden]
</span><br>
<span class="quotelev1">&gt; Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users">https://www.open-mpi.org/mailman/listinfo.cgi/users</a>
</span><br>
<span class="quotelev1">&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/05/29295.php">http://www.open-mpi.org/community/lists/users/2016/05/29295.php</a>
</span><br>
<p><p><hr>
<ul>
<li>text/html attachment: <a href="http://www.open-mpi.org/community/lists/users/att-29296/attachment">attachment</a>
</ul>
<!-- attachment="attachment" -->
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="29297.php">Megdich Islem: "Re: [OMPI users] users Digest, Vol 3510, Issue 2"</a>
<li><strong>Previous message:</strong> <a href="29295.php">Megdich Islem: "Re: [OMPI users] users Digest, Vol 3510, Issue 2"</a>
<li><strong>In reply to:</strong> <a href="29295.php">Megdich Islem: "Re: [OMPI users] users Digest, Vol 3510, Issue 2"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="29297.php">Megdich Islem: "Re: [OMPI users] users Digest, Vol 3510, Issue 2"</a>
<li><strong>Reply:</strong> <a href="29297.php">Megdich Islem: "Re: [OMPI users] users Digest, Vol 3510, Issue 2"</a>
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
