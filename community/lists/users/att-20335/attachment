I&#39;m on the road the rest of this week, but can look at this when I return next week. It looks like something unrelated to the Java bindings failed to properly initialize - at a guess, I&#39;d suspect that you are missing the LD_LIBRARY_PATH setting so none of the OMPI libs were found.<br>
<br><div class="gmail_quote">On Wed, Sep 26, 2012 at 5:42 AM, Siegmar Gross <span dir="ltr">&lt;<a href="mailto:Siegmar.Gross@informatik.hs-fulda.de" target="_blank">Siegmar.Gross@informatik.hs-fulda.de</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi,<br>
<br>
yesterday I installed openmpi-1.9a1r27362 on Solaris and Linux and<br>
I have a problem with mpiJava on Linux (openSUSE-Linux 12.1, x86_64).<br>
<br>
<br>
linpc4 mpi_classfiles 104 javac HelloMainWithoutMPI.java<br>
linpc4 mpi_classfiles 105 mpijavac HelloMainWithBarrier.java<br>
linpc4 mpi_classfiles 106 mpijavac -showme<br>
/usr/local/jdk1.7.0_07-64/bin/javac \<br>
  -cp ...:.:/usr/local/openmpi-1.9_64_cc/lib64/mpi.jar<br>
<br>
<br>
It works with Java without MPI.<br>
<br>
linpc4 mpi_classfiles 107 mpiexec java -cp $HOME/mpi_classfiles \<br>
  HelloMainWithoutMPI<br>
Hello from <a href="http://linpc4.informatik.hs-fulda.de/193.174.26.225" target="_blank">linpc4.informatik.hs-fulda.de/193.174.26.225</a><br>
<br>
<br>
It breaks with Java and MPI.<br>
<br>
linpc4 mpi_classfiles 108 mpiexec java -cp $HOME/mpi_classfiles \<br>
  HelloMainWithBarrier<br>
--------------------------------------------------------------------------<br>
It looks like opal_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during opal_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
  mca_base_open failed<br>
  --&gt; Returned value -2 instead of OPAL_SUCCESS<br>
--------------------------------------------------------------------------<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
  opal_init failed<br>
  --&gt; Returned value Out of resource (-2) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
--------------------------------------------------------------------------<br>
It looks like MPI_INIT failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during MPI_INIT; some of which are due to configuration or environment<br>
problems.  This failure appears to be an internal failure; here&#39;s some<br>
additional information (which may only be relevant to an Open MPI<br>
developer):<br>
<br>
  ompi_mpi_init: orte_init failed<br>
  --&gt; Returned &quot;Out of resource&quot; (-2) instead of &quot;Success&quot; (0)<br>
--------------------------------------------------------------------------<br>
*** An error occurred in MPI_Init<br>
*** on a NULL communicator<br>
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>
***    and potentially your MPI job)<br>
[linpc4:15332] Local abort before MPI_INIT completed successfully; not able to<br>
aggregate error messages, and not able to guarantee that all other processes were<br>
killed!<br>
-------------------------------------------------------<br>
Primary job  terminated normally, but 1 process returned<br>
a non-zero exit code.. Per user-direction, the job has been aborted.<br>
-------------------------------------------------------<br>
--------------------------------------------------------------------------<br>
mpiexec detected that one or more processes exited with non-zero status, thus<br>
causing<br>
the job to be terminated. The first process to do so was:<br>
<br>
  Process name: [[58875,1],0]<br>
  Exit code:    1<br>
--------------------------------------------------------------------------<br>
<br>
<br>
I configured with the following command.<br>
<br>
../openmpi-1.9a1r27362/configure --prefix=/usr/local/openmpi-1.9_64_cc \<br>
  --libdir=/usr/local/openmpi-1.9_64_cc/lib64 \<br>
  --with-jdk-bindir=/usr/local/jdk1.7.0_07-64/bin \<br>
  --with-jdk-headers=/usr/local/jdk1.7.0_07-64/include \<br>
  JAVA_HOME=/usr/local/jdk1.7.0_07-64 \<br>
  LDFLAGS=&quot;-m64&quot; \<br>
  CC=&quot;cc&quot; CXX=&quot;CC&quot; FC=&quot;f95&quot; \<br>
  CFLAGS=&quot;-m64&quot; CXXFLAGS=&quot;-m64 -library=stlport4&quot; FCFLAGS=&quot;-m64&quot; \<br>
  CPP=&quot;cpp&quot; CXXCPP=&quot;cpp&quot; \<br>
  CPPFLAGS=&quot;&quot; CXXCPPFLAGS=&quot;&quot; \<br>
  C_INCL_PATH=&quot;&quot; C_INCLUDE_PATH=&quot;&quot; CPLUS_INCLUDE_PATH=&quot;&quot; \<br>
  OBJC_INCLUDE_PATH=&quot;&quot; OPENMPI_HOME=&quot;&quot; \<br>
  --enable-cxx-exceptions \<br>
  --enable-mpi-java \<br>
  --enable-heterogeneous \<br>
  --enable-opal-multi-threads \<br>
  --enable-mpi-thread-multiple \<br>
  --with-threads=posix \<br>
  --with-hwloc=internal \<br>
  --without-verbs \<br>
  --without-udapl \<br>
  --with-wrapper-cflags=-m64 \<br>
  --enable-debug \<br>
  |&amp; tee log.configure.$SYSTEM_ENV.$MACHINE_ENV.64_cc<br>
<br>
<br>
It works fine on Solaris machines as long as the hosts belong to the<br>
same kind (Sparc or x86_64).<br>
<br>
tyr mpi_classfiles 194 mpiexec -host sunpc0,sunpc1,sunpc4 \<br>
  java -cp $HOME/mpi_classfiles HelloMainWithBarrier<br>
Process 1 of 3 running on sunpc1<br>
Process 2 of 3 running on <a href="http://sunpc4.informatik.hs-fulda.de" target="_blank">sunpc4.informatik.hs-fulda.de</a><br>
Process 0 of 3 running on sunpc0<br>
<br>
sunpc4 fd1026 107 mpiexec -host tyr,rs0,rs1 \<br>
  java -cp $HOME/mpi_classfiles HelloMainWithBarrier<br>
Process 1 of 3 running on <a href="http://rs0.informatik.hs-fulda.de" target="_blank">rs0.informatik.hs-fulda.de</a><br>
Process 2 of 3 running on <a href="http://rs1.informatik.hs-fulda.de" target="_blank">rs1.informatik.hs-fulda.de</a><br>
Process 0 of 3 running on <a href="http://tyr.informatik.hs-fulda.de" target="_blank">tyr.informatik.hs-fulda.de</a><br>
<br>
<br>
It breaks if the hosts belong to both kinds of machines.<br>
<br>
sunpc4 fd1026 106 mpiexec -host tyr,rs0,sunpc1 \<br>
  java -cp $HOME/mpi_classfiles HelloMainWithBarrier<br>
[<a href="http://rs0.informatik.hs-fulda.de:7718" target="_blank">rs0.informatik.hs-fulda.de:7718</a>] *** An error occurred in MPI_Comm_dup<br>
[<a href="http://rs0.informatik.hs-fulda.de:7718" target="_blank">rs0.informatik.hs-fulda.de:7718</a>] *** reported by process [565116929,1]<br>
[<a href="http://rs0.informatik.hs-fulda.de:7718" target="_blank">rs0.informatik.hs-fulda.de:7718</a>] *** on communicator MPI_COMM_WORLD<br>
[<a href="http://rs0.informatik.hs-fulda.de:7718" target="_blank">rs0.informatik.hs-fulda.de:7718</a>] *** MPI_ERR_INTERN: internal error<br>
[<a href="http://rs0.informatik.hs-fulda.de:7718" target="_blank">rs0.informatik.hs-fulda.de:7718</a>] *** MPI_ERRORS_ARE_FATAL (processes<br>
  in this communicator will now abort,<br>
[<a href="http://rs0.informatik.hs-fulda.de:7718" target="_blank">rs0.informatik.hs-fulda.de:7718</a>] ***    and potentially your MPI job)<br>
[<a href="http://sunpc4.informatik.hs-fulda.de:07900" target="_blank">sunpc4.informatik.hs-fulda.de:07900</a>] 1 more process has sent help<br>
  message help-mpi-errors.txt / mpi_errors_are_fatal<br>
[<a href="http://sunpc4.informatik.hs-fulda.de:07900" target="_blank">sunpc4.informatik.hs-fulda.de:07900</a>] Set MCA parameter<br>
  &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>
<br>
<br>
Please let me know if I can provide anything else to track these errors.<br>
Thank you very much for any help in advance.<br>
<br>
<br>
Kind regards<br>
<br>
Siegmar<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br>

