<html><body>
<p>Hi Ron - <br>
<br>
I am well aware of the scaling problems related to the standard send requirements in MPI.  I t is a very difficult issue.<br>
<br>
However, here is what the standard says: MPI 1.2, page 32 lines 29-37<br>
<br>
<tt>=======</tt><br>
<tt>a standard send operation that cannot complete because of lack of buffer space will merely block, waiting for buffer space to become available or for a matching receive to be posted. This behavior is preferable in many situations. Consider a situation where a producer repeatedly produces new values and sends them to a consumer. Assume that the producer produces new values faster than the consumer can consume them. If buffered sends are used, then a buffer overflow will result. Additional synchronization has to be added to the program so as to prevent this from occurring. If standard sends are used, then the producer will be </tt><br>
<tt>automatically throttled, as its send operations will block when buffer space is unavailable. </tt><br>
<tt>========</tt><br>
<br>
<font size="4">I</font>f there are people who want to argue that this is unclear or that it should be changed, the MPI Forum can and should take up the discussion.  I think this particular wording is pretty clear.  <br>
<br>
The piece of MPI standard wording you quote is somewhat ambiguous:<br>
<tt>============</tt><br>
<tt>The amount<br>
of space available for buffering will be much smaller than program data<br>
memory on many systems. Then, it will be easy to write programs that<br>
overrun available buffer space.</tt><br>
<tt>============</tt><br>
But note that this wording mentions a problem that an application can create but does not say the MPI implementation can fail the job.  The language I have pointed to is where the standard says what the MPI implementation must do. <br>
<br>
The &quot;lack of resource&quot; statement is more about send and receive descriptors than buffer space.  If I write a program with an infinite loop of MPI_IRECV postings  the standard allows that to fail.<br>
<br>
<br>
                Dick <br>
<br>
Dick Treumann  -  MPI Team/TCEM            <br>
IBM Systems &amp; Technology Group<br>
Dept 0lva / MS P963 -- 2455 South Road -- Poughkeepsie, NY 12601<br>
Tele (845) 433-7846         Fax (845) 433-8363<br>
<br>
<br>
<tt>users-bounces@open-mpi.org wrote on 02/04/2008 12:24:11 PM:<br>
<br>
&gt; <br>
&gt; &gt; Is what George says accurate? If so, it sounds to me like OpenMPI<br>
&gt; &gt; does not comply with the MPI standard on the behavior of eager<br>
&gt; &gt; protocol. MPICH is getting dinged in this discussion because they<br>
&gt; &gt; have complied with the requirements of the MPI standard. IBM MPI<br>
&gt; &gt; also complies with the standard.<br>
&gt; &gt; <br>
&gt; &gt; If there is any debate about whether the MPI standard does (or<br>
&gt; &gt; should) require the behavior I describe below then we should move<br>
&gt; &gt; the discussion to the MPI 2.1 Forum and get a clarification.<br>
&gt; &gt; [...]<br>
&gt; <br>
&gt; The MPI Standard also says the following about resource limitations:<br>
&gt; <br>
&gt; &nbsp; Any pending communication operation consumes system resources that are<br>
&gt; &nbsp; limited. Errors may occur when lack of resources prevent the execution<br>
&gt; &nbsp; of an MPI call. A quality implementation will use a (small) fixed amount<br>
&gt; &nbsp; of resources for each pending send in the ready or synchronous mode and<br>
&gt; &nbsp; for each pending receive. However, buffer space may be consumed to store<br>
&gt; &nbsp; messages sent in standard mode, and must be consumed to store messages<br>
&gt; &nbsp; sent in buffered mode, when no matching receive is available. The amount<br>
&gt; &nbsp; of space available for buffering will be much smaller than program data<br>
&gt; &nbsp; memory on many systems. Then, it will be easy to write programs that<br>
&gt; &nbsp; overrun available buffer space.<br>
&gt; <br>
&gt; Since I work on MPI implementations that are expected to allow applications<br>
&gt; to scale to tens of thousands of processes, I don't want the overhead of<br>
&gt; a user-level flow control protocol that penalizes scalable applications in<br>
&gt; favor of non-scalable ones. &nbsp;I also don't want an MPI implementation that<br>
&gt; hides such non-scalable application behavior, but rather exposes it at lower<br>
&gt; processor counts -- preferably in a way that makes the application developer<br>
&gt; aware of the resources requirements of their code and allows them to make<br>
&gt; the appropriate choice regarding the structure of their code, the underlying<br>
&gt; protocols, and the amount of buffer resources.<br>
&gt; <br>
&gt; But I work in a place where codes are expected to scale and not just work.<br>
&gt; Most of the vendors aren't allowed to have this perspective....<br>
&gt; <br>
&gt; -Ron<br>
&gt; <br>
&gt; <br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; users@open-mpi.org<br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</tt></body></html>
