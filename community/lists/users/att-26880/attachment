<div dir="ltr">It looks like you have PSM enabled cards on your system as well as Ethernet, and we are picking that up. Try adding &quot;-mca pml ob1&quot; to your cmd line and see if that helps<div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Tue, May 19, 2015 at 5:04 AM, Nilo Menezes <span dir="ltr">&lt;<a href="mailto:nilo@nilo.pro.br" target="_blank">nilo@nilo.pro.br</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hello,<br>
<br>
I&#39;m trying to run openmpi with multithread support enabled.<br>
<br>
I&#39;m getting this error messages before init finishes:<br>
[node011:61627] PSM returned unhandled/unknown connect error: Operation timed out<br>
[node011:61627] PSM EP connect error (unknown connect error):<br>
<br>
*** An error occurred in MPI_Init_thread<br>
*** on a NULL communicator<br>
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>
***    and potentially your MPI job)<br>
[node005:51948] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!<br>
*** An error occurred in MPI_Init_thread<br>
*** on a NULL communicator<br>
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>
***    and potentially your MPI job)<br>
[node039:57062] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!<br>
*** An error occurred in MPI_Init_thread<br>
*** on a NULL communicator<br>
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>
***    and potentially your MPI job)<br>
[node012:64036] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!<br>
*** An error occurred in MPI_Init_thread<br>
*** on a NULL communicator<br>
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>
***    and potentially your MPI job)<br>
[node008:14098] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!<br>
*** An error occurred in MPI_Init_thread<br>
*** on a NULL communicator<br>
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>
***    and potentially your MPI job)<br>
[node011:61627] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!<br>
[node005:51887] 1 more process has sent help message help-mpi-runtime / mpi_init:startup:internal-failure<br>
[node005:51887] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>
<br>
The library was configured with:<br>
./configure \<br>
--prefix=/home/opt \<br>
--enable-static \<br>
--enable-mpi-thread-multiple \<br>
--with-threads<br>
<br>
gcc 4.8.2<br>
<br>
On Linux:<br>
Linux node001 2.6.32-279.14.1.el6.x86_64 #1 SMP Mon Oct 15 13:44:51 EDT 2012 x86_64 x86_64 x86_64 GNU/Linux<br>
<br>
The job was started with:<br>
sbatch --nodes=6 --ntasks=30 --mem=4096  -o result/TOn6t30.txt -e result/TEn6t30.txt job.sh<br>
<br>
<br>
job.sh contains:<br>
mpirun --mca btl tcp,self \<br>
       --mca btl_tcp_if_include <a href="http://172.24.38.0/24" target="_blank">172.24.38.0/24</a> \<br>
       --mca oob_tcp_if_include eth0 \<br>
/home/umons/info/menezes/drsim/build/NameResolution/gameoflife_mpi2 --columns=1000 --rows=1000<br>
<br>
I call MPI_INIT with:<br>
    int provided;<br>
    MPI_Init_thread(&amp;argc, &amp;argv, MPI_THREAD_MULTIPLE, &amp;provided);<br>
<br>
The program is a simple game of life simulation. It runs fine in a single node (with one or many tasks). But fails at random nodes when distributed.<br>
<br>
Any hint may help.<br>
<br>
Best Regards,<br>
<br>
Nilo Menezes<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/05/26879.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/05/26879.php</a><br>
</blockquote></div><br></div>

