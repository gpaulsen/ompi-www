<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML><HEAD><TITLE>Ompi failing on mx only</TITLE>
<META http-equiv=Content-Type content="text/html; charset=us-ascii">
<META content="MSHTML 6.00.2900.3020" name=GENERATOR>
<STYLE></STYLE>
</HEAD>
<BODY bgColor=#ffffff>
<DIV dir=ltr align=left><SPAN class=913400821-02012007><FONT face=Arial 
color=#0000ff size=2>
<DIV dir=ltr align=left><SPAN class=913400821-02012007><FONT face=Arial 
color=#0000ff size=2>About the -x, I've been trying it both ways and prefer the 
latter, and results for either are the same. But it's value is 
correct.&nbsp;I've attached the ompi_info from node-1 and node-2. Sorry for not 
zipping them, but they were small and I think I'd have firewall 
issues.</FONT></SPAN></DIV></FONT></SPAN></DIV>
<DIV dir=ltr align=left><SPAN class=913400821-02012007><FONT face=Arial 
color=#0000ff size=2></FONT></SPAN>&nbsp;</DIV>
<DIV dir=ltr align=left><SPAN class=913400821-02012007><FONT face=Arial 
color=#0000ff size=2>$ mpirun --prefix /usr/local/openmpi-1.2b2 -x 
LD_LIBRARY_PATH --hostfile ./h13-15 -np 6 --mca pml cm ./cpi 
</FONT></SPAN></DIV>
<DIV dir=ltr align=left><SPAN class=913400821-02012007><FONT face=Arial 
color=#0000ff size=2>[node-14:19260] mx_connect fail for node-14:0 with key 
aaaaffff (error Endpoint closed or not connectable!)<BR>[node-14:19261] 
mx_connect fail for node-14:0 with key aaaaffff (error Endpoint closed or not 
connectable!)<BR>...</FONT></SPAN></DIV>
<DIV dir=ltr align=left><SPAN class=913400821-02012007><FONT face=Arial 
color=#0000ff size=2></FONT></SPAN>&nbsp;</DIV>
<DIV dir=ltr align=left><SPAN class=913400821-02012007><FONT face=Arial 
color=#0000ff size=2>Is there any info anywhere's on MTL?&nbsp;Anyways, I've run 
w/ mtl, and sometimes it actually worked once. But now I can't reproduce it and 
it's throwing sig 7's, 11's, and 4's depending upon the number of procs I give 
it. But now that you mention mapper, I take it that's what SEGV_MAPERR might be 
referring to. I'm looking into the </FONT></SPAN></DIV>
<DIV dir=ltr align=left><SPAN class=913400821-02012007><FONT face=Arial 
color=#0000ff size=2></FONT></SPAN>&nbsp;</DIV>
<DIV dir=ltr align=left><SPAN class=913400821-02012007><FONT face=Arial 
color=#0000ff size=2>$ mpirun --prefix /usr/local/openmpi-1.2b2 -x 
LD_LIBRARY_PATH=${LD_LIBRARY_PATH} --hostfile ./h1-3 -np 5 --mca mtl mx,self 
./cpi <BR>Process 4 of 5 is on node-2<BR>Process 0 of 5 is on node-1<BR>Process 
1 of 5 is on node-1<BR>Process 2 of 5 is on node-1<BR>Process 3 of 5 is on 
node-1<BR>pi is approximately 3.1415926544231225, Error is 
0.0000000008333294<BR>wall clock time = 0.019305<BR>Signal:11 
info.si_errno:0(Success) si_code:1(SEGV_MAPERR)<BR>Failing at 
addr:0x2b88243862be<BR>mpirun noticed that job rank 0 with PID 0 on node node-1 
exited on signal 1.<BR>4 additional processes aborted (not 
shown)<BR></FONT></SPAN></DIV>
<DIV dir=ltr align=left><SPAN class=913400821-02012007><FONT face=Arial 
color=#0000ff size=2>Or sometimes I'll get this error, just depending upon the 
number of procs ...</FONT></SPAN></DIV>
<DIV dir=ltr align=left><SPAN class=913400821-02012007><FONT face=Arial 
color=#0000ff size=2>&nbsp;</DIV>
<DIV>&nbsp;mpirun --prefix /usr/local/openmpi-1.2b2 -x 
LD_LIBRARY_PATH=${LD_LIBRARY_PATH} --hostfile ./h1-3 -np 7 --mca mtl mx,self 
./cpi <BR>Signal:7 info.si_errno:0(Success) si_code:2()<BR>Failing at 
addr:0x2aaaaaaab000<BR>[0] 
func:/usr/local/openmpi-1.2b2/lib/libopen-pal.so.0(opal_backtrace_print+0x1f) 
[0x2b9b7fa52d1f]<BR>[1] func:/usr/local/openmpi-1.2b2/lib/libopen-pal.so.0 
[0x2b9b7fa51871]<BR>[2] func:/lib/libpthread.so.0 [0x2b9b80013d00]<BR>[3] 
func:/usr/local/openmpi-1.2b2/lib/libmca_common_sm.so.0(mca_common_sm_mmap_init+0x1e3) 
[0x2b9b8270ef83]<BR>[4] 
func:/usr/local/openmpi-1.2b2/lib/openmpi/mca_mpool_sm.so 
[0x2b9b8260d0ff]<BR>[5] 
func:/usr/local/openmpi-1.2b2/lib/libmpi.so.0(mca_mpool_base_module_create+0x70) 
[0x2b9b7f7afac0]<BR>[6] 
func:/usr/local/openmpi-1.2b2/lib/openmpi/mca_btl_sm.so(mca_btl_sm_add_procs_same_base_addr+0x907) 
[0x2b9b83070517]<BR>[7] 
func:/usr/local/openmpi-1.2b2/lib/openmpi/mca_bml_r2.so(mca_bml_r2_add_procs+0x206) 
[0x2b9b82d5f576]<BR>[8] 
func:/usr/local/openmpi-1.2b2/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_add_procs+0xe3) 
[0x2b9b82a2d0a3]<BR>[9] 
func:/usr/local/openmpi-1.2b2/lib/libmpi.so.0(ompi_mpi_init+0x697) 
[0x2b9b7f77be07]<BR>[10] 
func:/usr/local/openmpi-1.2b2/lib/libmpi.so.0(MPI_Init+0x83) 
[0x2b9b7f79c943]<BR>[11] func:./cpi(main+0x42) [0x400cd5]<BR>[12] 
func:/lib/libc.so.6(__libc_start_main+0xf4) [0x2b9b8013a134]<BR>[13] func:./cpi 
[0x400bd9]<BR>*** End of error message ***<BR>Process 4 of 7 is on 
node-2<BR>Process 5 of 7 is on node-2<BR>Process 6 of 7 is on node-2<BR>Process 
0 of 7 is on node-1<BR>Process 1 of 7 is on node-1<BR>Process 2 of 7 is on 
node-1<BR>Process 3 of 7 is on node-1<BR>pi is approximately 3.1415926544231239, 
Error is 0.0000000008333307<BR>wall clock time = 0.009331<BR>Signal:11 
info.si_errno:0(Success) si_code:1(SEGV_MAPERR)<BR>Failing at 
addr:0x2b4ba33652be<BR>Signal:11 info.si_errno:0(Success) 
si_code:1(SEGV_MAPERR)<BR>Failing at addr:0x2b8685aba2be<BR>Signal:11 
info.si_errno:0(Success) si_code:1(SEGV_MAPERR)<BR>Failing at 
addr:0x2b304ffbe2be<BR>mpirun noticed that job rank 0 with PID 0 on node node-1 
exited on signal 1.<BR>6 additional processes aborted (not shown)<BR></DIV>
<DIV></FONT></SPAN><FONT face=Arial color=#0000ff size=2></FONT><FONT face=Arial 
color=#0000ff size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial color=#0000ff size=2><SPAN class=913400821-02012007>Ok, so 
I take it one is down. Would this be the cause for all the different errors I'm 
seeing?</SPAN></FONT></DIV>
<DIV><FONT face=Arial color=#0000ff size=2><SPAN 
class=913400821-02012007></SPAN></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial color=#0000ff size=2><SPAN 
class=913400821-02012007></SPAN></FONT><FONT face=Arial color=#0000ff 
size=2></FONT><FONT face=Arial color=#0000ff size=2>$ fm_status <BR>FMS Fabric 
status</FONT></DIV>
<DIV><FONT face=Arial color=#0000ff size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial color=#0000ff size=2>17&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
hosts known<BR>16&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; FMAs 
found<BR>3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; un-ACKed alerts<BR>Mapping is 
complete, last map generated by node-20<BR>Database generation not yet 
complete.<BR></FONT></DIV>
<DIV><FONT face=Arial color=#0000ff size=2></FONT><BR>&nbsp;</DIV>
<DIV class=OutlookMessageHeader lang=en-us dir=ltr align=left>
<HR tabIndex=-1>
<FONT face=Tahoma size=2><B>From:</B> users-bounces@open-mpi.org 
[mailto:users-bounces@open-mpi.org] <B>On Behalf Of </B>Reese 
Faucette<BR><B>Sent:</B> Tuesday, January 02, 2007 2:52 PM<BR><B>To:</B> Open 
MPI Users<BR><B>Subject:</B> Re: [OMPI users] Ompi failing on mx 
only<BR></FONT><BR></DIV>
<DIV></DIV>
<DIV><FONT face=Arial size=2>Hi, Gary-</FONT></DIV>
<DIV><FONT face=Arial size=2>This looks like a config problem, and not a code 
problem yet.&nbsp; Could you send the output of mx_info from node-1 and from 
node-2?&nbsp; Also, forgive me counter-asking a possibly dumb OMPI question, but 
is "-x LD_LIBRARY_PATH" really what you want, as opposed to "-x 
LD_LIBRARY_PATH=${LD_LIBRARY_PATH}" ?&nbsp; (I would not be surprised if not 
specifying a value defaults to this behavior, but have to ask).</FONT></DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Also, have you tried MX MTL as opposed to 
BTL?&nbsp; --mca pml cm --mca mtl mx,self&nbsp; (it looks like you 
did)</FONT></DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>"[node-2:10464] mx_connect fail for node-2:0 with 
key aaaaffff " makes it look like your fabric may not be fully mapped or that 
you may have a down link.</FONT></DIV>
<DIV><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>thanks,</FONT></DIV>
<DIV><FONT face=Arial size=2>-reese</FONT></DIV>
<DIV><FONT face=Arial size=2>Myricom, Inc.</FONT></DIV>
<BLOCKQUOTE 
style="PADDING-RIGHT: 0px; PADDING-LEFT: 5px; MARGIN-LEFT: 5px; BORDER-LEFT: #000000 2px solid; MARGIN-RIGHT: 0px"><!-- Converted from text/rtf format --><FONT 
  face=Arial size=2></FONT><BR>
  <P><FONT face=Arial size=2>I was initially using 1.1.2 and moved to 1.2b2 
  because of a hang on MPI_Bcast() which 1.2b2 reports to fix, and seemed to 
  have done so. My compute nodes are 2 dual core xeons on myrinet with mx. The 
  problem is trying to get ompi running on mx only. My machine file is as 
  follows &#8230;</FONT></P>
  <P><FONT face=Arial size=2>node-1 slots=4 max-slots=4</FONT> <BR><FONT 
  face=Arial size=2>node-2 slots=4 max-slots=4</FONT> <BR><FONT face=Arial 
  size=2>node-3 slots=4 max-slots=4</FONT> </P>
  <P><FONT face=Arial size=2>'mpirun' with the minimum number of processes in 
  order to get the error ...</FONT> 
  <BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <FONT face=Arial size=2>mpirun 
  --prefix /usr/local/openmpi-1.2b2 -x LD_LIBRARY_PATH --hostfile ./h1-3 -np 2 
  --mca btl mx,self ./cpi</FONT> </P>
  <P><FONT face=Arial size=2>I don't believe there'a anything wrong w/ the 
  hardware as I can ping on mx between this failed node and the master fine. So 
  I tried a different set of 3 nodes and I got the same error, it always fails 
  on the 2nd node of any group of nodes I 
choose.</FONT></P></BLOCKQUOTE></BODY></HTML>

