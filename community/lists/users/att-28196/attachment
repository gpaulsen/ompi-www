<div dir="ltr">Ralph,<div><br></div><div>Huh. That isn&#39;t in the Open MPI 1.8.8 mpirun man page. It is in Open MPI 1.10, so I&#39;m guessing someone noticed it wasn&#39;t there. Explains why I didn&#39;t try it out. I&#39;m assuming this option is respected on all nodes?</div><div><br></div><div>Note: a SmarterManThanI™ here at Goddard thought up this:</div><div><br></div><div><div><font face="monospace, monospace">#!/bin/bash</font></div><div><font face="monospace, monospace">rank=0</font></div><div><font face="monospace, monospace">for node in $(srun uname -n | sort); do</font></div><div><font face="monospace, monospace">        echo &quot;rank $rank=$node slots=1:*&quot;</font></div><div><font face="monospace, monospace">        let rank+=1</font></div><div><font face="monospace, monospace">done </font></div></div><div><br></div><div>It does seem to work in synthetic tests so I&#39;m trying it now in my real job. I had to hack a few run scripts so I&#39;ll probably spend the next hour debugging something dumb I did.</div><div><br></div><div>What I&#39;m wondering about all this is: can this be done with --slot-list? Or, perhaps, does --slot-list even work? </div><div><br></div><div>I have tried about 20 different variations of it, e.g., --slot-list 1:*, --slot-list &#39;1:*&#39;, --slot-list 1:0,1,2,3,4,5,6,7, --slot-list 1:8,9,10,11,12,13,14,15, --slot-list 8-15, &amp;c., and every time I seem to trigger an error via help-rmaps_rank_file.txt. I tried to read through opal_hwloc_base_slot_list_parse in the source, but my C isn&#39;t great (see my gmail address name) so that didn&#39;t help. Might not even be the right function, but I was just acking the code.</div><div><br></div><div>Thanks,</div><div>Matt</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Mon, Dec 21, 2015 at 10:51 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">Try adding —cpu-set a,b,c,…  where the a,b,c… are the core id’s of your second socket. I’m working on a cleaner option as this has come up before.<div><br><div><br><div><blockquote type="cite"><div><div class="h5"><div>On Dec 21, 2015, at 5:29 AM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com" target="_blank">fortran@gmail.com</a>&gt; wrote:</div><br></div></div><div><div><div class="h5"><div dir="ltr">Dear Open MPI Gurus,<div><br></div><div>I&#39;m currently trying to do something with Open MPI 1.8.8 that I&#39;m pretty sure is possible, but I&#39;m just not smart enough to figure out. Namely, I&#39;m seeing some odd GPU timings and I think it&#39;s because I was dumb and assumed the GPU was on the PCI bus next to Socket #0 as some older GPU nodes I ran on were like that. </div><div><br></div><div>But, a trip through lspci and lstopo has shown me that the GPU is actually on Socket #1. These are dual socket Sandy Bridge nodes and I&#39;d like to do some tests where I run a 8 processes per node and those processes all land on Socket #1.</div><div><br></div><div>So, what I&#39;m trying to figure out is how to have Open MPI bind processes like that. My first thought as always is to run a helloworld job with -report-bindings on. I can manage to do this:</div><div><br></div><div><div><div><font face="monospace, monospace">(1061) $ mpirun -np 8 -report-bindings -map-by core ./helloWorld.exe</font></div><div><font face="monospace, monospace">[borg01z205:16306] MCW rank 4 bound to socket 0[core 4[hwt 0]]: [././././B/././.][./././././././.]</font></div><div><font face="monospace, monospace">[borg01z205:16306] MCW rank 5 bound to socket 0[core 5[hwt 0]]: [./././././B/./.][./././././././.]</font></div><div><font face="monospace, monospace">[borg01z205:16306] MCW rank 6 bound to socket 0[core 6[hwt 0]]: [././././././B/.][./././././././.]</font></div><div><font face="monospace, monospace">[borg01z205:16306] MCW rank 7 bound to socket 0[core 7[hwt 0]]: [./././././././B][./././././././.]</font></div><div><font face="monospace, monospace">[borg01z205:16306] MCW rank 0 bound to socket 0[core 0[hwt 0]]: [B/././././././.][./././././././.]</font></div><div><font face="monospace, monospace">[borg01z205:16306] MCW rank 1 bound to socket 0[core 1[hwt 0]]: [./B/./././././.][./././././././.]</font></div><div><font face="monospace, monospace">[borg01z205:16306] MCW rank 2 bound to socket 0[core 2[hwt 0]]: [././B/././././.][./././././././.]</font></div><div><font face="monospace, monospace">[borg01z205:16306] MCW rank 3 bound to socket 0[core 3[hwt 0]]: [./././B/./././.][./././././././.]</font></div><div><font face="monospace, monospace">Process    7 of    8 is on borg01z205</font></div><div><font face="monospace, monospace">Process    5 of    8 is on borg01z205</font></div><div><font face="monospace, monospace">Process    2 of    8 is on borg01z205</font></div><div><font face="monospace, monospace">Process    3 of    8 is on borg01z205</font></div><div><font face="monospace, monospace">Process    4 of    8 is on borg01z205</font></div><div><font face="monospace, monospace">Process    6 of    8 is on borg01z205</font></div><div><font face="monospace, monospace">Process    0 of    8 is on borg01z205</font></div><div><font face="monospace, monospace">Process    1 of    8 is on borg01z205</font></div></div><div><br></div><div>Great...but wrong socket! Is there a way to tell it to use Socket 1 instead? </div><div><br></div><div>Note I&#39;ll be running under SLURM, so I will only have 8 processes per node, so it shouldn&#39;t need to use Socket 0.</div>-- <br><div><div dir="ltr"><div><div dir="ltr"><div>Matt Thompson</div></div></div><blockquote style="margin:0px 0px 0px 40px;border:none;padding:0px"><div><div><div>Man Among Men</div></div></div><div><div><div>Fulcrum of History</div></div></div></blockquote></div></div>
</div></div></div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/12/28190.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/12/28190.php</a></div></blockquote></div><br></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/12/28195.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/12/28195.php</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br><div class="gmail_signature"><div dir="ltr"><div><div dir="ltr"><div>Matt Thompson</div></div></div><blockquote style="margin:0 0 0 40px;border:none;padding:0px"><div><div><div>Man Among Men</div></div></div><div><div><div>Fulcrum of History</div></div></div></blockquote></div></div>
</div>

