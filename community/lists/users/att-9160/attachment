If you are running on a single node, then btl=openib,sm,self would be equivalent to btl=sm,self. OMPI is smart enough to know not to use IB if you are on a single node, and instead uses the shared memory subsystem.<br><br>
Are you saying that the inclusion of openib is causing a difference in behavior, even though all procs are on the same node??<br><br>Just want to ensure I understand the problem.<br><br>Thanks<br>Ralph<br><br><br><div class="gmail_quote">
On Fri, May 1, 2009 at 11:16 AM, Gus Correa <span dir="ltr">&lt;<a href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Hi OpenMPI and HPC experts<br>
<br>
This may or may not be the right forum to post this,<br>
and I am sorry to bother those that think it is not.<br>
<br>
I am trying to run the HPL benchmark on our cluster,<br>
compiling it with Gnu and linking to<br>
GotoBLAS (1.26) and OpenMPI (1.3.1),<br>
both also Gnu-compiled.<br>
<br>
I have got failures that suggest a memory leak when the<br>
problem size is large, but still within the memory limits<br>
recommended by HPL.<br>
The problem only happens when &quot;openib&quot; is among the OpenMPI<br>
MCA parameters (and the problem size is large).<br>
Any help is appreciated.<br>
<br>
Here is a description of what happens.<br>
<br>
For starters I am trying HPL on a single node, to get a feeling for<br>
the right parameters (N &amp; NB, P &amp; Q, etc) on dual-socked quad-core<br>
AMD Opteron 2376 &quot;Shanghai&quot;<br>
<br>
The HPL recommendation is to use close to 80% of your physical memory,<br>
to reach top Gigaflop performance.<br>
Our physical memory on a node is 16GB, and this gives a problem size<br>
N=40,000 to keep the 80% memory use.<br>
I tried several block sizes, somewhat correlated to the size of the<br>
processor cache:  NB=64 80 96 128 ...<br>
<br>
When I run HPL with N=20,000 or smaller all works fine,<br>
and the HPL run completes, regardless of whether &quot;openib&quot;<br>
is present or not on my MCA parameters.<br>
<br>
However, moving when I move N=40,000, or even N=35,000,<br>
the run starts OK with NB=64,<br>
but as NB is switched to larger values<br>
the total memory use increases in jumps (as shown by Ganglia),<br>
and becomes uneven across the processors (as shown by &quot;top&quot;).<br>
The problem happens if &quot;openib&quot; is among the MCA parameters,<br>
but doesn&#39;t happen if I remove &quot;openib&quot; from the MCA list and use<br>
only &quot;sm,self&quot;.<br>
<br>
For N=35,000, when NB reaches 96 memory use is already above the physical limit<br>
(16GB), having increased from 12.5GB to over 17GB.<br>
For N=40,000 the problem happens even earlier, with NB=80.<br>
At this point memory swapping kicks in,<br>
and eventually the run dies with memory allocation errors:<br>
<br>
================================================================================<br>
T/V                N    NB     P     Q               Time   Gflops<br>
--------------------------------------------------------------------------------<br>
WR01L2L4       35000   128     8     1             539.66 5.297e+01<br>
--------------------------------------------------------------------------------<br>
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=        0.0043992 ...... PASSED<br>
HPL ERROR from process # 0, on line 172 of function HPL_pdtest:<br>
&gt;&gt;&gt; [7,0] Memory allocation failed for A, x and b. Skip. &lt;&lt;&lt;<br>
...<br>
<br>
***<br>
<br>
The code snippet that corresponds to HPL_pdest.c is this,<br>
although the leak is probably somewhere else:<br>
<br>
/*<br>
 * Allocate dynamic memory<br>
 */<br>
   vptr = (void*)malloc( ( (size_t)(ALGO-&gt;align) +<br>
                           (size_t)(mat.ld+1) * (size_t)(mat.nq) ) *<br>
                         sizeof(double) );<br>
   info[0] = (vptr == NULL); info[1] = myrow; info[2] = mycol;<br>
   (void) HPL_all_reduce( (void *)(info), 3, HPL_INT, HPL_max,<br>
                          GRID-&gt;all_comm );<br>
   if( info[0] != 0 )<br>
   {<br>
      if( ( myrow == 0 ) &amp;&amp; ( mycol == 0 ) )<br>
         HPL_pwarn( TEST-&gt;outfp, __LINE__, &quot;HPL_pdtest&quot;,<br>
                    &quot;[%d,%d] %s&quot;, info[1], info[2],<br>
                    &quot;Memory allocation failed for A, x and b. Skip.&quot; );<br>
      (TEST-&gt;kskip)++;<br>
      return;<br>
   }<br>
<br>
***<br>
<br>
I found this continued increase in memory use rather strange,<br>
and suggestive of a memory leak in one of the codes being used.<br>
<br>
Everything (OpenMPI, GotoBLAS, and HPL)<br>
was compiled using Gnu only (gcc, gfortran, g++).<br>
<br>
I haven&#39;t changed anything on the compiler&#39;s memory model,<br>
i.e., I haven&#39;t used or changed the &quot;-mcmodel&quot; flag of gcc<br>
(I don&#39;t know if the Makefiles on HPL, GotoBLAS, and OpenMPI use it.)<br>
<br>
No additional load is present on the node,<br>
other than the OS (Linux CentOS 5.2), HPL is running alone.<br>
<br>
The cluster has Infiniband.<br>
However, I am running on a single node.<br>
<br>
The surprising thing is that if I run on shared memory only<br>
(-mca btl sm,self) there is no memory problem,<br>
the memory use is stable at about 13.9GB,<br>
and the run completes.<br>
So, there is a way around to run on a single node.<br>
(Actually shared memory is presumably the way to go on a single node.)<br>
<br>
However, if I introduce IB (-mca btl openib,sm,self)<br>
among the MCA btl parameters, then memory use blows up.<br>
<br>
This is bad news for me, because I want to extend the experiment<br>
to run HPL also across the whole cluster using IB,<br>
which is actually the ultimate goal of HPL, of course!<br>
It also suggests that the problem is somehow related to Infiniband,<br>
maybe hidden under OpenMPI.<br>
<br>
Here is the mpiexec command I use (with and without openib):<br>
<br>
/path/to/openmpi/bin/mpiexec \<br>
        -prefix /the/run/directory \<br>
        -np 8 \<br>
        -mca btl [openib,]sm,self \<br>
        xhpl<br>
<br>
<br>
Any help, insights, suggestions, reports of previous experiences,<br>
are much appreciated.<br>
<br>
Thank you,<br><font color="#888888">
Gus Correa<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</font></blockquote></div><br>

