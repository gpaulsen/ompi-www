Thank you Jeff for your opinion. It was really helpful.<br><br>Concerning reduce operation in case of small messages: it is possible to wrap also a reduction operator<br>and make it work with wrapped data. This operator could reduce only the original data and simply collect the piggybacked data (instead of reducing it). And as you say for big messages, I could be more appropriate to send separate messages asynchronously to root. 
<br><br>Thanks again,<br>--Oleg<br><br><div><span class="gmail_quote">On 11/1/07, <b class="gmail_sendername">Jeff Squyres</b> &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:</span><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
On Oct 31, 2007, at 5:52 PM, Oleg Morajko wrote:<br><br>&gt; Let me clarify the context of the problem. I&#39;m implementing a MPI<br>&gt; piggyback mechanism that should allow for attaching extra data to<br>&gt; any MPI message. The idea is to wrap MPI communication calls with
<br>&gt; PMPI interface (or with dynamic instrumentation or whatsoever) and<br>&gt; add/receive extra data in a non expensive way. The best solution I<br>&gt; have found so far is dynamic datatype wrapping. That is when a user
<br>&gt; calls MPI_Send (datatype, count) I create dynamically a new<br>&gt; structure type that contains an array [count] of datatype and extra<br>&gt; data. To avoid copying the original send buffer I use absolute<br>&gt; addresses to define displacaments in the structure. This works fine
<br>&gt; for all P2P calls and MPI_Bcast. And definitely it has performance<br>&gt; benefits when compared to copying bufferers or sending an<br>&gt; additional message in a different communicator. Or would you expect<br>
&gt; something different?<br>&gt;<br>&gt; The only problem are collective calls like MPI_Gather when a root<br>&gt; process receives an array of data items. There is no problem to<br>&gt; wrap the message on the sender side (for each task), but the
<br>&gt; question is how to define a datatype that points both to original<br>&gt; receive buffer and extra buffer for piggybacked data AND has an<br>&gt; adecuate extent to work as an array element.<br>&gt;<br>&gt; The real problem is that a structure datatype { original data,
<br>&gt; extra data} does not have a constant displacement between the<br>&gt; original data and extra data. Eg. consider original data = receive<br>&gt; buffer in MPI_Gather and extra data is an array of ints somewhere<br>
&gt; in memory). So it cannot be directly used as an array datatype.<br><br>I guess I don&#39;t see why this is a problem...?&nbsp;&nbsp;If you&#39;re already<br>making a specific datatype for this communication, MPI&#39;s datatype
<br>primitives are flexible enough to allow what you describe.&nbsp;&nbsp;Keep in<br>mind that you can nest datatypes (e.g., with TYPE_CREATE_STRUCT).<br><br>But for collectives, I think you need to decide exactly what<br>information you want to generate / save.&nbsp;&nbsp;Specifically, if you&#39;re
<br>piggybacking on collectives, you are stuck using the same<br>communication pattern as that collective.&nbsp;&nbsp;I.e., if the application<br>calls MPI_REDUCE with MPI_SUM, I imagine you&#39;ll have a difficult time<br>piggybacking your data on that reduction without it being summed
<br>across all the processes.<br><br>There are a few other canonical solutions to the &quot;need to save extra<br>data about every communication&quot; technique:<br><br>- for small messages, do what you&#39;re doing: a) make a new/specific
<br>datatype for p2p messages or b) memcpy the user+extra data into a<br>small contiguous buffer and then just send that (and memcpy out on<br>the receiver).&nbsp;&nbsp;If making datatypes is cheap in MPI, then a) is<br>effectively the same as b), and potentially more optimized/tuned.
<br><br>- for large messages, don&#39;t bother making a new datatype -- just send<br>around another message with your extra data.&nbsp;&nbsp;The performance impact<br>will be minimal because it&#39;s already a long message; don&#39;t force the
<br>MPI do to additional copies with a non-contiguous datatype if you can<br>avoid it.<br><br>- for collectives, if you can&#39;t piggyback (e.g., REDUCE with SUM and<br>others), just send around another short message.&nbsp;&nbsp;Yes, you&#39;ll take a
<br>performance hit for this.<br><br>- depending on what data you&#39;re piggybacking / collecting, it may be<br>possible to implement a &quot;lazy&quot; collection scheme in the meta/PMPI<br>layer.&nbsp;&nbsp;E.g., for when you send separate messages with your meta
<br>data, always use non-blocking sends.&nbsp;&nbsp;The receiver PMPI layer can<br>lazily collect this data and match it with application sends/receives<br>after the fact (i.e., don&#39;t be trapped into thinking that you have to<br>
do the match exactly when the application data is actually sent or<br>received -- it could be done after that).<br><br>Hope that helps.<br><br><br>&gt; Any solution? It could be complex, I don&#39;t mind ;)<br>&gt;<br>&gt;
<br>&gt; On 11/1/07, George Bosilca &lt;<a href="mailto:bosilca@eecs.utk.edu">bosilca@eecs.utk.edu</a>&gt; wrote: The MPI<br>&gt; standard defines the upper bound and the upper bound for<br>&gt; similar problems. However, even with all the functions in the MPI
<br>&gt; standard we cannot describe all types of data. There is always a<br>&gt; solution, but sometimes one has to ask if the performance gain is<br>&gt; worth the complexity introduced.<br>&gt;<br>&gt;<br>&gt; As I said there is always a solution. In fact there are 2 solution,
<br>&gt; one somehow optimal the other ... as bad as you can imagine.<br>&gt;<br>&gt; The bad approach:<br>&gt;&nbsp;&nbsp; 1. Use an MPI_Type_struct to create exactly what you want, element<br>&gt; by element (i.e single pair). This can work in all cases.&nbsp;&nbsp;2. If
<br>&gt; the sizeof(int) == sizeof(double) then the displacement inside<br>&gt; each tuple (double_i, int_i) is constant. Therefore, you can start by<br>&gt; creating one &quot;single element&quot; type and then use for each send the
<br>&gt; correct displacement in the array (added to the send buffer,<br>&gt; respectively to the receive one).<br>&gt;<br>&gt;&nbsp;&nbsp;&nbsp;&nbsp;george.<br>&gt;<br>&gt; On Oct 31, 2007, at 1:40 PM, Oleg Morajko wrote:<br>&gt;<br>&gt; &gt; Hello,
<br>&gt; &gt;<br>&gt; &gt; I have the following problem. There areI two arrays somewere in the<br>&gt; &gt; program:<br>&gt; &gt;<br>&gt; &gt; double weights [MAX_SIZE];<br>&gt; &gt; ...<br>&gt; &gt; int&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; values [MAX_SIZE];
<br>&gt; &gt; ...<br>&gt; &gt;<br>&gt; &gt; I need to be able to send a single pair { weights [i], values [i] }<br>&gt; &gt; with a single MPI_Send call Or receive it directly into both arrays<br>&gt; &gt; at at given index i. How can I define a datatype that spans this
<br>&gt; &gt; pair over both arrays?<br>&gt; &gt;<br>&gt; &gt; The only additional constraint it the fact that the memory location<br>&gt; &gt; of both arrays is fixed and cannot be changed and I should avoid<br>&gt; &gt; extra copies.
<br>&gt; &gt;<br>&gt; &gt; Is it possible?<br>&gt; &gt;<br>&gt; &gt; Any help welcome,<br>&gt; &gt; Oleg Morajko<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt; _______________________________________________<br>&gt; &gt; users mailing list
<br>&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<br>&gt;<br>
&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users
</a><br>&gt;<br>&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><br>--<br>Jeff Squyres<br>Cisco Systems<br><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org
</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

