<html><head><meta http-equiv="Content-Type" content="text/html charset=us-ascii"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">I think I begin to grok at least part of the problem. If you are assigning different cpus on each node, then you'll need to tell us that by setting --hetero-nodes otherwise we won't have any way to report that back to mpirun for its binding calculation.<div><br></div><div>Otherwise, we expect that the cpuset of the first node we launch a daemon onto (or where mpirun is executing, if we are only launching local to mpirun) accurately represents the cpuset on every node in the allocation.</div><div><br></div><div>We still might well have a bug in our binding computation - but the above will definitely impact what you said the user did.</div><div><br><div><div>On Jun 20, 2014, at 10:06 AM, Brock Palen &lt;<a href="mailto:brockp@umich.edu">brockp@umich.edu</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;">Extra data point if I do:<br><br>[brockp@nyx5508 34241]$ mpirun --report-bindings --bind-to core hostname<br>--------------------------------------------------------------------------<br>A request was made to bind to that would result in binding more<br>processes than cpus on a resource:<br><br>&nbsp;&nbsp;Bind to: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CORE<br>&nbsp;&nbsp;Node: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nyx5513<br>&nbsp;&nbsp;#processes: &nbsp;2<br>&nbsp;&nbsp;#cpus: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1<br><br>You can override this protection by adding the "overload-allowed"<br>option to your binding directive.<br>--------------------------------------------------------------------------<br><br>[brockp@nyx5508 34241]$ mpirun -H nyx5513 uptime<br>13:01:37 up 31 days, 23:06, &nbsp;0 users, &nbsp;load average: 10.13, 10.90, 12.38<br>13:01:37 up 31 days, 23:06, &nbsp;0 users, &nbsp;load average: 10.13, 10.90, 12.38<br>[brockp@nyx5508 34241]$ mpirun -H nyx5513 --bind-to core hwloc-bind --get<br>0x00000010<br>0x00001000<br>[brockp@nyx5508 34241]$ cat $PBS_NODEFILE | grep nyx5513<br>nyx5513<br>nyx5513<br><br>Interesting, if I force bind to core, MPI barfs saying there is only 1 cpu available, PBS says it gave it two, and if I force (this is all inside an interactive job) just on that node hwloc-bind --get I get what I expect,<br><br>Is there a way to get a map of what MPI thinks it has on each host?<br><br>Brock Palen<br><a href="http://www.umich.edu/~brockp">www.umich.edu/~brockp</a><br>CAEN Advanced Computing<br>XSEDE Campus Champion<br><a href="mailto:brockp@umich.edu">brockp@umich.edu</a><br>(734)936-1985<br><br><br><br>On Jun 20, 2014, at 12:38 PM, Brock Palen &lt;<a href="mailto:brockp@umich.edu">brockp@umich.edu</a>&gt; wrote:<br><br><blockquote type="cite">I was able to produce it in my test.<br><br>orted affinity set by cpuset:<br>[root@nyx5874 ~]# hwloc-bind --get --pid 103645<br>0x0000c002<br><br>This mask (1, 14,15) which is across sockets, matches the cpu set setup by the batch system.<span class="Apple-converted-space">&nbsp;</span><br>[root@nyx5874 ~]# cat /dev/cpuset/torque/12719806.<a href="http://nyx.engin.umich.edu/cpus">nyx.engin.umich.edu/cpus</a><span class="Apple-converted-space">&nbsp;</span><br>1,14-15<br><br>The ranks though were then all set to the same core:<br><br>[root@nyx5874 ~]# hwloc-bind --get --pid 103871<br>0x00008000<br>[root@nyx5874 ~]# hwloc-bind --get --pid 103872<br>0x00008000<br>[root@nyx5874 ~]# hwloc-bind --get --pid 103873<br>0x00008000<br><br>Which is core 15:<br><br>report-bindings gave me:<br>You can see how a few nodes were bound to all the same core, the last one in each case. &nbsp;I only gave you the results for the hose nyx5874.<br><br>[<a href="http://nyx5526.engin.umich.edu">nyx5526.engin.umich.edu</a>:23726] MCW rank 0 is not bound (or bound to all available processors)<br>[<a href="http://nyx5878.engin.umich.edu">nyx5878.engin.umich.edu</a>:103925] MCW rank 8 is not bound (or bound to all available processors)<br>[<a href="http://nyx5533.engin.umich.edu">nyx5533.engin.umich.edu</a>:123988] MCW rank 1 is not bound (or bound to all available processors)<br>[<a href="http://nyx5879.engin.umich.edu">nyx5879.engin.umich.edu</a>:102808] MCW rank 9 is not bound (or bound to all available processors)<br>[<a href="http://nyx5874.engin.umich.edu">nyx5874.engin.umich.edu</a>:103645] MCW rank 41 bound to socket 1[core 15[hwt 0]]: [./././././././.][./././././././B]<br>[<a href="http://nyx5874.engin.umich.edu">nyx5874.engin.umich.edu</a>:103645] MCW rank 42 bound to socket 1[core 15[hwt 0]]: [./././././././.][./././././././B]<br>[<a href="http://nyx5874.engin.umich.edu">nyx5874.engin.umich.edu</a>:103645] MCW rank 43 bound to socket 1[core 15[hwt 0]]: [./././././././.][./././././././B]<br>[<a href="http://nyx5888.engin.umich.edu">nyx5888.engin.umich.edu</a>:117400] MCW rank 11 is not bound (or bound to all available processors)<br>[<a href="http://nyx5786.engin.umich.edu">nyx5786.engin.umich.edu</a>:30004] MCW rank 19 bound to socket 1[core 15[hwt 0]]: [./././././././.][./././././././B]<br>[<a href="http://nyx5786.engin.umich.edu">nyx5786.engin.umich.edu</a>:30004] MCW rank 18 bound to socket 1[core 15[hwt 0]]: [./././././././.][./././././././B]<br>[<a href="http://nyx5594.engin.umich.edu">nyx5594.engin.umich.edu</a>:33884] MCW rank 24 bound to socket 1[core 15[hwt 0]]: [./././././././.][./././././././B]<br>[<a href="http://nyx5594.engin.umich.edu">nyx5594.engin.umich.edu</a>:33884] MCW rank 25 bound to socket 1[core 15[hwt 0]]: [./././././././.][./././././././B]<br>[<a href="http://nyx5594.engin.umich.edu">nyx5594.engin.umich.edu</a>:33884] MCW rank 26 bound to socket 1[core 15[hwt 0]]: [./././././././.][./././././././B]<br>[<a href="http://nyx5798.engin.umich.edu">nyx5798.engin.umich.edu</a>:53026] MCW rank 59 bound to socket 1[core 15[hwt 0]]: [./././././././.][./././././././B]<br>[<a href="http://nyx5798.engin.umich.edu">nyx5798.engin.umich.edu</a>:53026] MCW rank 60 bound to socket 1[core 15[hwt 0]]: [./././././././.][./././././././B]<br>[<a href="http://nyx5798.engin.umich.edu">nyx5798.engin.umich.edu</a>:53026] MCW rank 56 bound to socket 1[core 15[hwt 0]]: [./././././././.][./././././././B]<br>[<a href="http://nyx5798.engin.umich.edu">nyx5798.engin.umich.edu</a>:53026] MCW rank 57 bound to socket 1[core 15[hwt 0]]: [./././././././.][./././././././B]<br>[<a href="http://nyx5798.engin.umich.edu">nyx5798.engin.umich.edu</a>:53026] MCW rank 58 bound to socket 1[core 15[hwt 0]]: [./././././././.][./././././././B]<br>[<a href="http://nyx5545.engin.umich.edu">nyx5545.engin.umich.edu</a>:88170] MCW rank 2 is not bound (or bound to all available processors)<br>[<a href="http://nyx5613.engin.umich.edu">nyx5613.engin.umich.edu</a>:25229] MCW rank 31 is not bound (or bound to all available processors)<br>[<a href="http://nyx5880.engin.umich.edu">nyx5880.engin.umich.edu</a>:01406] MCW rank 10 is not bound (or bound to all available processors)<br>[<a href="http://nyx5770.engin.umich.edu">nyx5770.engin.umich.edu</a>:86538] MCW rank 6 is not bound (or bound to all available processors)<br>[<a href="http://nyx5613.engin.umich.edu">nyx5613.engin.umich.edu</a>:25228] MCW rank 30 is not bound (or bound to all available processors)<br>[<a href="http://nyx5577.engin.umich.edu">nyx5577.engin.umich.edu</a>:65949] MCW rank 4 is not bound (or bound to all available processors)<br>[<a href="http://nyx5607.engin.umich.edu">nyx5607.engin.umich.edu</a>:30379] MCW rank 14 is not bound (or bound to all available processors)<br>[<a href="http://nyx5544.engin.umich.edu">nyx5544.engin.umich.edu</a>:72960] MCW rank 47 is not bound (or bound to all available processors)<br>[<a href="http://nyx5544.engin.umich.edu">nyx5544.engin.umich.edu</a>:72959] MCW rank 46 is not bound (or bound to all available processors)<br>[<a href="http://nyx5848.engin.umich.edu">nyx5848.engin.umich.edu</a>:04332] MCW rank 33 is not bound (or bound to all available processors)<br>[<a href="http://nyx5848.engin.umich.edu">nyx5848.engin.umich.edu</a>:04333] MCW rank 34 is not bound (or bound to all available processors)<br>[<a href="http://nyx5544.engin.umich.edu">nyx5544.engin.umich.edu</a>:72958] MCW rank 45 is not bound (or bound to all available processors)<br>[<a href="http://nyx5858.engin.umich.edu">nyx5858.engin.umich.edu</a>:12165] MCW rank 35 is not bound (or bound to all available processors)<br>[<a href="http://nyx5607.engin.umich.edu">nyx5607.engin.umich.edu</a>:30380] MCW rank 15 is not bound (or bound to all available processors)<br>[<a href="http://nyx5544.engin.umich.edu">nyx5544.engin.umich.edu</a>:72957] MCW rank 44 is not bound (or bound to all available processors)<br>[<a href="http://nyx5858.engin.umich.edu">nyx5858.engin.umich.edu</a>:12167] MCW rank 37 is not bound (or bound to all available processors)<br>[<a href="http://nyx5870.engin.umich.edu">nyx5870.engin.umich.edu</a>:33811] MCW rank 7 is not bound (or bound to all available processors)<br>[<a href="http://nyx5582.engin.umich.edu">nyx5582.engin.umich.edu</a>:81994] MCW rank 5 is not bound (or bound to all available processors)<br>[<a href="http://nyx5848.engin.umich.edu">nyx5848.engin.umich.edu</a>:04331] MCW rank 32 is not bound (or bound to all available processors)<br>[<a href="http://nyx5557.engin.umich.edu">nyx5557.engin.umich.edu</a>:46654] MCW rank 50 is not bound (or bound to all available processors)<br>[<a href="http://nyx5858.engin.umich.edu">nyx5858.engin.umich.edu</a>:12166] MCW rank 36 is not bound (or bound to all available processors)<br>[<a href="http://nyx5799.engin.umich.edu">nyx5799.engin.umich.edu</a>:67802] MCW rank 22 is not bound (or bound to all available processors)<br>[<a href="http://nyx5799.engin.umich.edu">nyx5799.engin.umich.edu</a>:67803] MCW rank 23 is not bound (or bound to all available processors)<br>[<a href="http://nyx5556.engin.umich.edu">nyx5556.engin.umich.edu</a>:50889] MCW rank 3 is not bound (or bound to all available processors)<br>[<a href="http://nyx5625.engin.umich.edu">nyx5625.engin.umich.edu</a>:95931] MCW rank 53 is not bound (or bound to all available processors)<br>[<a href="http://nyx5625.engin.umich.edu">nyx5625.engin.umich.edu</a>:95930] MCW rank 52 is not bound (or bound to all available processors)<br>[<a href="http://nyx5557.engin.umich.edu">nyx5557.engin.umich.edu</a>:46655] MCW rank 51 is not bound (or bound to all available processors)<br>[<a href="http://nyx5625.engin.umich.edu">nyx5625.engin.umich.edu</a>:95932] MCW rank 54 is not bound (or bound to all available processors)<br>[<a href="http://nyx5625.engin.umich.edu">nyx5625.engin.umich.edu</a>:95933] MCW rank 55 is not bound (or bound to all available processors)<br>[<a href="http://nyx5866.engin.umich.edu">nyx5866.engin.umich.edu</a>:16306] MCW rank 40 is not bound (or bound to all available processors)<br>[<a href="http://nyx5861.engin.umich.edu">nyx5861.engin.umich.edu</a>:22761] MCW rank 61 is not bound (or bound to all available processors)<br>[<a href="http://nyx5861.engin.umich.edu">nyx5861.engin.umich.edu</a>:22762] MCW rank 62 is not bound (or bound to all available processors)<br>[<a href="http://nyx5861.engin.umich.edu">nyx5861.engin.umich.edu</a>:22763] MCW rank 63 is not bound (or bound to all available processors)<br>[<a href="http://nyx5557.engin.umich.edu">nyx5557.engin.umich.edu</a>:46652] MCW rank 48 is not bound (or bound to all available processors)<br>[<a href="http://nyx5557.engin.umich.edu">nyx5557.engin.umich.edu</a>:46653] MCW rank 49 is not bound (or bound to all available processors)<br>[<a href="http://nyx5866.engin.umich.edu">nyx5866.engin.umich.edu</a>:16304] MCW rank 38 is not bound (or bound to all available processors)<br>[<a href="http://nyx5788.engin.umich.edu">nyx5788.engin.umich.edu</a>:02465] MCW rank 20 is not bound (or bound to all available processors)<br>[<a href="http://nyx5597.engin.umich.edu">nyx5597.engin.umich.edu</a>:68071] MCW rank 27 is not bound (or bound to all available processors)<br>[<a href="http://nyx5775.engin.umich.edu">nyx5775.engin.umich.edu</a>:27952] MCW rank 17 is not bound (or bound to all available processors)<br>[<a href="http://nyx5866.engin.umich.edu">nyx5866.engin.umich.edu</a>:16305] MCW rank 39 is not bound (or bound to all available processors)<br>[<a href="http://nyx5788.engin.umich.edu">nyx5788.engin.umich.edu</a>:02466] MCW rank 21 is not bound (or bound to all available processors)<br>[<a href="http://nyx5775.engin.umich.edu">nyx5775.engin.umich.edu</a>:27951] MCW rank 16 is not bound (or bound to all available processors)<br>[<a href="http://nyx5597.engin.umich.edu">nyx5597.engin.umich.edu</a>:68073] MCW rank 29 is not bound (or bound to all available processors)<br>[<a href="http://nyx5597.engin.umich.edu">nyx5597.engin.umich.edu</a>:68072] MCW rank 28 is not bound (or bound to all available processors)<br>[<a href="http://nyx5552.engin.umich.edu">nyx5552.engin.umich.edu</a>:30481] MCW rank 12 is not bound (or bound to all available processors)<br>[<a href="http://nyx5552.engin.umich.edu">nyx5552.engin.umich.edu</a>:30482] MCW rank 13 is not bound (or bound to all available processors)<br><br><br>Brock Palen<br><a href="http://www.umich.edu/~brockp">www.umich.edu/~brockp</a><br>CAEN Advanced Computing<br>XSEDE Campus Champion<br>brockp@umich.edu<br>(734)936-1985<br><br><br><br>On Jun 20, 2014, at 12:20 PM, Brock Palen &lt;brockp@umich.edu&gt; wrote:<br><br><blockquote type="cite">Got it,<br><br>I have the input from the user and am testing it out.<br><br>It probably has less todo with torque and more cpuset's,<span class="Apple-converted-space">&nbsp;</span><br><br>I'm working on producing it myself also.<br><br>Brock Palen<br>www.umich.edu/~brockp<br>CAEN Advanced Computing<br>XSEDE Campus Champion<br>brockp@umich.edu<br>(734)936-1985<br><br><br><br>On Jun 20, 2014, at 12:18 PM, Ralph Castain &lt;rhc@open-mpi.org&gt; wrote:<br><br><blockquote type="cite">Thanks - I'm just trying to reproduce one problem case so I can look at it. Given that I don't have access to a Torque machine, I need to "fake" it.<br><br><br>On Jun 20, 2014, at 9:15 AM, Brock Palen &lt;brockp@umich.edu&gt; wrote:<br><br><blockquote type="cite">In this case they are a single socket, but as you can see they could be ether/or depending on the job.<br><br>Brock Palen<br>www.umich.edu/~brockp<br>CAEN Advanced Computing<br>XSEDE Campus Champion<br>brockp@umich.edu<br>(734)936-1985<br><br><br><br>On Jun 19, 2014, at 2:44 PM, Ralph Castain &lt;rhc@open-mpi.org&gt; wrote:<br><br><blockquote type="cite">Sorry, I should have been clearer - I was asking if cores 8-11 are all on one socket, or span multiple sockets<br><br><br>On Jun 19, 2014, at 11:36 AM, Brock Palen &lt;brockp@umich.edu&gt; wrote:<br><br><blockquote type="cite">Ralph,<br><br>It was a large job spread across. &nbsp;Our system allows users to ask for 'procs' which are laid out in any format.<span class="Apple-converted-space">&nbsp;</span><br><br>The list:<br><br><blockquote type="cite">[nyx5406:2][nyx5427:2][nyx5506:2][nyx5311:3]<br>[nyx5329:4][nyx5398:4][nyx5396:11][nyx5397:11]<br>[nyx5409:11][nyx5411:11][nyx5412:3]<br></blockquote><br>Shows that nyx5406 had 2 cores, &nbsp;nyx5427 also 2, &nbsp;nyx5411 had 11.<br><br>They could be spread across any number of sockets configuration. &nbsp;We start very lax "user requests X procs" and then the user can request more strict requirements from there. &nbsp;We support mostly serial users, and users can colocate on nodes.<br><br>That is good to know, I think we would want to turn our default to 'bind to core' except for our few users who use hybrid mode.<br><br>Our CPU set tells you what cores the job is assigned. &nbsp;So in the problem case provided, the cpuset/cgroup shows only cores 8-11 are available to this job on this node.<br><br>Brock Palen<br>www.umich.edu/~brockp<br>CAEN Advanced Computing<br>XSEDE Campus Champion<br>brockp@umich.edu<br>(734)936-1985<br><br><br><br>On Jun 18, 2014, at 11:10 PM, Ralph Castain &lt;rhc@open-mpi.org&gt; wrote:<br><br><blockquote type="cite">The default binding option depends on the number of procs - it is bind-to core for np=2, and bind-to socket for np &gt; 2. You never said, but should I assume you ran 4 ranks? If so, then we should be trying to bind-to socket.<br><br>I'm not sure what your cpuset is telling us - are you binding us to a socket? Are some cpus in one socket, and some in another?<br><br>It could be that the cpuset + bind-to socket is resulting in some odd behavior, but I'd need a little more info to narrow it down.<br><br><br>On Jun 18, 2014, at 7:48 PM, Brock Palen &lt;brockp@umich.edu&gt; wrote:<br><br><blockquote type="cite">I have started using 1.8.1 for some codes (meep in this case) and it sometimes works fine, but in a few cases I am seeing ranks being given overlapping CPU assignments, not always though.<br><br>Example job, default binding options (so by-core right?):<br><br>Assigned nodes, the one in question is nyx5398, we use torque CPU sets, and use TM to spawn.<br><br>[nyx5406:2][nyx5427:2][nyx5506:2][nyx5311:3]<br>[nyx5329:4][nyx5398:4][nyx5396:11][nyx5397:11]<br>[nyx5409:11][nyx5411:11][nyx5412:3]<br><br>[root@nyx5398 ~]# hwloc-bind --get --pid 16065<br>0x00000200<br>[root@nyx5398 ~]# hwloc-bind --get --pid 16066<br>0x00000800<br>[root@nyx5398 ~]# hwloc-bind --get --pid 16067<br>0x00000200<br>[root@nyx5398 ~]# hwloc-bind --get --pid 16068<br>0x00000800<br><br>[root@nyx5398 ~]# cat /dev/cpuset/torque/12703230.nyx.engin.umich.edu/cpus<span class="Apple-converted-space">&nbsp;</span><br>8-11<br><br>So torque claims the CPU set setup for the job has 4 cores, but as you can see the ranks were giving identical binding.<span class="Apple-converted-space">&nbsp;</span><br><br>I checked the pids they were part of the correct CPU set, I also checked, orted:<br><br>[root@nyx5398 ~]# hwloc-bind --get --pid 16064<br>0x00000f00<br>[root@nyx5398 ~]# hwloc-calc --intersect PU 16064<br>ignored unrecognized argument 16064<br><br>[root@nyx5398 ~]# hwloc-calc --intersect PU 0x00000f00<br>8,9,10,11<br><br>Which is exactly what I would expect.<br><br>So ummm, i'm lost why this might happen? &nbsp;What else should I check? &nbsp;Like I said not all jobs show this behavior.<br><br>Brock Palen<br>www.umich.edu/~brockp<br>CAEN Advanced Computing<br>XSEDE Campus Champion<br>brockp@umich.edu<br>(734)936-1985<br><br><br><br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post: http://www.open-mpi.org/community/lists/users/2014/06/24672.php<br></blockquote><br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post: http://www.open-mpi.org/community/lists/users/2014/06/24673.php<br></blockquote><br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post: http://www.open-mpi.org/community/lists/users/2014/06/24675.php<br></blockquote><br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post: http://www.open-mpi.org/community/lists/users/2014/06/24676.php<br></blockquote><br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post: http://www.open-mpi.org/community/lists/users/2014/06/24677.php<br></blockquote><br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post: http://www.open-mpi.org/community/lists/users/2014/06/24678.php<br></blockquote><br></blockquote><br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>Link to this post:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/06/24681.php">http://www.open-mpi.org/community/lists/users/2014/06/24681.php</a></div></blockquote></div><br></div></body></html>
