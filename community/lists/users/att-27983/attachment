<div dir="ltr">This is what I do to successfully get the best performance for my application using OpenMP and OpenMPI:<div><br></div><div>(note this is for 8 cores per socket)</div><div><br></div><div>mpirun -x OMP_PROC_BIND=true --report-bindings -x OMP_NUM_THREADS=8 --map-by ppr:1:socket:pe=8 &lt;exec&gt;</div><div><br></div><div>It assigns 8 cores per MPI processor separated by sockets, each MPI processor gets 8 threads and I bind the OpenMP threads for affinity. This is for OpenMPI &gt;= 1.8.</div><div><br></div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">2015-11-03 18:52 GMT+01:00 Dave Love <span dir="ltr">&lt;<a href="mailto:d.love@liverpool.ac.uk" target="_blank">d.love@liverpool.ac.uk</a>&gt;</span>:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><span class="">Lloyd Brown &lt;<a href="mailto:lloyd_brown@byu.edu">lloyd_brown@byu.edu</a>&gt; writes:<br>
<br>
&gt; No problem.  It wasn&#39;t much of a delay.<br>
&gt;<br>
&gt; The scenario involves a combination of MPI and OpenMP (or other<br>
&gt; threading scheme).  Basically, the software will launch one or more<br>
&gt; processes via MPI, which then spawn threads to do the work.<br>
&gt;<br>
&gt; What we&#39;ve been seeing is that, without something like &#39;--bind-to none&#39;<br>
&gt; or similar, those threads end up being pinned to the same processor as<br>
&gt; the process that spawned them.<br>
<br>
</span>The default binding is supposed to be to sockets, as --report-bindings<br>
should show.  Otherwise see another message I just posted to for an<br>
empirical test (and possibly examples in the tutorials referenced -- I<br>
don&#39;t remember).<br>
<span class=""><br>
&gt; We&#39;re okay with a bind=none, since we already have cgroups in place to<br>
&gt; constrain the user to the resources they request.  We might get more<br>
&gt; process/thread migration between processors (but within the cgroup) than<br>
&gt; we would like, but that&#39;s still probably acceptable in this scenario.<br>
&gt;<br>
&gt; If there&#39;s a better solution, we&#39;d love to hear it.<br>
<br>
</span>--cpus-per-proc, or whatever the non-deprecated version is in mpirun(1).<br>
[You needed --loadbalance in OMPI 1.6 to make that work.]<br>
<br>
You might also like to supply environment variables to get the OpenMP<br>
runtime to DTRT for thread affinity, if it doesn&#39;t; there isn&#39;t an OMPI<br>
mechanism for that but you can do it with a wrapper or simple LD_PRELOAD<br>
library.<br>
<span class="">_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</span>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/11/27982.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/11/27982.php</a><br>
</blockquote></div><br><br clear="all"><div><br></div>-- <br><div class="gmail_signature"><div dir="ltr"><div>Kind regards Nick</div></div></div>
</div>

