<html>
<head>
<style><!--
.hmmessage P
{
margin:0px;
padding:0px
}
body.hmmessage
{
font-size: 12pt;
font-family:Calibri
}
--></style></head>
<body class='hmmessage'><div dir='ltr'>Hello guys<div><br></div><div>I used the command&nbsp;</div><div><br></div><div>ulimit -s unlimited</div><div><br></div><div>and got&nbsp;</div><div><br></div><div><div>stack size &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(kbytes, -s) unlimited</div><div><br></div><div>but when I ran the program got the same error. So I used t<span style="font-size: 12pt;">he gdb debugger, I compiled using&nbsp;</span></div><div><span style="font-size: 12pt;"><br></span></div><div><div style="font-size: 12pt;">mpif90 -g -o mpivfsa_versao2.f &nbsp;exe</div><div style="font-size: 12pt;"><br></div><div>I ran the program and then I ran gdb with&nbsp;<font size="3" style="background-color: rgb(255, 255, 255);"><span style="font-family: helvetica, arial, sans-serif;">both the executable and the core file name as argument</span><span style="font-family: helvetica, arial, sans-serif;">s</span></font><span style="font-size: 12pt;">&nbsp;and got the following</span></div><div><span style="font-size: 12pt;"><br></span></div><div><div style="font-size: 12pt;">Program received signal SIGSEGV, Segmentation fault.</div><div style="font-size: 12pt;">0x00002aaaab59b54c in free () from /lib/x86_64-linux-gnu/libc.so.6</div><div style="font-size: 12pt;">(gdb) backtrace</div><div style="font-size: 12pt;">#0 &nbsp;0x00002aaaab59b54c in free () from /lib/x86_64-linux-gnu/libc.so.6</div><div style="font-size: 12pt;">#1 &nbsp;0x0000000000406801 in inv_grav3d_vfsa () at mpivfsa_versao2.f:131</div><div style="font-size: 12pt;">#2 &nbsp;0x0000000000406b88 in main (argc=1, argv=0x7fffffffe387) at mpivfsa_versao2.f:9</div><div style="font-size: 12pt;">#3 &nbsp;0x00002aaaab53976d in __libc_start_main () from /lib/x86_64-linux-gnu/libc.so.6</div><div style="font-size: 12pt;">#4 &nbsp;0x0000000000401399 in _start ()</div><div style="font-size: 12pt;"><br></div><div style="font-size: 12pt;">These are the lines</div><div style="font-size: 12pt;"><br></div><div>9 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;use mpi</div></div><div>131&nbsp;&nbsp; &nbsp; &nbsp; deallocate(zv,xrec,yrec,xprm,yprm)</div><div><br></div><div>I think the problem is not memory, the problem is related to MPI</div><div style="font-size: 12pt;"><br></div></div><div><span style="font-family: Helvetica, Arial, 'lucida grande', tahoma, verdana, arial, sans-serif; line-height: 17.066667556762695px; white-space: pre-wrap; background-color: rgb(255, 255, 255);"><font size="3">Which could be the error?</font></span></div><br><u>Oscar Fabian Mojica Ladino</u><br><font style="font-size:8pt" size="1">Geologist M.S. in&nbsp; Geophysics</font><br><br><br><div>&gt; From: o_mojical@hotmail.com<br>&gt; Date: Wed, 16 Apr 2014 15:17:51 -0300<br>&gt; To: users@open-mpi.org<br>&gt; Subject: Re: [OMPI users] Where is the error? (MPI program in fortran)<br>&gt; <br>&gt; Gus<br>&gt; It is a single machine and i have installed Ubuntu 12.04 LTS. I left my computer in the college but  I will try to follow your advice when I can and tell you about it.<br>&gt; <br>&gt; Thanks <br>&gt; <br>&gt; Enviado desde mi iPad<br>&gt; <br>&gt; &gt; El 16/04/2014, a las 14:17, "Gus Correa" &lt;gus@ldeo.columbia.edu&gt; escribió:<br>&gt; &gt; <br>&gt; &gt; Hi Oscar<br>&gt; &gt; <br>&gt; &gt; This is a long shot, but maybe worth trying.<br>&gt; &gt; I am assuming you're using Linux, or some form or Unix, right?<br>&gt; &gt; <br>&gt; &gt; You may try to increase the stack size.<br>&gt; &gt; The default in Linux is often too small for large programs.<br>&gt; &gt; Sometimes this may cause a segmentation fault, even if the<br>&gt; &gt; program is correct.<br>&gt; &gt; <br>&gt; &gt; You can check what you have with:<br>&gt; &gt; <br>&gt; &gt; ulimit -a        (bash)<br>&gt; &gt; <br>&gt; &gt; or<br>&gt; &gt; <br>&gt; &gt; limit             (csh or tcsh)<br>&gt; &gt; <br>&gt; &gt; Then set it to a larger number or perhaps to unlimited,<br>&gt; &gt; e.g.:<br>&gt; &gt; <br>&gt; &gt; ulimit -s unlimited<br>&gt; &gt; <br>&gt; &gt; or<br>&gt; &gt; <br>&gt; &gt; limit stacksize unlimited<br>&gt; &gt; <br>&gt; &gt; You didn't say anything about the computer(s) you are using.<br>&gt; &gt; Is this a single machine, a cluster, something else?<br>&gt; &gt; <br>&gt; &gt; Anyway, resetting the statck size may depend a bit on what you<br>&gt; &gt; have in /etc/security/limits.conf,<br>&gt; &gt; and whether it allows you to increase the stack size.<br>&gt; &gt; If it is a single computer that you have root access, you may<br>&gt; &gt; do it yourself.<br>&gt; &gt; There are other limits worth increasing (number of open files,<br>&gt; &gt; max locked memory).<br>&gt; &gt; For instance, this could go in limits.conf:<br>&gt; &gt; <br>&gt; &gt; *   -   memlock     -1<br>&gt; &gt; *   -   stack       -1<br>&gt; &gt; *   -   nofile      4096<br>&gt; &gt; <br>&gt; &gt; See 'man limits.conf' for details.<br>&gt; &gt; <br>&gt; &gt; If it is a cluster, and this should be set on all nodes,<br>&gt; &gt; and you may need to ask your system administrator to do it.<br>&gt; &gt; <br>&gt; &gt; I hope this helps,<br>&gt; &gt; Gus Correa<br>&gt; &gt; <br>&gt; &gt;&gt; On 04/16/2014 11:24 AM, Gus Correa wrote:<br>&gt; &gt;&gt;&gt; On 04/16/2014 08:30 AM, Oscar Mojica wrote:<br>&gt; &gt;&gt;&gt; How would be the command line to compile with the option -g ? What<br>&gt; &gt;&gt;&gt; debugger can I use?<br>&gt; &gt;&gt;&gt; Thanks<br>&gt; &gt;&gt;&gt; <br>&gt; &gt;&gt; <br>&gt; &gt;&gt; Replace any optimization flags (-O2, or similar) by -g.<br>&gt; &gt;&gt; Check if your compiler has the -traceback flag or similar<br>&gt; &gt;&gt; (man compiler-name).<br>&gt; &gt;&gt; <br>&gt; &gt;&gt; The gdb debugger is normally available on Linux (or you can install it<br>&gt; &gt;&gt; with yum, apt-get, etc).  An alternative is ddd, with a GUI (can also be<br>&gt; &gt;&gt; installed from yum, etc).<br>&gt; &gt;&gt; If you use a commercial compiler you may have a debugger with a GUI.<br>&gt; &gt;&gt; <br>&gt; &gt;&gt;&gt; Enviado desde mi iPad<br>&gt; &gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt; El 15/04/2014, a las 18:20, "Gus Correa" &lt;gus@ldeo.columbia.edu&gt;<br>&gt; &gt;&gt;&gt;&gt; escribió:<br>&gt; &gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt; Or just compiling with -g or -traceback (depending on the compiler) will<br>&gt; &gt;&gt;&gt;&gt; give you more information about the point of failure<br>&gt; &gt;&gt;&gt;&gt; in the error message.<br>&gt; &gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt; On 04/15/2014 04:25 PM, Ralph Castain wrote:<br>&gt; &gt;&gt;&gt;&gt;&gt; Have you tried using a debugger to look at the resulting core file? It<br>&gt; &gt;&gt;&gt;&gt;&gt; will probably point you right at the problem. Most likely a case of<br>&gt; &gt;&gt;&gt;&gt;&gt; overrunning some array when #temps &gt; 5<br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt; On Tue, Apr 15, 2014 at 10:46 AM, Oscar Mojica &lt;o_mojical@hotmail.com<br>&gt; &gt;&gt;&gt;&gt;&gt; &lt;mailto:o_mojical@hotmail.com&gt;&gt; wrote:<br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt;    Hello everybody<br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt;    I implemented a parallel simulated annealing algorithm in fortran.<br>&gt; &gt;&gt;&gt;&gt;&gt;      The algorithm is describes as follows<br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt;    1. The MPI program initially generates P processes that have rank<br>&gt; &gt;&gt;&gt;&gt;&gt;    0,1,...,P-1.<br>&gt; &gt;&gt;&gt;&gt;&gt;    2. The MPI program generates a starting point and sends it  for all<br>&gt; &gt;&gt;&gt;&gt;&gt;    processes set T=T0<br>&gt; &gt;&gt;&gt;&gt;&gt;    3. At the current temperature T, each process begins to execute<br>&gt; &gt;&gt;&gt;&gt;&gt;    iterative operations<br>&gt; &gt;&gt;&gt;&gt;&gt;    4. At end of iterations, process with rank 0 is responsible for<br>&gt; &gt;&gt;&gt;&gt;&gt;    collecting the solution obatined by<br>&gt; &gt;&gt;&gt;&gt;&gt;    5. Each process at current temperature and broadcast the best<br>&gt; &gt;&gt;&gt;&gt;&gt;    solution of them among all participating<br>&gt; &gt;&gt;&gt;&gt;&gt;    process<br>&gt; &gt;&gt;&gt;&gt;&gt;    6. Each process cools the temperatue and goes back to step 3, until<br>&gt; &gt;&gt;&gt;&gt;&gt;    the maximum number of temperatures<br>&gt; &gt;&gt;&gt;&gt;&gt;    is reach<br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt;    I compiled with: mpif90 -o exe mpivfsa_version2.f<br>&gt; &gt;&gt;&gt;&gt;&gt;    and run with: mpirun -np 4 ./exe in a single machine<br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt;    So I have 4 processes, 1 iteration per temperature and for example<br>&gt; &gt;&gt;&gt;&gt;&gt;    15 temperatures. When I run the program<br>&gt; &gt;&gt;&gt;&gt;&gt;    with just 5 temperatures it works well, but when the number of<br>&gt; &gt;&gt;&gt;&gt;&gt;    temperatures is higher than 5 it doesn't write the<br>&gt; &gt;&gt;&gt;&gt;&gt;    ouput files and I get the following error message:<br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06740] *** Process received signal ***<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06741] *** Process received signal ***<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06741] Signal: Segmentation fault (11)<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06741] Signal code: Address not mapped (1)<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06741] Failing at address: 0xad6af<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06742] *** Process received signal ***<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06740] Signal: Segmentation fault (11)<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06740] Signal code: Address not mapped (1)<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06740] Failing at address: 0xad6af<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06742] Signal: Segmentation fault (11)<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06742] Signal code: Address not mapped (1)<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06742] Failing at address: 0xad6af<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06740] [ 0]<br>&gt; &gt;&gt;&gt;&gt;&gt;    /lib/x86_64-linux-gnu/libc.so.6(+0x364a0) [0x7f49ee2224a0]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06740] [ 1]<br>&gt; &gt;&gt;&gt;&gt;&gt;    /lib/x86_64-linux-gnu/libc.so.6(cfree+0x1c) [0x7f49ee26f54c]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06740] [ 2] ./exe() [0x406742]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06740] [ 3] ./exe(main+0x34) [0x406ac9]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06740] [ 4]<br>&gt; &gt;&gt;&gt;&gt;&gt;    /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xed)<br>&gt; &gt;&gt;&gt;&gt;&gt; [0x7f49ee20d76d]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06742] [ 0]<br>&gt; &gt;&gt;&gt;&gt;&gt;    /lib/x86_64-linux-gnu/libc.so.6(+0x364a0) [0x7f6877fdc4a0]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06742] [ 1]<br>&gt; &gt;&gt;&gt;&gt;&gt;    /lib/x86_64-linux-gnu/libc.so.6(cfree+0x1c) [0x7f687802954c]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06742] [ 2] ./exe() [0x406742]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06742] [ 3] ./exe(main+0x34) [0x406ac9]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06742] [ 4]<br>&gt; &gt;&gt;&gt;&gt;&gt;    /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xed)<br>&gt; &gt;&gt;&gt;&gt;&gt; [0x7f6877fc776d]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06742] [ 5] ./exe() [0x401399]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06742] *** End of error message ***<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06740] [ 5] ./exe() [0x401399]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06740] *** End of error message ***<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06741] [ 0]<br>&gt; &gt;&gt;&gt;&gt;&gt;    /lib/x86_64-linux-gnu/libc.so.6(+0x364a0) [0x7fa6c4c6e4a0]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06741] [ 1]<br>&gt; &gt;&gt;&gt;&gt;&gt;    /lib/x86_64-linux-gnu/libc.so.6(cfree+0x1c) [0x7fa6c4cbb54c]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06741] [ 2] ./exe() [0x406742]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06741] [ 3] ./exe(main+0x34) [0x406ac9]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06741] [ 4]<br>&gt; &gt;&gt;&gt;&gt;&gt;    /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xed)<br>&gt; &gt;&gt;&gt;&gt;&gt; [0x7fa6c4c5976d]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06741] [ 5] ./exe() [0x401399]<br>&gt; &gt;&gt;&gt;&gt;&gt;    [oscar-Vostro-3550:06741] *** End of error message ***<br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt;    mpirun noticed that process rank 0 with PID 6917 on node<br>&gt; &gt;&gt;&gt;&gt;&gt;    oscar-Vostro-3550 exited on signal 11 (Segmentation fault).<br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt;    2 total processes killed (some possibly by mpirun during cleanup)<br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt;    If there is a segmentation fault in no case it must work .<br>&gt; &gt;&gt;&gt;&gt;&gt;    I checked the program and didn't find the error. Why does the<br>&gt; &gt;&gt;&gt;&gt;&gt;    program work with five temperatures?<br>&gt; &gt;&gt;&gt;&gt;&gt;    Could someone help me to find the error and answer my question<br>&gt; &gt;&gt;&gt;&gt;&gt; please.<br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt;    The program and the necessary files to run it  are attached<br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt;    Thanks<br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt;    _Oscar Fabian Mojica Ladino_<br>&gt; &gt;&gt;&gt;&gt;&gt;    Geologist M.S. in  Geophysics<br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt;    _______________________________________________<br>&gt; &gt;&gt;&gt;&gt;&gt;    users mailing list<br>&gt; &gt;&gt;&gt;&gt;&gt;    users@open-mpi.org &lt;mailto:users@open-mpi.org&gt;<br>&gt; &gt;&gt;&gt;&gt;&gt;    http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt;&gt; _______________________________________________<br>&gt; &gt;&gt;&gt;&gt;&gt; users mailing list<br>&gt; &gt;&gt;&gt;&gt;&gt; users@open-mpi.org<br>&gt; &gt;&gt;&gt;&gt;&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt;&gt;&gt; <br>&gt; &gt;&gt;&gt;&gt; _______________________________________________<br>&gt; &gt;&gt;&gt;&gt; users mailing list<br>&gt; &gt;&gt;&gt;&gt; users@open-mpi.org<br>&gt; &gt;&gt;&gt;&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt;&gt; _______________________________________________<br>&gt; &gt;&gt;&gt; users mailing list<br>&gt; &gt;&gt;&gt; users@open-mpi.org<br>&gt; &gt;&gt;&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt;&gt; <br>&gt; &gt;&gt; <br>&gt; &gt; <br>&gt; &gt; _______________________________________________<br>&gt; &gt; users mailing list<br>&gt; &gt; users@open-mpi.org<br>&gt; &gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; users@open-mpi.org<br>&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br></div></div> 		 	   		  </div></body>
</html>
