<div dir="ltr">Thanks Rob. I&#39;ll keep track of it over there. How often do updated versions of ROMIO get pulled over from MPICH into OpenMPI?<div><br></div><div>On a slightly related note, I think I heard that you had fixed the 32bit issues in ROMIO that were causing it to break when reading more than 2 GB (i.e. <a href="http://www.open-mpi.org/community/lists/users/2012/07/19762.php">http://www.open-mpi.org/community/lists/users/2012/07/19762.php</a>). Have those been pulled into OpenMPI? I&#39;ve been staying clear of ROMIO for a while (in favour of OMPIO), to avoid those issues.</div>
<div><br></div><div>Thanks,</div><div>Richard</div>

</div><div class="gmail_extra"><br><br><div class="gmail_quote">On 7 May 2014 12:36, Rob Latham <span dir="ltr">&lt;<a href="mailto:robl@mcs.anl.gov" target="_blank">robl@mcs.anl.gov</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div class=""><br>
<br>
On 05/05/2014 09:20 PM, Richard Shaw wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Hello,<br>
<br>
I think I&#39;ve come across a bug when using ROMIO to read in a 2D<br>
distributed array. I&#39;ve attached a test case to this email.<br>
</blockquote>
<br></div>
Thanks for the bug report and the test case.<br>
<br>
I&#39;ve opened MPICH bug (because this is ROMIO&#39;s fault, not OpenMPI&#39;s fault... until I can prove otherwise ! :&gt;)<br>
<br>
<a href="http://trac.mpich.org/projects/mpich/ticket/2089" target="_blank">http://trac.mpich.org/<u></u>projects/mpich/ticket/2089</a><br>
<br>
==rob<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div class="h5">
<br>
In the testcase I first initialise an array of 25 doubles (which will be<br>
a 5x5 grid), then I create a datatype representing a 5x5 matrix<br>
distributed in 3x3 blocks over a 2x2 process grid. As a reference I use<br>
MPI_Pack to pull out the block cyclic array elements local to each<br>
process (which I think is correct). Then I write the original array of<br>
25 doubles to disk, and use MPI-IO to read it back in (performing the<br>
Open, Set_view, and Real_all), and compare to the reference.<br>
<br>
Running this with OMPI, the two match on all ranks.<br>
<br>
 &gt; mpirun -mca io ompio -np 4 ./darr_read.x<br>
=== Rank 0 === (9 elements)<br>
Packed:  0.0  1.0  2.0  5.0  6.0  7.0 10.0 11.0 12.0<br>
Read:    0.0  1.0  2.0  5.0  6.0  7.0 10.0 11.0 12.0<br>
<br>
=== Rank 1 === (6 elements)<br>
Packed: 15.0 16.0 17.0 20.0 21.0 22.0<br>
Read:   15.0 16.0 17.0 20.0 21.0 22.0<br>
<br>
=== Rank 2 === (6 elements)<br>
Packed:  3.0  4.0  8.0  9.0 13.0 14.0<br>
Read:    3.0  4.0  8.0  9.0 13.0 14.0<br>
<br>
=== Rank 3 === (4 elements)<br>
Packed: 18.0 19.0 23.0 24.0<br>
Read:   18.0 19.0 23.0 24.0<br>
<br>
<br>
<br>
However, using ROMIO the two differ on two of the ranks:<br>
<br>
 &gt; mpirun -mca io romio -np 4 ./darr_read.x<br>
=== Rank 0 === (9 elements)<br>
Packed:  0.0  1.0  2.0  5.0  6.0  7.0 10.0 11.0 12.0<br>
Read:    0.0  1.0  2.0  5.0  6.0  7.0 10.0 11.0 12.0<br>
<br>
=== Rank 1 === (6 elements)<br>
Packed: 15.0 16.0 17.0 20.0 21.0 22.0<br>
Read:    0.0  1.0  2.0  0.0  1.0  2.0<br>
<br>
=== Rank 2 === (6 elements)<br>
Packed:  3.0  4.0  8.0  9.0 13.0 14.0<br>
Read:    3.0  4.0  8.0  9.0 13.0 14.0<br>
<br>
=== Rank 3 === (4 elements)<br>
Packed: 18.0 19.0 23.0 24.0<br>
Read:    0.0  1.0  0.0  1.0<br>
<br>
<br>
<br>
My interpretation is that the behaviour with OMPIO is correct.<br>
Interestingly everything matches up using both ROMIO and OMPIO if I set<br>
the block shape to 2x2.<br>
<br>
This was run on OS X using 1.8.2a1r31632. I have also run this on Linux<br>
with OpenMPI 1.7.4, and OMPIO is still correct, but using ROMIO I just<br>
get segfaults.<br>
<br>
Thanks,<br>
Richard<br>
<br>
<br></div></div>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
<br><span class="HOEnZb"><font color="#888888">
</font></span></blockquote><span class="HOEnZb"><font color="#888888">
<br>
-- <br>
Rob Latham<br>
Mathematics and Computer Science Division<br>
Argonne National Lab, IL USA<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
<br>
</font></span></blockquote></div><br></div>

