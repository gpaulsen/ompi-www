<div dir="ltr">&lt;quote&gt;<br><div>I don&#39;t think the Open MPI TCP BTL will pass the SDP socket type when 
creating sockets -- SDP is much lower performance than native 
verbs/RDMA.  You should use a &quot;native&quot; interface to your RDMA network 
instead (which one you use depends on which kind of network you have).<br></div><div>&lt;/quote&gt;<br><br></div><div>I have a rather naive follow-up question along this line: why is there not a native mode for (garden variety) Ethernet? Is it because it lacks the end-to-end guarantees of TCP, Infiniband and the like? These days, switched Ethernet is very reliable, isn&#39;t it? (I mean by the rate of packet drop because of congestion). So if the application only needs data chunks of around 8KB max, which would not need to be fragmented (using jumbo frames), won&#39;t a native ethernet be much more efficient?<br><br></div><div>Or perhaps these constraints are too limiting in practice?<br><br></div><div>Thanks<br></div><div>Durga<br></div></div><div class="gmail_extra"><br clear="all"><div><div class="gmail_signature">Life is complex. It has real and imaginary parts.</div></div>
<br><div class="gmail_quote">On Tue, Mar 1, 2016 at 9:54 PM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">On Mar 1, 2016, at 6:55 PM, Matthew Larkin &lt;<a href="mailto:larkym@yahoo.com">larkym@yahoo.com</a>&gt; wrote:<br>
&gt;<br>
&gt; As far as PCIe, I am looking into:<br>
&gt;<br>
&gt; 1. Dolphin&#39;s implementation of IPoPCIe<br>
<br>
If it provides a TCP stack and an IP interface, you should be able to use Open MPI&#39;s TCP BTL interface over it.<br>
<br>
&gt; 2. SDP protocol and how it can be utilized, mapping TCP to RDMA<br>
<br>
I don&#39;t think the Open MPI TCP BTL will pass the SDP socket type when creating sockets -- SDP is much lower performance than native verbs/RDMA.  You should use a &quot;native&quot; interface to your RDMA network instead (which one you use depends on which kind of network you have).<br>
<br>
&gt; Not sure if the only answer for this is a custom stack, API/kernel module.<br>
&gt;<br>
&gt; Do you have any input on the above mentioned things?<br>
&gt;<br>
&gt; On Tuesday, March 1, 2016 6:42 AM, Jeff Squyres (jsquyres) &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br>
&gt;<br>
&gt;<br>
&gt; On Feb 29, 2016, at 6:48 PM, Matthew Larkin &lt;<a href="mailto:larkym@yahoo.com">larkym@yahoo.com</a>&gt; wrote:<br>
&gt; &gt;<br>
&gt; &gt; 1. I know OpenMPI supports ethernet, but where does it clearly state that?<br>
&gt; &gt; - I see on the FAQ on the web page there is a whole list of network interconnect, but how do I relate that to Ethernet network etc.?<br>
&gt;<br>
&gt; Open MPI actually supports multiple Ethernet-based interconnects: Cisco usNIC, iWARP, Mellanox RoCE, and TCP sockets.<br>
&gt;<br>
&gt; I suspect the one you are asking about is TCP sockets (which technically doesn&#39;t need to run over Ethernet, but TCP-over-Ethernet is probably its most common use case).<br>
&gt;<br>
&gt;<br>
&gt; &gt; 2. Does OpenMPI work with PCIe and PCIe switch?<br>
&gt; &gt; - Is there any specific configuration to get it to work?<br>
&gt;<br>
&gt;<br>
&gt; Do you have a specific vendor device / networking stack in mind?  In general, Open MPI will use:<br>
&gt;<br>
&gt; - some kind of local IPC mechanism (e.g., some flavor of shared memory) for intra-node communication<br>
&gt; - some kind of networking API for inter-node communication<br>
&gt;<br>
&gt; Extending PCIe between servers blurs this line a bit -- peer MPI processes on a remote server can make it look like they are actually local.  So it depends on your network stack: do you have some kind of API that effects messaging transport over PCIe?<br>
<span class="HOEnZb"><font color="#888888">&gt;<br>
&gt; --<br>
&gt; Jeff Squyres<br>
&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" rel="noreferrer" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" rel="noreferrer" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/03/28613.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/03/28613.php</a><br>
</font></span></blockquote></div><br></div>

