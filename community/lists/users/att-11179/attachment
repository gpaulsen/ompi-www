<div class="gmail_quote"><div>Thanks for all your inputs. </div><div><br></div><div>It is good to know this initial latency is an expected behavior and the workaround by using one dummy iteration before timing is started.</div>
<div><br></div><div>I did not notice this because my older parallel CFD code runs a large number  of time steps and the initial latency was compensated.</div><div><br></div><div>But recently I am teaching MPI stuff using small parallel codes and noticed this behavior.</div>
<div><br></div><div>This relieves my concern about our system performance.</div><div><br></div><div>Thanks again.</div><div><br></div><div> </div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">

Date: Thu, 12 Nov 2009 11:18:24 -0500<br>
From: Gus Correa &lt;<a href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt;<br>
Subject: Re: [OMPI users] mpi functions are slow when first called and<br>
        become normal afterwards<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Message-ID: &lt;<a href="mailto:4AFC3550.10500@ldeo.columbia.edu">4AFC3550.10500@ldeo.columbia.edu</a>&gt;<br>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed<br>
<br>
Eugene Loh wrote:<br>
&gt; RightCFD wrote:<br>
&gt;&gt;<br>
&gt;&gt;     Date: Thu, 29 Oct 2009 15:45:06 -0400<br>
&gt;&gt;     From: Brock Palen &lt;<a href="mailto:brockp@umich.edu">brockp@umich.edu</a> &lt;mailto:<a href="mailto:brockp@umich.edu">brockp@umich.edu</a>&gt;&gt;<br>
&gt;&gt;     Subject: Re: [OMPI users] mpi functions are slow when first called and<br>
&gt;&gt;            become normal afterwards<br>
&gt;&gt;     To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;&gt;<br>
&gt;&gt;     Message-ID: &lt;<a href="mailto:890CC430-68B0-4307-8260-24A6FADAE319@umich.edu">890CC430-68B0-4307-8260-24A6FADAE319@umich.edu</a><br>
&gt;&gt;     &lt;mailto:<a href="mailto:890CC430-68B0-4307-8260-24A6FADAE319@umich.edu">890CC430-68B0-4307-8260-24A6FADAE319@umich.edu</a>&gt;&gt;<br>
&gt;&gt;     Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes<br>
&gt;&gt;<br>
&gt;&gt;     &gt; When MPI_Bcast and MPI_Reduce are called for the first time, they<br>
&gt;&gt;     &gt; are very slow. But after that, they run at normal and stable speed.<br>
&gt;&gt;     &gt; Is there anybody out there who have encountered such problem? If you<br>
&gt;&gt;     &gt; need any other information, please let me know and I&#39;ll provide.<br>
&gt;&gt;     &gt; Thanks in advance.<br>
&gt;&gt;<br>
&gt;&gt;     This is expected, and I think you can dig though the message archive<br>
&gt;&gt;     to find the answer.  OMPI does not wireup all the communication at<br>
&gt;&gt;     startup, thus the first time you communicate with a host the<br>
&gt;&gt;     connection is made, but after that it is fast because it is already<br>
&gt;&gt;     open.  This behavior is expected, and is needed for very large systems<br>
&gt;&gt;     where you could run out of sockets for some types of communication<br>
&gt;&gt;     with so many hosts.<br>
&gt;&gt;<br>
&gt;&gt;     Brock Palen<br>
&gt;&gt;     <a href="http://www.umich.edu/~brockp" target="_blank">www.umich.edu/~brockp</a> &lt;<a href="http://www.umich.edu/%7Ebrockp" target="_blank">http://www.umich.edu/%7Ebrockp</a>&gt;<br>
&gt;&gt;     Center for Advanced Computing<br>
&gt;&gt;     <a href="mailto:brockp@umich.edu">brockp@umich.edu</a> &lt;mailto:<a href="mailto:brockp@umich.edu">brockp@umich.edu</a>&gt;<br>
&gt;&gt;     (734)936-1985<br>
&gt;&gt;<br>
&gt;&gt;     Thanks for your reply. I am surprised to know this is an expected<br>
&gt;&gt;     behavior of OMPI. I searched the archival but did not find many<br>
&gt;&gt;     relevant messages. I am wondering why other users of OMPI do not<br>
&gt;&gt;     complain this. Is there a way to avoid this when timing an MPI<br>
&gt;&gt;     program?<br>
&gt;&gt;<br>
&gt; An example of this is the NAS Parallel Benchmarks, which have been<br>
&gt; around nearly 20 years.  They:<br>
&gt;<br>
&gt; *) turn timers on after MPI_Init and off before MPI_Finalize<br>
&gt; *) execute at least one iteration before starting timers<br>
&gt;<br>
&gt; Even so, with at least one of the NPB tests and with at least one MPI<br>
&gt; implementation, I&#39;ve seen more than one iteration needed to warm things<br>
&gt; up.  That is, if you timed each iteration, you could see that multiple<br>
&gt; iterations were needed to warm everything up.  In performance analysis,<br>
&gt; it is reasonably common to expect to have to run multiple iterations and<br>
&gt; correct data set size to get representative behavior.<br>
&gt;<br>
&gt;<br>
<br>
And I would guess in OpenMPI, maybe in other implementations too,<br>
the time you spend warming up, probing the best way to do things,<br>
is widely compensated for during steady state execution,<br>
if the number of iterations is not very small.<br>
This seems to be required to accommodate for the large variety<br>
of hardware and software platforms, and be efficient on all of them.<br>
Right?<br>
<br>
AFAIK, other high quality software (e.g. FFTW)<br>
do follow a similar rationale.<br>
<br>
Gus Correa<br>
<br>
&gt; ------------------------------------------------------------------------<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
End of users Digest, Vol 1403, Issue 4<br>
**************************************<br>
</blockquote></div><br><br>

