There are two points here<div>1. slurm(stepd) is unable to put the processes in the (null) cgroup.</div><div>   at first glance, this looks more of a slurm jus configuration</div><div>2. the MPI process forking. though this has a much better support than in the past, that might not always work, especially with fast interconnects. since you are running on a single node, you should be fine. simply</div><div>export OMPI_MCA_mpi_warn_on_fork=0</div><div>before invoking srun, in order to silence this message.</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles</div><div><br>On Monday, June 20, 2016, Ahmed Rizwan &lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;rizwan.ahmed@aalto.fi&#39;);" target="_blank">rizwan.ahmed@aalto.fi</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">




<div>
<div style="direction:ltr;font-family:Tahoma;color:#000000;font-size:10pt">Dear MPI users,<br>
<br>
I am getting the errors below while submitting/executing following script, <br>
<br>
#!/bin/sh<br>
#SBATCH -p short<br>
#SBATCH -J layers<br>
#SBATCH -n 12<br>
#SBATCH -N 1 <br>
#SBATCH -t 01:30:00<br>
#SBATCH --mem-per-cpu=2500<br>
#SBATCH --exclusive<br>
#SBATCH --mail-type=END<br>
#SBATCH --mail-user=<a>rizwan.ahmed@aalto.fi</a><br>
#SBATCH -o output_%j.out<br>
#SBATCH -e errors_%j.err<br>
<br>
srun --mpi=pmi2 gpaw-python layers.py<br>
<br>
--------------------------------------------------------------------------<br>
slurmstepd: error: task/cgroup: unable to add task[pid=126453] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=80379] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124258] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124259] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124261] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124266] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124264] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124262] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124260] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124265] to memory cg &#39;(null)&#39;<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124263] to memory cg &#39;(null)&#39;<br>
--------------------------------------------------------------------------<br>
An MPI process has executed an operation involving a call to the<br>
&quot;fork()&quot; system call to create a child process.  Open MPI is currently<br>
operating in a condition that could result in memory corruption or<br>
other system errors; your MPI job may hang, crash, or produce silent<br>
data corruption.  The use of fork() (or system() or other calls that<br>
create child processes) is strongly discouraged.  <br>
<br>
The process that invoked fork was:<br>
<br>
  Local host:          pe38 (PID 80379)<br>
  MPI_COMM_WORLD rank: 1<br>
<br>
If you are *absolutely sure* that your application will successfully<br>
and correctly survive a call to fork(), you may disable this warning<br>
by setting the mpi_warn_on_fork MCA parameter to 0.<br>
--------------------------------------------------------------------------<br>
<br>
Is this error fatal or should it be ignored? Thanks<br>
Regards,<br>
Rizwan<br>
</div>
</div>

</blockquote></div>

