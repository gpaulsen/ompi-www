<html>
  <head>

    <meta http-equiv="content-type" content="text/html; charset=ISO-8859-1">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    <tt>Hi, <br>
      <br>
      I try to submit two MPI jobs using single OpenMPI mpirun command
      (command can be seen in job submission script). To test this
      configuration, i compiled simple mpihello application and run. The
      problem is that each distinct mpihello jobs (run1 and run2) uses
      same MPI_COMM_WORLD and rank of the process goes like following,<br>
      &nbsp;<br>
      --- out1 (comes from first mpihello.x) ---
      <br>
      &nbsp;node&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 17 : Hello world
      <br>
      &nbsp;node&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 28 : Hello world
      <br>
      ...
      <br>
      ...
      <br>
      <br>
      --- out2 (comes from second mpihello.x) ---
    </tt><tt><br>
      &nbsp;node&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 115 : Hello world
      <br>
      &nbsp;node&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 113 : Hello world
      <br>
      &nbsp;node&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 74 : Hello world
      <br>
      ...
      <br>
      ...
      <br>
    </tt><tt><br>
      If the MPI_COMM_WORLD is created separately for each jobs then the
      node number (or id or rank) must be start from 0 until 63 in each
      log file but this is not the case. So, in the second one the node
      numbers start from 64 to 131. If Fortran application uses
      MPI_COMM_SIZE and MPI_COMM_RANK to get the total number of
      processor (in this case it is 132), then rank and total number of
      processor will be wrong. I think mpirun is not smart enough in
      this case. What do you think? Any suggestions can help.<br>
    </tt><tt><br>
      PS: I am using OpenMPI version 1.5.3 compiled with Intel 12.0.4
      compilers.<br>
      <br>
      Regards,<br>
      <br>
      --ufuk<br>
      <br>
      <br>
      <b>--- job submission script (in OpenPBS) ---</b><br>
      <br>
      #!/bin/bash<br>
      #PBS -l walltime=01:00:00<br>
      #PBS -l nodes=11:ppn=12<br>
      #PBS -N both<br>
      #PBS -q esp<br>
      <br>
      # load modules<br>
      . /etc/profile.d/modules.sh<br>
      module load openmpi/1.5.3/intel/2011<br>
      module load netcdf/4.1.1/intel/11.1<br>
      <br>
      # parameters<br>
      WRKDIR1=/home/netapp/clima-users/users/uturunco/CAS/run.lake/BOTH<br>
      WRKDIR2=/home/netapp/clima-users/users/uturunco/CAS/run.lake/BOTH<br>
      <br>
      # create node files<br>
      head -n 64 $PBS_NODEFILE &gt;&amp; $WRKDIR1/nodes1.txt<br>
      tail -n 64 $PBS_NODEFILE &gt;&amp; $WRKDIR2/nodes2.txt<br>
      <br>
      # submit jobs<br>
      mpirun -np `cat $WRKDIR1/nodes1.txt | wc -l` -machinefile
      $WRKDIR1/nodes1.txt -wd $WRKDIR1 ./run1.sh : -np `cat
      $WRKDIR2/nodes2.txt | wc -l` -machinefile $WRKDIR2/nodes2.txt -wd
      $WRKDIR2 ./run2.sh<br>
      <br>
      <b>--- end of job submission script ---<br>
        <br>
        ---</b></tt><tt><b> script run1.sh ---</b><br>
      <br>
    </tt><tt>#!/bin/sh<br>
      ./mpihello.x &gt;&gt; out1.txt<br>
      <br>
      <b>--- end of </b><b> script run1.sh ---<br>
        <br>
      </b></tt><tt><b>--- script run2.sh ---</b><br>
      <br>
    </tt><tt>#!/bin/sh<br>
      ./mpihello.x &gt;&gt; out2.txt<br>
      <br>
      <b>--- end of </b></tt><tt><b> script run2.sh ---</b><br>
    </tt>
  </body>
</html>

