<div dir="ltr">I see, so if I understand correctly, the best scenario for threads would be to bind 2 procs to sockets as --map-by socket:pe=4 and use 4 threads in each proc. <div><br></div><div>Also, as you&#39;ve mentioned binding threads to get memory locality, I guess this has to be done at application level and not an option in OMPI</div>
<div><br></div><div>Thank you,</div><div>Saliya</div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Wed, Feb 26, 2014 at 4:50 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">Sorry, had to run some errands.<div><br><div><div class=""><div>On Feb 26, 2014, at 1:03 PM, Saliya Ekanayake &lt;<a href="mailto:esaliya@gmail.com" target="_blank">esaliya@gmail.com</a>&gt; wrote:</div>
<br><blockquote type="cite"><div dir="ltr">Is it possible to bind to cores of multiple sockets? Say I have a machine with 2 sockets each with 4 cores and if I run 8 threads with 1 proc can I utilize all 8 cores for 8 threads?</div>
</blockquote><div><br></div></div>In that scenario, you won&#39;t get any benefit from binding as we only bind at the proc level (and binding to the entire node does nothing). You might want to bind your threads, however, as otherwise the threads will not necessarily execute local to any memory they malloc.</div>
<div><div class="h5"><div><br><blockquote type="cite"><div dir="ltr"><div><br></div><div>Thank you for speedy replies</div>
<div><br></div><div>Saliya</div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Wed, Feb 26, 2014 at 3:21 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word"><br><div><div><div>On Feb 26, 2014, at 12:17 PM, Saliya Ekanayake &lt;<a href="mailto:esaliya@gmail.com" target="_blank">esaliya@gmail.com</a>&gt; wrote:</div>

<br><blockquote type="cite"><div dir="ltr">I have a followup question on this. In our application we have parallel for loops similar to OMP parallel for. I noticed that in order to gain speedup with threads I&#39;ve to set --bind-to none, otherwise multiple threads will bind to same core giving no increase in performance. For example, I get following (attached) performance for a simple 3point stencil computation run with T threads on 1 MPI process on 1 node (Tx1x1). <div>


<br></div><div>My understanding is even when there are multiple procs per node we should use --bind-to none in order to get performance with threads. Is this correct? Also, what&#39;s the disadvantage of not using --bind-to core?</div>

</div></blockquote><div><br></div></div>Your best performance with threads comes when you bind each process to multiple cores. Binding helps performance by ensuring your memory is always local, and provides some optimized scheduling benefits. You can bind to multiple cores by adding the qualifier &quot;pe=N&quot; to your mapping definition, like this:</div>

<div><br></div><div>mpirun --map-by socket:pe=4 ....</div><div><br></div><div>The above example will map processes by socket, and bind each process to 4 cores.</div><div><br></div><div>HTH</div><div>Ralph</div><div><br><blockquote type="cite">

<div><div><div dir="ltr">
<div><br></div><div>Thank you,</div><div>Saliya</div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Wed, Feb 26, 2014 at 11:01 AM, Saliya Ekanayake <span dir="ltr">&lt;<a href="mailto:esaliya@gmail.com" target="_blank">esaliya@gmail.com</a>&gt;</span> wrote:<br>


<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Thank you Ralph, I&#39;ll check this.</div><div><div class="gmail_extra">
<br><br><div class="gmail_quote">On Wed, Feb 26, 2014 at 10:04 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">It means that OMPI didn&#39;t get built against libnuma, and so we can&#39;t ensure that memory is being bound local to the proc binding. Check to see if numactl and numactl-devel are installed, or you can turn off the warning using &quot;-mca hwloc_base_mem_bind_failure_action silent&quot;<div>



<br></div><div><br><div><div><div>On Feb 25, 2014, at 10:32 PM, Saliya Ekanayake &lt;<a href="mailto:esaliya@gmail.com" target="_blank">esaliya@gmail.com</a>&gt; wrote:</div><br></div><blockquote type="cite">
<div><div dir="ltr">Hi,<div><br></div><div>I tried to run an MPI Java program with --bind-to core. I receive the following warning and wonder how to fix this.</div><div><br></div><div><div><br></div><div>WARNING: a request was made to bind a process. While the system</div>




<div>supports binding the process itself, at least one node does NOT</div><div>support binding memory to the process location.</div><div><br></div><div>  Node:  192.168.0.19</div><div><br></div><div>This is a warning only; your job will continue, though performance may</div>




<div>be degraded.</div></div><div><br></div><div><br></div><div>Thank you,</div><div>Saliya</div><div><div><br></div>-- <br><div dir="ltr"><span style="color:rgb(136,136,136)">Saliya Ekanayake <a href="mailto:esaliya@gmail.com" target="_blank">esaliya@gmail.com</a></span><span style="color:rgb(136,136,136)"> </span><br style="color:rgb(136,136,136)">




<span style="color:rgb(136,136,136)">Cell <a href="tel:812-391-4914" value="+18123914914" target="_blank">812-391-4914</a> Home <a href="tel:812-961-6383" value="+18129616383" target="_blank">812-961-6383</a></span><br style="color:rgb(136,136,136)">



<font color="#888888"><a href="http://saliya.org/" target="_blank">http://saliya.org</a></font></div>
</div></div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>



</div><br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br><div dir="ltr"><span style="color:rgb(136,136,136)">Saliya Ekanayake <a href="mailto:esaliya@gmail.com" target="_blank">esaliya@gmail.com</a></span><span style="color:rgb(136,136,136)"> </span><br style="color:rgb(136,136,136)">



<span style="color:rgb(136,136,136)">Cell <a href="tel:812-391-4914" value="+18123914914" target="_blank">812-391-4914</a> Home <a href="tel:812-961-6383" value="+18129616383" target="_blank">812-961-6383</a></span><br style="color:rgb(136,136,136)">


<font color="#888888"><a href="http://saliya.org/" target="_blank">http://saliya.org</a></font></div>
</div>
</div></blockquote></div><br><br clear="all"><div><br></div>-- <br><div dir="ltr"><span style="color:rgb(136,136,136)">Saliya Ekanayake <a href="mailto:esaliya@gmail.com" target="_blank">esaliya@gmail.com</a></span><span style="color:rgb(136,136,136)"> </span><br style="color:rgb(136,136,136)">


<span style="color:rgb(136,136,136)">Cell <a href="tel:812-391-4914" value="+18123914914" target="_blank">812-391-4914</a> Home <a href="tel:812-961-6383" value="+18129616383" target="_blank">812-961-6383</a></span><br style="color:rgb(136,136,136)">

<font color="#888888"><a href="http://saliya.org/" target="_blank">http://saliya.org</a></font></div>
</div>
</div></div><span>&lt;3pointstencil.png&gt;</span>_______________________________________________<div><br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div>

</blockquote></div><br></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br><div dir="ltr"><span style="color:rgb(136,136,136)">Saliya Ekanayake <a href="mailto:esaliya@gmail.com" target="_blank">esaliya@gmail.com</a></span><span style="color:rgb(136,136,136)"> </span><br style="color:rgb(136,136,136)">

<span style="color:rgb(136,136,136)">Cell <a href="tel:812-391-4914" value="+18123914914" target="_blank">812-391-4914</a> Home <a href="tel:812-961-6383" value="+18129616383" target="_blank">812-961-6383</a></span><br style="color:rgb(136,136,136)">
<font color="#888888"><a href="http://saliya.org/" target="_blank">http://saliya.org</a></font></div>
</div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br><div dir="ltr"><span style="color:rgb(136,136,136)">Saliya Ekanayake <a href="mailto:esaliya@gmail.com" target="_blank">esaliya@gmail.com</a></span><span style="color:rgb(136,136,136)"> </span><br style="color:rgb(136,136,136)">
<span style="color:rgb(136,136,136)">Cell 812-391-4914 Home 812-961-6383</span><br style="color:rgb(136,136,136)"><font color="#888888"><a href="http://saliya.org" target="_blank">http://saliya.org</a></font></div>
</div>

