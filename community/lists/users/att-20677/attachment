I gather from your other emails you are using 1.4.3, yes? I believe that has npersocket as an option. If so, you could do:<br><br>mpirun -npersocket 2 -bind-to-socket ...<br><br>That would put two processes in each socket, bind them to that socket, and rank them in series. So ranks 0-1 would be bound to the first socket, ranks 2-3 to the second.<br>
<br>Ralph<br><br><div class="gmail_extra"><br><br><div class="gmail_quote">On Thu, Nov 8, 2012 at 6:52 AM, Blosch, Edwin L <span dir="ltr">&lt;<a href="mailto:edwin.l.blosch@lmco.com" target="_blank">edwin.l.blosch@lmco.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Yes it is a Westmere system.<br>
<br>
Socket L#0 (P#0 CPUModel=&quot;Intel(R) Xeon(R) CPU E7- 8870  @ 2.40GHz&quot; CPUType=x86_64)<br>
      L3Cache L#0 (size=30720KB linesize=64 ways=24)<br>
        L2Cache L#0 (size=256KB linesize=64 ways=8)<br>
          L1dCache L#0 (size=32KB linesize=64 ways=8)<br>
            L1iCache L#0 (size=32KB linesize=64 ways=4)<br>
              Core L#0 (P#0)<br>
                PU L#0 (P#0)<br>
        L2Cache L#1 (size=256KB linesize=64 ways=8)<br>
          L1dCache L#1 (size=32KB linesize=64 ways=8)<br>
            L1iCache L#1 (size=32KB linesize=64 ways=4)<br>
              Core L#1 (P#1)<br>
                PU L#1 (P#1)<br>
<br>
So I guess each core has its own L1 and L2 caches.  Maybe I shouldn&#39;t care where or if the MPI processes are bound within a socket; if I can test it, that will be good enough for me.<br>
<br>
So my initial question is now changed to:<br>
<br>
What is the best/easiest way to get this mapping?  Rankfile?, --cpus-per-proc 2 --bind-to-socket, or something else?<br>
<br>
RANK  SOCKET  CORE<br>
0       0       unspecified<br>
1       0       unspecified<br>
2       1       unspecified<br>
3       1       unspecified<br>
<br>
<br>
Thanks<br>
<br>
-----Original Message-----<br>
From: <a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a> [mailto:<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>] On Behalf Of Brice Goglin<br>
Sent: Wednesday, November 07, 2012 6:17 PM<br>
To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subject: EXTERNAL: Re: [OMPI users] Best way to map MPI processes to sockets?<br>
<br>
What processor and kernel is this? (see /proc/cpuinfo, or run &quot;lstopo -v&quot; and look for attributes on the Socket line) You&#39;re hwloc output looks like an Intel Xeon Westmere-EX (E7-48xx or E7-88xx).<br>
The likwid output is likely wrong (maybe confused by the fact that hardware threads are disabled).<br>
<br>
Brice<br>
<br>
<br>
<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br></div>

