<html><body bgcolor="#FFFFFF"><div>You should consider reading about communicators in MPI.&nbsp;</div><div><br></div><div>Aurelien</div><div>--</div><div>Aurelien Bouteiller, Ph.D.</div><div>Innovative Computing Laboratory, The University of Tennessee.<br><br>Envoyé de mon iPad</div><div><br>Le Aug 7, 2010 à 1:05, Randolph Pullen &lt;<a href="mailto:randolph_pullen@yahoo.com.au">randolph_pullen@yahoo.com.au</a>&gt; a écrit&nbsp;:<br><br></div><div></div><blockquote type="cite"><div><table cellspacing="0" cellpadding="0" border="0"><tbody><tr><td valign="top" style="font: inherit;">I seem to be having a problem with MPI_Bcast.<br>My massive I/O intensive data movement program must broadcast from n to n nodes. My problem starts because I require 2 processes per node, a sender and a receiver and I have implemented these using MPI processes rather than tackle the complexities of threads on MPI.<br><br>Consequently, broadcast and calls like alltoall are not completely helpful.&nbsp; The dataset is huge and each node must end up with a complete copy built by the large number of contributing broadcasts from the sending nodes.&nbsp; Network efficiency and run time are paramount.<br><br>As I don’t want to needlessly broadcast all this data to the sending nodes and I have a perfectly good MPI program that distributes globally from a single node (1 to N), I took the unusual decision to start N copies of this program by spawning the MPI system
 from the PVM system in an effort to get my N to N concurrent transfers.<br><br>It seems that the broadcasts running on concurrent MPI environments collide and cause all but the first process to hang waiting for their broadcasts.&nbsp; This theory seems to be confirmed by introducing a sleep of n-1 seconds before the first MPI_Bcast&nbsp; call on each node, which results in the code working perfectly.&nbsp; (total run time 55 seconds, 3 nodes, standard TCP stack)<br><br>My guess is that unlike PVM, OpenMPI implements broadcasts with broadcasts rather than multicasts.&nbsp; Can someone confirm this?&nbsp; Is this a bug?<br><br>Is there any multicast or N to N broadcast where sender processes can avoid participating when they don’t need to?<br><br>Thanks in advance<br>Randolph<br><br></td></tr></tbody></table><br>



      &nbsp;</div></blockquote><blockquote type="cite"><div><span>_______________________________________________</span><br><span>users mailing list</span><br><span><a href="mailto:users@open-mpi.org">users@open-mpi.org</a></span><br><span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></span></div></blockquote></body></html>
