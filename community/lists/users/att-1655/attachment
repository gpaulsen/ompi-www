<div>I have the same error message:"forrtl: severe (174): SIGSEGV, segmentation fault occurred".&nbsp;I run my paralled code on single node&nbsp;or multi nodes, the error existes. Then&nbsp;i try three Intel compilers :&nbsp;8.1.037,&nbsp;9.0.032 and 9.1.033&nbsp;, but the error still existes. But my code work correctly on Windows XP with Visual Fortran 6.6. I doubt whether the intel compiler has some bugs. I also try to solve the problem, maybe the bug is in my code.</div>  <div>&nbsp;</div>  <div>Do you have the other compiler? Could you check the error and report it ?</div>  <div>&nbsp;</div>  <div>T.T. Shen</div>  <div><BR><BR><B><I>Frank Gruellich &lt;frank.gruellich@megware.com&gt;</I></B> 說：</div>  <BLOCKQUOTE class=replbq style="PADDING-LEFT: 5px; MARGIN-LEFT: 5px; BORDER-LEFT: #1010ff 2px solid">Hi,<BR><BR>I'm running OFED 1.0 with OpenMPI 1.1b1-1 compiled for Intel Compiler<BR>9.1. I get this error message during an MPI_Alltoall call:<BR><BR>Signal:11
 info.si_errno:0(Success) si_code:1(SEGV_MAPERR)<BR>Failing at addr:0x1cd04fe0<BR>[0] func:/usr/ofed/mpi/intel/openmpi-1.1b1-1/lib64/libopal.so.0 [0x2b56964acc75]<BR>[1] func:/lib64/libpthread.so.0 [0x2b569739b140]<BR>[2] func:/software/intel/fce/9.1.032/lib/libirc.so(__intel_new_memcpy+0x1540) [0x2b5697278cf0]<BR>*** End of error message ***<BR><BR>and have no idea about the problem. It arises if I exceed a specific<BR>number (10) of MPI nodes. The error occures in this code:<BR><BR>do i = 1,npuntos<BR>print *,'puntos',i<BR>tam = 2**(i-1)<BR>tmin = 1e5<BR>tavg = 0.0d0<BR>do j = 1,rep<BR>envio = 8.0d0*j<BR>call mpi_barrier(mpi_comm_world,ierr)<BR>time1 = mpi_wtime()<BR>do k = 1,rep2<BR>call mpi_alltoall(envio,tam,mpi_byte,recibe,tam,mpi_byte,mpi_comm_world,ierr)<BR>end do<BR>call mpi_barrier(mpi_comm_world,ierr)<BR>time2 = mpi_wtime()<BR>time = (time2 - time1)/(rep2)<BR>if (time &lt; tmin) tmin = time<BR>tavg = tavg + time<BR>end do<BR>m_tmin(i) = tmin<BR>m_tavg(i) =
 tavg/rep<BR>end do<BR><BR>this code is said to be running on another system (running IBGD 1.8.x).<BR>I already tested mpich_mlx_intel-0.9.7_mlx2.1.0-1, but get a similar<BR>error message when using 13 nodes:<BR><BR>forrtl: severe (174): SIGSEGV, segmentation fault occurred<BR>Image PC Routine Line Source<BR>libpthread.so.0 00002B65DA39B140 Unknown Unknown Unknown<BR>main.out 0000000000448BDB Unknown Unknown Unknown<BR>[9] Registration failed, file : intra_rdma_alltoall.c, line : 163<BR>[6] Registration failed, file : intra_rdma_alltoall.c, line : 163<BR>9 - MPI_ALLTOALL : Unknown error<BR>[9] [] Aborting Program!<BR>6 - MPI_ALLTOALL : Unknown error<BR>[6] [] Aborting Program!<BR>[2] Registration failed, file : intra_rdma_alltoall.c, line : 163<BR>[11] Registration failed, file : intra_rdma_alltoall.c, line : 163<BR>11 - MPI_ALLTOALL : Unknown error<BR>[11] [] Aborting Program!<BR>2 - MPI_ALLTOALL : Unknown error<BR>[2] [] Aborting Program!<BR>[10] Registration failed, file
 : intra_rdma_alltoall.c, line : 163<BR>10 - MPI_ALLTOALL : Unknown error<BR>[10] [] Aborting Program!<BR>[5] Registration failed, file : intra_rdma_alltoall.c, line : 163<BR>5 - MPI_ALLTOALL : Unknown error<BR>[5] [] Aborting Program!<BR>[3] Registration failed, file : intra_rdma_alltoall.c, line : 163<BR>[8] Registration failed, file : intra_rdma_alltoall.c, line : 163<BR>3 - MPI_ALLTOALL : Unknown error<BR>[3] [] Aborting Program!<BR>8 - MPI_ALLTOALL : Unknown error<BR>[8] [] Aborting Program!<BR>[4] Registration failed, file : intra_rdma_alltoall.c, line : 163<BR>4 - MPI_ALLTOALL : Unknown error<BR>[4] [] Aborting Program!<BR>[7] Registration failed, file : intra_rdma_alltoall.c, line : 163<BR>7 - MPI_ALLTOALL : Unknown error<BR>[7] [] Aborting Program!<BR>[0] Registration failed, file : intra_rdma_alltoall.c, line : 163<BR>0 - MPI_ALLTOALL : Unknown error<BR>[0] [] Aborting Program!<BR>[1] Registration failed, file : intra_rdma_alltoall.c, line : 163<BR>1 -
 MPI_ALLTOALL : Unknown error<BR>[1] [] Aborting Program!<BR><BR>I don't know wether this is a problem with MPI or Intel Compiler.<BR>Please, can anybody point me in the right direction what I could have<BR>done wrong? This is my first post (so be gentle) and at this time I'm<BR>not very used to the verbosity of this list, so if you need any further<BR>informations do not hesitate do request them.<BR><BR>Thanks in advance and kind regards,<BR>-- <BR>Frank Gruellich<BR>HPC-Techniker<BR><BR>Tel.: +49 3722 528 42<BR>Fax: +49 3722 528 15<BR>E-Mail: frank.gruellich@megware.com<BR><BR>MEGWARE Computer GmbH<BR>Vertrieb und Service<BR>Nordstrasse 19<BR>09247 Chemnitz/Roehrsdorf<BR>Germany<BR>http://www.megware.com/<BR>_______________________________________________<BR>users mailing list<BR>users@open-mpi.org<BR>http://www.open-mpi.org/mailman/listinfo.cgi/users<BR></BLOCKQUOTE><BR><p>&#32;___________________________________________________ <br> 最新版 Yahoo!奇摩即時通訊
 7.0，免費網路電話任你打！ <br> http://messenger.yahoo.com.tw/
