<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><div>Bonjour Thorsten,<br></div><div><br></div><div>&nbsp;I'm not surprised about the cluster type, indeed,</div><div>but I do not remember getting such specific hang up you mention.</div><div><br></div><div>&nbsp;Anyway, I suspect SGI Altix is a little bit special for OpenMPI,</div><div>and I usually run with the following setup:</div><div>- there is need to create for each job a specific tmp area,</div><div>like&nbsp;"/scratch/ggg/uuu/run/tmp/pbs.${PBS_JOBID}"</div><div>- then use something like that:</div><div><br></div><div><div>setenv TMPDIR "/scratch/ggg/uuu/run/tmp/pbs.${PBS_JOBID}"</div><div>setenv OMPI_PREFIX_ENV "/scratch/ggg/uuu/run/tmp/pbs.${PBS_JOBID}"</div><div>setenv OMPI_MCA_mpi_leave_pinned_pipeline 1</div><div><br></div><div>- then, for running, many of these -mca options are probably useless with your app,</div><div>while many of them may show to be useful. Your own way ...</div><div><br></div><div>mpiexec -mca coll_tuned_use_dynamic_rules 1 -hostfile $PBS_NODEFILE -mca rmaps seq -mca btl_openib_rdma_pipeline_send_length 65536 -mca btl_openib_rdma_pipeline_frag_size 65536 -mca btl_openib_min_rdma_pipeline_size 65536 -mca btl_self_rdma_pipeline_send_length 262144 -mca btl_self_rdma_pipeline_frag_size 262144 -mca plm_rsh_num_concurrent 4096 -mca mpi_paffinity_alone 1 -mca mpi_leave_pinned_pipeline 1 -mca btl_sm_max_send_size 128 -mca coll_tuned_pre_allocate_memory_comm_size_limit 1048576 -mca btl_openib_cq_size 128 -mca btl_ofud_rd_num 128 -mca mpi_preconnect_mpi 0 -mca mpool_sm_min_size 131072 -mca btl sm,openib,self -mca btl_openib_want_fork_support 0 -mca opal_set_max_sys_limits 1 -mca osc_pt2pt_no_locks 1 -mca osc_rdma_no_locks 1 YOUR_APP</div><div><br></div><div>&nbsp;(Watch the step : only one line&nbsp;only&nbsp;...)</div><div><br></div><div>&nbsp;This should be suitable for up to 8k cores.</div><div><br></div><div><br></div><div>&nbsp;HTH, &nbsp; Best, &nbsp; &nbsp;G.</div><div><br></div><div><br></div></div><div><br></div><div><div>Le 22 juin 11 à 09:13, Thorsten Schuett a écrit :</div><br class="Apple-interchange-newline"><blockquote type="cite"><div>Sure. It's an SGI ICE cluster with dual-rail IB. The HCAs are Mellanox <br>ConnectX IB DDR.<br><br>This is a 2040 cores job. I use 255 nodes with one MPI task on each node and <br>use 8-way OpenMP.<br><br>I don't need -np and -machinefile, because mpiexec picks up this information <br>from PBS.<br><br>Thorsten<br><br>On Tuesday, June 21, 2011, Gilbert Grosdidier wrote:<br><blockquote type="cite">Bonjour Thorsten,<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite"> &nbsp;Could you please be a little bit more specific about the cluster<br></blockquote><blockquote type="cite">itself ?<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite"> &nbsp;G.<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">Le 21 juin 11 à 17:46, Thorsten Schuett a écrit :<br></blockquote><blockquote type="cite"><blockquote type="cite">Hi,<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">I am running openmpi 1.5.3 on a IB cluster and I have problems<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">starting jobs<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">on larger node counts. With small numbers of tasks, it usually<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">works. But now<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">the startup failed three times in a row using 255 nodes. I am using<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">255 nodes<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">with one MPI task per node and the mpiexec looks as follows:<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">mpiexec --mca btl self,openib --mca mpi_leave_pinned 0 ./a.out<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">After ten minutes, I pulled a stracktrace on all nodes and killed<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">the job,<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">because there was no progress. In the following, you will find the<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">stack trace<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">generated with gdb thread apply all bt. The backtrace looks<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">basically the same<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">on all nodes. It seems to hang in mpi_init.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Any help is appreciated,<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Thorsten<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Thread 3 (Thread 46914544122176 (LWP 28979)):<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#0 &nbsp;0x00002b6ee912d9a2 in select () from /lib64/libc.so.6<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#1 &nbsp;0x00002b6eeabd928d in service_thread_start (context=&lt;value<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">optimized out&gt;)<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">at btl_openib_fd.c:427<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#2 &nbsp;0x00002b6ee835e143 in start_thread () from /lib64/libpthread.so.0<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#3 &nbsp;0x00002b6ee9133b8d in clone () from /lib64/libc.so.6<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#4 &nbsp;0x0000000000000000 in ?? ()<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Thread 2 (Thread 46916594338112 (LWP 28980)):<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#0 &nbsp;0x00002b6ee912b8b6 in poll () from /lib64/libc.so.6<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#1 &nbsp;0x00002b6eeabd7b8a in btl_openib_async_thread (async=&lt;value<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">optimized<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">out&gt;) at btl_openib_async.c:419<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#2 &nbsp;0x00002b6ee835e143 in start_thread () from /lib64/libpthread.so.0<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#3 &nbsp;0x00002b6ee9133b8d in clone () from /lib64/libc.so.6<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#4 &nbsp;0x0000000000000000 in ?? ()<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Thread 1 (Thread 47755361533088 (LWP 28978)):<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#0 &nbsp;0x00002b6ee9133fa8 in epoll_wait () from /lib64/libc.so.6<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#1 &nbsp;0x00002b6ee87745db in epoll_dispatch (base=0xb79050, arg=0xb558c0,<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">tv=&lt;value optimized out&gt;) at epoll.c:215<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#2 &nbsp;0x00002b6ee8773309 in opal_event_base_loop (base=0xb79050,<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">flags=&lt;value<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">optimized out&gt;) at event.c:838<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#3 &nbsp;0x00002b6ee875ee92 in opal_progress () at runtime/<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">opal_progress.c:189<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#4 &nbsp;0x0000000039f00001 in ?? ()<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#5 &nbsp;0x00002b6ee87979c9 in std::ios_base::Init::~Init () at<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">../../.././libstdc++-v3/src/ios_init.cc:123<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#6 &nbsp;0x00007fffc32c8cc8 in ?? ()<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#7 &nbsp;0x00002b6ee9d20955 in orte_grpcomm_bad_get_proc_attr (proc=&lt;value<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">optimized out&gt;, attribute_name=0x2b6ee88e5780 " \020322351n+",<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">val=0x2b6ee875ee92, size=0x7fffc32c8cd0) at grpcomm_bad_module.c:500<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#8 &nbsp;0x00002b6ee86dd511 in ompi_modex_recv_key_value (key=&lt;value<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">optimized<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">out&gt;, source_proc=&lt;value optimized out&gt;, value=0xbb3a00, dtype=14<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">'\016') at<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">runtime/ompi_module_exchange.c:125<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#9 &nbsp;0x00002b6ee86d7ea1 in ompi_proc_set_arch () at proc/proc.c:154<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#10 0x00002b6ee86db1b0 in ompi_mpi_init (argc=15, argv=0x7fffc32c92f8,<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">requested=&lt;value optimized out&gt;, provided=0x7fffc32c917c) at<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">runtime/ompi_mpi_init.c:699<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#11 0x00007fffc32c8e88 in ?? ()<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#12 0x00002b6ee77f8348 in ?? ()<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#13 0x00007fffc32c8e60 in ?? ()<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#14 0x00007fffc32c8e20 in ?? ()<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#15 0x0000000009efa994 in ?? ()<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">#16 0x0000000000000000 in ?? ()<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">_______________________________________________<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">users mailing list<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">--<br></blockquote><blockquote type="cite">*---------------------------------------------------------------------*<br></blockquote><blockquote type="cite"> &nbsp;&nbsp;Gilbert Grosdidier &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="mailto:Gilbert.Grosdidier@in2p3.fr">Gilbert.Grosdidier@in2p3.fr</a><br></blockquote><blockquote type="cite"> &nbsp;&nbsp;LAL / IN2P3 / CNRS &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Phone : +33 1 6446 8909<br></blockquote><blockquote type="cite"> &nbsp;&nbsp;Faculté des Sciences, Bat. 200 &nbsp;&nbsp;&nbsp;&nbsp;Fax &nbsp;&nbsp;: +33 1 6446 8546<br></blockquote><blockquote type="cite"> &nbsp;&nbsp;B.P. 34, F-91898 Orsay Cedex (FRANCE)<br></blockquote><blockquote type="cite">*---------------------------------------------------------------------*<br></blockquote><br><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></div></blockquote></div><br><div apple-content-edited="true"> <div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><span class="Apple-style-span" style="font-family: Helvetica; "><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><span class="Apple-style-span" style="border-collapse: separate; color: rgb(0, 0, 0); font-family: Helvetica; font-size: medium; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; "><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><div><div><font class="Apple-style-span" color="#2100FF" face="Monaco" size="3"><span class="Apple-style-span" style="font-size: 12px; "><b>--</b></span></font></div><div><font class="Apple-style-span" face="Monaco" size="3"><span class="Apple-style-span" style="font-size: 12px; "><font class="Apple-style-span" color="#2100FF"><b>*---------------------------------------------------------------------*</b></font></span></font></div><div><font class="Apple-style-span" face="Monaco" size="3"><span class="Apple-style-span" style="font-size: 12px; "><font class="Apple-style-span" color="#2100FF"><b>&nbsp;&nbsp;Gilbert Grosdidier &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:Gilbert.Grosdidier@in2p3.fr">Gilbert.Grosdidier@in2p3.fr</a></b></font></span></font></div><div><font class="Apple-style-span" face="Monaco" size="3"><span class="Apple-style-span" style="font-size: 12px; "><font class="Apple-style-span" color="#2100FF"><b>&nbsp;&nbsp;LAL / IN2P3 / CNRS &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Phone : +33 1 6446 8909</b></font></span></font></div><div><font class="Apple-style-span" face="Monaco" size="3"><span class="Apple-style-span" style="font-size: 12px; "><font class="Apple-style-span" color="#2100FF"><b>&nbsp;&nbsp;Faculté des Sciences, Bat. 200 &nbsp; &nbsp; Fax &nbsp; : +33 1 6446 8546</b></font></span></font></div><div><font class="Apple-style-span" face="Monaco" size="3"><span class="Apple-style-span" style="font-size: 12px; "><font class="Apple-style-span" color="#2100FF"><b>&nbsp;&nbsp;B.P. 34, F-91898 Orsay Cedex (FRANCE)</b></font></span></font></div><div><font class="Apple-style-span" face="Monaco" size="3"><span class="Apple-style-span" style="font-size: 12px; "><font class="Apple-style-span" color="#2100FF"><b>*---------------------------------------------------------------------*</b></font></span></font></div><div><font class="Apple-style-span" color="#2100FF" face="Monaco"><b><br></b></font></div></div></div></span><br class="Apple-interchange-newline"></div></span><br class="Apple-interchange-newline"></div><br class="Apple-interchange-newline"> </div><br></body></html>
