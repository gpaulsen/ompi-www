<div dir="ltr">Hi Ralph,<div><br></div><div>Attached is the map and reservaion output  (I was adjusting the number of spawned ranks using an env. variable.</div><div>I had one master which spawned 23 children.</div><div><br></div><div>Howard</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">2015-06-11 12:39 GMT-06:00 Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span>:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">Howard: could you add —display-devel-map —display-allocation and send the output along? I’d like to see why it things you are oversubscribed.<div><br></div><div>Thanks</div><div><br></div><div><br><div><blockquote type="cite"><div>On Jun 11, 2015, at 11:36 AM, Howard Pritchard &lt;<a href="mailto:hppritcha@gmail.com" target="_blank">hppritcha@gmail.com</a>&gt; wrote:</div><br><div><div dir="ltr">Hi Ken,<div><br></div><div>Could you post the output of your ompi_info?</div><div><br></div><div>I have PrgEnv-gnu/5.2.56 and gcc/4.9.2 loaded in my env on nersc system.  Following configure line:</div><div><br></div><div>./configure --enable-mpi-java --prefix=my_favorite_install_location</div><div><br></div><div>The general rule of thumb on cray&#39;s with master (not with older versions though) is you should be able to</div><div>do a ./configure (install location) and you&#39;re ready to go, no need for complicated platform files, etc.</div><div>to just build vanilla.</div><div><br></div><div>As you&#39;re probably guessing, I&#39;m going to say it works for me, at least up to 68 slave ranks.</div><div><br></div><div>I do notice there&#39;s some glitch with the mapping of the ranks though.  The binding logic seems</div><div>to think there&#39;s oversubscription of cores even when there should not be.  I had to use the</div><div><br></div><div>--bind-to none</div><div><br></div><div>option on the command line once I asked for more than 22 slave ranks.  edison system has</div><div>has 24 cores/node.</div><div><br></div><div>Howard</div><div><br></div><div><br></div><div class="gmail_extra"><br><div class="gmail_quote">2015-06-11 12:10 GMT-06:00 Leiter, Kenneth W CIV USARMY ARL (US) <span dir="ltr">&lt;<a href="mailto:kenneth.w.leiter2.civ@mail.mil" target="_blank">kenneth.w.leiter2.civ@mail.mil</a>&gt;</span>:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">I will try on a non-cray machine as well.<br>
<br>
- Ken<br>
<br>
-----Original Message-----<br>
From: users [mailto:<a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a>] On Behalf Of Howard Pritchard<br>
Sent: Thursday, June 11, 2015 12:21 PM<br>
To: Open MPI Users<br>
Subject: Re: [OMPI users] orted segmentation fault in pmix on master<br>
<br>
Hello Ken,<br>
<br>
Could you give the details of the allocation request (qsub args) as well as the mpirun command line args? I&#39;m trying to reproduce on the nersc system.<br>
<br>
It would be interesting if you have access to a similar size non-cray cluster if you get the same problems.<br>
<br>
Howard<br>
<br>
<br>
2015-06-11 9:13 GMT-06:00 Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a> &lt;mailto:<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt; &gt;:<br>
<br>
<br>
        I don’t have a Cray, but let me see if I can reproduce this on something else<br>
<br>
        &gt; On Jun 11, 2015, at 7:26 AM, Leiter, Kenneth W CIV USARMY ARL (US) &lt;<a href="mailto:kenneth.w.leiter2.civ@mail.mil" target="_blank">kenneth.w.leiter2.civ@mail.mil</a> &lt;mailto:<a href="mailto:kenneth.w.leiter2.civ@mail.mil" target="_blank">kenneth.w.leiter2.civ@mail.mil</a>&gt; &gt; wrote:<br>
        &gt;<br>
        &gt; Hello,<br>
        &gt;<br>
        &gt; I am attempting to use the openmpi development master for a code that uses<br>
        &gt; dynamic process management (i.e. MPI_Comm_spawn) on our Cray XC40 at the<br>
        &gt; Army Research Laboratory. After reading through the mailing list I came to<br>
        &gt; the conclusion that the master branch is the only hope for getting this to<br>
        &gt; work on the newer Cray machines.<br>
        &gt;<br>
        &gt; To test I am using the cpi-master.c cpi-worker.c example. The test works<br>
        &gt; when executing on a small number of processors, five or less, but begins to<br>
        &gt; fail with segmentation faults in orted when using more processors. Even with<br>
        &gt; five or fewer processors, I am spreading the computation to more than one<br>
        &gt; node. I am using the cray ugni btl through the alps scheduler.<br>
        &gt;<br>
        &gt; I get a core file from orted and have the seg fault tracked down to<br>
        &gt; pmix_server_process_msgs.c:420 where req-&gt;proxy is NULL. I have tried<br>
        &gt; reading the code to understand how this happens, but am unsure. I do see<br>
        &gt; that in the if statement where I take the else branch, the other branch<br>
        &gt; specifically checks &quot;if (NULL == req-&gt;proxy)&quot; - however, no such check is<br>
        &gt; done the the else branch.<br>
        &gt;<br>
        &gt; I have debug output dumped for the failing runs. I can provide the output<br>
        &gt; along with ompi_info output and config.log to anyone who is interested.<br>
        &gt;<br>
        &gt; - Ken Leiter<br>
        &gt;<br>
        &gt; _______________________________________________<br>
        &gt; users mailing list<br>
        &gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
        &gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
        &gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/06/27094.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/06/27094.php</a><br>
<br>
        _______________________________________________<br>
        users mailing list<br>
        <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
        Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
        Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/06/27095.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/06/27095.php</a><br>
<br>
<br>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/06/27103.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/06/27103.php</a><br></blockquote></div><br></div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/06/27104.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/06/27104.php</a></div></blockquote></div><br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/06/27105.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/06/27105.php</a><br></blockquote></div><br></div>

