I confess I am now confused. What version of OMPI are you using?<div><br></div><div>FWIW: OMPI was updated at some point to detect the actual cores of an external binding, and abide by them. If we aren&#39;t doing that, then we have a bug that needs to be resolved. Or it could be you are using a version that predates the change.</div>
<div><br></div><div>Thanks</div><div>Ralph</div><div><br><br><div class="gmail_quote">On Mon, Nov 15, 2010 at 5:38 AM, Reuti <span dir="ltr">&lt;<a href="mailto:reuti@staff.uni-marburg.de">reuti@staff.uni-marburg.de</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">Hi,<br>
<br>
Am 15.11.2010 um 13:13 schrieb Chris Jewell:<br>
<div class="im"><br>
&gt; Okay so I tried what you suggested.  You essentially get the requested number of bound cores on each execution node, so if I use<br>
&gt;<br>
&gt; $ qsub -pe openmpi 8 -binding linear:2 &lt;<a href="http://myscript.com" target="_blank">myscript.com</a>&gt;<br>
&gt;<br>
&gt; then I get 2 bound cores per node, irrespective of the number of slots (and hence parallel) processes allocated by GE.  This is irrespective of which setting I use for the allocation_rule.<br>
<br>
</div>but it should work fine with an &quot;allocation_rule 2&quot; then.<br>
<div class="im"><br>
<br>
&gt; My aim with this was to deal with badly behaved multithreaded algorithms<br>
<br>
</div>Yep, this causes sometimes the overloading of a machine. When I know that I want to compile a parallel Open MPI application, I use non-threaded versions of ATLAS, MKL or other libraries.<br>
<div class="im"><br>
<br>
&gt; which end up spreading across more cores on an execution node than the number of GE-allocated slots (thereby interfering with other GE scheduled tasks running on the same exec node).  By binding a process to one or more cores, one can &quot;box in&quot; processes and prevent them from spawning erroneous sub-processes and threads.  Unfortunately, the above solution sets the same core binding for each execution node to be the same.<br>

&gt;<br>
&gt;&gt; From exploring the software (both OpenMPI and GE) further, I have two comments:<br>
&gt;<br>
&gt; 1) The core binding feature in GE appears to apply the requested core-binding topology to every execution node involved in a parallel job, rather than assuming that the topology requested is *per parallel process*.  So, if I request &#39;qsub -pe mpi 8 -binding linear:1 &lt;<a href="http://myscript.com" target="_blank">myscript.com</a>&gt;&#39; with the intention of getting each of the 8 parallel processes to be bound to 1 core, I actually get all processes associated with the job_id on one exec node bound to 1 core.  Oops!<br>

&gt;<br>
&gt; 2) OpenMPI has its own core-binding feature (-mca mpi_paffinity_alone 1) which works well to bind each parallel process to one processor.  Unfortunately, the binding framework (hwloc) is different to that which GE uses (PLPA), resulting in binding overlaps between GE-bound tasks (eg serial and smp jobs) and OpenMPI-bound processes (ie my mpi jobs).  Again, oops ;-)<br>

<br>
&gt; If, indeed, it is not possible currently to implement this type of core-binding in tightly integrated OpenMPI/GE, then a solution might lie in a custom script run in the parallel environment&#39;s &#39;start proc args&#39;.  This script would have to find out which slots are allocated where on the cluster, and write an OpenMPI rankfile.<br>

<br>
</div>Exactly this should work.<br>
<br>
If you use &quot;binding_instance&quot; &quot;pe&quot; and reformat the information in the $PE_HOSTFILE to a &quot;rankfile&quot;, it should work to get the desired allocation. Maybe you can share the script with this list once you got it working.<br>

<font color="#888888"><br>
-- Reuti<br>
</font><div><div></div><div class="h5">_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div>

