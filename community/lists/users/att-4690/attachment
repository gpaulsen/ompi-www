<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta content="text/html;charset=ISO-8859-1" http-equiv="Content-Type">
  <title></title>
</head>
<body bgcolor="#ffffff" text="#000000">
<br>
<blockquote cite="midmailman.1144.1197906430.5110.users@open-mpi.org"
 type="cite">
  <pre wrap="">
HTML attachment scrubbed and removed

------------------------------

Message: 2
Date: Sun, 16 Dec 2007 18:49:30 -0500
From: Allan Menezes <a class="moz-txt-link-rfc2396E" href="mailto:amenezes007@sympatico.ca">&lt;amenezes007@sympatico.ca&gt;</a>
Subject: [OMPI users] Gigabit ethernet (PCI Express) and openmpi
	v1.2.4
To: <a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Message-ID: <a class="moz-txt-link-rfc2396E" href="mailto:4765B98A.30803@sympatico.ca">&lt;4765B98A.30803@sympatico.ca&gt;</a>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Hi,
 How many PCI-Express Gigabit ethernet cards does OpenMPI version 1.2.4 
support with a corresponding linear increase in bandwith measured with 
netpipe NPmpi and openmpi mpirun?
With two PCI express cards I get a B/W of 1.75Gbps for 892Mbps each ans 
for three pci express cards ( one built into the motherboard) i get 
1.95Gbps. They all are around 890Mbs indiviually measured with netpipe 
and NPtcp and NPmpi and openmpi. For two it seems there is a linear 
increase in b/w but not for three pci express gigabit eth cards.
I have tune the cards using netpipe and $HOME/.openmpi/mca-params.conf 
file for latency and percentage b/w .
Please advise.
Regards,
Allan Menezes


------------------------------

Message: 3
Date: Mon, 17 Dec 2007 14:14:42 +0200
From: <a class="moz-txt-link-abbreviated" href="mailto:glebn@voltaire.com">glebn@voltaire.com</a> (Gleb Natapov)
Subject: Re: [OMPI users] Gigabit ethernet (PCI Express) and openmpi
	v1.2.4
To: Open MPI Users <a class="moz-txt-link-rfc2396E" href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a>
Message-ID: <a class="moz-txt-link-rfc2396E" href="mailto:20071217121442.GD28587@minantech.com">&lt;20071217121442.GD28587@minantech.com&gt;</a>
Content-Type: text/plain; charset=us-ascii

On Sun, Dec 16, 2007 at 06:49:30PM -0500, Allan Menezes wrote:
  </pre>
  <blockquote type="cite">
    <pre wrap="">Hi,
 How many PCI-Express Gigabit ethernet cards does OpenMPI version 1.2.4 
support with a corresponding linear increase in bandwith measured with 
netpipe NPmpi and openmpi mpirun?
With two PCI express cards I get a B/W of 1.75Gbps for 892Mbps each ans 
for three pci express cards ( one built into the motherboard) i get 
1.95Gbps. They all are around 890Mbs indiviually measured with netpipe 
and NPtcp and NPmpi and openmpi. For two it seems there is a linear 
increase in b/w but not for three pci express gigabit eth cards.
I have tune the cards using netpipe and $HOME/.openmpi/mca-params.conf 
file for latency and percentage b/w .
Please advise.
    </pre>
  </blockquote>
  <pre wrap=""><!---->What is in your $HOME/.openmpi/mca-params.conf? May be are hitting your
chipset limit here. What is your HW configuration? Can you try to run
NPtcp on each interface simultaneously and see what BW do you get.

--
			Gleb.


  </pre>
</blockquote>
Hi ,<br>
My mca-params.conf file is:<br>
btl_tcp_latency_eth0=171<br>
btl_tcp_latency_eth2=50<br>
btl_tcp_latency_eth3=71<br>
btl_tcp_bandwidth_eth0=34<br>
btl_tcp_bandwidth_eth2=33<br>
btl_tcp_bandwidth_eth3=33<br>
<br>
HW config:<br>
host a1:<br>
On x4 PCI express a Syskonnect PCI express x1 gigabit ethernet card.<br>
On x16 PCI express a Intel pro 1000 pt gigabit pci express x1 gigabit
ethernet card.<br>
Built in mobo pci express gigabit ethernet card e1000 intel 82566DM
chipset<br>
all MTUs = 1500<br>
host a2: same hardware config as host a1<br>
I measure the latency and b/w this way:<br>
a1#&gt; ./NPtcp<br>
a2#&gt;./NPtcp -h 192.168.1.1 -n 50 for eth0<br>
a2#&gt;. /NPtcp -h 192.168.5.1 -n 50 for eth2<br>
a2#&gt;./NPtcp -h 192.168.8.1 -n 50 for eth3<br>
and I use the measurement straight at 64bytes as 171 micro secs for eth0<br>
etc .. and the highest band width.<br>
The bandwith measured for eth0 syskonnect 892Mbs latency 171microseconds<br>
The bandwith measured for eth2 intel pro 1000 pt 892Mbs latency
50microseconds<br>
The bandwith measured for eth3 intel built in pci ex 888Mbs latency
71microsecond#!/bin/sh<br>
Linux: FC8 kernel 2.6.23.11 with marvell drivers patch 10.22.4.3<br>
and intel e1000 version 7.6.12 from the intel website<br>
This is how i use /opt/openmpi124b to check the b/w:<br>
and the b/w i measure is max of 1950Mbps for three 890 Mbps gigabit pci
express eth cards with gigabit switches for each subnet!<br>
a1$&gt;mpirun --prefix /opt/openmpi124b --host a1,a2 -mca btl
tcp,sm,self<br>
&nbsp;-mca btl_tcp_if_include eth0,eth3,eth2<br>
&nbsp;-mca btl_tcp_if_exclude lo,eth1,eth4 -mca oob_tcp_include
eth0,eth3,eth2<br>
&nbsp;-mca oob_tcp_exclude lo,eth1,eth4 -np 2 ./NPmpi<br>
The motherboards are Asus P5B-VM DO and processors pentium-d intel 945<br>
each with 2gigabytes of ddr2 667mhz ram.<br>
Any help would bet appreciated.<br>
Thank you,<br>
Allan Menezes<br>
</body>
</html>

