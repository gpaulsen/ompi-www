<div dir="ltr"><br><div class="gmail_extra"><br clear="all"><div><div dir="ltr"><div style>Dear George,</div><div style><br></div><div style>Please see below.</div><div style><br></div></div></div><br><div class="gmail_quote">

On 29 September 2013 01:03, George Bosilca <span dir="ltr">&lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank">bosilca@icl.utk.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">

<div style="word-wrap:break-word"><br><div><div class="im"><div>On Sep 29, 2013, at 01:19 , Huangwei &lt;<a href="mailto:hz283@cam.ac.uk" target="_blank">hz283@cam.ac.uk</a>&gt; wrote:</div><br><blockquote type="cite"><div dir="ltr">

Dear All, <div><br></div><div>In my code I implement mpi_send/mpi_receive for an three dimensional real array, and process is as follows:</div><div><br></div><div>all other processors send the array to rank 0 and then rank 0 receives the array and put these arrays into a complete array. Then mpi_bcast is called to send the complete array from rank 0 to all others. </div>

</div></blockquote><div><br></div></div><div>This pattern of communication reminds me of an MPI_Allgather (or the more flexible version MPI_Allgatherv). </div></div></div></blockquote><div style><font color="#0000ff">I tried  MPI_Allgatherv in my case and found that it is a little slower than mpi_send and mpi_recv pairs. The array that needed to be transferred is not small. Generally, from your experience which option is more efficient (need less wall time for this data transferring of large data). Thanks. </font></div>

<blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div style="word-wrap:break-word"><div><blockquote type="cite">

<div dir="ltr">

<div>This is very basic usage of mpi_send and mpi_receive. In my fortran code I found that if I added call mpi_barrier(...) before the mpi_send and mpi_receive statements the wall time (60s) for this sending and receiving will be much lower than that if mpi_barrier is not called (2s). I used mpi_wtime to count the time. </div>

</div></blockquote><div><br></div><div>In a parallel application each process is out of sync to the others. I have no idea how you measure your time in the original version but I guess that in the MPI_Barrier case you start your timer after the barrier. As the barrier put in sync all processes, you only measure the real time to exchange the data, which might seem shorter.</div>

<div class="im"><div><br></div><blockquote type="cite"><div dir="ltr"><div>I think mpi_send and mpi_recv are blocking subroutines and thus no additional mpi_barrier is needed. Can anybody tell me what is the reason for this phenomena? Thank you very much. </div>

</div></blockquote><div><br></div></div><div>Yes, these operations are indeed blocking, which is why you see the slowdown. If one single process is late to send its contribution, the entire operation is be penalized (as the root , aka. process zero, is waiting for contributions in order). So you should either try to use the collective pattern I highlighted before, switch to using non-blocking point-to-point instead of blocking, or look into the potential benefit of using a non-blocking collective.</div>

<div><br></div><div>  George.</div><br><blockquote type="cite"><div dir="ltr"><div> <br clear="all">

<div><div dir="ltr"><div>best regards,<br>Huangwei</div><div><span style="font-family:&#39;Monotype Corsiva&#39;;font-size:12pt"><br></span></div><div> </div><div><br> </div><span></span><span></span><span></span></div>

</div>
</div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>

</div><br></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div></div>

