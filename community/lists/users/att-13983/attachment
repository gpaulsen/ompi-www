Hi Gus,<br><br>1 - first of all, turning off hyper-threading is not an option. And it gives pretty good results if I can find a way to arrange the cores.<br><br>2 - Actually Eugene (one of her messages in this thread) had suggested to arrange the slots. <br>
I did and wrote the results, it delivers the cores randomly, nothing changed.<br>but I haven&#39;t checked loadbalance option. -byslot or -bynode is not gonna help.<br><br>3 - Could you give me a bit more detail how affinity works? or what it does actually?<br>
<br>Thanks a lot for your suggestions<br><br>Saygin<br><br><div class="gmail_quote">On Wed, Aug 11, 2010 at 6:18 PM, Gus Correa <span dir="ltr">&lt;<a href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">Hi Saygin<br>
<br>
You could:<br>
<br>
1) turn off hyperthreading (on BIOS), or<br>
<br>
2) use the mpirun options (you didn&#39;t send your mpirun command)<br>
to distribute the processes across the nodes, cores, etc.<br>
&quot;man mpirun&quot; is a good resource, see the explanations about<br>
the -byslot, -bynode, -loadbalance options.<br>
<br>
3) In addition, you can use the mca parameters to set processor affinity<br>
in the mpirun command line &quot;mpirun -mca mpi_paffinity_alone 1 ...&quot;<br>
I don&#39;t know how this will play in a hyperthreaded machine,<br>
but it works fine in our dual processor quad-core computers<br>
(not hyperthreaded).<br>
<br>
Depending on your code, hyperthreading may not help performance anyway.<br>
<br>
I hope this helps,<br>
Gus Correa<br>
<br>
Saygin Arkan wrote:<br>
<blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;"><div><div></div><div class="h5">
Hello,<br>
<br>
I&#39;m running mpi jobs in non-homogeneous cluster. 4 of my machines have the following properties, os221, os222, os223, os224:<br>
<br>
vendor_id       : GenuineIntel<br>
cpu family      : 6<br>
model           : 23<br>
model name      : Intel(R) Core(TM)2 Quad  CPU   Q9300  @ 2.50GHz<br>
stepping        : 7<br>
cache size      : 3072 KB<br>
physical id     : 0<br>
siblings        : 4<br>
core id         : 3<br>
cpu cores       : 4<br>
fpu             : yes<br>
fpu_exception   : yes<br>
cpuid level     : 10<br>
wp              : yes<br>
flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good pni monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr sse4_1 lahf_lm<br>

bogomips        : 4999.40<br>
clflush size    : 64<br>
cache_alignment : 64<br>
address sizes   : 36 bits physical, 48 bits virtual<br>
<br>
and the problematic, hyper-threaded 2 machines are as follows, os228 and os229:<br>
<br>
vendor_id       : GenuineIntel<br>
cpu family      : 6<br>
model           : 26<br>
model name      : Intel(R) Core(TM) i7 CPU         920  @ 2.67GHz<br>
stepping        : 5<br>
cache size      : 8192 KB<br>
physical id     : 0<br>
siblings        : 8<br>
core id         : 3<br>
cpu cores       : 4<br>
fpu             : yes<br>
fpu_exception   : yes<br>
cpuid level     : 11<br>
wp              : yes<br>
flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good pni monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr sse4_1 sse4_2 popcnt lahf_lm ida<br>

bogomips        : 5396.88<br>
clflush size    : 64<br>
cache_alignment : 64<br>
address sizes   : 36 bits physical, 48 bits virtual<br>
<br>
<br>
The problem is: those 2 machines seem to be having 8 cores (virtually, actualy core number is 4).<br>
When I submit an MPI job, I calculated the comparison times in the cluster. I got strange results.<br>
<br>
I&#39;m running the job on 6 nodes, 3 core per node. And sometimes ( I can say 1/3 of the tests) os228 or os229 returns strange results. 2 cores are slow (slower than the first 4 nodes) but the 3rd core is extremely fast.<br>

<br>
2010-08-05 14:30:58,926 50672 DEBUG [0x7fcadf98c740] - RANK(0) Printing Times...<br>
2010-08-05 14:30:58,926 50672 DEBUG [0x7fcadf98c740] - os221 RANK(1)    :38 sec<br>
2010-08-05 14:30:58,926 50672 DEBUG [0x7fcadf98c740] - os222 RANK(2)    :38 sec<br>
2010-08-05 14:30:58,926 50672 DEBUG [0x7fcadf98c740] - os224 RANK(3)    :38 sec<br>
2010-08-05 14:30:58,926 50672 DEBUG [0x7fcadf98c740] - os228 RANK(4)    :37 sec<br>
2010-08-05 14:30:58,926 50672 DEBUG [0x7fcadf98c740] - os229 RANK(5)    :34 sec<br>
2010-08-05 14:30:58,926 50672 DEBUG [0x7fcadf98c740] - os223 RANK(6)    :38 sec<br>
2010-08-05 14:30:58,926 50672 DEBUG [0x7fcadf98c740] - os221 RANK(7)    :39 sec<br>
2010-08-05 14:30:58,926 50672 DEBUG [0x7fcadf98c740] - os222 RANK(8)    :37 sec<br>
2010-08-05 14:30:58,926 50672 DEBUG [0x7fcadf98c740] - os224 RANK(9)    :38 sec<br>
2010-08-05 14:30:58,926 50672 DEBUG [0x7fcadf98c740] - os228 RANK(10)    :*48 sec*<br>
2010-08-05 14:30:58,926 50672 DEBUG [0x7fcadf98c740] - os229 RANK(11)    :35 sec<br>
2010-08-05 14:30:58,926 50672 DEBUG [0x7fcadf98c740] - os223 RANK(12)    :38 sec<br>
2010-08-05 14:30:58,926 50672 DEBUG [0x7fcadf98c740] - os221 RANK(13)    :37 sec<br>
2010-08-05 14:30:58,926 50673 DEBUG [0x7fcadf98c740] - os222 RANK(14)    :37 sec<br>
2010-08-05 14:30:58,926 50673 DEBUG [0x7fcadf98c740] - os224 RANK(15)    :38 sec<br>
2010-08-05 14:30:58,926 50673 DEBUG [0x7fcadf98c740] - os228 RANK(16)    :*43 sec*<br>
2010-08-05 14:30:58,926 50673 DEBUG [0x7fcadf98c740] - os229 RANK(17)    :35 sec<br>
TOTAL CORRELATION TIME: 48 sec<br>
<br>
<br>
or another test:<br>
<br>
2010-08-09 15:28:10,947 272904 DEBUG [0x7f27dec27740] - RANK(0) Printing Times...<br>
2010-08-09 15:28:10,947 272904 DEBUG [0x7f27dec27740] - os221 RANK(1)    :170 sec<br>
2010-08-09 15:28:10,947 272904 DEBUG [0x7f27dec27740] - os222 RANK(2)    :161 sec<br>
2010-08-09 15:28:10,947 272904 DEBUG [0x7f27dec27740] - os224 RANK(3)    :158 sec<br>
2010-08-09 15:28:10,947 272904 DEBUG [0x7f27dec27740] - os228 RANK(4)    :142 sec<br>
2010-08-09 15:28:10,947 272904 DEBUG [0x7f27dec27740] - os229 RANK(5)    :*256 sec*<br>
2010-08-09 15:28:10,947 272904 DEBUG [0x7f27dec27740] - os223 RANK(6)    :156 sec<br>
2010-08-09 15:28:10,947 272904 DEBUG [0x7f27dec27740] - os221 RANK(7)    :162 sec<br>
2010-08-09 15:28:10,947 272905 DEBUG [0x7f27dec27740] - os222 RANK(8)    :159 sec<br>
2010-08-09 15:28:10,947 272905 DEBUG [0x7f27dec27740] - os224 RANK(9)    :168 sec<br>
2010-08-09 15:28:10,947 272905 DEBUG [0x7f27dec27740] - os228 RANK(10)    :141 sec<br>
2010-08-09 15:28:10,947 272905 DEBUG [0x7f27dec27740] - os229 RANK(11)    :136 sec<br>
2010-08-09 15:28:10,947 272905 DEBUG [0x7f27dec27740] - os223 RANK(12)    :173 sec<br>
2010-08-09 15:28:10,947 272905 DEBUG [0x7f27dec27740] - os221 RANK(13)    :164 sec<br>
2010-08-09 15:28:10,947 272905 DEBUG [0x7f27dec27740] - os222 RANK(14)    :171 sec<br>
2010-08-09 15:28:10,947 272905 DEBUG [0x7f27dec27740] - os224 RANK(15)    :156 sec<br>
2010-08-09 15:28:10,947 272905 DEBUG [0x7f27dec27740] - os228 RANK(16)    :136 sec<br>
2010-08-09 15:28:10,947 272905 DEBUG [0x7f27dec27740] - os229 RANK(17)    :*250 sec*<br>
2010-08-09 15:28:10,947 272905 DEBUG [0x7f27dec27740] - TOTAL CORRELATION TIME: 256 sec<br>
<br>
<br>
Do you have any idea? Why it is happening?<br>
I assume that it gives 2 jobs to 2 cores in os229, but actually those 2 are one core.<br>
Do you have any idea? If you have, how can I fix it? because the longest time affects the whole time information. 100 sec delay is too much for 250 sec comparison time,<br>
and it might have finish around 160 sec.<br>
<br>
<br>
<br>
-- <br>
Saygin<br>
<br>
<br></div></div>
------------------------------------------------------------------------<div class="im"><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></blockquote><div><div></div><div class="h5">
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br><br clear="all"><br>-- <br>Saygin<br><br><br>

