<div dir="ltr">Jeff,<div><br></div><div>I tried your script and I saw:</div><div><br></div><div><div><font face="courier new, monospace">(1027) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2/bin/mpirun -np 8 ./script.sh</font></div>

<div><font face="courier new, monospace">(1028) $ </font></div></div><div><br></div><div>Now, the very first time I ran it, I think I might have noticed a blip of orted on the nodes, but it disappeared fast. When I re-run the same command, it just seems to exit immediately with nothing showing up.</div>

<div><br></div><div>If I use my &quot;debug-patch&quot; version, I see:</div><div><br></div><div><div><font face="courier new, monospace">(1028) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2-debug-patch//bin/mpirun -np 8 ./script.sh</font></div>

<div><font face="courier new, monospace">hello world</font></div><div><font face="courier new, monospace">hello world</font></div><div><font face="courier new, monospace">hello world</font></div><div><font face="courier new, monospace">hello world</font></div>

<div><font face="courier new, monospace">hello world</font></div><div><font face="courier new, monospace">hello world</font></div><div><font face="courier new, monospace">hello world</font></div><div><font face="courier new, monospace">hello world</font></div>

</div><div><br></div><div>And, well, it&#39;s there for 10 minutes, I&#39;m guessing. If I ssh to another of the nodes in my allocation:</div><div><br></div><div><div><font face="courier new, monospace">(1005) $ ps aux | grep openmpi</font></div>

<div><font face="courier new, monospace">mathomp4 20317  0.0  0.0  59952  4256 ?        S    09:17   0:00 /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2-debug-patch/bin/orted -mca orte_ess_jobid 1842544640 -mca orte_ess_vpid 1 -mca orte_ess_num_procs 6 -mca orte_hnp_uri 1842544640.0;tcp://<a href="http://10.1.24.169">10.1.24.169</a>,172.31.1.254,<a href="http://10.12.24.169:41684">10.12.24.169:41684</a></font></div>

<div><font face="courier new, monospace">mathomp4 20389  0.0  0.0   5524   844 pts/0    S+   09:19   0:00 grep --color=auto openmpi</font></div></div><div><br></div><div><br></div><div>Matt</div></div><div class="gmail_extra">

<br><br><div class="gmail_quote">On Tue, Sep 2, 2014 at 5:35 PM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">

Matt --<br>
<br>
We were discussing this issue on our weekly OMPI engineering call today.<br>
<br>
Can you check one thing for me?  With the un-edited 1.8.2 tarball installation, I see that you&#39;re getting no output for commands that you run -- but also no errors.<br>
<br>
Can you verify and see if your commands are actually *running*?  E.g, try:<br>
<br>
$ cat &gt; script.sh &lt;&lt;EOF<br>
#!/bin/sh<br>
echo hello world<br>
sleep 600<br>
echo goodbye world<br>
EOF<br>
$ chmod +x script.sh<br>
$ setenv OMPI_MCA_shmem_mmap_enable_nfs_warning 0<br>
$ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2-clean/bin/mpirun -np 8 script.sh<br>
<br>
and then go &quot;ps&quot; on the back-end nodes and see if there is an &quot;orted&quot; process and N &quot;sleep 600&quot; processes running on them.<br>
<br>
I&#39;m *assuming* you won&#39;t see the &quot;hello world&quot; output.<br>
<br>
The purpose of this test is that I want to see if OMPI is just totally erring out and not even running your job (which is quite unlikely; OMPI should be much more noisy when this happens), or whether we&#39;re simply not seeing the stdout from the job.<br>


<br>
Thanks.<br>
<div><div class="h5"><br>
<br>
<br>
On Sep 2, 2014, at 9:36 AM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com">fortran@gmail.com</a>&gt; wrote:<br>
<br>
&gt; On that machine, it would be SLES 11 SP1. I think it&#39;s soon transitioning to SLES 11 SP3.<br>
&gt;<br>
&gt; I also use Open MPI on an RHEL 6.5 box (possibly soon to be RHEL 7).<br>
&gt;<br>
&gt;<br>
&gt; On Mon, Sep 1, 2014 at 8:41 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt; Thanks - I expect we&#39;ll have to release 1.8.3 soon to fix this in case others have similar issues. Out of curiosity, what OS are you using?<br>
&gt;<br>
&gt;<br>
&gt; On Sep 1, 2014, at 9:00 AM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com">fortran@gmail.com</a>&gt; wrote:<br>
&gt;<br>
&gt;&gt; Ralph,<br>
&gt;&gt;<br>
&gt;&gt; Okay that seems to have done it here (well, minus the usual shmem_mmap_enable_nfs_warning that our system always generates):<br>
&gt;&gt;<br>
&gt;&gt; (1033) $ setenv OMPI_MCA_shmem_mmap_enable_nfs_warning 0<br>
&gt;&gt; (1034) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2-debug-patch/bin/mpirun -np 8 ./helloWorld.182-debug-patch.x<br>
&gt;&gt; Process    7 of    8 is on borg01w218<br>
&gt;&gt; Process    5 of    8 is on borg01w218<br>
&gt;&gt; Process    1 of    8 is on borg01w218<br>
&gt;&gt; Process    3 of    8 is on borg01w218<br>
&gt;&gt; Process    0 of    8 is on borg01w218<br>
&gt;&gt; Process    2 of    8 is on borg01w218<br>
&gt;&gt; Process    4 of    8 is on borg01w218<br>
&gt;&gt; Process    6 of    8 is on borg01w218<br>
&gt;&gt;<br>
&gt;&gt; I&#39;ll ask the admin to apply the patch locally...and wait for 1.8.3, I suppose.<br>
&gt;&gt;<br>
&gt;&gt; Thanks,<br>
&gt;&gt; Matt<br>
&gt;&gt;<br>
&gt;&gt; On Sun, Aug 31, 2014 at 10:08 AM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt;&gt; Hmmm....I may see the problem. Would you be so kind as to apply the attached patch to your 1.8.2 code, rebuild, and try again?<br>
&gt;&gt;<br>
&gt;&gt; Much appreciate the help. Everyone&#39;s system is slightly different, and I think you&#39;ve uncovered one of those differences.<br>
&gt;&gt; Ralph<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; On Aug 31, 2014, at 6:25 AM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com">fortran@gmail.com</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt;&gt; Ralph,<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Sorry it took me a bit of time. Here you go:<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; (1002) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2-debug/bin/mpirun --leave-session-attached --debug-daemons --mca oob_base_verbose 10 -mca plm_base_verbose 5 -np 8 ./helloWorld.182-debug.x<br>


&gt;&gt;&gt; [borg01w063:03815] mca:base:select:(  plm) Querying component [isolated]<br>
&gt;&gt;&gt; [borg01w063:03815] mca:base:select:(  plm) Query of component [isolated] set priority to 0<br>
&gt;&gt;&gt; [borg01w063:03815] mca:base:select:(  plm) Querying component [rsh]<br>
&gt;&gt;&gt; [borg01w063:03815] [[INVALID],INVALID] plm:rsh_lookup on agent ssh : rsh path NULL<br>
&gt;&gt;&gt; [borg01w063:03815] mca:base:select:(  plm) Query of component [rsh] set priority to 10<br>
&gt;&gt;&gt; [borg01w063:03815] mca:base:select:(  plm) Querying component [slurm]<br>
&gt;&gt;&gt; [borg01w063:03815] [[INVALID],INVALID] plm:slurm: available for selection<br>
&gt;&gt;&gt; [borg01w063:03815] mca:base:select:(  plm) Query of component [slurm] set priority to 75<br>
&gt;&gt;&gt; [borg01w063:03815] mca:base:select:(  plm) Selected component [slurm]<br>
&gt;&gt;&gt; [borg01w063:03815] plm:base:set_hnp_name: initial bias 3815 nodename hash 1757783593<br>
&gt;&gt;&gt; [borg01w063:03815] plm:base:set_hnp_name: final jobfam 49163<br>
&gt;&gt;&gt; [borg01w063:03815] mca: base: components_register: registering oob components<br>
&gt;&gt;&gt; [borg01w063:03815] mca: base: components_register: found loaded component tcp<br>
&gt;&gt;&gt; [borg01w063:03815] mca: base: components_register: component tcp register function successful<br>
&gt;&gt;&gt; [borg01w063:03815] mca: base: components_open: opening oob components<br>
&gt;&gt;&gt; [borg01w063:03815] mca: base: components_open: found loaded component tcp<br>
&gt;&gt;&gt; [borg01w063:03815] mca: base: components_open: component tcp open function successful<br>
&gt;&gt;&gt; [borg01w063:03815] mca:oob:select: checking available component tcp<br>
&gt;&gt;&gt; [borg01w063:03815] mca:oob:select: Querying component [tcp]<br>
&gt;&gt;&gt; [borg01w063:03815] oob:tcp: component_available called<br>
&gt;&gt;&gt; [borg01w063:03815] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w063:03815] WORKING INTERFACE 2 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w063:03815] WORKING INTERFACE 3 KERNEL INDEX 2 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] oob:tcp:init adding 10.1.24.63 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w063:03815] WORKING INTERFACE 4 KERNEL INDEX 4 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] oob:tcp:init adding 172.31.1.254 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w063:03815] WORKING INTERFACE 5 KERNEL INDEX 5 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] oob:tcp:init adding 10.12.24.63 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] TCP STARTUP<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] attempting to bind to IPv4 port 0<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] assigned IPv4 port 41373<br>
&gt;&gt;&gt; [borg01w063:03815] mca:oob:select: Adding component to end<br>
&gt;&gt;&gt; [borg01w063:03815] mca:oob:select: Found 1 active transports<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:base:receive start comm<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:base:setup_job<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:slurm: LAUNCH DAEMONS CALLED<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:base:setup_vm<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:base:setup_vm creating map<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:base:setup_vm add new daemon [[49163,0],1]<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:base:setup_vm assigning new daemon [[49163,0],1] to node borg01w064<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:base:setup_vm add new daemon [[49163,0],2]<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:base:setup_vm assigning new daemon [[49163,0],2] to node borg01w065<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:base:setup_vm add new daemon [[49163,0],3]<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:base:setup_vm assigning new daemon [[49163,0],3] to node borg01w069<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:base:setup_vm add new daemon [[49163,0],4]<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:base:setup_vm assigning new daemon [[49163,0],4] to node borg01w070<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:base:setup_vm add new daemon [[49163,0],5]<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:base:setup_vm assigning new daemon [[49163,0],5] to node borg01w071<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:slurm: launching on nodes borg01w064,borg01w065,borg01w069,borg01w070,borg01w071<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:slurm: Set prefix:/discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2-debug<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:slurm: final top-level argv:<br>
&gt;&gt;&gt;     srun --ntasks-per-node=1 --kill-on-bad-exit --cpu_bind=none --nodes=5 --nodelist=borg01w064,borg01w065,borg01w069,borg01w070,borg01w071 --ntasks=5 orted -mca orte_debug_daemons 1 -mca orte_leave_session_attached 1 -mca orte_ess_jobid 3221946368 -mca orte_ess_vpid 1 -mca orte_ess_num_procs 6 -mca orte_hnp_uri 3221946368.0;tcp://<a href="http://10.1.24.63" target="_blank">10.1.24.63</a>,172.31.1.254,<a href="http://10.12.24.63:41373" target="_blank">10.12.24.63:41373</a> --mca oob_base_verbose 10 -mca plm_base_verbose 5<br>


&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:slurm: reset PATH: /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2-debug/bin:/usr/local/other/SLES11/gcc/4.9.1/bin:/usr/local/other/SLES11.1/git/<a href="http://1.8.5.2/libexec/git-core:/usr/local/other/SLES11.1/git/1.8.5.2/bin:/usr/local/other/SLES11/svn/1.6.17/bin:/usr/local/other/SLES11/tkcvs/8.2.3/gcc-4.3.2/bin:.:/home/mathomp4/bin:/home/mathomp4/cvstools:/discover/nobackup/projects/gmao/share/dasilva/opengrads/Contents:/usr/local/other/Htop/1.0/bin:/usr/local/other/SLES11/gnuplot/4.6.0/gcc-4.3.2/bin:/usr/local/other/SLES11/xpdf/3.03-gcc-4.3.2/bin:/home/mathomp4/src/pdtoolkit-3.16/x86_64/bin:/discover/nobackup/mathomp4/WavewatchIII-GMAO/bin:/discover/nobackup/mathomp4/WavewatchIII-GMAO/exe:/usr/local/other/pods:/usr/local/other/SLES11.1/R/3.1.0/gcc-4.3.4/lib64/R/bin:.:/home/mathomp4/bin:/home/mathomp4/cvstools:/discover/nobackup/projects/gmao/share/dasilva/opengrads/Contents:/usr/local/other/Htop/1.0/bin:/usr/local/other/SLES11/gnuplot/4.6" target="_blank">1.8.5.2/libexec/git-core:/usr/local/other/SLES11.1/git/1.8.5.2/bin:/usr/local/other/SLES11/svn/1.6.17/bin:/usr/local/other/SLES11/tkcvs/8.2.3/gcc-4.3.2/bin:.:/home/mathomp4/bin:/home/mathomp4/cvstools:/discover/nobackup/projects/gmao/share/dasilva/opengrads/Contents:/usr/local/other/Htop/1.0/bin:/usr/local/other/SLES11/gnuplot/4.6.0/gcc-4.3.2/bin:/usr/local/other/SLES11/xpdf/3.03-gcc-4.3.2/bin:/home/mathomp4/src/pdtoolkit-3.16/x86_64/bin:/discover/nobackup/mathomp4/WavewatchIII-GMAO/bin:/discover/nobackup/mathomp4/WavewatchIII-GMAO/exe:/usr/local/other/pods:/usr/local/other/SLES11.1/R/3.1.0/gcc-4.3.4/lib64/R/bin:.:/home/mathomp4/bin:/home/mathomp4/cvstools:/discover/nobackup/projects/gmao/share/dasilva/opengrads/Contents:/usr/local/other/Htop/1.0/bin:/usr/local/other/SLES11/gnuplot/4.6</a><br>


 .0/gcc-4.3.2/bin:/usr/local/other/SLES11/xpdf/3.03-gcc-4.3.2/bin:/home/mathomp4/src/pdtoolkit-3.16/x86_64/bin:/discover/nobackup/mathomp4/WavewatchIII-GMAO/bin:/discover/nobackup/mathomp4/WavewatchIII-GMAO/exe:/usr/local/other/pods:/usr/local/other/SLES11.1/R/3.1.0/gcc-4.3.4/lib64/R/bin:/home/mathomp4/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/usr/bin/X11:/usr/X11R6/bin:/usr/games:/opt/kde3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/usr/slurm/bin<br>


&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:slurm: reset LD_LIBRARY_PATH: /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2-debug/lib:/usr/local/other/SLES11/gcc/4.9.1/lib64:/usr/local/other/SLES11.1/git/<a href="http://1.8.5.2/lib:/usr/local/other/SLES11/svn/1.6.17/lib:/usr/local/other/SLES11/tkcvs/8.2.3/gcc-4.3.2/lib" target="_blank">1.8.5.2/lib:/usr/local/other/SLES11/svn/1.6.17/lib:/usr/local/other/SLES11/tkcvs/8.2.3/gcc-4.3.2/lib</a><br>


&gt;&gt;&gt; srun.slurm: cluster configuration lacks support for cpu binding<br>
&gt;&gt;&gt; srun.slurm: cluster configuration lacks support for cpu binding<br>
&gt;&gt;&gt; [borg01w065:15893] mca: base: components_register: registering oob components<br>
&gt;&gt;&gt; [borg01w065:15893] mca: base: components_register: found loaded component tcp<br>
&gt;&gt;&gt; [borg01w065:15893] mca: base: components_register: component tcp register function successful<br>
&gt;&gt;&gt; [borg01w065:15893] mca: base: components_open: opening oob components<br>
&gt;&gt;&gt; [borg01w065:15893] mca: base: components_open: found loaded component tcp<br>
&gt;&gt;&gt; [borg01w065:15893] mca: base: components_open: component tcp open function successful<br>
&gt;&gt;&gt; [borg01w065:15893] mca:oob:select: checking available component tcp<br>
&gt;&gt;&gt; [borg01w065:15893] mca:oob:select: Querying component [tcp]<br>
&gt;&gt;&gt; [borg01w065:15893] oob:tcp: component_available called<br>
&gt;&gt;&gt; [borg01w065:15893] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w065:15893] WORKING INTERFACE 2 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w065:15893] WORKING INTERFACE 3 KERNEL INDEX 2 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w065:15893] [[49163,0],2] oob:tcp:init adding 10.1.24.65 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w065:15893] WORKING INTERFACE 4 KERNEL INDEX 4 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w065:15893] [[49163,0],2] oob:tcp:init adding 172.31.1.254 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w065:15893] WORKING INTERFACE 5 KERNEL INDEX 5 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w065:15893] [[49163,0],2] oob:tcp:init adding 10.12.24.65 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w065:15893] [[49163,0],2] TCP STARTUP<br>
&gt;&gt;&gt; [borg01w065:15893] [[49163,0],2] attempting to bind to IPv4 port 0<br>
&gt;&gt;&gt; [borg01w065:15893] [[49163,0],2] assigned IPv4 port 43456<br>
&gt;&gt;&gt; [borg01w065:15893] mca:oob:select: Adding component to end<br>
&gt;&gt;&gt; [borg01w065:15893] mca:oob:select: Found 1 active transports<br>
&gt;&gt;&gt; [borg01w070:12645] mca: base: components_register: registering oob components<br>
&gt;&gt;&gt; [borg01w070:12645] mca: base: components_register: found loaded component tcp<br>
&gt;&gt;&gt; [borg01w070:12645] mca: base: components_register: component tcp register function successful<br>
&gt;&gt;&gt; [borg01w070:12645] mca: base: components_open: opening oob components<br>
&gt;&gt;&gt; [borg01w070:12645] mca: base: components_open: found loaded component tcp<br>
&gt;&gt;&gt; [borg01w070:12645] mca: base: components_open: component tcp open function successful<br>
&gt;&gt;&gt; [borg01w070:12645] mca:oob:select: checking available component tcp<br>
&gt;&gt;&gt; [borg01w070:12645] mca:oob:select: Querying component [tcp]<br>
&gt;&gt;&gt; [borg01w070:12645] oob:tcp: component_available called<br>
&gt;&gt;&gt; [borg01w070:12645] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w070:12645] WORKING INTERFACE 2 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w070:12645] WORKING INTERFACE 3 KERNEL INDEX 2 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w070:12645] [[49163,0],4] oob:tcp:init adding 10.1.24.70 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w070:12645] WORKING INTERFACE 4 KERNEL INDEX 4 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w070:12645] [[49163,0],4] oob:tcp:init adding 172.31.1.254 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w070:12645] WORKING INTERFACE 5 KERNEL INDEX 5 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w070:12645] [[49163,0],4] oob:tcp:init adding 10.12.24.70 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w070:12645] [[49163,0],4] TCP STARTUP<br>
&gt;&gt;&gt; [borg01w070:12645] [[49163,0],4] attempting to bind to IPv4 port 0<br>
&gt;&gt;&gt; [borg01w070:12645] [[49163,0],4] assigned IPv4 port 53062<br>
&gt;&gt;&gt; [borg01w070:12645] mca:oob:select: Adding component to end<br>
&gt;&gt;&gt; [borg01w070:12645] mca:oob:select: Found 1 active transports<br>
&gt;&gt;&gt; [borg01w064:16565] mca: base: components_register: registering oob components<br>
&gt;&gt;&gt; [borg01w064:16565] mca: base: components_register: found loaded component tcp<br>
&gt;&gt;&gt; [borg01w064:16565] mca: base: components_register: component tcp register function successful<br>
&gt;&gt;&gt; [borg01w071:14879] mca: base: components_register: registering oob components<br>
&gt;&gt;&gt; [borg01w071:14879] mca: base: components_register: found loaded component tcp<br>
&gt;&gt;&gt; [borg01w064:16565] mca: base: components_open: opening oob components<br>
&gt;&gt;&gt; [borg01w064:16565] mca: base: components_open: found loaded component tcp<br>
&gt;&gt;&gt; [borg01w064:16565] mca: base: components_open: component tcp open function successful<br>
&gt;&gt;&gt; [borg01w064:16565] mca:oob:select: checking available component tcp<br>
&gt;&gt;&gt; [borg01w064:16565] mca:oob:select: Querying component [tcp]<br>
&gt;&gt;&gt; [borg01w064:16565] oob:tcp: component_available called<br>
&gt;&gt;&gt; [borg01w064:16565] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w064:16565] WORKING INTERFACE 2 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w064:16565] WORKING INTERFACE 3 KERNEL INDEX 2 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w064:16565] [[49163,0],1] oob:tcp:init adding 10.1.24.64 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w064:16565] WORKING INTERFACE 4 KERNEL INDEX 4 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w064:16565] [[49163,0],1] oob:tcp:init adding 172.31.1.254 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w064:16565] WORKING INTERFACE 5 KERNEL INDEX 5 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w064:16565] [[49163,0],1] oob:tcp:init adding 10.12.24.64 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w064:16565] [[49163,0],1] TCP STARTUP<br>
&gt;&gt;&gt; [borg01w064:16565] [[49163,0],1] attempting to bind to IPv4 port 0<br>
&gt;&gt;&gt; [borg01w064:16565] [[49163,0],1] assigned IPv4 port 43828<br>
&gt;&gt;&gt; [borg01w064:16565] mca:oob:select: Adding component to end<br>
&gt;&gt;&gt; [borg01w069:30276] mca: base: components_register: registering oob components<br>
&gt;&gt;&gt; [borg01w069:30276] mca: base: components_register: found loaded component tcp<br>
&gt;&gt;&gt; [borg01w071:14879] mca: base: components_register: component tcp register function successful<br>
&gt;&gt;&gt; [borg01w069:30276] mca: base: components_register: component tcp register function successful<br>
&gt;&gt;&gt; [borg01w071:14879] mca: base: components_open: opening oob components<br>
&gt;&gt;&gt; [borg01w071:14879] mca: base: components_open: found loaded component tcp<br>
&gt;&gt;&gt; [borg01w071:14879] mca: base: components_open: component tcp open function successful<br>
&gt;&gt;&gt; [borg01w071:14879] mca:oob:select: checking available component tcp<br>
&gt;&gt;&gt; [borg01w071:14879] mca:oob:select: Querying component [tcp]<br>
&gt;&gt;&gt; [borg01w071:14879] oob:tcp: component_available called<br>
&gt;&gt;&gt; [borg01w069:30276] mca: base: components_open: opening oob components<br>
&gt;&gt;&gt; [borg01w069:30276] mca: base: components_open: found loaded component tcp<br>
&gt;&gt;&gt; [borg01w069:30276] mca: base: components_open: component tcp open function successful<br>
&gt;&gt;&gt; [borg01w071:14879] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w071:14879] WORKING INTERFACE 2 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w071:14879] WORKING INTERFACE 3 KERNEL INDEX 2 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w071:14879] [[49163,0],5] oob:tcp:init adding 10.1.24.71 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w071:14879] WORKING INTERFACE 4 KERNEL INDEX 4 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w069:30276] mca:oob:select: checking available component tcp<br>
&gt;&gt;&gt; [borg01w069:30276] mca:oob:select: Querying component [tcp]<br>
&gt;&gt;&gt; [borg01w069:30276] oob:tcp: component_available called<br>
&gt;&gt;&gt; [borg01w071:14879] [[49163,0],5] oob:tcp:init adding 172.31.1.254 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w071:14879] WORKING INTERFACE 5 KERNEL INDEX 5 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w071:14879] [[49163,0],5] oob:tcp:init adding 10.12.24.71 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w071:14879] [[49163,0],5] TCP STARTUP<br>
&gt;&gt;&gt; [borg01w069:30276] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w069:30276] WORKING INTERFACE 2 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w069:30276] WORKING INTERFACE 3 KERNEL INDEX 2 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w069:30276] [[49163,0],3] oob:tcp:init adding 10.1.24.69 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w069:30276] WORKING INTERFACE 4 KERNEL INDEX 4 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w069:30276] [[49163,0],3] oob:tcp:init adding 172.31.1.254 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w069:30276] WORKING INTERFACE 5 KERNEL INDEX 5 FAMILY: V4<br>
&gt;&gt;&gt; [borg01w069:30276] [[49163,0],3] oob:tcp:init adding 10.12.24.69 to our list of V4 connections<br>
&gt;&gt;&gt; [borg01w069:30276] [[49163,0],3] TCP STARTUP<br>
&gt;&gt;&gt; [borg01w071:14879] [[49163,0],5] attempting to bind to IPv4 port 0<br>
&gt;&gt;&gt; [borg01w069:30276] [[49163,0],3] attempting to bind to IPv4 port 0<br>
&gt;&gt;&gt; [borg01w069:30276] [[49163,0],3] assigned IPv4 port 39299<br>
&gt;&gt;&gt; [borg01w064:16565] mca:oob:select: Found 1 active transports<br>
&gt;&gt;&gt; [borg01w069:30276] mca:oob:select: Adding component to end<br>
&gt;&gt;&gt; [borg01w069:30276] mca:oob:select: Found 1 active transports<br>
&gt;&gt;&gt; [borg01w071:14879] [[49163,0],5] assigned IPv4 port 56113<br>
&gt;&gt;&gt; [borg01w071:14879] mca:oob:select: Adding component to end<br>
&gt;&gt;&gt; [borg01w071:14879] mca:oob:select: Found 1 active transports<br>
&gt;&gt;&gt; srun.slurm: error: borg01w064: task 0: Exited with exit code 213<br>
&gt;&gt;&gt; srun.slurm: Terminating job step 2347743.3<br>
&gt;&gt;&gt; srun.slurm: Job step aborted: Waiting up to 2 seconds for job step to finish.<br>
&gt;&gt;&gt; [borg01w070:12645] [[49163,0],4] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161<br>
&gt;&gt;&gt; [borg01w070:12645] [[49163,0],4] ORTE_ERROR_LOG: Bad parameter in file routed_binomial.c at line 498<br>
&gt;&gt;&gt; [borg01w070:12645] [[49163,0],4] ORTE_ERROR_LOG: Bad parameter in file base/ess_base_std_orted.c at line 539<br>
&gt;&gt;&gt; [borg01w065:15893] [[49163,0],2] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161<br>
&gt;&gt;&gt; [borg01w065:15893] [[49163,0],2] ORTE_ERROR_LOG: Bad parameter in file routed_binomial.c at line 498<br>
&gt;&gt;&gt; [borg01w065:15893] [[49163,0],2] ORTE_ERROR_LOG: Bad parameter in file base/ess_base_std_orted.c at line 539<br>
&gt;&gt;&gt; slurmd[borg01w065]: *** STEP 2347743.3 KILLED AT 2014-08-31T09:24:17 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt; slurmd[borg01w070]: *** STEP 2347743.3 KILLED AT 2014-08-31T09:24:17 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt; [borg01w064:16565] [[49163,0],1] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161<br>
&gt;&gt;&gt; [borg01w064:16565] [[49163,0],1] ORTE_ERROR_LOG: Bad parameter in file routed_binomial.c at line 498<br>
&gt;&gt;&gt; [borg01w064:16565] [[49163,0],1] ORTE_ERROR_LOG: Bad parameter in file base/ess_base_std_orted.c at line 539<br>
&gt;&gt;&gt; [borg01w069:30276] [[49163,0],3] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161<br>
&gt;&gt;&gt; [borg01w069:30276] [[49163,0],3] ORTE_ERROR_LOG: Bad parameter in file routed_binomial.c at line 498<br>
&gt;&gt;&gt; [borg01w069:30276] [[49163,0],3] ORTE_ERROR_LOG: Bad parameter in file base/ess_base_std_orted.c at line 539<br>
&gt;&gt;&gt; slurmd[borg01w069]: *** STEP 2347743.3 KILLED AT 2014-08-31T09:24:17 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt; [borg01w071:14879] [[49163,0],5] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161<br>
&gt;&gt;&gt; [borg01w071:14879] [[49163,0],5] ORTE_ERROR_LOG: Bad parameter in file routed_binomial.c at line 498<br>
&gt;&gt;&gt; [borg01w071:14879] [[49163,0],5] ORTE_ERROR_LOG: Bad parameter in file base/ess_base_std_orted.c at line 539<br>
&gt;&gt;&gt; slurmd[borg01w071]: *** STEP 2347743.3 KILLED AT 2014-08-31T09:24:17 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt; slurmd[borg01w065]: *** STEP 2347743.3 KILLED AT 2014-08-31T09:24:17 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt; slurmd[borg01w069]: *** STEP 2347743.3 KILLED AT 2014-08-31T09:24:17 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt; slurmd[borg01w070]: *** STEP 2347743.3 KILLED AT 2014-08-31T09:24:17 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt; slurmd[borg01w071]: *** STEP 2347743.3 KILLED AT 2014-08-31T09:24:17 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt; srun.slurm: error: borg01w069: task 2: Exited with exit code 213<br>
&gt;&gt;&gt; srun.slurm: error: borg01w065: task 1: Exited with exit code 213<br>
&gt;&gt;&gt; srun.slurm: error: borg01w071: task 4: Exited with exit code 213<br>
&gt;&gt;&gt; srun.slurm: error: borg01w070: task 3: Exited with exit code 213<br>
&gt;&gt;&gt; sh: tcp://<a href="http://10.1.24.63" target="_blank">10.1.24.63</a>,172.31.1.254,<a href="http://10.12.24.63:41373" target="_blank">10.12.24.63:41373</a>: No such file or directory<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:slurm: primary daemons complete!<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] plm:base:receive stop comm<br>
&gt;&gt;&gt; [borg01w063:03815] [[49163,0],0] TCP SHUTDOWN<br>
&gt;&gt;&gt; [borg01w063:03815] mca: base: close: component tcp closed<br>
&gt;&gt;&gt; [borg01w063:03815] mca: base: close: unloading component tcp<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Fri, Aug 29, 2014 at 3:18 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt;&gt;&gt; Rats - I also need &quot;-mca plm_base_verbose 5&quot; on there so I can see the cmd line being executed. Can you add it?<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Aug 29, 2014, at 11:16 AM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com">fortran@gmail.com</a>&gt; wrote:<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Ralph,<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Here you go:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; (1080) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2-debug/bin/mpirun --leave-session-attached --debug-daemons --mca oob_base_verbose 10 -np 8 ./helloWorld.182-debug.x<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] mca: base: components_register: registering oob components<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] mca: base: components_register: found loaded component tcp<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] mca: base: components_register: component tcp register function successful<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] mca: base: components_open: opening oob components<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] mca: base: components_open: found loaded component tcp<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] mca: base: components_open: component tcp open function successful<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] mca:oob:select: checking available component tcp<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] mca:oob:select: Querying component [tcp]<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] oob:tcp: component_available called<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] WORKING INTERFACE 2 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] WORKING INTERFACE 3 KERNEL INDEX 2 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] [[52298,0],0] oob:tcp:init adding 10.1.25.142 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] WORKING INTERFACE 4 KERNEL INDEX 4 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] [[52298,0],0] oob:tcp:init adding 172.31.1.254 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] WORKING INTERFACE 5 KERNEL INDEX 5 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] [[52298,0],0] oob:tcp:init adding 10.12.25.142 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] [[52298,0],0] TCP STARTUP<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] [[52298,0],0] attempting to bind to IPv4 port 0<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] [[52298,0],0] assigned IPv4 port 41686<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] mca:oob:select: Adding component to end<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] mca:oob:select: Found 1 active transports<br>
&gt;&gt;&gt;&gt; srun.slurm: cluster configuration lacks support for cpu binding<br>
&gt;&gt;&gt;&gt; srun.slurm: cluster configuration lacks support for cpu binding<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] mca: base: components_register: registering oob components<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] mca: base: components_register: found loaded component tcp<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] mca: base: components_register: registering oob components<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] mca: base: components_register: found loaded component tcp<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] mca: base: components_register: component tcp register function successful<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] mca: base: components_open: opening oob components<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] mca: base: components_open: found loaded component tcp<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] mca: base: components_open: component tcp open function successful<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] mca:oob:select: checking available component tcp<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] mca:oob:select: Querying component [tcp]<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] oob:tcp: component_available called<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] WORKING INTERFACE 2 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] WORKING INTERFACE 3 KERNEL INDEX 2 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] [[52298,0],4] oob:tcp:init adding 10.1.25.153 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] WORKING INTERFACE 4 KERNEL INDEX 4 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] [[52298,0],4] oob:tcp:init adding 172.31.1.254 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] WORKING INTERFACE 5 KERNEL INDEX 5 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] [[52298,0],4] oob:tcp:init adding 10.12.25.153 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] [[52298,0],4] TCP STARTUP<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] [[52298,0],4] attempting to bind to IPv4 port 0<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] mca: base: components_register: component tcp register function successful<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] [[52298,0],4] assigned IPv4 port 38028<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] mca: base: components_open: opening oob components<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] mca: base: components_open: found loaded component tcp<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] mca: base: components_open: component tcp open function successful<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] mca:oob:select: checking available component tcp<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] mca:oob:select: Querying component [tcp]<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] oob:tcp: component_available called<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] WORKING INTERFACE 2 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] WORKING INTERFACE 3 KERNEL INDEX 2 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] [[52298,0],1] oob:tcp:init adding 10.1.25.143 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] WORKING INTERFACE 4 KERNEL INDEX 4 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] [[52298,0],1] oob:tcp:init adding 172.31.1.254 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] WORKING INTERFACE 5 KERNEL INDEX 5 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] [[52298,0],1] oob:tcp:init adding 10.12.25.143 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] [[52298,0],1] TCP STARTUP<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] [[52298,0],1] attempting to bind to IPv4 port 0<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] mca:oob:select: Adding component to end<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] mca:oob:select: Found 1 active transports<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] [[52298,0],1] assigned IPv4 port 44719<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] mca:oob:select: Adding component to end<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] mca:oob:select: Found 1 active transports<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] mca: base: components_register: registering oob components<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] mca: base: components_register: found loaded component tcp<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] mca: base: components_register: component tcp register function successful<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] mca: base: components_open: opening oob components<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] mca: base: components_open: found loaded component tcp<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] mca: base: components_open: component tcp open function successful<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] mca:oob:select: checking available component tcp<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] mca:oob:select: Querying component [tcp]<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] oob:tcp: component_available called<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] WORKING INTERFACE 2 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] WORKING INTERFACE 3 KERNEL INDEX 2 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] [[52298,0],2] oob:tcp:init adding 10.1.25.144 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] WORKING INTERFACE 4 KERNEL INDEX 4 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] [[52298,0],2] oob:tcp:init adding 172.31.1.254 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] WORKING INTERFACE 5 KERNEL INDEX 5 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] [[52298,0],2] oob:tcp:init adding 10.12.25.144 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] [[52298,0],2] TCP STARTUP<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] [[52298,0],2] attempting to bind to IPv4 port 0<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] [[52298,0],2] assigned IPv4 port 40700<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] mca:oob:select: Adding component to end<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] mca:oob:select: Found 1 active transports<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] mca: base: components_register: registering oob components<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] mca: base: components_register: found loaded component tcp<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] mca: base: components_register: component tcp register function successful<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] mca: base: components_open: opening oob components<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] mca: base: components_open: found loaded component tcp<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] mca: base: components_open: component tcp open function successful<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] mca:oob:select: checking available component tcp<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] mca:oob:select: Querying component [tcp]<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] oob:tcp: component_available called<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] WORKING INTERFACE 2 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] WORKING INTERFACE 3 KERNEL INDEX 2 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] [[52298,0],5] oob:tcp:init adding 10.1.25.154 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] WORKING INTERFACE 4 KERNEL INDEX 4 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] [[52298,0],5] oob:tcp:init adding 172.31.1.254 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] WORKING INTERFACE 5 KERNEL INDEX 5 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] [[52298,0],5] oob:tcp:init adding 10.12.25.154 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] [[52298,0],5] TCP STARTUP<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] [[52298,0],5] attempting to bind to IPv4 port 0<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] [[52298,0],5] assigned IPv4 port 41191<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] mca:oob:select: Adding component to end<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] mca:oob:select: Found 1 active transports<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] mca: base: components_register: registering oob components<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] mca: base: components_register: found loaded component tcp<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] mca: base: components_register: component tcp register function successful<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] mca: base: components_open: opening oob components<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] mca: base: components_open: found loaded component tcp<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] mca: base: components_open: component tcp open function successful<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] mca:oob:select: checking available component tcp<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] mca:oob:select: Querying component [tcp]<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] oob:tcp: component_available called<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] WORKING INTERFACE 2 KERNEL INDEX 1 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] WORKING INTERFACE 3 KERNEL INDEX 2 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] [[52298,0],3] oob:tcp:init adding 10.1.25.145 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] WORKING INTERFACE 4 KERNEL INDEX 4 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] [[52298,0],3] oob:tcp:init adding 172.31.1.254 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] WORKING INTERFACE 5 KERNEL INDEX 5 FAMILY: V4<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] [[52298,0],3] oob:tcp:init adding 10.12.25.145 to our list of V4 connections<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] [[52298,0],3] TCP STARTUP<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] [[52298,0],3] attempting to bind to IPv4 port 0<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] [[52298,0],3] assigned IPv4 port 51079<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] mca:oob:select: Adding component to end<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] mca:oob:select: Found 1 active transports<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] [[52298,0],2] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] [[52298,0],2] ORTE_ERROR_LOG: Bad parameter in file routed_binomial.c at line 498<br>
&gt;&gt;&gt;&gt; [borg01x144:30878] [[52298,0],2] ORTE_ERROR_LOG: Bad parameter in file base/ess_base_std_orted.c at line 539<br>
&gt;&gt;&gt;&gt; srun.slurm: error: borg01x143: task 0: Exited with exit code 213<br>
&gt;&gt;&gt;&gt; srun.slurm: Terminating job step 2332583.24<br>
&gt;&gt;&gt;&gt; slurmd[borg01x144]: *** STEP 2332583.24 KILLED AT 2014-08-29T13:59:30 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt;&gt; srun.slurm: Job step aborted: Waiting up to 2 seconds for job step to finish.<br>
&gt;&gt;&gt;&gt; srun.slurm: error: borg01x153: task 3: Exited with exit code 213<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] [[52298,0],4] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] [[52298,0],4] ORTE_ERROR_LOG: Bad parameter in file routed_binomial.c at line 498<br>
&gt;&gt;&gt;&gt; [borg01x153:01290] [[52298,0],4] ORTE_ERROR_LOG: Bad parameter in file base/ess_base_std_orted.c at line 539<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] [[52298,0],1] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] [[52298,0],1] ORTE_ERROR_LOG: Bad parameter in file routed_binomial.c at line 498<br>
&gt;&gt;&gt;&gt; [borg01x143:13793] [[52298,0],1] ORTE_ERROR_LOG: Bad parameter in file base/ess_base_std_orted.c at line 539<br>
&gt;&gt;&gt;&gt; slurmd[borg01x144]: *** STEP 2332583.24 KILLED AT 2014-08-29T13:59:30 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt;&gt; srun.slurm: error: borg01x144: task 1: Exited with exit code 213<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] [[52298,0],5] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] [[52298,0],5] ORTE_ERROR_LOG: Bad parameter in file routed_binomial.c at line 498<br>
&gt;&gt;&gt;&gt; [borg01x154:01154] [[52298,0],5] ORTE_ERROR_LOG: Bad parameter in file base/ess_base_std_orted.c at line 539<br>
&gt;&gt;&gt;&gt; slurmd[borg01x154]: *** STEP 2332583.24 KILLED AT 2014-08-29T13:59:30 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt;&gt; slurmd[borg01x154]: *** STEP 2332583.24 KILLED AT 2014-08-29T13:59:30 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt;&gt; srun.slurm: error: borg01x154: task 4: Exited with exit code 213<br>
&gt;&gt;&gt;&gt; srun.slurm: error: borg01x145: task 2: Exited with exit code 213<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] [[52298,0],3] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] [[52298,0],3] ORTE_ERROR_LOG: Bad parameter in file routed_binomial.c at line 498<br>
&gt;&gt;&gt;&gt; [borg01x145:02419] [[52298,0],3] ORTE_ERROR_LOG: Bad parameter in file base/ess_base_std_orted.c at line 539<br>
&gt;&gt;&gt;&gt; slurmd[borg01x145]: *** STEP 2332583.24 KILLED AT 2014-08-29T13:59:30 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt;&gt; slurmd[borg01x145]: *** STEP 2332583.24 KILLED AT 2014-08-29T13:59:30 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt;&gt; sh: tcp://<a href="http://10.1.25.142" target="_blank">10.1.25.142</a>,172.31.1.254,<a href="http://10.12.25.142:41686" target="_blank">10.12.25.142:41686</a>: No such file or directory<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] [[52298,0],0] TCP SHUTDOWN<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] mca: base: close: component tcp closed<br>
&gt;&gt;&gt;&gt; [borg01x142:29232] mca: base: close: unloading component tcp<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Note, if I can get the allocation today, I want to try doing all this on a single SandyBridge node, rather than on 6. It might make comparing various runs a bit easier!<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Matt<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; On Fri, Aug 29, 2014 at 12:42 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt;&gt;&gt;&gt; Okay, something quite weird is happening here. I can&#39;t replicate using the 1.8.2 release tarball on a slurm machine, so my guess is that something else is going on here.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Could you please rebuild the 1.8.2 code with --enable-debug on the configure line (assuming you haven&#39;t already done so), and then rerun that version as before but adding &quot;--mca oob_base_verbose 10&quot; to the cmd line?<br>


&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; On Aug 29, 2014, at 4:22 AM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com">fortran@gmail.com</a>&gt; wrote:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; Ralph,<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; For 1.8.2rc4 I get:<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; (1003) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2rc4/bin/mpirun --leave-session-attached --debug-daemons -np 8 ./helloWorld.182.x<br>
&gt;&gt;&gt;&gt;&gt; srun.slurm: cluster configuration lacks support for cpu binding<br>
&gt;&gt;&gt;&gt;&gt; srun.slurm: cluster configuration lacks support for cpu binding<br>
&gt;&gt;&gt;&gt;&gt; Daemon [[47143,0],5] checking in as pid 10990 on host borg01x154<br>
&gt;&gt;&gt;&gt;&gt; [borg01x154:10990] [[47143,0],5] orted: up and running - waiting for commands!<br>
&gt;&gt;&gt;&gt;&gt; Daemon [[47143,0],1] checking in as pid 23473 on host borg01x143<br>
&gt;&gt;&gt;&gt;&gt; Daemon [[47143,0],2] checking in as pid 8250 on host borg01x144<br>
&gt;&gt;&gt;&gt;&gt; [borg01x144:08250] [[47143,0],2] orted: up and running - waiting for commands!<br>
&gt;&gt;&gt;&gt;&gt; [borg01x143:23473] [[47143,0],1] orted: up and running - waiting for commands!<br>
&gt;&gt;&gt;&gt;&gt; Daemon [[47143,0],3] checking in as pid 12320 on host borg01x145<br>
&gt;&gt;&gt;&gt;&gt; Daemon [[47143,0],4] checking in as pid 10902 on host borg01x153<br>
&gt;&gt;&gt;&gt;&gt; [borg01x153:10902] [[47143,0],4] orted: up and running - waiting for commands!<br>
&gt;&gt;&gt;&gt;&gt; [borg01x145:12320] [[47143,0],3] orted: up and running - waiting for commands!<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_cmd: received add_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x144:08250] [[47143,0],2] orted_cmd: received add_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x153:10902] [[47143,0],4] orted_cmd: received add_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x143:23473] [[47143,0],1] orted_cmd: received add_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x145:12320] [[47143,0],3] orted_cmd: received add_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x154:10990] [[47143,0],5] orted_cmd: received add_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_recv: received sync+nidmap from local proc [[47143,1],0]<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_recv: received sync+nidmap from local proc [[47143,1],2]<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_recv: received sync+nidmap from local proc [[47143,1],3]<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_recv: received sync+nidmap from local proc [[47143,1],1]<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_recv: received sync+nidmap from local proc [[47143,1],5]<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_recv: received sync+nidmap from local proc [[47143,1],4]<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_recv: received sync+nidmap from local proc [[47143,1],6]<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_recv: received sync+nidmap from local proc [[47143,1],7]<br>
&gt;&gt;&gt;&gt;&gt;   MPIR_being_debugged = 0<br>
&gt;&gt;&gt;&gt;&gt;   MPIR_debug_state = 1<br>
&gt;&gt;&gt;&gt;&gt;   MPIR_partial_attach_ok = 1<br>
&gt;&gt;&gt;&gt;&gt;   MPIR_i_am_starter = 0<br>
&gt;&gt;&gt;&gt;&gt;   MPIR_forward_output = 0<br>
&gt;&gt;&gt;&gt;&gt;   MPIR_proctable_size = 8<br>
&gt;&gt;&gt;&gt;&gt;   MPIR_proctable:<br>
&gt;&gt;&gt;&gt;&gt;     (i, host, exe, pid) = (0, borg01x142, /home/mathomp4/HelloWorldTest/./helloWorld.182.x, 1647)<br>
&gt;&gt;&gt;&gt;&gt;     (i, host, exe, pid) = (1, borg01x142, /home/mathomp4/HelloWorldTest/./helloWorld.182.x, 1648)<br>
&gt;&gt;&gt;&gt;&gt;     (i, host, exe, pid) = (2, borg01x142, /home/mathomp4/HelloWorldTest/./helloWorld.182.x, 1650)<br>
&gt;&gt;&gt;&gt;&gt;     (i, host, exe, pid) = (3, borg01x142, /home/mathomp4/HelloWorldTest/./helloWorld.182.x, 1652)<br>
&gt;&gt;&gt;&gt;&gt;     (i, host, exe, pid) = (4, borg01x142, /home/mathomp4/HelloWorldTest/./helloWorld.182.x, 1654)<br>
&gt;&gt;&gt;&gt;&gt;     (i, host, exe, pid) = (5, borg01x142, /home/mathomp4/HelloWorldTest/./helloWorld.182.x, 1656)<br>
&gt;&gt;&gt;&gt;&gt;     (i, host, exe, pid) = (6, borg01x142, /home/mathomp4/HelloWorldTest/./helloWorld.182.x, 1658)<br>
&gt;&gt;&gt;&gt;&gt;     (i, host, exe, pid) = (7, borg01x142, /home/mathomp4/HelloWorldTest/./helloWorld.182.x, 1660)<br>
&gt;&gt;&gt;&gt;&gt; MPIR_executable_path: NULL<br>
&gt;&gt;&gt;&gt;&gt; MPIR_server_arguments: NULL<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x144:08250] [[47143,0],2] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x143:23473] [[47143,0],1] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x153:10902] [[47143,0],4] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x154:10990] [[47143,0],5] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x145:12320] [[47143,0],3] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x143:23473] [[47143,0],1] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x144:08250] [[47143,0],2] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x153:10902] [[47143,0],4] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x145:12320] [[47143,0],3] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; Process    2 of    8 is on borg01x142<br>
&gt;&gt;&gt;&gt;&gt; Process    5 of    8 is on borg01x142<br>
&gt;&gt;&gt;&gt;&gt; Process    4 of    8 is on borg01x142<br>
&gt;&gt;&gt;&gt;&gt; Process    1 of    8 is on borg01x142<br>
&gt;&gt;&gt;&gt;&gt; Process    0 of    8 is on borg01x142<br>
&gt;&gt;&gt;&gt;&gt; Process    3 of    8 is on borg01x142<br>
&gt;&gt;&gt;&gt;&gt; Process    6 of    8 is on borg01x142<br>
&gt;&gt;&gt;&gt;&gt; Process    7 of    8 is on borg01x142<br>
&gt;&gt;&gt;&gt;&gt; [borg01x154:10990] [[47143,0],5] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x144:08250] [[47143,0],2] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x143:23473] [[47143,0],1] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x153:10902] [[47143,0],4] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x154:10990] [[47143,0],5] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x145:12320] [[47143,0],3] orted_cmd: received message_local_procs<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_recv: received sync from local proc [[47143,1],2]<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_recv: received sync from local proc [[47143,1],1]<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_recv: received sync from local proc [[47143,1],3]<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_recv: received sync from local proc [[47143,1],0]<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_recv: received sync from local proc [[47143,1],4]<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_recv: received sync from local proc [[47143,1],6]<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_recv: received sync from local proc [[47143,1],5]<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_recv: received sync from local proc [[47143,1],7]<br>
&gt;&gt;&gt;&gt;&gt; [borg01x142:01629] [[47143,0],0] orted_cmd: received exit cmd<br>
&gt;&gt;&gt;&gt;&gt; [borg01x144:08250] [[47143,0],2] orted_cmd: received exit cmd<br>
&gt;&gt;&gt;&gt;&gt; [borg01x144:08250] [[47143,0],2] orted_cmd: all routes and children gone - exiting<br>
&gt;&gt;&gt;&gt;&gt; [borg01x153:10902] [[47143,0],4] orted_cmd: received exit cmd<br>
&gt;&gt;&gt;&gt;&gt; [borg01x153:10902] [[47143,0],4] orted_cmd: all routes and children gone - exiting<br>
&gt;&gt;&gt;&gt;&gt; [borg01x143:23473] [[47143,0],1] orted_cmd: received exit cmd<br>
&gt;&gt;&gt;&gt;&gt; [borg01x154:10990] [[47143,0],5] orted_cmd: received exit cmd<br>
&gt;&gt;&gt;&gt;&gt; [borg01x154:10990] [[47143,0],5] orted_cmd: all routes and children gone - exiting<br>
&gt;&gt;&gt;&gt;&gt; [borg01x145:12320] [[47143,0],3] orted_cmd: received exit cmd<br>
&gt;&gt;&gt;&gt;&gt; [borg01x145:12320] [[47143,0],3] orted_cmd: all routes and children gone - exiting<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; Using the 1.8.2 mpirun:<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; (1004) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2/bin/mpirun --leave-session-attached --debug-daemons -np 8 ./helloWorld.182.x<br>
&gt;&gt;&gt;&gt;&gt; srun.slurm: cluster configuration lacks support for cpu binding<br>
&gt;&gt;&gt;&gt;&gt; srun.slurm: cluster configuration lacks support for cpu binding<br>
&gt;&gt;&gt;&gt;&gt; [borg01x143:23494] [[47330,0],1] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161<br>
&gt;&gt;&gt;&gt;&gt; [borg01x143:23494] [[47330,0],1] ORTE_ERROR_LOG: Bad parameter in file routed_binomial.c at line 498<br>
&gt;&gt;&gt;&gt;&gt; [borg01x143:23494] [[47330,0],1] ORTE_ERROR_LOG: Bad parameter in file base/ess_base_std_orted.c at line 539<br>
&gt;&gt;&gt;&gt;&gt; srun.slurm: error: borg01x143: task 0: Exited with exit code 213<br>
&gt;&gt;&gt;&gt;&gt; srun.slurm: Terminating job step 2332583.4<br>
&gt;&gt;&gt;&gt;&gt; [borg01x153:10915] [[47330,0],4] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161<br>
&gt;&gt;&gt;&gt;&gt; [borg01x153:10915] [[47330,0],4] ORTE_ERROR_LOG: Bad parameter in file routed_binomial.c at line 498<br>
&gt;&gt;&gt;&gt;&gt; [borg01x153:10915] [[47330,0],4] ORTE_ERROR_LOG: Bad parameter in file base/ess_base_std_orted.c at line 539<br>
&gt;&gt;&gt;&gt;&gt; [borg01x144:08263] [[47330,0],2] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161<br>
&gt;&gt;&gt;&gt;&gt; [borg01x144:08263] [[47330,0],2] ORTE_ERROR_LOG: Bad parameter in file routed_binomial.c at line 498<br>
&gt;&gt;&gt;&gt;&gt; [borg01x144:08263] [[47330,0],2] ORTE_ERROR_LOG: Bad parameter in file base/ess_base_std_orted.c at line 539<br>
&gt;&gt;&gt;&gt;&gt; srun.slurm: Job step aborted: Waiting up to 2 seconds for job step to finish.<br>
&gt;&gt;&gt;&gt;&gt; slurmd[borg01x145]: *** STEP 2332583.4 KILLED AT 2014-08-29T07:16:20 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt;&gt;&gt; slurmd[borg01x154]: *** STEP 2332583.4 KILLED AT 2014-08-29T07:16:20 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt;&gt;&gt; slurmd[borg01x153]: *** STEP 2332583.4 KILLED AT 2014-08-29T07:16:20 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt;&gt;&gt; slurmd[borg01x153]: *** STEP 2332583.4 KILLED AT 2014-08-29T07:16:20 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt;&gt;&gt; srun.slurm: error: borg01x144: task 1: Exited with exit code 213<br>
&gt;&gt;&gt;&gt;&gt; slurmd[borg01x144]: *** STEP 2332583.4 KILLED AT 2014-08-29T07:16:20 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt;&gt;&gt; slurmd[borg01x144]: *** STEP 2332583.4 KILLED AT 2014-08-29T07:16:20 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt;&gt;&gt; srun.slurm: error: borg01x153: task 3: Exited with exit code 213<br>
&gt;&gt;&gt;&gt;&gt; slurmd[borg01x154]: *** STEP 2332583.4 KILLED AT 2014-08-29T07:16:20 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt;&gt;&gt; slurmd[borg01x145]: *** STEP 2332583.4 KILLED AT 2014-08-29T07:16:20 WITH SIGNAL 9 ***<br>
&gt;&gt;&gt;&gt;&gt; srun.slurm: error: borg01x154: task 4: Killed<br>
&gt;&gt;&gt;&gt;&gt; srun.slurm: error: borg01x145: task 2: Killed<br>
&gt;&gt;&gt;&gt;&gt; sh: tcp://<a href="http://10.1.25.142" target="_blank">10.1.25.142</a>,172.31.1.254,<a href="http://10.12.25.142:34169" target="_blank">10.12.25.142:34169</a>: No such file or directory<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; On Thu, Aug 28, 2014 at 7:17 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt;&gt;&gt;&gt;&gt; I&#39;m unaware of any changes to the Slurm integration between rc4 and final release. It sounds like this might be something else going on - try adding &quot;--leave-session-attached --debug-daemons&quot; to your 1.8.2 command line and let&#39;s see if any errors get reported.<br>


&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; On Aug 28, 2014, at 12:20 PM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com">fortran@gmail.com</a>&gt; wrote:<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; Open MPI List,<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; I recently encountered an odd bug with Open MPI 1.8.1 and GCC 4.9.1 on our cluster (reported on this list), and decided to try it with 1.8.2. However, we seem to be having an issue with Open MPI 1.8.2 and SLURM. Even weirder, Open MPI 1.8.2rc4 doesn&#39;t show the bug. And the bug is: I get no stdout with Open MPI 1.8.2. That is, HelloWorld doesn&#39;t work.<br>


&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; To wit, our sysadmin has two tarballs:<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; (1441) $ sha1sum openmpi-1.8.2rc4.tar.bz2<br>
&gt;&gt;&gt;&gt;&gt;&gt; 7e7496913c949451f546f22a1a159df25f8bb683  openmpi-1.8.2rc4.tar.bz2<br>
&gt;&gt;&gt;&gt;&gt;&gt; (1442) $ sha1sum openmpi-1.8.2.tar.gz<br>
&gt;&gt;&gt;&gt;&gt;&gt; cf2b1e45575896f63367406c6c50574699d8b2e1  openmpi-1.8.2.tar.gz<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; I then build each with a script in the method our sysadmin usually does:<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; #!/bin/sh<br>
&gt;&gt;&gt;&gt;&gt;&gt; set -x<br>
&gt;&gt;&gt;&gt;&gt;&gt; export PREFIX=/discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2<br>
&gt;&gt;&gt;&gt;&gt;&gt; export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/nlocal/slurm/2.6.3/lib64<br>
&gt;&gt;&gt;&gt;&gt;&gt; build() {<br>
&gt;&gt;&gt;&gt;&gt;&gt;   echo `pwd`<br>
&gt;&gt;&gt;&gt;&gt;&gt;   ./configure --with-slurm --disable-wrapper-rpath --enable-shared --enable-mca-no-build=btl-usnic \<br>
&gt;&gt;&gt;&gt;&gt;&gt;       CC=gcc CXX=g++ F77=gfortran FC=gfortran \<br>
&gt;&gt;&gt;&gt;&gt;&gt;       CFLAGS=&quot;-mtune=generic -fPIC -m64&quot; CXXFLAGS=&quot;-mtune=generic -fPIC -m64&quot; FFLAGS=&quot;-mtune=generic -fPIC -m64&quot; \<br>
&gt;&gt;&gt;&gt;&gt;&gt;       F77FLAGS=&quot;-mtune=generic -fPIC -m64&quot; FCFLAGS=&quot;-mtune=generic -fPIC -m64&quot; F90FLAGS=&quot;-mtune=generic -fPIC -m64&quot; \<br>
&gt;&gt;&gt;&gt;&gt;&gt;       LDFLAGS=&quot;-L/usr/nlocal/slurm/2.6.3/lib64&quot; CPPFLAGS=&quot;-I/usr/nlocal/slurm/2.6.3/include&quot; LIBS=&quot;-lpciaccess&quot; \<br>
&gt;&gt;&gt;&gt;&gt;&gt;      --prefix=${PREFIX} 2&gt;&amp;1 | tee configure.1.8.2.log<br>
&gt;&gt;&gt;&gt;&gt;&gt;   make 2&gt;&amp;1 | tee make.1.8.2.log<br>
&gt;&gt;&gt;&gt;&gt;&gt;   make check 2&gt;&amp;1 | tee makecheck.1.8.2.log<br>
&gt;&gt;&gt;&gt;&gt;&gt;   make install 2&gt;&amp;1 | tee makeinstall.1.8.2.log<br>
&gt;&gt;&gt;&gt;&gt;&gt; }<br>
&gt;&gt;&gt;&gt;&gt;&gt; echo &quot;calling build&quot;<br>
&gt;&gt;&gt;&gt;&gt;&gt; build<br>
&gt;&gt;&gt;&gt;&gt;&gt; echo &quot;exiting&quot;<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; The only difference between the two is &#39;1.8.2&#39; or &#39;1.8.2rc4&#39; in the PREFIX and log file tees.  Now, let us test. First, I grab some nodes with slurm:<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; $ salloc --nodes=6 --ntasks-per-node=16 --constraint=sand --time=09:00:00 --account=g0620 --mail-type=BEGIN<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; Once I get my nodes, I run with 1.8.2rc4:<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; (1142) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2rc4/bin/mpifort -o helloWorld.182rc4.x helloWorld.F90<br>
&gt;&gt;&gt;&gt;&gt;&gt; (1143) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2rc4/bin/mpirun -np 8 ./helloWorld.182rc4.x<br>
&gt;&gt;&gt;&gt;&gt;&gt; Process    0 of    8 is on borg01w044<br>
&gt;&gt;&gt;&gt;&gt;&gt; Process    5 of    8 is on borg01w044<br>
&gt;&gt;&gt;&gt;&gt;&gt; Process    3 of    8 is on borg01w044<br>
&gt;&gt;&gt;&gt;&gt;&gt; Process    7 of    8 is on borg01w044<br>
&gt;&gt;&gt;&gt;&gt;&gt; Process    1 of    8 is on borg01w044<br>
&gt;&gt;&gt;&gt;&gt;&gt; Process    2 of    8 is on borg01w044<br>
&gt;&gt;&gt;&gt;&gt;&gt; Process    4 of    8 is on borg01w044<br>
&gt;&gt;&gt;&gt;&gt;&gt; Process    6 of    8 is on borg01w044<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; Now 1.8.2:<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; (1144) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2/bin/mpifort -o helloWorld.182.x helloWorld.F90<br>
&gt;&gt;&gt;&gt;&gt;&gt; (1145) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2/bin/mpirun -np 8 ./helloWorld.182.x<br>
&gt;&gt;&gt;&gt;&gt;&gt; (1146) $<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; No output at all. But, if I take the helloWorld.x from 1.8.2 and run it with 1.8.2rc4&#39;s mpirun:<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; (1146) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2rc4/bin/mpirun -np 8 ./helloWorld.182.x<br>
&gt;&gt;&gt;&gt;&gt;&gt; Process    5 of    8 is on borg01w044<br>
&gt;&gt;&gt;&gt;&gt;&gt; Process    7 of    8 is on borg01w044<br>
&gt;&gt;&gt;&gt;&gt;&gt; Process    2 of    8 is on borg01w044<br>
&gt;&gt;&gt;&gt;&gt;&gt; Process    4 of    8 is on borg01w044<br>
&gt;&gt;&gt;&gt;&gt;&gt; Process    1 of    8 is on borg01w044<br>
&gt;&gt;&gt;&gt;&gt;&gt; Process    3 of    8 is on borg01w044<br>
&gt;&gt;&gt;&gt;&gt;&gt; Process    6 of    8 is on borg01w044<br>
&gt;&gt;&gt;&gt;&gt;&gt; Process    0 of    8 is on borg01w044<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; So...any idea what is happening here? There did seem to be a few SLURM related changes between the two tarballs involving /dev/null but it&#39;s a bit above me to decipher.<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; You can find the ompi_info, build, make, config, etc logs at these links (they are ~300kB which is over the mailing list limit according to the Open MPI web page):<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; <a href="https://dl.dropboxusercontent.com/u/61696/OMPI-1.8.2rc4-Output.tar.bz2" target="_blank">https://dl.dropboxusercontent.com/u/61696/OMPI-1.8.2rc4-Output.tar.bz2</a><br>
&gt;&gt;&gt;&gt;&gt;&gt; <a href="https://dl.dropboxusercontent.com/u/61696/OMPI-1.8.2-Output.tar.bz2" target="_blank">https://dl.dropboxusercontent.com/u/61696/OMPI-1.8.2-Output.tar.bz2</a><br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; Thank you for any help and please let me know if you need more information,<br>
&gt;&gt;&gt;&gt;&gt;&gt; Matt<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; --<br>
&gt;&gt;&gt;&gt;&gt;&gt; &quot;And, isn&#39;t sanity really just a one-trick pony anyway? I mean all you<br>
&gt;&gt;&gt;&gt;&gt;&gt;  get is one trick: rational thinking. But when you&#39;re good and crazy,<br>
&gt;&gt;&gt;&gt;&gt;&gt;  oooh, oooh, oooh, the sky is the limit!&quot; -- The Tick<br>
&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt;&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt;&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25182.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25182.php</a><br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25184.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25184.php</a><br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; --<br>
&gt;&gt;&gt;&gt;&gt; &quot;And, isn&#39;t sanity really just a one-trick pony anyway? I mean all you<br>
&gt;&gt;&gt;&gt;&gt;  get is one trick: rational thinking. But when you&#39;re good and crazy,<br>
&gt;&gt;&gt;&gt;&gt;  oooh, oooh, oooh, the sky is the limit!&quot; -- The Tick<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25187.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25187.php</a><br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25193.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25193.php</a><br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; --<br>
&gt;&gt;&gt;&gt; &quot;And, isn&#39;t sanity really just a one-trick pony anyway? I mean all you<br>
&gt;&gt;&gt;&gt;  get is one trick: rational thinking. But when you&#39;re good and crazy,<br>
&gt;&gt;&gt;&gt;  oooh, oooh, oooh, the sky is the limit!&quot; -- The Tick<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25196.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25196.php</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25197.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25197.php</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; --<br>
&gt;&gt;&gt; &quot;And, isn&#39;t sanity really just a one-trick pony anyway? I mean all you<br>
&gt;&gt;&gt;  get is one trick: rational thinking. But when you&#39;re good and crazy,<br>
&gt;&gt;&gt;  oooh, oooh, oooh, the sky is the limit!&quot; -- The Tick<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25204.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25204.php</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25205.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25205.php</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; &quot;And, isn&#39;t sanity really just a one-trick pony anyway? I mean all you<br>
&gt;&gt;  get is one trick: rational thinking. But when you&#39;re good and crazy,<br>
&gt;&gt;  oooh, oooh, oooh, the sky is the limit!&quot; -- The Tick<br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/09/25210.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/09/25210.php</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/09/25211.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/09/25211.php</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; &quot;And, isn&#39;t sanity really just a one-trick pony anyway? I mean all you<br>
&gt;  get is one trick: rational thinking. But when you&#39;re good and crazy,<br>
&gt;  oooh, oooh, oooh, the sky is the limit!&quot; -- The Tick<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/09/25219.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/09/25219.php</a><br>
<br>
<br>
--<br>
</div></div>Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<div class=""><br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/09/25232.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/09/25232.php</a><br>
</blockquote></div><br><br clear="all"><div><br></div>-- <br><div dir="ltr"><div>&quot;And, isn&#39;t sanity really just a one-trick pony anyway? I mean all you</div><div> get is one trick: rational thinking. But when you&#39;re good and crazy, </div>

<div> oooh, oooh, oooh, the sky is the limit!&quot; -- The Tick</div><div><br></div></div>
</div>

