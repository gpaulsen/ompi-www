<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML><HEAD>
<META content="text/html; charset=iso-8859-1" http-equiv=Content-Type>
<META content="MSHTML 5.00.3314.2100" name=GENERATOR>
<STYLE></STYLE>
</HEAD>
<BODY bgColor=#ffffff>
<DIV><FONT face=Arial size=2>
<DIV><FONT face=Arial size=2>Isn't in evident from the theory of random 
processes and probability theory that&nbsp;</FONT></FONT><FONT face=Arial 
size=2>in the limit of infinitely </FONT></DIV>
<DIV><FONT face=Arial size=2>large cluster and parallel process, </FONT><FONT 
face=Arial size=2>the probability of deadlocks </FONT><FONT face=Arial 
size=2>with current implementation is unfortunately </FONT></DIV>
<DIV><FONT face=Arial size=2>quite a finite quantity and in limit approaches to 
unity regardless on any particular details of the program.</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Just my two cents.</FONT></DIV>
<DIV><FONT face=Arial size=2>Alex Granovsky</FONT></DIV>
<DIV>&nbsp;</DIV></DIV>
<BLOCKQUOTE 
style="BORDER-LEFT: #000000 2px solid; MARGIN-LEFT: 5px; MARGIN-RIGHT: 0px; PADDING-LEFT: 5px; PADDING-RIGHT: 0px">
  <DIV style="FONT: 10pt arial">----- Original Message ----- </DIV>
  <DIV 
  style="BACKGROUND: #e4e4e4; FONT: 10pt arial; font-color: black"><B>From:</B> 
  <A href="mailto:treumann@us.ibm.com" title=treumann@us.ibm.com>Richard 
  Treumann</A> </DIV>
  <DIV style="FONT: 10pt arial"><B>To:</B> <A href="mailto:users@open-mpi.org" 
  title=users@open-mpi.org>Open MPI Users</A> </DIV>
  <DIV style="FONT: 10pt arial"><B>Sent:</B> Thursday, September 09, 2010 10:10 
  PM</DIV>
  <DIV style="FONT: 10pt arial"><B>Subject:</B> Re: [OMPI users] MPI_Reduce 
  performance</DIV>
  <DIV><BR></DIV><BR><FONT face=sans-serif size=2>I was pointing out that most 
  programs have some degree of elastic synchronization built in. Tasks (or 
  groups or components in a coupled model) seldom only produce data.they also 
  consume what other tasks produce and that limits the potential skew. &nbsp; 
  </FONT><BR><BR><FONT face=sans-serif size=2>If step n for a task (or group or 
  coupled component) depends on data produced by step n-1 in another task 
  &nbsp;(or group or coupled component) &nbsp;then no task can be farther ahead 
  of the task it depends on than one step. &nbsp; If there are 2 tasks that each 
  need the others step n-1 result to compute step n then they can never get 
  farther than one step out of synch. &nbsp;If there were a rank ordered loop of 
  &nbsp;8 tasks so each one needs the output of the prior step on task ((me-1) 
  &nbsp;mod tasks) to compute then you can get more skew because if 
  </FONT><BR><FONT face=sans-serif size=2>task 5 gets stalled in step 3,</FONT> 
  <BR><FONT face=sans-serif size=2>task 6 will finish step 3 and send results to 
  7 but stall on recv for step 4 (lacking the end of step 3 send by task 
  5)</FONT> <BR><FONT face=sans-serif size=2>task 7 will finish step 4 and send 
  results to 0 &nbsp;but stall on recv for step 5</FONT> <BR><FONT 
  face=sans-serif size=2>task 0 will finish step 5 and send results to 1 
  &nbsp;but stall on recv for step 6</FONT> <BR><FONT face=sans-serif 
  size=2>etc</FONT> <BR><BR><FONT face=sans-serif size=2>In a 2D or 3D grid, the 
  dependency is tighter so the possible skew is less. but it is still 
  significant on a huge grid &nbsp; In a program with frequent calls to 
  MPI_Allreduce on COMM_WORLD, the skew is very limited. The available skew gets 
  harder to predict as the interdependencies grow more complex.</FONT> 
  <BR><BR><FONT face=sans-serif size=2>I call this "elasticity" because the 
  amount of stretch varies but, like a bungee cord or an waist band, only goes 
  so far. Every parallel program has some degree of elasticity built into the 
  way its parts interact.</FONT> <BR><BR><FONT face=sans-serif size=2>I assume a 
  coupler has some elasticity too. That is, ocean and atmosphere each model 
  Monday and report in to coupler but neither can model Tuesday until they get 
  some of the Monday results generated by the other. (I am pretending 
  granularity is day by day) &nbsp;Wouldn't the right level of synchronization 
  among component result automatically form the data dependencies among 
  them?</FONT> <BR><BR><FONT face=sans-serif size=2>&nbsp;</FONT> <BR><BR><FONT 
  face=sans-serif size=2>Dick Treumann &nbsp;- &nbsp;MPI Team &nbsp; &nbsp; 
  &nbsp; &nbsp; &nbsp; <BR>IBM Systems &amp; Technology Group<BR>Dept X2ZA / MS 
  P963 -- 2455 South Road -- Poughkeepsie, NY 12601<BR>Tele (845) 433-7846 
  &nbsp; &nbsp; &nbsp; &nbsp; Fax (845) 433-8363<BR></FONT><BR><BR><BR>
  <TABLE width="100%">
    <TBODY>
    <TR vAlign=top>
      <TD><FONT color=#5f5f5f face=sans-serif size=1>From:</FONT> 
      <TD><FONT face=sans-serif size=1>Eugene Loh 
        &lt;eugene.loh@oracle.com&gt;</FONT> 
    <TR vAlign=top>
      <TD><FONT color=#5f5f5f face=sans-serif size=1>To:</FONT> 
      <TD><FONT face=sans-serif size=1>Open MPI Users 
        &lt;users@open-mpi.org&gt;</FONT> 
    <TR vAlign=top>
      <TD><FONT color=#5f5f5f face=sans-serif size=1>Date:</FONT> 
      <TD><FONT face=sans-serif size=1>09/09/2010 12:40 PM</FONT> 
    <TR vAlign=top>
      <TD><FONT color=#5f5f5f face=sans-serif size=1>Subject:</FONT> 
      <TD><FONT face=sans-serif size=1>Re: [OMPI users] MPI_Reduce 
        performance</FONT> 
    <TR vAlign=top>
      <TD><FONT color=#5f5f5f face=sans-serif size=1>Sent by:</FONT> 
      <TD><FONT face=sans-serif 
    size=1>users-bounces@open-mpi.org</FONT></TR></TBODY></TABLE><BR>
  <HR noShade>
  <BR><BR><BR><TT><FONT size=2>Gus Correa wrote:<BR><BR>&gt; More often than not 
  some components lag behind (regardless of how<BR>&gt; much you tune the number 
  of processors assigned to each component),<BR>&gt; slowing down the whole 
  scheme.<BR>&gt; The coupler must sit and wait for that late component,<BR>&gt; 
  the other components must sit and wait for the coupler,<BR>&gt; and the 
  (vicious) "positive feedback" cycle that<BR>&gt; Ashley mentioned goes on and 
  on.<BR><BR>I think "sit and wait" is the "typical" scenario that Dick 
  mentions. &nbsp;<BR>Someone lags, so someone else has to wait.<BR><BR>In 
  contrast, the "feedback" cycle Ashley mentions is where someone lags <BR>and 
  someone else keeps racing ahead, pumping even more data at the <BR>laggard, 
  forcing the laggard ever further 
  behind.<BR>_______________________________________________<BR>users mailing 
  list<BR>users@open-mpi.org<BR></FONT></TT><A 
  href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><TT><FONT 
  size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</FONT></TT></A><TT><FONT 
  size=2><BR></FONT></TT><BR><BR>
  <P>
  <HR>

  <P></P>_______________________________________________<BR>users mailing 
  list<BR>users@open-mpi.org<BR>http://www.open-mpi.org/mailman/listinfo.cgi/users</BLOCKQUOTE></BODY></HTML>

