Thanks, I&#39;ll go to the FAQs.  ---John<br><br><div class="gmail_quote">On Sun, Sep 16, 2012 at 3:21 AM, Jingcha Joba <span dir="ltr">&lt;<a href="mailto:pukkimonkey@gmail.com" target="_blank">pukkimonkey@gmail.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div bgcolor="#FFFFFF"><div>John,</div><div><br></div><div>BTL refers to Byte Transfer Layer, a framework to send/receive point to point messages on different network. It has several components (implementations) like openib, tcp, mx, shared mem, etc.</div>
<div><br></div><div>^openib means &quot;not&quot; to use openib component for p2p messages.</div><div><br></div><div>On a side note, do you have an RDMA supporting device ( Infiniband/RoCE/iWarp) ? If so, is OFED installed correctly and is running?</div>
<div>If you do not have, is the OFED running, which it should not, otherwise ?</div><div><br></div><div>The message that you are getting could be because of this. As a consequence, if you have a RDMA supported device, you might be getting poor performance.</div>
<div> </div><div>A wealth of information is available in the FAQ section regarding these things.<br><br>--<div>Sent from my iPhone</div></div><div><div class="h5"><div><br>On Sep 15, 2012, at 9:49 PM, John Chludzinski &lt;<a href="mailto:john.chludzinski@gmail.com" target="_blank">john.chludzinski@gmail.com</a>&gt; wrote:<br>
<br></div><div></div><blockquote type="cite"><div><span style="font-family:courier new,monospace">BTW, I looked up the -mca option:<br><br> -mca |--mca &lt;arg0&gt; &lt;arg1&gt;  <br>              Pass context-specific MCA parameters; they are<br>
              considered global if --gmca is not used and only<br>
              one context is specified (arg0 is the parameter<br>              name; arg1 is the parameter value)<br><br>Could you explain the args: btl and ^openib ?<br></span><br>---John<br><br><br><div class="gmail_quote">

On Sun, Sep 16, 2012 at 12:26 AM, John Chludzinski <span dir="ltr">&lt;<a href="mailto:john.chludzinski@gmail.com" target="_blank">john.chludzinski@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">

BINGO!  That did it.  Thanks.  ---John<div><div><br><br><div class="gmail_quote">On Sat, Sep 15, 2012 at 9:32 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>


<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">No - the mca param has to be specified *before* your executable<div><br></div><div>mpiexec -mca btl ^openib -n 4 ./a.out</div>


<div><br></div><div>Also, note the space between &quot;btl&quot; and &quot;^openib&quot;</div><div><div><div><br></div><div><br><div><div>On Sep 15, 2012, at 5:45 PM, John Chludzinski &lt;<a href="mailto:john.chludzinski@gmail.com" target="_blank">john.chludzinski@gmail.com</a>&gt; wrote:</div>


<br><blockquote type="cite"><span style="font-family:courier new,monospace">Is this what you intended(?):<br><br><b>$ mpiexec -n 4 ./a.out -mca btl^openib<br><br></b>librdmacm: couldn&#39;t read ABI version.<br>librdmacm: assuming: 4<br>


CMA: unable to get RDMA device list<br>
--------------------------------------------------------------------------<br>[[5991,1],0]: A high-performance Open MPI point-to-point messaging module<br>was unable to find any relevant network interfaces:<br><br>Module: OpenFabrics (openib)<br>



  Host: elzbieta<br><br>Another transport will be used instead, although this may result in<br>lower performance.<br>--------------------------------------------------------------------------<br>librdmacm: couldn&#39;t read ABI version.<br>



librdmacm: assuming: 4<br>CMA: unable to get RDMA device list<br>librdmacm: couldn&#39;t read ABI version.<br>librdmacm: assuming: 4<br>CMA: unable to get RDMA device list<br>librdmacm: couldn&#39;t read ABI version.<br>


librdmacm: assuming: 4<br>
CMA: unable to get RDMA device list<br> rank=            1  Results:    5.0000000       6.0000000       7.0000000       8.0000000    <br> rank=            0  Results:    1.0000000       2.0000000       3.0000000       4.0000000    <br>



 rank=            2  Results:    9.0000000       10.000000       11.000000       12.000000    <br> rank=            3  Results:    13.000000       14.000000       15.000000       16.000000    <br>[elzbieta:02374] 3 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics<br>



[elzbieta:02374] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br></span><br><br><div class="gmail_quote">On Sat, Sep 15, 2012 at 8:22 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>



<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">Try adding &quot;-mca btl ^openib&quot; to your cmd line and see if that cleans it up.<div>



<div><div><br></div><div><br><div><div>On Sep 15, 2012, at 12:44 PM, John Chludzinski &lt;<a href="mailto:john.chludzinski@gmail.com" target="_blank">john.chludzinski@gmail.com</a>&gt; wrote:</div><br><blockquote type="cite">



<span style="font-family:courier new,monospace">There was a bug in the code.  So now I get this, which is correct but how do I get rid of all these ABI, CMA, etc. messages?<br><br>$ mpiexec -n 4 ./a.out <br>librdmacm: couldn&#39;t read ABI version.<br>




librdmacm: couldn&#39;t read ABI version.<br>librdmacm: assuming: 4<br>CMA: unable to get RDMA device list<br>librdmacm: assuming: 4<br>CMA: unable to get RDMA device list<br>CMA: unable to get RDMA device list<br>librdmacm: couldn&#39;t read ABI version.<br>




librdmacm: assuming: 4<br>librdmacm: couldn&#39;t read ABI version.<br>librdmacm: assuming: 4<br>CMA: unable to get RDMA device list<br>--------------------------------------------------------------------------<br>[[6110,1],1]: A high-performance Open MPI point-to-point messaging module<br>




was unable to find any relevant network interfaces:<br><br>Module: OpenFabrics (openib)<br>  Host: elzbieta<br><br>Another transport will be used instead, although this may result in<br>lower performance.<br>--------------------------------------------------------------------------<br>




 rank=            1  Results:    5.0000000       6.0000000       7.0000000       8.0000000    <br> rank=            2  Results:    9.0000000       10.000000       11.000000       12.000000    <br> rank=            0  Results:    1.0000000       2.0000000       3.0000000       4.0000000    <br>




 rank=            3  Results:    13.000000       14.000000       15.000000       16.000000    <br>[elzbieta:02559] 3 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics<br>[elzbieta:02559] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages</span><br>




<br><br><div class="gmail_quote">On Sat, Sep 15, 2012 at 3:34 PM, John Chludzinski <span dir="ltr">&lt;<a href="mailto:john.chludzinski@gmail.com" target="_blank">john.chludzinski@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">




<span style="font-family:courier new,monospace">BTW, here the example code:<br><br>program scatter<br>include &#39;mpif.h&#39;<br><br>integer, parameter :: SIZE=4<br>integer :: numtasks, rank, sendcount, recvcount, source, ierr<br>





real :: sendbuf(SIZE,SIZE), recvbuf(SIZE)<br><br>!  Fortran stores this array in column major order, so the <br>!  scatter will actually scatter columns, not rows.<br>data sendbuf /1.0, 2.0, 3.0, 4.0, &amp;<br>5.0, 6.0, 7.0, 8.0, &amp;<br>





9.0, 10.0, 11.0, 12.0, &amp;<br>13.0, 14.0, 15.0, 16.0 /<br><br>call MPI_INIT(ierr)<br>call MPI_COMM_RANK(MPI_COMM_WORLD, rank, ierr)<br>call MPI_COMM_SIZE(MPI_COMM_WORLD, numtasks, ierr)<br><br>if (numtasks .eq. SIZE) then<br>





  source = 1<br>  sendcount = SIZE<br>  recvcount = SIZE<br>  call MPI_SCATTER(sendbuf, sendcount, MPI_REAL, recvbuf, &amp;<br>                   recvcount, MPI_REAL, source, MPI_COMM_WORLD, ierr)<br>  print *, &#39;rank= &#39;,rank,&#39; Results: &#39;,recvbuf <br>





else<br>   print *, &#39;Must specify&#39;,SIZE,&#39; processors.  Terminating.&#39; <br>endif<br><br>call MPI_FINALIZE(ierr)<br><br>end program<br></span><div><br><br><div class="gmail_quote">
On Sat, Sep 15, 2012 at 3:02 PM, John Chludzinski <span dir="ltr">&lt;<a href="mailto:john.chludzinski@gmail.com" target="_blank">john.chludzinski@gmail.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><span style="font-family:courier new,monospace"># export LD_LIBRARY_PATH<div><br><br># mpiexec -n 1 printenv | grep PATH<br>





</div>LD_LIBRARY_PATH=/usr/lib/openmpi/lib/<div><br>PATH=/usr/lib/openmpi/bin/:/usr/lib/ccache:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/jski/.local/bin:/home/jski/bin<br>
MODULEPATH=/usr/share/Modules/modulefiles:/etc/modulefiles<br>WINDOWPATH=1<br><br></div># mpiexec -n 4 ./a.out <br>librdmacm: couldn&#39;t read ABI version.<br>librdmacm: assuming: 4<br>CMA: unable to get RDMA device list<br>





--------------------------------------------------------------------------<br>
[[3598,1],0]: A high-performance Open MPI point-to-point messaging module<br>was unable to find any relevant network interfaces:<br><br>Module: OpenFabrics (openib)<br>  Host: elzbieta<br><br>Another transport will be used instead, although this may result in<br>






lower performance.<br>--------------------------------------------------------------------------<br>librdmacm: couldn&#39;t read ABI version.<br>librdmacm: assuming: 4<br>librdmacm: couldn&#39;t read ABI version.<br>CMA: unable to get RDMA device list<br>






librdmacm: assuming: 4<br>CMA: unable to get RDMA device list<br>librdmacm: couldn&#39;t read ABI version.<br>librdmacm: assuming: 4<br>CMA: unable to get RDMA device list<br>[elzbieta:4145] *** An error occurred in MPI_Scatter<br>






[elzbieta:4145] *** on communicator MPI_COMM_WORLD<br>[elzbieta:4145] *** MPI_ERR_TYPE: invalid datatype<br>[elzbieta:4145] *** MPI_ERRORS_ARE_FATAL: your MPI job will now abort<br>--------------------------------------------------------------------------<br>






mpiexec has exited due to process rank 1 with PID 4145 on<br>node elzbieta exiting improperly. There are two reasons this could occur:<br><br>1. this process did not call &quot;init&quot; before exiting, but others in<br>






the job did. This can cause a job to hang indefinitely while it waits<br>for all processes to call &quot;init&quot;. By rule, if one process calls &quot;init&quot;,<br>then ALL processes must call &quot;init&quot; prior to termination.<br>






<br>2. this process called &quot;init&quot;, but exited without calling &quot;finalize&quot;.<br>By rule, all processes that call &quot;init&quot; MUST call &quot;finalize&quot; prior to<br>exiting or it will be considered an &quot;abnormal termination&quot;<br>






<br>This may have caused other processes in the application to be<br>terminated by signals sent by mpiexec (as reported here).<br>--------------------------------------------------------------------------</span><div>
<div><br><br><br>
<div class="gmail_quote">On Sat, Sep 15, 2012 at 2:24 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">






<div style="word-wrap:break-word">Ah - note that there is no LD_LIBRARY_PATH in the environment. That&#39;s the problem<div><br><div><div>On Sep 15, 2012, at 11:19 AM, John Chludzinski &lt;<a href="mailto:john.chludzinski@gmail.com" target="_blank">john.chludzinski@gmail.com</a>&gt; wrote:</div>






<br><blockquote type="cite">$ which mpiexec<br>/usr/lib/openmpi/bin/mpiexec<br><br># mpiexec -n 1 printenv | grep PATH<br>PATH=/usr/lib/openmpi/bin/:/usr/lib/ccache:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/jski/.local/bin:/home/jski/bin<br>







MODULEPATH=/usr/share/Modules/modulefiles:/etc/modulefiles<br>WINDOWPATH=1<br><br><br><br><div class="gmail_quote">On Sat, Sep 15, 2012 at 1:11 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>







<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">Couple of things worth checking:<div><br></div><div>1. verify that you executed the &quot;mpiexec&quot; you think you did - a simple &quot;which mpiexec&quot; should suffice</div>







<div><br></div><div>2. verify that your environment is correct by &quot;mpiexec -n 1 printenv | grep PATH&quot;. Sometimes the ld_library_path doesn&#39;t carry over like you think it should</div><div><br></div><div><br>






<div>
<div><div>On Sep 15, 2012, at 10:00 AM, John Chludzinski &lt;<a href="mailto:john.chludzinski@gmail.com" target="_blank">john.chludzinski@gmail.com</a>&gt; wrote:</div><br></div><blockquote type="cite">
<div><span style="font-family:courier new,monospace">I installed OpenMPI (I have a simple dual core AMD notebook with Fedora 16) via:<br><br># yum install openmpi<br># yum install openmpi-devel<br># mpirun --version<br>



mpirun (Open MPI) 1.5.4<br>
<br>I added: <br><br>$ PATH=PATH=/usr/lib/openmpi/bin/:$PATH<br>$ LD_LIBRARY_PATH=/usr/lib/openmpi/lib/<br><br>Then:<br><br>$ mpif90 ex1.f95<br>$ mpiexec -n 4 ./a.out <br>./a.out: error while loading shared libraries: libmpi_f90.so.1: cannot open shared object file: No such file or directory<br>








./a.out: error while loading shared libraries: libmpi_f90.so.1: cannot open shared object file: No such file or directory<br>./a.out: error while loading shared libraries: libmpi_f90.so.1: cannot open shared object file: No such file or directory<br>








./a.out: error while loading shared libraries: libmpi_f90.so.1: cannot open shared object file: No such file or directory<br>--------------------------------------------------------------------------<br>mpiexec noticed that the job aborted, but has no info as to the process<br>








that caused that situation.<br>--------------------------------------------------------------------------<br><br>ls -l /usr/lib/openmpi/lib/<br>total 6788<br>lrwxrwxrwx. 1 root root      25 Sep 15 12:25 libmca_common_sm.so -&gt; libmca_common_sm.so.2.0.0<br>








lrwxrwxrwx. 1 root root      25 Sep 14 16:14 libmca_common_sm.so.2 -&gt; libmca_common_sm.so.2.0.0<br>-rwxr-xr-x. 1 root root    8492 Jan 20  2012 libmca_common_sm.so.2.0.0<br>lrwxrwxrwx. 1 root root      19 Sep 15 12:25 libmpi_cxx.so -&gt; libmpi_cxx.so.1.0.1<br>








lrwxrwxrwx. 1 root root      19 Sep 14 16:14 libmpi_cxx.so.1 -&gt; libmpi_cxx.so.1.0.1<br>-rwxr-xr-x. 1 root root   87604 Jan 20  2012 libmpi_cxx.so.1.0.1<br>lrwxrwxrwx. 1 root root      19 Sep 15 12:25 libmpi_f77.so -&gt; libmpi_f77.so.1.0.2<br>








lrwxrwxrwx. 1 root root      19 Sep 14 16:14 libmpi_f77.so.1 -&gt; libmpi_f77.so.1.0.2<br>-rwxr-xr-x. 1 root root  179912 Jan 20  2012 libmpi_f77.so.1.0.2<br>lrwxrwxrwx. 1 root root      19 Sep 15 12:25 libmpi_f90.so -&gt; libmpi_f90.so.1.1.0<br>








lrwxrwxrwx. 1 root root      19 Sep 14 16:14 libmpi_f90.so.1 -&gt; libmpi_f90.so.1.1.0<br>-rwxr-xr-x. 1 root root   10364 Jan 20  2012 libmpi_f90.so.1.1.0<br>lrwxrwxrwx. 1 root root      15 Sep 15 12:25 libmpi.so -&gt; libmpi.so.1.0.2<br>








lrwxrwxrwx. 1 root root      15 Sep 14 16:14 libmpi.so.1 -&gt; libmpi.so.1.0.2<br>-rwxr-xr-x. 1 root root 1383444 Jan 20  2012 libmpi.so.1.0.2<br>lrwxrwxrwx. 1 root root      21 Sep 15 12:25 libompitrace.so -&gt; libompitrace.so.0.0.0<br>








lrwxrwxrwx. 1 root root      21 Sep 14 16:14 libompitrace.so.0 -&gt; libompitrace.so.0.0.0<br>-rwxr-xr-x. 1 root root   13572 Jan 20  2012 libompitrace.so.0.0.0<br>lrwxrwxrwx. 1 root root      20 Sep 15 12:25 libopen-pal.so -&gt; libopen-pal.so.3.0.0<br>








lrwxrwxrwx. 1 root root      20 Sep 14 16:14 libopen-pal.so.3 -&gt; libopen-pal.so.3.0.0<br>-rwxr-xr-x. 1 root root  386324 Jan 20  2012 libopen-pal.so.3.0.0<br>lrwxrwxrwx. 1 root root      20 Sep 15 12:25 libopen-rte.so -&gt; libopen-rte.so.3.0.0<br>








lrwxrwxrwx. 1 root root      20 Sep 14 16:14 libopen-rte.so.3 -&gt; libopen-rte.so.3.0.0<br>-rwxr-xr-x. 1 root root  790052 Jan 20  2012 libopen-rte.so.3.0.0<br>-rw-r--r--. 1 root root  301520 Jan 20  2012 libotf.a<br>lrwxrwxrwx. 1 root root      15 Sep 15 12:25 libotf.so -&gt; libotf.so.0.0.1<br>








lrwxrwxrwx. 1 root root      15 Sep 14 16:14 libotf.so.0 -&gt; libotf.so.0.0.1<br>-rwxr-xr-x. 1 root root  206384 Jan 20  2012 libotf.so.0.0.1<br>-rw-r--r--. 1 root root  337970 Jan 20  2012 libvt.a<br>-rw-r--r--. 1 root root  591070 Jan 20  2012 libvt-hyb.a<br>








lrwxrwxrwx. 1 root root      18 Sep 15 12:25 libvt-hyb.so -&gt; libvt-hyb.so.0.0.0<br>lrwxrwxrwx. 1 root root      18 Sep 14 16:14 libvt-hyb.so.0 -&gt; libvt-hyb.so.0.0.0<br>-rwxr-xr-x. 1 root root  428844 Jan 20  2012 libvt-hyb.so.0.0.0<br>








-rw-r--r--. 1 root root  541004 Jan 20  2012 libvt-mpi.a<br>lrwxrwxrwx. 1 root root      18 Sep 15 12:25 libvt-mpi.so -&gt; libvt-mpi.so.0.0.0<br>lrwxrwxrwx. 1 root root      18 Sep 14 16:14 libvt-mpi.so.0 -&gt; libvt-mpi.so.0.0.0<br>








-rwxr-xr-x. 1 root root  396352 Jan 20  2012 libvt-mpi.so.0.0.0<br>-rw-r--r--. 1 root root  372352 Jan 20  2012 libvt-mt.a<br>lrwxrwxrwx. 1 root root      17 Sep 15 12:25 libvt-mt.so -&gt; libvt-mt.so.0.0.0<br>lrwxrwxrwx. 1 root root      17 Sep 14 16:14 libvt-mt.so.0 -&gt; libvt-mt.so.0.0.0<br>








-rwxr-xr-x. 1 root root  266104 Jan 20  2012 libvt-mt.so.0.0.0<br>-rw-r--r--. 1 root root   60390 Jan 20  2012 libvt-pomp.a<br>lrwxrwxrwx. 1 root root      14 Sep 15 12:25 libvt.so -&gt; libvt.so.0.0.0<br>lrwxrwxrwx. 1 root root      14 Sep 14 16:14 libvt.so.0 -&gt; libvt.so.0.0.0<br>








-rwxr-xr-x. 1 root root  242604 Jan 20  2012 libvt.so.0.0.0<br>-rwxr-xr-x. 1 root root  303591 Jan 20  2012 mpi.mod<br>drwxr-xr-x. 2 root root    4096 Sep 14 16:14 openmpi<br><br><br>The file (actually, a link) it claims it can&#39;t find: libmpi_f90.so.1, is clearly there. And LD_LIBRARY_PATH=/usr/lib/openmpi/lib/.<br>








<br>What&#39;s the problem?<br><br>---John</span><br></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>







</div><br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>






</div><br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>
</div></div></blockquote></div><br>
</div></blockquote></div><br>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>



</div><br></div></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>


</div><br></div></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>
</div></div></blockquote></div><br>
</div></blockquote><blockquote type="cite"><div><span>_______________________________________________</span><br><span>users mailing list</span><br><span><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a></span><br>
<span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></span></div></blockquote></div></div></div><br>_______________________________________________<br>

users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

