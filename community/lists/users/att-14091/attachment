<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>

    <meta http-equiv="content-type" content="text/html; charset=ISO-8859-1">
  </head>
  <body text="#000000" bgcolor="#ffffff">
    Hello OMPI:<br>
    <br>
    We have installed OMPI V1.4.2 on a Nehalem cluster running
    CentOS5.4. OMPI was built uisng Intel compilers 11.1.072. I am
    attaching the configuration log and output from ompi_info -a.<br>
    <br>
    The problem we are encountering is that whenever we use option
    '-npernode N' in the mpirun command line we get a segmentation fault
    as in below:<br>
    <br>
    <tt><br>
      miket@login002[pts/7]PS $ mpirun -npernode 1&nbsp; --display-devel-map&nbsp;
      --tag-output -np 6 -cpus-per-proc 2 -H
      'login001,login002,login003' hostname<br>
      <br>
      &nbsp;Map generated by mapping policy: 0402<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Npernode: 1&nbsp;&nbsp;&nbsp;&nbsp; Oversubscribe allowed: TRUE&nbsp;&nbsp;&nbsp;&nbsp; CPU Lists:
      FALSE<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Num new daemons: 2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; New daemon starting vpid 1<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Num nodes: 3<br>
      <br>
      &nbsp;Data for node: Name: login001&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Launch id: -1&nbsp;&nbsp; Arch: 0
      State: 2<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Num boards: 1&nbsp;&nbsp; Num sockets/board: 2&nbsp;&nbsp;&nbsp; Num cores/socket:
      4<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Daemon: [[44812,0],1]&nbsp;&nbsp; Daemon launched: False<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Num slots: 1&nbsp;&nbsp;&nbsp; Slots in use: 2<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Num slots allocated: 1&nbsp; Max slots: 0<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Username on node: NULL<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Num procs: 1&nbsp;&nbsp;&nbsp; Next node_rank: 1<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data for proc: [[44812,1],0]<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pid: 0&nbsp; Local rank: 0&nbsp;&nbsp; Node rank: 0<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; State: 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; App_context: 0&nbsp; Slot list: NULL<br>
      <br>
      &nbsp;Data for node: Name: login002&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Launch id: -1&nbsp;&nbsp; Arch:
      ffc91200&nbsp; State: 2<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Num boards: 1&nbsp;&nbsp; Num sockets/board: 2&nbsp;&nbsp;&nbsp; Num cores/socket:
      4<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Daemon: [[44812,0],0]&nbsp;&nbsp; Daemon launched: True<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Num slots: 1&nbsp;&nbsp;&nbsp; Slots in use: 2<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Num slots allocated: 1&nbsp; Max slots: 0<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Username on node: NULL<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Num procs: 1&nbsp;&nbsp;&nbsp; Next node_rank: 1<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data for proc: [[44812,1],0]<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pid: 0&nbsp; Local rank: 0&nbsp;&nbsp; Node rank: 0<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; State: 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; App_context: 0&nbsp; Slot list: NULL<br>
      <br>
      &nbsp;Data for node: Name: login003&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Launch id: -1&nbsp;&nbsp; Arch: 0
      State: 2<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Num boards: 1&nbsp;&nbsp; Num sockets/board: 2&nbsp;&nbsp;&nbsp; Num cores/socket:
      4<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Daemon: [[44812,0],2]&nbsp;&nbsp; Daemon launched: False<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Num slots: 1&nbsp;&nbsp;&nbsp; Slots in use: 2<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Num slots allocated: 1&nbsp; Max slots: 0<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Username on node: NULL<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Num procs: 1&nbsp;&nbsp;&nbsp; Next node_rank: 1<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data for proc: [[44812,1],0]<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pid: 0&nbsp; Local rank: 0&nbsp;&nbsp; Node rank: 0<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; State: 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; App_context: 0&nbsp; Slot list: NULL<br>
      [login002:02079] *** Process received signal ***<br>
      [login002:02079] Signal: Segmentation fault (11)<br>
      [login002:02079] Signal code: Address not mapped (1)<br>
      [login002:02079] Failing at address: 0x50<br>
      [login002:02079] [ 0] /lib64/libpthread.so.0 [0x3569a0e7c0]<br>
      [login002:02079] [ 1]
      /g/software/openmpi-1.4.2/intel/lib/libopen-rte.so.0(orte_util_encode_pidmap+0xa7)
      [0x2afa70d25de7]<br>
      [login002:02079] [ 2]
      /g/software/openmpi-1.4.2/intel/lib/libopen-rte.so.0(orte_odls_base_default_get_add_procs_data+0x3b8)
      [0x2afa70d36088]<br>
      [login002:02079] [ 3]
      /g/software/openmpi-1.4.2/intel/lib/libopen-rte.so.0(orte_plm_base_launch_apps+0xd7)
      [0x2afa70d37fc7]<br>
      [login002:02079] [ 4]
      /g/software/openmpi-1.4.2/intel/lib/openmpi/mca_plm_rsh.so
      [0x2afa721085a1]<br>
      [login002:02079] [ 5] mpirun [0x404c27]<br>
      [login002:02079] [ 6] mpirun [0x403e38]<br>
      [login002:02079] [ 7] /lib64/libc.so.6(__libc_start_main+0xf4)
      [0x3568e1d994]<br>
      [login002:02079] [ 8] mpirun [0x403d69]<br>
      [login002:02079] *** End of error message ***<br>
      Segmentation fault</tt><br>
    <br>
    We tried version 1.4.1 and this problem did not emerge. <br>
    <br>
    This option is necessary for when our users launch hybrid MPI-OMP
    code were they can request M nodes and n ppn in a <b>PBS/Torque</b>
    setup so they can only get the right amount of MPI taks.
    Unfortunately, as soon as we use the 'npernode N' option mprun
    crashes. <br>
    <br>
    Is this a known issue? I found related problem (of around May,
    2010)&nbsp; when people were using the same option but in a SLURM
    environment. <br>
    <br>
    regards<br>
    <br>
    Michael<br>
    <br>
  </body>
</html>

