<div>I try a couple of things including your suggestion. I also find out this has been reported before, </div><div><a href="http://www.open-mpi.org/community/lists/users/2007/03/2904.php">http://www.open-mpi.org/community/lists/users/2007/03/2904.php</a></div>
<div>but there seems to be no clear solution so far:</div><div><br></div><div>Here is what I observe: </div><div>I keep the problem size fixed with 24 processes. I use two nodes with 8-core each and 2-core each.</div><div>
<br></div><div>1. When it is oversubscribed (12 process/processor), sys vs. user time is much higher than less-subscribed (1.5 process/processor).</div><div>Almost The wall clock does not improve too much :-( </div><div><br>
</div><div>2.  I try following options, individually and collectively, no difference </div><div>mpirun --mpi_yield_when_idle 1 --mca btl tcp,sm,self --mca coll_hierarch_priority 100 ...</div><div><br></div><div>3. older openmpi version (1.3) seems to be better than new version (1.3.2), but not significantly. </div>
<div><br></div><div>By the way, I am working on Amazon EC2 (VM-host). Will that make any difference? </div><div><br></div><div>Please advise</div><div><br></div><div>Thanks</div><div><br></div><br><br><div class="gmail_quote">
On Fri, Jun 26, 2009 at 11:28 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
If you are running fewer processes on your nodes than they have processors, then you can improve performance by adding<br>
<br>
-mca mpi_paffinity_alone 1<br>
<br>
to your cmd line. This will bind your processes to individual cores, which helps with latency. If your program involves collectives, then you can try setting<br>
<br>
-mca coll_hierarch_priority 100<br>
<br>
This will activate the hierarchical collectives, which utilize shared memory for messages between procs on the same node.<br>
<br>
Ralph<div><div></div><div class="h5"><br>
<br>
<br>
On Jun 26, 2009, at 9:09 PM, Qiming He wrote:<br>
<br>
</div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div></div><div class="h5">
Hi all,<br>
<br>
I am new to OpenMPI, and have an urgent run-time question. I have openmpi-1.3.2 compiled with Intel Fortran compiler v.11 simply by<br>
<br>
./configure --prefix=&lt;my-dir&gt; F77=ifort FC=ifort<br>
then I set my LD_LIBRARY_PATH to include &lt;openmpi-lib&gt; and &lt;intel-lib&gt;<br>
and compile my Fortran program properly. No compilation error.<br>
<br>
I run my program on single node. Everything looks ok. However, when I run it on multiple nodes.<br>
mpirun -np &lt;num&gt; --hostfile &lt;my-hosts&gt; &lt;my-program&gt;<br>
The performance is much worse than a single node with the same size of the problem to solve (MPICH2 has 50% improvement)<br>
<br>
I use top and saidar to find that user time (CPU user) is much lower than system time (CPU system), i.e,<br>
only small portion of CPU time is used by user application, while the rest is busy with system.<br>
No wonder I got bad performance.  I am assuming &quot;CPU system&quot; is used for MPI communication.<br>
I notice the total traffic (on eth0) is not that big (~5Mb/sec). What is CPU system busy for?<br>
<br>
Can anyone help? Anything I need to tune?<br>
<br>
Thanks in advance<br>
<br>
-Qiming<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br></div></div>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br><br clear="all"><br><br>

