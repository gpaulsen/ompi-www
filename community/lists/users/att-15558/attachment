Hello:<br><br>I&#39;ve the same version ob Ubuntu 10.04. The original version was Ubuntu Server 9.1 (64) and upgraded both of them to 10.04. <br>Yesterday I&#39;ve updated and upgraded to the same level again. But I&#39;ve got the same error after that.<br>
The machine are exactly the same, HP Compaq with inter Core I5.<br><br>Anyway I&#39;ve compared the version of openmpi and gcc, and are the same too: 1.4.1-2 and 4.4.4.3 respectly. I&#39;m attaching the exit of the dpkg-l on the two system.<br>
<br>I would appreciate a lot any help to solve it.<br>Thank you.<br><br>Marcela.<br><div class="gmail_quote">2011/2/10 Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span><br>
<blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">I typically see these kinds of errors when there&#39;s an Open MPI version mismatch between the nodes, and/or if there are slightly different flavors of Linux installed on each node (i.e., you&#39;re technically in a heterogeneous situation, but you&#39;re trying to run a single application binary).  Can you verify:<br>

<br>
1. that you have exactly the same version of Open MPI installed on all nodes?  (and that your application was compiled against that exact version)<br>
<br>
2. that you have exactly the same OS/update level installed on all nodes (e.g., same versions of glibc, etc.)<br>
<div><div></div><div class="h5"><br>
<br>
On Feb 10, 2011, at 3:13 AM, Marcela Castro León wrote:<br>
<br>
&gt; Hello<br>
&gt; I&#39;ve a program that allways works fine, but i&#39;m trying it on a new cluster and fails when I execute it on more than one machine.<br>
&gt; I mean, if I execute alone on each host, everything works fine.<br>
&gt; radic@santacruz:~/gaps/caso3-i1$ mpirun -np 3 ../test parcorto.txt<br>
&gt;<br>
&gt; But when I execute<br>
&gt; radic@santacruz:~/gaps/caso3-i1$ mpirun -np 3 -machinefile /home/radic/mfile ../test parcorto.txt<br>
&gt;<br>
&gt; I get this error:<br>
&gt;<br>
&gt; mpirun has exited due to process rank 0 with PID 2132 on<br>
&gt; node santacruz exiting without calling &quot;finalize&quot;. This may<br>
&gt; have caused other processes in the application to be<br>
&gt; terminated by signals sent by mpirun (as reported here).<br>
&gt; --------------------------------------------------------------------------<br>
&gt;<br>
&gt; Though the machinefile (mfile) had only one machine, the programs fails.<br>
&gt; This is the current content:<br>
&gt;<br>
&gt; radic@santacruz:~/gaps/caso3-i1$ cat /home/radic/mfile<br>
&gt; santacruz<br>
&gt; chubut<br>
&gt;<br>
&gt; I&#39;ve debug the program and the error occurs after proc0 do an<br>
&gt; MPI_Recv(&amp;nomproc,lennomproc,MPI_CHAR,i,tag,MPI_COMM_WORLD,&amp;Stat);<br>
&gt; from the remote process.<br>
&gt;<br>
&gt; I&#39;ve done several test I&#39;ll mention:<br>
&gt;<br>
&gt; 1) Change the order on machinefile<br>
&gt; radic@santacruz:~/gaps/caso3-i1$ cat /home/radic/mfile<br>
&gt; chubut<br>
&gt; santacruz<br>
&gt;<br>
&gt; In that case, I get this error:<br>
&gt; [chubut:2194] *** An error occurred in MPI_Recv<br>
&gt; [chubut:2194] *** on communicator MPI_COMM_WORLD<br>
&gt; [chubut:2194] *** MPI_ERR_TRUNCATE: message truncated<br>
&gt; [chubut:2194] *** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>
&gt; and then<br>
&gt; --------------------------------------------------------------------------<br>
&gt; mpirun has exited due to process rank 0 with PID 2194 on<br>
&gt; node chubut exiting without calling &quot;finalize&quot;. This may<br>
&gt; have caused other processes in the application to be<br>
&gt; terminated by signals sent by mpirun (as reported here).<br>
&gt; --------------------------------------------------------------------------<br>
&gt;<br>
&gt; 2) I&#39;ve got the same error executing on host chubut intead of santacruz,<br>
&gt; 3) a simple mpi programs like  MPI_Hello world are working fine, but I suppose that are very simple program.<br>
&gt;<br>
&gt; radic@santacruz:~/gaps$ mpirun -np 3 -machinefile /home/radic/mfile MPI_Hello<br>
&gt; Hola Mundo Hola Marce 1<br>
&gt; Hola Mundo Hola Marce 0<br>
&gt; Hola Mundo Hola Marce 2<br>
&gt;<br>
&gt;<br>
&gt; This is the information you ask for tuntime problem.<br>
&gt; a) radic@santacruz:~$ mpirun -version<br>
&gt; mpirun (Open MPI) 1.4.1<br>
&gt; b) i&#39;m using ubuntu 10,04. I&#39;m installing the packages using apt-get install, so, I don&#39;t have a config.log<br>
&gt; c) The ompi_info --all is on the file ompi_info.zip<br>
&gt; d) These are PATH and LD_LIBRARY_PATH<br>
&gt; radic@santacruz:~$ echo $PATH<br>
&gt; /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games<br>
&gt; radic@santacruz:~$ echo $LD_LIBRARY_PATH<br>
&gt;<br>
&gt;<br>
&gt; Thank you very much.<br>
&gt;<br>
&gt; Marcela.<br>
&gt;<br>
</div></div>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to:<br>
<a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br>

