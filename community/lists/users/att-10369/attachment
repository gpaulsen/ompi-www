Thanks Warner,<div><br></div><div>This is frustrating... I read the ticket. 6 months already and 2 releases postponed... Frankly, I am very skeptical that this will be fixed for 1.3.4. I really hope so, but when 1.3.4 will be released?</div>

<div><br></div><div>I have to think about going with 1.2.x and possible disruptions in my configuration (I use Fink) or wait.</div><div><br></div><div>And I offered myself to test any nightly snapshot claiming this bug is fixed.</div>

<div><br></div><div>Cheers,</div><div>Alan<br><br><div class="gmail_quote">On Fri, Aug 14, 2009 at 17:20, Warner Yuen <span dir="ltr">&lt;<a href="mailto:wyuen@apple.com">wyuen@apple.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">

Hi Alan,<br>
<br>
Xgrid support for Open MPI is currently broken in the latest version of Open MPI. See the ticket below. However, I believe that Xgrid still works with one of the earlier 1.2  versions of Open MPI. I don&#39;t recall for sure, but I think that it&#39;s Open MPI 1.2.3.<br>


<br>
#1777: Xgrid support is broken in the v1.3 series<br>
---------------------+------------------------------------------------------<br>
Reporter:  jsquyres  |        Owner:  brbarret<br>
   Type:  defect    |       Status:  accepted<br>
Priority:  major     |    Milestone:  Open MPI 1.3.4<br>
Version:  trunk     |   Resolution:<br>
Keywords:            |<br>
---------------------+------------------------------------------------------<br>
Changes (by bbenton):<br>
<br>
 * milestone:  Open MPI 1.3.3 =&gt; Open MPI 1.3.4<br>
<br>
<br>
Warner Yuen<br>
Scientific Computing<br>
Consulting Engineer<br>
Apple, Inc.<br>
email: <a href="mailto:wyuen@apple.com" target="_blank">wyuen@apple.com</a><br>
Tel: 408.718.2859<br>
<br>
<br>
<br>
<br>
On Aug 14, 2009, at 6:21 AM, <a href="mailto:users-request@open-mpi.org" target="_blank">users-request@open-mpi.org</a> wrote:<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<br>
Message: 1<br>
Date: Fri, 14 Aug 2009 14:21:30 +0100<br>
From: Alan &lt;<a href="mailto:alanwilter@gmail.com" target="_blank">alanwilter@gmail.com</a>&gt;<br>
Subject: [OMPI users] openmpi with xgrid<br>
To: <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Message-ID:<br>
        &lt;<a href="mailto:cf58c8d00908140621v18d384f2wef97ee80ca3ded0c@mail.gmail.com" target="_blank">cf58c8d00908140621v18d384f2wef97ee80ca3ded0c@mail.gmail.com</a>&gt;<br>
Content-Type: text/plain; charset=&quot;utf-8&quot;<div><div></div><div class="h5"><br>
<br>
Hi there,<br>
I saw that <a href="http://www.open-mpi.org/community/lists/users/2007/08/3900.php" target="_blank">http://www.open-mpi.org/community/lists/users/2007/08/3900.php</a>.<br>
<br>
I use fink, and so I changed the <a href="http://openmpi.info" target="_blank">openmpi.info</a> file in order to get openmpi<br>
with xgrid support.<br>
<br>
As you can see:<br>
amadeus[2081]:~/Downloads% /sw/bin/ompi_info<br>
                Package: Open MPI root@amadeus.local Distribution<br>
               Open MPI: 1.3.3<br>
  Open MPI SVN revision: r21666<br>
  Open MPI release date: Jul 14, 2009<br>
               Open RTE: 1.3.3<br>
  Open RTE SVN revision: r21666<br>
  Open RTE release date: Jul 14, 2009<br>
                   OPAL: 1.3.3<br>
      OPAL SVN revision: r21666<br>
      OPAL release date: Jul 14, 2009<br>
           Ident string: 1.3.3<br>
                 Prefix: /sw<br>
Configured architecture: x86_64-apple-darwin9<br>
         Configure host: amadeus.local<br>
          Configured by: root<br>
          Configured on: Fri Aug 14 12:58:12 BST 2009<br>
         Configure host: amadeus.local<br>
               Built by:<br>
               Built on: Fri Aug 14 13:07:46 BST 2009<br>
             Built host: amadeus.local<br>
             C bindings: yes<br>
           C++ bindings: yes<br>
     Fortran77 bindings: yes (single underscore)<br>
     Fortran90 bindings: yes<br>
Fortran90 bindings size: small<br>
             C compiler: gcc<br>
    C compiler absolute: /sw/var/lib/fink/path-prefix-10.6/gcc<br>
           C++ compiler: g++<br>
  C++ compiler absolute: /sw/var/lib/fink/path-prefix-10.6/g++<br>
     Fortran77 compiler: gfortran<br>
 Fortran77 compiler abs: /sw/bin/gfortran<br>
     Fortran90 compiler: gfortran<br>
 Fortran90 compiler abs: /sw/bin/gfortran<br>
            C profiling: yes<br>
          C++ profiling: yes<br>
    Fortran77 profiling: yes<br>
    Fortran90 profiling: yes<br>
         C++ exceptions: no<br>
         Thread support: posix (mpi: no, progress: no)<br>
          Sparse Groups: no<br>
 Internal debug support: no<br>
    MPI parameter check: runtime<br>
Memory profiling support: no<br>
Memory debugging support: no<br>
        libltdl support: yes<br>
  Heterogeneous support: no<br>
mpirun default --prefix: no<br>
        MPI I/O support: yes<br>
      MPI_WTIME support: gettimeofday<br>
Symbol visibility support: yes<br>
  FT Checkpoint support: no  (checkpoint thread: no)<br>
          MCA backtrace: execinfo (MCA v2.0, API v2.0, Component v1.3.3)<br>
          MCA paffinity: darwin (MCA v2.0, API v2.0, Component v1.3.3)<br>
              MCA carto: auto_detect (MCA v2.0, API v2.0, Component v1.3.3)<br>
              MCA carto: file (MCA v2.0, API v2.0, Component v1.3.3)<br>
          MCA maffinity: first_use (MCA v2.0, API v2.0, Component v1.3.3)<br>
              MCA timer: darwin (MCA v2.0, API v2.0, Component v1.3.3)<br>
        MCA installdirs: env (MCA v2.0, API v2.0, Component v1.3.3)<br>
        MCA installdirs: config (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA dpm: orte (MCA v2.0, API v2.0, Component v1.3.3)<br>
             MCA pubsub: orte (MCA v2.0, API v2.0, Component v1.3.3)<br>
          MCA allocator: basic (MCA v2.0, API v2.0, Component v1.3.3)<br>
          MCA allocator: bucket (MCA v2.0, API v2.0, Component v1.3.3)<br>
               MCA coll: basic (MCA v2.0, API v2.0, Component v1.3.3)<br>
               MCA coll: hierarch (MCA v2.0, API v2.0, Component v1.3.3)<br>
               MCA coll: inter (MCA v2.0, API v2.0, Component v1.3.3)<br>
               MCA coll: self (MCA v2.0, API v2.0, Component v1.3.3)<br>
               MCA coll: sm (MCA v2.0, API v2.0, Component v1.3.3)<br>
               MCA coll: sync (MCA v2.0, API v2.0, Component v1.3.3)<br>
               MCA coll: tuned (MCA v2.0, API v2.0, Component v1.3.3)<br>
                 MCA io: romio (MCA v2.0, API v2.0, Component v1.3.3)<br>
              MCA mpool: fake (MCA v2.0, API v2.0, Component v1.3.3)<br>
              MCA mpool: rdma (MCA v2.0, API v2.0, Component v1.3.3)<br>
              MCA mpool: sm (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA pml: cm (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA pml: csum (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA pml: ob1 (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA pml: v (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA bml: r2 (MCA v2.0, API v2.0, Component v1.3.3)<br>
             MCA rcache: vma (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA btl: self (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA btl: sm (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA btl: tcp (MCA v2.0, API v2.0, Component v1.3.3)<br>
               MCA topo: unity (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA osc: pt2pt (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA osc: rdma (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA iof: hnp (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA iof: orted (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA iof: tool (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA oob: tcp (MCA v2.0, API v2.0, Component v1.3.3)<br>
               MCA odls: default (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA ras: slurm (MCA v2.0, API v2.0, Component v1.3.3)<br>
              MCA rmaps: rank_file (MCA v2.0, API v2.0, Component v1.3.3)<br>
              MCA rmaps: round_robin (MCA v2.0, API v2.0, Component v1.3.3)<br>
              MCA rmaps: seq (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA rml: oob (MCA v2.0, API v2.0, Component v1.3.3)<br>
             MCA routed: binomial (MCA v2.0, API v2.0, Component v1.3.3)<br>
             MCA routed: direct (MCA v2.0, API v2.0, Component v1.3.3)<br>
             MCA routed: linear (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA plm: rsh (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA plm: slurm (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA plm: xgrid (MCA v2.0, API v2.0, Component v1.3.3)<br>
              MCA filem: rsh (MCA v2.0, API v2.0, Component v1.3.3)<br>
             MCA errmgr: default (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA ess: env (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA ess: hnp (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA ess: singleton (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA ess: slurm (MCA v2.0, API v2.0, Component v1.3.3)<br>
                MCA ess: tool (MCA v2.0, API v2.0, Component v1.3.3)<br>
            MCA grpcomm: bad (MCA v2.0, API v2.0, Component v1.3.3)<br>
            MCA grpcomm: basic (MCA v2.0, API v2.0, Component v1.3.3)<br>
<br>
All seemed fine and I also have xgrid controller and agent running in my<br>
laptop, and then when I tried:<br>
<br>
/sw/bin/om-mpirun -c 2 mpiapp  # hello world example for mpi<br>
[amadeus.local:40293] [[804,0],0] ORTE_ERROR_LOG: Unknown error: 1 in file<br>
src/plm_xgrid_module.m at line 119<br>
[amadeus.local:40293] [[804,0],0] ORTE_ERROR_LOG: Unknown error: 1 in file<br>
src/plm_xgrid_module.m at line 153<br>
--------------------------------------------------------------------------<br>
om-mpirun was unable to start the specified application as it encountered an<br>
error.<br>
More information may be available above.<br>
--------------------------------------------------------------------------<br>
2009-08-14 14:16:19.715 om-mpirun[40293:10b] *** Terminating app due to<br>
uncaught exception &#39;NSInvalidArgumentException&#39;, reason: &#39;***<br>
-[NSKVONotifying_XGConnection&lt;0x1001164b0&gt; finalize]: called when collecting<br>
not enabled&#39;<br>
2009-08-14 14:16:19.716 om-mpirun[40293:10b] Stack: (<br>
   140735390096156,<br>
   140735366109391,<br>
   140735390122388,<br>
   4295943988,<br>
   4295939168,<br>
   4295171139,<br>
   4295883300,<br>
   4295025321,<br>
   4294973498,<br>
   4295401605,<br>
   4295345774,<br>
   4295056598,<br>
   4295116412,<br>
   4295119970,<br>
   4295401605,<br>
   4294972881,<br>
   4295401605,<br>
   4295345774,<br>
   4295056598,<br>
   4295172615,<br>
   4295938185,<br>
   4294971936,<br>
   4294969401,<br>
   4294969340<br>
)<br>
terminate called after throwing an instance of &#39;NSException&#39;<br>
[amadeus:40293] *** Process received signal ***<br>
[amadeus:40293] Signal: Abort trap (6)<br>
[amadeus:40293] Signal code:  (0)<br>
[amadeus:40293] [ 0] 2   libSystem.B.dylib<br>
0x00000000831443fa _sigtramp + 26<br>
[amadeus:40293] [ 1] 3   ???<br>
0x000000005fbfb1e8 0x0 + 1606398440<br>
[amadeus:40293] [ 2] 4   libstdc++.6.dylib<br>
0x00000000827f2085 _ZN9__gnu_cxx27__verbose_terminate_handlerEv + 377<br>
[amadeus:40293] [ 3] 5   libobjc.A.dylib<br>
0x0000000081811adf objc_end_catch + 280<br>
[amadeus:40293] [ 4] 6   libstdc++.6.dylib<br>
0x00000000827f0425 __gxx_personality_v0 + 1259<br>
[amadeus:40293] [ 5] 7   libstdc++.6.dylib<br>
0x00000000827f045b _ZSt9terminatev + 19<br>
[amadeus:40293] [ 6] 8   libstdc++.6.dylib<br>
0x00000000827f054c __cxa_rethrow + 0<br>
[amadeus:40293] [ 7] 9   libobjc.A.dylib<br>
0x0000000081811966 objc_exception_rethrow + 0<br>
[amadeus:40293] [ 8] 10  CoreFoundation<br>
0x0000000082ef8194 _CF_forwarding_prep_0 + 5700<br>
[amadeus:40293] [ 9] 11  mca_plm_xgrid.so<br>
0x00000000000ee734 orte_plm_xgrid_finalize + 4884<br>
[amadeus:40293] [10] 12  mca_plm_xgrid.so<br>
0x00000000000ed460 orte_plm_xgrid_finalize + 64<br>
[amadeus:40293] [11] 13  libopen-rte.0.dylib<br>
0x0000000000031c43 orte_plm_base_close + 195<br>
[amadeus:40293] [12] 14  mca_ess_hnp.so<br>
0x00000000000dfa24 0x0 + 916004<br>
[amadeus:40293] [13] 15  libopen-rte.0.dylib<br>
0x000000000000e2a9 orte_finalize + 89<br>
[amadeus:40293] [14] 16  om-mpirun<br>
0x000000000000183a start + 4210<br>
[amadeus:40293] [15] 17  libopen-pal.0.dylib<br>
0x000000000006a085 opal_event_add_i + 1781<br>
[amadeus:40293] [16] 18  libopen-pal.0.dylib<br>
0x000000000005c66e opal_progress + 142<br>
[amadeus:40293] [17] 19  libopen-rte.0.dylib<br>
0x0000000000015cd6 orte_trigger_event + 70<br>
[amadeus:40293] [18] 20  libopen-rte.0.dylib<br>
0x000000000002467c orte_daemon_recv + 4332<br>
[amadeus:40293] [19] 21  libopen-rte.0.dylib<br>
0x0000000000025462 orte_daemon_cmd_processor + 722<br>
[amadeus:40293] [20] 22  libopen-pal.0.dylib<br>
0x000000000006a085 opal_event_add_i + 1781<br>
[amadeus:40293] [21] 23  om-mpirun<br>
0x00000000000015d1 start + 3593<br>
[amadeus:40293] [22] 24  libopen-pal.0.dylib<br>
0x000000000006a085 opal_event_add_i + 1781<br>
[amadeus:40293] [23] 25  libopen-pal.0.dylib<br>
0x000000000005c66e opal_progress + 142<br>
[amadeus:40293] [24] 26  libopen-rte.0.dylib<br>
0x0000000000015cd6 orte_trigger_event + 70<br>
[amadeus:40293] [25] 27  libopen-rte.0.dylib<br>
0x0000000000032207 orte_plm_base_launch_failed + 135<br>
[amadeus:40293] [26] 28  mca_plm_xgrid.so<br>
0x00000000000ed089 orte_plm_xgrid_spawn + 89<br>
[amadeus:40293] [27] 29  om-mpirun<br>
0x0000000000001220 start + 2648<br>
[amadeus:40293] [28] 30  om-mpirun<br>
0x0000000000000839 start + 113<br>
[amadeus:40293] [29] 31  om-mpirun<br>
0x00000000000007fc start + 52<br>
[amadeus:40293] *** End of error message ***<br>
[1]    40293 abort      /sw/bin/om-mpirun -c 2 mpiapp<br>
<br>
<br>
Is there anyone using openmpi with xgrid successfully keen to share his/her<br>
experience? I am not new to xgrid or mpi, but to both integrated I must say<br>
that I am in uncharted waters.<br>
<br>
Any help would be very appreciated.<br>
<br>
Many thanks in advance,<br>
Alan<br>
-- <br>
Alan Wilter S. da Silva, D.Sc. - CCPN Research Associate<br>
Department of Biochemistry, University of Cambridge.<br>
80 Tennis Court Road, Cambridge CB2 1GA, UK.<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<a href="http://www.bio.cam.ac.uk/~awd28" target="_blank">http://www.bio.cam.ac.uk/~awd28</a>&lt;&lt;<br>
</blockquote></blockquote></div></div>
-------------- next part --------------<br>
HTML attachment scrubbed and removed<br>
<br>
------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
End of users Digest, Vol 1318, Issue 2<br>
**************************************<br>
</blockquote>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br><br clear="all"><br>-- <br>Alan Wilter S. da Silva, D.Sc. - CCPN Research Associate<br>Department of Biochemistry, University of Cambridge.<br>80 Tennis Court Road, Cambridge CB2 1GA, UK.<br>&gt;&gt;<a href="http://www.bio.cam.ac.uk/~awd28">http://www.bio.cam.ac.uk/~awd28</a>&lt;&lt;<br>


</div>

