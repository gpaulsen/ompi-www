<div class="gmail_extra"><p>
&gt;I just re-read the thread.
I think there&#39;s a little confusion between the terms &quot;processor&quot; and &quot;MPI process&quot; here.  
You said &quot;As a pre-processing step, each processor must figure 
out which other processors it must communicate with by virtue of sharing
 neighboring grid points.&quot;
Did you mean &quot;MPI process&quot; instead of &quot;processor&quot;? </p><p>The code is designed to be run using only one MPI process per core/slot/whatever word you want to use. I believe what is happening here is that OMPI is launching all MPI processes on a single slot. This is why my code is freaking out and telling me that a slot is asking for information it already owns. So, in order to answer your second point:<br>
</p><p>&gt;Secondly, if you&#39;re just running on a single machine with no scheduler and no hostile, you should be able to:
mpirun -np &lt;whatever_you_want&gt; your_program_name
When you get the &quot;There are not enough slots available in the 
system...&quot; message, that usually means that *something* is telling Open 
MPI a maximum number of processes that can be run, and your -np value is
 greater than that.  This is *usually* a scheduler, but can also be a 
hostile and/or an environment variable or file-based MCA parameter. <br></p>I wanted to force MPI to only assign a single process per each slot, so I used the -nooversubscribe option. This is when I get the error about there not being enough slots in the system to fulfill my request. I can use mpirun with np set to whatever I want and it will launch succesfully, but then my code kills itself because the processes are being oversubscribed to a single slot, which doesn&#39;t do me or my code any good at all.<br>
<br>So the problem is that even though I have 8, 24, and 48 core machines, OMPI thinks each one of them only has a single core, and will launch all MPI processes on that one core.<br><br>Kyle<br></div>

