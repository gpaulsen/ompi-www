<html dir="ltr">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<style id="owaParaStyle" type="text/css">P {margin-top:0;margin-bottom:0;}</style>
</head>
<body ocsi="0" fpstyle="1">
<div style="direction: ltr;font-family: Tahoma;color: #000000;font-size: 10pt;">I'm running a mixed cluster of Blades (HS21 and HS22 chassis), x3550-M3 and X3550-M4 systems, some of which support hyperthreading, while others<br>
don't (specifically the HS21 blades) all on CentOS 6.3 w/SGE.<br>
<br>
I have no problems running my simple OpenMPI 1.8.7 test code outside of SGE (with or without the --bind-to core switch, but can only run the jobs within<br>
SGE via qrsh on a limited number of slots (4 at most) successfully. The errors are very similar to the ones I was getting running OpenMPI 1.8.5 - 1.8.6 outside of SGE<br>
on this same cluster.<br>
<br>
Strangely, when running the test code outside of SGE w/the --bind-to core switch, mpirun still binds processes to hyperthreading cores. Additionally,<br>
the --bind-to core switch prevents the OpenMPI 1.8.7 test code from running at all within SGE (it outputs warnings about missing NUMA libraries reducing performance<br>
then exits).<br>
<br>
We would rather run out OpenMPI jobs from within SGE so that we can get accounting data on OpenMPI jobs for administrative purposes.<br>
<br>
The orte PE I'm been using seems to meet all the requirements for previous versions of OpenMPI:<br>
<blockquote>the allocation rule is fill-up, rather than round-robin (I'm not sure if this makes a difference or not)<br>
&nbsp;<br>
The value NONE in user_lists and xuser_lists mean enable everybody and exclude nobody.
<br>
&nbsp;<br>
The value of control_slaves must be TRUE; otherwise, qrsh exits with an error message.
<br>
&nbsp;<br>
The value of job_is_first_task must be FALSE or the job launcher consumes a slot. In other words, mpirun itself will count as one of the slots and the job will fail, because only n-1 processes will start.&nbsp;
<br>
&nbsp;<br>
And be sure the queue will make use of the PE that you specified<br>
</blockquote>
<br>
Below is the command line I've been using to generate the errors found in the attached file out.txt:<br>
<br>
qrsh -V -now yes -pe orte 186 mpirun -np 186 --prefix /hpc/apps/mpi/openmpi/1.8.7/ --mca btl_tcp_if_include eth0 --mca pls_gridengine_verbose 1 /hpc/home/lanew/mpi/openmpi/ProcessColors3 &gt;&gt; out.txt 2&gt;&amp;1<br>
<br>
Sorry for the length. Thanks in advance for any help in resolving this nagging issue (I wish we had a homogenous cluster now).<br>
<br>
-Bill L.<br>
</div>
IMPORTANT WARNING: This message is intended for the use of the person or entity to which it is addressed and may contain information that is privileged and confidential, the disclosure of which is governed by applicable law. If the reader of this message is
 not the intended recipient, or the employee or agent responsible for delivering it to the intended recipient, you are hereby notified that any dissemination, distribution or copying of this information is strictly prohibited. Thank you for your cooperation.
</body>
</html>

