Hi there,<div><br></div><div>I saw that <a href="http://www.open-mpi.org/community/lists/users/2007/08/3900.php" target="_blank">http://www.open-mpi.org/community/lists/users/2007/08/3900.php</a>.</div><div><br></div><div>

I use fink, and so I changed the <a href="http://openmpi.info" target="_blank">openmpi.info</a> file in order to get openmpi with xgrid support.</div>
<div><br></div><div>As you can see:</div><div><div>amadeus[2081]:~/Downloads% /sw/bin/ompi_info </div><div>                 Package: Open MPI root@amadeus.local Distribution</div><div>                Open MPI: 1.3.3</div>


<div>   Open MPI SVN revision: r21666</div><div>   Open MPI release date: Jul 14, 2009</div><div>                Open RTE: 1.3.3</div><div>   Open RTE SVN revision: r21666</div><div>   Open RTE release date: Jul 14, 2009</div>


<div>                    OPAL: 1.3.3</div><div>       OPAL SVN revision: r21666</div><div>       OPAL release date: Jul 14, 2009</div><div>            Ident string: 1.3.3</div><div>                  Prefix: /sw</div><div>


 Configured architecture: x86_64-apple-darwin9</div><div>          Configure host: amadeus.local</div><div>           Configured by: root</div><div>           Configured on: Fri Aug 14 12:58:12 BST 2009</div><div>          Configure host: amadeus.local</div>


<div>                Built by: </div><div>                Built on: Fri Aug 14 13:07:46 BST 2009</div><div>              Built host: amadeus.local</div><div>              C bindings: yes</div><div>            C++ bindings: yes</div>


<div>      Fortran77 bindings: yes (single underscore)</div><div>      Fortran90 bindings: yes</div><div> Fortran90 bindings size: small</div><div>              C compiler: gcc</div><div>     C compiler absolute: /sw/var/lib/fink/path-prefix-10.6/gcc</div>


<div>            C++ compiler: g++</div><div>   C++ compiler absolute: /sw/var/lib/fink/path-prefix-10.6/g++</div><div>      Fortran77 compiler: gfortran</div><div>  Fortran77 compiler abs: /sw/bin/gfortran</div><div>      Fortran90 compiler: gfortran</div>


<div>  Fortran90 compiler abs: /sw/bin/gfortran</div><div>             C profiling: yes</div><div>           C++ profiling: yes</div><div>     Fortran77 profiling: yes</div><div>     Fortran90 profiling: yes</div><div>          C++ exceptions: no</div>


<div>          Thread support: posix (mpi: no, progress: no)</div><div>           Sparse Groups: no</div><div>  Internal debug support: no</div><div>     MPI parameter check: runtime</div><div>Memory profiling support: no</div>


<div>Memory debugging support: no</div><div>         libltdl support: yes</div><div>   Heterogeneous support: no</div><div> mpirun default --prefix: no</div><div>         MPI I/O support: yes</div><div>       MPI_WTIME support: gettimeofday</div>


<div>Symbol visibility support: yes</div><div>   FT Checkpoint support: no  (checkpoint thread: no)</div><div>           MCA backtrace: execinfo (MCA v2.0, API v2.0, Component v1.3.3)</div><div>           MCA paffinity: darwin (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>               MCA carto: auto_detect (MCA v2.0, API v2.0, Component v1.3.3)</div><div>               MCA carto: file (MCA v2.0, API v2.0, Component v1.3.3)</div><div>           MCA maffinity: first_use (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>               MCA timer: darwin (MCA v2.0, API v2.0, Component v1.3.3)</div><div>         MCA installdirs: env (MCA v2.0, API v2.0, Component v1.3.3)</div><div>         MCA installdirs: config (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>                 MCA dpm: orte (MCA v2.0, API v2.0, Component v1.3.3)</div><div>              MCA pubsub: orte (MCA v2.0, API v2.0, Component v1.3.3)</div><div>           MCA allocator: basic (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>           MCA allocator: bucket (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                MCA coll: basic (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                MCA coll: hierarch (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>                MCA coll: inter (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                MCA coll: self (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                MCA coll: sm (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>                MCA coll: sync (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                MCA coll: tuned (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                  MCA io: romio (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>               MCA mpool: fake (MCA v2.0, API v2.0, Component v1.3.3)</div><div>               MCA mpool: rdma (MCA v2.0, API v2.0, Component v1.3.3)</div><div>               MCA mpool: sm (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>                 MCA pml: cm (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                 MCA pml: csum (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                 MCA pml: ob1 (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>                 MCA pml: v (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                 MCA bml: r2 (MCA v2.0, API v2.0, Component v1.3.3)</div><div>              MCA rcache: vma (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>                 MCA btl: self (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                 MCA btl: sm (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                 MCA btl: tcp (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>                MCA topo: unity (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                 MCA osc: pt2pt (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                 MCA osc: rdma (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>                 MCA iof: hnp (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                 MCA iof: orted (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                 MCA iof: tool (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>                 MCA oob: tcp (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                MCA odls: default (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                 MCA ras: slurm (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>               MCA rmaps: rank_file (MCA v2.0, API v2.0, Component v1.3.3)</div><div>               MCA rmaps: round_robin (MCA v2.0, API v2.0, Component v1.3.3)</div><div>               MCA rmaps: seq (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>                 MCA rml: oob (MCA v2.0, API v2.0, Component v1.3.3)</div><div>              MCA routed: binomial (MCA v2.0, API v2.0, Component v1.3.3)</div><div>              MCA routed: direct (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>              MCA routed: linear (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                 MCA plm: rsh (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                 MCA plm: slurm (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>                 MCA plm: xgrid (MCA v2.0, API v2.0, Component v1.3.3)</div><div>               MCA filem: rsh (MCA v2.0, API v2.0, Component v1.3.3)</div><div>              MCA errmgr: default (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>                 MCA ess: env (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                 MCA ess: hnp (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                 MCA ess: singleton (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>                 MCA ess: slurm (MCA v2.0, API v2.0, Component v1.3.3)</div><div>                 MCA ess: tool (MCA v2.0, API v2.0, Component v1.3.3)</div><div>             MCA grpcomm: bad (MCA v2.0, API v2.0, Component v1.3.3)</div>


<div>             MCA grpcomm: basic (MCA v2.0, API v2.0, Component v1.3.3)</div><div><br></div><div>All seemed fine and I also have xgrid controller and agent running in my laptop, and then when I tried:</div><div><br></div>

<div><div>/sw/bin/om-mpirun -c 2 mpiapp  # hello world example for mpi            </div>
<div>[amadeus.local:40293] [[804,0],0] ORTE_ERROR_LOG: Unknown error: 1 in file src/plm_xgrid_module.m at line 119</div><div>[amadeus.local:40293] [[804,0],0] ORTE_ERROR_LOG: Unknown error: 1 in file src/plm_xgrid_module.m at line 153</div>


<div><div>--------------------------------------------------------------------------</div><div>om-mpirun was unable to start the specified application as it encountered an error.</div><div>More information may be available above.</div>


<div>--------------------------------------------------------------------------</div><div>2009-08-14 14:16:19.715 om-mpirun[40293:10b] *** Terminating app due to uncaught exception &#39;NSInvalidArgumentException&#39;, reason: &#39;*** -[NSKVONotifying_XGConnection&lt;0x1001164b0&gt; finalize]: called when collecting not enabled&#39;</div>


<div>2009-08-14 14:16:19.716 om-mpirun[40293:10b] Stack: (</div><div>    140735390096156,</div><div>    140735366109391,</div><div>    140735390122388,</div><div>    4295943988,</div><div>    4295939168,</div><div>    4295171139,</div>


<div>    4295883300,</div><div>    4295025321,</div><div>    4294973498,</div><div>    4295401605,</div><div>    4295345774,</div><div>    4295056598,</div><div>    4295116412,</div><div>    4295119970,</div><div>    4295401605,</div>


<div>    4294972881,</div><div>    4295401605,</div><div>    4295345774,</div><div>    4295056598,</div><div>    4295172615,</div><div>    4295938185,</div><div>    4294971936,</div><div>    4294969401,</div><div>    4294969340</div>


<div>)</div><div>terminate called after throwing an instance of &#39;NSException&#39;</div><div>[amadeus:40293] *** Process received signal ***</div><div>[amadeus:40293] Signal: Abort trap (6)</div><div>[amadeus:40293] Signal code:  (0)</div>


<div>[amadeus:40293] [ 0] 2   libSystem.B.dylib                   0x00000000831443fa _sigtramp + 26</div><div>[amadeus:40293] [ 1] 3   ???                                 0x000000005fbfb1e8 0x0 + 1606398440</div><div>[amadeus:40293] [ 2] 4   libstdc++.6.dylib                   0x00000000827f2085 _ZN9__gnu_cxx27__verbose_terminate_handlerEv + 377</div>


<div>[amadeus:40293] [ 3] 5   libobjc.A.dylib                     0x0000000081811adf objc_end_catch + 280</div><div>[amadeus:40293] [ 4] 6   libstdc++.6.dylib                   0x00000000827f0425 __gxx_personality_v0 + 1259</div>


<div>[amadeus:40293] [ 5] 7   libstdc++.6.dylib                   0x00000000827f045b _ZSt9terminatev + 19</div><div>[amadeus:40293] [ 6] 8   libstdc++.6.dylib                   0x00000000827f054c __cxa_rethrow + 0</div><div>


[amadeus:40293] [ 7] 9   libobjc.A.dylib                     0x0000000081811966 objc_exception_rethrow + 0</div><div>[amadeus:40293] [ 8] 10  CoreFoundation                      0x0000000082ef8194 _CF_forwarding_prep_0 + 5700</div>


<div>[amadeus:40293] [ 9] 11  mca_plm_xgrid.so                    0x00000000000ee734 orte_plm_xgrid_finalize + 4884</div><div>[amadeus:40293] [10] 12  mca_plm_xgrid.so                    0x00000000000ed460 orte_plm_xgrid_finalize + 64</div>


<div>[amadeus:40293] [11] 13  libopen-rte.0.dylib                 0x0000000000031c43 orte_plm_base_close + 195</div><div>[amadeus:40293] [12] 14  mca_ess_hnp.so                      0x00000000000dfa24 0x0 + 916004</div><div>


[amadeus:40293] [13] 15  libopen-rte.0.dylib                 0x000000000000e2a9 orte_finalize + 89</div><div>[amadeus:40293] [14] 16  om-mpirun                           0x000000000000183a start + 4210</div><div>[amadeus:40293] [15] 17  libopen-pal.0.dylib                 0x000000000006a085 opal_event_add_i + 1781</div>


<div>[amadeus:40293] [16] 18  libopen-pal.0.dylib                 0x000000000005c66e opal_progress + 142</div><div>[amadeus:40293] [17] 19  libopen-rte.0.dylib                 0x0000000000015cd6 orte_trigger_event + 70</div>


<div>[amadeus:40293] [18] 20  libopen-rte.0.dylib                 0x000000000002467c orte_daemon_recv + 4332</div><div>[amadeus:40293] [19] 21  libopen-rte.0.dylib                 0x0000000000025462 orte_daemon_cmd_processor + 722</div>


<div>[amadeus:40293] [20] 22  libopen-pal.0.dylib                 0x000000000006a085 opal_event_add_i + 1781</div><div>[amadeus:40293] [21] 23  om-mpirun                           0x00000000000015d1 start + 3593</div><div>


[amadeus:40293] [22] 24  libopen-pal.0.dylib                 0x000000000006a085 opal_event_add_i + 1781</div><div>[amadeus:40293] [23] 25  libopen-pal.0.dylib                 0x000000000005c66e opal_progress + 142</div><div>


[amadeus:40293] [24] 26  libopen-rte.0.dylib                 0x0000000000015cd6 orte_trigger_event + 70</div><div>[amadeus:40293] [25] 27  libopen-rte.0.dylib                 0x0000000000032207 orte_plm_base_launch_failed + 135</div>


<div>[amadeus:40293] [26] 28  mca_plm_xgrid.so                    0x00000000000ed089 orte_plm_xgrid_spawn + 89</div><div>[amadeus:40293] [27] 29  om-mpirun                           0x0000000000001220 start + 2648</div><div>


[amadeus:40293] [28] 30  om-mpirun                           0x0000000000000839 start + 113</div><div>[amadeus:40293] [29] 31  om-mpirun                           0x00000000000007fc start + 52</div><div>[amadeus:40293] *** End of error message ***</div>


<div>[1]    40293 abort      /sw/bin/om-mpirun -c 2 mpiapp</div><div><br></div></div><div><br></div><div>Is there anyone using openmpi with xgrid successfully keen to share his/her experience? I am not new to xgrid or mpi, but to both integrated I must say that I am in uncharted waters.</div>


<div><br></div><div>Any help would be very appreciated.</div><div><br></div><div>Many thanks in advance,</div><div>Alan</div></div>-- <br>Alan Wilter S. da Silva, D.Sc. - CCPN Research Associate<br>Department of Biochemistry, University of Cambridge.<br>


80 Tennis Court Road, Cambridge CB2 1GA, UK.<br>&gt;&gt;<a href="http://www.bio.cam.ac.uk/~awd28" target="_blank">http://www.bio.cam.ac.uk/~awd28</a>&lt;&lt;<br>
</div>

