<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML><HEAD><TITLE></TITLE>
<META content=text/html;charset=ISO-8859-1 http-equiv=Content-Type>
<META content="MSHTML 5.00.3314.2100" name=GENERATOR></HEAD>
<BODY bgColor=#ffffff text=#000000>
<DIV><FONT face=Arial size=2>Eugene,</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>you did not take into account the 
dispersion/dephasing between different processes. As cluster size and the 
</FONT></DIV>
<DIV><FONT face=Arial size=2>number of instances of parallel process increase, 
</FONT><FONT face=Arial size=2>the dispersion increases as well, making 
different&nbsp;instances </FONT></DIV>
<DIV><FONT face=Arial size=2>to be a kind out of sync - </FONT><FONT face=Arial 
size=2>not really out of sync, </FONT><FONT face=Arial size=2>but just because 
of different speed of execution on different nodes, delays, etc... </FONT></DIV>
<DIV><FONT face=Arial size=2>If you account for this, </FONT><FONT face=Arial 
size=2>you get the result I </FONT><FONT face=Arial 
size=2>mentioned.</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT face=Arial size=2>Alex</FONT></DIV>
<DIV>&nbsp;</DIV>
<BLOCKQUOTE 
style="BORDER-LEFT: #000000 2px solid; MARGIN-LEFT: 5px; MARGIN-RIGHT: 0px; PADDING-LEFT: 5px; PADDING-RIGHT: 0px">
  <DIV style="FONT: 10pt arial">----- Original Message ----- </DIV>
  <DIV 
  style="BACKGROUND: #e4e4e4; FONT: 10pt arial; font-color: black"><B>From:</B> 
  <A href="mailto:eugene.loh@oracle.com" title=eugene.loh@oracle.com>Eugene 
  Loh</A> </DIV>
  <DIV style="FONT: 10pt arial"><B>To:</B> <A href="mailto:users@open-mpi.org" 
  title=users@open-mpi.org>Open MPI Users</A> </DIV>
  <DIV style="FONT: 10pt arial"><B>Sent:</B> Thursday, September 09, 2010 11:32 
  PM</DIV>
  <DIV style="FONT: 10pt arial"><B>Subject:</B> Re: [OMPI users] MPI_Reduce 
  performance</DIV>
  <DIV><BR></DIV>Alex A. Granovsky wrote: 
  <BLOCKQUOTE cite="mid000201cb504e$d7d96190$3c45efc3@alexg" type="cite">
    <META content="MSHTML 5.00.3314.2100" name=GENERATOR>
    <STYLE></STYLE>

    <DIV><FONT face=Arial size=2></FONT>
    <DIV><FONT face=Arial size=2><FONT face=Arial size=2>Isn't in evident from 
    the theory of random processes and probability theory 
    that&nbsp;</FONT></FONT><FONT face=Arial size=2>in the limit of infinitely 
    </FONT></DIV>
    <DIV><FONT face=Arial size=2>large cluster and parallel process, 
    </FONT><FONT face=Arial size=2>the probability of deadlocks </FONT><FONT 
    face=Arial size=2>with current implementation is unfortunately </FONT></DIV>
    <DIV><FONT face=Arial size=2>quite a finite quantity and in limit approaches 
    to unity regardless on any particular details of the 
    program.</FONT></DIV></DIV></BLOCKQUOTE>No, not at all.&nbsp; Consider 
  simulating a physical volume.&nbsp; Each process is assigned to some small 
  subvolume.&nbsp; It updates conditions locally, but on the surface of its 
  simulation subvolume it needs information from "nearby" processes.&nbsp; It 
  cannot proceed along the surface until it has that neighboring 
  information.&nbsp; Its neighbors, in turn, cannot proceed until their 
  neighbors have reached some point.&nbsp; Two distant processes can be quite 
  out of step with one another, but only by some bounded amount.&nbsp; At some 
  point, a leading process has to wait for information from a laggard to 
  propagate to it.&nbsp; All processes proceed together, in some loose lock-step 
  fashion.&nbsp; Many applications behave in this fashion.&nbsp; Actually, in 
  many applications, the synchronization is tightened in that "physics" is made 
  to propagate faster than neighbor-by-neighbor.<BR><BR>As the number of 
  processes increases, the laggard might seem relatively slower in comparison, 
  but that isn't deadlock.<BR><BR>As the size of the cluster increases, the 
  chances of a system component failure increase, but that also is a different 
  matter.<BR>
  <P>
  <HR>

  <P></P>_______________________________________________<BR>users mailing 
  list<BR>users@open-mpi.org<BR>http://www.open-mpi.org/mailman/listinfo.cgi/users</BLOCKQUOTE></BODY></HTML>

