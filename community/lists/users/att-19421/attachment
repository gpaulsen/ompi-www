<div class="gmail_quote">On Fri, Jun 1, 2012 at 8:09 AM, Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div id=":8x">It&#39;s been a loooong time since I&#39;ve run under PBS, so I don&#39;t remember if your script&#39;s environment is copied out to the remote nodes where your application actually runs.<br>
<br>
Can you verify that PATH and LD_LIBRARY_PATH are the same on all nodes in your PBS allocation after you module load?</div></blockquote></div><br>I compiled the following program and invoked it with &quot;mpiexec -bynode ./test-env&quot; in a PBS script.<br>
<br><div style="margin-left:40px"><span style="font-family:courier new,monospace">#include &quot;mpi.h&quot;</span><br style="font-family:courier new,monospace"><span style="font-family:courier new,monospace">#include &lt;stdio.h&gt;</span><br style="font-family:courier new,monospace">
<span style="font-family:courier new,monospace">#include &lt;string.h&gt;</span><br style="font-family:courier new,monospace"><span style="font-family:courier new,monospace">#include &lt;stdlib.h&gt;</span><br style="font-family:courier new,monospace">
<br style="font-family:courier new,monospace"><span style="font-family:courier new,monospace">int main (int argc, char *argv[])</span><br style="font-family:courier new,monospace"><span style="font-family:courier new,monospace">{</span><br style="font-family:courier new,monospace">
<span style="font-family:courier new,monospace">  int i, rank, size, namelen;</span><br style="font-family:courier new,monospace"><span style="font-family:courier new,monospace">  MPI_Status stat;</span><br style="font-family:courier new,monospace">
<br style="font-family:courier new,monospace"><span style="font-family:courier new,monospace">  MPI_Init (&amp;argc, &amp;argv);</span><br style="font-family:courier new,monospace"><br style="font-family:courier new,monospace">
<span style="font-family:courier new,monospace">  MPI_Comm_size (MPI_COMM_WORLD, &amp;size);</span><br style="font-family:courier new,monospace"><span style="font-family:courier new,monospace">  MPI_Comm_rank (MPI_COMM_WORLD, &amp;rank);</span><br style="font-family:courier new,monospace">
<br style="font-family:courier new,monospace"><span style="font-family:courier new,monospace">  printf(&quot;rank: %d: ld_library_path: %s\n&quot;, rank, getenv(&quot;LD_LIBRARY_PATH&quot;));</span><br style="font-family:courier new,monospace">
<br style="font-family:courier new,monospace"><span style="font-family:courier new,monospace">  MPI_Finalize ();</span><br style="font-family:courier new,monospace"><br style="font-family:courier new,monospace"><span style="font-family:courier new,monospace">  return (0);</span><br style="font-family:courier new,monospace">
<span style="font-family:courier new,monospace">}</span><br></div><br> I submitted the script with &quot;qsub -l procs=24 job.pbs&quot;, and got<br><br><div style="margin-left:40px"><span style="font-family:courier new,monospace">rank: 4: ld_library_path: /lustre/jasper/software/openmpi/openmpi-1.6-intel/lib:/lustre/jasper/software/intel//l_ics_2012.0.032/composer_xe_2011_sp1.6.233/ipp/lib/intel64:/lustre/jasper/software/intel//l_ics_2012.0.032/composer_xe_2011_sp1.6.233/mkl/lib/intel64:/lustre/jasper/software/intel//l_ics_2012.0.032/composer_xe_2011_sp1.6.233/compiler/lib/intel64:/lustre/jasper/software/intel//l_ics_2012.0.032/composer_xe_2011_sp1.6.233/tbb/lib/intel64:/home/esumbar/local/lib:/usr/lib/jvm/jre-1.6.0-sun/lib/amd64/server:/usr/lib/jvm/jre-1.6.0-sun/lib/amd64:/opt/sgi/sgimc/lib:/lustre/jasper/software/intel//l_ics_2012.0.032/composer_xe_2011_sp1.6.233/debugger/lib/intel64:/lustre/jasper/software/intel//l_ics_2012.0.032/composer_xe_2011_sp1.6.233/mpirt/lib/intel64</span><br>
<br style="font-family:courier new,monospace"><span style="font-family:courier new,monospace">rank: 3: ld_library_path: /lustre/jasper/software/openmpi/openmpi-1.6-intel/lib:/lustre/jasper/software/intel//l_ics_2012.0.032/composer_xe_2011_sp1.6.233/ipp/lib/intel64:/lustre/jasper/software/intel//l_ics_2012.0.032/composer_xe_2011_sp1.6.233/mkl/lib/intel64:/lustre/jasper/software/intel//l_ics_2012.0.032/composer_xe_2011_sp1.6.233/compiler/lib/intel64:/lustre/jasper/software/intel//l_ics_2012.0.032/composer_xe_2011_sp1.6.233/tbb/lib/intel64:/home/esumbar/local/lib:/usr/lib/jvm/jre-1.6.0-sun/lib/amd64/server:/usr/lib/jvm/jre-1.6.0-sun/lib/amd64:/opt/sgi/sgimc/lib:/lustre/jasper/software/intel//l_ics_2012.0.032/composer_xe_2011_sp1.6.233/debugger/lib/intel64:/lustre/jasper/software/intel//l_ics_2012.0.032/composer_xe_2011_sp1.6.233/mpirt/lib/intel64</span><br>
<br>...more of the same...<br></div><br clear="all">When I submitted it with -l procs=48, I got<br><br><div style="margin-left:40px"><span style="font-family:courier new,monospace">[cl2n004:11617] *** Process received signal ***</span><br style="font-family:courier new,monospace">
<span style="font-family:courier new,monospace">[cl2n004:11617] Signal: Segmentation fault (11)</span><br style="font-family:courier new,monospace"><span style="font-family:courier new,monospace">[cl2n004:11617] Signal code: Address not mapped (1)</span><br style="font-family:courier new,monospace">
<span style="font-family:courier new,monospace">[cl2n004:11617] Failing at address: 0x10</span><br style="font-family:courier new,monospace"><span style="font-family:courier new,monospace">[cl2n004:11617] [ 0] /lib64/libpthread.so.0 [0x376ca0ebe0]</span><br style="font-family:courier new,monospace">
<span style="font-family:courier new,monospace">[cl2n004:11617] [ 1] /lustre/jasper/software/openmpi/openmpi-1.6-intel/lib/libmpi.so.1(opal_memory_ptmalloc2_int_malloc+0x4b3) [0x2af788a98113]</span><br style="font-family:courier new,monospace">
<span style="font-family:courier new,monospace">[cl2n004:11617] [ 2] /lustre/jasper/software/openmpi/openmpi-1.6-intel/lib/libmpi.so.1(opal_memory_ptmalloc2_malloc+0x59) [0x2af788a9a8a9]</span><br style="font-family:courier new,monospace">
<span style="font-family:courier new,monospace">[cl2n004:11617] [ 3] /lustre/jasper/software/openmpi/openmpi-1.6-intel/lib/libmpi.so.1 [0x2af788a9a596]</span><br style="font-family:courier new,monospace"><span style="font-family:courier new,monospace">[cl2n004:11617] [ 4] /lustre/jasper/software/openmpi/openmpi-1.6-intel/lib/openmpi/mca_btl_openib.so [0x2af78c916654]</span><br style="font-family:courier new,monospace">
<span style="font-family:courier new,monospace">[cl2n004:11617] [ 5] /lib64/libpthread.so.0 [0x376ca0677d]</span><br style="font-family:courier new,monospace"><span style="font-family:courier new,monospace">[cl2n004:11617] [ 6] /lib64/libc.so.6(clone+0x6d) [0x376bed325d]</span><br style="font-family:courier new,monospace">
<span style="font-family:courier new,monospace">[cl2n004:11617] *** End of error message ***</span><br style="font-family:courier new,monospace"><span style="font-family:courier new,monospace">--------------------------------------------------------------------------</span><br style="font-family:courier new,monospace">
<span style="font-family:courier new,monospace">mpiexec noticed that process rank 4 with PID 11617 on node cl2n004 exited on signal 11 (Segmentation fault).</span><br style="font-family:courier new,monospace"><span style="font-family:courier new,monospace">--------------------------------------------------------------------------</span><br>
</div><br>It seems that failures happen for arbitrary reasons. When I added a line in the PBS script to print out the node allocation, the procs=24 case failed, but then it worked a few seconds later, with the same list of allocated nodes. So there&#39;s definitely something amiss with the cluster, although I wouldn&#39;t know where to start investigating. Perhaps there is a pre-installed OMPI somewhere that&#39;s interfering, but I&#39;m doubtful.<br>
<br>By the way, thanks for all the support.<br><br>-- <br><div><div></div><div>Edmund Sumbar<font color="#999999"><br>University of Alberta<br></font></div><div><font color="#999999">+1 780 492 9360</font></div></div><br>


