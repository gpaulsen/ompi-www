Sorry, I don&#39;t understand, how can I try the fortran from macports??.<br><br><div class="gmail_quote">2009/5/6 Luis Vitorio Cargnini <span dir="ltr">&lt;<a href="mailto:lvcargnini@gmail.com">lvcargnini@gmail.com</a>&gt;</span><br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">This problem is occuring because the fortran wasn&#39;t compiled with the debug symbols:<div class="im"><br>
warning: Could not find object file &quot;/Users/admin/build/i386-apple-darwin9.0.0/libgcc/_udiv_w_sdiv_s.o&quot; - no debug information available for &quot;../../../gcc-4.3-20071026/libgcc/../gcc/libgcc2.c&quot;.<br>
<br></div>
Is the same problem for who is using LLVM in Xcode, there is no debug symbols to create a debug release, try create a release and see if it will compile at all and try the fortran from macports it will works smoothly.<br>

<br>
<br>
Le 09-05-05 à 17:33, Jeff Squyres a écrit :<div><div></div><div class="h5"><br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
I agree; that is a bummer.  :-(<br>
<br>
Warner -- do you have any advice here, perchance?<br>
<br>
<br>
On May 4, 2009, at 7:26 PM, Vicente Puig wrote:<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
But it doesn&#39;t work well.<br>
<br>
For example, I am trying to debug a program, &quot;floyd&quot; in this case, and when I make a breakpoint:<br>
<br>
No line 26 in file &quot;../../../gcc-4.2-20060805/libgfortran/fmain.c&quot;.<br>
<br>
I am getting disappointed and frustrated that I can not work well with openmpi in my Mac. There should be a was to make it run in Xcode, uff...<br>
<br>
2009/5/4 Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;<br>
I get those as well.  I believe that they are (annoying but) harmless -- an artifact of how the freeware gcc/gofrtran that I use was built.<br>
<br>
<br>
<br>
On May 4, 2009, at 1:47 PM, Vicente Puig wrote:<br>
<br>
Maybe I had to open a new thread, but if you have any idea why I receive it when I use gdb for debugging an openmpi program:<br>
<br>
warning: Could not find object file &quot;/Users/admin/build/i386-apple-darwin9.0.0/libgcc/_umoddi3_s.o&quot; - no debug information available for &quot;../../../gcc-4.3-20071026/libgcc/../gcc/libgcc2.c&quot;.<br>
<br>
<br>
warning: Could not find object file &quot;/Users/admin/build/i386-apple-darwin9.0.0/libgcc/_udiv_w_sdiv_s.o&quot; - no debug information available for &quot;../../../gcc-4.3-20071026/libgcc/../gcc/libgcc2.c&quot;.<br>
<br>
<br>
warning: Could not find object file &quot;/Users/admin/build/i386-apple-darwin9.0.0/libgcc/_udivmoddi4_s.o&quot; - no debug information available for &quot;../../../gcc-4.3-20071026/libgcc/../gcc/libgcc2.c&quot;.<br>
<br>
<br>
warning: Could not find object file &quot;/Users/admin/build/i386-apple-darwin9.0.0/libgcc/unwind-dw2_s.o&quot; - no debug information available for &quot;../../../gcc-4.3-20071026/libgcc/../gcc/unwind-dw2.c&quot;.<br>
<br>
<br>
warning: Could not find object file &quot;/Users/admin/build/i386-apple-darwin9.0.0/libgcc/unwind-dw2-fde-darwin_s.o&quot; - no debug information available for &quot;../../../gcc-4.3-20071026/libgcc/../gcc/unwind-dw2-fde-darwin.c&quot;.<br>

<br>
<br>
warning: Could not find object file &quot;/Users/admin/build/i386-apple-darwin9.0.0/libgcc/unwind-c_s.o&quot; - no debug information available for &quot;../../../gcc-4.3-20071026/libgcc/../gcc/unwind-c.c&quot;.<br>
.......<br>
<br>
<br>
<br>
There is no &#39;admin&#39; so I don&#39;t know why it happen. It works well with a C program.<br>
<br>
Any idea??.<br>
<br>
Thanks.<br>
<br>
<br>
Vincent<br>
<br>
<br>
<br>
<br>
<br>
2009/5/4 Vicente Puig &lt;<a href="mailto:vpuibor@gmail.com" target="_blank">vpuibor@gmail.com</a>&gt;<br>
I can run openmpi perfectly with command line, but I wanted a graphic interface for debugging because I was having problems.<br>
<br>
Thanks anyway.<br>
<br>
Vincent<br>
<br>
2009/5/4 Warner Yuen &lt;<a href="mailto:wyuen@apple.com" target="_blank">wyuen@apple.com</a>&gt;<br>
<br>
Admittedly, I don&#39;t use Xcode to build Open MPI either.<br>
<br>
You can just compile Open MPI from the command line and install everything in /usr/local/. Make sure that gfortran is set in your path and you should just be able to do a &#39;./configure --prefix=/usr/local&#39;<br>
<br>
After the installation, just make sure that your path is set correctly when you go to use the newly installed Open MPI. If you don&#39;t set your path, it will always default to using the version of OpenMPI that ships with Leopard.<br>

<br>
<br>
Warner Yuen<br>
Scientific Computing<br>
Consulting Engineer<br>
Apple, Inc.<br>
email: <a href="mailto:wyuen@apple.com" target="_blank">wyuen@apple.com</a><br>
Tel: 408.718.2859<br>
<br>
<br>
<br>
<br>
On May 4, 2009, at 9:13 AM, <a href="mailto:users-request@open-mpi.org" target="_blank">users-request@open-mpi.org</a> wrote:<br>
<br>
Send users mailing list submissions to<br>
     <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<br>
To subscribe or unsubscribe via the World Wide Web, visit<br>
     <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
or, via email, send a message with subject or body &#39;help&#39; to<br>
     <a href="mailto:users-request@open-mpi.org" target="_blank">users-request@open-mpi.org</a><br>
<br>
You can reach the person managing the list at<br>
     <a href="mailto:users-owner@open-mpi.org" target="_blank">users-owner@open-mpi.org</a><br>
<br>
When replying, please edit your Subject line so it is more specific<br>
than &quot;Re: Contents of users digest...&quot;<br>
<br>
<br>
Today&#39;s Topics:<br>
<br>
1. Re: How do I compile OpenMPI in Xcode 3.1 (Vicente Puig)<br>
<br>
<br>
----------------------------------------------------------------------<br>
<br>
Message: 1<br>
Date: Mon, 4 May 2009 18:13:45 +0200<br>
From: Vicente Puig &lt;<a href="mailto:vpuibor@gmail.com" target="_blank">vpuibor@gmail.com</a>&gt;<br>
Subject: Re: [OMPI users] How do I compile OpenMPI in Xcode 3.1<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
Message-ID:<br>
     &lt;<a href="mailto:3e9a21680905040913u3f36d3c9rdcd3413bfdcd0c9@mail.gmail.com" target="_blank">3e9a21680905040913u3f36d3c9rdcd3413bfdcd0c9@mail.gmail.com</a>&gt;<br>
Content-Type: text/plain; charset=&quot;iso-8859-1&quot;<br>
<br>
If I can not make it work with Xcode,  which one could I use?, which one do<br>
you use to compile and debug OpenMPI?.<br>
Thanks<br>
<br>
Vincent<br>
<br>
<br>
2009/5/4 Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;<br>
<br>
Open MPI comes pre-installed in Leopard; as Warner noted, since Leopard<br>
doesn&#39;t ship with a Fortran compiler, the Open MPI that Apple ships has<br>
non-functional mpif77 and mpif90 wrapper compilers.<br>
<br>
So the Open MPI that you installed manually will use your Fortran<br>
compilers, and therefore will have functional mpif77 and mpif90 wrapper<br>
compilers.  Hence, you probably need to be sure to use the &quot;right&quot; wrapper<br>
compilers.  It looks like you specified the full path specified to ExecPath,<br>
so I&#39;m not sure why Xcode wouldn&#39;t work with that (like I mentioned, I<br>
unfortunately don&#39;t use Xcode myself, so I don&#39;t know why that wouldn&#39;t<br>
work).<br>
<br>
<br>
<br>
<br>
On May 4, 2009, at 11:53 AM, Vicente wrote:<br>
<br>
Yes, I already have gfortran compiler on /usr/local/bin, the same path<br>
as my mpif90 compiler. But I&#39;ve seen when I use the mpif90 on /usr/bin<br>
and on  /Developer/usr/bin says it:<br>
<br>
&quot;Unfortunately, this installation of Open MPI was not compiled with<br>
Fortran 90 support.  As such, the mpif90 compiler is non-functional.&quot;<br>
<br>
<br>
That should be the problem, I will have to change the path to use the<br>
gfortran I have installed.<br>
How could I do it? (Sorry, I am beginner)<br>
<br>
Thanks.<br>
<br>
<br>
El 04/05/2009, a las 17:38, Warner Yuen escribi?:<br>
<br>
Have you installed a Fortran compiler? Mac OS X&#39;s developer tools do<br>
not come with a Fortran compiler, so you&#39;ll need to install one if<br>
you haven&#39;t already done so. I routinely use the Intel IFORT<br>
compilers with success. However, I hear many good things about the<br>
gfortran compilers on Mac OS X, you can&#39;t beat the price of gfortran!<br>
<br>
<br>
Warner Yuen<br>
Scientific Computing<br>
Consulting Engineer<br>
Apple, Inc.<br>
email: <a href="mailto:wyuen@apple.com" target="_blank">wyuen@apple.com</a><br>
Tel: 408.718.2859<br>
<br>
<br>
<br>
<br>
On May 4, 2009, at 7:28 AM, <a href="mailto:users-request@open-mpi.org" target="_blank">users-request@open-mpi.org</a> wrote:<br>
<br>
Send users mailing list submissions to<br>
 <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<br>
To subscribe or unsubscribe via the World Wide Web, visit<br>
 <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
or, via email, send a message with subject or body &#39;help&#39; to<br>
 <a href="mailto:users-request@open-mpi.org" target="_blank">users-request@open-mpi.org</a><br>
<br>
You can reach the person managing the list at<br>
 <a href="mailto:users-owner@open-mpi.org" target="_blank">users-owner@open-mpi.org</a><br>
<br>
When replying, please edit your Subject line so it is more specific<br>
than &quot;Re: Contents of users digest...&quot;<br>
<br>
<br>
Today&#39;s Topics:<br>
<br>
1. How do I compile OpenMPI in Xcode 3.1 (Vicente)<br>
2. Re: 1.3.1 -rf rankfile behaviour ?? (Ralph Castain)<br>
<br>
<br>
----------------------------------------------------------------------<br>
<br>
Message: 1<br>
Date: Mon, 4 May 2009 16:12:44 +0200<br>
From: Vicente &lt;<a href="mailto:vpuibor@gmail.com" target="_blank">vpuibor@gmail.com</a>&gt;<br>
Subject: [OMPI users] How do I compile OpenMPI in Xcode 3.1<br>
To: <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Message-ID: &lt;<a href="mailto:1C2C0085-940F-43BB-910F-975871AE2F09@gmail.com" target="_blank">1C2C0085-940F-43BB-910F-975871AE2F09@gmail.com</a>&gt;<br>
Content-Type: text/plain; charset=&quot;windows-1252&quot;; Format=&quot;flowed&quot;;<br>
 DelSp=&quot;yes&quot;<br>
<br>
Hi, I&#39;ve seen the FAQ &quot;How do I use Open MPI wrapper compilers in<br>
Xcode&quot;, but it&#39;s only for MPICC. I am using MPIF90, so I did the<br>
same,<br>
but changing MPICC for MPIF90, and also the path, but it did not<br>
work.<br>
<br>
Building target ?fortran? of project ?fortran? with configuration<br>
?Debug?<br>
<br>
<br>
Checking Dependencies<br>
Invalid value &#39;MPIF90&#39; for GCC_VERSION<br>
<br>
<br>
The file &quot;MPIF90.cpcompspec&quot; looks like this:<br>
<br>
1 /**<br>
2         Xcode Coompiler Specification for MPIF90<br>
3<br>
4 */<br>
5<br>
6 {   Type = Compiler;<br>
7     Identifier = com.apple.compilers.mpif90;<br>
8     BasedOn = com.apple.compilers.gcc.4_0;<br>
9     Name = &quot;MPIF90&quot;;<br>
10     Version = &quot;Default&quot;;<br>
11     Description = &quot;MPI GNU C/C++ Compiler 4.0&quot;;<br>
12     ExecPath = &quot;/usr/local/bin/mpif90&quot;;      // This gets<br>
converted to the g++ variant automatically<br>
13     PrecompStyle = pch;<br>
14 }<br>
<br>
and is located in &quot;/Developer/Library/Xcode/Plug-ins&quot;<br>
<br>
and when I do mpif90 -v on terminal it works well:<br>
<br>
Using built-in specs.<br>
Target: i386-apple-darwin8.10.1<br>
Configured with: /tmp/gfortran-20090321/ibin/../gcc/configure --<br>
prefix=/usr/local/gfortran --enable-languages=c,fortran --with-gmp=/<br>
tmp/gfortran-20090321/gfortran_libs --enable-bootstrap<br>
Thread model: posix<br>
gcc version 4.4.0 20090321 (experimental) [trunk revision 144983]<br>
(GCC)<br>
<br>
<br>
Any idea??<br>
<br>
Thanks.<br>
<br>
Vincent<br>
-------------- next part --------------<br>
HTML attachment scrubbed and removed<br>
<br>
------------------------------<br>
<br>
Message: 2<br>
Date: Mon, 4 May 2009 08:28:26 -0600<br>
From: Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;<br>
Subject: Re: [OMPI users] 1.3.1 -rf rankfile behaviour ??<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
Message-ID:<br>
 &lt;<a href="mailto:71d2d8cc0905040728h2002f4d7s4c49219eee29e86f@mail.gmail.com" target="_blank">71d2d8cc0905040728h2002f4d7s4c49219eee29e86f@mail.gmail.com</a>&gt;<br>
Content-Type: text/plain; charset=&quot;iso-8859-1&quot;<br>
<br>
Unfortunately, I didn&#39;t write any of that code - I was just fixing<br>
the<br>
mapper so it would properly map the procs. From what I can tell,<br>
the proper<br>
things are happening there.<br>
<br>
I&#39;ll have to dig into the code that specifically deals with parsing<br>
the<br>
results to bind the processes. Afraid that will take awhile longer<br>
- pretty<br>
dark in that hole.<br>
<br>
<br>
On Mon, May 4, 2009 at 8:04 AM, Geoffroy Pignot<br>
&lt;<a href="mailto:geopignot@gmail.com" target="_blank">geopignot@gmail.com</a>&gt; wrote:<br>
<br>
Hi,<br>
<br>
So, there are no more crashes with my &quot;crazy&quot; mpirun command. But<br>
the<br>
paffinity feature seems to be broken. Indeed I am not able to pin my<br>
processes.<br>
<br>
Simple test with a program using your plpa library :<br>
<br>
r011n006% cat hostf<br>
r011n006 slots=4<br>
<br>
r011n006% cat rankf<br>
rank 0=r011n006 slot=0   ----&gt; bind to CPU 0 , exact ?<br>
<br>
r011n006% /tmp/HALMPI/openmpi-1.4a/bin/mpirun --hostfile hostf --<br>
rankfile<br>
rankf --wdir /tmp -n 1 a.out<br>
PLPA Number of processors online: 4<br>
PLPA Number of processor sockets: 2<br>
PLPA Socket 0 (ID 0): 2 cores<br>
PLPA Socket 1 (ID 3): 2 cores<br>
<br>
Ctrl+Z<br>
r011n006%bg<br>
<br>
r011n006% ps axo stat,user,psr,pid,pcpu,comm | grep gpignot<br>
R+   gpignot    3  9271 97.8 a.out<br>
<br>
In fact whatever the slot number I put in my rankfile , a.out<br>
always runs<br>
on the CPU 3. I was looking for it on CPU 0 accordind to my<br>
cpuinfo file<br>
(see below)<br>
The result is the same if I try another syntax (rank 0=r011n006<br>
slot=0:0<br>
bind to socket 0 - core 0  , exact ? )<br>
<br>
Thanks in advance<br>
<br>
Geoffroy<br>
<br>
PS: I run on rhel5<br>
<br>
r011n006% uname -a<br>
Linux r011n006 2.6.18-92.1.1NOMAP32.el5 #1 SMP Sat Mar 15 01:46:39<br>
CDT 2008<br>
x86_64 x86_64 x86_64 GNU/Linux<br>
<br>
My configure is :<br>
./configure --prefix=/tmp/openmpi-1.4a --libdir=&#39;${exec_prefix}/<br>
lib64&#39;<br>
--disable-dlopen --disable-mpi-cxx --enable-heterogeneous<br>
<br>
<br>
r011n006% cat /proc/cpuinfo<br>
processor       : 0<br>
vendor_id       : GenuineIntel<br>
cpu family      : 6<br>
model           : 15<br>
model name      : Intel(R) Xeon(R) CPU            5150  @ 2.66GHz<br>
stepping        : 6<br>
cpu MHz         : 2660.007<br>
cache size      : 4096 KB<br>
physical id     : 0<br>
siblings        : 2<br>
core id         : 0<br>
cpu cores       : 2<br>
fpu             : yes<br>
fpu_exception   : yes<br>
cpuid level     : 10<br>
wp              : yes<br>
flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr<br>
pge mca<br>
cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm syscall<br>
nx lm<br>
constant_tsc pni monitor ds_cpl vmx est tm2 cx16 xtpr lahf_lm<br>
bogomips        : 5323.68<br>
clflush size    : 64<br>
cache_alignment : 64<br>
address sizes   : 36 bits physical, 48 bits virtual<br>
power management:<br>
<br>
processor       : 1<br>
vendor_id       : GenuineIntel<br>
cpu family      : 6<br>
model           : 15<br>
model name      : Intel(R) Xeon(R) CPU            5150  @ 2.66GHz<br>
stepping        : 6<br>
cpu MHz         : 2660.007<br>
cache size      : 4096 KB<br>
physical id     : 3<br>
siblings        : 2<br>
core id         : 0<br>
cpu cores       : 2<br>
fpu             : yes<br>
fpu_exception   : yes<br>
cpuid level     : 10<br>
wp              : yes<br>
flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr<br>
pge mca<br>
cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm syscall<br>
nx lm<br>
constant_tsc pni monitor ds_cpl vmx est tm2 cx16 xtpr lahf_lm<br>
bogomips        : 5320.03<br>
clflush size    : 64<br>
cache_alignment : 64<br>
address sizes   : 36 bits physical, 48 bits virtual<br>
power management:<br>
<br>
processor       : 2<br>
vendor_id       : GenuineIntel<br>
cpu family      : 6<br>
model           : 15<br>
model name      : Intel(R) Xeon(R) CPU            5150  @ 2.66GHz<br>
stepping        : 6<br>
cpu MHz         : 2660.007<br>
cache size      : 4096 KB<br>
physical id     : 0<br>
siblings        : 2<br>
core id         : 1<br>
cpu cores       : 2<br>
fpu             : yes<br>
fpu_exception   : yes<br>
cpuid level     : 10<br>
wp              : yes<br>
flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr<br>
pge mca<br>
cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm syscall<br>
nx lm<br>
constant_tsc pni monitor ds_cpl vmx est tm2 cx16 xtpr lahf_lm<br>
bogomips        : 5319.39<br>
clflush size    : 64<br>
cache_alignment : 64<br>
address sizes   : 36 bits physical, 48 bits virtual<br>
power management:<br>
<br>
processor       : 3<br>
vendor_id       : GenuineIntel<br>
cpu family      : 6<br>
model           : 15<br>
model name      : Intel(R) Xeon(R) CPU            5150  @ 2.66GHz<br>
stepping        : 6<br>
cpu MHz         : 2660.007<br>
cache size      : 4096 KB<br>
physical id     : 3<br>
siblings        : 2<br>
core id         : 1<br>
cpu cores       : 2<br>
fpu             : yes<br>
fpu_exception   : yes<br>
cpuid level     : 10<br>
wp              : yes<br>
flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr<br>
pge mca<br>
cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm syscall<br>
nx lm<br>
constant_tsc pni monitor ds_cpl vmx est tm2 cx16 xtpr lahf_lm<br>
bogomips        : 5320.03<br>
clflush size    : 64<br>
cache_alignment : 64<br>
address sizes   : 36 bits physical, 48 bits virtual<br>
power management:<br>
<br>
<br>
------------------------------<br>
<br>
Message: 2<br>
Date: Mon, 4 May 2009 04:45:57 -0600<br>
From: Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;<br>
Subject: Re: [OMPI users] 1.3.1 -rf rankfile behaviour ??<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
Message-ID: &lt;<a href="mailto:D01D7B16-4B47-46F3-AD41-D1A90B2E4927@open-mpi.org" target="_blank">D01D7B16-4B47-46F3-AD41-D1A90B2E4927@open-mpi.org</a>&gt;<br>
<br>
Content-Type: text/plain; charset=&quot;us-ascii&quot;; Format=&quot;flowed&quot;;<br>
 DelSp=&quot;yes&quot;<br>
<br>
My apologies - I wasn&#39;t clear enough. You need a tarball from<br>
r21111<br>
or greater...such as:<br>
<br>
<a href="http://www.open-mpi.org/nightly/trunk/openmpi-1.4a1r21142.tar.gz" target="_blank">http://www.open-mpi.org/nightly/trunk/openmpi-1.4a1r21142.tar.gz</a><br>
<br>
HTH<br>
Ralph<br>
<br>
<br>
On May 4, 2009, at 2:14 AM, Geoffroy Pignot wrote:<br>
<br>
Hi ,<br>
<br>
I got the openmpi-1.4a1r21095.tar.gz tarball, but unfortunately my<br>
command doesn&#39;t work<br>
<br>
cat rankf:<br>
rank 0=node1 slot=*<br>
rank 1=node2 slot=*<br>
<br>
cat hostf:<br>
node1 slots=2<br>
node2 slots=2<br>
<br>
mpirun  --rankfile rankf --hostfile hostf  --host node1 -n 1<br>
hostname : --host node2 -n 1 hostname<br>
<br>
Error, invalid rank (1) in the rankfile (rankf)<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
[r011n006:28986] [[45541,0],0] ORTE_ERROR_LOG: Bad parameter in<br>
file<br>
rmaps_rank_file.c at line 403<br>
[r011n006:28986] [[45541,0],0] ORTE_ERROR_LOG: Bad parameter in<br>
file<br>
base/rmaps_base_map_job.c at line 86<br>
[r011n006:28986] [[45541,0],0] ORTE_ERROR_LOG: Bad parameter in<br>
file<br>
base/plm_base_launch_support.c at line 86<br>
[r011n006:28986] [[45541,0],0] ORTE_ERROR_LOG: Bad parameter in<br>
file<br>
plm_rsh_module.c at line 1016<br>
<br>
<br>
Ralph, could you tell me if my command syntax is correct or<br>
not ? if<br>
not, give me the expected one ?<br>
<br>
Regards<br>
<br>
Geoffroy<br>
<br>
<br>
<br>
<br>
2009/4/30 Geoffroy Pignot &lt;<a href="mailto:geopignot@gmail.com" target="_blank">geopignot@gmail.com</a>&gt;<br>
Immediately Sir !!! :)<br>
<br>
Thanks again Ralph<br>
<br>
Geoffroy<br>
<br>
<br>
<br>
<br>
<br>
------------------------------<br>
<br>
Message: 2<br>
Date: Thu, 30 Apr 2009 06:45:39 -0600<br>
From: Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;<br>
Subject: Re: [OMPI users] 1.3.1 -rf rankfile behaviour ??<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
Message-ID:<br>
 &lt;<a href="mailto:71d2d8cc0904300545v61a42fe1k50086d2704d0f7e6@mail.gmail.com" target="_blank">71d2d8cc0904300545v61a42fe1k50086d2704d0f7e6@mail.gmail.com</a>&gt;<br>
Content-Type: text/plain; charset=&quot;iso-8859-1&quot;<br>
<br>
I believe this is fixed now in our development trunk - you can<br>
download any<br>
tarball starting from last night and give it a try, if you like.<br>
Any<br>
feedback would be appreciated.<br>
<br>
Ralph<br>
<br>
<br>
On Apr 14, 2009, at 7:57 AM, Ralph Castain wrote:<br>
<br>
Ah now, I didn&#39;t say it -worked-, did I? :-)<br>
<br>
Clearly a bug exists in the program. I&#39;ll try to take a look at it<br>
(if Lenny<br>
doesn&#39;t get to it first), but it won&#39;t be until later in the week.<br>
<br>
On Apr 14, 2009, at 7:18 AM, Geoffroy Pignot wrote:<br>
<br>
I agree with you Ralph , and that &#39;s what I expect from openmpi<br>
but my<br>
second example shows that it&#39;s not working<br>
<br>
cat hostfile.0<br>
r011n002 slots=4<br>
r011n003 slots=4<br>
<br>
cat rankfile.0<br>
rank 0=r011n002 slot=0<br>
rank 1=r011n003 slot=1<br>
<br>
mpirun --hostfile hostfile.0 -rf rankfile.0 -n 1 hostname : -n 1<br>
hostname<br>
### CRASHED<br>
<br>
Error, invalid rank (1) in the rankfile (rankfile.0)<br>
<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
[r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in<br>
file<br>
rmaps_rank_file.c at line 404<br>
[r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in<br>
file<br>
base/rmaps_base_map_job.c at line 87<br>
[r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in<br>
file<br>
base/plm_base_launch_support.c at line 77<br>
[r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in<br>
file<br>
plm_rsh_module.c at line 985<br>
<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
A daemon (pid unknown) died unexpectedly on signal 1  while<br>
attempting to<br>
launch so we are aborting.<br>
<br>
There may be more information reported by the environment (see<br>
above).<br>
<br>
This may be because the daemon was unable to find all the needed<br>
shared<br>
libraries on the remote node. You may set your LD_LIBRARY_PATH<br>
to<br>
have the<br>
location of the shared libraries on the remote nodes and this<br>
will<br>
automatically be forwarded to the remote nodes.<br>
<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
orterun noticed that the job aborted, but has no info as to the<br>
process<br>
that caused that situation.<br>
<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
orterun: clean termination accomplished<br>
<br>
<br>
<br>
Message: 4<br>
Date: Tue, 14 Apr 2009 06:55:58 -0600<br>
From: Ralph Castain &lt;<a href="mailto:rhc@lanl.gov" target="_blank">rhc@lanl.gov</a>&gt;<br>
Subject: Re: [OMPI users] 1.3.1 -rf rankfile behaviour ??<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
Message-ID: &lt;<a href="mailto:F6290ADA-A196-43F0-A853-CBCB802D8D9C@lanl.gov" target="_blank">F6290ADA-A196-43F0-A853-CBCB802D8D9C@lanl.gov</a>&gt;<br>
Content-Type: text/plain; charset=&quot;us-ascii&quot;; Format=&quot;flowed&quot;;<br>
DelSp=&quot;yes&quot;<br>
<br>
The rankfile cuts across the entire job - it isn&#39;t applied on an<br>
app_context basis. So the ranks in your rankfile must correspond<br>
to<br>
the eventual rank of each process in the cmd line.<br>
<br>
Unfortunately, that means you have to count ranks. In your case,<br>
you<br>
only have four, so that makes life easier. Your rankfile would<br>
look<br>
something like this:<br>
<br>
rank 0=r001n001 slot=0<br>
rank 1=r001n002 slot=1<br>
rank 2=r001n001 slot=1<br>
rank 3=r001n002 slot=2<br>
<br>
HTH<br>
Ralph<br>
<br>
On Apr 14, 2009, at 12:19 AM, Geoffroy Pignot wrote:<br>
<br>
Hi,<br>
<br>
I agree that my examples are not very clear. What I want to do<br>
is to<br>
launch a multiexes application (masters-slaves) and benefit<br>
from the<br>
processor affinity.<br>
Could you show me how to convert this command , using -rf option<br>
(whatever the affinity is)<br>
<br>
mpirun -n 1 -host r001n001 master.x options1  : -n 1 -host<br>
r001n002<br>
master.x options2 : -n 1 -host r001n001 slave.x options3 : -n 1 -<br>
host r001n002 slave.x options4<br>
<br>
Thanks for your help<br>
<br>
Geoffroy<br>
<br>
<br>
<br>
<br>
<br>
Message: 2<br>
Date: Sun, 12 Apr 2009 18:26:35 +0300<br>
From: Lenny Verkhovsky &lt;<a href="mailto:lenny.verkhovsky@gmail.com" target="_blank">lenny.verkhovsky@gmail.com</a>&gt;<br>
Subject: Re: [OMPI users] 1.3.1 -rf rankfile behaviour ??<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
Message-ID:<br>
<br>
&lt;<a href="mailto:453d39990904120826t2e1d1d33l7bb1fe3de65b5361@mail.gmail.com" target="_blank">453d39990904120826t2e1d1d33l7bb1fe3de65b5361@mail.gmail.com</a>&gt;<br>
Content-Type: text/plain; charset=&quot;iso-8859-1&quot;<br>
<br>
Hi,<br>
<br>
The first &quot;crash&quot; is OK, since your rankfile has ranks 0 and 1<br>
defined,<br>
while n=1, which means only rank 0 is present and can be<br>
allocated.<br>
<br>
NP must be &gt;= the largest rank in rankfile.<br>
<br>
What exactly are you trying to do ?<br>
<br>
I tried to recreate your seqv but all I got was<br>
<br>
~/work/svn/ompi/trunk/build_x86-64/install/bin/mpirun --hostfile<br>
hostfile.0<br>
-rf rankfile.0 -n 1 hostname : -rf rankfile.1 -n 1 hostname<br>
[witch19:30798] mca: base: component_find: paffinity<br>
&quot;mca_paffinity_linux&quot;<br>
uses an MCA interface that is not recognized (component MCA<br>
v1.0.0 !=<br>
supported MCA v2.0.0) -- ignored<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
It looks like opal_init failed for some reason; your parallel<br>
process is<br>
likely to abort. There are many reasons that a parallel process<br>
can<br>
fail during opal_init; some of which are due to configuration or<br>
environment problems. This failure appears to be an internal<br>
failure;<br>
here&#39;s some additional information (which may only be relevant<br>
to an<br>
Open MPI developer):<br>
<br>
opal_carto_base_select failed<br>
--&gt; Returned value -13 instead of OPAL_SUCCESS<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
[witch19:30798] [[INVALID],INVALID] ORTE_ERROR_LOG: Not found in<br>
file<br>
../../orte/runtime/orte_init.c at line 78<br>
[witch19:30798] [[INVALID],INVALID] ORTE_ERROR_LOG: Not found in<br>
file<br>
../../orte/orted/orted_main.c at line 344<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
A daemon (pid 11629) died unexpectedly with status 243 while<br>
attempting<br>
to launch so we are aborting.<br>
<br>
There may be more information reported by the environment (see<br>
above).<br>
<br>
This may be because the daemon was unable to find all the needed<br>
shared<br>
libraries on the remote node. You may set your LD_LIBRARY_PATH to<br>
have the<br>
location of the shared libraries on the remote nodes and this<br>
will<br>
automatically be forwarded to the remote nodes.<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
mpirun noticed that the job aborted, but has no info as to the<br>
process<br>
that caused that situation.<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
mpirun: clean termination accomplished<br>
<br>
<br>
Lenny.<br>
<br>
<br>
On 4/10/09, Geoffroy Pignot &lt;<a href="mailto:geopignot@gmail.com" target="_blank">geopignot@gmail.com</a>&gt; wrote:<br>
<br>
Hi ,<br>
<br>
I am currently testing the process affinity capabilities of<br>
openmpi and I<br>
would like to know if the rankfile behaviour I will describe<br>
below<br>
is normal<br>
or not ?<br>
<br>
cat hostfile.0<br>
r011n002 slots=4<br>
r011n003 slots=4<br>
<br>
cat rankfile.0<br>
rank 0=r011n002 slot=0<br>
rank 1=r011n003 slot=1<br>
<br>
<br>
<br>
<br>
<br>
<br>
##################################################################################<br>
<br>
mpirun --hostfile hostfile.0 -rf rankfile.0 -n 2  hostname ###<br>
OK<br>
r011n002<br>
r011n003<br>
<br>
<br>
<br>
<br>
<br>
<br>
##################################################################################<br>
but<br>
mpirun --hostfile hostfile.0 -rf rankfile.0 -n 1 hostname : -n 1<br>
hostname<br>
### CRASHED<br>
*<br>
<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
Error, invalid rank (1) in the rankfile (rankfile.0)<br>
<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
[r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in<br>
file<br>
rmaps_rank_file.c at line 404<br>
[r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in<br>
file<br>
base/rmaps_base_map_job.c at line 87<br>
[r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in<br>
file<br>
base/plm_base_launch_support.c at line 77<br>
[r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in<br>
file<br>
plm_rsh_module.c at line 985<br>
<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
A daemon (pid unknown) died unexpectedly on signal 1  while<br>
attempting to<br>
launch so we are aborting.<br>
<br>
There may be more information reported by the environment (see<br>
above).<br>
<br>
This may be because the daemon was unable to find all the needed<br>
shared<br>
libraries on the remote node. You may set your LD_LIBRARY_PATH<br>
to<br>
have the<br>
location of the shared libraries on the remote nodes and this<br>
will<br>
automatically be forwarded to the remote nodes.<br>
<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
orterun noticed that the job aborted, but has no info as to the<br>
process<br>
that caused that situation.<br>
<br>
<br>
<br>
<br>
--------------------------------------------------------------------------<br>
orterun: clean termination accomplished<br>
*<br>
It seems that the rankfile option is not propagted to the second<br>
command<br>
line ; there is no global understanding of the ranking inside a<br>
mpirun<br>
command.<br>
<br>
<br>
<br>
<br>
<br>
<br>
##################################################################################<br>
<br>
Assuming that , I tried to provide a rankfile to each command<br>
line:<br>
<br>
cat rankfile.0<br>
rank 0=r011n002 slot=0<br>
<br>
cat rankfile.1<br>
rank 0=r011n003 slot=1<br>
<br>
mpirun --hostfile hostfile.0 -rf rankfile.0 -n 1 hostname : -rf<br>
rankfile.1<br>
-n 1 hostname ### CRASHED<br>
*[r011n002:28778] *** Process received signal ***<br>
[r011n002:28778] Signal: Segmentation fault (11)<br>
[r011n002:28778] Signal code: Address not mapped (1)<br>
[r011n002:28778] Failing at address: 0x34<br>
[r011n002:28778] [ 0] [0xffffe600]<br>
[r011n002:28778] [ 1]<br>
/tmp/HALMPI/openmpi-1.3.1/lib/libopen-rte.so.<br>
0(orte_odls_base_default_get_add_procs_data+0x55d)<br>
[0x5557decd]<br>
[r011n002:28778] [ 2]<br>
/tmp/HALMPI/openmpi-1.3.1/lib/libopen-rte.so.<br>
0(orte_plm_base_launch_apps+0x117)<br>
[0x555842a7]<br>
[r011n002:28778] [ 3] /tmp/HALMPI/openmpi-1.3.1/lib/openmpi/<br>
mca_plm_rsh.so<br>
[0x556098c0]<br>
[r011n002:28778] [ 4] /tmp/HALMPI/openmpi-1.3.1/bin/orterun<br>
[0x804aa27]<br>
[r011n002:28778] [ 5] /tmp/HALMPI/openmpi-1.3.1/bin/orterun<br>
[0x804a022]<br>
[r011n002:28778] [ 6] /lib/libc.so.6(__libc_start_main+0xdc)<br>
[0x9f1dec]<br>
[r011n002:28778] [ 7] /tmp/HALMPI/openmpi-1.3.1/bin/orterun<br>
[0x8049f71]<br>
[r011n002:28778] *** End of error message ***<br>
Segmentation fault (core dumped)*<br>
<br>
<br>
<br>
I hope that I&#39;ve found a bug because it would be very important<br>
for me to<br>
have this kind of capabiliy .<br>
Launch a multiexe mpirun command line and be able to bind my<br>
exes<br>
and<br>
sockets together.<br>
<br>
Thanks in advance for your help<br>
<br>
Geoffroy<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
-------------- next part --------------<br>
HTML attachment scrubbed and removed<br>
<br>
------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
End of users Digest, Vol 1202, Issue 2<br>
**************************************<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
-------------- next part --------------<br>
HTML attachment scrubbed and removed<br>
<br>
------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
End of users Digest, Vol 1218, Issue 2<br>
**************************************<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
-------------- next part --------------<br>
HTML attachment scrubbed and removed<br>
<br>
------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
End of users Digest, Vol 1221, Issue 3<br>
**************************************<br>
<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
-------------- next part --------------<br>
HTML attachment scrubbed and removed<br>
<br>
------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
End of users Digest, Vol 1221, Issue 6<br>
**************************************<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
--<br>
Jeff Squyres<br>
Cisco Systems<br>
<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
-------------- next part --------------<br>
HTML attachment scrubbed and removed<br>
<br>
------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
End of users Digest, Vol 1221, Issue 12<br>
***************************************<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
-- <br>
Jeff Squyres<br>
Cisco Systems<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote>
<br>
<br>
-- <br>
Jeff Squyres<br>
Cisco Systems<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote>
<br>
</div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

