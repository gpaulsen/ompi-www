<html xmlns:v="urn:schemas-microsoft-com:vml" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:w="urn:schemas-microsoft-com:office:word" xmlns:m="http://schemas.microsoft.com/office/2004/12/omml" xmlns="http://www.w3.org/TR/REC-html40">

<head>
<meta http-equiv=Content-Type content="text/html; charset=us-ascii">
<meta name=Generator content="Microsoft Word 12 (filtered medium)">
<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin:0cm;
	margin-bottom:.0001pt;
	font-size:11.0pt;
	font-family:"Calibri","sans-serif";}
a:link, span.MsoHyperlink
	{mso-style-priority:99;
	color:blue;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{mso-style-priority:99;
	color:purple;
	text-decoration:underline;}
span.EmailStyle17
	{mso-style-type:personal-compose;
	font-family:"Calibri","sans-serif";
	color:windowtext;}
.MsoChpDefault
	{mso-style-type:export-only;
	font-size:10.0pt;}
@page Section1
	{size:612.0pt 792.0pt;
	margin:70.85pt 70.85pt 2.0cm 70.85pt;}
div.Section1
	{page:Section1;}
-->
</style>
<!--[if gte mso 9]><xml>
 <o:shapedefaults v:ext="edit" spidmax="1026" />
</xml><![endif]--><!--[if gte mso 9]><xml>
 <o:shapelayout v:ext="edit">
  <o:idmap v:ext="edit" data="1" />
 </o:shapelayout></xml><![endif]-->
</head>

<body lang=DE-CH link=blue vlink=purple>

<div class=Section1>

<p class=MsoNormal style='margin-bottom:12.0pt'><span lang=EN-US>Dear Open MPI
User's and Developers,<br>
<br>
I encountered a problem with Open MPI when porting an application, which
successfully ran with LAM MPI and MPICH.<br>
<br>
The program produces a segmentation fault (see [1] for the stack trace) when doing
the MPI_Send with the following arguments:<br>
<br>
MPI_Send((void *)0, 1, datatype, rank, tag, comm_); <br>
<br>
The first argument seems to be wrong at first sight, but is correct because the
argument &quot;datatype&quot; is an MPI_Datatype, <br>
which describes the memory layout of the object to be sent and is zero-based.
The other arguments are as expected: one such object is sent to rank
&quot;rank&quot; with tag &quot;tag&quot; with the help of the communicator
&quot;comm_&quot;. The MPI_Datatype is constructed programmatically from the
objects member definitions using MPI_Type_struct. The MPI types involved are
solely <br>
MPI_DOUBLE and MPI_UNSIGNED_INT.<br>
<br>
I can reproduce the problem with the stable 1.2 release as well as the 1.2.1a
snapshot of Open MPI.<br>
My OS is Linux with Kernel 2.6.18 (Debian Etch) running on standard Dual Xeon
Hardware with GigE.<br>
<br>
I tried to reduce the amount of data sent by excluding some of the object's
members from the transmission. There does not seem to be a certain member or
type which causes the problem. There seems to be a limit of members/data/size
which determines &nbsp;the success of the call. The &quot;datatype&quot;
structure describes the type and location of approx. 2'000'000 numbers. The
data itself is approx. 16MB (2M * 8 bytes/number assuming doubles), which I
expect not to cause any problem to a MPI implementation.<br>
<br>
Thank you for hints, ideas or suggestions where the problem could be.<br>
<br>
Regards, <br>
Michael<br>
<br>
[1]<br>
<br>
[head:09133] *** Process received signal ***<br>
[head:09133] Signal: Segmentation fault (11)<br>
[head:09133] Signal code: Address not mapped (1)<br>
[head:09133] Failing at address: 0xb0127475<br>
[head:09133] [ 0] [0xb7f0f440]<br>
[head:09133] [ 1] /usr/lib/libmpi.so.0(ompi_convertor_pack+0x90) [0xb668f9a0]<br>
[head:09133] [ 2]
/usr/lib/openmpi/mca_btl_tcp.so(mca_btl_tcp_prepare_src+0x210) [0xb56daef0]<br>
[head:09133] [ 3]
/usr/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_send_request_schedule_exclusive+0x1de)
[0xb5726ede]<br>
[head:09133] [ 4] /usr/lib/openmpi/mca_pml_ob1.so [0xb5728238]<br>
[head:09133] [ 5] /usr/lib/openmpi/mca_btl_tcp.so [0xb56ddc65]<br>
[head:09133] [ 6] /usr/lib/libopen-pal.so.0(opal_event_base_loop+0x462)
[0xb65bcf12]<br>
[head:09133] [ 7] /usr/lib/libopen-pal.so.0(opal_event_loop+0x29) [0xb65bcfd9]<br>
[head:09133] [ 8] /usr/lib/libopen-pal.so.0(opal_progress+0xc0) [0xb65b7260]<br>
[head:09133] [ 9] /usr/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_send+0x3e5)
[0xb571f965]<br>
[head:09133] [10] /usr/lib/libmpi.so.0(MPI_Send+0x12f) [0xb66abf0f]<br>
[head:09133] [11]
/opt/plato/release_1.0/bin/engine(_ZN2GP15MPIProcessGroup4sendERKNS_9MemoryMapEii+0xd9)
[0x81cec03]<br>
[head:09133] [12]
/opt/plato/release_1.0/bin/engine(_ZN2GP15MPIProcessGroup4sendEN5boost10shared_ptrINS_6EntityEEEii+0x2d0)
[0x81d0358]<br>
[head:09133] [13]
/opt/plato/release_1.0/bin/engine(_ZN2GP20ParallelDataAccessor4loadEN5boost10shared_ptrINS_6EntityEEE+0x23b)
[0x853c939]<br>
[head:09133] [14]
/opt/plato/release_1.0/bin/engine(_ZN2GP12Transactions6createEPKN11xercesc_2_77DOMNodeE+0x57f)
[0x8426553]<br>
[head:09133] [15]
/opt/plato/release_1.0/bin/engine(_ZN2GP7FactoryIN5boost10shared_ptrINS_7XmlBaseEEESsPFS4_PKN11xercesc_2_77DOMNodeEENS_19DefaultFactoryErrorEE12createObjectES8_+0x76)
[0x81ca06a]<br>
[head:09133] [16]
/opt/plato/release_1.0/bin/engine(_ZN2GP16XmlFactoryParser7descentEPN11xercesc_2_77DOMNodeEb+0x5b2)
[0x81cd700]<br>
[head:09133] [17] /opt/plato/release_1.0/bin/engine(_ZN2GP9XmlParser8traverseEb+0x278)
[0x81c1eca]<br>
[head:09133] [18]
/opt/plato/release_1.0/bin/engine(_ZN2GP16XmlFactoryParser8traverseEb+0x19)
[0x81c9eeb]<br>
[head:09133] [19] /opt/plato/release_1.0/bin/engine(main+0x1d23) [0x81617f7]<br>
[head:09133] [20] /lib/tls/i686/cmov/libc.so.6(__libc_start_main+0xc8)
[0xb6348ea8]<br>
[head:09133] [21] /opt/plato/release_1.0/bin/engine(__gxx_personality_v0+0x15d)
[0x815a731]<br>
[head:09133] *** End of error message ***<br>
mpirun noticed that job rank 0 with PID 9133 on node head exited on signal 11
(Segmentation fault).<br>
</span>2 additional processes aborted (not shown)<br>
<br>
<br>
<o:p></o:p></p>

</div>

</body>

</html>

