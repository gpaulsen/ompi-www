i am experiencing some issues w/ openmpi 1.2 running on a rocks 4.2.1 cluster(the issues also appear to occur w/ openmpi 1.1.5 and 1.1.4).<br><br>when i run my program with the frontend in the list of nodes, they deadlock.
<br>
<br>when i run my program without the frontend in the list of nodes, they run to completion.<br>
<br>the simplest test program that does this(test1.c) does an &quot;MPI_Init&quot;, followed by an &quot;MPI_Barrier&quot;, and a &quot;MPI_Finalize&quot;.<br><br>so the following deadlocks:<br><br><span style="font-family: courier new,monospace;">
&nbsp;&nbsp;&nbsp; /users/gunter $ mpirun -np 3 -H frontend,compute-0-0,compute-0-1 ./test1</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; host:compute-0-1.local made it past the barrier, ret:0
</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; mpirun: killing job...</span><br style="font-family: courier new,monospace;"><br style="font-family: courier new,monospace;">
<span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; mpirun noticed that job rank 0 with PID 15384 on node frontend exited on signal 15 (Terminated).</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">
&nbsp;&nbsp;&nbsp; 2 additional processes aborted (not shown)</span><br style="font-family: courier new,monospace;"><br>this runs to completion:<br><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">
&nbsp;&nbsp;&nbsp; /users/gunter $ mpirun -np 3 -H compute-0-0,compute-0-1,compute-0-2 ./test1</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; host:compute-0-1.local made it past the barrier, ret:0
</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; host:compute-0-0.local made it past the barrier, ret:0</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">
&nbsp;&nbsp;&nbsp; host:compute-0-2.local made it past the barrier, ret:0</span><br style="font-family: courier new,monospace;"><br>if i have the compute nodes send a message to the frontend prior to the barrier, it runs to completion:<br>
<br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; /users/gunter $ mpirun -np 3 -H frontend,compute-0-0,compute-0-1 ./test2 0</span><br style="font-family: courier new,monospace;">
<span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; host:&nbsp;&nbsp;&nbsp;&nbsp; frontend.domain node:&nbsp; 0 is the master</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; host:&nbsp;&nbsp; compute-0-0.local
 node:&nbsp; 1 sent:&nbsp; 1 to:&nbsp;&nbsp;&nbsp; 0</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; host:&nbsp;&nbsp; compute-0-1.local node:&nbsp; 2 sent:&nbsp; 2 to:&nbsp;&nbsp;&nbsp; 0</span><br style="font-family: courier new,monospace;">
<span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; host:&nbsp;&nbsp;&nbsp;&nbsp; frontend.domain node:&nbsp; 0 recv:&nbsp; 1 from:&nbsp; 1</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; host:&nbsp;&nbsp;&nbsp;&nbsp; 
frontend.domain node:&nbsp; 0 recv:&nbsp; 2 from:&nbsp; 2</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; host:&nbsp;&nbsp;&nbsp;&nbsp; frontend.domain made it past the barrier, ret:0</span><br style="font-family: courier new,monospace;">
<span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; host:&nbsp;&nbsp; compute-0-1.local made it past the barrier, ret:0</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; host:&nbsp;&nbsp; 
compute-0-0.local made it past the barrier, ret:0</span><br style="font-family: courier new,monospace;"><br>if i have a different node function as the master, it deadlocks:<br><br style="font-family: courier new,monospace;">
<span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; /users/gunter $ mpirun -np 3 -H frontend,compute-0-0,compute-0-1 ./test2 1</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">
&nbsp;&nbsp;&nbsp; host:&nbsp;&nbsp; compute-0-0.local node:&nbsp; 1 is the master</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; host:&nbsp;&nbsp; compute-0-1.local node:&nbsp; 2 sent:&nbsp; 2 to:&nbsp;&nbsp;&nbsp; 1</span><br style="font-family: courier new,monospace;">
<span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; mpirun: killing job...</span><br style="font-family: courier new,monospace;"><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">
&nbsp;&nbsp;&nbsp; mpirun noticed that job rank 0 with PID 15411 on node frontend exited on signal 15 (Terminated).</span><br style="font-family: courier new,monospace;"><span style="font-family: courier new,monospace;">&nbsp;&nbsp;&nbsp; 2 additional processes aborted (not shown)
</span><br style="font-family: courier new,monospace;"><br>how is it that in the first example, one node makes it past the barrier, and the rest deadlock?<br><br>these programs both run to completion on two other MPI implementations.
<br><br>is there something mis-configured on my cluster? or is this potentially an openmpi bug?<br><br>what is the best way to debug this?<br><br>any help would be appreciated!<br><br>--tim<br>

