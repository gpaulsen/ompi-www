<? include("../../include/msg-header.inc"); ?>
<!-- received="Tue Jun 20 20:36:30 2006" -->
<!-- isoreceived="20060621003630" -->
<!-- sent="Tue, 20 Jun 2006 20:35:55 -0400" -->
<!-- isosent="20060621003555" -->
<!-- name="Eric Thibodeau" -->
<!-- email="kyron_at_[hidden]" -->
<!-- subject="Re: [OMPI users] Installing OpenMPI on a solaris" -->
<!-- id="200606202035.55294.kyron_at_neuralbs.com" -->
<!-- charset="iso-8859-1" -->
<!-- inreplyto="200606201706.39777.kyron_at_neuralbs.com" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>From:</strong> Eric Thibodeau (<em>kyron_at_[hidden]</em>)<br>
<strong>Date:</strong> 2006-06-20 20:35:55
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1456.php">Jeff Squyres \(jsquyres\): "Re: [OMPI users] MPI_Wtime"</a>
<li><strong>Previous message:</strong> <a href="1454.php">Eric Thibodeau: "Re: [OMPI users] Installing OpenMPI on a solaris"</a>
<li><strong>In reply to:</strong> <a href="1454.php">Eric Thibodeau: "Re: [OMPI users] Installing OpenMPI on a solaris"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1479.php">Jeff Squyres \(jsquyres\): "Re: [OMPI users] Installing OpenMPI on a solaris"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
Hello Brian (and all),
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Well, the joy was short lived. On a 12 CPU Enterprise machine and on a 4 CPU one, I seem to be able to start up to 4 processes. Above 4, I seem to inevitably get BUS_ADRALN (Bus collisions?). Below are some traces of the failling runs as well as a detailed (mpirun -d) of one of these situations and ompi_info output. Obviously, don't hesitate to ask if more information is requred.
<br>
<p>Buid version: openmpi-1.1b5r10421
<br>
Config parameters:
<br>
Open MPI config.status 1.1b5
<br>
configured by ./configure, generated by GNU Autoconf 2.59,
<br>
&nbsp;&nbsp;with options \&quot;'--cache-file=config.cache' 'CFLAGS=-mcpu=v9' 'CXXFLAGS=-mcpu=v9' 'FFLAGS=-mcpu=v9' '--prefix=/export/lca/home/lca0/etudiants/ac38820/openmp
<br>
i_sun4u' --enable-ltdl-convenience\&quot;
<br>
<p>The traces:
<br>
sshd_at_enterprise ~/1_Files/1_ETS/1_Maitrise/MGL810/Devoir2 $ ~/openmpi_sun4u/bin/mpirun -np 10 mandelbrot-mpi 100 400 400
<br>
Signal:10 info.si_errno:0(Error 0) si_code:1(BUS_ADRALN)
<br>
Failing at addr:2f4f04
<br>
*** End of error message ***
<br>
sshd_at_enterprise ~/1_Files/1_ETS/1_Maitrise/MGL810/Devoir2 $ ~/openmpi_sun4u/bin/mpirun -np 8 mandelbrot-mpi 100 400 400
<br>
Signal:10 info.si_errno:0(Error 0) si_code:1(BUS_ADRALN)
<br>
Failing at addr:2b354c
<br>
*** End of error message ***
<br>
sshd_at_enterprise ~/1_Files/1_ETS/1_Maitrise/MGL810/Devoir2 $ ~/openmpi_sun4u/bin/mpirun -np 6 mandelbrot-mpi 100 400 400
<br>
Signal:10 info.si_errno:0(Error 0) si_code:1(BUS_ADRALN)
<br>
Failing at addr:2b1ecc
<br>
*** End of error message ***
<br>
sshd_at_enterprise ~/1_Files/1_ETS/1_Maitrise/MGL810/Devoir2 $ ~/openmpi_sun4u/bin/mpirun -np 5 mandelbrot-mpi 100 400 400
<br>
Signal:10 info.si_errno:0(Error 0) si_code:1(BUS_ADRALN)
<br>
Failing at addr:2b12cc
<br>
*** End of error message ***
<br>
sshd_at_enterprise ~/1_Files/1_ETS/1_Maitrise/MGL810/Devoir2 $ ~/openmpi_sun4u/bin/mpirun -np 4 mandelbrot-mpi 100 400 400
<br>
maxiter = 100, width = 400, height = 400
<br>
execution time in seconds = 1.48
<br>
Taper q pour quitter le programme, autrement, on fait un refresh
<br>
q
<br>
sshd_at_enterprise ~/1_Files/1_ETS/1_Maitrise/MGL810/Devoir2 $ ~/openmpi_sun4u/bin/mpirun -np 5 mandelbrot-mpi 100 400 400
<br>
Signal:10 info.si_errno:0(Error 0) si_code:1(BUS_ADRALN)
<br>
Failing at addr:2b12cc
<br>
*** End of error message ***
<br>
<p>I also got the same behaviour on a different machine (with the exact same code base, $HOME is an NFS mount) and same hardware but limited to 4 CPUs. The following is a debug run of such the failling execution:
<br>
<p>sshd_at_enterprise ~/1_Files/1_ETS/1_Maitrise/MGL810/Devoir2 $ ~/openmpi_sun4u/bin/mpirun -d -v -np 5 mandelbrot-mpi 100 400 400
<br>
[enterprise:24786] [0,0,0] setting up session dir with
<br>
[enterprise:24786]      universe default-universe
<br>
[enterprise:24786]      user sshd
<br>
[enterprise:24786]      host enterprise
<br>
[enterprise:24786]      jobid 0
<br>
[enterprise:24786]      procid 0
<br>
[enterprise:24786] procdir: /tmp/openmpi-sessions-sshd_at_enterprise_0/default-universe/0/0
<br>
[enterprise:24786] jobdir: /tmp/openmpi-sessions-sshd_at_enterprise_0/default-universe/0
<br>
[enterprise:24786] unidir: /tmp/openmpi-sessions-sshd_at_enterprise_0/default-universe
<br>
[enterprise:24786] top: openmpi-sessions-sshd_at_enterprise_0
<br>
[enterprise:24786] tmp: /tmp
<br>
[enterprise:24786] [0,0,0] contact_file /tmp/openmpi-sessions-sshd_at_enterprise_0/default-universe/universe-setup.txt
<br>
[enterprise:24786] [0,0,0] wrote setup file
<br>
[enterprise:24786] pls:rsh: local csh: 0, local bash: 0
<br>
[enterprise:24786] pls:rsh: assuming same remote shell as local shell
<br>
[enterprise:24786] pls:rsh: remote csh: 0, remote bash: 0
<br>
[enterprise:24786] pls:rsh: final template argv:
<br>
[enterprise:24786] pls:rsh:     /usr/local/bin/ssh &lt;template&gt; ( ! [ -e ./.profile ] || . ./.profile; orted --debug --bootproxy 1 --name &lt;template&gt; --num_procs 2 --vpid_start 0 --nodename &lt;template&gt; --universe sshd_at_enterprise:default-universe --nsreplica &quot;0.0.0;tcp://10.45.117.37:40236&quot; --gprreplica &quot;0.0.0;tcp://10.45.117.37:40236&quot; --mpi-call-yield 0 )
<br>
[enterprise:24786] pls:rsh: launching on node localhost
<br>
[enterprise:24786] pls:rsh: oversubscribed -- setting mpi_yield_when_idle to 1 (1 5)
<br>
[enterprise:24786] pls:rsh: localhost is a LOCAL node
<br>
[enterprise:24786] pls:rsh: reset PATH: /export/lca/home/lca0/etudiants/ac38820/openmpi_sun4u/bin:/bin:/usr/local/bin:/usr/bin:/usr/sbin:/usr/ccs/bin:/usr/dt/bin:/usr/local/lam-mpi/7.1.1/bin:/export/lca/appl/Forte/SUNWspro/WS6U2/bin:/opt/sfw/bin:/usr/bin:/usr/ucb:/etc:/usr/local/bin:.
<br>
[enterprise:24786] pls:rsh: reset LD_LIBRARY_PATH: /export/lca/home/lca0/etudiants/ac38820/openmpi_sun4u/lib:/export/lca/appl/Forte/SUNWspro/WS6U2/lib:/usr/local/lib:/usr/local/lam-mpi/7.1.1/lib:/opt/sfw/lib
<br>
[enterprise:24786] pls:rsh: changing to directory /export/lca/home/lca0/etudiants/ac38820
<br>
[enterprise:24786] pls:rsh: executing: orted --debug --bootproxy 1 --name 0.0.1 --num_procs 2 --vpid_start 0 --nodename localhost --universe sshd_at_enterprise:default-universe --nsreplica &quot;0.0.0;tcp://10.45.117.37:40236&quot; --gprreplica &quot;0.0.0;tcp://10.45.117.37:40236&quot; --mpi-call-yield 1
<br>
[enterprise:24787] [0,0,1] setting up session dir with
<br>
[enterprise:24787]      universe default-universe
<br>
[enterprise:24787]      user sshd
<br>
[enterprise:24787]      host localhost
<br>
[enterprise:24787]      jobid 0
<br>
[enterprise:24787]      procid 1
<br>
[enterprise:24787] procdir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe/0/1
<br>
[enterprise:24787] jobdir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe/0
<br>
[enterprise:24787] unidir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe
<br>
[enterprise:24787] top: openmpi-sessions-sshd_at_localhost_0
<br>
[enterprise:24787] tmp: /tmp
<br>
[enterprise:24789] [0,1,0] setting up session dir with
<br>
[enterprise:24789]      universe default-universe
<br>
[enterprise:24789]      user sshd
<br>
[enterprise:24789]      host localhost
<br>
[enterprise:24789]      jobid 1
<br>
[enterprise:24789]      procid 0
<br>
[enterprise:24789] procdir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe/1/0
<br>
[enterprise:24789] jobdir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe/1
<br>
[enterprise:24789] unidir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe
<br>
[enterprise:24789] top: openmpi-sessions-sshd_at_localhost_0
<br>
[enterprise:24789] tmp: /tmp
<br>
[enterprise:24791] [0,1,1] setting up session dir with
<br>
[enterprise:24791]      universe default-universe
<br>
[enterprise:24791]      user sshd
<br>
[enterprise:24791]      host localhost
<br>
[enterprise:24791]      jobid 1
<br>
[enterprise:24791]      procid 1
<br>
[enterprise:24791] procdir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe/1/1
<br>
[enterprise:24791] jobdir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe/1
<br>
[enterprise:24791] unidir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe
<br>
[enterprise:24791] top: openmpi-sessions-sshd_at_localhost_0
<br>
[enterprise:24791] tmp: /tmp
<br>
[enterprise:24793] [0,1,2] setting up session dir with
<br>
[enterprise:24793]      universe default-universe
<br>
[enterprise:24793]      user sshd
<br>
[enterprise:24793]      host localhost
<br>
[enterprise:24793]      jobid 1
<br>
[enterprise:24793]      procid 2
<br>
[enterprise:24793] procdir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe/1/2
<br>
[enterprise:24793] jobdir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe/1
<br>
[enterprise:24793] unidir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe
<br>
[enterprise:24793] top: openmpi-sessions-sshd_at_localhost_0
<br>
[enterprise:24793] tmp: /tmp
<br>
[enterprise:24795] [0,1,3] setting up session dir with
<br>
[enterprise:24795]      universe default-universe
<br>
[enterprise:24795]      user sshd
<br>
[enterprise:24795]      host localhost
<br>
[enterprise:24795]      jobid 1
<br>
[enterprise:24795]      procid 3
<br>
[enterprise:24795] procdir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe/1/3
<br>
[enterprise:24795] jobdir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe/1
<br>
[enterprise:24795] unidir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe
<br>
[enterprise:24795] top: openmpi-sessions-sshd_at_localhost_0
<br>
[enterprise:24795] tmp: /tmp
<br>
[enterprise:24797] [0,1,4] setting up session dir with
<br>
[enterprise:24797]      universe default-universe
<br>
[enterprise:24797]      user sshd
<br>
[enterprise:24797]      host localhost
<br>
[enterprise:24797]      jobid 1
<br>
[enterprise:24797]      procid 4
<br>
[enterprise:24797] procdir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe/1/4
<br>
[enterprise:24797] jobdir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe/1
<br>
[enterprise:24797] unidir: /tmp/openmpi-sessions-sshd_at_localhost_0/default-universe
<br>
[enterprise:24797] top: openmpi-sessions-sshd_at_localhost_0
<br>
[enterprise:24797] tmp: /tmp
<br>
[enterprise:24786] spawn: in job_state_callback(jobid = 1, state = 0x4)
<br>
[enterprise:24786] Info: Setting up debugger process table for applications
<br>
&nbsp;&nbsp;MPIR_being_debugged = 0
<br>
&nbsp;&nbsp;MPIR_debug_gate = 0
<br>
&nbsp;&nbsp;MPIR_debug_state = 1
<br>
&nbsp;&nbsp;MPIR_acquired_pre_main = 0
<br>
&nbsp;&nbsp;MPIR_i_am_starter = 0
<br>
&nbsp;&nbsp;MPIR_proctable_size = 5
<br>
&nbsp;&nbsp;MPIR_proctable:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;(i, host, exe, pid) = (0, localhost, mandelbrot-mpi, 24789)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;(i, host, exe, pid) = (1, localhost, mandelbrot-mpi, 24791)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;(i, host, exe, pid) = (2, localhost, mandelbrot-mpi, 24793)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;(i, host, exe, pid) = (3, localhost, mandelbrot-mpi, 24795)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;(i, host, exe, pid) = (4, localhost, mandelbrot-mpi, 24797)
<br>
[enterprise:24789] [0,1,0] ompi_mpi_init completed
<br>
[enterprise:24791] [0,1,1] ompi_mpi_init completed
<br>
[enterprise:24793] [0,1,2] ompi_mpi_init completed
<br>
[enterprise:24795] [0,1,3] ompi_mpi_init completed
<br>
[enterprise:24797] [0,1,4] ompi_mpi_init completed
<br>
Signal:10 info.si_errno:0(Error 0) si_code:1(BUS_ADRALN)
<br>
Failing at addr:2b12cc
<br>
*** End of error message ***
<br>
[enterprise:24787] sess_dir_finalize: found proc session dir empty - deleting
<br>
[enterprise:24787] sess_dir_finalize: job session dir not empty - leaving
<br>
[enterprise:24787] orted: job_state_callback(jobid = 1, state = ORTE_PROC_STATE_ABORTED)
<br>
[enterprise:24787] sess_dir_finalize: found job session dir empty - deleting
<br>
[enterprise:24787] sess_dir_finalize: univ session dir not empty - leaving
<br>
--------------------------------------------------------------------------
<br>
WARNING: A process refused to die!
<br>
<p>Host: enterprise
<br>
PID:  24789
<br>
<p>This process may still be running and/or consuming resources.
<br>
--------------------------------------------------------------------------
<br>
--------------------------------------------------------------------------
<br>
WARNING: A process refused to die!
<br>
<p>Host: enterprise
<br>
PID:  24791
<br>
<p>This process may still be running and/or consuming resources.
<br>
--------------------------------------------------------------------------
<br>
--------------------------------------------------------------------------
<br>
WARNING: A process refused to die!
<br>
<p>Host: enterprise
<br>
PID:  24793
<br>
<p>This process may still be running and/or consuming resources.
<br>
--------------------------------------------------------------------------
<br>
--------------------------------------------------------------------------
<br>
WARNING: A process refused to die!
<br>
<p>Host: enterprise
<br>
PID:  24795
<br>
<p>This process may still be running and/or consuming resources.
<br>
--------------------------------------------------------------------------
<br>
--------------------------------------------------------------------------
<br>
WARNING: A process refused to die!
<br>
<p>Host: enterprise
<br>
PID:  24797
<br>
<p>This process may still be running and/or consuming resources.
<br>
--------------------------------------------------------------------------
<br>
--------------------------------------------------------------------------
<br>
WARNING: A process refused to die!
<br>
<p>Host: enterprise
<br>
PID:  24789
<br>
<p>This process may still be running and/or consuming resources.
<br>
--------------------------------------------------------------------------
<br>
--------------------------------------------------------------------------
<br>
WARNING: A process refused to die!
<br>
<p>Host: enterprise
<br>
PID:  24791
<br>
<p>This process may still be running and/or consuming resources.
<br>
--------------------------------------------------------------------------
<br>
--------------------------------------------------------------------------
<br>
WARNING: A process refused to die!
<br>
<p>Host: enterprise
<br>
PID:  24793
<br>
<p>This process may still be running and/or consuming resources.
<br>
--------------------------------------------------------------------------
<br>
--------------------------------------------------------------------------
<br>
WARNING: A process refused to die!
<br>
<p>Host: enterprise
<br>
PID:  24795
<br>
<p>This process may still be running and/or consuming resources.
<br>
--------------------------------------------------------------------------
<br>
--------------------------------------------------------------------------
<br>
WARNING: A process refused to die!
<br>
<p>Host: enterprise
<br>
PID:  24797
<br>
<p>This process may still be running and/or consuming resources.
<br>
--------------------------------------------------------------------------
<br>
[enterprise:24787] sess_dir_finalize: proc session dir not empty - leaving
<br>
[enterprise:24787] sess_dir_finalize: proc session dir not empty - leaving
<br>
[enterprise:24787] sess_dir_finalize: proc session dir not empty - leaving
<br>
[enterprise:24787] sess_dir_finalize: proc session dir not empty - leaving
<br>
[enterprise:24787] orted: job_state_callback(jobid = 1, state = ORTE_PROC_STATE_TERMINATED)
<br>
[enterprise:24787] sess_dir_finalize: found proc session dir empty - deleting
<br>
[enterprise:24787] sess_dir_finalize: found job session dir empty - deleting
<br>
[enterprise:24787] sess_dir_finalize: found univ session dir empty - deleting
<br>
[enterprise:24787] sess_dir_finalize: found top session dir empty - deleting
<br>
<p>ompi_info output:
<br>
sshd_at_enterprise ~ $ ~/openmpi_sun4u/bin/ompi_info
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Open MPI: 1.1b5r10421
<br>
&nbsp;&nbsp;&nbsp;Open MPI SVN revision: r10421
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Open RTE: 1.1b5r10421
<br>
&nbsp;&nbsp;&nbsp;Open RTE SVN revision: r10421
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OPAL: 1.1b5r10421
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OPAL SVN revision: r10421
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Prefix: /export/lca/home/lca0/etudiants/ac38820/openmpi_sun4u
<br>
&nbsp;Configured architecture: sparc-sun-solaris2.8
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Configured by: sshd
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Configured on: Tue Jun 20 15:25:44 EDT 2006
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Configure host: averoes
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Built by: ac38820
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Built on: Tue Jun 20 15:59:47 EDT 2006
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Built host: averoes
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C bindings: yes
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C++ bindings: yes
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Fortran77 bindings: yes (all)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Fortran90 bindings: no
<br>
&nbsp;Fortran90 bindings size: na
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C compiler: gcc
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C compiler absolute: /usr/local/bin/gcc
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C++ compiler: g++
<br>
&nbsp;&nbsp;&nbsp;C++ compiler absolute: /usr/local/bin/g++
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Fortran77 compiler: g77
<br>
&nbsp;&nbsp;Fortran77 compiler abs: /usr/local/bin/g77
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Fortran90 compiler: f90
<br>
&nbsp;&nbsp;Fortran90 compiler abs: /export/lca/appl/Forte/SUNWspro/WS6U2/bin/f90
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C profiling: yes
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C++ profiling: yes
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Fortran77 profiling: yes
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Fortran90 profiling: no
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C++ exceptions: no
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Thread support: solaris (mpi: no, progress: no)
<br>
&nbsp;&nbsp;Internal debug support: no
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MPI parameter check: runtime
<br>
Memory profiling support: no
<br>
Memory debugging support: no
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;libltdl support: yes
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA paffinity: solaris (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA maffinity: first_use (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA timer: solaris (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA allocator: basic (MCA v1.0, API v1.0, Component v1.0)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA allocator: bucket (MCA v1.0, API v1.0, Component v1.0)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA coll: basic (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA coll: hierarch (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA coll: self (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA coll: sm (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA coll: tuned (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA io: romio (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA mpool: sm (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA pml: dr (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA pml: ob1 (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA bml: r2 (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA rcache: rb (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA btl: self (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA btl: sm (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA btl: tcp (MCA v1.0, API v1.0, Component v1.0)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA topo: unity (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA osc: pt2pt (MCA v1.0, API v1.0, Component v1.0)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA gpr: null (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA gpr: proxy (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA gpr: replica (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA iof: proxy (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA iof: svc (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA ns: proxy (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA ns: replica (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA oob: tcp (MCA v1.0, API v1.0, Component v1.0)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA ras: dash_host (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA ras: hostfile (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA ras: localhost (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA rds: hostfile (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA rds: resfile (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA rmaps: round_robin (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA rmgr: proxy (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA rmgr: urm (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA rml: oob (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA pls: fork (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA pls: rsh (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA sds: env (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA sds: pipe (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA sds: seed (MCA v1.0, API v1.0, Component v1.1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MCA sds: singleton (MCA v1.0, API v1.0, Component v1.1)
<br>
<p>Le mardi 20 juin 2006 17:06, Eric Thibodeau a &#233;crit&#160;:
<br>
<span class="quotelev1">&gt; Thanks for the pointer, it WORKS!! (yay)
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; Le mardi 20 juin 2006 12:21, Brian Barrett a &#233;crit&#160;:
</span><br>
<span class="quotelev2">&gt; &gt; On Jun 19, 2006, at 12:15 PM, Eric Thibodeau wrote:
</span><br>
<span class="quotelev2">&gt; &gt; 
</span><br>
<span class="quotelev3">&gt; &gt; &gt; I checked the thread with the same title as this e-mail and tried  
</span><br>
<span class="quotelev3">&gt; &gt; &gt; compiling openmpi-1.1b4r10418 with:
</span><br>
<span class="quotelev3">&gt; &gt; &gt;
</span><br>
<span class="quotelev3">&gt; &gt; &gt; ./configure CFLAGS=&quot;-mv8plus&quot; CXXFLAGS=&quot;-mv8plus&quot; FFLAGS=&quot;-mv8plus&quot;  
</span><br>
<span class="quotelev3">&gt; &gt; &gt; FCFLAGS=&quot;-mv8plus&quot; --prefix=$HOME/openmpi-SUN-`uname -r` --enable- 
</span><br>
<span class="quotelev3">&gt; &gt; &gt; pretty-print-stacktrace
</span><br>
<span class="quotelev2">&gt; &gt; I put the incorrect flags in the error message - can you try again with:
</span><br>
<span class="quotelev2">&gt; &gt; 
</span><br>
<span class="quotelev2">&gt; &gt; 
</span><br>
<span class="quotelev2">&gt; &gt;    ./configure CFLAGS=-mcpu=v9 CXXFLAGS=-mcpu=v9 FFLAGS=-mcpu=v9  
</span><br>
<span class="quotelev2">&gt; &gt; FCFLAGS=-mcpu=v9 --prefix=$HOME/openmpi-SUN-`uname -r` --enable- 
</span><br>
<span class="quotelev2">&gt; &gt; pretty-print-stacktrace
</span><br>
<span class="quotelev2">&gt; &gt; 
</span><br>
<span class="quotelev2">&gt; &gt; 
</span><br>
<span class="quotelev2">&gt; &gt; and see if that helps?  By the way, I'm not sure if Solaris has the  
</span><br>
<span class="quotelev2">&gt; &gt; required support for the pretty-print stack trace feature.  It likely  
</span><br>
<span class="quotelev2">&gt; &gt; will print what signal caused the error, but will not actually print  
</span><br>
<span class="quotelev2">&gt; &gt; the stack trace.  It's enabled by default on Solaris, with this  
</span><br>
<span class="quotelev2">&gt; &gt; limited functionality (the option exists for platforms that have  
</span><br>
<span class="quotelev2">&gt; &gt; broken half-support for GNU libc's stack trace feature, and for users  
</span><br>
<span class="quotelev2">&gt; &gt; that don't like us registering a signal handler to do the work).
</span><br>
<span class="quotelev2">&gt; &gt; 
</span><br>
<span class="quotelev2">&gt; &gt; Brian
</span><br>
<span class="quotelev2">&gt; &gt; 
</span><br>
<span class="quotelev2">&gt; &gt; 
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<p><pre>
-- 
Eric Thibodeau
Neural Bucket Solutions Inc.
T. (514) 736-1436
C. (514) 710-0517
</pre>
<hr>
<ul>
<li>text/html attachment: <a href="http://www.open-mpi.org/community/lists/users/att-1455/attachment">attachment</a>
</ul>
<!-- attachment="attachment" -->
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1456.php">Jeff Squyres \(jsquyres\): "Re: [OMPI users] MPI_Wtime"</a>
<li><strong>Previous message:</strong> <a href="1454.php">Eric Thibodeau: "Re: [OMPI users] Installing OpenMPI on a solaris"</a>
<li><strong>In reply to:</strong> <a href="1454.php">Eric Thibodeau: "Re: [OMPI users] Installing OpenMPI on a solaris"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1479.php">Jeff Squyres \(jsquyres\): "Re: [OMPI users] Installing OpenMPI on a solaris"</a>
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
