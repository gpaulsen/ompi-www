<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><br><div><div>On Aug 12, 2009, at &nbsp;12:31 PM, Ralph Castain wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite">Well, it is getting better! :-)<br><br>On your cmd line, what btl's are you specifying? You should try -mca btl sm,tcp,self for this to work. Reason: sometimes systems block tcp loopback on the node. What I see below indicates that inter-node comm was fine, but the two procs that share a node couldn't communicate. Including shared memory should remove that problem.<br></blockquote><div><br></div><div>It looks like sm,tcp,self are being initialized automatically - this repeats for each node:</div><div><br></div><div><div>[xserve03.local:01008] mca: base: components_open: Looking for btl components</div><div>[xserve03.local:01008] mca: base: components_open: opening btl components</div><div>[xserve03.local:01008] mca: base: components_open: found loaded component self</div><div>[xserve03.local:01008] mca: base: components_open: component self has no register function</div><div>[xserve03.local:01008] mca: base: components_open: component self open function successful</div><div>[xserve03.local:01008] mca: base: components_open: found loaded component sm</div><div>[xserve03.local:01008] mca: base: components_open: component sm has no register function</div><div>[xserve03.local:01008] mca: base: components_open: component sm open function successful</div><div>[xserve03.local:01008] mca: base: components_open: found loaded component tcp</div><div>[xserve03.local:01008] mca: base: components_open: component tcp has no register function</div><div>[xserve03.local:01008] mca: base: components_open: component tcp open function successful</div><div>[xserve03.local:01008] select: initializing btl component self</div><div>[xserve03.local:01008] select: init of component self returned success</div><div>[xserve03.local:01008] select: initializing btl component sm</div><div>[xserve03.local:01008] select: init of component sm returned success</div><div>[xserve03.local:01008] select: initializing btl component tcp</div><div>[xserve03.local:01008] select: init of component tcp returned success</div><div><br></div></div><div><div>I should have reminded you of the command line:</div><div><br></div><div><div>usr/local/openmpi/bin/mpirun &nbsp;-n 3 -mca btl_base_verbose 30 -mca btl_tcp_if_include en0 --bynode -host xserve02,xserve03 connectivity_c &gt;&amp; connectivity_c3_2host.txt</div><div><br></div><div>So I think ranks 0 and 2 are on xserve02 and rank 1 is on xserve01, in which case I still think it is tcp communication...</div><div><br></div><div><br></div><div><div>Done MPI init</div><div>checking connection between rank 0 on xserve02.local and rank 1 &nbsp;&nbsp;</div><div>Done MPI init</div><div>[xserve02.local:01382] btl: tcp: attempting to connect() to address 192.168.2.103 on port 4</div><div>Done MPI init</div><div>checking connection between rank 1 on xserve03.local and rank 2 &nbsp;&nbsp;</div><div>[xserve03.local:01008] btl: tcp: attempting to connect() to address 192.168.2.102 on port 4</div><div>Done checking connection between rank 0 on xserve02.local and rank 1 &nbsp;&nbsp;</div><div>checking connection between rank 0 on xserve02.local and rank 2 &nbsp;&nbsp;</div><div>Done checking connection between rank 0 on xserve02.local and rank 2 &nbsp;&nbsp;</div><div>mpirun: killing job...</div><div><br></div><div>--------------------------------------------------------------------------</div><div>mpirun noticed that process rank 1 with PID 1008 on node xserve03 exited on signal 0 (Signal 0).</div><div>--------------------------------------------------------------------------</div><div>mpirun: clean termination accomplished</div><div><br></div></div></div><div><br></div><div>Thanks, &nbsp;Jody</div><div><br></div></div><br><blockquote type="cite"> <br>The port numbers are fine and can be different or the same - it is totally random. The procs exchange their respective port info during wireup.<br><br><br><div class="gmail_quote">On Wed, Aug 12, 2009 at 12:51 PM, Jody Klymak <span dir="ltr">&lt;<a href="mailto:jklymak@uvic.ca">jklymak@uvic.ca</a>&gt;</span> wrote:<br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div style="">Hi Ralph,<div><br></div><div>That gives me something more to work with...</div> <div><br></div><div><br><div><div class="im"><div>On Aug 12, 2009, at &nbsp;9:44 AM, Ralph Castain wrote:</div><br><blockquote type="cite">I believe TCP works fine, Jody, as it is used on Macs fairly widely. I suspect this is something funny about your installation.<br> <br>One thing I have found is that you can get this error message when you have multiple NICs installed, each with a different subnet, and the procs try to connect across different ones. Do you by chance have multiple NICs?<br> </blockquote><div><br></div></div><div>The head node has two active NICs:</div><div>en0: public</div><div>en1: private</div><div><br></div><div>The server nodes only have one connection&nbsp;</div><div>en0:private</div><div class="im"> <br><blockquote type="cite"> <br>Have you tried telling OMPI which TCP interface to use? You can do so with -mca btl_tcp_if_include eth0 (or whatever you want to use).<br></blockquote><div><br></div></div><div>If I try this, I get the same results. (though I need to use "en0" on my machine)...</div> <div><br></div><div><div>If I include -mca btl_base_verbose 30 I get for n=2:</div><div><br></div><div><div>++++++++++</div><div></div><div>[xserve03.local:00841] select: init of component tcp returned success</div><div>Done MPI init</div> <div>checking connection between rank 0 on xserve02.local and rank 1 &nbsp;&nbsp;</div><div>Done MPI init</div><div>[xserve02.local:01094] btl: tcp: attempting to connect() to address 192.168.2.103 on port 4</div><div>Done checking connection between rank 0 on xserve02.local and rank 1 &nbsp;&nbsp;</div> <div>Connectivity test on 2 processes PASSED.</div><div><div>++++++++++</div><div></div></div><div><br></div><div>If I try n=3 the job hangs and I have to kill:</div><div><br></div><div><div>++++++++++</div><div></div><div> Done MPI init</div><div>checking connection between rank 0 on xserve02.local and rank 1 &nbsp;&nbsp;</div><div>[xserve02.local:01110] btl: tcp: attempting to connect() to address 192.168.2.103 on port 4</div><div>Done MPI init</div> <div>Done MPI init</div><div>checking connection between rank 1 on xserve03.local and rank 2 &nbsp;&nbsp;</div><div>[xserve03.local:00860] btl: tcp: attempting to connect() to address 192.168.2.102 on port 4</div><div>Done checking connection between rank 0 on xserve02.local and rank 1 &nbsp;&nbsp;</div> <div>checking connection between rank 0 on xserve02.local and rank 2 &nbsp;&nbsp;</div><div>Done checking connection between rank 0 on xserve02.local and rank 2 &nbsp;&nbsp;</div><div>mpirun: killing job...</div><div><div>++++++++++</div><div> </div></div></div></div><div><br></div><div>Those ip addresses are correct, no idea if port 4 make sense. &nbsp;Sometimes I get port 260. &nbsp;Should xserve03 and xserve02 be trying to use the same port for these comms?&nbsp;</div><div> <br></div><div><br></div><div>Thanks, &nbsp;Jody</div><div><br></div><div><br></div></div><div><div></div><div class="h5"><br><blockquote type="cite"><br><br><div class="gmail_quote">On Wed, Aug 12, 2009 at 10:01 AM, Jody Klymak <span dir="ltr">&lt;<a href="mailto:jklymak@uvic.ca" target="_blank">jklymak@uvic.ca</a>&gt;</span> wrote:<br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><br> On Aug 11, 2009, at &nbsp;18:55 PM, Gus Correa wrote:<br> <br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> <br> Did you wipe off the old directories before reinstalling?<br> </blockquote> <br> Check.<br> <br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> I prefer to install on a NFS mounted directory,<br> </blockquote> <br> Check<br> <br> <br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> Have you tried to ssh from node to node on all possible pairs?<br> </blockquote> <br> check - fixed this today, works fine with the spawning user...<br> <br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> How could you roll back to 1.1.5,<br> now that you overwrote the directories?<br> </blockquote> <br> Oh, I still have it on another machine off the cluster in /usr/local/openmpi. &nbsp;Will take just 5 mintues to reinstall.<br> <br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> Launching jobs with Torque is way much better than<br> using barebones mpirun.<br> </blockquote> <br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> And you don't want to stay behind with the OpenMPI versions<br> and improvements either.<br> </blockquote> <br> Sure, but I'd like the jobs to be able to run at all..<br> <br> Is there any sense in rolling back to to 1.2.3 since that is known to work with OS X (its the one that comes with 10.5)? &nbsp;My only guess at this point is other OS X users are using non-tcpip communication, and the tcp stuff just doesn't work in 1.3.3.<br> <br> Thanks, &nbsp;Jody<br> <br> --<br> Jody Klymak<br> <a href="http://web.uvic.ca/%7Ejklymak/" target="_blank">http://web.uvic.ca/~jklymak/</a><br> <br> <br> <br> <br> _______________________________________________<br> users mailing list<br> <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> </blockquote> </div><br> _______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote> </div></div></div><div><div></div><div class="h5"><br><div> <span style="font-size: 12px;"><div style=""><span style="border-collapse: separate; border-spacing: 0px; color: rgb(0, 0, 0); font-family: Lucida Sans Typewriter; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;"><div> --</div><div>Jody Klymak &nbsp; &nbsp;</div><div><a href="http://web.uvic.ca/%7Ejklymak/" target="_blank">http://web.uvic.ca/~jklymak/</a></div><div><br></div><div><br></div><br></span></div></span> </div><br></div></div></div></div> <br>_______________________________________________<br> users mailing list<br> <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br> _______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br><div apple-content-edited="true"> <span class="Apple-style-span" style="font-size: 12px; "><div style="word-wrap: break-word; -khtml-nbsp-mode: space; -khtml-line-break: after-white-space; "><span class="Apple-style-span" style="border-collapse: separate; border-spacing: 0px 0px; color: rgb(0, 0, 0); font-family: Lucida Sans Typewriter; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-align: auto; -khtml-text-decorations-in-effect: none; text-indent: 0px; -apple-text-size-adjust: auto; text-transform: none; orphans: 2; white-space: normal; widows: 2; word-spacing: 0px; "><div>--</div><div>Jody Klymak &nbsp; &nbsp;</div><div><a href="http://web.uvic.ca/~jklymak/">http://web.uvic.ca/~jklymak/</a></div><div><br class="khtml-block-placeholder"></div><div><br class="khtml-block-placeholder"></div><br class="Apple-interchange-newline"></span></div></span> </div><br></body></html>
