<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">Dear Ralph,<div><br>On May 27, 2014, at 6:31 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br><blockquote type="cite">So out of curiosity - how was this job launched? Via mpirun or directly using srun?<br></blockquote><div><br></div><div><br></div>The job has been submitted using mpirun. However Open MPI is compiled with SLURM support (and I start to believe this is might not ideal after all !!!). I have a partial job trace dumped by the process when it died:</div><div><br></div><div><font face="Andale Mono">--------------------------------------------------------------------------<br>mpirun noticed that process rank 8190 with PID 29319 on node sand-8-39 exited on signal 11 (Segmentation fault).<br>--------------------------------------------------------------------------<br></font></div><div><font face="Andale Mono"><br></font></div><div><font face="Andale Mono">forrtl: error (78): process killed (SIGTERM)<br>Image &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;PC &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Routine &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Line &nbsp; &nbsp; &nbsp; &nbsp;Source&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br>diag_OMPI-INTEL.x &nbsp;0000000000537349 &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>diag_OMPI-INTEL.x &nbsp;0000000000535C1E &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>diag_OMPI-INTEL.x &nbsp;000000000050CF52 &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>diag_OMPI-INTEL.x &nbsp;00000000004F0BB3 &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>diag_OMPI-INTEL.x &nbsp;00000000004BEB99 &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>libpthread.so.0 &nbsp; &nbsp;00007FE5B5BE5710 &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>libmlx4-rdmav2.so &nbsp;00007FE5A8C0A867 &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>mca_btl_openib.so &nbsp;00007FE5ADA36644 &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>libopen-pal.so.6 &nbsp; 00007FE5B288262A &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>mca_pml_ob1.so &nbsp; &nbsp; 00007FE5AC344FAF &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>libmpi.so.1 &nbsp; &nbsp; &nbsp; &nbsp;00007FE5B5064E7D &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>libmpi_mpifh.so.2 &nbsp;00007FE5B531919B &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>libelpa.so.0 &nbsp; &nbsp; &nbsp; 00007FE5B82EC0CE &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>libelpa.so.0 &nbsp; &nbsp; &nbsp; 00007FE5B82EBE36 &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>libelpa.so.0 &nbsp; &nbsp; &nbsp; 00007FE5B82EBDFD &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>libelpa.so.0 &nbsp; &nbsp; &nbsp; 00007FE5B82EC2CD &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>libelpa.so.0 &nbsp; &nbsp; &nbsp; 00007FE5B82EB798 &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>libelpa.so.0 &nbsp; &nbsp; &nbsp; 00007FE5B82E571A &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>diag_OMPI-INTEL.x &nbsp;00000000004101C2 &nbsp;MAIN__ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;562 &nbsp;dirac_exomol_eigen.f90<br>diag_OMPI-INTEL.x &nbsp;000000000040A1A6 &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>libc.so.6 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;00007FE5B4A89D1D &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown<br>diag_OMPI-INTEL.x &nbsp;000000000040A099 &nbsp;Unknown &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Unknown &nbsp;Unknown</font><br><br></div><div>(plus many other trace information like this)</div><div><br></div><div>No more information that this unfortunately because not everything library has been built using debug flags. The computation is all concentrated in ScaLAPACK and ELPA that I recompiled by myself, I run over 8192 MPI and the memory allocated per MPI process was below 1 GByte (per MPI). My compute nodes have 64 GByte of RAM and 2 eight-core Intel Sandy Bridge. Since 512 nodes are 80% of the cluster I have available for this test, I cannot easily reschedule a repetition of the test.</div><div><br></div><div>I wonder if this message that can be related to libevent may in principle cause this seg fault error. I am working to understand the cause on my side but so far a reduced problem size using less nodes never failed.</div><div><br></div><div>Any help is much appreciated!</div><div><br></div><div>Regards,</div><div>F</div><div><br></div><div><div>--<br>Mr. Filippo SPIGA, M.Sc.<br><a href="http://www.linkedin.com/in/filippospiga">http://www.linkedin.com/in/filippospiga</a>&nbsp;~ skype: filippo.spiga<br><br>«Nobody will drive us out of Cantor's paradise.» ~ David Hilbert<br><br>*****<br>Disclaimer: "Please note this message and any attachments are CONFIDENTIAL and may be privileged or otherwise protected from disclosure. The contents are not to be disclosed to anyone other than the addressee. Unauthorized recipients are requested to&nbsp;preserve this confidentiality and to advise the sender immediately of any error in transmission."<br><br></div><br></div></body></html>
