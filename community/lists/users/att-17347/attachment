<br><div class="gmail_quote">Hi all - and sorry for the multiple postings, but I have more information.<br><br>1: After a reboot of two nodes I ran again, and the inter-node freeze didn&#39;t happen until the third iteration. I take that to mean that the basic communication works, but that something is saturating. Is there some notion of buffer size somewhere in the MPI system that could explain this?<br>

2: The nodes have 4 ethernet cards each. Could the mapping be a problem?<br>3: The cpus are running at a 100% for all processes involved in the freeze<br>4: The same test program (<a href="http://code.google.com/p/pypar/source/browse/source/mpi_test.c" target="_blank">http://code.google.com/p/pypar/source/browse/source/mpi_test.c</a>) works fine when run within one node so the problem must be with MPI and/or our network. <br>
5: The network and ssh works otherwise fine.<br>
<br><br>Again many thanks for any hint that can get us going again. The main thing we need is some diagnostics that may point to what causes this problem for MPI.<br>Cheers<br>Ole Nielsen<br><br>------<br><br>Here&#39;s the output which shows the freeze in the third iteration:<br>
<br>nielso@alamba:~/sandpit/pypar/source$ mpirun --hostfile /etc/mpihosts --host node5,node6 --npernode 2 a.out <br>
Number of processes = 4<br>Test repeated 3 times for reliability<br>I am process 2 on node node6<br>P2: Waiting to receive from to P1<br>P2: Sending to to P3<br>I am process 3 on node node6<br>P3: Waiting to receive from to P2<br>

I am process 1 on node node5<br>P1: Waiting to receive from to P0<br>P1: Sending to to P2<br>P1: Waiting to receive from to P0<br>I am process 0 on node node5<br>Run 1 of 3<br>P0: Sending to P1<br>P0: Waiting to receive from P3<br>

P2: Waiting to receive from to P1<br>P3: Sending to to P0<br>P3: Waiting to receive from to P2<br>P1: Sending to to P2<br>P0: Received from to P3<br>Run 2 of 3<br>P0: Sending to P1<br>P0: Waiting to receive from P3<br>P1: Waiting to receive from to P0<br>

<br>
</div><br>

