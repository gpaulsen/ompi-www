<html><head><meta http-equiv="Content-Type" content="text/html charset=us-ascii"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">Yeah, that's a bit of a problem. The issue is that we tie a specific build to its prefix, which means we expect to find the OMPI binaries in that location. So you are supposed to set your path etc to point to the actual installation, not an abstraction.<div class=""><br class=""></div><div class="">There are some ways to work around it (e.g., to handle relocation) using OPAL_PREFIX in your environment, but I honestly don't recall the details of how to make that work as it isn't something we generally recommend.</div><div class=""><br class=""></div><div class=""><br class=""></div><div class=""><div><blockquote type="cite" class=""><div class=""><br class=""></div><div class="">On Feb 6, 2015, at 12:12 PM, Evan Samanas &lt;<a href="mailto:evan.samanas@gmail.com" class="">evan.samanas@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><div dir="ltr" class=""><div class=""><div class="">Hi Ralph,<br class=""><br class=""></div><div class="">Thanks for addressing this issue.<br class=""></div><div class=""><br class=""></div>I tried downloading your fork from that pull request and the seg fault appears to be gone.&nbsp; However I didn't install it on my remote machine before testing, and I got this error: <br class=""><br class="">bash: /opt/ompi-release-cmr-singlespawn/bin/orted: No such file or directory (along with the usual complaints about ORTE not being able to start one of the daemons).<br class=""><br class=""></div>On both machines I have openmpi installed to a directory in /opt, and /opt/openmpi is a symlink to whatever installation I want to use...then my paths point to the symlink.&nbsp; I went to the remote machine and simply changed the name of the 
directory to match the other one and I just got a version mismatch 
error...a much more expected error. I'm not familiar with OMPI source, but does this have to do with the prefix issue you mentioned in the pull request? Should it handle symlinks?&nbsp; Apologies if I'm misguided.<br class=""><br class="">Evan<br class=""></div><div class="gmail_extra"><br class=""><div class="gmail_quote">On Thu, Feb 5, 2015 at 9:51 AM, Ralph Castain <span dir="ltr" class="">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank" class="">rhc@open-mpi.org</a>&gt;</span> wrote:<br class=""><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word" class="">Okay, I tracked this down - thanks for your patience! I have a fix pending review. You can track it here:<div class=""><br class=""></div><div class=""><a href="https://github.com/open-mpi/ompi-release/pull/179" target="_blank" class="">https://github.com/open-mpi/ompi-release/pull/179</a></div><div class=""><br class=""></div><div class=""><br class=""><div class=""><blockquote type="cite" class=""><div class=""><div class="h5"><div class="">On Feb 4, 2015, at 5:14 PM, Evan Samanas &lt;<a href="mailto:evan.samanas@gmail.com" target="_blank" class="">evan.samanas@gmail.com</a>&gt; wrote:</div><br class=""></div></div><div class=""><div class=""><div class="h5"><div dir="ltr" style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" class=""><div class=""><div class=""><div class=""><div class=""><div class=""><div class="">Indeed, I simply commented out all the MPI_Info stuff, which you essentially did by passing a dummy argument.&nbsp; I'm still not able to get it to succeed.<br class=""><br class=""></div>So here we go, my results defy logic.&nbsp; I'm sure this could be my fault...I've only been an occasional user of OpenMPI and MPI in general over the years and I've never used MPI_Comm_spawn before this project. I tested simple_spawn like so:<br class=""></div>mpicc simple_spawn.c -o simple_spawn<br class=""></div>./simple_spawn<br class=""><br class=""></div>When my default hostfile points to a file that just lists localhost, this test completes successfully.&nbsp; If it points to my hostfile with localhost and 5 remote hosts, here's the output:<br class="">evan@lasarti:~/devel/toy_progs/mpi_spawn$ mpicc simple_spawn.c -o simple_spawn<br class="">evan@lasarti:~/devel/toy_progs/mpi_spawn$ ./simple_spawn<br class="">[pid 5703] starting up!<br class="">0 completed MPI_Init<br class="">Parent [pid 5703] about to spawn!<br class="">[lasarti:05703] [[14661,1],0] FORKING HNP: orted --hnp --set-sid --report-uri 14 --singleton-died-pipe 15 -mca state_novm_select 1 -mca ess_base_jobid 960823296<br class="">[lasarti:05705] *** Process received signal ***<br class="">[lasarti:05705] Signal: Segmentation fault (11)<br class="">[lasarti:05705] Signal code: Address not mapped (1)<br class="">[lasarti:05705] Failing at address: (nil)<br class="">[lasarti:05705] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x10340)[0x7fc185dcf340]<br class="">[lasarti:05705] [ 1] /opt/openmpi-v1.8.4-54-g07f735a/lib/libopen-rte.so.7(orte_rmaps_base_compute_bindings+0x650)[0x7fc186033bb0]<br class="">[lasarti:05705] [ 2] /opt/openmpi-v1.8.4-54-g07f735a/lib/libopen-rte.so.7(orte_rmaps_base_map_job+0x939)[0x7fc18602fb99]<br class="">[lasarti:05705] [ 3] /opt/openmpi-v1.8.4-54-g07f735a/lib/libopen-pal.so.6(opal_libevent2021_event_base_loop+0x6e4)[0x7fc18577dcc4]<br class="">[lasarti:05705] [ 4] /opt/openmpi-v1.8.4-54-g07f735a/lib/libopen-rte.so.7(orte_daemon+0xdf8)[0x7fc186010438]<br class="">[lasarti:05705] [ 5] orted(main+0x47)[0x400887]<br class="">[lasarti:05705] [ 6] /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf5)[0x7fc185a1aec5]<br class="">[lasarti:05705] [ 7] orted[0x4008db]<br class="">[lasarti:05705] *** End of error message ***<br class=""><br class=""></div>You can see from the message that this particular run IS from the latest snapshot, though the failure happens on v.1.8.4 as well.&nbsp; I didn't bother installing the snapshot on the remote nodes though.&nbsp; Should I do that?&nbsp; It looked to me like this error happened well before we got to a remote node, so that's why I didn't.<br class=""></div><div class=""><br class=""></div><div class="">Your thoughts?<br class=""></div><div class=""><br class=""></div><div class="">Evan<br class=""></div><div class=""><div class=""><br class=""><br class=""></div></div></div><div class="gmail_extra" style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px"><br class=""><div class="gmail_quote">On Tue, Feb 3, 2015 at 7:40 PM, Ralph Castain<span class="">&nbsp;</span><span dir="ltr" class="">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank" class="">rhc@open-mpi.org</a>&gt;</span><span class="">&nbsp;</span>wrote:<br class=""><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div dir="ltr" class="">I confess I am sorely puzzled. I replace the Info key with MPI_INFO_NULL, but still had to pass a bogus argument to master since you still have the Info_set code in there - otherwise, info_set segfaults due to a NULL argv[1]. Doing that (and replacing "hostname" with an MPI example code) makes everything work just fine.<div class=""><br class=""></div><div class="">I've attached one of our example comm_spawn codes that we test against - it also works fine with the current head of the 1.8 code base. I confess that some changes have been made since 1.8.4 was released, and it is entirely possible that this was a problem in 1.8.4 and has since been fixed.</div><div class=""><br class=""></div><div class="">So I'd suggest trying with the nightly 1.8 tarball and seeing if it works for you. You can download it from here:</div><div class=""><br class=""></div><div class=""><a href="http://www.open-mpi.org/nightly/v1.8/" target="_blank" class="">http://www.open-mpi.org/nightly/v1.8/</a><br class=""></div><div class=""><br class=""></div><div class="">HTH</div><div class="">Ralph</div><div class=""><br class=""></div></div><div class="gmail_extra"><br class=""><div class="gmail_quote"><div class=""><div class="">On Tue, Feb 3, 2015 at 6:20 PM, Evan Samanas<span class="">&nbsp;</span><span dir="ltr" class="">&lt;<a href="mailto:evan.samanas@gmail.com" target="_blank" class="">evan.samanas@gmail.com</a>&gt;</span><span class="">&nbsp;</span>wrote:<br class=""></div></div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div class=""><div class=""><div dir="ltr" class="">Yes, I did.&nbsp; I replaced the info argument of MPI_Comm_spawn with MPI_INFO_NULL.<br class=""></div><div class="gmail_extra"><br class=""><div class="gmail_quote"><div class=""><div class="">On Tue, Feb 3, 2015 at 5:54 PM, Ralph Castain<span class="">&nbsp;</span><span dir="ltr" class="">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank" class="">rhc@open-mpi.org</a>&gt;</span><span class="">&nbsp;</span>wrote:<br class=""></div></div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div class=""><div class=""><div dir="ltr" class="">When running your comm_spawn code, did you remove the Info key code? You wouldn't need to provide a hostfile or hosts any more, which is why it should resolve that problem.<div class=""><br class=""></div><div class="">I agree that providing either hostfile or host as an Info key will cause the program to segfault - I'm woking on that issue.</div><div class=""><br class=""></div></div><div class="gmail_extra"><br class=""><div class="gmail_quote"><div class=""><div class="">On Tue, Feb 3, 2015 at 3:46 PM, Evan Samanas<span class="">&nbsp;</span><span dir="ltr" class="">&lt;<a href="mailto:evan.samanas@gmail.com" target="_blank" class="">evan.samanas@gmail.com</a>&gt;</span><span class="">&nbsp;</span>wrote:<br class=""></div></div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div class=""><div class=""><div dir="ltr" class=""><div class="">Setting these environment variables did indeed change the way mpirun maps things, and I didn't have to specify a hostfile.&nbsp; However, setting these for my MPI_Comm_spawn code still resulted in the same segmentation fault.<br class=""><br class=""></div>Evan<br class=""></div><div class="gmail_extra"><br class=""><div class="gmail_quote"><div class=""><div class="">On Tue, Feb 3, 2015 at 10:09 AM, Ralph Castain<span class="">&nbsp;</span><span dir="ltr" class="">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank" class="">rhc@open-mpi.org</a>&gt;</span><span class="">&nbsp;</span>wrote:<br class=""></div></div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div class=""><div class=""><div dir="ltr" class="">If you add the following to your environment, you should run on multiple nodes:<div class=""><br class=""></div><div class="">OMPI_MCA_<span style="font-family:Menlo;font-size:11px" class="">rmaps_base_mapping_policy=node</span></div><div class=""><span style="font-family:Menlo;font-size:11px" class="">OMPI_MCA_</span><span style="font-family:Menlo;font-size:11px" class="">orte_default_hostfile=&lt;your hostfile&gt;</span></div><div class=""><span style="font-family:Menlo;font-size:11px" class=""><br class=""></span></div><div class=""><span style="font-family:Menlo;font-size:11px" class="">The first tells OMPI to map-by node. The second passes in your default hostfile so you don't need to specify it as an Info key.</span></div><div class=""><span style="font-family:Menlo;font-size:11px" class=""><br class=""></span></div><div class=""><span style="font-family:Menlo;font-size:11px" class="">HTH</span></div><div class=""><span style="font-family:Menlo;font-size:11px" class="">Ralph</span></div><div class=""><span style="font-family:Menlo;font-size:11px" class=""><br class=""></span></div></div><div class="gmail_extra"><br class=""><div class="gmail_quote"><div class=""><div class="">On Tue, Feb 3, 2015 at 9:23 AM, Evan Samanas<span class="">&nbsp;</span><span dir="ltr" class="">&lt;<a href="mailto:evan.samanas@gmail.com" target="_blank" class="">evan.samanas@gmail.com</a>&gt;</span><span class="">&nbsp;</span>wrote:<br class=""></div></div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div class=""><div class=""><div dir="ltr" class=""><div class=""><div class=""><div class="">Hi Ralph,<br class=""><br class=""></div>Good to know you've reproduced it.&nbsp; I was experiencing this using both the hostfile and host key.&nbsp; A simple comm_spawn was working for me as well, but it was only launching locally, and I'm pretty sure each node only has 4 slots given past behavior (the mpirun -np 8 example I gave in my first email launches on both hosts).&nbsp; Is there a way to specify the hosts I want to launch on without the hostfile or host key so I can test remote launch?<br class=""><br class=""></div>And to the "hostname" response...no wonder it was hanging!&nbsp; I just constructed that as a basic example.&nbsp; In my real use I'm launching something that calls MPI_Init.<span class=""><font color="#888888" class=""><br class=""><br class=""></font></span></div><span class=""><font color="#888888" class="">Evan<br class=""><div class=""><div class=""><div class=""><div class=""><br class=""><br class=""></div></div></div></div></font></span></div><br class=""></div></div>_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">Subscription:<span class="">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this post:<span class="">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2015/02/26271.php" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/02/26271.php</a><br class=""></blockquote></div><br class=""></div><br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">Subscription:<span class="">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class=""></div></div>Link to this post:<span class="">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2015/02/26272.php" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/02/26272.php</a><br class=""></blockquote></div><br class=""></div><br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">Subscription:<span class="">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class=""></div></div>Link to this post:<span class="">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2015/02/26281.php" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/02/26281.php</a><br class=""></blockquote></div><br class=""></div><br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">Subscription:<span class="">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class=""></div></div>Link to this post:<span class="">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2015/02/26285.php" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/02/26285.php</a><br class=""></blockquote></div><br class=""></div><br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">Subscription:<span class="">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class=""></div></div>Link to this post:<span class="">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2015/02/26286.php" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/02/26286.php</a><br class=""></blockquote></div><br class=""></div><br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">Subscription:<span class="">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this post:<span class="">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2015/02/26287.php" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/02/26287.php</a><br class=""></blockquote></div><br class=""></div><span style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;float:none;display:inline!important" class="">_______________________________________________</span><br style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" class=""><span style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;float:none;display:inline!important" class="">users mailing list</span><br style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" class=""><a href="mailto:users@open-mpi.org" style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" target="_blank" class="">users@open-mpi.org</a><br style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" class=""><span style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;float:none;display:inline!important" class="">Subscription:<span class="">&nbsp;</span></span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" class=""></div></div><span style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;float:none;display:inline!important" class="">Link to this post:<span class="">&nbsp;</span></span><a href="http://www.open-mpi.org/community/lists/users/2015/02/26292.php" style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/02/26292.php</a></div></blockquote></div><br class=""></div></div><br class="">_______________________________________________<br class="">
users mailing list<br class="">
<a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/02/26294.php" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2015/02/26294.php</a><br class=""></blockquote></div><br class=""></div>
_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2015/02/26302.php</div></blockquote></div><br class=""></div></body></html>
