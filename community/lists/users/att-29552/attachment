<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="Generator" content="Microsoft Exchange Server">
<!-- converted from rtf -->
<style><!-- .EmailQuote { margin-left: 1pt; padding-left: 4pt; border-left: #800000 2px solid; } --></style>
</head>
<body>
<font face="Calibri, sans-serif" size="2">
<div>Hi Open MPI Users and Developers,</div>
<div>&nbsp;</div>
<div>I would like to know your experience with the optional middleware and the corresponding Open MPI framework/components for recent Mellanox Infiniband HCA, especially concerning MXM, FCA (the latest versions bring HCOLL I think) and the related Open MPI
framework/components such as the MTL/mxm, PML/yalla, the COLL/fca and COLL/hcoll.</div>
<div>&nbsp;</div>
<div>Does MXM when used with MTL/mxm or PML/yalla really improve communication speed over the plain BTL/openib ? </div>
<div>&nbsp;</div>
<div>Especially since MXM allows matching message tags, I suppose that in addition to improving a little the usual latency/bandwidth metrics, it would increase the communication/computation overlap potential when used with non-blocking MPI calls since the adapter
is more autonomous.</div>
<div>&nbsp;</div>
<div>I remember that with old Myrinet networks, the matching MX middleware for our application was way better than the earlier non-matching GM middleware. I guess it is the same thing now with Infiniband / OpenFabric networks. Matching middleware should therefore
be better.</div>
<div>&nbsp;</div>
<div>&nbsp;</div>
<div>Also concerning FCA and HCOLL,&nbsp; do they really improve the speed of the collective operations ?</div>
<div>&nbsp;</div>
<div>From the Mellanox documentation I saw they are supposed to use hardware broadcast and take into account the topology to favor the faster connections between process located on the same nodes. I also saw in these documents that recent version of FCA is
able to perform the reduction operations on the HCA itself, even the floating point ones. This should greatly improve the speed of MPI_Allreduce() in our codes !</div>
<div>&nbsp;</div>
<div>So for those lucky who have access to a recent well configured Mellanox Infiniband cluster with recent middleware and an Open MPI library well configured to take advantage of this, does it deliver its promises ?</div>
<div>&nbsp;</div>
<div>The only documentation/reports I could find on Internet on these subjects are from Mellanox in addition to this for PML/yalla and MTL/mxm (slide 32):</div>
<div>&nbsp;</div>
<div>&nbsp; <a href="https://www.open-mpi.org/papers/sc-2014/Open-MPI-SC14-BOF.pdf"><font color="#0000FF"><u>https://www.open-mpi.org/papers/sc-2014/Open-MPI-SC14-BOF.pdf</u></font></a></div>
<div>&nbsp;</div>
<div>Thanks in advance,</div>
<div>&nbsp;</div>
<div>&nbsp;</div>
<div>Martin Audet</div>
<div>&nbsp;</div>
</font>
</body>
</html>

