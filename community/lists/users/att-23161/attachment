<div dir="ltr">Probably it was the changing from eager to rendezvous protocols as Jeff said.<div><br></div><div>If you don&#39;t know what are these, read this:</div><div><a href="https://computing.llnl.gov/tutorials/mpi_performance/#Protocols">https://computing.llnl.gov/tutorials/mpi_performance/#Protocols</a><br>

</div><div><a href="http://blogs.cisco.com/performance/what-is-an-mpi-eager-limit/">http://blogs.cisco.com/performance/what-is-an-mpi-eager-limit/</a><br></div><div><a href="http://blogs.cisco.com/performance/eager-limits-part-2/">http://blogs.cisco.com/performance/eager-limits-part-2/</a><br>

</div><div><br></div><div>You can tune eager limit chaning mca parameters btl_tcp_eager_limit (for tcp), btl_self_eager_limit (comunication fron one process to itself), btl_sm_eager_limit (shared memory) and btl_udapl_eager_limit or btl_openib_eager_limit (if you use infiniband).</div>

<div class="gmail_extra"><br><br><div class="gmail_quote">2013/12/6 Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span><br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">

I sent you some further questions yesterday:<br>
<br>
&nbsp; &nbsp; <a href="http://www.open-mpi.org/community/lists/users/2013/12/23158.php" target="_blank">http://www.open-mpi.org/community/lists/users/2013/12/23158.php</a><br>
<div class="HOEnZb"><div class="h5"><br>
<br>
On Dec 6, 2013, at 1:35 AM, 胡杨 &lt;<a href="mailto:781578278@qq.com">781578278@qq.com</a>&gt; wrote:<br>
<br>
&gt; Here is &nbsp;my code:<br>
&gt; int*a=(int*)malloc(sizeof(int)*number);<br>
&gt; MPI_Send(a,number, MPI_INT, 1, 1,MPI_COMM_WORLD);<br>
&gt;<br>
&gt; int*b=(int*)malloc(sizeof(int)*number);<br>
&gt; MPI_Recv(b, number, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status);<br>
&gt;<br>
&gt; number &nbsp;here is the size of my array(eg,a or b).<br>
&gt; I &nbsp;have try it on my local compute and my rocks cluster.On rocks cluster, one processor &nbsp;on &nbsp;one frontend node &nbsp;use &quot;MPI_Send&quot; send a message ,other processors on compute nodes use &quot;MPI_Recv&quot; receive message .<br>


&gt; when number is least than 10000,other processors can receive message fast;<br>
&gt; but when &nbsp;number is more than 15000,other processors can receive message slowly<br>
&gt; why?? &nbsp;becesue openmpi API ?? or other &nbsp;problems?<br>
&gt;<br>
&gt; it spends me a few days , I want your help,thanks for all readers. good luck for you<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; ------------------ 原始邮件 ------------------<br>
&gt; 发件人: &quot;Ralph Castain&quot;;&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;;<br>
&gt; 发送时间: 2013年12月5日(星期四) 晚上6:52<br>
&gt; 收件人: &quot;Open MPI Users&quot;&lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;;<br>
&gt; 主题: Re: [OMPI users] can you help me please ?thanks<br>
&gt;<br>
&gt; You are running 15000 ranks on two nodes?? My best guess is that you are swapping like crazy as your memory footprint problem exceeds available physical memory.<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; On Thu, Dec 5, 2013 at 1:04 AM, 胡杨 &lt;<a href="mailto:781578278@qq.com">781578278@qq.com</a>&gt; wrote:<br>
&gt; My ROCKS cluster includes one frontend and two &nbsp;compute nodes.In my program,I have use the openmpi API &nbsp;such as &nbsp;MPI_Send and &nbsp;MPI_Recv . &nbsp;but &nbsp;when I &nbsp;run &nbsp;the progam with 3 processors . one processor &nbsp;send a message ,other receive message .here are some code.<br>


&gt; int*a=(int*)malloc(sizeof(int)*number);<br>
&gt; MPI_Send(a,number, MPI_INT, 1, 1,MPI_COMM_WORLD);<br>
&gt;<br>
&gt; int*b=(int*)malloc(sizeof(int)*number);<br>
&gt; MPI_Recv(b, number, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status);<br>
&gt;<br>
&gt; when number is least than 10000,it runs fast.<br>
&gt; but number is more than 15000,it runs slowly<br>
&gt;<br>
&gt; why?? &nbsp;becesue openmpi API ?? or other &nbsp;problems?<br>
&gt; ------------------ 原始邮件 ------------------<br>
&gt; 发件人: &quot;Ralph Castain&quot;;&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;;<br>
&gt; 发送时间: 2013年12月3日(星期二) 中午1:39<br>
&gt; 收件人: &quot;Open MPI Users&quot;&lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;;<br>
&gt; 主题: Re: [OMPI users] can you help me please ?thanks<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; On Mon, Dec 2, 2013 at 9:23 PM, 胡杨 &lt;<a href="mailto:781578278@qq.com">781578278@qq.com</a>&gt; wrote:<br>
&gt; A simple program at my 4-node ROCKS cluster runs fine with command:<br>
&gt; /opt/openmpi/bin/mpirun -np 4 -machinefile machines ./sort_mpi6<br>
&gt;<br>
&gt;<br>
&gt; Another bigger programs runs fine on the head node only with command:<br>
&gt;<br>
&gt; cd ./sphere; /opt/openmpi/bin/mpirun -np 4 ../bin/sort_mpi6<br>
&gt;<br>
&gt; But with the command:<br>
&gt;<br>
&gt; cd /sphere; /opt/openmpi/bin/mpirun -np 4 -machinefile ../machines<br>
&gt; ../bin/sort_mpi6<br>
&gt;<br>
&gt; It gives output that:<br>
&gt;<br>
&gt; ../bin/sort_mpi6: error while loading shared libraries: libgdal.so.1: cannot open<br>
&gt; shared object file: No such file or directory<br>
&gt; ../bin/sort_mpi6: error while loading shared libraries: libgdal.so.1: cannot open<br>
&gt; shared object file: No such file or directory<br>
&gt; ../bin/sort_mpi6: error while loading shared libraries: libgdal.so.1: cannot open<br>
&gt; shared object file: No such file or directory<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
</div></div><div class="im HOEnZb">--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
</div><div class="HOEnZb"><div class="h5">_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></div></blockquote></div><br></div></div>

