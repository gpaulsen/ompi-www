<div dir="ltr"><div><div><div>Hi,<br><br></div>here&#39;s another test with openmpi 1.8.3. With 1.8.1, 32GB was detected, now it is just 16:<br>% mpirun -np 2 /usr/lib64/openmpi-intel/bin/mpitests-osu_get_bw <br>--------------------------------------------------------------------------<br>WARNING: It appears that your OpenFabrics subsystem is configured to only<br>allow registering part of your physical memory.  This can cause MPI jobs to<br>run with erratic performance, hang, and/or crash.<br><br>This may be caused by your OpenFabrics vendor limiting the amount of<br>physical memory that can be registered.  You should investigate the<br>relevant Linux kernel module parameters that control how much physical<br>memory can be registered, and increase them to allow registering all<br>physical memory on your machine.<br><br>See this Open MPI FAQ item for more information on these Linux kernel module<br>parameters:<br><br>    <a href="http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages">http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages</a><br><br>  Local host:              pax95<br>  Registerable memory:     16384 MiB<br>  Total memory:            49106 MiB<br><br>Your MPI job will continue, but may be behave poorly and/or hang.<br>--------------------------------------------------------------------------<br># OSU MPI_Get Bandwidth Test v4.3<br># Window creation: MPI_Win_allocate<br># Synchronization: MPI_Win_flush<br># Size      Bandwidth (MB/s)<br>1                      28.56<br>2                      58.74<br><br><br></div>So it wasn&#39;t fixed for RHEL 6.6.<br><br></div>Regards, Götz<br></div><div class="gmail_extra"><br><div class="gmail_quote">On Mon, Dec 8, 2014 at 4:00 PM, Götz Waschk <span dir="ltr">&lt;<a href="mailto:goetz.waschk@gmail.com" target="_blank">goetz.waschk@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi,<br>
<br>
I had tested 1.8.4rc1 and it wasn&#39;t fixed. I can try again though,<br>
maybe I had made an error.<br>
<br>
Regards, Götz Waschk<br>
<div><div class="h5"><br>
On Mon, Dec 8, 2014 at 3:17 PM, Joshua Ladd &lt;<a href="mailto:jladd.mlnx@gmail.com">jladd.mlnx@gmail.com</a>&gt; wrote:<br>
&gt; Hi,<br>
&gt;<br>
&gt; This should be fixed in OMPI 1.8.3. Is it possible for you to give 1.8.3 a<br>
&gt; shot?<br>
&gt;<br>
&gt; Best,<br>
&gt;<br>
&gt; Josh<br>
&gt;<br>
&gt; On Mon, Dec 8, 2014 at 8:43 AM, Götz Waschk &lt;<a href="mailto:goetz.waschk@gmail.com">goetz.waschk@gmail.com</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; Dear Open-MPI experts,<br>
&gt;&gt;<br>
&gt;&gt; I have updated my little cluster from Scientific Linux 6.5 to 6.6,<br>
&gt;&gt; this included extensive changes in the Infiniband drivers and a newer<br>
&gt;&gt; openmpi version (1.8.1). Now I&#39;m getting this message on all nodes<br>
&gt;&gt; with more than 32 GB of RAM:<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; WARNING: It appears that your OpenFabrics subsystem is configured to only<br>
&gt;&gt; allow registering part of your physical memory.  This can cause MPI jobs<br>
&gt;&gt; to<br>
&gt;&gt; run with erratic performance, hang, and/or crash.<br>
&gt;&gt;<br>
&gt;&gt; This may be caused by your OpenFabrics vendor limiting the amount of<br>
&gt;&gt; physical memory that can be registered.  You should investigate the<br>
&gt;&gt; relevant Linux kernel module parameters that control how much physical<br>
&gt;&gt; memory can be registered, and increase them to allow registering all<br>
&gt;&gt; physical memory on your machine.<br>
&gt;&gt;<br>
&gt;&gt; See this Open MPI FAQ item for more information on these Linux kernel<br>
&gt;&gt; module<br>
&gt;&gt; parameters:<br>
&gt;&gt;<br>
&gt;&gt;     <a href="http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages" target="_blank">http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages</a><br>
&gt;&gt;<br>
&gt;&gt;   Local host:              pax98<br>
&gt;&gt;   Registerable memory:     32768 MiB<br>
&gt;&gt;   Total memory:            49106 MiB<br>
&gt;&gt;<br>
&gt;&gt; Your MPI job will continue, but may be behave poorly and/or hang.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; The issue is similar to the one described in a previous thread about<br>
&gt;&gt; Ubuntu nodes:<br>
&gt;&gt; <a href="http://www.open-mpi.org/community/lists/users/2014/08/25090.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25090.php</a><br>
&gt;&gt; But the Infiniband driver is different, the values log_num_mtt and<br>
&gt;&gt; log_mtts_per_seg both still exist, but they cannot be changed and have<br>
&gt;&gt; on all configurations the same values:<br>
&gt;&gt; [pax52] /root # cat /sys/module/mlx4_core/parameters/log_num_mtt<br>
&gt;&gt; 0<br>
&gt;&gt; [pax52] /root # cat /sys/module/mlx4_core/parameters/log_mtts_per_seg<br>
&gt;&gt; 3<br>
&gt;&gt;<br>
&gt;&gt; The kernel changelog says that Red Hat has included this commit:<br>
&gt;&gt; mlx4: Scale size of MTT table with system RAM (Doug Ledford)<br>
&gt;&gt; so it should be all fine, the buffers scale automatically, however, as<br>
&gt;&gt; far as I can see, the wrong value calculated by calculate_max_reg() is<br>
&gt;&gt; used in the code, so I think I cannot simply ignore the warning. Also,<br>
&gt;&gt; a user has reported a problem with a job, I cannot confirm that this<br>
&gt;&gt; is the cause.<br>
&gt;&gt;<br>
&gt;&gt; My workaround was to simply load the mlx5_core kernel module, as this<br>
&gt;&gt; is used by calculate_max_reg() to detect OFED 2.0.<br>
&gt;&gt;<br>
&gt;&gt; Regards, Götz Waschk<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; Link to this post:<br>
&gt;&gt; <a href="http://www.open-mpi.org/community/lists/users/2014/12/25923.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/12/25923.php</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post:<br>
</div></div>&gt; <a href="http://www.open-mpi.org/community/lists/users/2014/12/25924.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/12/25924.php</a><br>
<span class="HOEnZb"><font color="#888888"><br>
<br>
<br>
--<br>
AL I:40: Do what thou wilt shall be the whole of the Law.<br>
</font></span></blockquote></div><br><br clear="all"><br>-- <br><div class="gmail_signature">AL I:40: Do what thou wilt shall be the whole of the Law.</div>
</div>

