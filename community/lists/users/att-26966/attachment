<div dir="ltr"><div><br></div><div>it is fine to recompile OMPI from HPCx to apply site default (choice of job scheduler for example, OMPI from HPCX compiled with ssh support only, etc.).</div><div><br></div><div>If ssh launcher is working on your system - than OMPI from HPCX should work as well.</div><div><br></div><div>could you please send to Alina (in cc) the command line and its output from hpcx/ompi failure?</div><div><br></div><div>Thanks</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Thu, May 28, 2015 at 7:33 PM, Timur Ismagilov <span dir="ltr">&lt;<a href="mailto:tismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div><p>Is it normal to rebuild openmpi from hpcx?<br>Why binaries don&#39;t work?<br></p><p><br><br><br>Четверг, 28 мая 2015, 14:01 +03:00 от Alina Sklarevich &lt;<a href="mailto:alinas@dev.mellanox.co.il" target="_blank">alinas@dev.mellanox.co.il</a>&gt;:<br>
</p><blockquote style="border-left:1px solid #0857a6;margin:10px;padding:0 0 0 10px">
	<div>
	



    











	
	


	
	
	

	

	
	

	

	
	



<div>
	
 	<div>
		
		
            <div><div dir="ltr">Thank you for this info.<div><br></div><div>If &#39;yalla&#39; now works for you, is there anything that is still wrong?</div><div><br></div><div>Thanks,</div><div>Alina.</div></div><div><br><div>On Thu, May 28, 2015 at 10:21 AM, Timur Ismagilov <span dir="ltr">&lt;<a href="//e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span> wrote:<br><blockquote style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div><p><span lang="en"><span>I&#39;m sorry for the delay</span></span>.<br><br>Here it is:<br>(<strong>I used 5 min </strong><strong>time limit</strong>)<br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.330-icc-OFED-1.5.4.1-redhat6.2-x86_64/ompi-mellanox-v1.8/bin/mpirun -x LD_PRELOAD=/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.330-icc-OFED-1.5.4.1-          redhat6.2-x86_64/mxm/debug/lib/libmxm.so -x MXM_LOG_LEVEL=data -x MXM_IB_PORTS=mlx4_0:1 -x MXM_SHM_KCOPY_MODE=off --mca pml yalla --hostfile hostlist ./hello 1&gt; hello_debugMXM_n-2_ppn-2.out  2&gt;hello_debugMXM_n-2_ppn-2.err  </p><p>P.S.<br>yalla warks fine with rebuilded ompi: --with-mxm=$HPCX_MXM_DIR<br><br><br><br></p><p><br><br><br>Вторник, 26 мая 2015, 16:22 +03:00 от Alina Sklarevich &lt;<a href="//e.mail.ru/compose/?mailto=mailto%3aalinas@dev.mellanox.co.il" target="_blank">alinas@dev.mellanox.co.il</a>&gt;:<br>
</p><blockquote style="border-left:1px solid #0857a6;margin:10px;padding:0 0 0 10px">
	<div>
	



    











	
	


	
	
	

	

	
	

	

	
	



<div>
	
 	<div>
		
		
            <div><div dir="ltr">Hi Timur,<div><br></div><div>HPCX has a debug version of MXM. Can you please add the following to your command line with pml yalla in order to use it and attach the output? </div><div>&quot;-x LD_PRELOAD=$HPCX_MXM_DIR/debug/lib/libmxm.so -x MXM_LOG_LEVEL=data&quot;</div><div><br></div><div>Also, could you please attach the entire output of &quot;$HPCX_MPI_DIR/bin/ompi_info -a&quot; </div><div><br></div><div>Thank you,</div><div>Alina. </div></div><div><br><div>On Tue, May 26, 2015 at 3:39 PM, Mike Dubman <span dir="ltr">&lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3amiked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt;</span> wrote:<br><blockquote style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div>Alina - could you please take a look?</div><div>Thx</div><div><br></div><br><div><div><div>---------- Forwarded message ----------<br>From: <b>Timur Ismagilov</b> <span dir="ltr">&lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span><br>Date: Tue, May 26, 2015 at 12:40 PM<br>Subject: Re[12]: [OMPI users] MXM problem<br>To: Open MPI Users &lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3ausers@open%2dmpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>Cc: Mike Dubman &lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3amiked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt;<br><br><br>
</div></div><div><div><div><p>It does not work for single node:<br><br><strong>1)</strong> host: $  $HPCX_MPI_DIR/bin/mpirun -x MXM_IB_PORTS=mlx4_0:1 -x MXM_SHM_KCOPY_MODE=off -host node5 -mca pml yalla -x MXM_TLS=ud,self,shm --prefix $HPCX_MPI_DIR -mca plm_base_verbose 5  -mca oob_base_verbose 10 -mca rml_base_verbose 10 --debug-daemons  -np 1 ./hello &amp;&gt; <strong>yalla.out </strong>                               </p><p><strong>2)</strong> host: $  $HPCX_MPI_DIR/bin/mpirun -x MXM_IB_PORTS=mlx4_0:1 -x MXM_SHM_KCOPY_MODE=off -host node5  --mca pml cm --mca mtl mxm --prefix $HPCX_MPI_DIR -mca plm_base_verbose 5  -mca oob_base_verbose 10 -mca rml_base_verbose 10 --debug-daemons -np 1 ./hello &amp;&gt; <strong>cm_mxm.out</strong> <br><br>I&#39;ve attached the <strong>yalla.out</strong> and <strong>cm_mxm.out</strong>  to this email.<br><br><br><br>Вторник, 26 мая 2015, 11:54 +03:00 от Mike Dubman &lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3amiked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt;:<br>
</p></div></div><blockquote style="border-left:1px solid #0857a6;margin:10px;padding:0 0 0 10px">
	<div>
	



    











	
	


	
	
	

	

	
	

	

	
	



<div>
	
 	<div>
		
		
            <div><div><div><div dir="ltr">does it work from single node?<div>could you please run with opts below and attach output?</div><div><br></div><div> -mca plm_base_verbose 5  -mca oob_base_verbose 10 -mca rml_base_verbose 10 --debug-daemons<br></div></div></div></div><div><br><div><div><div>On Tue, May 26, 2015 at 11:38 AM, Timur Ismagilov <span dir="ltr">&lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span> wrote:<br></div></div><blockquote style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div><div><div><p>1. mxm_perf_test - OK.<br>2. no_tree_spawn  - OK.<br>3. ompi yalla and &quot;--mca pml cm --mca mtl mxm&quot; still <span lang="en"><span>does not </span><span>work</span></span> (I use prebuild ompi-1.8.5 from hpcx-v1.3.330)<br><strong>3.a) host:$  $HPCX_MPI_DIR/bin/mpirun -x MXM_IB_PORTS=mlx4_0:1 -x MXM_SHM_KCOPY_MODE=off -host node5,node153  --mca pml cm --mca mtl mxm --prefix $HPCX_MPI_DIR ./hello</strong><br>--------------------------------------------------------------------------                               <br>A requested component was not found, or was unable to be opened.  This                                   <br>means that this component is either not installed or is unable to be                                     <br>used on your system (e.g., sometimes this means that shared libraries                                    <br>that the component requires are unable to be found/loaded).  Note that                                   <br>Open MPI stopped checking at the first component that it did not find.                                   <br>                                                                                                         <br>Host:      node153                                                                                       <br>Framework: mtl                                                                                           <br>Component: mxm                                                                                           <br>--------------------------------------------------------------------------                               <br>[node5:113560] PML cm cannot be selected                                                                 <br>--------------------------------------------------------------------------                               <br>No available pml components were found!                                                                  <br>                                                                                                         <br>This means that there are no components of this type installed on your                                   <br>system or all the components reported that they could not be used.                                       <br>                                                                                                         <br>This is a fatal error; your MPI process is likely to abort.  Check the                                   <br>output of the &quot;ompi_info&quot; command and ensure that components of this                                     <br>type are available on your system.  You may also wish to check the                                       <br>value of the &quot;component_path&quot; MCA parameter and ensure that it has at                                    <br>least one directory that contains valid MCA components.                                                  <br>--------------------------------------------------------------------------                               <br>[node153:44440] PML cm cannot be selected                                                                <br>-------------------------------------------------------                                                  <br>Primary job  terminated normally, but 1 process returned                                                 <br>a non-zero exit code.. Per user-direction, the job has been aborted.                                     <br>-------------------------------------------------------                                                  <br>--------------------------------------------------------------------------                               <br>mpirun detected that one or more processes exited with non-zero status, thus causing                     <br>the job to be terminated. The first process to do so was:                                                <br>                                                                                                         <br>  Process name: [[43917,1],0]                                                                            <br>  Exit code:    1                                                                                        <br>--------------------------------------------------------------------------                               <br>[login:110455] 1 more process has sent help message help-mca-base.txt / find-available:not-valid         <br>[login:110455] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages        <br>[login:110455] 1 more process has sent help message help-mca-base.txt / find-available:none-found        <br>                            <br><strong>3.b) host:$  $HPCX_MPI_DIR/bin/mpirun -x MXM_IB_PORTS=mlx4_0:1 -x MXM_SHM_KCOPY_MODE=off -host node5,node153 -mca pml yalla --prefix $HPCX_MPI_DIR ./hello</strong><br>--------------------------------------------------------------------------                               <br>A requested component was not found, or was unable to be opened.  This                                   <br>means that this component is either not installed or is unable to be                                     <br>used on your system (e.g., sometimes this means that shared libraries                                    <br>that the component requires are unable to be found/loaded).  Note that                                   <br>Open MPI stopped checking at the first component that it did not find.                                   <br>                                                                                                         <br>Host:      node153                                                                                       <br>Framework: pml                                                                                           <br>Component: yalla                                                                                         <br>--------------------------------------------------------------------------                               <br>*** An error occurred in MPI_Init                                                                        <br>--------------------------------------------------------------------------                               <br>It looks like MPI_INIT failed for some reason; your parallel process is                                  <br>likely to abort.  There are many reasons that a parallel process can                                     <br>fail during MPI_INIT; some of which are due to configuration or environment                              <br>problems.  This failure appears to be an internal failure; here&#39;s some                                   <br>additional information (which may only be relevant to an Open MPI                                        <br>developer):                                                                                              <br>                                                                                                         <br>  mca_pml_base_open() failed                                                                             <br>  --&gt; Returned &quot;Not found&quot; (-13) instead of &quot;Success&quot; (0)                                                <br>--------------------------------------------------------------------------                               <br>*** on a NULL communicator                                                                               <br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,                                 <br>***    and potentially your MPI job)                                                                     <br>[node153:43979] Local abort before MPI_INIT completed successfully; not able to aggregate error messages,<br> and not able to guarantee that all other processes were killed!                                         <br>-------------------------------------------------------                                                  <br>Primary job  terminated normally, but 1 process returned                                                 <br>a non-zero exit code.. Per user-direction, the job has been aborted.                                     <br>-------------------------------------------------------                                                  <br>--------------------------------------------------------------------------                               <br>mpirun detected that one or more processes exited with non-zero status, thus causing                     <br>the job to be terminated. The first process to do so was:                                                <br>                                                                                                         <br>  Process name: [[44992,1],1]                                                                            <br>  Exit code:    1                                                                                        <br>--------------------------------------------------------------------------                               <br><br><br><br><strong>host:$  echo $HPCX_MPI_DIR</strong>                                                                         <br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.330-icc-OFED-1.5.4.1-redhat6.2-x86_64/ompi-mellanox-v1.8<br><strong>host:$ ompi_info | grep pml </strong>                                                                       <br>                 MCA pml: v (MCA v2.0, API v2.0, Component v1.8.5)                                       <br>                 MCA pml: cm (MCA v2.0, API v2.0, Component v1.8.5)                                      <br>                 MCA pml: bfo (MCA v2.0, API v2.0, Component v1.8.5)                                     <br>                 MCA pml: ob1 (MCA v2.0, API v2.0, Component v1.8.5)                                     <br>                 MCA pml: yalla (MCA v2.0, API v2.0, Component v1.8.5)  <br><strong>host: tests$  ompi_info | grep mtl                                   </strong><br>                 MCA mtl: mxm (MCA v2.0, API v2.0, Component v1.8.5)<br><br><strong>P.S.</strong><br>possible error in the FAQ? (<a href="http://www.open-mpi.org/faq/?category=openfabrics#mxm" target="_blank">http://www.open-mpi.org/faq/?category=openfabrics#mxm</a>)<br></p><p>47. Does Open MPI support MXM?<br>............<br><strong><font color="red">NOTE:</font></strong> Please note that the &#39;yalla&#39; pml is available only from Open MPI v1.9 and above<br>...........<br>But here we have(or not...) yalla in ompi 1.8.5<br><br><br><br>Вторник, 26 мая 2015, 9:53 +03:00 от Mike Dubman &lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3amiked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt;:<br>
</p></div></div><blockquote style="border-left:1px solid #0857a6;margin:10px;padding:0 0 0 10px">
	<div>
	



    











	
	


	
	
	

	

	
	

	

	
	



<div>
	
 	<div>
		
		
            <div><div><div><div dir="ltr">Hi Timur,<div><br></div><div>Here it goes:</div><div><br></div><div>wget <a href="ftp://bgate.mellanox.com/hpc/hpcx/custom/v1.3/hpcx-v1.3.330-icc-OFED-1.5.4.1-redhat6.2-x86_64.tbz" target="_blank">ftp://bgate.mellanox.com/hpc/hpcx/custom/v1.3/hpcx-v1.3.330-icc-OFED-1.5.4.1-redhat6.2-x86_64.tbz</a><br></div><div><br></div><div>Please let me know if it works for you and will add 1.5.4.1 mofed to the default distribution list.</div><div><br></div><div>M</div><div><br></div></div></div></div><div><br><div><div><div>On Mon, May 25, 2015 at 9:38 PM, Timur Ismagilov <span dir="ltr">&lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span> wrote:<br></div></div><blockquote style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div><div><div><span lang="en"><span>Thanks a lot</span></span>.<br><br>Понедельник, 25 мая 2015, 21:28 +03:00 от Mike Dubman &lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3amiked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt;:</div></div><div><div><br>
<blockquote style="border-left:1px solid #0857a6;margin:10px;padding:0 0 0 10px">
	<div>
	



    











	
	


	
	
	

	

	
	

	

	
	



<div>
	
 	<div>
		
		
            <div><div><div><div dir="ltr">will send u the link tomorrow.</div></div></div><div><br><div><div><div>On Mon, May 25, 2015 at 9:15 PM, Timur Ismagilov <span dir="ltr">&lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span> wrote:<br></div></div><blockquote style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div><div><div>Where can i find MXM for ofed 1.5.4.1?<br><br><br>Понедельник, 25 мая 2015, 21:11 +03:00 от Mike Dubman &lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3amiked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt;:</div></div><div><div><br>
<blockquote style="border-left:1px solid #0857a6;margin:10px;padding:0 0 0 10px">
	<div>
	



    











	
	


	
	
	

	

	
	

	

	
	



<div>
	
 	<div>
		
		
            <div><div><div><div dir="ltr">btw, the ofed on your system is 1.5.4.1 while HPCx in use is for ofed 1.5.3<div><br></div><div>seems like ABI issue between ofed versions</div></div></div></div><div><br><div><div><div>On Mon, May 25, 2015 at 8:59 PM, Timur Ismagilov <span dir="ltr">&lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span> wrote:<br></div></div><blockquote style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div><div><div><p>I did as you said, but got an error:</p><p><br>node1$ export MXM_IB_PORTS=mlx4_0:1<br>node1$  ./mxm_perftest                                                                            <br>Waiting for connection...                                                                                <br>Accepted connection from 10.65.0.253                                                                     <br>[1432576262.370195] [node153:35388:0]         shm.c:65   MXM  WARN  Could not open the KNEM device file at /dev/knem : No such file or directory. Won&#39;t use knem.                                                 <span><br>Failed to create endpoint: No such device                                                                <br><br></span>node2$ export MXM_IB_PORTS=mlx4_0:1<br>node2$ ./mxm_perftest node1  -t send_lat                                                       <br>[1432576262.367523] [node158:99366:0]         shm.c:65   MXM  WARN  Could not open the KNEM device file at /dev/knem : No such file or directory. Won&#39;t use knem.<span><br>Failed to create endpoint: No such device<br><br><br><br><br></span>Понедельник, 25 мая 2015, 20:31 +03:00 от Mike Dubman &lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3amiked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt;:<br>
</p></div></div><div><div><blockquote style="border-left:1px solid #0857a6;margin:10px;padding:0 0 0 10px">
	<div>
	



    











	
	


	
	
	

	

	
	

	

	
	



<div>
	
 	<div>
		
		
            <div><div><div><div dir="ltr">scif is a OFA device from Intel.<div>can you please select export MXM_IB_PORTS=mlx4_0:1 explicitly and retry</div></div></div></div><div><br><div><div><div>On Mon, May 25, 2015 at 8:26 PM, Timur Ismagilov <span dir="ltr">&lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span> wrote:<br></div></div><blockquote style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div><div><div><p>Hi, Mike,<br>that is what i have:<br></p><p>$ echo $LD_LIBRARY_PATH | tr &quot;:&quot; &quot;\n&quot;<br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/fca/lib               <br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/hcoll/lib             <br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/mxm/lib               <br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/ompi-mellanox-v1.8/lib<br> +intel compiler paths<br><br>$ echo $OPAL_PREFIX                                                                     <br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/ompi-mellanox-v1.8<br><br>I don&#39;t use LD_PRELOAD.<br><br>In the attached file(ompi_info.out) you will find the output of ompi_info -l 9  command.<br><br><strong>P.S</strong>. <br>node1 $ ./mxm_perftest<br>node2 $  ./mxm_perftest node1  -t send_lat<br>[1432568685.067067] [node151:87372:0]         shm.c:65   MXM  WARN  Could not open the KNEM device file $t /dev/knem : No such file or directory. Won&#39;t use knem.         <strong>( I don&#39;t have knem)</strong><br>[1432568685.069699] [node151:87372:0]      ib_dev.c:531  MXM  WARN  skipping device scif0 (vendor_id/par$_id = 0x8086/0x0) - not a Mellanox device                               <strong>(???)</strong><br>Failed to create endpoint: No such device<br><br>$  ibv_devinfo                                         <br>hca_id: mlx4_0                                                  <br>        transport:                      InfiniBand (0)          <br>        fw_ver:                         2.10.600                <br>        node_guid:                      0002:c903:00a1:13b0     <br>        sys_image_guid:                 0002:c903:00a1:13b3     <br>        vendor_id:                      0x02c9                  <br>        vendor_part_id:                 4099                    <br>        hw_ver:                         0x0                     <br>        board_id:                       MT_1090120019           <br>        phys_port_cnt:                  2                       <br>                port:   1                                       <br>                        state:                  PORT_ACTIVE (4) <br>                        max_mtu:                4096 (5)        <br>                        active_mtu:             4096 (5)        <br>                        sm_lid:                 1               <br>                        port_lid:               83              <br>                        port_lmc:               0x00            <br>                                                                <br>                port:   2                                       <br>                        state:                  PORT_DOWN (1)   <br>                        max_mtu:                4096 (5)        <br>                        active_mtu:             4096 (5)        <br>                        sm_lid:                 0               <br>                        port_lid:               0               <br>                        port_lmc:               0x00            <br><br><span style="font-family:arial,helvetica,sans-serif">Best regards,</span><br><span style="font-family:arial,helvetica,sans-serif">Timur.</span><br><br></p><p><br>Понедельник, 25 мая 2015, 19:39 +03:00 от Mike Dubman &lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3amiked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt;:<br>
</p></div></div><div><div><blockquote style="border-left:1px solid #0857a6;margin:10px;padding:0 0 0 10px">
	<div>
	



    











	
	


	
	
	

	

	
	

	

	
	



<div>
	
 	<div>
		
		
            <div><div><div><div dir="ltr">Hi Timur,<div>seems that yalla component was not found in your OMPI tree.</div><div>can it be that your mpirun is not from hpcx? Can you please check LD_LIBRARY_PATH,PATH, LD_PRELOAD and OPAL_PREFIX that it is pointing to the right mpirun?</div><div><br></div><div>Also, could you please check that yalla is present in the ompi_info -l 9 output?</div><div><br></div><div>Thanks</div></div></div></div><div><br><div><div><div>On Mon, May 25, 2015 at 7:11 PM, Timur Ismagilov <span dir="ltr">&lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span> wrote:<br></div></div><blockquote style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div><div><div>I can password-less ssh to all nodes:<br>base$ ssh node1<br>node1$ssh node2<br>Last login: Mon May 25 18:41:23 <br>node2$ssh node3<br>Last login: Mon May 25 16:25:01<br>node3$ssh node4<br>Last login: Mon May 25 16:27:04<br>node4$<br><br>Is this correct?<br><br>In ompi-1.9 i do not have no-tree-spawn problem.<br><br><br>Понедельник, 25 мая 2015, 9:04 -07:00 от Ralph Castain &lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3arhc@open%2dmpi.org" target="_blank">rhc@open-mpi.org</a>&gt;:</div></div><div><div><br>
<blockquote style="border-left:1px solid #0857a6;margin:10px;padding:0 0 0 10px">
	<div>
	



    











	
	


	
	
	

	

	
	

	

	
	



<div>
	
 	<div>
		
		
            <div><div><div>I can’t speak to the mxm problem, but the no-tree-spawn issue indicates that you don’t have password-less ssh authorized between the compute nodes<div><br></div></div></div><div><br><div><blockquote type="cite"><div><div><div>On May 25, 2015, at 8:55 AM, Timur Ismagilov &lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt; wrote:</div><br></div></div><div>
<div><p></p><div><div>Hello!<br><br>I use ompi-v1.8.4 from hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2;<br>OFED-1.5.4.1;<br>CentOS release 6.2;<br>infiniband 4x FDR<br><br><br><br>I have two problems:<br><strong>1. I can not use mxm</strong>:<br><strong><span style="text-decoration:underline">1.a) $mpirun --mca pml cm --mca mtl mxm -host node5,node14,node28,node29 -mca plm_rsh_no_tree_spawn 1 -np 4 ./hello</span> </strong><br>--------------------------------------------------------------------------                               <br>A requested component was not found, or was unable to be opened.  This                                   <br>means that this component is either not installed or is unable to be                                     <br>used on your system (e.g., sometimes this means that shared libraries                                    <br>that the component requires are unable to be found/loaded).  Note that                                   <br>Open MPI stopped checking at the first component that it did not find.                                   <br>                                                                                                         <br>Host:      node14                                                                                        <br>Framework: pml                                                                                           <br>Component: yalla                                                                                         <br>--------------------------------------------------------------------------                               <br>*** An error occurred in MPI_Init                                                                        <br>--------------------------------------------------------------------------                               <br>It looks like MPI_INIT failed for some reason; your parallel process is                                  <br>likely to abort.  There are many reasons that a parallel process can                                     <br>fail during MPI_INIT; some of which are due to configuration or environment                              <br>problems.  This failure appears to be an internal failure; here&#39;s some                                   <br>additional information (which may only be relevant to an Open MPI                                        <br>developer):                                                                                              <br>                                                                                                         <br>  mca_pml_base_open() failed                                                                             <br>  --&gt; Returned &quot;Not found&quot; (-13) instead of &quot;Success&quot; (0)                                                <br>--------------------------------------------------------------------------                               <br>*** on a NULL communicator                                                                               <br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,                                 <br>***    and potentially your MPI job)                                                                     <br>*** An error occurred in MPI_Init                                                                        <br>[node28:102377] Local abort before MPI_INIT completed successfully; not able to aggregate error messages,<br> and not able to guarantee that all other processes were killed!                                         <br>*** on a NULL communicator                                                                               <br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,                                 <br>***    and potentially your MPI job)                                                                     <br>[node29:105600] Local abort before MPI_INIT completed successfully; not able to aggregate error messages,<br> and not able to guarantee that all other processes were killed!                                         <br>*** An error occurred in MPI_Init                                                                        <br>*** on a NULL communicator                                                                               <br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,                                 <br>***    and potentially your MPI job)                                                                     <br>[node5:102409] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, <br>and not able to guarantee that all other processes were killed!                                          <br>*** An error occurred in MPI_Init                                                                        <br>*** on a NULL communicator                                                                               <br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,                                 <br>***    and potentially your MPI job)                                                                     <br>[node14:85284] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, <br>and not able to guarantee that all other processes were killed!                                          <br>-------------------------------------------------------                                                  <br>Primary job  terminated normally, but 1 process returned                                                 <br>a non-zero exit code.. Per user-direction, the job has been aborted.                                     <br>-------------------------------------------------------                                                  <br>--------------------------------------------------------------------------                               <br>mpirun detected that one or more processes exited with non-zero status, thus causing                     <br>the job to be terminated. The first process to do so was:                                                <br>                                                                                                         <br>  Process name: [[9372,1],2]<br>  Exit code:    1                                                                                        <br>--------------------------------------------------------------------------                               <br>[login:08295] 3 more processes have sent help message help-mca-base.txt / find-available:not-valid       <br>[login:08295] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages         <br>[login:08295] 3 more processes have sent help message help-mpi-runtime / mpi_init:startup:internal-failur<br>e                                                                                                        <br><br><strong><span style="text-decoration:underline">1.b $mpirun --mca pml yalla -host node5,node14,node28,node29 -mca plm_rsh_no_tree_spawn 1 -np 4 ./hello</span> </strong><br>--------------------------------------------------------------------------                              <br>A requested component was not found, or was unable to be opened.  This                                  <br>means that this component is either not installed or is unable to be                                    <br>used on your system (e.g., sometimes this means that shared libraries                                   <br>that the component requires are unable to be found/loaded).  Note that                                  <br>Open MPI stopped checking at the first component that it did not find.                                  <br>                                                                                                        <br>Host:      node5                                                                                        <br>Framework: pml                                                                                          <br>Component: yalla                                                                                        <br>--------------------------------------------------------------------------                              <br>*** An error occurred in MPI_Init                                                                       <br>*** on a NULL communicator                                                                              <br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,                                <br>***    and potentially your MPI job)                                                                    <br>[node5:102449] Local abort before MPI_INIT completed successfully; not able to aggregate error messages,<br>and not able to guarantee that all other processes were killed!                                         <br>--------------------------------------------------------------------------                              <br>It looks like MPI_INIT failed for some reason; your parallel process is                                 <br>likely to abort.  There are many reasons that a parallel process can                                    <br>fail during MPI_INIT; some of which are due to configuration or environment                             <br>problems.  This failure appears to be an internal failure; here&#39;s some                                  <br>additional information (which may only be relevant to an Open MPI                                       <br>developer):                                                                                             <br>                                                                                                        <br>  mca_pml_base_open() failed                                                                            <br>  --&gt; Returned &quot;Not found&quot; (-13) instead of &quot;Success&quot; (0)                                               <br>--------------------------------------------------------------------------                              <br>-------------------------------------------------------                                                 <br>Primary job  terminated normally, but 1 process returned                                                <br>a non-zero exit code.. Per user-direction, the job has been aborted.                                    <br>-------------------------------------------------------                                                 <br>*** An error occurred in MPI_Init                                                                       <br>*** on a NULL communicator                                                                              <br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,                                <br>***    and potentially your MPI job)                                                                    <br>[node14:85325] Local abort before MPI_INIT completed successfully; not able to aggregate error messages,<br>and not able to guarantee that all other processes were killed!                                         <br>--------------------------------------------------------------------------                              <br>mpirun detected that one or more processes exited with non-zero status, thus causing                    <br>the job to be terminated. The first process to do so was:                                               <br>                                                                                                        <br>  Process name: [[9619,1],0]                                                                            <br>  Exit code:    1                                                                                       <br>--------------------------------------------------------------------------                              <br>[login:08552] 1 more process has sent help message help-mca-base.txt / find-available:not-valid         <br>[login:08552] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages        <br><strong><br>2. I can not remove <strong>-mca plm_rsh_no_tree_spawn 1 from mpirun cmd line</strong>:</strong><br>$mpirun -host node5,node14,node28,node29 -np 4 ./hello<br>sh: -c: line 0: syntax error near unexpected token `--tree-spawn&#39;                                        <br>sh: -c: line 0: `( test ! -r ./.profile || . ./.profile; OPAL_PREFIX=/gpfs/NETHOME/oivt1/nicevt/itf/sourc<br>es/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/ompi-mellanox-v1.8 ; export OPAL_PREFIX; PATH=/gpfs/NETHOME/o<br>ivt1/nicevt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/ompi-mellanox-v1.8/bin:$PATH ; export PA<br>TH ; LD_LIBRARY_PATH=/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/ompi<br>-mellanox-v1.8/lib:$LD_LIBRARY_PATH ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/gpfs/NETHOME/oivt1/nice<br>vt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/ompi-mellanox-v1.8/lib:$DYLD_LIBRARY_PATH ; expor<br>t DYLD_LIBRARY_PATH ;   /gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/o<br>mpi-mellanox-v1.8/bin/orted --hnp-topo-sig 2N:2S:2L3:16L2:16L1:16C:32H:x86_64 -mca ess &quot;env&quot; -mca orte_es<br>s_jobid &quot;625606656&quot; -mca orte_ess_vpid 3 -mca orte_ess_num_procs &quot;5&quot; -mca orte_parent_uri &quot;625606656.1;tc<br><a>p://10.65.0.105,10.64.0.105,10.67.0.105:56862</a>&quot; -mca orte_hnp_uri &quot;625606656.0;<a>tcp://10.65.0.2,10.67.0.2,8</a><br>3.149.214.101,<a href="http://10.64.0.2:54893" target="_blank">10.64.0.2:54893</a>&quot; --mca pml &quot;yalla&quot; -mca plm_rsh_no_tree_spawn &quot;0&quot; -mca plm &quot;rsh&quot; ) --tree-s<br>pawn&#39;                                                                                                    <br>--------------------------------------------------------------------------                               <br>ORTE was unable to reliably start one or more daemons.                                                   <br>This usually is caused by:                                                                               <br>                                                                                                         <br>* not finding the required libraries and/or binaries on                                                  <br>  one or more nodes. Please check your PATH and LD_LIBRARY_PATH                                          <br>  settings, or configure OMPI with --enable-orterun-prefix-by-default                                    <br>                                                                                                         <br>* lack of authority to execute on one or more specified nodes.                                           <br>  Please verify your allocation and authorities.                                                         <br>                                                                                                         <br>* the inability to write startup files into /tmp (--tmpdir/orte_tmpdir_base).                            <br>  Please check with your sys admin to determine the correct location to use.                             <br>                                                                                                         <br>*  compilation of the orted with dynamic libraries when static are required                              <br>  (e.g., on Cray). Please check your configure cmd line and consider using                               <br>  one of the contrib/platform definitions for your system type.                                          <br>                                                                                                         <br>* an inability to create a connection back to mpirun due to a                                            <br></div></div><span>  lack of common network interfaces and/or no route found between                                        <br>  them. Please check network connectivity (including firewalls                                           <br>  and network routing requirements).                                                                     <br>--------------------------------------------------------------------------                               <br>mpirun: abort is already in progress...hit ctrl-c again to forcibly terminate                          <br>                                                                                                         <br></span><p></p><div><span style="font-family:arial,helvetica,sans-serif">Thank you for your comments.</span></div><div><span style="font-family:arial,helvetica,sans-serif"> </span></div><div><span style="font-family:arial,helvetica,sans-serif">Best regards,<br>Timur.<br></span></div><div><span style="font-family:arial,helvetica,sans-serif"> </span><br></div><p><br><br></p></div><div><div>
_______________________________________________<br>users mailing list<br><a href="https://e.mail.ru/compose/?mailto=mailto%3ausers@open%2dmpi.org" target="_blank">users@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/05/26919.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/05/26919.php</a><br></div></div></div></blockquote></div><br></div></div>
            
        
		
	</div>

	
</div>


</div>
</blockquote>
<br>
<br> <br></div></div></div><div><div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="https://e.mail.ru/compose/?mailto=mailto%3ausers@open%2dmpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/05/26922.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/05/26922.php</a><br></div></div></blockquote></div><br><br clear="all"><div><br></div>-- <br><div><div dir="ltr"><br><div>Kind Regards,</div><div><br></div><div>M.</div></div></div>
</div>
</div>
            
        
		
	</div>

	
</div>


</div>
</blockquote>
<br>
<br> <br></div></div></div>
</blockquote></div><br><br clear="all"><div><br></div>-- <br><div><div dir="ltr"><br><div>Kind Regards,</div><div><br></div><div>M.</div></div></div>
</div>
</div>
            
        
		
	</div>

	
</div>


</div>
</blockquote>
<br>
<br> <br></div></div></div>
</blockquote></div><br><br clear="all"><div><br></div>-- <br><div><div dir="ltr"><br><div>Kind Regards,</div><div><br></div><div>M.</div></div></div>
</div>
</div>
            
        
		
	</div>

	
</div>


</div>
</blockquote>
<br>
<br><br></div></div></div>
</blockquote></div><br><br clear="all"><span><font color="#888888"><div><br></div>-- <br><div><div dir="ltr"><br><div>Kind Regards,</div><div><br></div><div>M.</div></div></div>
</font></span></div><span><font color="#888888">
</font></span></div><span><font color="#888888">
            
        
		
	</font></span></div><span><font color="#888888">

	
</font></span></div><span><font color="#888888">


</font></span></div><span><font color="#888888">
</font></span></blockquote><span><font color="#888888">
<br>
<br><br></font></span></div></div></div><span><font color="#888888">
</font></span></blockquote></div><span><font color="#888888"><br><br clear="all"><span><font color="#888888"><div><br></div>-- <br><div><div dir="ltr"><br><div>Kind Regards,</div><div><br></div><div>M.</div></div></div>
</font></span></font></span></div><span><font color="#888888">
</font></span></div><span><font color="#888888">
            
        
		
	</font></span></div><span><font color="#888888">

	
</font></span></div><span><font color="#888888">


</font></span></div><span><font color="#888888">
</font></span></blockquote><span><font color="#888888">
<br>
<br> <br></font></span></div><span><font color="#888888">
</font></span></blockquote></div><span><font color="#888888"><br><br clear="all"><span class="HOEnZb"><font color="#888888"><span><font color="#888888"><span><font color="#888888"><div><br></div>-- <br><div><div dir="ltr"><br><div>Kind Regards,</div><div><br></div><div>M.</div></div></div>
</font></span></font></span></font></span></font></span></div><span class="HOEnZb"><font color="#888888"><span><font color="#888888"><span><font color="#888888">
</font></span></font></span></font></span></div><span class="HOEnZb"><font color="#888888"><span><font color="#888888"><span><font color="#888888">
            
        
		
	</font></span></font></span></font></span></div><span class="HOEnZb"><font color="#888888"><span><font color="#888888"><span><font color="#888888">

	
</font></span></font></span></font></span></div><span class="HOEnZb"><font color="#888888"><span><font color="#888888"><span><font color="#888888">


</font></span></font></span></font></span></div><span class="HOEnZb"><font color="#888888"><span><font color="#888888"><span><font color="#888888">
</font></span></font></span></font></span></blockquote><span class="HOEnZb"><font color="#888888"><span><font color="#888888"><span><font color="#888888">
<br>
<br> <br></font></span></font></span></font></span></div><span class="HOEnZb"><font color="#888888"><span><font color="#888888"><span><font color="#888888">
</font></span></font></span></font></span></div><span class="HOEnZb"><font color="#888888"><span><font color="#888888"><span><font color="#888888"><br><br clear="all"><div><br></div>-- <br><div><div dir="ltr"><br><div>Kind Regards,</div><div><br></div><div>M.</div></div></div>
</font></span></font></span></font></span></div><span class="HOEnZb"><font color="#888888"><span><font color="#888888">
</font></span></font></span></blockquote></div><span class="HOEnZb"><font color="#888888"><span><font color="#888888"><br></font></span></font></span></div><span class="HOEnZb"><font color="#888888">
</font></span></div><span class="HOEnZb"><font color="#888888">
            
        
		
	</font></span></div><span class="HOEnZb"><font color="#888888">

	
</font></span></div><span class="HOEnZb"><font color="#888888">


</font></span></div><span class="HOEnZb"><font color="#888888">
</font></span></blockquote><span class="HOEnZb"><font color="#888888">
<br>
<br> <br></font></span></div><span class="HOEnZb"><font color="#888888">
</font></span></blockquote></div><span class="HOEnZb"><font color="#888888"><br></font></span></div>
</div>
            
        
		
	</div>

	
</div>


</div>
</blockquote>
<br>
<br><br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/05/26965.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/05/26965.php</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br><div class="gmail_signature"><div dir="ltr"><br><div>Kind Regards,</div><div><br></div><div>M.</div></div></div>
</div>
