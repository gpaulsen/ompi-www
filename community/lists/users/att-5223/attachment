Hi Andreas, thanks for the reply!<br><br>I'm using openmpi-1.2.5. It was installed using my distro's (Gentoo) default package:<br><br>&nbsp;sys-cluster/openmpi-1.2.5&nbsp; USE="fortran ipv6 -debug -heterogeneous -nocxx -pbs -romio -smp -threads"<br><br>I've tried setting the mpi_yield_when_idle parameter as you asked. However, the program still hangs.<br><br>Just in case, the command line I'm using to call it is this:<br>/usr/bin/mpirun --hostfile mpi-config.txt --mca mpi_yield_when_idle 1 -np 3 /home/gfaccin/desenvolvimento/Eclipse/mpiplay/Debug/mpiplay<br><br>where mpi-config.txt contains the following line:<br>localhost slots=1<br><br>Anything else I could try?<br><br>Thank you!<br><br>Giovani<br><br><b><i>Andreas Schäfer &lt;gentryx@gmx.de&gt;</i></b> escreveu:<blockquote class="replbq" style="border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;"> Hmm, strange. It doesn't hang for me and AFAICS it shouldn't hang at<br>all. I'm using 1.2.5. Which
 version of Open MPI are you using? <br><br>Hanging with 100% CPU utilization often means that your processes are<br>caught in a busy wait. You could try to set mpi_yield_when_idle:<br><br>&gt; gentryx@hex ~ $ cat .openmpi/mca-params.conf<br>&gt; mpi_yield_when_idle=1<br><br>But I don't think this should be necessary.<br><br>HTH<br>-Andreas<br><br><br>On 21:35 Mon 17 Mar     , Giovani Faccin wrote:<br>&gt; Hi there!<br>&gt; <br>&gt; I'm learning MPI,  and got really puzzled... Please take a look at this very short code:<br>&gt; <br>&gt; #include <iostream><br>&gt; #include "mpicxx.h"<br>&gt; using namespace std;<br>&gt; int main(int argc, char *argv[])<br>&gt; {<br>&gt;     MPI::Init();        <br>&gt; <br>&gt;     for (unsigned long t = 0; t &lt; 10000000; t++)<br>&gt;     {<br>&gt;         //If we are process 0:<br>&gt;         if ( MPI::COMM_WORLD.Get_rank() == 0 )<br>&gt;         {<br>&gt;             MPI::Status mpi_status;<br>&gt;             unsigned long d =
 0;<br>&gt;             unsigned long d2 = 0;<br>&gt;             MPI::COMM_WORLD.Recv(&amp;d, 1, MPI::UNSIGNED_LONG, MPI::ANY_SOURCE, MPI::ANY_TAG, mpi_status );<br>&gt;             MPI::COMM_WORLD.Recv(&amp;d2, 1, MPI::UNSIGNED_LONG, MPI::ANY_SOURCE, MPI::ANY_TAG, mpi_status );<br>&gt;             cout &lt;&lt; "Time = " &lt;&lt; t &lt;&lt; "; Node 0 received: " &lt;&lt; d &lt;&lt; " and " &lt;&lt; d2 &lt;&lt; endl;<br>&gt;         }<br>&gt;         //Else:<br>&gt;         else<br>&gt;         {<br>&gt;             unsigned long  d = MPI::COMM_WORLD.Get_rank();<br>&gt;             MPI::COMM_WORLD.Send( &amp;d, 1, MPI::UNSIGNED_LONG, 0, 0);<br>&gt;         };<br>&gt;     };<br>&gt;     MPI::Finalize();<br>&gt; }<br>&gt; <br>&gt; Ok, so what I'm trying to do is to make a gather operation using point to point communication. In my real application instead of sending an unsigned long I'd be calling an object's send and receive methods, which in turn would call their inner
 object's similar methods and so on until all data is syncronized. I'm using this loop because the number of objects to be sent to process rank 0 varies depending on the sender.<br>&gt; <br>&gt; When running this test with 3 processes on a dual core, oversubscribed node, I get this output:<br>&gt; (skipped previous output)<br>&gt; Time = 5873; Node 0 received: 1 and 2<br>&gt; Time = 5874; Node 0 received: 1 and 2<br>&gt; Time = 5875; Node 0 received: 1 and 2<br>&gt; Time = 5876; Node 0 received: 1 and 2<br>&gt; <br>&gt; and then the application hangs, with processor usage at 100%. The exact time when this condition occurs varies on each run, but it usually happens quite fast.<br>&gt; <br>&gt; What would I have to modify, in this simple example, so that the application works as expected? Must I always use Gather, instead of point to point, to make a syncronization like this?<br>&gt; <br>&gt; Thank you very much!<br>&gt; <br>&gt; Giovani<br>&gt; <br>&gt; <br>&gt; <br>&gt;
 <br>&gt; <br>&gt; <br>&gt; <br>&gt;  __________________________________________________<br>&gt; Fale com seus amigos  de graça com o novo Yahoo! Messenger <br>&gt; http://br.messenger.yahoo.com/ <br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; users@open-mpi.org<br>&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br><br>-- <br>============================================<br>Andreas Schäfer<br>Cluster and Metacomputing Working Group<br>Friedrich-Schiller-Universität Jena, Germany<br>PGP/GPG key via keyserver<br>I'm a bright... http://www.the-brights.net<br>============================================<br><br>(\___/)<br>(+'.'+)<br>(")_(")<br>This is Bunny. Copy and paste Bunny into your <br>signature to help him gain world domination!<br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>http://www.open-mpi.org/mailman/listinfo.cgi/users</iostream></blockquote><br><p>&#32;


      <hr size=1>Abra sua conta no <a href="http://br.rd.yahoo.com/mail/taglines/mail/*http://br.mail.yahoo.com/">Yahoo! Mail</a>, o único sem limite de espaço para armazenamento! 

