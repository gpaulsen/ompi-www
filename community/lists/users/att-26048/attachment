<html>
<head>
<style><!--
.hmmessage P
{
margin:0px;
padding:0px
}
body.hmmessage
{
font-size: 12pt;
font-family:Calibri
}
--></style></head>
<body class='hmmessage'><div dir='ltr'>Dear Brice, the BIOS is the most latest. However, i wonder if this could be&nbsp; a hardware error, as openmpi sources claim.&nbsp; Is there any way to find out if this is a hardware error?<br><br>Thanks<br><br><br><div>&gt; From: users-request@open-mpi.org<br>&gt; Subject: users Digest, Vol 3074, Issue 1<br>&gt; To: users@open-mpi.org<br>&gt; Date: Sat, 20 Dec 2014 12:00:02 -0500<br>&gt; <br>&gt; Send users mailing list submissions to<br>&gt; 	users@open-mpi.org<br>&gt; <br>&gt; To subscribe or unsubscribe via the World Wide Web, visit<br>&gt; 	http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; or, via email, send a message with subject or body 'help' to<br>&gt; 	users-request@open-mpi.org<br>&gt; <br>&gt; You can reach the person managing the list at<br>&gt; 	users-owner@open-mpi.org<br>&gt; <br>&gt; When replying, please edit your Subject line so it is more specific<br>&gt; than "Re: Contents of users digest..."<br>&gt; <br>&gt; <br>&gt; Today's Topics:<br>&gt; <br>&gt;    1. Re: Deadlock in OpenMPI 1.8.3 and PETSc 3.4.5<br>&gt;       (Jeff Squyres (jsquyres))<br>&gt;    2. Hwloc error with Openmpi 1.8.3 on AMD 64 (Sergio Manzetti)<br>&gt;    3. Re: Hwloc error with Openmpi 1.8.3 on AMD 64 (Brice Goglin)<br>&gt;    4. best function to send data (Diego Avesani)<br>&gt; <br>&gt; <br>&gt; ----------------------------------------------------------------------<br>&gt; <br>&gt; Message: 1<br>&gt; Date: Fri, 19 Dec 2014 19:26:58 +0000<br>&gt; From: "Jeff Squyres (jsquyres)" &lt;jsquyres@cisco.com&gt;<br>&gt; To: "Open MPI User's List" &lt;users@open-mpi.org&gt;<br>&gt; Cc: "petsc-maint@mcs.anl.gov" &lt;petsc-maint@mcs.anl.gov&gt;<br>&gt; Subject: Re: [OMPI users] Deadlock in OpenMPI 1.8.3 and PETSc 3.4.5<br>&gt; Message-ID: &lt;027AB453-DE85-4F08-BDD7-A676CA90E239@cisco.com&gt;<br>&gt; Content-Type: text/plain; charset="us-ascii"<br>&gt; <br>&gt; On Dec 19, 2014, at 10:44 AM, George Bosilca &lt;bosilca@icl.utk.edu&gt; wrote:<br>&gt; <br>&gt; &gt; Regarding your second point, while I do tend to agree that such issue is better addressed in the MPI Forum, the last attempt to fix this was certainly not a resounding success.<br>&gt; <br>&gt; Yeah, fair enough -- but it wasn't a failure, either.  It could definitely be moved forward, but it will take time/effort, which I unfortunately don't have. I would be willing, however, to spin up someone who *does* have time/effort available to move the proposal forward.<br>&gt; <br>&gt; &gt; Indeed, there is a slight window of opportunity for inconsistencies in the recursive behavior.<br>&gt; <br>&gt; You're right; it's a small window in the threading case, but a) that's the worst kind :-), and b) the non-threaded case is actually worse (because the global state can change from underneath the loop).<br>&gt; <br>&gt; &gt; But the inconsistencies were already in the code, especially in the single threaded case. As we never received any complaints related to this topic I did not deemed interesting to address them with my last commit. Moreover, the specific behavior needed by PETSc is available in Open MPI when compiled without thread support, as the only thing that "protects" the attributes is that global mutex.<br>&gt; <br>&gt; Mmmm.  Ok, I see your point.  But this is a (very) slippery slope.<br>&gt; <br>&gt; &gt; For example, in ompi_attr_delete_all(), it gets the count of all attributes and then loops &lt;count&gt; times to delete each attribute.  But each attribute callback can now insert or delete attributes on that entity.  This can mean that the loop can either fail to delete an attribute (because some attribute callback already deleted it) or fail to delete *all* attributes (because some attribute callback added more).<br>&gt; &gt; <br>&gt; &gt; To be extremely precise the deletion part is always correct<br>&gt; <br>&gt; ...as long as the hash map is not altered from the application (e.g., by adding or deleting another attribute during a callback).<br>&gt; <br>&gt; I understand that you mention above that you're not worried about this case.  I'm just picking here because there is quite definitely a case where the loop is *not* correct.  PETSc apparently doesn't trigger this badness, but... like I said above, it's a (very) slippery slope.<br>&gt; <br>&gt; &gt; as it copies the values to be deleted into a temporary array before calling any callbacks (and before releasing the mutex), so we only remove what was in the object attribute hash when the function was called. Don't misunderstand we have an extremely good reason to do it this way, we need to call the callbacks in the order in which they were created (mandated by the MPI standard).<br>&gt; &gt;  <br>&gt; &gt; ompi_attr_copy_all() has similar problems -- in general, the hash that it is looping over can change underneath it.<br>&gt; &gt; <br>&gt; &gt; For the copy it is a little bit more tricky, as the calling order is not imposed. Our peculiar implementation of the hash table (with array) makes the code work, with a single (possible minor) exception when the hash table itself is grown between 2 calls. However, as stated before this issue was already present in the code in single threaded cases for years. Addressing it is another 2 line patch, but I leave this exercise to an interested reader.<br>&gt; <br>&gt; Yeah, thanks for that.  :-)<br>&gt; <br>&gt; To be clear: both the copy and the delete code could be made thread safe.  I just don't think we should be encouraging users to be exercising undefined / probably not-portable MPI code.<br>&gt; <br>&gt; -- <br>&gt; Jeff Squyres<br>&gt; jsquyres@cisco.com<br>&gt; For corporate legal information go to: http://www.cisco.com/web/about/doing_business/legal/cri/<br>&gt; <br>&gt; <br>&gt; <br>&gt; ------------------------------<br>&gt; <br>&gt; Message: 2<br>&gt; Date: Fri, 19 Dec 2014 20:58:46 +0100<br>&gt; From: Sergio Manzetti &lt;sergio.manzetti@outlook.com&gt;<br>&gt; To: "users@open-mpi.org" &lt;users@open-mpi.org&gt;<br>&gt; Subject: [OMPI users] Hwloc error with Openmpi 1.8.3 on AMD 64<br>&gt; Message-ID: &lt;DUB126-W2190E22E21596A1B834CF4E36B0@phx.gbl&gt;<br>&gt; Content-Type: text/plain; charset="iso-8859-1"<br>&gt; <br>&gt; <br>&gt; <br>&gt; <br>&gt; <br>&gt; <br>&gt; Dear all, when trying to run NWchem with openmpi, I get this error.<br>&gt;  <br>&gt;  <br>&gt;  <br>&gt; ****************************************************************************<br>&gt; * Hwloc has encountered what looks like an error from the operating system.<br>&gt; *<br>&gt; * object intersection without inclusion!<br>&gt; * Error occurred in topology.c line 594<br>&gt; *<br>&gt; * Please report this error message to the hwloc user's mailing list,<br>&gt; * along with the output from the hwloc-gather-topology.sh script.<br>&gt;  <br>&gt; Is there any rationale for solving this?<br>&gt;  <br>&gt; Thanks<br>&gt;  		 	   		   		 	   		  <br>&gt; -------------- next part --------------<br>&gt; HTML attachment scrubbed and removed<br>&gt; <br>&gt; ------------------------------<br>&gt; <br>&gt; Message: 3<br>&gt; Date: Fri, 19 Dec 2014 21:13:19 +0100<br>&gt; From: Brice Goglin &lt;Brice.Goglin@inria.fr&gt;<br>&gt; To: Open MPI Users &lt;users@open-mpi.org&gt;<br>&gt; Subject: Re: [OMPI users] Hwloc error with Openmpi 1.8.3 on AMD 64<br>&gt; Message-ID: &lt;549486DF.50405@inria.fr&gt;<br>&gt; Content-Type: text/plain; charset="windows-1252"<br>&gt; <br>&gt; Hello,<br>&gt; <br>&gt; The rationale is to read the message and do what it says :)<br>&gt; <br>&gt; Have a look at<br>&gt;     www.open-mpi.org/projects/hwloc/doc/v1.10.0/a00028.php#faq_os_error<br>&gt; Try upgrading your BIOS and kernel.<br>&gt; <br>&gt; Otherwise install hwloc and send the output (tarball) of<br>&gt; hwloc-gather-topology to hwloc-users (not to OMPI users).<br>&gt; <br>&gt; thanks<br>&gt; Brice<br>&gt; <br>&gt; <br>&gt; <br>&gt; Le 19/12/2014 20:58, Sergio Manzetti a ?crit :<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt; Dear all, when trying to run NWchem with openmpi, I get this error.<br>&gt; &gt;  <br>&gt; &gt;  <br>&gt; &gt;  <br>&gt; &gt; ****************************************************************************<br>&gt; &gt; * Hwloc has encountered what looks like an error from the operating<br>&gt; &gt; system.<br>&gt; &gt; *<br>&gt; &gt; * object intersection without inclusion!<br>&gt; &gt; * Error occurred in topology.c line 594<br>&gt; &gt; *<br>&gt; &gt; * Please report this error message to the hwloc user's mailing list,<br>&gt; &gt; * along with the output from the hwloc-gather-topology.sh script.<br>&gt; &gt;  <br>&gt; &gt; Is there any rationale for solving this?<br>&gt; &gt;  <br>&gt; &gt; Thanks<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt; _______________________________________________<br>&gt; &gt; users mailing list<br>&gt; &gt; users@open-mpi.org<br>&gt; &gt; Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt; Link to this post: http://www.open-mpi.org/community/lists/users/2014/12/26045.php<br>&gt; <br>&gt; -------------- next part --------------<br>&gt; HTML attachment scrubbed and removed<br>&gt; <br>&gt; ------------------------------<br>&gt; <br>&gt; Message: 4<br>&gt; Date: Fri, 19 Dec 2014 23:56:36 +0100<br>&gt; From: Diego Avesani &lt;diego.avesani@gmail.com&gt;<br>&gt; To: Open MPI Users &lt;users@open-mpi.org&gt;<br>&gt; Subject: [OMPI users] best function to send data<br>&gt; Message-ID:<br>&gt; 	&lt;CAG8o1y4B0uwYdTrB+SwdBra4Tbk6ih5tOeYpga8b6vs-OtYY9g@mail.gmail.com&gt;<br>&gt; Content-Type: text/plain; charset="utf-8"<br>&gt; <br>&gt; dear all users,<br>&gt; I am new in MPI world.<br>&gt; I would like to know what is the best choice and meaning between different<br>&gt; function.<br>&gt; <br>&gt; In my program I would like that each process send a vector of data to all<br>&gt; the other process. What do you suggest?<br>&gt; Is it correct MPI_Bcast or I am missing something?<br>&gt; <br>&gt; Thanks a lot<br>&gt; <br>&gt; Diego<br>&gt; -------------- next part --------------<br>&gt; HTML attachment scrubbed and removed<br>&gt; <br>&gt; ------------------------------<br>&gt; <br>&gt; Subject: Digest Footer<br>&gt; <br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; users@open-mpi.org<br>&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; <br>&gt; ------------------------------<br>&gt; <br>&gt; End of users Digest, Vol 3074, Issue 1<br>&gt; **************************************<br></div> 		 	   		  </div></body>
</html>
