Hello,<br>
<br>
we have been hit observing a strange behavior with OpenMPI 1.6.3<br>
<br>
    strace -f /share/apps/openmpi/1.6.3/bin/mpiexec -n 2<br>
--nooversubscribe --display-allocation --display-map --tag-output<br>
/share/apps/gamess/2011R1/gamess.2011R1.x<br>
/state/partition1/rmurri/29515/exam01.F05 -scr<br>
/state/partition1/rmurri/29515<br>
<br>
    ======================   ALLOCATED NODES   ======================<br>
<br>
     Data for node: nh64-1-17.local Num slots: 0    Max slots: 0<br>
     Data for node: nh64-1-17       Num slots: 2    Max slots: 0<br>
<br>
    =================================================================<br>
<br>
     ========================   JOB MAP   ========================<br>
<br>
     Data for node: nh64-1-17       Num procs: 2<br>
            Process OMPI jobid: [37108,1] Process rank: 0<br>
            Process OMPI jobid: [37108,1] Process rank: 1<br>
<br>
     =============================================================<br>
<br>
As you can see, the host file lists the *unqualified* local host name;<br>
OpenMPI fails to recognize that as the same host where it is running,<br>
and uses `ssh` to spawn a remote `orted`, as use of `strace -f` shows:<br>
<br>
    Process 16552 attached<br>
    [pid 16552] execve(&quot;//usr/bin/ssh&quot;, [&quot;/usr/bin/ssh&quot;, &quot;-x&quot;,<br>
&quot;nh64-1-17&quot;, &quot;OPAL_PREFIX=/share/apps/openmpi/1.6.3 ; export<br>
OPAL_PREFIX; PATH=/share/apps/openmpi/1.6.3/bin:$PATH ; export PATH ;<br>
LD_LIBRARY_PATH=/share/apps/openmpi/1.6.3/lib:$LD_LIBRARY_PATH ;<br>
export LD_LIBRARY_PATH ;<br>
DYLD_LIBRARY_PATH=/share/apps/openmpi/1.6.3/lib:$&quot;, &quot;--daemonize&quot;,<br>
&quot;-mca&quot;, &quot;ess&quot;, &quot;env&quot;, &quot;-mca&quot;, &quot;orte_ess_jobid&quot;, &quot;2431909888&quot;, &quot;-mca&quot;,<br>
&quot;orte_ess_vpid&quot;, &quot;1&quot;, &quot;-mca&quot;, &quot;orte_ess_num_procs&quot;, &quot;2&quot;, &quot;--hnp-uri&quot;,<br>
&quot;\&quot;2431909888.0;tcp://<a href="http://10.1.255.237:33154" target="_blank">10.1.255.237:33154</a>\&quot;&quot;, &quot;-mca&quot;, &quot;plm&quot;, &quot;rsh&quot;],<br>
[&quot;OLI235=/state/partition1/rmurri/29515/exam01.F235&quot;, ...<br>
<br>
If the machine file lists the FQDNs instead, `mpiexec` spawns the jobs<br>
directly via fork()/exec().<br>
<br>
This seems related to the fact that each compute node advertises<br>
127.0.1.1 as the IP address associated to its hostname:<br>
<br>
    $ ssh nh64-1-17 getent hosts nh64-1-17<br>
    127.0.1.1    nh64-1-17.local nh64-1-17<br>
<br>
Indeed, if I change /etc/hosts so that a compute node associates a<br>
&quot;real&quot; IP with its hostname, `mpiexec` works as expected.<br>
<br>
Is this a known feature/bug/easter egg?<br>
<br>
For the record: using OpenMPI 1.6.3 on Rocks 5.2.<br>
<br>
Thanks,<br>on behalf of the GC3 Team<br>Sergio :)<br><br><font face="Default Monospace,Courier New,Courier,monospace">GC3: Grid Computing Competence Center<br><a target="_blank" href="http://www.gc3.uzh.ch/">http://www.gc3.uzh.ch/</a><br>
University of Zurich<br>Winterthurerstrasse 190<br>CH-8057 Zurich Switzerland<br>Tel: +41 44 635 4222<br>Fax: +41 44 635 6888</font><br>

