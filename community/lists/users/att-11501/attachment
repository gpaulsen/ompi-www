<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">Mark,&nbsp;<div><br></div><div>Exciting.. SOLVED.. There is an open ticket #2043 regarding Nehelem/OpenMPI/Hang problem&nbsp;(<a href="https://svn.open-mpi.org/trac/ompi/ticket/2043">https://svn.open-mpi.org/trac/ompi/ticket/2043</a>).. Seems like the problem might be specific to gcc4.4x and OMPI &lt;1.3.2.. It seems like there is a group up us with dual socket nehalems trying to use ompi without much luck (or at least not without headaches)..&nbsp;</div><div><br></div><div>Of note,&nbsp;mca btl_sm_num_fifos 7 seems to work as well..</div><div><br></div><div>now off to see if I can get some real code to work...&nbsp;</div><div><br></div><div>Thanks, Mark, Gus, and the rest of the OMPI Users Group!</div><div><br></div><div><br></div><div><br></div><div><br></div><div><br><div><div>On Dec 10, 2009, at 7:42 AM, Mark Bolstad wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><br>Just a quick interjection, I also have a dual-quad Nehalem system, HT on, 24GB ram, hand compiled 1.3.4 with options: --enable-mpi-threads --enable-mpi-f77=no --with-openib=no<br><br>With v1.3.4 I see roughly the same behavior, hello, ring work, connectivity fails randomly with np &gt;= 8. Turning on -v increased the success, but still hangs. np = 16 fails more often, and the hang  is random in which pair of processes are communicating.<br>
<br>However, it seems to be related to the shared memory layer problem. Running with -mca btl ^sm works consistently through np = 128.<br><br>Hope this helps.<br><br>Mark<br><br><div class="gmail_quote">On Wed, Dec 9, 2009 at 8:03 PM, Gus Correa <span dir="ltr">&lt;<a href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">Hi Matthew<br>
<br>
Save any misinterpretation I may have made of the code:<br>
<br>
Hello_c has no real communication, except for a final Barrier<br>
synchronization.<br>
Each process prints "hello world" and that's it.<br>
<br>
Ring probes a little more, with processes Send(ing) and<br>
Recv(cieving) messages.<br>
Ring just passes a message sequentially along all process<br>
ranks, then back to rank 0, and repeat the game 10 times.<br>
Rank 0 is in charge of counting turns, decrementing the counter,<br>
and printing that (nobody else prints).<br>
With 4 processes:<br>
0-&gt;1-&gt;2-&gt;3-&gt;0-&gt;1... 10 times<br>
<br>
In connectivity every pair of processes exchange a message.<br>
Therefore it probes all pairwise connections.<br>
In verbose mode you can see that.<br>
<br>
These programs shouldn't hang at all, if the system were sane.<br>
Actually, they should even run with a significant level of<br>
oversubscription, say,<br>
-np 128 &nbsp;should work easily for all three programs on a powerful<br>
machine like yours.<br>
<br>
<br>
**<br>
<br>
Suggestions<br>
<br>
1) Stick to the OpenMPI you compiled.<br>
<br>
**<br>
<br>
2) You can run connectivity_c in verbose mode:<br>
<br>
home/macmanes/apps/openmpi1.4/bin/mpirun -np 8 connectivity_c -v<br>
<br>
(Note the trailing "-v".)<br>
<br>
It should tell more about who's talking to who.<br>
<br>
**<br>
<br>
3) I wonder if there are any BIOS settings that may be required<br>
(and perhaps not in place) to make the Nehalem hyperthreading to<br>
work properly in your computer.<br>
<br>
You reach the BIOS settings by typing &lt;DEL&gt; or &lt;F2&gt;<br>
when the computer boots up.<br>
The key varies by<br>
BIOS and computer vendor, but shows quickly on the bootup screen.<br>
<br>
You may ask the computer vendor about the recommended BIOS settings.<br>
If you haven't done this before, be careful to change and save only<br>
what really needs to change (if anything really needs to change),<br>
or the result may be worse.<br>
(Overclocking is for gamers, not for genome researchers ... :) )<br>
<br>
**<br>
<br>
4) What I read about Nehalem DDR3 memory is that it is optimal<br>
on configurations that are multiples of 3GB per CPU.<br>
Common configs. in dual CPU machines like yours are<br>
6, 12, 24 and 48GB.<br>
The sockets where you install the memory modules also matter.<br>
<br>
Your computer has 20GB.<br>
Did you build the computer or upgrade the memory yourself?<br>
Do you know how the memory is installed, in which memory sockets?<br>
What does the vendor have to say about it?<br>
<br>
See this:<br>
<a href="http://en.community.dell.com/blogs/dell_tech_center/archive/2009/04/08/nehalem-and-memory-configurations.aspx" target="_blank">http://en.community.dell.com/blogs/dell_tech_center/archive/2009/04/08/nehalem-and-memory-configurations.aspx</a><br>

<br>
**<br>
<br>
5) As I said before, typing "f" then "j" on "top" will add<br>
a column (labeled "P") that shows in which core each process is running.<br>
This will let you observe how the Linux scheduler is distributing<br>
the MPI load across the cores.<br>
Hopefully it is load-balanced, and different processes go to different<br>
cores.<br>
<br>
***<br>
<br>
It is very disconcerting when MPI processes hang.<br>
You are not alone.<br>
The reasons are not always obvious.<br>
At least in your case there is no network involved or to troubleshoot.<br>
<br>
<br>
**<br>
<br>
I hope it helps,<div class="im"><br>
Gus Correa<br>
---------------------------------------------------------------------<br>
Gustavo Correa<br>
Lamont-Doherty Earth Observatory - Columbia University<br>
Palisades, NY, 10964-8000 - USA<br>
---------------------------------------------------------------------<br>
<br>
<br>
<br>
<br>
<br>
Matthew MacManes wrote:<br>
</div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div><div></div><div class="h5">
Hi Gus and List,<br>
<br>
1st of all Gus, I want to say thanks.. you have been a huge help, and when I get this fixed, I owe you big time!<br>
<br>
However, the problems continue...<br>
<br>
I formatted the HD, reinstalled OS to make sure that I was working from scratch. &nbsp;I did your step A, which seemed to go fine:<br>
<br>
macmanes@macmanes:~$ which mpicc<br>
/home/macmanes/apps/openmpi1.4/bin/mpicc<br>
macmanes@macmanes:~$ which mpirun<br>
/home/macmanes/apps/openmpi1.4/bin/mpirun<br>
<br>
Good stuff there...<br>
<br>
I then compiled the example files:<br>
<br>
macmanes@macmanes:~/Downloads/openmpi-1.4/examples$ /home/macmanes/apps/openmpi1.4/bin/mpirun -np 8 ring_c<br>
Process 0 sending 10 to 1, tag 201 (8 processes in ring)<br>
Process 0 sent to 1<br>
Process 0 decremented value: 9<br>
Process 0 decremented value: 8<br>
Process 0 decremented value: 7<br>
Process 0 decremented value: 6<br>
Process 0 decremented value: 5<br>
Process 0 decremented value: 4<br>
Process 0 decremented value: 3<br>
Process 0 decremented value: 2<br>
Process 0 decremented value: 1<br>
Process 0 decremented value: 0<br>
Process 0 exiting<br>
Process 1 exiting<br>
Process 2 exiting<br>
Process 3 exiting<br>
Process 4 exiting<br>
Process 5 exiting<br>
Process 6 exiting<br>
Process 7 exiting<br>
macmanes@macmanes:~/Downloads/openmpi-1.4/examples$ /home/macmanes/apps/openmpi1.4/bin/mpirun -np 8 connectivity_c<br>
Connectivity test on 8 processes PASSED.<br>
macmanes@macmanes:~/Downloads/openmpi-1.4/examples$ /home/macmanes/apps/openmpi1.4/bin/mpirun -np 8 connectivity_c<br>
..HANGS..NO OUTPUT<br>
<br>
this is maddening because ring_c works.. and connectivity_c worked the 1st time, but not the second... I did it 10 times, and it worked twice.. here is the TOP screenshot:<br>
<br>
<a href="http://picasaweb.google.com/macmanes/DropBox?authkey=Gv1sRgCLKokNOVqo7BYw#5413382182027669394" target="_blank">http://picasaweb.google.com/macmanes/DropBox?authkey=Gv1sRgCLKokNOVqo7BYw#5413382182027669394</a><br>

<br>
What is the difference between connectivity_c and ring_c? Under what circumstances should one fail and not the other...<br>
<br>
I'm off to the Linux forums to see about the Nehalem kernel issues..<br>
<br>
Matt<br>
<br>
<br>
<br></div></div><div><div></div><div class="h5">
On Wed, Dec 9, 2009 at 13:25, Gus Correa &lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a> &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;&gt; wrote:<br>

<br>
 &nbsp; &nbsp;Hi Matthew<br>
<br>
 &nbsp; &nbsp;There is no point in trying to troubleshoot MrBayes and ABySS<br>
 &nbsp; &nbsp;if not even the OpenMPI test programs run properly.<br>
 &nbsp; &nbsp;You must straighten them out first.<br>
<br>
 &nbsp; &nbsp;**<br>
<br>
 &nbsp; &nbsp;Suggestions:<br>
<br>
 &nbsp; &nbsp;**<br>
<br>
 &nbsp; &nbsp;A) While you are at OpenMPI, do yourself a favor,<br>
 &nbsp; &nbsp;and install it from source on a separate directory.<br>
 &nbsp; &nbsp;Who knows if the OpenMPI package distributed with Ubuntu<br>
 &nbsp; &nbsp;works right on Nehalem?<br>
 &nbsp; &nbsp;Better install OpenMPI yourself from source code.<br>
 &nbsp; &nbsp;It is not a big deal, and may save you further trouble.<br>
<br>
 &nbsp; &nbsp;Recipe:<br>
<br>
 &nbsp; &nbsp;1) Install gfortran and g++ if you don't have them using apt-get.<br>
 &nbsp; &nbsp;2) Put the OpenMPI tarball in, say /home/matt/downolads/openmpi<br>
 &nbsp; &nbsp;3) Make another install directory *not in the system directory tree*.<br>
 &nbsp; &nbsp;Something like "mkdir /home/matt/apps/openmpi-X.Y.Z/" (X.Y.Z=version)<br>
 &nbsp; &nbsp;will work<br>
 &nbsp; &nbsp;4) cd /home/matt/downolads/openmpi<br>
 &nbsp; &nbsp;5) ./configure CC=gcc CXX=g++ F77=gfortran FC=gfortran &nbsp;\<br>
 &nbsp; &nbsp;--prefix=/home/matt/apps/openmpi-X.Y.Z<br>
 &nbsp; &nbsp;(Use the prefix flag to install in the directory of item 3.)<br>
 &nbsp; &nbsp;6) make<br>
 &nbsp; &nbsp;7) make install<br>
 &nbsp; &nbsp;8) At the bottom of your /home/matt/.bashrc or .profile file<br>
 &nbsp; &nbsp;put these lines:<br>
<br>
 &nbsp; &nbsp;export PATH=/home/matt/apps/openmpi-X.Y.Z/bin:${PATH}<br>
 &nbsp; &nbsp;export MANPATH=/home/matt/apps/openmpi-X.Y.Z/share/man:`man -w`<br>
 &nbsp; &nbsp;export<br>
 &nbsp; &nbsp;LD_LIBRARY_PATH=home/matt/apps/openmpi-X.Y.Z/lib:${LD_LIBRARY_PATH}<br>
<br>
 &nbsp; &nbsp;(If you use csh/tcsh use instead:<br>
 &nbsp; &nbsp;setenv PATH /home/matt/apps/openmpi-X.Y.Z/bin:${PATH}<br>
 &nbsp; &nbsp;etc)<br>
<br>
 &nbsp; &nbsp;9) Logout and login again to freshen um the environment variables.<br>
 &nbsp; &nbsp;10) Do "which mpicc" &nbsp;to check that it is pointing to your newly<br>
 &nbsp; &nbsp;installed OpenMPI.<br>
 &nbsp; &nbsp;11) Recompile and rerun the OpenMPI test programs<br>
 &nbsp; &nbsp;with 2, 4, 8, 16, .... processors.<br>
 &nbsp; &nbsp;Use full path names to mpicc and to mpirun,<br>
 &nbsp; &nbsp;if the change of PATH above doesn't work right.<br>
<br>
 &nbsp; &nbsp;********<br>
<br>
 &nbsp; &nbsp;B) Nehalem is quite new hardware.<br>
 &nbsp; &nbsp;I don't know if the Ubuntu kernel 2.6.31-16 fully supports all<br>
 &nbsp; &nbsp;of Nehalem features, particularly hyperthreading, and NUMA,<br>
 &nbsp; &nbsp;which are used by MPI programs.<br>
 &nbsp; &nbsp;I am not the right person to give you advice about this.<br>
 &nbsp; &nbsp;I googled out but couldn't find a clear information about<br>
 &nbsp; &nbsp;minimal kernel age/requirements to have Nehalem fully supported.<br>
 &nbsp; &nbsp;Some Nehalem owner in the list could come forward and tell.<br>
<br>
 &nbsp; &nbsp;**<br>
<br>
 &nbsp; &nbsp;C) On the top screenshot you sent me, please try it again<br>
 &nbsp; &nbsp;(after you do item A) but type "f" and "j" to show the processors<br>
 &nbsp; &nbsp;that are running each process.<br>
<br>
 &nbsp; &nbsp;**<br>
<br>
 &nbsp; &nbsp;D) Also, the screeshot shows 20GB of memory.<br>
 &nbsp; &nbsp;This sounds not as a optimal memory for Nehalem,<br>
 &nbsp; &nbsp;which tend to be 6GB, 12GB, 24GB, 48GB.<br>
 &nbsp; &nbsp;Did you put together the system, or upgraded the memory yourself,<br>
 &nbsp; &nbsp;of did you buy the computer as is?<br>
 &nbsp; &nbsp;However, this should not break MPI anyway.<br>
<br>
 &nbsp; &nbsp;**<br>
<br>
 &nbsp; &nbsp;E) Answering your question:<br>
 &nbsp; &nbsp;It is true that different flavors of MPI<br>
 &nbsp; &nbsp;used to compile (mpicc) and run (mpiexec) a program would probably<br>
 &nbsp; &nbsp;break right away, regardless of the number of processes.<br>
 &nbsp; &nbsp;However, when it comes to different versions of the<br>
 &nbsp; &nbsp;same MPI flavor (say OpenMPI 1.3.4 and OpenMPI 1.3.3)<br>
 &nbsp; &nbsp;I am not sure it will break.<br>
 &nbsp; &nbsp;I would guess it may run but not in a reliable way.<br>
 &nbsp; &nbsp;Problems may appear as you stress the system with more cores, etc.<br>
 &nbsp; &nbsp;But this is just a guess.<br>
<br>
 &nbsp; &nbsp;**<br>
<br>
 &nbsp; &nbsp;I hope this helps,<br>
<br>
 &nbsp; &nbsp;Gus Correa<br>
 &nbsp; &nbsp;---------------------------------------------------------------------<br>
 &nbsp; &nbsp;Gustavo Correa<br>
 &nbsp; &nbsp;Lamont-Doherty Earth Observatory - Columbia University<br>
 &nbsp; &nbsp;Palisades, NY, 10964-8000 - USA<br>
 &nbsp; &nbsp;---------------------------------------------------------------------<br>
<br>
<br>
 &nbsp; &nbsp;Matthew MacManes wrote:<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp;Hi Gus,<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp;Interestingly the results for the connectivity_c test... works<br>
 &nbsp; &nbsp; &nbsp; &nbsp;fine with -np &lt;8. For -np &gt;8 it works some of the time, other<br>
 &nbsp; &nbsp; &nbsp; &nbsp;times it HANGS. I have got to believe that this is a big clue!!<br>
 &nbsp; &nbsp; &nbsp; &nbsp;Also, when it hangs, sometimes I get the message "mpirun was<br>
 &nbsp; &nbsp; &nbsp; &nbsp;unable to cleanly terminate the daemons on the nodes shown<br>
 &nbsp; &nbsp; &nbsp; &nbsp;below" Note that NO nodes are shown below. &nbsp; Once, I got -np 250<br>
 &nbsp; &nbsp; &nbsp; &nbsp;to pass the connectivity test, but I was not able to replicate<br>
 &nbsp; &nbsp; &nbsp; &nbsp;this reliable, so I'm not sure if it was a fluke, or what. &nbsp;Here<br>
 &nbsp; &nbsp; &nbsp; &nbsp;is a like to a screenshop of TOP when connectivity_c is hung<br>
 &nbsp; &nbsp; &nbsp; &nbsp;with -np 14.. I see that 2 processes are only at 50% CPU usage..<br>
 &nbsp; &nbsp; &nbsp; &nbsp;Hmmmm &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href="http://picasaweb.google.com/lh/photo/87zVEucBNFaQ0TieNVZtdw?authkey=Gv1sRgCLKokNOVqo7BYw&amp;feat=directlink" target="_blank">http://picasaweb.google.com/lh/photo/87zVEucBNFaQ0TieNVZtdw?authkey=Gv1sRgCLKokNOVqo7BYw&amp;feat=directlink</a><br>

 &nbsp; &nbsp; &nbsp; &nbsp;&lt;<a href="http://picasaweb.google.com/lh/photo/87zVEucBNFaQ0TieNVZtdw?authkey=Gv1sRgCLKokNOVqo7BYw&amp;feat=directlink" target="_blank">http://picasaweb.google.com/lh/photo/87zVEucBNFaQ0TieNVZtdw?authkey=Gv1sRgCLKokNOVqo7BYw&amp;feat=directlink</a>&gt;<br>

 &nbsp; &nbsp; &nbsp; &nbsp;&lt;<a href="http://picasaweb.google.com/lh/photo/87zVEucBNFaQ0TieNVZtdw?authkey=Gv1sRgCLKokNOVqo7BYw&amp;feat=directlink" target="_blank">http://picasaweb.google.com/lh/photo/87zVEucBNFaQ0TieNVZtdw?authkey=Gv1sRgCLKokNOVqo7BYw&amp;feat=directlink</a><br>

 &nbsp; &nbsp; &nbsp; &nbsp;&lt;<a href="http://picasaweb.google.com/lh/photo/87zVEucBNFaQ0TieNVZtdw?authkey=Gv1sRgCLKokNOVqo7BYw&amp;feat=directlink" target="_blank">http://picasaweb.google.com/lh/photo/87zVEucBNFaQ0TieNVZtdw?authkey=Gv1sRgCLKokNOVqo7BYw&amp;feat=directlink</a>&gt;&gt;<br>

<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp;The other tests, ring_c, hello_c, as well as the cxx versions of<br>
 &nbsp; &nbsp; &nbsp; &nbsp;these guys with with all values of -np.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp;Using -mca mpi-paffinity_alone 1 I get the same behavior.<br>
 &nbsp; &nbsp; &nbsp; &nbsp;I agree that I am should worry about the mismatch between where<br>
 &nbsp; &nbsp; &nbsp; &nbsp;the libraries are installed versus where I am telling my<br>
 &nbsp; &nbsp; &nbsp; &nbsp;programs to look for them. Would this type of mismatch cause<br>
 &nbsp; &nbsp; &nbsp; &nbsp;behavior like what I am seeing, i.e. working with &nbsp;a small<br>
 &nbsp; &nbsp; &nbsp; &nbsp;number of processors, but failing with larger? &nbsp;It seems like a<br>
 &nbsp; &nbsp; &nbsp; &nbsp;mismatch would have the same effect regardless of the number of<br>
 &nbsp; &nbsp; &nbsp; &nbsp;processors used. Maybe I am mistaken. Anyway, to address this,<br>
 &nbsp; &nbsp; &nbsp; &nbsp;which mpirun gives me /usr/local/bin/mpirun.. so to configure<br>
 &nbsp; &nbsp; &nbsp; &nbsp;./configure --with-mpi=/usr/local/bin/mpirun and to run<br>
 &nbsp; &nbsp; &nbsp; &nbsp;/usr/local/bin/mpirun -np X ... &nbsp;This should<br>
 &nbsp; &nbsp; &nbsp; &nbsp;uname -a gives me: Linux macmanes 2.6.31-16-generic #52-Ubuntu<br>
 &nbsp; &nbsp; &nbsp; &nbsp;SMP Thu Dec 3 22:07:16 UTC 2006 x86_64 GNU/Linux<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp;Matt<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp;On Dec 8, 2009, at 8:50 PM, Gus Correa wrote:<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Hi Matthew<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Please see comments/answers inline below.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Matthew MacManes wrote:<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Hi Gus, Thanks for your ideas.. I have a few questions,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;and will try to answer yours in hopes of solving this!!<br>
<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;A simple way to test OpenMPI on your system is to run the<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;test programs that come with the OpenMPI source code,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;hello_c.c, connectivity_c.c, and ring_c.c:<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href="http://www.open-mpi.org/" target="_blank">http://www.open-mpi.org/</a><br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Get the tarball from the OpenMPI site, gzip and untar it,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;and look for it in the "examples" directory.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Compile it with /your/path/to/openmpi/bin/mpicc hello_c.c<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Run it with /your/path/to/openmpi/bin/mpiexec -np X a.out<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;using X = 2, 4, 8, 16, 32, 64, ...<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;This will tell if your OpenMPI is functional,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;and if you can run on many Nehalem cores,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;even with oversubscription perhaps.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;It will also set the stage for further investigation of your<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;actual programs.<br>
<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Should I worry about setting things like --num-cores<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;--bind-to-cores? &nbsp;This, I think, gets at your questions<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;about processor affinity.. Am I right? I could not<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;exactly figure out the -mca mpi-paffinity_alone stuff...<br>
<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;I use the simple minded -mca mpi-paffinity_alone 1.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;This is probably the easiest way to assign a process to a core.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;There more complex &nbsp;ways in OpenMPI, but I haven't tried.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Indeed, -mca mpi-paffinity_alone 1 does improve performance of<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;our programs here.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;There is a chance that without it the 16 virtual cores of<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;your Nehalem get confused with more than 3 processes<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(you reported that -np &gt; 3 breaks).<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Did you try adding just -mca mpi-paffinity_alone 1 &nbsp;to<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;your mpiexec command line?<br>
<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1. Additional load: nope. nothing else, most of the time<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;not even firefox.<br>
<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Good.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Turn off firefox, etc, to make it even better.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Ideally, use runlevel 3, no X, like a computer cluster node,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;but this may not be required.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2. RAM: no problems apparent when monitoring through<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;TOP. Interesting, I did wonder about oversubscription,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;so I tried the option --nooversubscription, but this<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;gave me an error mssage.<br>
<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Oversubscription from your program would only happen if<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;you asked for more processes than available cores, i.e.,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-np &gt; 8 (or "virtual" cores, in case of Nehalem hyperthreading,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-np &gt; 16).<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Since you have -np=4 there is no oversubscription,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;unless you have other external load (e.g. Matlab, etc),<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;but you said you don't.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Yet another possibility would be if your program is threaded<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(e.g. using OpenMP along with MPI), but considering what you<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;said about OpenMP I would guess the programs don't use it.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;For instance, you launch the program with 4 MPI processes,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;and each process decides to start, say, 8 OpenMP threads.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;You end up with 32 threads and 8 (real) cores (or 16<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;hyperthreaded<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ones on Nehalem).<br>
<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;What else does top say?<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Any hog processes (memory- or CPU-wise)<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;besides your program processes?<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3. I have not tried other MPI flavors.. Ive been<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;speaking to the authors of the programs, and they are<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;both using openMPI. &nbsp;<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;I was not trying to convince you to use another MPI.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;I use MPICH2 also, but OpenMPI reigns here.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;The idea or trying it with MPICH2 was just to check whether<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;OpenMPI<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;is causing the problem, but I don't think it is.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4. I don't think that this is a problem, as I'm<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;specifying --with-mpi=/usr/bin/... &nbsp;when I compile the<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;programs. Is there any other way to be sure that this is<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;not a problem?<br>
<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Hmmm ....<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;I don't know about your Ubuntu (we have CentOS and Fedora on<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;various<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;machines).<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;However, most Linux distributions come with their MPI flavors,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;and so do compilers, etc.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Often times they install these goodies in unexpected places,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;and this has caused a lot of frustration.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;There are tons of postings on this list that eventually<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;boiled down to mismatched versions of MPI in unexpected places.<br>
<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;The easy way is to use full path names to compile and to run.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Something like this:<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;/my/openmpi/bin/mpicc on your program configuration script),<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;and something like this<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;/my/openmpi/bin/mpiexec -np &nbsp;... bla, bla ...<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;when you submit the job.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;You can check your version with "which mpicc", "which mpiexec",<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;and (perhaps using full path names) with<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"ompi_info", "mpicc --showme", "mpiexec --help".<br>
<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5. I had not been, and you could see some shuffling when<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;monitoring the load on specific processors. I have tried<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;to use --bind-to-cores to deal with this. I don't<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;understand how to use the -mca options you asked about.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;6. I am using Ubuntu 9.10. gcc 4.4.1 and g++ &nbsp;4.4.1<br>
<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;I am afraid I won't be of help, because I don't have Nehalem.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;However, I read about Nehalem requiring quite recent kernels<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;to get all of its features working right.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;What is the output of "uname -a"?<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;This will tell the kernel version, etc.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Other list subscribers may give you a suggestion if you post the<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;information.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MyBayes is a for bayesian phylogenetics:<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <a href="http://mrbayes.csit.fsu.edu/wiki/index.php/Main_Page" target="_blank">http://mrbayes.csit.fsu.edu/wiki/index.php/Main_Page</a><br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ABySS: is a program for assembly of DNA sequence data:<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href="http://www.bcgsc.ca/platform/bioinfo/software/abyss" target="_blank">http://www.bcgsc.ca/platform/bioinfo/software/abyss</a><br>
<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Thanks for the links!<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;I had found the MrBayes link.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;I eventually found what your ABySS was about, but no links.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Amazing that it is about DNA/gene sequencing.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Our abyss here is the deep ocean ... :)<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Abysmal difference!<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Do the programs mix MPI (message passing) with<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;OpenMP (threads)?<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Im honestly not sure what this means..<br>
<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Some programs mix the two.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;OpenMP only works in a shared memory environment (e.g. a single<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;computer like yours), whereas MPI can use both shared memory<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;and work across a network (e.g. in a cluster).<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;There are other differences too.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Unlikely that you have this hybrid type of parallel program,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;otherwise there would be some reference to OpenMP<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;on the very program configuration files, program<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;documentation, etc.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Also, in general the configuration scripts of these hybrid<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;programs can turn on MPI only, or OpenMP only, or both,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;depending on how you configure.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Even to compile with OpenMP you would need a proper compiler<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;flag, but that one might be hidden in a Makefile too, making<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;a bit hard to find. "grep -n mp Makefile" may give a clue.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Anything on the documentation that mentions threads or OpenMP?<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;FYI, here is OpenMP:<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href="http://openmp.org/wp/" target="_blank">http://openmp.org/wp/</a><br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Thanks for all your help!<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &gt; Matt<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Well, so far it didn't really help. :(<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;But let's hope to find a clue,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;maybe with a little help of<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;our list subscriber friends.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Gus Correa<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;---------------------------------------------------------------------<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Gustavo Correa<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Lamont-Doherty Earth Observatory - Columbia University<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Palisades, NY, 10964-8000 - USA<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;---------------------------------------------------------------------<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Hi Matthew<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;More guesses/questions than anything else:<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1) Is there any additional load on this machine?<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;We had problems like that (on different machines) when<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;users start listening to streaming video, doing<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Matlab calculations,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;etc, while the MPI programs are running.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;This tends to oversubscribe the cores, and may lead<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;to crashes.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2) RAM:<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Can you monitor the RAM usage through "top"?<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(I presume you are on Linux.)<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;It may show unexpected memory leaks, if they exist.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;On "top", type "1" (one) see all cores, type "f"<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;then "j"<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;to see the core number associated to each process.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3) Do the programs work right with other MPI flavors<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(e.g. MPICH2)?<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;If not, then it is not OpenMPI's fault.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4) Any possibility that the MPI versions/flavors of<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;mpicc and<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;mpirun that you are using to compile and launch the<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;program are not the<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;same?<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5) Are you setting processor affinity on mpiexec?<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;mpiexec -mca mpi_paffinity_alone 1 -np ... bla, bla ...<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Context switching across the cores may also cause<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;trouble, I suppose.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;6) Which Linux are you using (uname -a)?<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;On other mailing lists I read reports that only<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;quite recent kernels<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;support all the Intel Nehalem processor features well.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;I don't have Nehalem, I can't help here,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;but the information may be useful<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for other list subscribers to help you.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;***<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;As for the programs, some programs require specific<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;setup,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(and even specific compilation) when the number of<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MPI processes<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;vary.<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;It may help if you tell us a link to the program sites.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Baysian statistics is not totally out of our business,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;but phylogenetic genetic trees is not really my league,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;hence forgive me any bad guesses, please,<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;but would it need specific compilation or a different<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;set of input parameters to run correctly on a different<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;number of processors?<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Do the programs mix MPI (message passing) with<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;OpenMP (threads)?<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;I found this MrBayes, which seems to do the above:<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href="http://mrbayes.csit.fsu.edu/" target="_blank">http://mrbayes.csit.fsu.edu/</a><br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href="http://mrbayes.csit.fsu.edu/wiki/index.php/Main_Page" target="_blank">http://mrbayes.csit.fsu.edu/wiki/index.php/Main_Page</a><br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;As for the ABySS, what is it, where can it be found?<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Doesn't look like a deep ocean circulation model, as<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;the name suggest.<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;My $0.02<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Gus Correa<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;------------------------------------------------------------------------<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;_______________________________________________<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;users mailing list<br></div></div>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<div class="im"><br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;_______________________________________________<br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;users mailing list<br></div>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<div class="im"><br>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp;_________________________________<br>
 &nbsp; &nbsp; &nbsp; &nbsp;Matthew MacManes<br>
 &nbsp; &nbsp; &nbsp; &nbsp;PhD Candidate<br>
 &nbsp; &nbsp; &nbsp; &nbsp;University of California- Berkeley<br>
 &nbsp; &nbsp; &nbsp; &nbsp;Museum of Vertebrate Zoology<br>
 &nbsp; &nbsp; &nbsp; &nbsp;Phone: 510-495-5833<br>
 &nbsp; &nbsp; &nbsp; &nbsp;Lab Website: <a href="http://ib.berkeley.edu/labs/lacey" target="_blank">http://ib.berkeley.edu/labs/lacey</a><br>
 &nbsp; &nbsp; &nbsp; &nbsp;Personal Website: <a href="http://macmanes.com/" target="_blank">http://macmanes.com/</a><br>
<br>
<br>
<br>
<br>
<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp;------------------------------------------------------------------------<br>
<br>
 &nbsp; &nbsp; &nbsp; &nbsp;_______________________________________________<br>
 &nbsp; &nbsp; &nbsp; &nbsp;users mailing list<br></div>
 &nbsp; &nbsp; &nbsp; &nbsp;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<div class="im"><br>
 &nbsp; &nbsp; &nbsp; &nbsp;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
 &nbsp; &nbsp;_______________________________________________<br>
 &nbsp; &nbsp;users mailing list<br></div>
 &nbsp; &nbsp;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<div class="im"><br>
 &nbsp; &nbsp;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
------------------------------------------------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></blockquote><div><div></div><div class="h5">
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br><div>
<span class="Apple-style-span" style="border-collapse: separate; color: rgb(0, 0, 0); font-family: Optima; font-size: medium; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; "><span class="Apple-style-span" style="border-collapse: separate; color: rgb(0, 0, 0); font-family: Optima; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; "><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><div><div>_________________________________<br>Matthew MacManes<br>PhD Candidate<br>University of California- Berkeley<br>Museum of Vertebrate Zoology<br>Phone: 510-495-5833<br>Lab Website:&nbsp;<a href="http://ib.berkeley.edu/labs/lacey">http://ib.berkeley.edu/labs/lacey</a><br>Personal Website: <a href="http://macmanes.com/">http://macmanes.com/</a></div><div><br></div></div><br></div></span><br class="Apple-interchange-newline"></span><br class="Apple-interchange-newline">
</div>
<br></div></body></html>
