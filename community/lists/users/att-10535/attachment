I&#39;m receiving the error posted at the bottom of this message with a code compiled with Intel Fortran/C Version 11.1 against OpenMPI version 1.3.2.  <br><br>The same code works correctly when compiled against MPICH2.  (We have re-compiled with OpenMPI to take advantage of newly-installed Infiniband hardware.  The &quot;ring&quot; test problem appears to work correctly over Infiniband.)  <br>
<br>There are no &quot;fork()&quot; calls in our code, so I can only guess that something weird is going on with MPI_COMM_WORLD.  The code in question is a Fortran 90 code.  Right now, it is being compiled with &quot;include &#39;mpif.h&#39;&quot; statements at the beginning of each MPI subroutine, instead of  making use of the &quot;mpi&quot; modules.  Could this be causing the problem?  How else should I go about diagnosing the problem?<br>
<br>Thanks,<br>Greg<br><br>--------------------------------------------------------------------------<br>An MPI process has executed an operation involving a call to the<br>&quot;fork()&quot; system call to create a child process.  Open MPI is currently<br>
operating in a condition that could result in memory corruption or<br>other system errors; your MPI job may hang, crash, or produce silent<br>data corruption.  The use of fork() (or system() or other calls that<br>create child processes) is strongly discouraged.  <br>
<br>The process that invoked fork was:<br><br>  Local host:          bl316 (PID 26806)<br>  MPI_COMM_WORLD rank: 0<br><br>If you are *absolutely sure* that your application will successfully<br>and correctly survive a call to fork(), you may disable this warning<br>
by setting the mpi_warn_on_fork MCA parameter to 0.<br>--------------------------------------------------------------------------<br>[bl205:5014] *** An error occurred in MPI_Cart_create<br>[bl205:5014] *** on communicator MPI_COMM_WORLD<br>
[bl205:5014] *** MPI_ERR_ARG: invalid argument of some other kind<br>[bl205:5014] *** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br><br>--------------------------------------------------------------------------<br>
mpirun has exited due to process rank 4 with PID 5010 on<br>node bl205 exiting without calling &quot;finalize&quot;. This may<br>have caused other processes in the application to be<br>terminated by signals sent by mpirun (as reported here).<br>
--------------------------------------------------------------------------<br>[bl205:05008] 7 more processes have sent help message help-mpi-errors.txt / mpi_errors_are_fatal<br>[bl205:05008] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>

