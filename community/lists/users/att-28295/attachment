<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">This doesn’t provide info beyond the local node topology, so it won’t help answer the common switch question<div class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Jan 15, 2016, at 8:35 AM, Nick Papior &lt;<a href="mailto:nickpapior@gmail.com" class="">nickpapior@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><div dir="ltr" class="">Wouldn't this be partially available via&nbsp;<a href="https://github.com/open-mpi/ompi/pull/326" class="">https://github.com/open-mpi/ompi/pull/326</a> in the trunk?<div class=""><br class=""></div><div class="">Of course the switch is not gathered from this, but it might work as an initial step towards what you seek Matt?</div></div><div class="gmail_extra"><br class=""><div class="gmail_quote">2016-01-15 17:27 GMT+01:00 Ralph Castain <span dir="ltr" class="">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank" class="">rhc@open-mpi.org</a>&gt;</span>:<br class=""><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word" class="">Yes, we don’t propagate envars ourselves other than MCA params. You can ask mpirun to forward specific envars to every proc, but that would only push the same value to everyone, and that doesn’t sound like what you are looking for.<div class=""><br class=""></div><div class="">FWIW: we are working on adding the ability to directly query the info you are seeking - i.e., to ask for things like “which procs are on the same switch as me?”. Hoping to have it later this year, perhaps in the summer.</div><div class=""><br class=""></div><div class=""><br class=""><div class=""><blockquote type="cite" class=""><div class=""><div class="h5"><div class="">On Jan 15, 2016, at 7:56 AM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com" target="_blank" class="">fortran@gmail.com</a>&gt; wrote:</div><br class=""></div></div><div class=""><div class=""><div class="h5"><div dir="ltr" class="">Ralph,<div class=""><br class=""></div><div class="">That doesn't help:</div><div class=""><br class=""></div><div class=""><div class="">(1004) $ mpirun -map-by node -np 8 ./hostenv.x | sort -g -k2</div><div class="">Process &nbsp; &nbsp;0 of &nbsp; &nbsp;8 is on host borgo086</div><div class="">Process &nbsp; &nbsp;0 of &nbsp; &nbsp;8 is on processor borgo086</div><div class="">Process &nbsp; &nbsp;1 of &nbsp; &nbsp;8 is on host borgo086</div><div class="">Process &nbsp; &nbsp;1 of &nbsp; &nbsp;8 is on processor borgo140</div><div class="">Process &nbsp; &nbsp;2 of &nbsp; &nbsp;8 is on host borgo086</div><div class="">Process &nbsp; &nbsp;2 of &nbsp; &nbsp;8 is on processor borgo086</div><div class="">Process &nbsp; &nbsp;3 of &nbsp; &nbsp;8 is on host borgo086</div><div class="">Process &nbsp; &nbsp;3 of &nbsp; &nbsp;8 is on processor borgo140</div><div class="">Process &nbsp; &nbsp;4 of &nbsp; &nbsp;8 is on host borgo086</div><div class="">Process &nbsp; &nbsp;4 of &nbsp; &nbsp;8 is on processor borgo086</div><div class="">Process &nbsp; &nbsp;5 of &nbsp; &nbsp;8 is on host borgo086</div><div class="">Process &nbsp; &nbsp;5 of &nbsp; &nbsp;8 is on processor borgo140</div><div class="">Process &nbsp; &nbsp;6 of &nbsp; &nbsp;8 is on host borgo086</div><div class="">Process &nbsp; &nbsp;6 of &nbsp; &nbsp;8 is on processor borgo086</div><div class="">Process &nbsp; &nbsp;7 of &nbsp; &nbsp;8 is on host borgo086</div><div class="">Process &nbsp; &nbsp;7 of &nbsp; &nbsp;8 is on processor borgo140</div></div><div class=""><br class=""></div><div class="">But it was doing the right thing before. It saw my SLURM_* bits and correctly put 4 processes on the first node and 4 on the second (see the processor line which is from MPI, not the environment), and I only asked for 4 tasks per node:</div><div class=""><br class=""></div><div class=""><div class="">SLURM_NODELIST=borgo[086,140]</div><div class="">SLURM_NTASKS_PER_NODE=4</div><div class="">SLURM_NNODES=2</div><div class="">SLURM_NTASKS=8</div><div class="">SLURM_TASKS_PER_NODE=4(x2)</div></div><div class=""><br class=""></div><div class="">My guess is no MPI stack wants to propagate an environment variable to every process. I'm picturing an 1000 node/28000 core job...and poor Open MPI (or MPT or Intel MPI) would have to marshall 28000xN environment variables around and keep track of who gets what...</div><div class=""><br class=""></div><div class="">Matt</div><div class=""><br class=""></div></div><div class="gmail_extra"><br class=""><div class="gmail_quote">On Fri, Jan 15, 2016 at 10:48 AM, Ralph Castain <span dir="ltr" class="">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank" class="">rhc@open-mpi.org</a>&gt;</span> wrote:<br class=""><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word" class="">Actually, the explanation is much simpler. You probably have more than 8 slots on borgj020, and so your job is simply small enough that we put it all on one host. If you want to force the job to use both hosts, add “-map-by node” to your cmd line<div class=""><br class=""></div><div class=""><br class=""><div class=""><blockquote type="cite" class=""><div class=""><div class=""><div class="">On Jan 15, 2016, at 7:02 AM, Jim Edwards &lt;<a href="mailto:jedwards@ucar.edu" target="_blank" class="">jedwards@ucar.edu</a>&gt; wrote:</div><br class=""></div></div><div class=""><div class=""><div class=""><div dir="ltr" style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" class=""><div class="gmail_extra"><br class=""><br class=""><div class="gmail_quote">On Fri, Jan 15, 2016 at 7:53 AM, Matt Thompson<span class="">&nbsp;</span><span dir="ltr" class="">&lt;<a href="mailto:fortran@gmail.com" target="_blank" class="">fortran@gmail.com</a>&gt;</span><span class="">&nbsp;</span>wrote:<br class=""><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div dir="ltr" class="">All,<div class=""><br class=""></div><div class="">I'm not too sure if this is an MPI issue, a Fortran issue, or something else but I thought I'd ask the MPI gurus here first since my web search failed me.</div><div class=""><br class=""></div><div class="">There is a chance in the future I might want/need to query an environment variable in a Fortran program, namely to figure out what switch a currently running process is on (via SLURM_TOPOLOGY_ADDR in my case) and perhaps make a "per-switch" communicator.[1]</div><div class=""><br class=""></div><div class="">So, I coded up a boring Fortran program whose only exciting lines are:</div><div class=""><br class=""></div><div class=""><div class=""><font face="monospace, monospace" class="">&nbsp; &nbsp;call MPI_Get_Processor_Name(processor_name,name_length,ierror)</font></div><div class=""><font face="monospace, monospace" class="">&nbsp; &nbsp;call get_environment_variable("HOST",host_name)</font></div><div class=""><font face="monospace, monospace" class=""><br class=""></font></div><div class=""><font face="monospace, monospace" class="">&nbsp; &nbsp;write (*,'(A,X,I4,X,A,X,I4,X,A,X,A)') "Process", myid, "of", npes, "is on processor", trim(processor_name)</font></div><div class=""><font face="monospace, monospace" class="">&nbsp; &nbsp;write (*,'(A,X,I4,X,A,X,I4,X,A,X,A)') "Process", myid, "of", npes, "is on host", trim(host_name)</font></div></div><div class=""><br class=""></div><div class="">I decided to try out with the&nbsp;<font face="monospace, monospace" class="">HOST</font>&nbsp;environment variable first because it is simple and different per node (I didn't want to take many, many nodes to find the point when a switch is traversed). I then grabbed two nodes with 4 processes per node and...:</div><div class=""><br class=""></div><div class=""><div class=""><font face="monospace, monospace" class="">(1046) $ echo "$SLURM_NODELIST"</font></div><div class=""><font face="monospace, monospace" class="">borgj[020,036]</font></div><div class=""><font face="monospace, monospace" class="">(1047) $ pdsh -w "$SLURM_NODELIST" echo '$HOST'</font></div><div class=""><font face="monospace, monospace" class="">borgj036: borgj036<br class=""></font></div><div class=""><font face="monospace, monospace" class="">borgj020: borgj020</font></div><div class=""><font face="monospace, monospace" class="">(1048) $ mpifort -o hostenv.x hostenv.F90</font></div><div class=""><font face="monospace, monospace" class="">(1049) $ mpirun -np 8 ./hostenv.x | sort -g -k2</font></div><div class=""><font face="monospace, monospace" class="">Process &nbsp; &nbsp;0 of &nbsp; &nbsp;8 is on host borgj020</font></div><div class=""><font face="monospace, monospace" class="">Process &nbsp; &nbsp;0 of &nbsp; &nbsp;8 is on processor borgj020</font></div><div class=""><font face="monospace, monospace" class="">Process &nbsp; &nbsp;1 of &nbsp; &nbsp;8 is on host borgj020</font></div><div class=""><font face="monospace, monospace" class="">Process &nbsp; &nbsp;1 of &nbsp; &nbsp;8 is on processor borgj020</font></div><div class=""><font face="monospace, monospace" class="">Process &nbsp; &nbsp;2 of &nbsp; &nbsp;8 is on host borgj020</font></div><div class=""><font face="monospace, monospace" class="">Process &nbsp; &nbsp;2 of &nbsp; &nbsp;8 is on processor borgj020</font></div><div class=""><font face="monospace, monospace" class="">Process &nbsp; &nbsp;3 of &nbsp; &nbsp;8 is on host borgj020</font></div><div class=""><font face="monospace, monospace" class="">Process &nbsp; &nbsp;3 of &nbsp; &nbsp;8 is on processor borgj020</font></div><div class=""><font face="monospace, monospace" class="">Process &nbsp; &nbsp;4 of &nbsp; &nbsp;8 is on host borgj020</font></div><div class=""><font face="monospace, monospace" class="">Process &nbsp; &nbsp;4 of &nbsp; &nbsp;8 is on processor borgj036</font></div><div class=""><font face="monospace, monospace" class="">Process &nbsp; &nbsp;5 of &nbsp; &nbsp;8 is on host borgj020</font></div><div class=""><font face="monospace, monospace" class="">Process &nbsp; &nbsp;5 of &nbsp; &nbsp;8 is on processor borgj036</font></div><div class=""><font face="monospace, monospace" class="">Process &nbsp; &nbsp;6 of &nbsp; &nbsp;8 is on host borgj020</font></div><div class=""><font face="monospace, monospace" class="">Process &nbsp; &nbsp;6 of &nbsp; &nbsp;8 is on processor borgj036</font></div><div class=""><font face="monospace, monospace" class="">Process &nbsp; &nbsp;7 of &nbsp; &nbsp;8 is on host borgj020</font></div><div class=""><font face="monospace, monospace" class="">Process &nbsp; &nbsp;7 of &nbsp; &nbsp;8 is on processor borgj036</font></div></div><div class=""><br class=""></div><div class="">It looks like MPI_Get_Processor_Name is doing its thing, but the HOST one seems to only be reflecting the first host. My guess is that OpenMPI doesn't export every processes' environment separately to every process so it is reflecting HOST from process 0.</div><div class=""><div class="gmail_default" style="font-family:'comic sans ms',sans-serif;color:rgb(56,118,29)">​</div></div></div></blockquote><div class=""><br class=""></div><div class=""><div class="gmail_default" style="font-family:'comic sans ms',sans-serif;color:rgb(56,118,29)">​I would guess that what is actually happening is that slurm is exporting all of the variables from the host node including the $HOST variable and overwriting the ​</div><div class="gmail_default" style="font-family:'comic sans ms',sans-serif;color:rgb(56,118,29)">​defaults on other nodes. &nbsp; You should use the SLURM options to limit the list of&nbsp;</div><div class="gmail_default" style="font-family:'comic sans ms',sans-serif;color:rgb(56,118,29)">variables that you export from the host to only those that you need.​</div><br class=""></div><div class=""><br class=""></div><div class="">&nbsp;</div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div dir="ltr" class=""><div class=""><div class="gmail_default" style="font-family:'comic sans ms',sans-serif;color:rgb(56,118,29)">​</div><br class=""></div><div class="">So, I guess my question is: can this be done? Is there an option to Open MPI that might do it? Or is this just something MPI doesn't do? Or is my Google-fu just too weak to figure out the right search-phrase to find the answer to this probable FAQ?</div><div class=""><br class=""></div><div class="">Matt</div><div class=""><br class=""></div><div class="">[1] Note, this might be unnecessary, but I got to the point where I wanted to see if I *could* do it, rather than *should*.<span class=""><font color="#888888" class=""><br clear="all" class=""><div class=""><br class=""></div>--<span class="">&nbsp;</span><br class=""><div class=""><div dir="ltr" class=""><div class=""><div dir="ltr" class=""><div class="">Matt Thompson</div></div></div><blockquote style="margin:0px 0px 0px 40px;border:none;padding:0px" class=""><div class=""><div class=""><div class="">Man Among Men</div></div></div><div class=""><div class=""><div class="">Fulcrum of History</div></div></div></blockquote></div></div></font></span></div></div><br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">Subscription:<span class="">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this post:<span class="">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2016/01/28287.php" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2016/01/28287.php</a><br class=""></blockquote></div><br class=""><br clear="all" class=""><div class=""><br class=""></div>--<span class="">&nbsp;</span><br class=""><div class=""><div dir="ltr" class=""><div class=""><div class=""><div class="">Jim Edwards<br class=""><br class=""></div><font size="1" class="">CESM Software Engineer<br class=""></font></div><font size="1" class="">National Center for Atmospheric Research<br class=""></font></div><font size="1" class="">Boulder, CO</font><span class="">&nbsp;</span><br class=""></div></div></div></div><span style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;float:none;display:inline!important" class="">_______________________________________________</span><br style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" class=""><span style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;float:none;display:inline!important" class="">users mailing list</span><br style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" class=""><a href="mailto:users@open-mpi.org" style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" target="_blank" class="">users@open-mpi.org</a><br style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" class=""><span style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;float:none;display:inline!important" class="">Subscription:<span class="">&nbsp;</span></span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" class=""></div></div><span style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;float:none;display:inline!important" class="">Link to this post:<span class="">&nbsp;</span></span><a href="http://www.open-mpi.org/community/lists/users/2016/01/28289.php" style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2016/01/28289.php</a></div></blockquote></div><br class=""></div></div><br class="">_______________________________________________<br class="">
users mailing list<br class="">
<a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/01/28290.php" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2016/01/28290.php</a><br class=""></blockquote></div><br class=""><br clear="all" class=""><div class=""><br class=""></div>-- <br class=""><div class=""><div dir="ltr" class=""><div class=""><div dir="ltr" class=""><div class="">Matt Thompson</div></div></div><blockquote style="margin:0 0 0 40px;border:none;padding:0px" class=""><div class=""><div class=""><div class="">Man Among Men</div></div></div><div class=""><div class=""><div class="">Fulcrum of History</div></div></div></blockquote></div></div>
</div>
_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class=""></div></div>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/01/28291.php" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2016/01/28291.php</a></div></blockquote></div><br class=""></div></div><br class="">_______________________________________________<br class="">
users mailing list<br class="">
<a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/01/28292.php" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2016/01/28292.php</a><br class=""></blockquote></div><br class=""><br clear="all" class=""><div class=""><br class=""></div>-- <br class=""><div class="gmail_signature"><div dir="ltr" class=""><div class="">Kind regards Nick</div></div></div>
</div>
_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2016/01/28294.php</div></blockquote></div><br class=""></div></body></html>
