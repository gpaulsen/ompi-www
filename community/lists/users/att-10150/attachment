<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML><HEAD>
<META http-equiv=Content-Type content="text/html; charset=us-ascii">
<META content="MSHTML 6.00.6000.16850" name=GENERATOR></HEAD>
<BODY 
style="WORD-WRAP: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space">
<DIV dir=ltr align=left><FONT face=Arial color=#0000ff size=2>I found the manual 
pages for mpirun and orte_hosts, which have a pretty thorough description of 
these features.&nbsp; Let me know if there's anything else I should check 
out.</FONT></DIV>
<DIV><FONT face=Arial color=#0000ff size=2></FONT>&nbsp;</DIV>
<DIV dir=ltr align=left><FONT face=Arial color=#0000ff size=2>My quick 
impression is that this will meet at least 90% of user needs out of the box as 
most (all?) users will run with number of job processors that divides into PPN 
or is a multiple of PPN.&nbsp; </FONT></DIV>
<DIV><BR><FONT face=Arial color=#0000ff size=2>The docs imply that the relative 
indexing is relative to nodes as opposed to slots.&nbsp; If so, it's easy to 
cover my cases 1 and 2 with modulo arithmetic right on the command line, but the 
edge case 3 might require creation of host files or a maps file, specifying 
specific nodes and slots.&nbsp; I'm still trying to fully wrap my head around 
the way the scheduling works and the nodes/slots distinction, so have to think 
on this.&nbsp; For example, in a PBS/Torque environment, I guess each processor 
gets an entry as a "node" in the $PBS_NODEFILE, so maybe this won't be an issue 
for many of our machines.</FONT></DIV>
<DIV><FONT face=Arial color=#0000ff size=2></FONT>&nbsp;</DIV>
<DIV dir=ltr align=left><FONT face=Arial color=#0000ff size=2>Another potential 
gotcha is the scheduler driving my M scheduler processes ideally runs 
asynchronously (since not all compute jobs take equal time as parameters vary), 
so initially jobs 0--M could be on resource blocks 0--M in order, but if job 2 
finishes first, I'd want to put job M+1 on block 2.&nbsp; So I might end up 
needing some glue code anyway for the dynamic scheduling case.&nbsp; What's in 
ORTE today will<SPAN class=157093522-30072009> likely</SPAN>&nbsp;work great in 
the static scheduling case.</FONT></DIV>
<DIV><FONT face=Arial color=#0000ff size=2></FONT>&nbsp;</DIV>
<DIV dir=ltr align=left><FONT face=Arial color=#0000ff size=2>Brian</FONT></DIV>
<DIV><FONT face=Arial color=#0000ff size=2></FONT>&nbsp;</DIV><BR>
<BLOCKQUOTE dir=ltr 
style="PADDING-LEFT: 5px; MARGIN-LEFT: 5px; BORDER-LEFT: #0000ff 2px solid; MARGIN-RIGHT: 0px">
  <DIV class=OutlookMessageHeader lang=en-us dir=ltr align=left>
  <HR tabIndex=-1>
  <FONT face=Tahoma size=2><B>From:</B> users-bounces@open-mpi.org 
  [mailto:users-bounces@open-mpi.org] <B>On Behalf Of </B>Ralph 
  Castain<BR><B>Sent:</B> Thursday, July 30, 2009 2:08 PM<BR><B>To:</B> Open MPI 
  Users<BR><B>Subject:</B> Re: [OMPI users] Multiple mpiexec's within a job 
  (schedule within a scheduled machinefile/job allocation)<BR></FONT><BR></DIV>
  <DIV></DIV>Let me know how it goes, if you don't mind. It would be nice to 
  know if we actually met your needs, or if a tweak might help make it 
easier.&nbsp;
  <DIV><BR></DIV>
  <DIV>Thanks</DIV>
  <DIV>Ralph</DIV>
  <DIV><BR>
  <DIV>
  <DIV>On Jul 30, 2009, at 1:36 PM, Adams, Brian M wrote:</DIV><BR 
  class=Apple-interchange-newline>
  <BLOCKQUOTE type="cite">
    <DIV 
    style="WORD-WRAP: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space">
    <DIV dir=ltr align=left><SPAN class=915373419-30072009><FONT face=Arial 
    color=#0000ff size=2>Thanks Ralph, I wasn't aware of the relative indexing 
    or sequential mapper capabilities.&nbsp; I will check those out and report 
    back if I still have a feature request. -- Brian</FONT></SPAN></DIV><BR>
    <BLOCKQUOTE 
    style="PADDING-LEFT: 5px; MARGIN-LEFT: 5px; BORDER-LEFT: #0000ff 2px solid; MARGIN-RIGHT: 0px">
      <DIV class=OutlookMessageHeader lang=en-us dir=ltr align=left>
      <HR tabIndex=-1>
      <FONT face=Tahoma size=2><B>From:</B> users-bounces@open-mpi.org [<A 
      href="mailto:users-bounces@open-mpi.org">mailto:users-bounces@open-mpi.org</A>] 
      <B>On Behalf Of </B>Ralph Castain<BR><B>Sent:</B> Thursday, July 30, 2009 
      12:26 PM<BR><B>To:</B> Open MPI Users<BR><B>Subject:</B> Re: [OMPI users] 
      Multiple mpiexec's within a job (schedule within a scheduled 
      machinefile/job allocation)<BR></FONT><BR></DIV>
      <DIV></DIV><BR>
      <DIV>
      <DIV>On Jul 30, 2009, at 11:49 AM, Adams, Brian M wrote:</DIV><BR 
      class=Apple-interchange-newline>
      <BLOCKQUOTE type="cite">
        <DIV>Apologies if I'm being confusing; I'm probably trying to get at 
        atypical use cases. &nbsp;M and N &nbsp;need not correspond to the 
        number of nodes/ppn nor ppn/nodes available. &nbsp;By node vs. slot 
        doesn't much matter, as long as in the end I don't oversubscribe any 
        node. &nbsp;By slot might be good for efficiency in some apps, but I 
        can't make a general case for it.<BR><BR>I think what you proposed 
        offers some help in the case where N is an integer multiple of the 
        number of available nodes, but perhaps not in other cases. &nbsp;I must 
        be missing something here, so instead of being fully general, perhaps 
        consider a &nbsp;specific case. &nbsp;Suppose we have 4 nodes, 8 ppn (32 
        slots is I think the ompi language). &nbsp;I might want to schedule, for 
        example<BR><BR>1. M=2 simultaneous N=16 processor jobs: Here I believe 
        what you suggested will work since N is a multiple of the available 
        number of nodes. &nbsp;I could use either npernode 4 or just bynode and 
        I think get the same result: an even distribution of tasks. 
        &nbsp;(similar applies to, e.g., 8x4, 4x8)<BR></DIV></BLOCKQUOTE>
      <DIV><BR></DIV>Yes, agreed</DIV>
      <DIV><BR>
      <BLOCKQUOTE type="cite">
        <DIV><BR>2. M=16 simultaneous N=2 processor jobs: it seems if I use 
        bynode or npernode, I would end up with 16 processes on each of the 
        first two nodes (similar applies to, e.g., 32x1 or 10x3). 
        &nbsp;Scheduling many small jobs is a common problem for 
      us.<BR></DIV></BLOCKQUOTE></DIV>
      <DIV>
      <BLOCKQUOTE type="cite">
        <DIV><FONT class=Apple-style-span color=#000000><BR></FONT>3. M=3 
        simultaneous, N=10 processor jobs: I think we'd end up with this 
        distribution (where A-D are nodes and 0-2 jobs)<BR><BR>A 0 0 0 1 1 1 2 2 
        2<BR>B 0 0 0 1 1 1 2 2 2<BR>C 0 0 &nbsp;&nbsp;1 1 &nbsp;&nbsp;2 2<BR>D 0 
        0 &nbsp;&nbsp;1 1 &nbsp;&nbsp;2 2<BR><BR>where A and B are 
        over-subscribed and there are more than the two unused slots I'd expect 
        in the whole allocation.<BR><BR>Again, I can manage all these via a 
        script that partitions the machine files, just wondering which scenarios 
        OpenMPI can manage.<BR><BR></DIV></BLOCKQUOTE><BR>
      <DIV>Have you looked at the relative indexing in 1.3.3? You could specify 
      any of these in relative index terms, and have one "hostfile" that would 
      support 16x2 operations. This would work then for any allocation.</DIV>
      <DIV><BR></DIV>
      <DIV>Your launch script could even just do it, something like this:</DIV>
      <DIV><BR></DIV>
      <DIV>mpirun -n 2 -host +n0:1,+n1:1 app</DIV>
      <DIV>mpirun -n 2 -host +n0:2,+n1:2 app</DIV>
      <DIV><BR></DIV>
      <DIV>etc. Obviously, you could compute the relative indexing and just 
      stick it in as required.</DIV>
      <DIV><BR></DIV>
      <DIV>Likewise, you could use the new "seq" (sequential) mapper to achieve 
      any desired layout, again utilizing relative indexing to avoid having to 
      create a special hostfile for each run.</DIV>
      <DIV><BR></DIV>
      <DIV>Note that in all cases, you can specify a -n N that will tell OMPI to 
      only execute N processes, regardless of what is in the sequential mapper 
      file or -host.</DIV>
      <DIV><BR></DIV>
      <DIV>If none of those work well, please let me know. I'm happy to create 
      the required capability as I'm sure LANL will use it too (know of several 
      similar cases here, but the current options seem okay for them).</DIV>
      <DIV><BR>
      <BLOCKQUOTE type="cite"></BLOCKQUOTE></DIV>
      <BLOCKQUOTE type="cite">
        <DIV>Thanks!<BR>Brian<BR><BR>
        <BLOCKQUOTE type="cite">-----Original Message-----<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">From: <A 
          href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</A> 
          <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">[<A 
          href="mailto:users-bounces@open-mpi.org">mailto:users-bounces@open-mpi.org</A>] 
          On Behalf Of Ralph Castain<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">Sent: Wednesday, July 29, 2009 4:19 
        PM<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">To: Open MPI Users<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">Subject: Re: [OMPI users] Multiple mpiexec's 
          within a job <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">(schedule within a scheduled machinefile/job 
          allocation)<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">Oh my - that does take me back a long way! 
          :-)<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">Do you need these processes to be mapped 
          byslot (i.e., do you <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">care if the process ranks are sharing nodes)? 
          If not, why not <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">add "-bynode" to your cmd line?<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">Alternatively, given the mapping you want, 
          just do<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">mpirun -npernode 1 
        application.exe<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">This would launch one copy on each of your N 
          nodes. So if you <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">fork M times, you'll wind up with the exact 
          pattern you <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">wanted. And, as each one exits, you could 
          immediately launch <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">a replacement without worrying about 
          oversubscription.<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">Does that help?<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">Ralph<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">PS. we dropped that "persistent" operation - 
          caused way too <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">many problems with cleanup and other things. 
          :-)<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">On Jul 29, 2009, at 3:46 PM, Adams, Brian M 
          wrote:<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">Hi Ralph (all),<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">I'm resurrecting this 2006 thread for a 
            status check. &nbsp;The <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">new 1.3.x <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">machinefile behavior is great (thanks!) -- I 
            can use <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">machinefiles to <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">manage multiple simultaneous mpiruns within 
            a single torque<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">allocation (where the hosts are a subset of 
            $PBS_NODEFILE). &nbsp;&nbsp;<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">However, this requires some careful 
            management of machinefiles.<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">I'm curious if OpenMPI now directly supports 
            the behavior I need, <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">described in general in the quote below. 
            &nbsp;Specifically, <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">given a single <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">PBS/Torque allocation of M*N processors, I 
            will run a <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">serial program <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">that will fork M times. &nbsp;Each of the M 
            forked processes<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">calls 'mpirun -np N application.exe' and 
            blocks until completion. &nbsp;&nbsp;<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">This seems akin to the case you described of 
            "mpiruns executed in <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">separate 
        windows/prompts."<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">What I'd like to see is the M processes 
            "tiled" across the <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">available <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">slots, so all M*N processors are used. 
            &nbsp;What I see instead <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">appears at <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">face value to be the first N resources being 
            oversubscribed M times.<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">Also, when one of the forked processes 
            returns, I'd like to <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">be able to <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">spawn another and have its mpirun schedule 
            on the resources <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">freed by <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">the previous one that exited. &nbsp;Is any 
            of this possible?<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">I tried starting an orted (1.3.3, roughly as 
            you suggested <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">below), but <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">got this error:<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">orted 
        --daemonize<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">[gy8:25871] [[INVALID],INVALID] 
            ORTE_ERROR_LOG: Not found in file <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">runtime/orte_init.c at line 
          125<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE 
          type="cite">----------------------------------------------------------------------<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">---- It looks like orte_init failed for some 
            reason; your parallel <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">process is likely to abort. &nbsp;There are 
            many reasons that a parallel <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">process can fail during orte_init; some of 
            which are due to <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">configuration or environment problems. 
            &nbsp;This failure <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">appears to be an <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">internal failure; here's some additional 
            information (which <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">may only <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">be relevant to an Open MPI 
          developer):<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">orte_ess_base_select 
        failed<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">--&gt; Returned value Not found (-13) 
            instead of ORTE_SUCCESS<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE 
          type="cite">----------------------------------------------------------------------<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">---- [gy8:25871] [[INVALID],INVALID] 
            ORTE_ERROR_LOG: Not <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">found in file <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">orted/orted_main.c at line 
        323<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">I spared the debugging info as I'm not even 
            sure this is a correct <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">invocation...<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">Thanks for any suggestions you can 
          offer!<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">Brian<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">----------<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">Brian M. Adams, PhD (<A 
            href="mailto:briadam@sandia.gov">briadam@sandia.gov</A>) 
            Optimization and <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">Uncertainty <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">Quantification Sandia National Laboratories, 
            Albuquerque, NM <BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite"><A 
            href="http://www.sandia.gov/~briadam">http://www.sandia.gov/~briadam</A><BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">From: Ralph Castain 
            (rhc_at_[hidden])<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">Date: 2006-12-12 
          00:46:59<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">Hi 
        Chris<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">Some of this is doable with today's 
              code....and one of these <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">behaviors is not. 
          :-(<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">Open MPI/OpenRTE can be run in 
              "persistent" mode - this allows 
        <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">multiple jobs to share the same 
              allocation. This works much as you 
        <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">describe (syntax is slightly different, 
              of<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">course!) - the first mpirun will map using 
              whatever mode was <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">requested, then the next mpirun will map 
              starting from where the <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">first one left 
          off.<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">I *believe* you can run each mpirun in the 
              background.<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">However, I don't know if this has really 
              been tested enough to <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">support such a claim. All testing that I 
              know about to-date has <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">executed mpirun in the foreground - thus, 
              your example <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">would execute <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">sequentially instead of in 
            parallel.<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">I know people have tested multiple 
              mpirun's operating in parallel 
        <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">within a single allocation (i.e., 
              persistent mode) where <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">the mpiruns <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">are executed in separate 
              windows/prompts.<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">So I suspect you could do something like 
              you describe - <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">just haven't <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">personally verified 
          it.<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">Where we definitely differ is that Open 
              MPI/RTE will *not* block <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">until resources are freed up from the 
              prior mpiruns.<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">Instead, we will attempt to execute each 
              mpirun immediately - and <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">will error out the one(s) that try to 
              execute without sufficient <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">resources. I imagine we could provide the 
              kind of "flow <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">control" you <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">describe, but I'm not sure when that might 
              happen.<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">I am (in my copious free time...haha) 
              working on an "orteboot" <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">program that will startup a virtual 
              machine to make the persistent 
        <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">mode of operation a little easier. For 
              now, though, you <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">can do it by:<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">1. starting up the "server" using the 
              following command:<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">orted --seed --persistent --scope public 
              [--universe foo]<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">2. do your mpirun commands. They will 
              automagically find <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">the "server" <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">and connect to it. If you specified a 
              universe name when <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">starting the <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">server, then you must specify the same 
              universe name on <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">your mpirun <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE 
        type="cite">commands.<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">When you are done, you will have to 
              (unfortunately) <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">manually "kill" <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">the server and remove its session 
              directory. I have a <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">program called <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE 
        type="cite">"ortehalt"<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">in the trunk that will do this cleanly for 
              you, but it <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">isn't yet in <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">the release distributions. You are welcome 
              to use it, <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">though, if you <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">are working with the trunk - I can't 
              promise it is <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">bulletproof yet, <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">but it seems to be 
          working.<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">Ralph<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">On 12/11/06 8:07 PM, "Maestas, Christopher 
              Daniel"<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE 
          type="cite">&lt;cdmaest_at_[hidden]&gt;<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE 
type="cite">wrote:<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE 
          type="cite">Hello,<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE 
        type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">Sometimes we have users that like to do 
                from within a single job 
        <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">(think schedule within an job scheduler 
                allocation):<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">"mpiexec -n X 
            myprog"<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">"mpiexec -n Y 
            myprog2"<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">Does mpiexec within Open MPI keep track 
                of the node list 
        it<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">is 
        using<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">if it binds to a particular 
                scheduler?<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">For example with 4 nodes (2ppn 
              SMP):<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">"mpiexec -n 2 
            myprog"<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">"mpiexec -n 2 
            myprog2"<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">"mpiexec -n 1 
            myprog3"<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">And assume this is by-slot allocation we 
                would have the 
        following<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE 
          type="cite">allocation:<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">node1 - processor1 - 
              myprog<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">- processor2 - 
            myprog<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">node2 - processor1 - 
              myprog2<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">- processor2 - 
            myprog2<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">And for a by-node 
              allocation:<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">node1 - processor1 - 
              myprog<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">- processor2 - 
            myprog2<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">node2 - processor1 - 
              myprog<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">- processor2 - 
            myprog2<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE 
        type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">I think this is possible using ssh cause 
                it shouldn't <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">really matter <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">how many times it spawns, but with 
                something like torque 
        it<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">would 
        get<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">restricted to a max process launch of 4. 
                We would want the third 
        <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">mpiexec to block processes and 
                eventually be run on the first 
          <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">available node allocation that frees up 
                from myprog or <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">myprog2 ....<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE 
        type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">For example for torque, we had to add 
                the following to 
<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">osc mpiexec:<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE 
        type="cite">---<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">Finally, since only one mpiexec can be 
                the master at a<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">time, if 
        your<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">code setup requires that mpiexec exit to 
                get a result, 
you<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">can start 
        a<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE 
          type="cite">"dummy"<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">mpiexec first in your 
              batch<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE 
        type="cite">job:<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE 
        type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">mpiexec 
          -server<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE 
        type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">It runs no tasks itself but handles the 
                connections of<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">other 
        transient<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">mpiexec 
            clients.<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">It will shut down cleanly when the batch 
                job exits or you<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">may kill 
        the<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">server 
            explicitly.<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">If the server is killed with SIGTERM (or 
                HUP or INT), it <BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">will exit <BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">with a status of zero if there were no 
                clients connected 
        at<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">the 
        time.<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">If there were still clients using the 
                server, the 
        server<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">will kill 
        all<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">their tasks, disconnect from the 
                clients, and exit with status 
          1.<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE 
        type="cite">---<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE 
        type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">So a user 
          ran:<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">mpiexec 
          -server<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">mpiexec -n 2 
            myprog<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">mpiexec -n 2 
            myprog2<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">And the server kept track of the 
                allocation ... I 
        would<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">think that 
        the<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">orted could do 
            this?<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE 
        type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">Sorry if this sounds confusing ... But 
                I'm sure it will<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">clear up 
        with<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">any further responses I make. :-) 
                -cdm<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE 
        type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE 
        type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE 
                type="cite">_______________________________________________<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite">users mailing 
            list<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE 
            type="cite">users_at_[hidden]<BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite">
              <BLOCKQUOTE type="cite"><A 
                href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</A><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">
            <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE 
            type="cite">_______________________________________________<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite">users mailing list<BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite"><A 
            href="mailto:users@open-mpi.org">users@open-mpi.org</A><BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">
          <BLOCKQUOTE type="cite"><A 
            href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</A><BR></BLOCKQUOTE></BLOCKQUOTE>
        <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE>
        <BLOCKQUOTE 
          type="cite">_______________________________________________<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite">users mailing list<BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite"><A 
          href="mailto:users@open-mpi.org">users@open-mpi.org</A><BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite"><A 
          href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</A><BR></BLOCKQUOTE>
        <BLOCKQUOTE type="cite"><BR></BLOCKQUOTE>
        <BLOCKQUOTE 
        type="cite"><BR></BLOCKQUOTE><BR>_______________________________________________<BR>users 
        mailing list<BR><A 
        href="mailto:users@open-mpi.org">users@open-mpi.org</A><BR><A 
        href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</A><BR></DIV></BLOCKQUOTE></DIV><BR></BLOCKQUOTE></DIV>_______________________________________________<BR>users 
    mailing list<BR><A 
    href="mailto:users@open-mpi.org">users@open-mpi.org</A><BR>http://www.open-mpi.org/mailman/listinfo.cgi/users</BLOCKQUOTE></DIV><BR></DIV></BLOCKQUOTE></BODY></HTML>

