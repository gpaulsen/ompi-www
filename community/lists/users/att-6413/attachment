<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">Short answer is: yes.<div><br></div><div>Unfortunately, different systems store that info in different places. For Linux, we use the PLPA to help us discover the required info. Solaris, OSX, and Windows all have their own ways of providing it. The paffinity framework detects the type of system we are running on and "does the right thing" to get the info.</div><div><br></div><div>Where we simply cannot get it, we return an error and let you know that we cannot support processor affinity on this machine. You can still execute, of course - you just can't set mpi_paffinity_alone since we can't meet that request on such a system.</div><div><br></div><div>Ralph</div><div><br></div><div><br><div><div>On Aug 22, 2008, at 8:01 AM, Mi Yan wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div><p>Ralph,<br> <br>      How does OpenMPI pick up the map between physical vs.  logical processors?    Does OMPI  look into "/sys/devices/system/node/node&lt;id> for the cpu topology?<br> <br> <br> Thanks,<br> Mi Yan <br> <span>&lt;graycol.gif></span>Ralph Castain &lt;<a href="mailto:rhc@lanl.gov">rhc@lanl.gov</a>><br> <br> <br> <table width="100%" border="0" cellspacing="0" cellpadding="0"> <tbody><tr valign="top"><td style="background-image:url(cid:2__=0ABBFE3EDFDFD41E8f9e8a93df938@us.ibm.com); background-repeat: no-repeat; " width="40%"> <ul> <ul> <ul> <ul><b><font size="2">Ralph Castain &lt;<a href="mailto:rhc@lanl.gov">rhc@lanl.gov</a>></font></b><font size="2"> </font><br> <font size="2">Sent by: <a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a></font> <p><font size="2">08/22/2008 09:16 AM</font> <table border="1"> <tbody><tr valign="top"><td width="168" bgcolor="#FFFFFF"><div align="center"><font size="2">Please respond to<br> Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>></font></div></td></tr> </tbody></table> </p></ul> </ul> </ul> </ul> </td><td width="60%"> <table width="100%" border="0" cellspacing="0" cellpadding="0"> <tbody><tr valign="top"><td width="1%"><span>&lt;ecblank.gif></span><br> <div align="right"><font size="2">To</font></div></td><td width="100%"><span>&lt;ecblank.gif></span><br> <font size="2">Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>></font></td></tr> <tr valign="top"><td width="1%"><span>&lt;ecblank.gif></span><br> <div align="right"><font size="2">cc</font></div></td><td width="100%"><span>&lt;ecblank.gif></span><br> </td></tr> <tr valign="top"><td width="1%"><span>&lt;ecblank.gif></span><br> <div align="right"><font size="2">Subject</font></div></td><td width="100%"><span>&lt;ecblank.gif></span><br> <font size="2">Re: [OMPI users] problem when mpi_paffinity_alone is set to 1</font></td></tr> </tbody></table> <table border="0" cellspacing="0" cellpadding="0"> <tbody><tr valign="top"><td width="58"><span>&lt;ecblank.gif></span></td><td width="336"><span>&lt;ecblank.gif></span></td></tr> </tbody></table> </td></tr> </tbody></table> <br> <tt>Okay, I'll look into it. I suspect the problem is due to the &nbsp;<br> redefinition of the paffinity API to clarify physical vs logical &nbsp;<br> processors - more than likely, the maffinity interface suffers from &nbsp;<br> the same problem we had to correct over there.<br> <br> We'll report back later with an estimate of how quickly this can be &nbsp;<br> fixed.<br> <br> Thanks<br> Ralph<br> <br> On Aug 22, 2008, at 7:03 AM, Camille Coti wrote:<br> <br> ><br> > Ralph,<br> ><br> > I compiled a clean checkout from the trunk (r19392), the problem is &nbsp;<br> > still the same.<br> ><br> > Camille<br> ><br> ><br> > Ralph Castain a écrit :<br> >> Hi Camille<br> >> What OMPI version are you using? We just changed the paffinity &nbsp;<br> >> module last night, but did nothing to maffinity. However, it is &nbsp;<br> >> possible that the maffinity framework makes some calls into &nbsp;<br> >> paffinity that need to adjust.<br> >> So version number would help a great deal in this case.<br> >> Thanks<br> >> Ralph<br> >> On Aug 22, 2008, at 5:23 AM, Camille Coti wrote:<br> >>> Hello,<br> >>><br> >>> I am trying to run applications on a shared-memory machine. For &nbsp;<br> >>> the moment I am just trying to run tests on point-to-point &nbsp;<br> >>> communications (a &nbsp;trivial token ring) and collective operations &nbsp;<br> >>> (from the SkaMPI tests suite).<br> >>><br> >>> It runs smoothly if mpi_paffinity_alone is set to 0. For a number &nbsp;<br> >>> of processes which is larger than about 10, global communications &nbsp;<br> >>> just don't seem possible. Point-to-point communications seem to be &nbsp;<br> >>> OK.<br> >>><br> >>> But when I specify &nbsp;--mca mpi_paffinity_alone 1 in my command &nbsp;<br> >>> line, I get the following error:<br> >>><br> >>> mbind: Invalid argument<br> >>><br> >>> I looked into the code of maffinity/libnuma, and found out the &nbsp;<br> >>> error comes from<br> >>><br> >>> &nbsp; &nbsp; &nbsp; numa_setlocal_memory(segments[i].mbs_start_addr,<br> >>> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;segments[i].mbs_len);<br> >>><br> >>> in maffinity_libnuma_module.c.<br> >>><br> >>> The machine I am using is a Linux box running a 2.6.5-7 kernel.<br> >>><br> >>> Has anyone experienced a similar problem?<br> >>><br> >>> Camille<br> >>> _______________________________________________<br> >>> users mailing list<br> >>> <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br> >>> </tt><tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></tt><tt><br> >> _______________________________________________<br> >> users mailing list<br> >> <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br> >> </tt><tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></tt><tt><br> ><br> > _______________________________________________<br> > users mailing list<br> > <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br> > </tt><tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></tt><tt><br> <br> <br> _______________________________________________<br> users mailing list<br> <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br> </tt><tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></tt><tt><br> </tt><br> </p></div> _______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
