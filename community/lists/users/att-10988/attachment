I see. No, we don&#39;t copy your envars and ship them to remote nodes. Simple reason is that we don&#39;t know which ones we can safely move, and which would cause problems.<br><br>However, we do provide a mechanism for you to tell us which envars to move. Just add:<br>
<br>-x LD_LIBRARY_PATH<br><br>to your mpirun cmd line and we will pickup that value and move it. You can use that option any number of times.<br><br>I know it&#39;s a tad tedious if you have to move many of them, but it&#39;s the only safe mechanism we could devise.<br>
<br>HTH<br>Ralph<br><br><br><div class="gmail_quote">On Wed, Oct 28, 2009 at 2:36 PM, Luke Shulenburger <span dir="ltr">&lt;<a href="mailto:lshulenburger@gmail.com">lshulenburger@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
My apologies for not being clear.  These variables are set in my<br>
environment, they just are not published to the other nodes in the<br>
cluster when the jobs are run through the scheduler.  At the moment,<br>
even though I can use mpirun to run jobs locally on the head node<br>
without touching my environment, if I use the scheduler I am forced to<br>
do something like source my bashrc in the jub submission script to get<br>
them set.  I had always assumed that mpirun just copied my current<br>
environment variables to the nodes, but this does not seem to be<br>
happening in this case.<br>
<font color="#888888"><br>
Luke<br>
</font><div><div></div><div class="h5"><br>
On Wed, Oct 28, 2009 at 4:30 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt; Normally, one does simply set the ld_library_path in your environment to<br>
&gt; point to the right thing. Alternatively, you could configure OMPI with<br>
&gt;<br>
&gt; --enable-mpirun-prefix-by-default<br>
&gt;<br>
&gt; This tells OMPI to automatically add the prefix you configured the system<br>
&gt; with to your ld_library_path and path envars. It should solve your problem,<br>
&gt; if you don&#39;t want to simply set those values in your environment anyway.<br>
&gt;<br>
&gt; Ralph<br>
&gt;<br>
&gt;<br>
&gt; On Wed, Oct 28, 2009 at 2:10 PM, Luke Shulenburger &lt;<a href="mailto:lshulenburger@gmail.com">lshulenburger@gmail.com</a>&gt;<br>
&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; Thanks for the quick reply.  This leads me to another issue I have<br>
&gt;&gt; been having with openmpi as it relates to sge.  The &quot;tight<br>
&gt;&gt; integration&quot; works where I do not have to give mpirun a hostfile when<br>
&gt;&gt; I use the scheduler, but it does not seem to be passing on my<br>
&gt;&gt; environment variables.  Specifically because I used intel compilers to<br>
&gt;&gt; compile openmpi, I have to be sure to set the LD_LIBRARY_PATH<br>
&gt;&gt; correctly in my job submission script or openmpi will not run (giving<br>
&gt;&gt; the error discussed in the FAQ).  Where I am a little lost is whether<br>
&gt;&gt; this is a problem with the way I built openmpi or whether it is a<br>
&gt;&gt; configuration problem with sge.<br>
&gt;&gt;<br>
&gt;&gt; This may be unrelated to my previous problem, but the similarities<br>
&gt;&gt; with the environment variables made me think of it.<br>
&gt;&gt;<br>
&gt;&gt; Thanks for your consideration,<br>
&gt;&gt; Luke Shulenburger<br>
&gt;&gt; Geophysical Laboratory<br>
&gt;&gt; Carnegie Institution of Washington<br>
&gt;&gt;<br>
&gt;&gt; On Wed, Oct 28, 2009 at 3:48 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt;&gt; &gt; I&#39;m afraid we have never really supported this kind of nested<br>
&gt;&gt; &gt; invocations of<br>
&gt;&gt; &gt; mpirun. If it works with any version of OMPI, it is totally a fluke - it<br>
&gt;&gt; &gt; might work one time, and then fail the next.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; The problem is that we pass envars to the launched processes to control<br>
&gt;&gt; &gt; their behavior, and these conflict with what mpirun needs. We have tried<br>
&gt;&gt; &gt; various scrubbing mechanisms (i.e., having mpirun start out by scrubbing<br>
&gt;&gt; &gt; the<br>
&gt;&gt; &gt; environment of envars that would have come from the initial mpirun, but<br>
&gt;&gt; &gt; they<br>
&gt;&gt; &gt; all have the unfortunate possibility of removing parameters provided by<br>
&gt;&gt; &gt; the<br>
&gt;&gt; &gt; user - and that can cause its own problems.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; I don&#39;t know if we will ever support nested operations - occasionally, I<br>
&gt;&gt; &gt; do<br>
&gt;&gt; &gt; give it some thought, but have yet to find a foolproof solution.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Ralph<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; On Wed, Oct 28, 2009 at 1:11 PM, Luke Shulenburger<br>
&gt;&gt; &gt; &lt;<a href="mailto:lshulenburger@gmail.com">lshulenburger@gmail.com</a>&gt;<br>
&gt;&gt; &gt; wrote:<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; Hello,<br>
&gt;&gt; &gt;&gt; I am having trouble with a script that calls mpi.  Basically my<br>
&gt;&gt; &gt;&gt; problem distills to wanting to call a script with:<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; mpirun -np # ./script.sh<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; where script.sh looks like:<br>
&gt;&gt; &gt;&gt; #!/bin/bash<br>
&gt;&gt; &gt;&gt; mpirun -np 2 ./mpiprogram<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; Whenever I invoke script.sh normally (as ./script.sh for instance) it<br>
&gt;&gt; &gt;&gt; works fine, but if I do mpirun -np 2 ./script.sh I get the following<br>
&gt;&gt; &gt;&gt; error:<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; [<a href="http://ppv.stanford.edu:08814" target="_blank">ppv.stanford.edu:08814</a>] [[27860,1],0] ORTE_ERROR_LOG: A message is<br>
&gt;&gt; &gt;&gt; attempting to be sent to a process whose contact information is<br>
&gt;&gt; &gt;&gt; unknown in file rml_oob_send.c at line 105<br>
&gt;&gt; &gt;&gt; [<a href="http://ppv.stanford.edu:08814" target="_blank">ppv.stanford.edu:08814</a>] [[27860,1],0] could not get route to<br>
&gt;&gt; &gt;&gt; [[INVALID],INVALID]<br>
&gt;&gt; &gt;&gt; [<a href="http://ppv.stanford.edu:08814" target="_blank">ppv.stanford.edu:08814</a>] [[27860,1],0] ORTE_ERROR_LOG: A message is<br>
&gt;&gt; &gt;&gt; attempting to be sent to a process whose contact information is<br>
&gt;&gt; &gt;&gt; unknown in file base/plm_base_proxy.c at line 86<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; I have also tried running with mpirun -d to get some debugging info<br>
&gt;&gt; &gt;&gt; and it appears that the proctable is not being created for the second<br>
&gt;&gt; &gt;&gt; mpirun.  The command hangs like so:<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; [<a href="http://ppv.stanford.edu:08823" target="_blank">ppv.stanford.edu:08823</a>] procdir:<br>
&gt;&gt; &gt;&gt; /tmp/openmpi-sessions-sluke@ppv.stanford.edu_0/27855/0/0<br>
&gt;&gt; &gt;&gt; [<a href="http://ppv.stanford.edu:08823" target="_blank">ppv.stanford.edu:08823</a>] jobdir:<br>
&gt;&gt; &gt;&gt; /tmp/openmpi-sessions-sluke@ppv.stanford.edu_0/27855/0<br>
&gt;&gt; &gt;&gt; [<a href="http://ppv.stanford.edu:08823" target="_blank">ppv.stanford.edu:08823</a>] top: openmpi-sessions-sluke@ppv.stanford.edu_0<br>
&gt;&gt; &gt;&gt; [<a href="http://ppv.stanford.edu:08823" target="_blank">ppv.stanford.edu:08823</a>] tmp: /tmp<br>
&gt;&gt; &gt;&gt; [<a href="http://ppv.stanford.edu:08823" target="_blank">ppv.stanford.edu:08823</a>] [[27855,0],0] node[0].name ppv daemon 0 arch<br>
&gt;&gt; &gt;&gt; ffc91200<br>
&gt;&gt; &gt;&gt; [<a href="http://ppv.stanford.edu:08823" target="_blank">ppv.stanford.edu:08823</a>] Info: Setting up debugger process table for<br>
&gt;&gt; &gt;&gt; applications<br>
&gt;&gt; &gt;&gt;  MPIR_being_debugged = 0<br>
&gt;&gt; &gt;&gt;  MPIR_debug_state = 1<br>
&gt;&gt; &gt;&gt;  MPIR_partial_attach_ok = 1<br>
&gt;&gt; &gt;&gt;  MPIR_i_am_starter = 0<br>
&gt;&gt; &gt;&gt;  MPIR_proctable_size = 1<br>
&gt;&gt; &gt;&gt;  MPIR_proctable:<br>
&gt;&gt; &gt;&gt;    (i, host, exe, pid) = (0, <a href="http://ppv.stanford.edu" target="_blank">ppv.stanford.edu</a>,<br>
&gt;&gt; &gt;&gt; /home/sluke/maintenance/openmpi-1.3.3/examples/./shell.sh, 8824)<br>
&gt;&gt; &gt;&gt; [<a href="http://ppv.stanford.edu:08825" target="_blank">ppv.stanford.edu:08825</a>] procdir:<br>
&gt;&gt; &gt;&gt; /tmp/openmpi-sessions-sluke@ppv.stanford.edu_0/27855/1/0<br>
&gt;&gt; &gt;&gt; [<a href="http://ppv.stanford.edu:08825" target="_blank">ppv.stanford.edu:08825</a>] jobdir:<br>
&gt;&gt; &gt;&gt; /tmp/openmpi-sessions-sluke@ppv.stanford.edu_0/27855/1<br>
&gt;&gt; &gt;&gt; [<a href="http://ppv.stanford.edu:08825" target="_blank">ppv.stanford.edu:08825</a>] top: openmpi-sessions-sluke@ppv.stanford.edu_0<br>
&gt;&gt; &gt;&gt; [<a href="http://ppv.stanford.edu:08825" target="_blank">ppv.stanford.edu:08825</a>] tmp: /tmp<br>
&gt;&gt; &gt;&gt; [<a href="http://ppv.stanford.edu:08825" target="_blank">ppv.stanford.edu:08825</a>] [[27855,1],0] ORTE_ERROR_LOG: A message is<br>
&gt;&gt; &gt;&gt; attempting to be sent to a process whose contact information is<br>
&gt;&gt; &gt;&gt; unknown in file rml_oob_send.c at line 105<br>
&gt;&gt; &gt;&gt; [<a href="http://ppv.stanford.edu:08825" target="_blank">ppv.stanford.edu:08825</a>] [[27855,1],0] could not get route to<br>
&gt;&gt; &gt;&gt; [[INVALID],INVALID]<br>
&gt;&gt; &gt;&gt; [<a href="http://ppv.stanford.edu:08825" target="_blank">ppv.stanford.edu:08825</a>] [[27855,1],0] ORTE_ERROR_LOG: A message is<br>
&gt;&gt; &gt;&gt; attempting to be sent to a process whose contact information is<br>
&gt;&gt; &gt;&gt; unknown in file base/plm_base_proxy.c at line 86<br>
&gt;&gt; &gt;&gt; [<a href="http://ppv.stanford.edu:08825" target="_blank">ppv.stanford.edu:08825</a>] Info: Setting up debugger process table for<br>
&gt;&gt; &gt;&gt; applications<br>
&gt;&gt; &gt;&gt;  MPIR_being_debugged = 0<br>
&gt;&gt; &gt;&gt;  MPIR_debug_state = 1<br>
&gt;&gt; &gt;&gt;  MPIR_partial_attach_ok = 1<br>
&gt;&gt; &gt;&gt;  MPIR_i_am_starter = 0<br>
&gt;&gt; &gt;&gt;  MPIR_proctable_size = 0<br>
&gt;&gt; &gt;&gt;  MPIR_proctable:<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; In this case, it does not matter what the ultimate mpiprogram I try to<br>
&gt;&gt; &gt;&gt; run is, the shell script fails in the same way regardless (I&#39;ve tried<br>
&gt;&gt; &gt;&gt; the hello_f90 executable from the openmpi examples directory).  Here<br>
&gt;&gt; &gt;&gt; are some details of my setup:<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; I have built openmpi 1.3.3 with the intel fortran in c compilers<br>
&gt;&gt; &gt;&gt; (version 11.1).  The machine uses rocks with the SGE scheduler, so I<br>
&gt;&gt; &gt;&gt; have run autoconf with ./configure --prefix=/home/sluke --with-sge,<br>
&gt;&gt; &gt;&gt; however this problem persists even if I am running on the head node<br>
&gt;&gt; &gt;&gt; outside of the scheduler.  I am attaching the resulting config.log to<br>
&gt;&gt; &gt;&gt; this email as well as output to ompi_info --all and ifconfig.  I hope<br>
&gt;&gt; &gt;&gt; this gives the experts on the list enough to go from, but I will be<br>
&gt;&gt; &gt;&gt; happy to provide any more information that might be helpful.<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; Luke Shulenburger<br>
&gt;&gt; &gt;&gt; Geophysical Laboratory<br>
&gt;&gt; &gt;&gt; Carnegie Institution of Washington<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; PS I have tried this on a machine with openmpi-1.2.6 and cannot<br>
&gt;&gt; &gt;&gt; reproduce the error, however on a second machine with openmpi-1.3.2 I<br>
&gt;&gt; &gt;&gt; have the same problem.<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; _______________________________________________<br>
&gt;&gt; &gt;&gt; users mailing list<br>
&gt;&gt; &gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; &gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; _______________________________________________<br>
&gt;&gt; &gt; users mailing list<br>
&gt;&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; &gt;<br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

