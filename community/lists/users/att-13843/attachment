<br><br><div class="gmail_quote">On Wed, Jul 28, 2010 at 3:28 PM, Gus Correa <span dir="ltr">&lt;<a href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">

Hi Cristobal<br>
<br>
Cristobal Navarro wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div class="im">
<br>
<br>
On Wed, Jul 28, 2010 at 11:09 AM, Gus Correa &lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a> &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;&gt; wrote:<br>


<br>
    Hi Cristobal<br>
<br></div><div class="im">
    In case you are not using full path name for mpiexec/mpirun,<br>
    what does &quot;which mpirun&quot; say?<br>
<br>
<br>
--&gt; $which mpirun<br>
      /opt/openmpi-1.4.2<br>
<br>
<br>
    Often times this is a source of confusion, old versions may<br>
    be first on the PATH.<br>
<br>
    Gus<br>
<br>
<br>
openMPI version problem is now gone, i can confirm that the version is consistent now :), thanks.<br>
<br>
</div></blockquote>
<br>
This is good news.<div class="im"><br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
however, i keep getting this kernel crash randomnly when i execute with -np higher than 5<br>
these are Xeons, with Hyperthreading On, is that a problem??<br>
<br>
</blockquote>
<br></div>
The problem may be with Hyperthreading, maybe not.<br>
Which Xeons?<br></blockquote><div><br></div><div>--&gt; they are not so old, not so new either</div><div><div>fcluster@agua:~$ cat /proc/cpuinfo | more</div><div>processor<span class="Apple-tab-span" style="white-space:pre">	</span>: 0</div>

<div>vendor_id<span class="Apple-tab-span" style="white-space:pre">	</span>: GenuineIntel</div><div>cpu family<span class="Apple-tab-span" style="white-space:pre">	</span>: 6</div><div>model<span class="Apple-tab-span" style="white-space:pre">		</span>: 26</div>

<div>model name<span class="Apple-tab-span" style="white-space:pre">	</span>: Intel(R) Xeon(R) CPU           E5520  @ 2.27GHz</div><div>stepping<span class="Apple-tab-span" style="white-space:pre">	</span>: 5</div><div>cpu MHz<span class="Apple-tab-span" style="white-space:pre">		</span>: 1596.000</div>

<div>cache size<span class="Apple-tab-span" style="white-space:pre">	</span>: 8192 KB</div><div>physical id<span class="Apple-tab-span" style="white-space:pre">	</span>: 0</div><div>siblings<span class="Apple-tab-span" style="white-space:pre">	</span>: 8</div>

<div>core id<span class="Apple-tab-span" style="white-space:pre">		</span>: 0</div><div>cpu cores<span class="Apple-tab-span" style="white-space:pre">	</span>: 4</div><div>apicid<span class="Apple-tab-span" style="white-space:pre">		</span>: 0</div>

<div>initial apicid<span class="Apple-tab-span" style="white-space:pre">	</span>: 0</div><div>fpu<span class="Apple-tab-span" style="white-space:pre">		</span>: yes</div><div>fpu_exception<span class="Apple-tab-span" style="white-space:pre">	</span>: yes</div>

<div>cpuid level<span class="Apple-tab-span" style="white-space:pre">	</span>: 11</div><div>wp<span class="Apple-tab-span" style="white-space:pre">		</span>: yes</div><div>flags<span class="Apple-tab-span" style="white-space:pre">		</span>: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss h</div>

<div>t tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_</div><div>cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 popcnt lahf_lm ida tpr_shadow vnmi flexpriority ept vpid</div>

<div>bogomips<span class="Apple-tab-span" style="white-space:pre">	</span>: 4522.21</div><div>clflush size<span class="Apple-tab-span" style="white-space:pre">	</span>: 64</div><div>cache_alignment<span class="Apple-tab-span" style="white-space:pre">	</span>: 64</div>

<div>address sizes<span class="Apple-tab-span" style="white-space:pre">	</span>: 40 bits physical, 48 bits virtual</div><div>power management:</div></div><div>...same for cpu1, 2, 3, ..., 15.</div><div><br></div><div>information on how the cpu is distributed</div>

<div><div>fcluster@agua:~$ lstopo</div><div>System(7992MB)</div><div>  Socket#0 + L3(8192KB)</div><div>    L2(256KB) + L1(32KB) + Core#0</div><div>      P#0</div><div>      P#8</div><div>    L2(256KB) + L1(32KB) + Core#1</div>

<div>      P#2</div><div>      P#10</div><div>    L2(256KB) + L1(32KB) + Core#2</div><div>      P#4</div><div>      P#12</div><div>    L2(256KB) + L1(32KB) + Core#3</div><div>      P#6</div><div>      P#14</div><div>  Socket#1 + L3(8192KB)</div>

<div>    L2(256KB) + L1(32KB) + Core#0</div><div>      P#1</div><div>      P#9</div><div>    L2(256KB) + L1(32KB) + Core#1</div><div>      P#3</div><div>      P#11</div><div>    L2(256KB) + L1(32KB) + Core#2</div><div>      P#5</div>

<div>      P#13</div><div>    L2(256KB) + L1(32KB) + Core#3</div><div>      P#7</div><div>      P#15</div></div><div><br></div><div><br></div><div><br></div><div> </div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">


If I remember right, the old hyperthreading on old Xeons was problematic.<br>
<br>
OTOH, about 1-2 months ago I had trouble with OpenMPI on a relatively new Xeon Nehalem machine with (the new) Hyperthreading turned on,<br>
and Fedora Core 13.<br>
The machine would hang with the OpenMPI connectivity example.<br>
I reported this to the list, you may find in the archives.<br></blockquote><div><br></div><div>--i foudn the archives recently about an hour ago, was not sure if it was the same problem but i removed HT for testing with setting the online flag to 0 on the extra cpus showed with lstopo, unfortenately i also crashes, so HT may not be the problem. </div>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
Apparently other people got everything (OpenMPI with HT on Nehalem)<br>
working in more stable distributions (CentOS, RHEL, etc).<br>
<br>
That problem was likely to be in the FC13 kernel,<br>
because even turning off HT I still had the machine hanging.<br>
Nothing worked with shared memory turned on,<br>
so I had to switch OpenMPI to use tcp instead,<br>
which is kind of ridiculous in a standalone machine.</blockquote><div><br></div><div>--&gt; very interesting, sm can be the problem</div><div> </div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">

<div class="im"><br>
<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
im trying to locate the kernel error on logs, but after rebooting a crash, the error is not in the kern.log (neither kern.log.1).<br>
all i remember is that it starts with &quot;Kernel BUG...&quot;<br>
and somepart it mentions a certain CPU X, where that cpu can be any from 0 to 15 (im testing only in main node).  Someone knows where the log of kernel error could be?<br>
<br>
</blockquote>
<br></div>
Have you tried to turn off hyperthreading?<br></blockquote><div><br></div><div>--&gt; yes, tried, same crashes.</div><div> </div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">


In any case, depending on the application, it may not help much performance to have HT on.<br>
<br>
A more radical alternative is to try<br>
-mca btl tcp,self<br>
in the mpirun command line.<br>
That is what worked in the case I mentioned above.<br></blockquote><div><br></div><div>wow!, this worked really :),  you pointed out the problem, it was shared memory.</div><div>i have 4 nodes, so anyways there will be node comunication, do you think i can rely on working with -mca btl tcp,self?? i dont mind small lag.</div>

<div><br></div><div>i just have one more question, is this a problem of the ubuntu server kernel?? from the Nehalem Cpus?? from openMPI (i dont think) ?? </div><div><br></div><div>and on what depends that in the future, sm could be possible on the same configuration i have?? kernel update?.</div>

<div><br></div><div>Thanks very much Gus, really!</div><div>Cristobal</div><div><br></div><div><br></div><div> </div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">

<br>
My $0.02<br>
Gus Correa<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div class="im">
<br>
    Cristobal Navarro wrote:<br>
<br>
<br>
        On Tue, Jul 27, 2010 at 7:29 PM, Gus Correa<br>
        &lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a> &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;<br></div>
        &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a> &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;&gt;&gt;<div><div></div><div class="h5">

<br>
        wrote:<br>
<br>
           Hi Cristobal<br>
<br>
           Does it run only on the head node alone?<br>
           (Fuego? Agua? Acatenango?)<br>
           Try to put only the head node on the hostfile and execute<br>
        with mpiexec.<br>
<br>
        --&gt; i will try only with the head node, and post results back<br>
           This may help sort out what is going on.<br>
           Hopefully it will run on the head node.<br>
<br>
           Also, do you have Infinband connecting the nodes?<br>
           The error messages refer to the openib btl (i.e. Infiniband),<br>
           and complains of<br>
<br>
<br>
        no we are just using normal network 100MBit/s , since i am just<br>
        testing yet.<br>
<br>
<br>
           &quot;perhaps a missing symbol, or compiled for a different<br>
           version of Open MPI?&quot;.<br>
           It sounds as a mixup of versions/builds.<br>
<br>
<br>
        --&gt; i agree, somewhere there must be the remains of the older<br>
        version<br>
<br>
           Did you configure/build OpenMPI from source, or did you install<br>
           it with apt-get?<br>
           It may be easier/less confusing to install from source.<br>
           If you did, what configure options did you use?<br>
<br>
<br>
        --&gt;i installed from source, ./configure<br>
        --prefix=/opt/openmpi-1.4.2 --with-sge --without-xgid<br>
        --disable--static<br>
<br>
           Also, as for the OpenMPI runtime environment,<br>
           it is not enough to set it on<br>
           the command line, because it will be effective only on the<br>
        head node.<br>
           You need to either add them to the PATH and LD_LIBRARY_PATH<br>
           on your .bashrc/.cshrc files (assuming these files and your home<br>
           directory are *also* shared with the nodes via NFS),<br>
           or use the --prefix option of mpiexec to point to the OpenMPI<br>
        main<br>
           directory.<br>
<br>
<br>
        yes, all nodes have their PATH and LD_LIBRARY_PATH set up<br>
        properly inside the login scripts ( .bashrc in my case  )<br>
<br>
           Needless to say, you need to check and ensure that the OpenMPI<br>
           directory (and maybe your home directory, and your work<br>
        directory)<br>
           is (are)<br>
           really mounted on the nodes.<br>
<br>
<br>
        --&gt; yes, doublechecked that they are<br>
<br>
           I hope this helps,<br>
<br>
<br>
        --&gt; thanks really!<br>
<br>
           Gus Correa<br>
<br>
           Update: i just reinstalled openMPI, with the same parameters,<br>
        and it<br>
           seems that the problem has gone, i couldnt test entirely but<br>
        when i<br>
           get back to lab ill confirm.<br>
<br>
        best regards! Cristobal<br>
<br>
<br>
        ------------------------------------------------------------------------<br>
<br>
        _______________________________________________<br>
        users mailing list<br></div></div>
        <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<div class="im"><br>
        <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
    _______________________________________________<br>
    users mailing list<br></div>
    <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<div class="im"><br>
    <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
------------------------------------------------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></blockquote><div><div></div><div class="h5">
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

