Harald,<div><br></div><div>the answer is in ompi/mca/coll/libnbc/nbc_ibcast.c</div><div><br></div><div>this has been revamped (but not 100%) in v2.x</div><div>(e.g. no more calls to MPI_Comm_{size,rank} but MPI_Type_size is still being invoked)</div><div><br></div><div>I will review this.</div><div>basically, no MPI_* should be invoked internally (e.g. we should use the internal ompi_* or the PMPI_* symbol.</div><div><br></div><div>there is currently no plan for a v1.10.2 release, so you have to wait for the v2.0.0)</div><div><br></div><div>note you should wrap the C bindings (with a C library) and the Fortran bindings (with a Fortran library).</div><div>currently, the fortran wrapper will likely invoke the C wrapper, but that will no more be the case from v2.x</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles</div><div><br>On Friday, November 6, 2015, Harald Servat &lt;<a href="mailto:harald.servat@bsc.es">harald.servat@bsc.es</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Dear all,<br>
<br>
  we develop an instrumentation package based on PMPI and we&#39;ve observed that PMPI_Ibarrier and PMPI_Ibcast invoke regular MPI_Comm_size and MPI_Comm_rank instead to the PMPI symbols (i.e. PMPI_Comm_size and PMPI_Comm_rank) in OpenMPI 1.10.0.<br>
<br>
  I have attached simple example that demonstrates it when using OpenMPI 1.10.0. The example creates a library (libinstrument) that hooks MPI_Comm_size, MPI_Comm_rank and MPI_Ibarrier. Then, there&#39;s a single MPI application that executes MPI_Ibarrier and waits for it. The result when combining this binary with the instrumentation library is the following:<br>
<br>
# ~/aplic/openmpi/1.10.0/bin/mpirun -np 1 ./main<br>
entering MPI_Ibarrier<br>
entering MPI_Comm_rank<br>
leaving MPI_Comm_rank<br>
entering MPI_Comm_size<br>
leaving MPI_Comm_size<br>
leaving MPI_Ibarrier<br>
<br>
  which shows that MPI_Comm_rank and MPI_Comm_size are invoked within MPI_Ibarrier.<br>
<br>
  I looked into ompi/mpi/ibarrier.c and ./ompi/mpi/c/profile/pibarrier.c but it wasn&#39;t evident to me what might be wrong.<br>
<br>
  Can anyone check this? And if this could also occur to other MPI3 immediate collectives (MPI_Ireduce, MPI_Iallreduce, MPI_Igather, ...).<br>
<br>
Thank you!<br>
<br>
<br>
<br>
WARNING / LEGAL TEXT: This message is intended only for the use of the<br>
individual or entity to which it is addressed and may contain<br>
information which is privileged, confidential, proprietary, or exempt<br>
from disclosure under applicable law. If you are not the intended<br>
recipient or the person responsible for delivering the message to the<br>
intended recipient, you are strictly prohibited from disclosing,<br>
distributing, copying, or in any way using this message. If you have<br>
received this communication in error, please notify the sender and<br>
destroy and delete any copies you may have received.<br>
<br>
<a href="http://www.bsc.es/disclaimer" target="_blank">http://www.bsc.es/disclaimer</a></blockquote></div>

