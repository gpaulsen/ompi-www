<div dir="ltr">we did not get to the bottom for &quot;why&quot;.<div>Tried different mpi packages (mvapich,intel mpi) and the observation hold true.</div><div><br></div><div>it could be many factors affected by huge heap size (cpu cache misses? swapness?).</div></div><div class="gmail_extra"><br><div class="gmail_quote">On Wed, Sep 30, 2015 at 1:12 PM, Dave Love <span dir="ltr">&lt;<a href="mailto:d.love@liverpool.ac.uk" target="_blank">d.love@liverpool.ac.uk</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><span class="">Mike Dubman &lt;<a href="mailto:miked@dev.mellanox.co.il">miked@dev.mellanox.co.il</a>&gt; writes:<br>
<br>
&gt; Hello Grigory,<br>
&gt;<br>
&gt; We observed ~10% performance degradation with heap size set to unlimited<br>
&gt; for CFD applications.<br>
<br>
</span>OK, but why?Â  It would help to understand what the mechanism is, and why<br>
MXM specifically tells you to set the stack to the default, which may<br>
well be wrong for the application.<br>
<span class="">_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</span>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/09/27723.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/09/27723.php</a><br>
</blockquote></div><br><br clear="all"><div><br></div>-- <br><div class="gmail_signature"><div dir="ltr"><br><div>Kind Regards,</div><div><br></div><div>M.</div></div></div>
</div>

