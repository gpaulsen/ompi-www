Here is how you can do this without having to redescribe the data type all the time.  This will also keep your data layout together and improve cache coherency. <br><br><br>#include &lt;stdlib.h&gt;<br>#include &lt;iostream&gt;<br>

#include &lt;mpi.h&gt;<br>using namespace std;<br>int main()<br>{<br>  int N=2, M=3;<br>  //Allocate the matrix<br>  double **A=(double**)malloc(sizeof(double*)*N);<br>  double *A_data=(double*)malloc(sizeof(double)*N*M);<br>

<br>  //assign some values to the matrix<br>  for(int i=0;i&lt;N;i++)<br>    A[i]=&amp;A_data[i*M];<br><br>  int j=0;<br>  for(int n=0;n&lt;N;n++)<br>    for(int m=0;m&lt;M;m++)<br>      A[n][m]=j++;<br><br>  //print the matrix<br>

  cout &lt;&lt; &quot;Matrix:\n&quot;;<br>  for(int n=0;n&lt;N;n++)<br>  {<br>    for(int m=0;m&lt;M;m++)<br>    {<br>      cout &lt;&lt; A[n][m] &lt;&lt; &quot; &quot;;<br>    }<br>    cout &lt;&lt; endl;<br>  }<br><br>
  //to send over mpi<br>
  //MPI_Send(A_data,M*N,MPI_DOUBLE,dest,tag,MPI_COMM_WORLD);<br><br>  //delete the matrix<br>  free(A);<br>  free(A_data);<br><br>  return 0;<br>}<br><br><br><div class="gmail_quote">On Sat, Oct 31, 2009 at 11:32 AM, George Bosilca <span dir="ltr">&lt;<a href="mailto:bosilca@eecs.utk.edu">bosilca@eecs.utk.edu</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">Eugene is right, every time you create a new matrix you will have to describe it with a new datatype (even when using MPI_BOTTOM).<br>

<font color="#888888">
<br>
george.</font><div><div></div><div class="h5"><br>
<br>
On Oct 30, 2009, at 18:11 , Natarajan CS wrote:<br>
<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Thanks for the replies guys! Definitely two suggestions worth trying. Definitely didn&#39;t consider a derived datatype. I wasn&#39;t really sure that the MPI_Send call overhead was significant enough that increasing the buffer size and decreasing the number of calls would cause any speed up. Will change the code over the weekend and see what happens! Also, maybe if one passes the absolute address maybe there is no need for creating multiple definitions of the datatype? Haven&#39;t gone through the man pages yet, so apologies for ignorance!<br>


<br>
On Fri, Oct 30, 2009 at 2:44 PM, Eugene Loh &lt;<a href="mailto:Eugene.Loh@sun.com" target="_blank">Eugene.Loh@sun.com</a>&gt; wrote:<br>
Wouldn&#39;t you need to create a different datatype for each matrix instance?  E.g., let&#39;s say you create twelve 5x5 matrices.  Wouldn&#39;t you need twelve different derived datatypes?  I would think so because each time you create a matrix, the footprint of that matrix in memory will depend on the whims of malloc().<br>


<br>
George Bosilca wrote:<br>
<br>
Even with the original way to create the matrices, one can use  MPI_Create_type_struct to create an MPI datatype (<a href="http://web.mit.edu/course/13/13.715/OldFiles/build/mpich2-1.0.6p1/www/www3/MPI_Type_create_struct.html" target="_blank">http://web.mit.edu/course/13/13.715/OldFiles/build/mpich2-1.0.6p1/www/www3/MPI_Type_create_struct.html</a> ) using MPI_BOTTOM as the original displacement.<br>


<br>
On Oct 29, 2009, at 15:31 , Justin Luitjens wrote:<br>
<br>
Why not do something like this:<br>
<br>
double **A=new double*[N];<br>
double *A_data new double [N*N];<br>
<br>
for(int i=0;i&lt;N;i++)<br>
A[i]=&amp;A_data[i*N];<br>
<br>
This way you have contiguous data (in A_data) but can access it as a  2D array using A[i][j].<br>
<br>
(I haven&#39;t compiled this but I know we represent our matrices this  way).<br>
<br>
On Thu, Oct 29, 2009 at 12:30 PM, Natarajan CS &lt;<a href="mailto:csnataraj@gmail.com" target="_blank">csnataraj@gmail.com</a>&gt;  wrote:<br>
Hi<br>
thanks for the quick response. Yes, that is what I meant. I  thought there was no other way around what I am doing but It is  always good to ask a expert rather than assume!<br>
<br>
Cheers,<br>
<br>
C.S.N<br>
<br>
<br>
On Thu, Oct 29, 2009 at 11:25 AM, Eugene Loh &lt;<a href="mailto:Eugene.Loh@sun.com" target="_blank">Eugene.Loh@sun.com</a>&gt;  wrote:<br>
Natarajan CS wrote:<br>
<br>
Hello all,<br>
    Firstly, My apologies for a duplicate post in LAM/MPI list I  have the following simple MPI code. I was wondering if there was a  workaround for sending a dynamically allocated 2-D matrix? Currently  I can send the matrix row-by-row, however, since rows are not  contiguous I cannot send the entire matrix at once. I realize one  option is to change the malloc to act as one contiguous block but  can I keep the matrix definition as below and still send the entire  matrix in one go?<br>


<br>
You mean with one standard MPI call?  I don&#39;t think so.<br>
<br>
In MPI, there is a notion of derived datatypes, but I&#39;m not  convinced this is what you want.  A derived datatype is basically a  static template of data and holes in memory.  E.g., 3 bytes, then  skip 7 bytes, then another 2 bytes, then skip 500 bytes, then 1 last  byte.  Something like that.  Your 2d matrices differ in two  respects.  One is that the pattern in memory is different for each  matrix you allocate.  The other is that your matrix definition  includes pointer information that won&#39;t be the same in every  process&#39;s address space.  I guess you could overcome the first  problem by changing alloc_matrix() to some fixed pattern in memory  for some r and c, but you&#39;d still have pointer information in there  that you couldn&#39;t blindly copy from one process address space to  another.<br>


<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
</div></div></blockquote></div><br>

