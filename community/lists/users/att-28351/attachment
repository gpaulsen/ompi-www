Steve,<div><br></div><div>if I understand correctly, running on one node with 4 MPI tasks is three times faster than running on 10 nodes with 40 (10 ?) tasks.</div><div><br></div><div>did you try this test on a x86 cluster and with tcp interconnect, and did you get better performance when increasing the number of nodes ?</div><div><br></div><div>can you try to run on the pi cluster with one task per node, and increase the number of nodes one step at a time. does the performance improve ?</div><div>then you can increase the number of tasks per node and see how it impacts performances.</div><div><br></div><div>you can also run some standard MPI benchmark (osu, imb) and see if you get the performance you expect.</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<br><br>On Sunday, January 24, 2016, Steve O&#39;Hara &lt;<a href="mailto:sohara@pivotal-solutions.co.uk">sohara@pivotal-solutions.co.uk</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">





<div lang="EN-GB" link="#0563C1" vlink="#954F72">
<div>
<p class="MsoNormal">Hi,<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">I’m afraid I’m pretty new to both OpenFOAM and openMPI so please excuse me if my questions are either stupid or badly framed.<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">I’ve created a 10 Raspberry pi beowulf cluster for testing out MPI concepts and see how they are harnessed in OpenFOAM.  After a helluva lot of hassle, I’ve got the thing running using OpneMPI to run a solver in parallel.<u></u><u></u></p>
<p class="MsoNormal">The problem I have is that if I switch the server node to not use the cluster (still use 3 cores in an MPI job) the job finishes in x minutes. If I tell it to use the 9 other members of the cluster, the same job takes x times 3!<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">This is what I’m doing;<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p><u></u><span>1.<span style="font:7.0pt &quot;Times New Roman&quot;">      
</span></span><u></u>Create a mesh, adjust it with some other OF stuff<u></u><u></u></p>
<p><u></u><span>2.<span style="font:7.0pt &quot;Times New Roman&quot;">      
</span></span><u></u>Run the process to split the job into processes for each node<u></u><u></u></p>
<p><u></u><span>3.<span style="font:7.0pt &quot;Times New Roman&quot;">      
</span></span><u></u>Copy the process directories to each of the affected nodes using scp<u></u><u></u></p>
<p><u></u><span>4.<span style="font:7.0pt &quot;Times New Roman&quot;">      
</span></span><u></u>Run mpirun with a hosts file<u></u><u></u></p>
<p><u></u><span>5.<span style="font:7.0pt &quot;Times New Roman&quot;">      
</span></span><u></u>Re-constitute the case directory by copying back the processor folders<u></u><u></u></p>
<p><u></u><span>6.<span style="font:7.0pt &quot;Times New Roman&quot;">      
</span></span><u></u>Re-construct the case<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">Only step 4 Uses MPI and the other steps have a reasonably linear response time.<u></u><u></u></p>
<p class="MsoNormal">Step 4 is characterised by a flurry of network activity, followed by all the Pis lighting up with CPU activity followed a long time of no CPU activity but huge network action.<u></u><u></u></p>
<p class="MsoNormal">It’s this last bit that is consuming all the time – is this a tear-down phase of MPI?<u></u><u></u></p>
<p class="MsoNormal">Each of the Pi nodes is set up as slots=4 max_slots=4<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">What is all the network activity?  It seems to happen after the solver has completed its job so I’m guessing it has to be MPI.<u></u><u></u></p>
<p class="MsoNormal">The network interface on the Pi is not a stellar performer so is there anything I can do to minimise the network traffic?<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">Thanks,<u></u><u></u></p>
<p class="MsoNormal">Steve<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
</div>
</div>

</blockquote></div>

