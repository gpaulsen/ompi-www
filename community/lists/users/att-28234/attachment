<div dir="ltr"><br><div class="gmail_extra"><br><div class="gmail_quote">On Wed, Jan 6, 2016 at 4:36 PM, Matt Thompson <span dir="ltr">&lt;<a href="mailto:fortran@gmail.com" target="_blank">fortran@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div dir="ltr"><div class="gmail_extra"><div class="gmail_quote"><span class="">On Wed, Jan 6, 2016 at 7:20 PM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles@rist.or.jp" target="_blank">gilles@rist.or.jp</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">
  
    
  
  <div bgcolor="#FFFFFF" text="#000000">
    FWIW,<br>
    <br>
    there has been one attempt to set the OMP_* environment variables
    within OpenMPI, and that was aborted<br>
    because that caused crashes with a prominent commercial compiler.<br>
    <br>
    also, i&#39;d like to clarify that OpenMPI does bind MPI tasks (e.g.
    processes), and it is up to the OpenMP runtime to bind the OpenMP
    threads to the resources made available by OpenMPI to the MPI task.<br>
    <br>
    in this case, that means OpenMPI will bind a MPI tasks to 7 cores
    (for example cores 7 to 13), and it is up to the OpenMP runtime to
    bind each 7 OpenMP threads to one core previously allocated by
    OpenMPI<br>
    (for example, OMP thread 0 to core 7, OMP thread 1 to core 8, ...)</div></blockquote><div><br></div></span><div>Indeed. Hybrid programming is a two-step tango. The harder task (in some ways) is the placing MPI processes where I want. With omplace I could just force things (though probably not with Open MPI...haven&#39;t tried it yet), but I&#39;d rather have a more &quot;formulaic&quot; way to place processes since then you can script it. Now that I know about the ppr: syntax, I can see it&#39;ll be quite useful!</div></div><br clear="all"><div>The other task is to get the OpenMP threads in the &quot;right way&quot;. I was pretty sure KMP_AFFINITY=compact was correct (worked once...and, yeah, using Intel at present. Figured start there, then expand to figure out GCC and PGI). I&#39;ll do some experimenting with the OMP_* versions as a more-respected standard is always a good thing. <br></div><div><br></div><div>For others with inquiries into this, I highly recommend this page I found after my query was answered here:</div><div><br></div><div><a href="https://www.olcf.ornl.gov/kb_articles/parallel-job-execution-on-commodity-clusters/" target="_blank">https://www.olcf.ornl.gov/kb_articles/parallel-job-execution-on-commodity-clusters/</a><br></div><div><br></div><div>At this point, I&#39;m thinking I should start up an MPI+OpenMP wiki to map all the combinations of compiler+mpistack. </div><div><br></div></div></div></blockquote><div><br></div><div>Just using Intel compilers, OpenMP and MPI.  Problem solved :-)</div><div><br></div><div>(I work for Intel and the previous statement should be interpreted as a joke, although Intel OpenMP and MPI interoperate as well as any implementations of which I am aware.)</div><div> </div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div dir="ltr"><div class="gmail_extra"><div></div><div>Or pray the MPI Forum and OpenMP combine and I can just look in a Standard. :D</div><div><br></div></div></div></blockquote><div><br></div><div>echo &quot;&quot; &gt; $OPENMP_STANDARD # critical step</div><div>cat $MPI_STANDARD $OPENMP_STANDARD &gt; $HPC_STANDARD</div><div><br></div><div>More seriously, hybrid programming sucks.  Just use MPI-3 and exploit your coherence domain via MPI_Win_allocate_shared.  That way, you won&#39;t have to mix runtimes, suffer mercilessly because of opaque race conditions in thread-unsafe libraries, or reason about a bolt-on pseudo-language that replicates features found in ISO languages without a well-defined interoperability model.  For example, what is the interoperability between OpenMP 4.5 threads/atomics and C++11 threads/atomics, C11 threads/atomics, or Fortran 2008 concurrency features (e.g. coarrays)?  Nobody knows out side of &quot;don&#39;t do that&quot;.  How about OpenMP parallel regions inside code that runs in a POSIX, C11 or C++11 thread?  Good luck.  I&#39;ve been trying to solve the latter problem for years and have made very little progress as far as the spec goes.</div><div><br></div><div>Related work: </div><div>- <a href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-1.pdf">http://www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-1.pdf</a></div><div>- <a href="http://www.orau.gov/hpcor2015/whitepapers/Exascale_Computing_without_Threads-Barry_Smith.pdf">http://www.orau.gov/hpcor2015/whitepapers/Exascale_Computing_without_Threads-Barry_Smith.pdf</a></div><div><br></div><div>Do not feed the trolls ;-)</div><div><br></div><div>Jeff</div><div><br></div><div>-- </div></div><div class="gmail_signature">Jeff Hammond<br><a href="mailto:jeff.science@gmail.com" target="_blank">jeff.science@gmail.com</a><br><a href="http://jeffhammond.github.io/" target="_blank">http://jeffhammond.github.io/</a></div>
</div></div>

