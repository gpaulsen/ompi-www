<html xmlns:v="urn:schemas-microsoft-com:vml" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:w="urn:schemas-microsoft-com:office:word" xmlns:m="http://schemas.microsoft.com/office/2004/12/omml" xmlns="http://www.w3.org/TR/REC-html40"><head><meta http-equiv=Content-Type content="text/html; charset=us-ascii"><meta name=Generator content="Microsoft Word 14 (filtered medium)"><style><!--
/* Font Definitions */
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:Tahoma;
	panose-1:2 11 6 4 3 5 4 4 2 4;}
/* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin:0in;
	margin-bottom:.0001pt;
	font-size:12.0pt;
	font-family:"Times New Roman","serif";}
a:link, span.MsoHyperlink
	{mso-style-priority:99;
	color:blue;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{mso-style-priority:99;
	color:purple;
	text-decoration:underline;}
span.EmailStyle17
	{mso-style-type:personal-reply;
	font-family:"Calibri","sans-serif";
	color:#1F497D;}
.MsoChpDefault
	{mso-style-type:export-only;
	font-family:"Calibri","sans-serif";}
@page WordSection1
	{size:8.5in 11.0in;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
	{page:WordSection1;}
--></style><!--[if gte mso 9]><xml>
<o:shapedefaults v:ext="edit" spidmax="1026" />
</xml><![endif]--><!--[if gte mso 9]><xml>
<o:shapelayout v:ext="edit">
<o:idmap v:ext="edit" data="1" />
</o:shapelayout></xml><![endif]--></head><body lang=EN-US link=blue vlink=purple><div class=WordSection1><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>You should be running with one GPU per MPI process.&nbsp; If I understand correctly, you have a 3 node cluster and each node has a GPU so you should run with np=3.<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>Maybe you can try that and see if your numbers come out better.<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><div style='border:none;border-left:solid blue 1.5pt;padding:0in 0in 0in 4.0pt'><div><div style='border:none;border-top:solid #B5C4DF 1.0pt;padding:3.0pt 0in 0in 0in'><p class=MsoNormal><b><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'>From:</span></b><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'> users-bounces@open-mpi.org [mailto:users-bounces@open-mpi.org] <b>On Behalf Of </b>Rohan Deshpande<br><b>Sent:</b> Monday, May 07, 2012 9:38 PM<br><b>To:</b> Open MPI Users<br><b>Subject:</b> [OMPI users] GPU and CPU timing - OpenMPI and Thrust<o:p></o:p></span></p></div></div><p class=MsoNormal><o:p>&nbsp;</o:p></p><p class=MsoNormal>&nbsp;I am running MPI and Thrust code on a cluster and measuring time for calculations.<o:p></o:p></p><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>My MPI code -&nbsp; &nbsp;<o:p></o:p></p></div><div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>#include &quot;mpi.h&quot;<o:p></o:p></p></div><div><p class=MsoNormal>#include &lt;stdio.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal>#include &lt;stdlib.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal>#include &lt;string.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal>#include &lt;time.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal>#include &lt;sys/time.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal>#include &lt;sys/resource.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>#define &nbsp;MASTER 0<o:p></o:p></p></div><div><p class=MsoNormal>#define ARRAYSIZE 20000000<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>int *masterarray,*onearray,*twoarray,*threearray,*fourarray,*fivearray,*sixarray,*sevenarray,*eightarray,*ninearray; &nbsp; &nbsp;&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp;int main(int argc, char* argv[])<o:p></o:p></p></div><div><p class=MsoNormal>{<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp;&nbsp;int &nbsp; numtasks, taskid,chunksize, namelen;&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp;&nbsp;int mysum,one,two,three,four,five,six,seven,eight,nine;<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>&nbsp;&nbsp;char myname[MPI_MAX_PROCESSOR_NAME];<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp;&nbsp;MPI_Status status;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp;&nbsp;int a,b,c,d,e,f,g,h,i,j;<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>/***** Initializations *****/<o:p></o:p></p></div><div><p class=MsoNormal>MPI_Init(&amp;argc, &amp;argv);<o:p></o:p></p></div><div><p class=MsoNormal>MPI_Comm_size(MPI_COMM_WORLD, &amp;numtasks);<o:p></o:p></p></div><div><p class=MsoNormal>MPI_Comm_rank(MPI_COMM_WORLD,&amp;taskid);&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal>MPI_Get_processor_name(myname, &amp;namelen);<o:p></o:p></p></div><div><p class=MsoNormal>printf (&quot;MPI task %d has started on host %s...\n&quot;, taskid, myname);<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>masterarray= malloc(ARRAYSIZE * sizeof(int));<o:p></o:p></p></div><div><p class=MsoNormal>onearray= malloc(ARRAYSIZE * sizeof(int));<o:p></o:p></p></div><div><p class=MsoNormal>twoarray= malloc(ARRAYSIZE * sizeof(int));<o:p></o:p></p></div><div><p class=MsoNormal>threearray= malloc(ARRAYSIZE * sizeof(int));<o:p></o:p></p></div><div><p class=MsoNormal>fourarray= malloc(ARRAYSIZE * sizeof(int));<o:p></o:p></p></div><div><p class=MsoNormal>fivearray= malloc(ARRAYSIZE * sizeof(int));<o:p></o:p></p></div><div><p class=MsoNormal>sixarray= malloc(ARRAYSIZE * sizeof(int));<o:p></o:p></p></div><div><p class=MsoNormal>sevenarray= malloc(ARRAYSIZE * sizeof(int));<o:p></o:p></p></div><div><p class=MsoNormal>eightarray= malloc(ARRAYSIZE * sizeof(int));<o:p></o:p></p></div><div><p class=MsoNormal>ninearray= malloc(ARRAYSIZE * sizeof(int)); <o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>/***** Master task only ******/<o:p></o:p></p></div><div><p class=MsoNormal>if (taskid == MASTER){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for(a=0; a &lt; ARRAYSIZE; a++){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;masterarray[a] = 1;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp;&nbsp; mysum = run_kernel0(masterarray,ARRAYSIZE,taskid, myname);<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>&nbsp;} &nbsp;/* end of master section */<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>&nbsp;&nbsp;if (taskid &gt; MASTER) {<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(taskid == 1){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for(b=0;b&lt;ARRAYSIZE;b++){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; onearray[b] = 1;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;one = run_kernel0(onearray,ARRAYSIZE,taskid, myname);<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(taskid == 2){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for(c=0;c&lt;ARRAYSIZE;c++){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;twoarray[c] = 1;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;two = run_kernel0(twoarray,ARRAYSIZE,taskid, myname);<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(taskid == 3){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for(d=0;d&lt;ARRAYSIZE;d++){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;threearray[d] = 1;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; three = run_kernel0(threearray,ARRAYSIZE,taskid, myname);<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp;&nbsp; &nbsp; if(taskid == 4){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for(e=0;e &lt; ARRAYSIZE; e++){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; fourarray[e] = 1;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;four = run_kernel0(fourarray,ARRAYSIZE,taskid, myname);<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(taskid == 5){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for(f=0;f&lt;ARRAYSIZE;f++){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; fivearray[f] = 1;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; five = run_kernel0(fivearray,ARRAYSIZE,taskid, myname);<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(taskid == 6){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for(g=0;g&lt;ARRAYSIZE;g++){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;sixarray[g] = 1;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;six = run_kernel0(sixarray,ARRAYSIZE,taskid, myname);<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;} <o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(taskid == 7){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for(h=0;h&lt;ARRAYSIZE;h++){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sevenarray[h] = 1;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;seven = run_kernel0(sevenarray,ARRAYSIZE,taskid, myname);<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;} <o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(taskid == 8){<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for(i=0;i&lt;ARRAYSIZE;i++){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; eightarray[i] = 1;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;eight = run_kernel0(eightarray,ARRAYSIZE,taskid, myname);<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;} <o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if(taskid == 9){<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for(j=0;j&lt;ARRAYSIZE;j++){<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ninearray[j] = 1;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nine = run_kernel0(ninearray,ARRAYSIZE,taskid, myname);<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;} <o:p></o:p></p></div><div><p class=MsoNormal>&nbsp;&nbsp; }<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp;MPI_Finalize();<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>} &nbsp;&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>All the tasks just initialize their own array and then calculate the sum using cuda thrust.<o:p></o:p></p></div><div><p class=MsoNormal>My CUDA Thrust code -&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>&nbsp;#include &lt;stdio.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal>#include &lt;cutil_inline.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal>#include &lt;cutil.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal>#include &lt;thrust/version.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal>#include &lt;thrust/generate.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal>#include &lt;thrust/host_vector.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal>#include &lt;thrust/device_vector.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal>#include &lt;thrust/functional.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal>#include &lt;thrust/transform_reduce.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal>#include &lt;time.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal>#include &lt;sys/time.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal>#include &lt;sys/resource.h&gt;<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>&nbsp; extern &quot;C&quot;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp;int run_kernel0( int array[], int nelements, int taskid, char hostname[])<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp;{<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp;float elapsedTime;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; int result = 0;<o:p></o:p></p></div><div><p class=MsoNormal>int threshold = 25000000;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; cudaEvent_t start, stop;<o:p></o:p></p></div><div><p class=MsoNormal>cudaEventCreate(&amp;start);<o:p></o:p></p></div><div><p class=MsoNormal>cudaEventCreate(&amp;stop);<o:p></o:p></p></div><div><p class=MsoNormal>cudaEventRecord(start, 0);<o:p></o:p></p></div><div><p class=MsoNormal>thrust::device_vector&lt;int&gt; gpuarray;<o:p></o:p></p></div><div><p class=MsoNormal>int *begin = array;<o:p></o:p></p></div><div><p class=MsoNormal>int *end = array + nelements;<o:p></o:p></p></div><div><p class=MsoNormal>while(begin != end)<o:p></o:p></p></div><div><p class=MsoNormal>{<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp;int chunk_size = thrust::min(threshold,end - begin);<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp;gpuarray.assign(begin, begin + chunk_size);&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp;result += thrust::reduce(gpuarray.begin(), gpuarray.end());<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp;begin += chunk_size;<o:p></o:p></p></div><div><p class=MsoNormal>}<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; cudaEventRecord(stop, 0);<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; cudaEventSynchronize(stop); &nbsp; &nbsp;&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal>cudaEventElapsedTime(&amp;elapsedTime, start, stop);<o:p></o:p></p></div><div><p class=MsoNormal>cudaEventDestroy(start);<o:p></o:p></p></div><div><p class=MsoNormal>cudaEventDestroy(stop);<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp; &nbsp; printf(&quot; Task %d on has sum (on GPU): %ld Time for the kernel: %f ms \n&quot;, taskid, result, elapsedTime);&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; &nbsp;<o:p></o:p></p></div><div><p class=MsoNormal>return result;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; }<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>I also calculate the sum using CPU and the code is as below -&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><div><p class=MsoNormal>&nbsp; struct timespec time1, time2, temp_time;<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>&nbsp; clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &amp;time1);<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; int i;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; int cpu_sum = 0;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; long diff = 0;<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>&nbsp; for (i = 0; i &lt; nelements; i++) {<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; &nbsp; cpu_sum += array[i];<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; } &nbsp; &nbsp;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &amp;time2);<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; temp_time.tv_sec = time2.tv_sec - time1.tv_sec;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; temp_time.tv_nsec = time2.tv_nsec - time1.tv_nsec;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; diff = temp_time.tv_sec * 1000000000 + temp_time.tv_nsec;&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; printf(&quot;Task %d calculated sum: %d using CPU in %lf ms \n&quot;, taskid, cpu_sum, (double) diff/1000000);&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal>&nbsp; return cpu_sum;<o:p></o:p></p></div></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>Now when I run the job on cluster with 10 MPI tasks and compare the timings of CPU and GPU, I get weird results where GPU time is much much higher than CPU time.&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal>But the case should be opposite isnt it?<o:p></o:p></p></div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><div><p class=MsoNormal>The CPU time is almost same for all the task but GPU time increases.&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>Just wondering what might be the cause of this or are these results correct? Anything wrong with MPI code?<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>My cluster has 3 machines. 4 MPI tasks run on 2 machine and 2 Tasks run on 1 machine.&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal>Each machine has 1 GPU - GForce 9500 GT with 512 MB memory.&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>Can anyone please help me with this.?<o:p></o:p></p></div></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>Thanks<o:p></o:p></p></div><p class=MsoNormal>-- <o:p></o:p></p><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div></div>
<DIV>
<HR>
</DIV>
<DIV>This email message is for the sole use of the intended recipient(s) and may 
contain confidential information.&nbsp; Any unauthorized review, use, disclosure 
or distribution is prohibited.&nbsp; If you are not the intended recipient, 
please contact the sender by reply email and destroy all copies of the original 
message. </DIV>
<DIV>
<HR>
</DIV>
<P></P>
</body></html>

