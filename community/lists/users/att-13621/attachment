Like Ralph says, the slow down may not be coming from the kernel, but rather on waiting for messages.  What MPI send/recv commands are you using?<br><br><div class="gmail_quote">On Tue, Jul 13, 2010 at 11:53 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;"><div style="word-wrap:break-word">I&#39;m afraid that having 2 cores on a single machine will always outperform having 1 core on each machine if any communication is involved.<div>

<br></div><div>The most likely thing that is happening is that OMPI is polling waiting for messages to arrive. You might look closer at your code to try and optimize it better so that number-crunching can get more attention.</div>

<div><br></div><div>Others on this list are far more knowledgeable than I am about doing such things, so I&#39;ll let them take it from here. Glad it is now running!</div><div><div></div><div class="h5"><div><br></div><div>

<br><div><div>On Jul 13, 2010, at 12:22 PM, Robert Walters wrote:</div><br><blockquote type="cite"><table cellspacing="0" cellpadding="0" border="0"><tbody><tr><td valign="top" style="font:inherit">OpenMPI,<br><br>Following up. The sysadmin opened ports for machine to machine communication and OpenMPI is running successfully with no errors in connectivity_c, hello_c, or ring_c. Since, I have started to implement our MPP software (finite element analysis) that we have, and upon running a simple, 1 core on machine1, 1 core on machine2, job, I notice it is considerably slower than a 2 core job on a single machine. <br>

<br>A quick look at top shows me kernel usage is almost twice what cpu usage is! On a 16 core job, (8 cores per node so 2 nodes total) test, OpenMPI was consuming ~65% of the cpu for kernel related items rather than number-crunching related items...Granted, we are running on GigE, but this is a finite element code we are running with no heavy data transfer within it. I&#39;m looking into benchmarking tools, but my sysadmin is not very open to
 installing third party softwares. Do you have any suggestions for what I can use that would be &quot;big name&quot; or guaranteed safe tools I can use to figure out what&#39;s causing the hold up with all the kernel usage? I&#39;m pretty sure its network traffic but I have no way of telling (as far as I know because I&#39;m not a Linux whiz) with the standard tools in RHEL.<br>

<br>Thanks for all the help! I&#39;m glad to get it finally working and I think with a little tweaking it should be ready to go very soon.<br><br>Regards,<br>Robert Walters<br>--- On <b>Sat, 7/10/10, Ralph Castain <i>&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</i></b> wrote:<br>

<blockquote style="border-left:2px solid rgb(16, 16, 255);margin-left:5px;padding-left:5px"><br>From: Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;<br>Subject: Re: [OMPI users] OpenMPI Hangs, No Error<br>

To: &quot;Open MPI Users&quot; &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>Date: Saturday, July 10, 2010, 4:37 PM<br><br><div>The &quot;static ports&quot; flag means
 something different - it is used when the daemon is given a fixed port to use. In some installations, we lock every daemon to the same port number so that each daemon can compute exactly how to contact its peers (i.e., no contact info exchange required for wireup).<div>

<br></div><div>You have a &quot;fixed range&quot;, but not &quot;static port&quot;, scenario. Hence the message.</div><div><br></div><div>Let us know how it goes - I agree it sounds like something to discuss with the sysadmin.</div>

<div><br></div><div><br><div><div>On Jul 10, 2010, at 1:47 PM, Robert Walters wrote:</div><br><blockquote type="cite"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td style="font:inherit" valign="top">I ran oob_tcp_verbose 99 and I am getting something interesting I never got before.<br>

<br>[machine 2:22347] bind() failed: no port available in the range [60001-60016]<br>[machine 2:22347] mca_oob_tcp_init: unable to create IPv4
 listen socket: Error<br><br>I never got that error before we messed with the iptables but now I get that error... Very interesting, I will have to talk to my sysadmin again and make sure he opened the right ports on my two test machines. It looks as though there are no open ports. Another interesting thing is I see that the Daemon is still report:<br>

<br>Daemon [[28845,0],1] checking in as pid 22347 on host machine 2<br>Daemon [[28845,0],1] not using static ports<br><br>Which, I may be misunderstanding, should have been taken care of when I specified what ports to use. I am telling it a static set of ports... Anyhow, I will get with my
 sysadmin again and see what he says. At least OpenMPI is correctly interpreting the range. <br><br>Thanks for the help.<br><br>--- On <b>Sat, 7/10/10, Ralph Castain <i>&lt;<a rel="nofollow">rhc@open-mpi.org</a>&gt;</i></b> wrote:<br>

<blockquote style="border-left:2px solid rgb(16, 16, 255);margin-left:5px;padding-left:5px"><br>From: Ralph Castain &lt;<a rel="nofollow">rhc@open-mpi.org</a>&gt;<br>Subject: Re: [OMPI users] OpenMPI Hangs, No Error<br>To: &quot;Open MPI Users&quot; &lt;<a rel="nofollow">users@open-mpi.org</a>&gt;<br>

Date: Saturday, July 10, 2010, 3:21 PM<br><br><div>Are there multiple interfaces on your nodes? I&#39;m wondering if we are using a different network
 than the one where you opened these ports.<div><br></div><div>You&#39;ll get quite a bit of output, but you can turn on debug output in the oob itself with -mca oob_tcp_verbose xx. The higher the number, the more you get.</div>

<div><br></div><div><br><div><div>On Jul 10, 2010, at 11:14 AM, Robert Walters wrote:</div><br><blockquote type="cite"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td style="font:inherit" valign="top">Hello again,<br>

<br>I believe my administrator has opened the ports I requested. The problem I am having now is that OpenMPI is not listening to my defined port assignments in openmpi-mca-params.conf (looks like permission 644 on those files should it be 755?)<br>

<br>When I perform netstat -ltnup I see that orted is listening 14 processes in tcp but scaterred in the 26000ish port range when I specified 60001-60016 in the mca-params file. Is there a parameter I am missing? In any case I
 am still hanging as mentioned originally even with the port forwarding enabled and specifications in mca-param enabled. <br><br>Any other ideas on what might be causing the hang? Is there a more verbose mode I can employ to see more deeply into the issue? I have run --debug-daemons and --mca plm_base_verbose
 99.<br><br>Thanks!<br>--- On <b>Tue, 7/6/10, Robert Walters
 <i>&lt;<a rel="nofollow">raw19896@yahoo.com</a>&gt;</i></b> wrote:<br><blockquote style="border-left:2px solid rgb(16, 16, 255);margin-left:5px;padding-left:5px"><br>From: Robert Walters &lt;<a rel="nofollow">raw19896@yahoo.com</a>&gt;<br>

Subject: Re: [OMPI users] OpenMPI Hangs, No Error<br>To: &quot;Open MPI Users&quot; &lt;<a rel="nofollow">users@open-mpi.org</a>&gt;<br>Date: Tuesday, July 6, 2010, 5:41 PM<br><br><div><table border="0" cellpadding="0" cellspacing="0">

<tbody><tr><td style="font:inherit" valign="top">Thanks for your expeditious responses, Ralph.<br><br>Just to confirm with you, I should change openmpi-mca-params.conf to
 include:<br><br>oob_tcp_port_min_v4 = (My minimum port in the range)<br>oob_tcp_port_range_v4 = (My port range)<br>btl_tcp_port_min_v4 = (My minimum port in the range)<br>btl_tcp_port_range_v4 = (My port range)<br><br>correct?<br>

<br>Also, for a cluster of around 32-64 processes (8 processors per node), how wide of a range will I require? I&#39;ve noticed some entries in
 the mailing list suggesting you need a few to get started and then it opens as necessary. Will I be safe with 20 or should I go for 100? <br><br>Thanks again for all of your help!<br><br>--- On <b>Tue, 7/6/10, Ralph Castain <i>&lt;<a rel="nofollow">rhc@open-mpi.org</a>&gt;</i></b> wrote:<br>

<blockquote style="border-left:2px solid rgb(16, 16, 255);margin-left:5px;padding-left:5px"><br>From:
 Ralph Castain &lt;<a rel="nofollow">rhc@open-mpi.org</a>&gt;<br>Subject: Re: [OMPI users] OpenMPI Hangs, No Error<br>To: &quot;Open MPI Users&quot; &lt;<a rel="nofollow">users@open-mpi.org</a>&gt;<br>Date: Tuesday, July 6, 2010, 5:31 PM<br>

<br><div>Problem isn&#39;t with ssh - the problem is that the daemons need to open a TCP connection back to the machine where mpirun is running. If the firewall blocks that connection, then we can&#39;t run.<div><br></div>

<div>If you can get a range of ports opened, then you can specify the ports OMPI should use for this purpose. If the sysadmin won&#39;t allow even that, then you are pretty well hosed.</div><div><br></div><div><br><div><div>

On Jul 6, 2010, at 2:23 PM, Robert Walters wrote:</div><br><blockquote type="cite"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td style="font:inherit" valign="top">Yes, there is a system firewall. I don&#39;t think the sysadmin will
 allow it to go disabled. Each Linux machine
 has the built-in RHEL firewall. SSH is enabled through the firewall though.<br><br>--- On <b>Tue, 7/6/10, Ralph Castain <i>&lt;<a rel="nofollow">rhc@open-mpi.org</a>&gt;</i></b> wrote:<br><blockquote style="border-left:2px solid rgb(16, 16, 255);margin-left:5px;padding-left:5px">

<br>From: Ralph Castain &lt;<a rel="nofollow">rhc@open-mpi.org</a>&gt;<br>Subject: Re: [OMPI users] OpenMPI Hangs, No Error<br>To: &quot;Open MPI Users&quot; &lt;<a rel="nofollow">users@open-mpi.org</a>&gt;<br>Date: Tuesday, July 6, 2010, 4:19 PM<br>

<br><div>It looks like the remote daemon is starting - is there a firewall in the way?<div><br><div><div>On Jul 6, 2010, at 2:04 PM, Robert Walters
 wrote:</div><br><blockquote type="cite"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td style="font:inherit" valign="top"><div>Hello all,<br><br>I am using OpenMPI 1.4.2 on RHEL. I have a cluster of AMD Opteron&#39;s and right now I am just working on getting OpenMPI itself up and running. I have a successful configure and make all install. LD_LIBRARY_PATH and PATH variables were correctly edited. mpirun -np 8 hello_c successfully works on all machines. I have setup my two test machines with DSA key pairs that successfully work with each other.<br>

<br>The problem comes when I initiate my hostfile to attempt to communicate across machines. The hostfile is setup correctly with &lt;host_name&gt; &lt;slots&gt; &lt;max-slots&gt;. When running with all verbose options enabled &quot;mpirun --mca plm_base_verbose 99 --debug-daemons --mca btl_base_verbose 30 --mca oob_base_verbose 99 --mca
 pml_base_verbose 99 -hostfile hostfile -np 16 hello_c&quot; I receive the following text
 output.<br><br>[machine1:03578] mca: base: components_open: Looking for plm components<br>[machine1:03578] mca: base: components_open: opening plm components<br>[machine1:03578] mca: base: components_open: found loaded component rsh<br>

[machine1:03578] mca: base: components_open: component rsh has no register function<br>[machine1:03578] mca: base: components_open: component rsh open function successful<br>[machine1:03578] mca: base: components_open: found loaded component slurm<br>

[machine1:03578] mca: base: components_open: component slurm has no register function<br>[machine1:03578] mca: base: components_open: component slurm open function successful<br>[machine1:03578] mca:base:select: Auto-selecting plm components<br>

[machine1:03578] mca:base:select:(  plm) Querying component [rsh]<br>[machine1:03578] mca:base:select:(  plm) Query of component [rsh] set priority to 10<br>[machine1:03578] mca:base:select:(  plm) Querying component
 [slurm]<br>[machine1:03578] mca:base:select:(  plm) Skipping component [slurm]. Query failed to return a module<br>[machine1:03578] mca:base:select:(  plm) Selected component [rsh]<br>[machine1:03578] mca: base: close: component slurm closed<br>

[machine1:03578] mca: base: close: unloading component slurm<br>[machine1:03578] mca: base: components_open: Looking for oob components<br>[machine1:03578] mca: base: components_open: opening oob components<br>[machine1:03578] mca: base: components_open: found loaded component tcp<br>

[machine1:03578] mca: base: components_open: component tcp has no register function<br>[machine1:03578] mca: base: components_open: component tcp open function successful<br>Daemon was launched on machine2- beginning to initialize<br>

[machine2:01962] mca: base: components_open: Looking for oob components<br>[machine2:01962] mca: base: components_open: opening oob components<br>[machine2:01962] mca: base: components_open:
 found loaded component tcp<br>[machine2:01962] mca: base: components_open: component tcp has no register function<br>[machine2:01962] mca: base: components_open: component tcp open function successful<br>Daemon [[1418,0],1] checking in as pid 1962 on host machine2<br>

Daemon [[1418,0],1] not using static ports<br><br>At this point the system hangs indefinitely. While running top on the machine2 terminal, I see several things come up briefly. These items are: sshd (root), tcsh (myuser), orted (myuser), and mcstransd (root). I was wondering if sshd needs to be initiated by myuser? It is currently turned off in sshd_config through UsePAM yes. This was setup by the sysadmin but it can be worked around if this is necessary.<br>

<br>So in summary, mpirun works on each machine individually, but hangs when initiated through a hostfile or with the -host flag. ./configure with defaults and --prefix. LD_LIBRARY_PATH and PATH set up correctly. Any help is
 appreciated. Thanks!<br><br></div></td></tr></tbody></table><br>

      _______________________________________________<br>users mailing list<br><a rel="nofollow">users@open-mpi.org</a><br><a rel="nofollow" href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>

</div><br></div></div><br>-----Inline Attachment Follows-----<br><br><div>_______________________________________________<br>users mailing list<br><a rel="nofollow">users@open-mpi.org</a><br><a rel="nofollow" href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div>

</blockquote></td></tr></tbody></table><br>







      _______________________________________________<br>users mailing list<br><a rel="nofollow">users@open-mpi.org</a><br><a rel="nofollow" href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>

</div><br></div></div><br>-----Inline Attachment Follows-----<br><br><div>_______________________________________________<br>users mailing list<br><a rel="nofollow">users@open-mpi.org</a><br><a rel="nofollow" href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div>

</blockquote></td></tr></tbody></table><br>

      </div></blockquote></td></tr></tbody></table><br>

      _______________________________________________<br>users mailing list<br><a rel="nofollow">users@open-mpi.org</a><br><a rel="nofollow" href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>

</div><br></div></div><br>-----Inline Attachment Follows-----<br><br><div>_______________________________________________<br>users mailing list<br><a rel="nofollow">users@open-mpi.org</a><br><a rel="nofollow" href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div>

</blockquote></td></tr></tbody></table><br>

      _______________________________________________<br>users mailing list<br><a rel="nofollow">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>

</div><br></div></div><br>-----Inline Attachment Follows-----<br><br><div>_______________________________________________<br>users mailing list<br><a>users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div>

</blockquote></td></tr></tbody></table><br>

      _______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>

</div><br></div></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br><br clear="all"><br>-- <br>David Zhang<br>University of California, San Diego<br>



