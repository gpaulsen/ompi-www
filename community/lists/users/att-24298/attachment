<html><head><meta http-equiv="Content-Type" content="text/html charset=us-ascii"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">-mca btl_tcp_if_include 192.168.0.0/16 -mca oob_tcp_if_include 192.168.0.0/16<div><br></div><div>should do the trick. Any MPI is going to have trouble with your arrangement - just need a little hint to help figure it out.</div><div><br></div><div><br><div><div>On May 6, 2014, at 5:14 PM, Clay Kirkland &lt;<a href="mailto:clay.kirkland@versityinc.com">clay.kirkland@versityinc.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr"><div><div><div><div><div><div><div>&nbsp;Someone suggested using some network address if all machines are on same subnet.<br></div>They are all on the same subnet, I think.&nbsp;&nbsp; I have no idea what to put for a param there.<br>
</div>I tried the ethernet address but of course it couldn't be that simple.&nbsp; Here are my ifconfig<br></div>outputs from a couple of machines:<br><br>[root@RAID MPI]# ifconfig -a<br>eth0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Ethernet&nbsp; HWaddr 00:25:90:73:2A:36&nbsp; <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inet addr:192.168.0.59&nbsp; Bcast:192.168.0.255&nbsp; Mask:255.255.255.0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inet6 addr: fe80::225:90ff:fe73:2a36/64 Scope:Link<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UP BROADCAST RUNNING MULTICAST&nbsp; MTU:1500&nbsp; Metric:1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; RX packets:17983 errors:0 dropped:0 overruns:0 frame:0<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TX packets:9952 errors:0 dropped:0 overruns:0 carrier:0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; collisions:0 txqueuelen:1000 <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; RX bytes:26309771 (25.0 MiB)&nbsp; TX bytes:758940 (741.1 KiB)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Interrupt:16 Memory:fbde0000-fbe00000 <br>
<br>eth1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Ethernet&nbsp; HWaddr 00:25:90:73:2A:37&nbsp; <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inet6 addr: fe80::225:90ff:fe73:2a37/64 Scope:Link<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UP BROADCAST RUNNING MULTICAST&nbsp; MTU:1500&nbsp; Metric:1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; RX packets:56 errors:0 dropped:0 overruns:0 frame:0<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TX packets:6 errors:0 dropped:0 overruns:0 carrier:0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; collisions:0 txqueuelen:1000 <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; RX bytes:3924 (3.8 KiB)&nbsp; TX bytes:468 (468.0 b)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Interrupt:17 Memory:fbee0000-fbf00000 <br>
<br></div>&nbsp;And from one that I can't get to work:<br><br>[root@centos ~]# ifconfig -a<br>eth0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Ethernet&nbsp; HWaddr 00:1E:4F:FB:30:34&nbsp; <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inet6 addr: fe80::21e:4fff:fefb:3034/64 Scope:Link<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UP BROADCAST RUNNING MULTICAST&nbsp; MTU:1500&nbsp; Metric:1<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; RX packets:45 errors:0 dropped:0 overruns:0 frame:0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TX packets:6 errors:0 dropped:0 overruns:0 carrier:0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; collisions:0 txqueuelen:1000 <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; RX bytes:2700 (2.6 KiB)&nbsp; TX bytes:468 (468.0 b)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Interrupt:21 Memory:fe9e0000-fea00000 <br><br>eth1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Ethernet&nbsp; HWaddr 00:14:D1:22:8E:50&nbsp; <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inet addr:192.168.0.154&nbsp; Bcast:192.168.0.255&nbsp; Mask:255.255.255.0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inet6 addr: fe80::214:d1ff:fe22:8e50/64 Scope:Link<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UP BROADCAST RUNNING MULTICAST&nbsp; MTU:1500&nbsp; Metric:1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; RX packets:160 errors:0 dropped:0 overruns:0 frame:0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TX packets:120 errors:0 dropped:0 overruns:0 carrier:0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; collisions:0 txqueuelen:1000 <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; RX bytes:31053 (30.3 KiB)&nbsp; TX bytes:18897 (18.4 KiB)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Interrupt:16 Base address:0x2f00 <br><br>&nbsp;<br></div>&nbsp;The centos machine is using eth1 and not eth0, therein lies the problem.&nbsp;&nbsp; <br><br></div>&nbsp;I don't really need all this optimization of using multiple ethernet adaptors to speed things<br>
</div>up.&nbsp;&nbsp; I am just using MPI to synchronize I/O tests.&nbsp;&nbsp; Can I go back to a really old version <br>and avoid all this pain full debugging???<br><br>&nbsp;<br></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Tue, May 6, 2014 at 6:50 PM,  <span dir="ltr">&lt;<a href="mailto:users-request@open-mpi.org" target="_blank">users-request@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Send users mailing list submissions to<br>
&nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<br>
To subscribe or unsubscribe via the World Wide Web, visit<br>
&nbsp; &nbsp; &nbsp; &nbsp; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
or, via email, send a message with subject or body 'help' to<br>
&nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a><br>
<br>
You can reach the person managing the list at<br>
&nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:users-owner@open-mpi.org">users-owner@open-mpi.org</a><br>
<br>
When replying, please edit your Subject line so it is more specific<br>
than "Re: Contents of users digest..."<br>
<br>
<br>
Today's Topics:<br>
<br>
&nbsp; &nbsp;1. Re: users Digest, Vol 2881, Issue 1 (Clay Kirkland)<br>
&nbsp; &nbsp;2. Re: users Digest, Vol 2881, Issue 1 (Clay Kirkland)<br>
<br>
<br>
----------------------------------------------------------------------<br>
<br>
Message: 1<br>
Date: Tue, 6 May 2014 18:28:59 -0500<br>
From: Clay Kirkland &lt;<a href="mailto:clay.kirkland@versityinc.com">clay.kirkland@versityinc.com</a>&gt;<br>
To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subject: Re: [OMPI users] users Digest, Vol 2881, Issue 1<br>
Message-ID:<br>
&nbsp; &nbsp; &nbsp; &nbsp; &lt;<a href="mailto:CAJDnjA90BuHWu_iHSSnNa1A4P35%2BO96RRXk19XnHWo-nSd_dqg@mail.gmail.com">CAJDnjA90BuHWu_iHSSnNa1A4P35+O96RRXk19XnHWo-nSd_dqg@mail.gmail.com</a>&gt;<br>
Content-Type: text/plain; charset="utf-8"<br>
<br>
&nbsp;That last trick seems to work. &nbsp;I can get it to work once in a while with<br>
those tcp options but it is<br>
tricky as I have three machines and two of them use eth0 as primary network<br>
interface and one<br>
uses eth1. &nbsp; But by fiddling with network options and perhaps moving a<br>
cable or two I think I can<br>
get it all to work &nbsp; &nbsp;Thanks much for the tip.<br>
<br>
&nbsp;Clay<br>
<br>
<br>
On Tue, May 6, 2014 at 11:00 AM, &lt;<a href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a>&gt; wrote:<br>
<br>
&gt; Send users mailing list submissions to<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;<br>
&gt; To subscribe or unsubscribe via the World Wide Web, visit<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; or, via email, send a message with subject or body 'help' to<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a><br>
&gt;<br>
&gt; You can reach the person managing the list at<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:users-owner@open-mpi.org">users-owner@open-mpi.org</a><br>
&gt;<br>
&gt; When replying, please edit your Subject line so it is more specific<br>
&gt; than "Re: Contents of users digest..."<br>
&gt;<br>
&gt;<br>
&gt; Today's Topics:<br>
&gt;<br>
&gt; &nbsp; &nbsp;1. Re: MPI_Barrier hangs on second attempt but only &nbsp;when<br>
&gt; &nbsp; &nbsp; &nbsp; multiple hosts used. (Daniels, Marcus G)<br>
&gt; &nbsp; &nbsp;2. ROMIO bug reading darrays (Richard Shaw)<br>
&gt; &nbsp; &nbsp;3. MPI File Open does not work (Imran Ali)<br>
&gt; &nbsp; &nbsp;4. Re: MPI File Open does not work (Jeff Squyres (jsquyres))<br>
&gt; &nbsp; &nbsp;5. Re: MPI File Open does not work (Imran Ali)<br>
&gt; &nbsp; &nbsp;6. Re: MPI File Open does not work (Jeff Squyres (jsquyres))<br>
&gt; &nbsp; &nbsp;7. Re: MPI File Open does not work (Imran Ali)<br>
&gt; &nbsp; &nbsp;8. Re: MPI File Open does not work (Jeff Squyres (jsquyres))<br>
&gt; &nbsp; &nbsp;9. Re: users Digest, Vol 2879, Issue 1 (Jeff Squyres (jsquyres))<br>
&gt;<br>
&gt;<br>
&gt; ----------------------------------------------------------------------<br>
&gt;<br>
&gt; Message: 1<br>
&gt; Date: Mon, 5 May 2014 19:28:07 +0000<br>
&gt; From: "Daniels, Marcus G" &lt;<a href="mailto:mdaniels@lanl.gov">mdaniels@lanl.gov</a>&gt;<br>
&gt; To: "'<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>'" &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt; Subject: Re: [OMPI users] MPI_Barrier hangs on second attempt but only<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; when &nbsp; &nbsp;multiple hosts used.<br>
&gt; Message-ID:<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;<br>
&gt; <a href="mailto:532C594B7920A549A2A91CB4312CC57640DC5007@ECS-EXG-P-MB01.win.lanl.gov">532C594B7920A549A2A91CB4312CC57640DC5007@ECS-EXG-P-MB01.win.lanl.gov</a>&gt;<br>
&gt; Content-Type: text/plain; charset="utf-8"<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; From: Clay Kirkland [mailto:<a href="mailto:clay.kirkland@versityinc.com">clay.kirkland@versityinc.com</a>]<br>
&gt; Sent: Friday, May 02, 2014 03:24 PM<br>
&gt; To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a> &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt; Subject: [OMPI users] MPI_Barrier hangs on second attempt but only when<br>
&gt; multiple hosts used.<br>
&gt;<br>
&gt; I have been using MPI for many many years so I have very well debugged mpi<br>
&gt; tests. &nbsp; I am<br>
&gt; having trouble on either openmpi-1.4.5 &nbsp;or &nbsp;openmpi-1.6.5 versions though<br>
&gt; with getting the<br>
&gt; MPI_Barrier calls to work. &nbsp; It works fine when I run all processes on one<br>
&gt; machine but when<br>
&gt; I run with two or more hosts the second call to MPI_Barrier always hangs.<br>
&gt; &nbsp; Not the first one,<br>
&gt; but always the second one. &nbsp; I looked at FAQ's and such but found nothing<br>
&gt; except for a comment<br>
&gt; that MPI_Barrier problems were often problems with fire walls. &nbsp;Also<br>
&gt; mentioned as a problem<br>
&gt; was not having the same version of mpi on both machines. &nbsp;I turned<br>
&gt; firewalls off and removed<br>
&gt; and reinstalled the same version on both hosts but I still see the same<br>
&gt; thing. &nbsp; I then installed<br>
&gt; lam mpi on two of my machines and that works fine. &nbsp; I can call the<br>
&gt; MPI_Barrier function when run on<br>
&gt; one of two machines by itself &nbsp;many times with no hangs. &nbsp;Only hangs if<br>
&gt; two or more hosts are involved.<br>
&gt; These runs are all being done on CentOS release 6.4. &nbsp; Here is test<br>
&gt; program I used.<br>
&gt;<br>
&gt; main (argc, argv)<br>
&gt; int argc;<br>
&gt; char **argv;<br>
&gt; {<br>
&gt; &nbsp; &nbsp; char message[20];<br>
&gt; &nbsp; &nbsp; char hoster[256];<br>
&gt; &nbsp; &nbsp; char nameis[256];<br>
&gt; &nbsp; &nbsp; int fd, i, j, jnp, iret, myrank, np, ranker, recker;<br>
&gt; &nbsp; &nbsp; MPI_Comm comm;<br>
&gt; &nbsp; &nbsp; MPI_Status status;<br>
&gt;<br>
&gt; &nbsp; &nbsp; MPI_Init( &amp;argc, &amp;argv );<br>
&gt; &nbsp; &nbsp; MPI_Comm_rank( MPI_COMM_WORLD, &amp;myrank);<br>
&gt; &nbsp; &nbsp; MPI_Comm_size( MPI_COMM_WORLD, &amp;np);<br>
&gt;<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; gethostname(hoster,256);<br>
&gt;<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt; 1.\n",myrank,hoster);<br>
&gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt; 2.\n",myrank,hoster);<br>
&gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt; 3.\n",myrank,hoster);<br>
&gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt; &nbsp; &nbsp; MPI_Finalize();<br>
&gt; &nbsp; &nbsp; exit(0);<br>
&gt; }<br>
&gt;<br>
&gt; &nbsp; Here are three runs of test program. &nbsp;First with two processes on one<br>
&gt; host, then with<br>
&gt; two processes on another host, and finally with one process on each of two<br>
&gt; hosts. &nbsp;The<br>
&gt; first two runs are fine but the last run hangs on the second MPI_Barrier.<br>
&gt;<br>
&gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host centos a.out<br>
&gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 1.<br>
&gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 1.<br>
&gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 2.<br>
&gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 3.<br>
&gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 2.<br>
&gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 3.<br>
&gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host RAID a.out<br>
&gt; /root/.bashrc: line 14: unalias: ls: not found<br>
&gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 3.<br>
&gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 3.<br>
&gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host centos,RAID a.out<br>
&gt; /root/.bashrc: line 14: unalias: ls: not found<br>
&gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 1.<br>
&gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 2.<br>
&gt; In rank 1 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt;<br>
&gt; &nbsp; Since it is such a simple test and problem and such a widely used MPI<br>
&gt; function, it must obviously<br>
&gt; be an installation or configuration problem. &nbsp; A pstack for each of the<br>
&gt; hung MPI_Barrier processes<br>
&gt; on the two machines shows this:<br>
&gt;<br>
&gt; [root@centos ~]# pstack 31666<br>
&gt; #0 &nbsp;0x0000003baf0e8ee3 in __epoll_wait_nocancel () from /lib64/libc.so.6<br>
&gt; #1 &nbsp;0x00007f5de06125eb in epoll_dispatch () from /usr/local/lib/libmpi.so.1<br>
&gt; #2 &nbsp;0x00007f5de061475a in opal_event_base_loop () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; #3 &nbsp;0x00007f5de0639229 in opal_progress () from /usr/local/lib/libmpi.so.1<br>
&gt; #4 &nbsp;0x00007f5de0586f75 in ompi_request_default_wait_all () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; #5 &nbsp;0x00007f5ddc59565e in ompi_coll_tuned_sendrecv_actual () from<br>
&gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt; #6 &nbsp;0x00007f5ddc59d8ff in ompi_coll_tuned_barrier_intra_two_procs () from<br>
&gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt; #7 &nbsp;0x00007f5de05941c2 in PMPI_Barrier () from /usr/local/lib/libmpi.so.1<br>
&gt; #8 &nbsp;0x0000000000400a43 in main ()<br>
&gt;<br>
&gt; [root@RAID openmpi-1.6.5]# pstack 22167<br>
&gt; #0 &nbsp;0x00000030302e8ee3 in __epoll_wait_nocancel () from /lib64/libc.so.6<br>
&gt; #1 &nbsp;0x00007f7ee46885eb in epoll_dispatch () from /usr/local/lib/libmpi.so.1<br>
&gt; #2 &nbsp;0x00007f7ee468a75a in opal_event_base_loop () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; #3 &nbsp;0x00007f7ee46af229 in opal_progress () from /usr/local/lib/libmpi.so.1<br>
&gt; #4 &nbsp;0x00007f7ee45fcf75 in ompi_request_default_wait_all () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; #5 &nbsp;0x00007f7ee060b65e in ompi_coll_tuned_sendrecv_actual () from<br>
&gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt; #6 &nbsp;0x00007f7ee06138ff in ompi_coll_tuned_barrier_intra_two_procs () from<br>
&gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt; #7 &nbsp;0x00007f7ee460a1c2 in PMPI_Barrier () from /usr/local/lib/libmpi.so.1<br>
&gt; #8 &nbsp;0x0000000000400a43 in main ()<br>
&gt;<br>
&gt; &nbsp;Which looks exactly the same on each machine. &nbsp;Any thoughts or ideas<br>
&gt; would be greatly appreciated as<br>
&gt; I am stuck.<br>
&gt;<br>
&gt; &nbsp;Clay Kirkland<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; -------------- next part --------------<br>
&gt; HTML attachment scrubbed and removed<br>
&gt;<br>
&gt; ------------------------------<br>
&gt;<br>
&gt; Message: 2<br>
&gt; Date: Mon, 5 May 2014 22:20:59 -0400<br>
&gt; From: Richard Shaw &lt;<a href="mailto:jrs65@cita.utoronto.ca">jrs65@cita.utoronto.ca</a>&gt;<br>
&gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt; Subject: [OMPI users] ROMIO bug reading darrays<br>
&gt; Message-ID:<br>
&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;<br>
&gt; <a href="mailto:CAN%2BevmkC%2B9KAcNPAUSScZiufwDJ3JfcSYMB-8ZdX1etDkabioQ@mail.gmail.com">CAN+evmkC+9KAcNPAUSScZiufwDJ3JfcSYMB-8ZdX1etDkabioQ@mail.gmail.com</a>&gt;<br>
&gt; Content-Type: text/plain; charset="utf-8"<br>
&gt;<br>
&gt; Hello,<br>
&gt;<br>
&gt; I think I've come across a bug when using ROMIO to read in a 2D distributed<br>
&gt; array. I've attached a test case to this email.<br>
&gt;<br>
&gt; In the testcase I first initialise an array of 25 doubles (which will be a<br>
&gt; 5x5 grid), then I create a datatype representing a 5x5 matrix distributed<br>
&gt; in 3x3 blocks over a 2x2 process grid. As a reference I use MPI_Pack to<br>
&gt; pull out the block cyclic array elements local to each process (which I<br>
&gt; think is correct). Then I write the original array of 25 doubles to disk,<br>
&gt; and use MPI-IO to read it back in (performing the Open, Set_view, and<br>
&gt; Real_all), and compare to the reference.<br>
&gt;<br>
&gt; Running this with OMPI, the two match on all ranks.<br>
&gt;<br>
&gt; &gt; mpirun -mca io ompio -np 4 ./darr_read.x<br>
&gt; === Rank 0 === (9 elements)<br>
&gt; Packed: &nbsp;0.0 &nbsp;1.0 &nbsp;2.0 &nbsp;5.0 &nbsp;6.0 &nbsp;7.0 10.0 11.0 12.0<br>
&gt; Read: &nbsp; &nbsp;0.0 &nbsp;1.0 &nbsp;2.0 &nbsp;5.0 &nbsp;6.0 &nbsp;7.0 10.0 11.0 12.0<br>
&gt;<br>
&gt; === Rank 1 === (6 elements)<br>
&gt; Packed: 15.0 16.0 17.0 20.0 21.0 22.0<br>
&gt; Read: &nbsp; 15.0 16.0 17.0 20.0 21.0 22.0<br>
&gt;<br>
&gt; === Rank 2 === (6 elements)<br>
&gt; Packed: &nbsp;3.0 &nbsp;4.0 &nbsp;8.0 &nbsp;9.0 13.0 14.0<br>
&gt; Read: &nbsp; &nbsp;3.0 &nbsp;4.0 &nbsp;8.0 &nbsp;9.0 13.0 14.0<br>
&gt;<br>
&gt; === Rank 3 === (4 elements)<br>
&gt; Packed: 18.0 19.0 23.0 24.0<br>
&gt; Read: &nbsp; 18.0 19.0 23.0 24.0<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; However, using ROMIO the two differ on two of the ranks:<br>
&gt;<br>
&gt; &gt; mpirun -mca io romio -np 4 ./darr_read.x<br>
&gt; === Rank 0 === (9 elements)<br>
&gt; Packed: &nbsp;0.0 &nbsp;1.0 &nbsp;2.0 &nbsp;5.0 &nbsp;6.0 &nbsp;7.0 10.0 11.0 12.0<br>
&gt; Read: &nbsp; &nbsp;0.0 &nbsp;1.0 &nbsp;2.0 &nbsp;5.0 &nbsp;6.0 &nbsp;7.0 10.0 11.0 12.0<br>
&gt;<br>
&gt; === Rank 1 === (6 elements)<br>
&gt; Packed: 15.0 16.0 17.0 20.0 21.0 22.0<br>
&gt; Read: &nbsp; &nbsp;0.0 &nbsp;1.0 &nbsp;2.0 &nbsp;0.0 &nbsp;1.0 &nbsp;2.0<br>
&gt;<br>
&gt; === Rank 2 === (6 elements)<br>
&gt; Packed: &nbsp;3.0 &nbsp;4.0 &nbsp;8.0 &nbsp;9.0 13.0 14.0<br>
&gt; Read: &nbsp; &nbsp;3.0 &nbsp;4.0 &nbsp;8.0 &nbsp;9.0 13.0 14.0<br>
&gt;<br>
&gt; === Rank 3 === (4 elements)<br>
&gt; Packed: 18.0 19.0 23.0 24.0<br>
&gt; Read: &nbsp; &nbsp;0.0 &nbsp;1.0 &nbsp;0.0 &nbsp;1.0<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; My interpretation is that the behaviour with OMPIO is correct.<br>
&gt; Interestingly everything matches up using both ROMIO and OMPIO if I set the<br>
&gt; block shape to 2x2.<br>
&gt;<br>
&gt; This was run on OS X using 1.8.2a1r31632. I have also run this on Linux<br>
&gt; with OpenMPI 1.7.4, and OMPIO is still correct, but using ROMIO I just get<br>
&gt; segfaults.<br>
&gt;<br>
&gt; Thanks,<br>
&gt; Richard<br>
&gt; -------------- next part --------------<br>
&gt; HTML attachment scrubbed and removed<br>
&gt; -------------- next part --------------<br>
&gt; A non-text attachment was scrubbed...<br>
&gt; Name: darr_read.c<br>
&gt; Type: text/x-csrc<br>
&gt; Size: 2218 bytes<br>
&gt; Desc: not available<br>
&gt; URL: &lt;<br>
&gt; <a href="http://www.open-mpi.org/MailArchives/users/attachments/20140505/5a5ab0ba/attachment.bin" target="_blank">http://www.open-mpi.org/MailArchives/users/attachments/20140505/5a5ab0ba/attachment.bin</a><br>
&gt; &gt;<br>
&gt;<br>
&gt; ------------------------------<br>
&gt;<br>
&gt; Message: 3<br>
&gt; Date: Tue, 06 May 2014 13:24:35 +0200<br>
&gt; From: Imran Ali &lt;<a href="mailto:imranal@student.matnat.uio.no">imranal@student.matnat.uio.no</a>&gt;<br>
&gt; To: &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt; Subject: [OMPI users] MPI File Open does not work<br>
&gt; Message-ID: &lt;<a href="mailto:d57bdf499c00360b737205b085c50660@ulrik.uio.no">d57bdf499c00360b737205b085c50660@ulrik.uio.no</a>&gt;<br>
&gt; Content-Type: text/plain; charset="utf-8"<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; I get the following error when I try to run the following python<br>
&gt; code<br>
&gt; import mpi4py.MPI as MPI<br>
&gt; comm = MPI.COMM_WORLD<br>
&gt;<br>
&gt; MPI.File.Open(comm,"some.file")<br>
&gt;<br>
&gt; $ mpirun -np 1 python<br>
&gt; test_mpi.py<br>
&gt; Traceback (most recent call last):<br>
&gt; &nbsp;File "test_mpi.py", line<br>
&gt; 3, in &lt;module&gt;<br>
&gt; &nbsp;MPI.File.Open(comm," h5ex_d_alloc.h5")<br>
&gt; &nbsp;File "File.pyx",<br>
&gt; line 67, in mpi4py.MPI.File.Open<br>
&gt; (src/mpi4py.MPI.c:89639)<br>
&gt; mpi4py.MPI.Exception: MPI_ERR_OTHER: known<br>
&gt; error not in<br>
&gt; list<br>
&gt; --------------------------------------------------------------------------<br>
&gt; mpirun<br>
&gt; noticed that the job aborted, but has no info as to the process<br>
&gt; that<br>
&gt; caused that<br>
&gt; situation.<br>
&gt; --------------------------------------------------------------------------<br>
&gt;<br>
&gt;<br>
&gt; My mpirun version is (Open MPI) 1.6.2. I installed openmpi using the<br>
&gt; dorsal script (<a href="https://github.com/FEniCS/dorsal" target="_blank">https://github.com/FEniCS/dorsal</a>) for Redhat Enterprise 6<br>
&gt; (OS I am using, release 6.5) . It configured the build as following :<br>
&gt;<br>
&gt;<br>
&gt; ./configure --enable-mpi-thread-multiple --enable-opal-multi-threads<br>
&gt; --with-threads=posix --disable-mpi-profile<br>
&gt;<br>
&gt; I need emphasize that I do<br>
&gt; not have root acces on the system I am running my application.<br>
&gt;<br>
&gt; Imran<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; -------------- next part --------------<br>
&gt; HTML attachment scrubbed and removed<br>
&gt;<br>
&gt; ------------------------------<br>
&gt;<br>
&gt; Message: 4<br>
&gt; Date: Tue, 6 May 2014 12:56:04 +0000<br>
&gt; From: "Jeff Squyres (jsquyres)" &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>
&gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt; Subject: Re: [OMPI users] MPI File Open does not work<br>
&gt; Message-ID: &lt;<a href="mailto:E7DF28CB-D4FB-4087-928E-18E61D1D24CF@cisco.com">E7DF28CB-D4FB-4087-928E-18E61D1D24CF@cisco.com</a>&gt;<br>
&gt; Content-Type: text/plain; charset="us-ascii"<br>
&gt;<br>
&gt; The thread support in the 1.6 series is not very good. &nbsp;You might try:<br>
&gt;<br>
&gt; - Upgrading to 1.6.5<br>
&gt; - Or better yet, upgrading to 1.8.1<br>
&gt;<br>
&gt;<br>
&gt; On May 6, 2014, at 7:24 AM, Imran Ali &lt;<a href="mailto:imranal@student.matnat.uio.no">imranal@student.matnat.uio.no</a>&gt;<br>
&gt; wrote:<br>
&gt;<br>
&gt; &gt; I get the following error when I try to run the following python code<br>
&gt; &gt;<br>
&gt; &gt; import mpi4py.MPI as MPI<br>
&gt; &gt; comm = &nbsp;MPI.COMM_WORLD<br>
&gt; &gt; MPI.File.Open(comm,"some.file")<br>
&gt; &gt;<br>
&gt; &gt; $ mpirun -np 1 python test_mpi.py<br>
&gt; &gt; Traceback (most recent call last):<br>
&gt; &gt; &nbsp; File "test_mpi.py", line 3, in &lt;module&gt;<br>
&gt; &gt; &nbsp; &nbsp; MPI.File.Open(comm," h5ex_d_alloc.h5")<br>
&gt; &gt; &nbsp; File "File.pyx", line 67, in mpi4py.MPI.File.Open<br>
&gt; (src/mpi4py.MPI.c:89639)<br>
&gt; &gt; mpi4py.MPI.Exception: MPI_ERR_OTHER: known error not in list<br>
&gt; &gt;<br>
&gt; --------------------------------------------------------------------------<br>
&gt; &gt; mpirun noticed that the job aborted, but has no info as to the process<br>
&gt; &gt; that caused that situation.<br>
&gt; &gt;<br>
&gt; --------------------------------------------------------------------------<br>
&gt; &gt;<br>
&gt; &gt; My mpirun version is (Open MPI) 1.6.2. I installed openmpi using the<br>
&gt; dorsal script (<a href="https://github.com/FEniCS/dorsal" target="_blank">https://github.com/FEniCS/dorsal</a>) for Redhat Enterprise 6<br>
&gt; (OS I am using, release 6.5) . It configured the build as following :<br>
&gt; &gt;<br>
&gt; &gt; ./configure --enable-mpi-thread-multiple --enable-opal-multi-threads<br>
&gt; --with-threads=posix --disable-mpi-profile<br>
&gt; &gt;<br>
&gt; &gt; I need emphasize that I do not have root acces on the system I am<br>
&gt; running my application.<br>
&gt; &gt;<br>
&gt; &gt; Imran<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Jeff Squyres<br>
&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; For corporate legal information go to:<br>
&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; ------------------------------<br>
&gt;<br>
&gt; Message: 5<br>
&gt; Date: Tue, 6 May 2014 15:32:21 +0200<br>
&gt; From: Imran Ali &lt;<a href="mailto:imranal@student.matnat.uio.no">imranal@student.matnat.uio.no</a>&gt;<br>
&gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt; Subject: Re: [OMPI users] MPI File Open does not work<br>
&gt; Message-ID: &lt;<a href="mailto:FA6DFFFF-6C66-4A47-84FC-148FB51CE162@math.uio.no">FA6DFFFF-6C66-4A47-84FC-148FB51CE162@math.uio.no</a>&gt;<br>
&gt; Content-Type: text/plain; charset=us-ascii<br>
&gt;<br>
&gt;<br>
&gt; 6. mai 2014 kl. 14:56 skrev Jeff Squyres (jsquyres) &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;:<br>
&gt;<br>
&gt; &gt; The thread support in the 1.6 series is not very good. &nbsp;You might try:<br>
&gt; &gt;<br>
&gt; &gt; - Upgrading to 1.6.5<br>
&gt; &gt; - Or better yet, upgrading to 1.8.1<br>
&gt; &gt;<br>
&gt;<br>
&gt; I will attempt that than. I read at<br>
&gt;<br>
&gt; <a href="http://www.open-mpi.org/faq/?category=building#install-overwrite" target="_blank">http://www.open-mpi.org/faq/?category=building#install-overwrite</a><br>
&gt;<br>
&gt; that I should completely uninstall my previous version. Could you<br>
&gt; recommend to me how I can go about doing it (without root access).<br>
&gt; I am uncertain where I can use make uninstall.<br>
&gt;<br>
&gt; Imran<br>
&gt;<br>
&gt; &gt;<br>
&gt; &gt; On May 6, 2014, at 7:24 AM, Imran Ali &lt;<a href="mailto:imranal@student.matnat.uio.no">imranal@student.matnat.uio.no</a>&gt;<br>
&gt; wrote:<br>
&gt; &gt;<br>
&gt; &gt;&gt; I get the following error when I try to run the following python code<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; import mpi4py.MPI as MPI<br>
&gt; &gt;&gt; comm = &nbsp;MPI.COMM_WORLD<br>
&gt; &gt;&gt; MPI.File.Open(comm,"some.file")<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; $ mpirun -np 1 python test_mpi.py<br>
&gt; &gt;&gt; Traceback (most recent call last):<br>
&gt; &gt;&gt; &nbsp;File "test_mpi.py", line 3, in &lt;module&gt;<br>
&gt; &gt;&gt; &nbsp; &nbsp;MPI.File.Open(comm," h5ex_d_alloc.h5")<br>
&gt; &gt;&gt; &nbsp;File "File.pyx", line 67, in mpi4py.MPI.File.Open<br>
&gt; (src/mpi4py.MPI.c:89639)<br>
&gt; &gt;&gt; mpi4py.MPI.Exception: MPI_ERR_OTHER: known error not in list<br>
&gt; &gt;&gt;<br>
&gt; --------------------------------------------------------------------------<br>
&gt; &gt;&gt; mpirun noticed that the job aborted, but has no info as to the process<br>
&gt; &gt;&gt; that caused that situation.<br>
&gt; &gt;&gt;<br>
&gt; --------------------------------------------------------------------------<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; My mpirun version is (Open MPI) 1.6.2. I installed openmpi using the<br>
&gt; dorsal script (<a href="https://github.com/FEniCS/dorsal" target="_blank">https://github.com/FEniCS/dorsal</a>) for Redhat Enterprise 6<br>
&gt; (OS I am using, release 6.5) . It configured the build as following :<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; ./configure --enable-mpi-thread-multiple --enable-opal-multi-threads<br>
&gt; --with-threads=posix --disable-mpi-profile<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; I need emphasize that I do not have root acces on the system I am<br>
&gt; running my application.<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; Imran<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; _______________________________________________<br>
&gt; &gt;&gt; users mailing list<br>
&gt; &gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; --<br>
&gt; &gt; Jeff Squyres<br>
&gt; &gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; &gt; For corporate legal information go to:<br>
&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; ------------------------------<br>
&gt;<br>
&gt; Message: 6<br>
&gt; Date: Tue, 6 May 2014 13:34:38 +0000<br>
&gt; From: "Jeff Squyres (jsquyres)" &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>
&gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt; Subject: Re: [OMPI users] MPI File Open does not work<br>
&gt; Message-ID: &lt;<a href="mailto:2A933C0E-80F6-4DED-B44C-53B5F37EFC0C@cisco.com">2A933C0E-80F6-4DED-B44C-53B5F37EFC0C@cisco.com</a>&gt;<br>
&gt; Content-Type: text/plain; charset="us-ascii"<br>
&gt;<br>
&gt; On May 6, 2014, at 9:32 AM, Imran Ali &lt;<a href="mailto:imranal@student.matnat.uio.no">imranal@student.matnat.uio.no</a>&gt;<br>
&gt; wrote:<br>
&gt;<br>
&gt; &gt; I will attempt that than. I read at<br>
&gt; &gt;<br>
&gt; &gt; <a href="http://www.open-mpi.org/faq/?category=building#install-overwrite" target="_blank">http://www.open-mpi.org/faq/?category=building#install-overwrite</a><br>
&gt; &gt;<br>
&gt; &gt; that I should completely uninstall my previous version.<br>
&gt;<br>
&gt; Yes, that is best. &nbsp;OR: you can install into a whole separate tree and<br>
&gt; ignore the first installation.<br>
&gt;<br>
&gt; &gt; Could you recommend to me how I can go about doing it (without root<br>
&gt; access).<br>
&gt; &gt; I am uncertain where I can use make uninstall.<br>
&gt;<br>
&gt; If you don't have write access into the installation tree (i.e., it was<br>
&gt; installed via root and you don't have root access), then your best bet is<br>
&gt; simply to install into a new tree. &nbsp;E.g., if OMPI is installed into<br>
&gt; /opt/openmpi-1.6.2, try installing into /opt/openmpi-1.6.5, or even<br>
&gt; $HOME/installs/openmpi-1.6.5, or something like that.<br>
&gt;<br>
&gt; --<br>
&gt; Jeff Squyres<br>
&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; For corporate legal information go to:<br>
&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; ------------------------------<br>
&gt;<br>
&gt; Message: 7<br>
&gt; Date: Tue, 6 May 2014 15:40:34 +0200<br>
&gt; From: Imran Ali &lt;<a href="mailto:imranal@student.matnat.uio.no">imranal@student.matnat.uio.no</a>&gt;<br>
&gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt; Subject: Re: [OMPI users] MPI File Open does not work<br>
&gt; Message-ID: &lt;<a href="mailto:14F0596C-C5C5-4B1A-A4A8-8849D44AB76A@math.uio.no">14F0596C-C5C5-4B1A-A4A8-8849D44AB76A@math.uio.no</a>&gt;<br>
&gt; Content-Type: text/plain; charset=us-ascii<br>
&gt;<br>
&gt;<br>
&gt; 6. mai 2014 kl. 15:34 skrev Jeff Squyres (jsquyres) &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;:<br>
&gt;<br>
&gt; &gt; On May 6, 2014, at 9:32 AM, Imran Ali &lt;<a href="mailto:imranal@student.matnat.uio.no">imranal@student.matnat.uio.no</a>&gt;<br>
&gt; wrote:<br>
&gt; &gt;<br>
&gt; &gt;&gt; I will attempt that than. I read at<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; <a href="http://www.open-mpi.org/faq/?category=building#install-overwrite" target="_blank">http://www.open-mpi.org/faq/?category=building#install-overwrite</a><br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; that I should completely uninstall my previous version.<br>
&gt; &gt;<br>
&gt; &gt; Yes, that is best. &nbsp;OR: you can install into a whole separate tree and<br>
&gt; ignore the first installation.<br>
&gt; &gt;<br>
&gt; &gt;&gt; Could you recommend to me how I can go about doing it (without root<br>
&gt; access).<br>
&gt; &gt;&gt; I am uncertain where I can use make uninstall.<br>
&gt; &gt;<br>
&gt; &gt; If you don't have write access into the installation tree (i.e., it was<br>
&gt; installed via root and you don't have root access), then your best bet is<br>
&gt; simply to install into a new tree. &nbsp;E.g., if OMPI is installed into<br>
&gt; /opt/openmpi-1.6.2, try installing into /opt/openmpi-1.6.5, or even<br>
&gt; $HOME/installs/openmpi-1.6.5, or something like that.<br>
&gt;<br>
&gt; My install was in my user directory (i.e $HOME). I managed to locate the<br>
&gt; source directory and successfully run make uninstall.<br>
&gt;<br>
&gt; Will let you know how things went after installation.<br>
&gt;<br>
&gt; Imran<br>
&gt;<br>
&gt; &gt;<br>
&gt; &gt; --<br>
&gt; &gt; Jeff Squyres<br>
&gt; &gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; &gt; For corporate legal information go to:<br>
&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; ------------------------------<br>
&gt;<br>
&gt; Message: 8<br>
&gt; Date: Tue, 6 May 2014 14:42:52 +0000<br>
&gt; From: "Jeff Squyres (jsquyres)" &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>
&gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt; Subject: Re: [OMPI users] MPI File Open does not work<br>
&gt; Message-ID: &lt;<a href="mailto:710E3328-EDAA-4A13-9F07-B45FE319113D@cisco.com">710E3328-EDAA-4A13-9F07-B45FE319113D@cisco.com</a>&gt;<br>
&gt; Content-Type: text/plain; charset="us-ascii"<br>
&gt;<br>
&gt; On May 6, 2014, at 9:40 AM, Imran Ali &lt;<a href="mailto:imranal@student.matnat.uio.no">imranal@student.matnat.uio.no</a>&gt;<br>
&gt; wrote:<br>
&gt;<br>
&gt; &gt; My install was in my user directory (i.e $HOME). I managed to locate the<br>
&gt; source directory and successfully run make uninstall.<br>
&gt;<br>
&gt;<br>
&gt; FWIW, I usually install Open MPI into its own subdir. &nbsp;E.g.,<br>
&gt; $HOME/installs/openmpi-x.y.z. &nbsp;Then if I don't want that install any more,<br>
&gt; I can just "rm -rf $HOME/installs/openmpi-x.y.z" -- no need to "make<br>
&gt; uninstall". &nbsp;Specifically: if there's nothing else installed in the same<br>
&gt; tree as Open MPI, you can just rm -rf its installation tree.<br>
&gt;<br>
&gt; --<br>
&gt; Jeff Squyres<br>
&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; For corporate legal information go to:<br>
&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; ------------------------------<br>
&gt;<br>
&gt; Message: 9<br>
&gt; Date: Tue, 6 May 2014 14:50:34 +0000<br>
&gt; From: "Jeff Squyres (jsquyres)" &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>
&gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt; Subject: Re: [OMPI users] users Digest, Vol 2879, Issue 1<br>
&gt; Message-ID: &lt;<a href="mailto:C60AA7E1-96A7-4CCD-9B5B-11A38FB87772@cisco.com">C60AA7E1-96A7-4CCD-9B5B-11A38FB87772@cisco.com</a>&gt;<br>
&gt; Content-Type: text/plain; charset="us-ascii"<br>
&gt;<br>
&gt; Are you using TCP as the MPI transport?<br>
&gt;<br>
&gt; If so, another thing to try is to limit the IP interfaces that MPI uses<br>
&gt; for its traffic to see if there's some kind of problem with specific<br>
&gt; networks.<br>
&gt;<br>
&gt; For example:<br>
&gt;<br>
&gt; &nbsp; &nbsp;mpirun --mca btl_tcp_if_include eth0 ...<br>
&gt;<br>
&gt; If that works, then try adding in any/all other IP interfaces that you<br>
&gt; have on your machines.<br>
&gt;<br>
&gt; A sorta-wild guess: you have some IP interfaces that aren't working, or at<br>
&gt; least, don't work in the way that OMPI wants them to work. &nbsp;So the first<br>
&gt; barrier works because it flows across eth0 (or some other first network<br>
&gt; that, as far as OMPI is concerned, works just fine). &nbsp;But then the next<br>
&gt; barrier round-robin advances to the next IP interface, and it doesn't work<br>
&gt; for some reason.<br>
&gt;<br>
&gt; We've seen virtual machine bridge interfaces cause problems, for example.<br>
&gt; &nbsp;E.g., if a machine has a Xen virtual machine interface (vibr0, IIRC?),<br>
&gt; then OMPI will try to use it to communicate with peer MPI processes because<br>
&gt; it has a "compatible" IP address, and OMPI think it should be<br>
&gt; connected/reachable to peers. &nbsp;If this is the case, you might want to<br>
&gt; disable such interfaces and/or use btl_tcp_if_include or btl_tcp_if_exclude<br>
&gt; to select the interfaces that you want to use.<br>
&gt;<br>
&gt; Pro tip: if you use btl_tcp_if_exclude, remember to exclude the loopback<br>
&gt; interface, too. &nbsp;OMPI defaults to a btl_tcp_if_include="" (blank) and<br>
&gt; btl_tcp_if_exclude="lo0". So if you override btl_tcp_if_exclude, you need<br>
&gt; to be sure to *also* include lo0 in the new value. &nbsp;For example:<br>
&gt;<br>
&gt; &nbsp; &nbsp;mpirun --mca btl_tcp_if_exclude lo0,virb0 ...<br>
&gt;<br>
&gt; Also, if possible, try upgrading to Open MPI 1.8.1.<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; On May 4, 2014, at 2:15 PM, Clay Kirkland &lt;<a href="mailto:clay.kirkland@versityinc.com">clay.kirkland@versityinc.com</a>&gt;<br>
&gt; wrote:<br>
&gt;<br>
&gt; &gt; &nbsp;I am configuring with all defaults. &nbsp; Just doing a ./configure and then<br>
&gt; &gt; make and make install. &nbsp; I have used open mpi on several kinds of<br>
&gt; &gt; unix &nbsp;systems this way and have had no trouble before. &nbsp; I believe I<br>
&gt; &gt; last had success on a redhat version of linux.<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; On Sat, May 3, 2014 at 11:00 AM, &lt;<a href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a>&gt; wrote:<br>
&gt; &gt; Send users mailing list submissions to<br>
&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt;<br>
&gt; &gt; To subscribe or unsubscribe via the World Wide Web, visit<br>
&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; &gt; or, via email, send a message with subject or body 'help' to<br>
&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a><br>
&gt; &gt;<br>
&gt; &gt; You can reach the person managing the list at<br>
&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:users-owner@open-mpi.org">users-owner@open-mpi.org</a><br>
&gt; &gt;<br>
&gt; &gt; When replying, please edit your Subject line so it is more specific<br>
&gt; &gt; than "Re: Contents of users digest..."<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; Today's Topics:<br>
&gt; &gt;<br>
&gt; &gt; &nbsp; &nbsp;1. MPI_Barrier hangs on second attempt but only when multiple<br>
&gt; &gt; &nbsp; &nbsp; &nbsp; hosts used. (Clay Kirkland)<br>
&gt; &gt; &nbsp; &nbsp;2. Re: MPI_Barrier hangs on second attempt but only when<br>
&gt; &gt; &nbsp; &nbsp; &nbsp; multiple hosts used. (Ralph Castain)<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; ----------------------------------------------------------------------<br>
&gt; &gt;<br>
&gt; &gt; Message: 1<br>
&gt; &gt; Date: Fri, 2 May 2014 16:24:04 -0500<br>
&gt; &gt; From: Clay Kirkland &lt;<a href="mailto:clay.kirkland@versityinc.com">clay.kirkland@versityinc.com</a>&gt;<br>
&gt; &gt; To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; Subject: [OMPI users] MPI_Barrier hangs on second attempt but only<br>
&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; when &nbsp; &nbsp;multiple hosts used.<br>
&gt; &gt; Message-ID:<br>
&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;CAJDnjA8Wi=FEjz6Vz+Bc34b+nFE=<br>
&gt; <a href="mailto:TF4B7g0BQgMbeKg7H-pV%2BA@mail.gmail.com">TF4B7g0BQgMbeKg7H-pV+A@mail.gmail.com</a>&gt;<br>
&gt; &gt; Content-Type: text/plain; charset="utf-8"<br>
&gt; &gt;<br>
&gt; &gt; I have been using MPI for many many years so I have very well debugged<br>
&gt; mpi<br>
&gt; &gt; tests. &nbsp; I am<br>
&gt; &gt; having trouble on either openmpi-1.4.5 &nbsp;or &nbsp;openmpi-1.6.5 versions though<br>
&gt; &gt; with getting the<br>
&gt; &gt; MPI_Barrier calls to work. &nbsp; It works fine when I run all processes on<br>
&gt; one<br>
&gt; &gt; machine but when<br>
&gt; &gt; I run with two or more hosts the second call to MPI_Barrier always hangs.<br>
&gt; &gt; Not the first one,<br>
&gt; &gt; but always the second one. &nbsp; I looked at FAQ's and such but found nothing<br>
&gt; &gt; except for a comment<br>
&gt; &gt; that MPI_Barrier problems were often problems with fire walls. &nbsp;Also<br>
&gt; &gt; mentioned as a problem<br>
&gt; &gt; was not having the same version of mpi on both machines. &nbsp;I turned<br>
&gt; &gt; firewalls off and removed<br>
&gt; &gt; and reinstalled the same version on both hosts but I still see the same<br>
&gt; &gt; thing. &nbsp; I then installed<br>
&gt; &gt; lam mpi on two of my machines and that works fine. &nbsp; I can call the<br>
&gt; &gt; MPI_Barrier function when run on<br>
&gt; &gt; one of two machines by itself &nbsp;many times with no hangs. &nbsp;Only hangs if<br>
&gt; two<br>
&gt; &gt; or more hosts are involved.<br>
&gt; &gt; These runs are all being done on CentOS release 6.4. &nbsp; Here is test<br>
&gt; program<br>
&gt; &gt; I used.<br>
&gt; &gt;<br>
&gt; &gt; main (argc, argv)<br>
&gt; &gt; int argc;<br>
&gt; &gt; char **argv;<br>
&gt; &gt; {<br>
&gt; &gt; &nbsp; &nbsp; char message[20];<br>
&gt; &gt; &nbsp; &nbsp; char hoster[256];<br>
&gt; &gt; &nbsp; &nbsp; char nameis[256];<br>
&gt; &gt; &nbsp; &nbsp; int fd, i, j, jnp, iret, myrank, np, ranker, recker;<br>
&gt; &gt; &nbsp; &nbsp; MPI_Comm comm;<br>
&gt; &gt; &nbsp; &nbsp; MPI_Status status;<br>
&gt; &gt;<br>
&gt; &gt; &nbsp; &nbsp; MPI_Init( &amp;argc, &amp;argv );<br>
&gt; &gt; &nbsp; &nbsp; MPI_Comm_rank( MPI_COMM_WORLD, &amp;myrank);<br>
&gt; &gt; &nbsp; &nbsp; MPI_Comm_size( MPI_COMM_WORLD, &amp;np);<br>
&gt; &gt;<br>
&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; gethostname(hoster,256);<br>
&gt; &gt;<br>
&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt; &gt; 1.\n",myrank,hoster);<br>
&gt; &gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt; &gt; 2.\n",myrank,hoster);<br>
&gt; &gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt; &gt; 3.\n",myrank,hoster);<br>
&gt; &gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt; &gt; &nbsp; &nbsp; MPI_Finalize();<br>
&gt; &gt; &nbsp; &nbsp; exit(0);<br>
&gt; &gt; }<br>
&gt; &gt;<br>
&gt; &gt; &nbsp; Here are three runs of test program. &nbsp;First with two processes on one<br>
&gt; &gt; host, then with<br>
&gt; &gt; two processes on another host, and finally with one process on each of<br>
&gt; two<br>
&gt; &gt; hosts. &nbsp;The<br>
&gt; &gt; first two runs are fine but the last run hangs on the second MPI_Barrier.<br>
&gt; &gt;<br>
&gt; &gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host centos a.out<br>
&gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 1.<br>
&gt; &gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 1.<br>
&gt; &gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 2.<br>
&gt; &gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 3.<br>
&gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 2.<br>
&gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 3.<br>
&gt; &gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host RAID a.out<br>
&gt; &gt; /root/.bashrc: line 14: unalias: ls: not found<br>
&gt; &gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt; &gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt; &gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 3.<br>
&gt; &gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt; &gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt; &gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 3.<br>
&gt; &gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host centos,RAID a.out<br>
&gt; &gt; /root/.bashrc: line 14: unalias: ls: not found<br>
&gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 1.<br>
&gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 2.<br>
&gt; &gt; In rank 1 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt; &gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt; &gt;<br>
&gt; &gt; &nbsp; Since it is such a simple test and problem and such a widely used MPI<br>
&gt; &gt; function, it must obviously<br>
&gt; &gt; be an installation or configuration problem. &nbsp; A pstack for each of the<br>
&gt; &gt; hung MPI_Barrier processes<br>
&gt; &gt; on the two machines shows this:<br>
&gt; &gt;<br>
&gt; &gt; [root@centos ~]# pstack 31666<br>
&gt; &gt; #0 &nbsp;0x0000003baf0e8ee3 in __epoll_wait_nocancel () from /lib64/libc.so.6<br>
&gt; &gt; #1 &nbsp;0x00007f5de06125eb in epoll_dispatch () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; #2 &nbsp;0x00007f5de061475a in opal_event_base_loop () from<br>
&gt; &gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; #3 &nbsp;0x00007f5de0639229 in opal_progress () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; #4 &nbsp;0x00007f5de0586f75 in ompi_request_default_wait_all () from<br>
&gt; &gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; #5 &nbsp;0x00007f5ddc59565e in ompi_coll_tuned_sendrecv_actual () from<br>
&gt; &gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt; &gt; #6 &nbsp;0x00007f5ddc59d8ff in ompi_coll_tuned_barrier_intra_two_procs () from<br>
&gt; &gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt; &gt; #7 &nbsp;0x00007f5de05941c2 in PMPI_Barrier () from /usr/local/lib/libmpi.so.1<br>
&gt; &gt; #8 &nbsp;0x0000000000400a43 in main ()<br>
&gt; &gt;<br>
&gt; &gt; [root@RAID openmpi-1.6.5]# pstack 22167<br>
&gt; &gt; #0 &nbsp;0x00000030302e8ee3 in __epoll_wait_nocancel () from /lib64/libc.so.6<br>
&gt; &gt; #1 &nbsp;0x00007f7ee46885eb in epoll_dispatch () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; #2 &nbsp;0x00007f7ee468a75a in opal_event_base_loop () from<br>
&gt; &gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; #3 &nbsp;0x00007f7ee46af229 in opal_progress () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; #4 &nbsp;0x00007f7ee45fcf75 in ompi_request_default_wait_all () from<br>
&gt; &gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; #5 &nbsp;0x00007f7ee060b65e in ompi_coll_tuned_sendrecv_actual () from<br>
&gt; &gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt; &gt; #6 &nbsp;0x00007f7ee06138ff in ompi_coll_tuned_barrier_intra_two_procs () from<br>
&gt; &gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt; &gt; #7 &nbsp;0x00007f7ee460a1c2 in PMPI_Barrier () from /usr/local/lib/libmpi.so.1<br>
&gt; &gt; #8 &nbsp;0x0000000000400a43 in main ()<br>
&gt; &gt;<br>
&gt; &gt; &nbsp;Which looks exactly the same on each machine. &nbsp;Any thoughts or ideas<br>
&gt; would<br>
&gt; &gt; be greatly appreciated as<br>
&gt; &gt; I am stuck.<br>
&gt; &gt;<br>
&gt; &gt; &nbsp;Clay Kirkland<br>
&gt; &gt; -------------- next part --------------<br>
&gt; &gt; HTML attachment scrubbed and removed<br>
&gt; &gt;<br>
&gt; &gt; ------------------------------<br>
&gt; &gt;<br>
&gt; &gt; Message: 2<br>
&gt; &gt; Date: Sat, 3 May 2014 06:39:20 -0700<br>
&gt; &gt; From: Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;<br>
&gt; &gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt; &gt; Subject: Re: [OMPI users] MPI_Barrier hangs on second attempt but only<br>
&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; when &nbsp; &nbsp;multiple hosts used.<br>
&gt; &gt; Message-ID: &lt;<a href="mailto:3CF53D73-15D9-40BB-A2DE-50BA3561A0F4@open-mpi.org">3CF53D73-15D9-40BB-A2DE-50BA3561A0F4@open-mpi.org</a>&gt;<br>
&gt; &gt; Content-Type: text/plain; charset="us-ascii"<br>
&gt; &gt;<br>
&gt; &gt; Hmmm...just testing on my little cluster here on two nodes, it works<br>
&gt; just fine with 1.8.2:<br>
&gt; &gt;<br>
&gt; &gt; [rhc@bend001 v1.8]$ mpirun -n 2 --map-by node ./a.out<br>
&gt; &gt; &nbsp;In rank 0 and host= bend001 &nbsp;Do Barrier call 1.<br>
&gt; &gt; &nbsp;In rank 0 and host= bend001 &nbsp;Do Barrier call 2.<br>
&gt; &gt; &nbsp;In rank 0 and host= bend001 &nbsp;Do Barrier call 3.<br>
&gt; &gt; &nbsp;In rank 1 and host= bend002 &nbsp;Do Barrier call 1.<br>
&gt; &gt; &nbsp;In rank 1 and host= bend002 &nbsp;Do Barrier call 2.<br>
&gt; &gt; &nbsp;In rank 1 and host= bend002 &nbsp;Do Barrier call 3.<br>
&gt; &gt; [rhc@bend001 v1.8]$<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; How are you configuring OMPI?<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; On May 2, 2014, at 2:24 PM, Clay Kirkland &lt;<a href="mailto:clay.kirkland@versityinc.com">clay.kirkland@versityinc.com</a>&gt;<br>
&gt; wrote:<br>
&gt; &gt;<br>
&gt; &gt; &gt; I have been using MPI for many many years so I have very well debugged<br>
&gt; mpi tests. &nbsp; I am<br>
&gt; &gt; &gt; having trouble on either openmpi-1.4.5 &nbsp;or &nbsp;openmpi-1.6.5 versions<br>
&gt; though with getting the<br>
&gt; &gt; &gt; MPI_Barrier calls to work. &nbsp; It works fine when I run all processes on<br>
&gt; one machine but when<br>
&gt; &gt; &gt; I run with two or more hosts the second call to MPI_Barrier always<br>
&gt; hangs. &nbsp; Not the first one,<br>
&gt; &gt; &gt; but always the second one. &nbsp; I looked at FAQ's and such but found<br>
&gt; nothing except for a comment<br>
&gt; &gt; &gt; that MPI_Barrier problems were often problems with fire walls. &nbsp;Also<br>
&gt; mentioned as a problem<br>
&gt; &gt; &gt; was not having the same version of mpi on both machines. &nbsp;I turned<br>
&gt; firewalls off and removed<br>
&gt; &gt; &gt; and reinstalled the same version on both hosts but I still see the<br>
&gt; same thing. &nbsp; I then installed<br>
&gt; &gt; &gt; lam mpi on two of my machines and that works fine. &nbsp; I can call the<br>
&gt; MPI_Barrier function when run on<br>
&gt; &gt; &gt; one of two machines by itself &nbsp;many times with no hangs. &nbsp;Only hangs<br>
&gt; if two or more hosts are involved.<br>
&gt; &gt; &gt; These runs are all being done on CentOS release 6.4. &nbsp; Here is test<br>
&gt; program I used.<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; main (argc, argv)<br>
&gt; &gt; &gt; int argc;<br>
&gt; &gt; &gt; char **argv;<br>
&gt; &gt; &gt; {<br>
&gt; &gt; &gt; &nbsp; &nbsp; char message[20];<br>
&gt; &gt; &gt; &nbsp; &nbsp; char hoster[256];<br>
&gt; &gt; &gt; &nbsp; &nbsp; char nameis[256];<br>
&gt; &gt; &gt; &nbsp; &nbsp; int fd, i, j, jnp, iret, myrank, np, ranker, recker;<br>
&gt; &gt; &gt; &nbsp; &nbsp; MPI_Comm comm;<br>
&gt; &gt; &gt; &nbsp; &nbsp; MPI_Status status;<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; &nbsp; &nbsp; MPI_Init( &amp;argc, &amp;argv );<br>
&gt; &gt; &gt; &nbsp; &nbsp; MPI_Comm_rank( MPI_COMM_WORLD, &amp;myrank);<br>
&gt; &gt; &gt; &nbsp; &nbsp; MPI_Comm_size( MPI_COMM_WORLD, &amp;np);<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; gethostname(hoster,256);<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt; 1.\n",myrank,hoster);<br>
&gt; &gt; &gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt; &gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt; 2.\n",myrank,hoster);<br>
&gt; &gt; &gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt; &gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt; 3.\n",myrank,hoster);<br>
&gt; &gt; &gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt; &gt; &gt; &nbsp; &nbsp; MPI_Finalize();<br>
&gt; &gt; &gt; &nbsp; &nbsp; exit(0);<br>
&gt; &gt; &gt; }<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; &nbsp; Here are three runs of test program. &nbsp;First with two processes on<br>
&gt; one host, then with<br>
&gt; &gt; &gt; two processes on another host, and finally with one process on each of<br>
&gt; two hosts. &nbsp;The<br>
&gt; &gt; &gt; first two runs are fine but the last run hangs on the second<br>
&gt; MPI_Barrier.<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host centos a.out<br>
&gt; &gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 1.<br>
&gt; &gt; &gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 1.<br>
&gt; &gt; &gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 2.<br>
&gt; &gt; &gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 3.<br>
&gt; &gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 2.<br>
&gt; &gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 3.<br>
&gt; &gt; &gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host RAID a.out<br>
&gt; &gt; &gt; /root/.bashrc: line 14: unalias: ls: not found<br>
&gt; &gt; &gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt; &gt; &gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt; &gt; &gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 3.<br>
&gt; &gt; &gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt; &gt; &gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt; &gt; &gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 3.<br>
&gt; &gt; &gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host centos,RAID<br>
&gt; a.out<br>
&gt; &gt; &gt; /root/.bashrc: line 14: unalias: ls: not found<br>
&gt; &gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 1.<br>
&gt; &gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 2.<br>
&gt; &gt; &gt; In rank 1 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt; &gt; &gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; &nbsp; Since it is such a simple test and problem and such a widely used<br>
&gt; MPI function, it must obviously<br>
&gt; &gt; &gt; be an installation or configuration problem. &nbsp; A pstack for each of<br>
&gt; the hung MPI_Barrier processes<br>
&gt; &gt; &gt; on the two machines shows this:<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; [root@centos ~]# pstack 31666<br>
&gt; &gt; &gt; #0 &nbsp;0x0000003baf0e8ee3 in __epoll_wait_nocancel () from<br>
&gt; /lib64/libc.so.6<br>
&gt; &gt; &gt; #1 &nbsp;0x00007f5de06125eb in epoll_dispatch () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; &gt; #2 &nbsp;0x00007f5de061475a in opal_event_base_loop () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; &gt; #3 &nbsp;0x00007f5de0639229 in opal_progress () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; &gt; #4 &nbsp;0x00007f5de0586f75 in ompi_request_default_wait_all () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; &gt; #5 &nbsp;0x00007f5ddc59565e in ompi_coll_tuned_sendrecv_actual () from<br>
&gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt; &gt; &gt; #6 &nbsp;0x00007f5ddc59d8ff in ompi_coll_tuned_barrier_intra_two_procs ()<br>
&gt; from /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt; &gt; &gt; #7 &nbsp;0x00007f5de05941c2 in PMPI_Barrier () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; &gt; #8 &nbsp;0x0000000000400a43 in main ()<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; [root@RAID openmpi-1.6.5]# pstack 22167<br>
&gt; &gt; &gt; #0 &nbsp;0x00000030302e8ee3 in __epoll_wait_nocancel () from<br>
&gt; /lib64/libc.so.6<br>
&gt; &gt; &gt; #1 &nbsp;0x00007f7ee46885eb in epoll_dispatch () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; &gt; #2 &nbsp;0x00007f7ee468a75a in opal_event_base_loop () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; &gt; #3 &nbsp;0x00007f7ee46af229 in opal_progress () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; &gt; #4 &nbsp;0x00007f7ee45fcf75 in ompi_request_default_wait_all () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; &gt; #5 &nbsp;0x00007f7ee060b65e in ompi_coll_tuned_sendrecv_actual () from<br>
&gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt; &gt; &gt; #6 &nbsp;0x00007f7ee06138ff in ompi_coll_tuned_barrier_intra_two_procs ()<br>
&gt; from /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt; &gt; &gt; #7 &nbsp;0x00007f7ee460a1c2 in PMPI_Barrier () from<br>
&gt; /usr/local/lib/libmpi.so.1<br>
&gt; &gt; &gt; #8 &nbsp;0x0000000000400a43 in main ()<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; &nbsp;Which looks exactly the same on each machine. &nbsp;Any thoughts or ideas<br>
&gt; would be greatly appreciated as<br>
&gt; &gt; &gt; I am stuck.<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; &nbsp;Clay Kirkland<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; _______________________________________________<br>
&gt; &gt; &gt; users mailing list<br>
&gt; &gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; &gt;<br>
&gt; &gt; -------------- next part --------------<br>
&gt; &gt; HTML attachment scrubbed and removed<br>
&gt; &gt;<br>
&gt; &gt; ------------------------------<br>
&gt; &gt;<br>
&gt; &gt; Subject: Digest Footer<br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; &gt;<br>
&gt; &gt; ------------------------------<br>
&gt; &gt;<br>
&gt; &gt; End of users Digest, Vol 2879, Issue 1<br>
&gt; &gt; **************************************<br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Jeff Squyres<br>
&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; For corporate legal information go to:<br>
&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; ------------------------------<br>
&gt;<br>
&gt; Subject: Digest Footer<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt; ------------------------------<br>
&gt;<br>
&gt; End of users Digest, Vol 2881, Issue 1<br>
&gt; **************************************<br>
&gt;<br>
-------------- next part --------------<br>
HTML attachment scrubbed and removed<br>
<br>
------------------------------<br>
<br>
Message: 2<br>
Date: Tue, 6 May 2014 18:50:50 -0500<br>
From: Clay Kirkland &lt;<a href="mailto:clay.kirkland@versityinc.com">clay.kirkland@versityinc.com</a>&gt;<br>
To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subject: Re: [OMPI users] users Digest, Vol 2881, Issue 1<br>
Message-ID:<br>
&nbsp; &nbsp; &nbsp; &nbsp; &lt;<a href="mailto:CAJDnjA-U4BTpto%2B87CZSho81t%2B-A1JzOTTc7Mwdfiar7%2BVZMzQ@mail.gmail.com">CAJDnjA-U4BTpto+87CZSho81t+-A1JzOTTc7Mwdfiar7+VZMzQ@mail.gmail.com</a>&gt;<br>
Content-Type: text/plain; charset="utf-8"<br>
<br>
&nbsp;Well it turns out &nbsp;I can't seem to get all three of my machines on the<br>
same page.<br>
Two of them are using eth0 and one is using eth1. &nbsp; Centos seems unable to<br>
bring<br>
up multiple network interfaces for some reason and when I use the mca param<br>
to<br>
use eth0 it works on two machines but not the other. &nbsp; Is there some way to<br>
use<br>
only eth1 on one host and only eth0 on the other two? &nbsp; Maybe environment<br>
variables<br>
but I can't seem to get that to work either.<br>
<br>
&nbsp;Clay<br>
<br>
<br>
On Tue, May 6, 2014 at 6:28 PM, Clay Kirkland<br>
&lt;<a href="mailto:clay.kirkland@versityinc.com">clay.kirkland@versityinc.com</a>&gt;wrote:<br>
<br>
&gt; &nbsp;That last trick seems to work. &nbsp;I can get it to work once in a while with<br>
&gt; those tcp options but it is<br>
&gt; tricky as I have three machines and two of them use eth0 as primary<br>
&gt; network interface and one<br>
&gt; uses eth1. &nbsp; But by fiddling with network options and perhaps moving a<br>
&gt; cable or two I think I can<br>
&gt; get it all to work &nbsp; &nbsp;Thanks much for the tip.<br>
&gt;<br>
&gt; &nbsp;Clay<br>
&gt;<br>
&gt;<br>
&gt; On Tue, May 6, 2014 at 11:00 AM, &lt;<a href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a>&gt; wrote:<br>
&gt;<br>
&gt;&gt; Send users mailing list submissions to<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;<br>
&gt;&gt; To subscribe or unsubscribe via the World Wide Web, visit<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; or, via email, send a message with subject or body 'help' to<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a><br>
&gt;&gt;<br>
&gt;&gt; You can reach the person managing the list at<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:users-owner@open-mpi.org">users-owner@open-mpi.org</a><br>
&gt;&gt;<br>
&gt;&gt; When replying, please edit your Subject line so it is more specific<br>
&gt;&gt; than "Re: Contents of users digest..."<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Today's Topics:<br>
&gt;&gt;<br>
&gt;&gt; &nbsp; &nbsp;1. Re: MPI_Barrier hangs on second attempt but only &nbsp;when<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; multiple hosts used. (Daniels, Marcus G)<br>
&gt;&gt; &nbsp; &nbsp;2. ROMIO bug reading darrays (Richard Shaw)<br>
&gt;&gt; &nbsp; &nbsp;3. MPI File Open does not work (Imran Ali)<br>
&gt;&gt; &nbsp; &nbsp;4. Re: MPI File Open does not work (Jeff Squyres (jsquyres))<br>
&gt;&gt; &nbsp; &nbsp;5. Re: MPI File Open does not work (Imran Ali)<br>
&gt;&gt; &nbsp; &nbsp;6. Re: MPI File Open does not work (Jeff Squyres (jsquyres))<br>
&gt;&gt; &nbsp; &nbsp;7. Re: MPI File Open does not work (Imran Ali)<br>
&gt;&gt; &nbsp; &nbsp;8. Re: MPI File Open does not work (Jeff Squyres (jsquyres))<br>
&gt;&gt; &nbsp; &nbsp;9. Re: users Digest, Vol 2879, Issue 1 (Jeff Squyres (jsquyres))<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; ----------------------------------------------------------------------<br>
&gt;&gt;<br>
&gt;&gt; Message: 1<br>
&gt;&gt; Date: Mon, 5 May 2014 19:28:07 +0000<br>
&gt;&gt; From: "Daniels, Marcus G" &lt;<a href="mailto:mdaniels@lanl.gov">mdaniels@lanl.gov</a>&gt;<br>
&gt;&gt; To: "'<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>'" &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt;&gt; Subject: Re: [OMPI users] MPI_Barrier hangs on second attempt but only<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; when &nbsp; &nbsp;multiple hosts used.<br>
&gt;&gt; Message-ID:<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;<br>
&gt;&gt; <a href="mailto:532C594B7920A549A2A91CB4312CC57640DC5007@ECS-EXG-P-MB01.win.lanl.gov">532C594B7920A549A2A91CB4312CC57640DC5007@ECS-EXG-P-MB01.win.lanl.gov</a>&gt;<br>
&gt;&gt; Content-Type: text/plain; charset="utf-8"<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; From: Clay Kirkland [mailto:<a href="mailto:clay.kirkland@versityinc.com">clay.kirkland@versityinc.com</a>]<br>
&gt;&gt; Sent: Friday, May 02, 2014 03:24 PM<br>
&gt;&gt; To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a> &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt;&gt; Subject: [OMPI users] MPI_Barrier hangs on second attempt but only when<br>
&gt;&gt; multiple hosts used.<br>
&gt;&gt;<br>
&gt;&gt; I have been using MPI for many many years so I have very well debugged<br>
&gt;&gt; mpi tests. &nbsp; I am<br>
&gt;&gt; having trouble on either openmpi-1.4.5 &nbsp;or &nbsp;openmpi-1.6.5 versions though<br>
&gt;&gt; with getting the<br>
&gt;&gt; MPI_Barrier calls to work. &nbsp; It works fine when I run all processes on<br>
&gt;&gt; one machine but when<br>
&gt;&gt; I run with two or more hosts the second call to MPI_Barrier always hangs.<br>
&gt;&gt; &nbsp; Not the first one,<br>
&gt;&gt; but always the second one. &nbsp; I looked at FAQ's and such but found nothing<br>
&gt;&gt; except for a comment<br>
&gt;&gt; that MPI_Barrier problems were often problems with fire walls. &nbsp;Also<br>
&gt;&gt; mentioned as a problem<br>
&gt;&gt; was not having the same version of mpi on both machines. &nbsp;I turned<br>
&gt;&gt; firewalls off and removed<br>
&gt;&gt; and reinstalled the same version on both hosts but I still see the same<br>
&gt;&gt; thing. &nbsp; I then installed<br>
&gt;&gt; lam mpi on two of my machines and that works fine. &nbsp; I can call the<br>
&gt;&gt; MPI_Barrier function when run on<br>
&gt;&gt; one of two machines by itself &nbsp;many times with no hangs. &nbsp;Only hangs if<br>
&gt;&gt; two or more hosts are involved.<br>
&gt;&gt; These runs are all being done on CentOS release 6.4. &nbsp; Here is test<br>
&gt;&gt; program I used.<br>
&gt;&gt;<br>
&gt;&gt; main (argc, argv)<br>
&gt;&gt; int argc;<br>
&gt;&gt; char **argv;<br>
&gt;&gt; {<br>
&gt;&gt; &nbsp; &nbsp; char message[20];<br>
&gt;&gt; &nbsp; &nbsp; char hoster[256];<br>
&gt;&gt; &nbsp; &nbsp; char nameis[256];<br>
&gt;&gt; &nbsp; &nbsp; int fd, i, j, jnp, iret, myrank, np, ranker, recker;<br>
&gt;&gt; &nbsp; &nbsp; MPI_Comm comm;<br>
&gt;&gt; &nbsp; &nbsp; MPI_Status status;<br>
&gt;&gt;<br>
&gt;&gt; &nbsp; &nbsp; MPI_Init( &amp;argc, &amp;argv );<br>
&gt;&gt; &nbsp; &nbsp; MPI_Comm_rank( MPI_COMM_WORLD, &amp;myrank);<br>
&gt;&gt; &nbsp; &nbsp; MPI_Comm_size( MPI_COMM_WORLD, &amp;np);<br>
&gt;&gt;<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; gethostname(hoster,256);<br>
&gt;&gt;<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt;&gt; 1.\n",myrank,hoster);<br>
&gt;&gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt;&gt; 2.\n",myrank,hoster);<br>
&gt;&gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt;&gt; 3.\n",myrank,hoster);<br>
&gt;&gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt;&gt; &nbsp; &nbsp; MPI_Finalize();<br>
&gt;&gt; &nbsp; &nbsp; exit(0);<br>
&gt;&gt; }<br>
&gt;&gt;<br>
&gt;&gt; &nbsp; Here are three runs of test program. &nbsp;First with two processes on one<br>
&gt;&gt; host, then with<br>
&gt;&gt; two processes on another host, and finally with one process on each of<br>
&gt;&gt; two hosts. &nbsp;The<br>
&gt;&gt; first two runs are fine but the last run hangs on the second MPI_Barrier.<br>
&gt;&gt;<br>
&gt;&gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host centos a.out<br>
&gt;&gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 1.<br>
&gt;&gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 1.<br>
&gt;&gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 2.<br>
&gt;&gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 3.<br>
&gt;&gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 2.<br>
&gt;&gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 3.<br>
&gt;&gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host RAID a.out<br>
&gt;&gt; /root/.bashrc: line 14: unalias: ls: not found<br>
&gt;&gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt;&gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt;&gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 3.<br>
&gt;&gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt;&gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt;&gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 3.<br>
&gt;&gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host centos,RAID a.out<br>
&gt;&gt; /root/.bashrc: line 14: unalias: ls: not found<br>
&gt;&gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 1.<br>
&gt;&gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 2.<br>
&gt;&gt; In rank 1 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt;&gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt;&gt;<br>
&gt;&gt; &nbsp; Since it is such a simple test and problem and such a widely used MPI<br>
&gt;&gt; function, it must obviously<br>
&gt;&gt; be an installation or configuration problem. &nbsp; A pstack for each of the<br>
&gt;&gt; hung MPI_Barrier processes<br>
&gt;&gt; on the two machines shows this:<br>
&gt;&gt;<br>
&gt;&gt; [root@centos ~]# pstack 31666<br>
&gt;&gt; #0 &nbsp;0x0000003baf0e8ee3 in __epoll_wait_nocancel () from /lib64/libc.so.6<br>
&gt;&gt; #1 &nbsp;0x00007f5de06125eb in epoll_dispatch () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; #2 &nbsp;0x00007f5de061475a in opal_event_base_loop () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; #3 &nbsp;0x00007f5de0639229 in opal_progress () from /usr/local/lib/libmpi.so.1<br>
&gt;&gt; #4 &nbsp;0x00007f5de0586f75 in ompi_request_default_wait_all () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; #5 &nbsp;0x00007f5ddc59565e in ompi_coll_tuned_sendrecv_actual () from<br>
&gt;&gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt;&gt; #6 &nbsp;0x00007f5ddc59d8ff in ompi_coll_tuned_barrier_intra_two_procs () from<br>
&gt;&gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt;&gt; #7 &nbsp;0x00007f5de05941c2 in PMPI_Barrier () from /usr/local/lib/libmpi.so.1<br>
&gt;&gt; #8 &nbsp;0x0000000000400a43 in main ()<br>
&gt;&gt;<br>
&gt;&gt; [root@RAID openmpi-1.6.5]# pstack 22167<br>
&gt;&gt; #0 &nbsp;0x00000030302e8ee3 in __epoll_wait_nocancel () from /lib64/libc.so.6<br>
&gt;&gt; #1 &nbsp;0x00007f7ee46885eb in epoll_dispatch () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; #2 &nbsp;0x00007f7ee468a75a in opal_event_base_loop () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; #3 &nbsp;0x00007f7ee46af229 in opal_progress () from /usr/local/lib/libmpi.so.1<br>
&gt;&gt; #4 &nbsp;0x00007f7ee45fcf75 in ompi_request_default_wait_all () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; #5 &nbsp;0x00007f7ee060b65e in ompi_coll_tuned_sendrecv_actual () from<br>
&gt;&gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt;&gt; #6 &nbsp;0x00007f7ee06138ff in ompi_coll_tuned_barrier_intra_two_procs () from<br>
&gt;&gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt;&gt; #7 &nbsp;0x00007f7ee460a1c2 in PMPI_Barrier () from /usr/local/lib/libmpi.so.1<br>
&gt;&gt; #8 &nbsp;0x0000000000400a43 in main ()<br>
&gt;&gt;<br>
&gt;&gt; &nbsp;Which looks exactly the same on each machine. &nbsp;Any thoughts or ideas<br>
&gt;&gt; would be greatly appreciated as<br>
&gt;&gt; I am stuck.<br>
&gt;&gt;<br>
&gt;&gt; &nbsp;Clay Kirkland<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; -------------- next part --------------<br>
&gt;&gt; HTML attachment scrubbed and removed<br>
&gt;&gt;<br>
&gt;&gt; ------------------------------<br>
&gt;&gt;<br>
&gt;&gt; Message: 2<br>
&gt;&gt; Date: Mon, 5 May 2014 22:20:59 -0400<br>
&gt;&gt; From: Richard Shaw &lt;<a href="mailto:jrs65@cita.utoronto.ca">jrs65@cita.utoronto.ca</a>&gt;<br>
&gt;&gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt;&gt; Subject: [OMPI users] ROMIO bug reading darrays<br>
&gt;&gt; Message-ID:<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;<br>
&gt;&gt; <a href="mailto:CAN%2BevmkC%2B9KAcNPAUSScZiufwDJ3JfcSYMB-8ZdX1etDkabioQ@mail.gmail.com">CAN+evmkC+9KAcNPAUSScZiufwDJ3JfcSYMB-8ZdX1etDkabioQ@mail.gmail.com</a>&gt;<br>
&gt;&gt; Content-Type: text/plain; charset="utf-8"<br>
&gt;&gt;<br>
&gt;&gt; Hello,<br>
&gt;&gt;<br>
&gt;&gt; I think I've come across a bug when using ROMIO to read in a 2D<br>
&gt;&gt; distributed<br>
&gt;&gt; array. I've attached a test case to this email.<br>
&gt;&gt;<br>
&gt;&gt; In the testcase I first initialise an array of 25 doubles (which will be a<br>
&gt;&gt; 5x5 grid), then I create a datatype representing a 5x5 matrix distributed<br>
&gt;&gt; in 3x3 blocks over a 2x2 process grid. As a reference I use MPI_Pack to<br>
&gt;&gt; pull out the block cyclic array elements local to each process (which I<br>
&gt;&gt; think is correct). Then I write the original array of 25 doubles to disk,<br>
&gt;&gt; and use MPI-IO to read it back in (performing the Open, Set_view, and<br>
&gt;&gt; Real_all), and compare to the reference.<br>
&gt;&gt;<br>
&gt;&gt; Running this with OMPI, the two match on all ranks.<br>
&gt;&gt;<br>
&gt;&gt; &gt; mpirun -mca io ompio -np 4 ./darr_read.x<br>
&gt;&gt; === Rank 0 === (9 elements)<br>
&gt;&gt; Packed: &nbsp;0.0 &nbsp;1.0 &nbsp;2.0 &nbsp;5.0 &nbsp;6.0 &nbsp;7.0 10.0 11.0 12.0<br>
&gt;&gt; Read: &nbsp; &nbsp;0.0 &nbsp;1.0 &nbsp;2.0 &nbsp;5.0 &nbsp;6.0 &nbsp;7.0 10.0 11.0 12.0<br>
&gt;&gt;<br>
&gt;&gt; === Rank 1 === (6 elements)<br>
&gt;&gt; Packed: 15.0 16.0 17.0 20.0 21.0 22.0<br>
&gt;&gt; Read: &nbsp; 15.0 16.0 17.0 20.0 21.0 22.0<br>
&gt;&gt;<br>
&gt;&gt; === Rank 2 === (6 elements)<br>
&gt;&gt; Packed: &nbsp;3.0 &nbsp;4.0 &nbsp;8.0 &nbsp;9.0 13.0 14.0<br>
&gt;&gt; Read: &nbsp; &nbsp;3.0 &nbsp;4.0 &nbsp;8.0 &nbsp;9.0 13.0 14.0<br>
&gt;&gt;<br>
&gt;&gt; === Rank 3 === (4 elements)<br>
&gt;&gt; Packed: 18.0 19.0 23.0 24.0<br>
&gt;&gt; Read: &nbsp; 18.0 19.0 23.0 24.0<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; However, using ROMIO the two differ on two of the ranks:<br>
&gt;&gt;<br>
&gt;&gt; &gt; mpirun -mca io romio -np 4 ./darr_read.x<br>
&gt;&gt; === Rank 0 === (9 elements)<br>
&gt;&gt; Packed: &nbsp;0.0 &nbsp;1.0 &nbsp;2.0 &nbsp;5.0 &nbsp;6.0 &nbsp;7.0 10.0 11.0 12.0<br>
&gt;&gt; Read: &nbsp; &nbsp;0.0 &nbsp;1.0 &nbsp;2.0 &nbsp;5.0 &nbsp;6.0 &nbsp;7.0 10.0 11.0 12.0<br>
&gt;&gt;<br>
&gt;&gt; === Rank 1 === (6 elements)<br>
&gt;&gt; Packed: 15.0 16.0 17.0 20.0 21.0 22.0<br>
&gt;&gt; Read: &nbsp; &nbsp;0.0 &nbsp;1.0 &nbsp;2.0 &nbsp;0.0 &nbsp;1.0 &nbsp;2.0<br>
&gt;&gt;<br>
&gt;&gt; === Rank 2 === (6 elements)<br>
&gt;&gt; Packed: &nbsp;3.0 &nbsp;4.0 &nbsp;8.0 &nbsp;9.0 13.0 14.0<br>
&gt;&gt; Read: &nbsp; &nbsp;3.0 &nbsp;4.0 &nbsp;8.0 &nbsp;9.0 13.0 14.0<br>
&gt;&gt;<br>
&gt;&gt; === Rank 3 === (4 elements)<br>
&gt;&gt; Packed: 18.0 19.0 23.0 24.0<br>
&gt;&gt; Read: &nbsp; &nbsp;0.0 &nbsp;1.0 &nbsp;0.0 &nbsp;1.0<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; My interpretation is that the behaviour with OMPIO is correct.<br>
&gt;&gt; Interestingly everything matches up using both ROMIO and OMPIO if I set<br>
&gt;&gt; the<br>
&gt;&gt; block shape to 2x2.<br>
&gt;&gt;<br>
&gt;&gt; This was run on OS X using 1.8.2a1r31632. I have also run this on Linux<br>
&gt;&gt; with OpenMPI 1.7.4, and OMPIO is still correct, but using ROMIO I just get<br>
&gt;&gt; segfaults.<br>
&gt;&gt;<br>
&gt;&gt; Thanks,<br>
&gt;&gt; Richard<br>
&gt;&gt; -------------- next part --------------<br>
&gt;&gt; HTML attachment scrubbed and removed<br>
&gt;&gt; -------------- next part --------------<br>
&gt;&gt; A non-text attachment was scrubbed...<br>
&gt;&gt; Name: darr_read.c<br>
&gt;&gt; Type: text/x-csrc<br>
&gt;&gt; Size: 2218 bytes<br>
&gt;&gt; Desc: not available<br>
&gt;&gt; URL: &lt;<br>
&gt;&gt; <a href="http://www.open-mpi.org/MailArchives/users/attachments/20140505/5a5ab0ba/attachment.bin" target="_blank">http://www.open-mpi.org/MailArchives/users/attachments/20140505/5a5ab0ba/attachment.bin</a><br>
&gt;&gt; &gt;<br>
&gt;&gt;<br>
&gt;&gt; ------------------------------<br>
&gt;&gt;<br>
&gt;&gt; Message: 3<br>
&gt;&gt; Date: Tue, 06 May 2014 13:24:35 +0200<br>
&gt;&gt; From: Imran Ali &lt;<a href="mailto:imranal@student.matnat.uio.no">imranal@student.matnat.uio.no</a>&gt;<br>
&gt;&gt; To: &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt;&gt; Subject: [OMPI users] MPI File Open does not work<br>
&gt;&gt; Message-ID: &lt;<a href="mailto:d57bdf499c00360b737205b085c50660@ulrik.uio.no">d57bdf499c00360b737205b085c50660@ulrik.uio.no</a>&gt;<br>
&gt;&gt; Content-Type: text/plain; charset="utf-8"<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; I get the following error when I try to run the following python<br>
&gt;&gt; code<br>
&gt;&gt; import mpi4py.MPI as MPI<br>
&gt;&gt; comm = MPI.COMM_WORLD<br>
&gt;&gt;<br>
&gt;&gt; MPI.File.Open(comm,"some.file")<br>
&gt;&gt;<br>
&gt;&gt; $ mpirun -np 1 python<br>
&gt;&gt; test_mpi.py<br>
&gt;&gt; Traceback (most recent call last):<br>
&gt;&gt; &nbsp;File "test_mpi.py", line<br>
&gt;&gt; 3, in &lt;module&gt;<br>
&gt;&gt; &nbsp;MPI.File.Open(comm," h5ex_d_alloc.h5")<br>
&gt;&gt; &nbsp;File "File.pyx",<br>
&gt;&gt; line 67, in mpi4py.MPI.File.Open<br>
&gt;&gt; (src/mpi4py.MPI.c:89639)<br>
&gt;&gt; mpi4py.MPI.Exception: MPI_ERR_OTHER: known<br>
&gt;&gt; error not in<br>
&gt;&gt; list<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt; mpirun<br>
&gt;&gt; noticed that the job aborted, but has no info as to the process<br>
&gt;&gt; that<br>
&gt;&gt; caused that<br>
&gt;&gt; situation.<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; My mpirun version is (Open MPI) 1.6.2. I installed openmpi using the<br>
&gt;&gt; dorsal script (<a href="https://github.com/FEniCS/dorsal" target="_blank">https://github.com/FEniCS/dorsal</a>) for Redhat Enterprise 6<br>
&gt;&gt; (OS I am using, release 6.5) . It configured the build as following :<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; ./configure --enable-mpi-thread-multiple --enable-opal-multi-threads<br>
&gt;&gt; --with-threads=posix --disable-mpi-profile<br>
&gt;&gt;<br>
&gt;&gt; I need emphasize that I do<br>
&gt;&gt; not have root acces on the system I am running my application.<br>
&gt;&gt;<br>
&gt;&gt; Imran<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; -------------- next part --------------<br>
&gt;&gt; HTML attachment scrubbed and removed<br>
&gt;&gt;<br>
&gt;&gt; ------------------------------<br>
&gt;&gt;<br>
&gt;&gt; Message: 4<br>
&gt;&gt; Date: Tue, 6 May 2014 12:56:04 +0000<br>
&gt;&gt; From: "Jeff Squyres (jsquyres)" &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>
&gt;&gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt;&gt; Subject: Re: [OMPI users] MPI File Open does not work<br>
&gt;&gt; Message-ID: &lt;<a href="mailto:E7DF28CB-D4FB-4087-928E-18E61D1D24CF@cisco.com">E7DF28CB-D4FB-4087-928E-18E61D1D24CF@cisco.com</a>&gt;<br>
&gt;&gt; Content-Type: text/plain; charset="us-ascii"<br>
&gt;&gt;<br>
&gt;&gt; The thread support in the 1.6 series is not very good. &nbsp;You might try:<br>
&gt;&gt;<br>
&gt;&gt; - Upgrading to 1.6.5<br>
&gt;&gt; - Or better yet, upgrading to 1.8.1<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; On May 6, 2014, at 7:24 AM, Imran Ali &lt;<a href="mailto:imranal@student.matnat.uio.no">imranal@student.matnat.uio.no</a>&gt;<br>
&gt;&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; &gt; I get the following error when I try to run the following python code<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; import mpi4py.MPI as MPI<br>
&gt;&gt; &gt; comm = &nbsp;MPI.COMM_WORLD<br>
&gt;&gt; &gt; MPI.File.Open(comm,"some.file")<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; $ mpirun -np 1 python test_mpi.py<br>
&gt;&gt; &gt; Traceback (most recent call last):<br>
&gt;&gt; &gt; &nbsp; File "test_mpi.py", line 3, in &lt;module&gt;<br>
&gt;&gt; &gt; &nbsp; &nbsp; MPI.File.Open(comm," h5ex_d_alloc.h5")<br>
&gt;&gt; &gt; &nbsp; File "File.pyx", line 67, in mpi4py.MPI.File.Open<br>
&gt;&gt; (src/mpi4py.MPI.c:89639)<br>
&gt;&gt; &gt; mpi4py.MPI.Exception: MPI_ERR_OTHER: known error not in list<br>
&gt;&gt; &gt;<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt; &gt; mpirun noticed that the job aborted, but has no info as to the process<br>
&gt;&gt; &gt; that caused that situation.<br>
&gt;&gt; &gt;<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; My mpirun version is (Open MPI) 1.6.2. I installed openmpi using the<br>
&gt;&gt; dorsal script (<a href="https://github.com/FEniCS/dorsal" target="_blank">https://github.com/FEniCS/dorsal</a>) for Redhat Enterprise 6<br>
&gt;&gt; (OS I am using, release 6.5) . It configured the build as following :<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; ./configure --enable-mpi-thread-multiple --enable-opal-multi-threads<br>
&gt;&gt; --with-threads=posix --disable-mpi-profile<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; I need emphasize that I do not have root acces on the system I am<br>
&gt;&gt; running my application.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Imran<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; _______________________________________________<br>
&gt;&gt; &gt; users mailing list<br>
&gt;&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; Jeff Squyres<br>
&gt;&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt;&gt; For corporate legal information go to:<br>
&gt;&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; ------------------------------<br>
&gt;&gt;<br>
&gt;&gt; Message: 5<br>
&gt;&gt; Date: Tue, 6 May 2014 15:32:21 +0200<br>
&gt;&gt; From: Imran Ali &lt;<a href="mailto:imranal@student.matnat.uio.no">imranal@student.matnat.uio.no</a>&gt;<br>
&gt;&gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt;&gt; Subject: Re: [OMPI users] MPI File Open does not work<br>
&gt;&gt; Message-ID: &lt;<a href="mailto:FA6DFFFF-6C66-4A47-84FC-148FB51CE162@math.uio.no">FA6DFFFF-6C66-4A47-84FC-148FB51CE162@math.uio.no</a>&gt;<br>
&gt;&gt; Content-Type: text/plain; charset=us-ascii<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; 6. mai 2014 kl. 14:56 skrev Jeff Squyres (jsquyres) &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;:<br>
&gt;&gt;<br>
&gt;&gt; &gt; The thread support in the 1.6 series is not very good. &nbsp;You might try:<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; - Upgrading to 1.6.5<br>
&gt;&gt; &gt; - Or better yet, upgrading to 1.8.1<br>
&gt;&gt; &gt;<br>
&gt;&gt;<br>
&gt;&gt; I will attempt that than. I read at<br>
&gt;&gt;<br>
&gt;&gt; <a href="http://www.open-mpi.org/faq/?category=building#install-overwrite" target="_blank">http://www.open-mpi.org/faq/?category=building#install-overwrite</a><br>
&gt;&gt;<br>
&gt;&gt; that I should completely uninstall my previous version. Could you<br>
&gt;&gt; recommend to me how I can go about doing it (without root access).<br>
&gt;&gt; I am uncertain where I can use make uninstall.<br>
&gt;&gt;<br>
&gt;&gt; Imran<br>
&gt;&gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; On May 6, 2014, at 7:24 AM, Imran Ali &lt;<a href="mailto:imranal@student.matnat.uio.no">imranal@student.matnat.uio.no</a>&gt;<br>
&gt;&gt; wrote:<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;&gt; I get the following error when I try to run the following python code<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; import mpi4py.MPI as MPI<br>
&gt;&gt; &gt;&gt; comm = &nbsp;MPI.COMM_WORLD<br>
&gt;&gt; &gt;&gt; MPI.File.Open(comm,"some.file")<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; $ mpirun -np 1 python test_mpi.py<br>
&gt;&gt; &gt;&gt; Traceback (most recent call last):<br>
&gt;&gt; &gt;&gt; &nbsp;File "test_mpi.py", line 3, in &lt;module&gt;<br>
&gt;&gt; &gt;&gt; &nbsp; &nbsp;MPI.File.Open(comm," h5ex_d_alloc.h5")<br>
&gt;&gt; &gt;&gt; &nbsp;File "File.pyx", line 67, in mpi4py.MPI.File.Open<br>
&gt;&gt; (src/mpi4py.MPI.c:89639)<br>
&gt;&gt; &gt;&gt; mpi4py.MPI.Exception: MPI_ERR_OTHER: known error not in list<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt; &gt;&gt; mpirun noticed that the job aborted, but has no info as to the process<br>
&gt;&gt; &gt;&gt; that caused that situation.<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; My mpirun version is (Open MPI) 1.6.2. I installed openmpi using the<br>
&gt;&gt; dorsal script (<a href="https://github.com/FEniCS/dorsal" target="_blank">https://github.com/FEniCS/dorsal</a>) for Redhat Enterprise 6<br>
&gt;&gt; (OS I am using, release 6.5) . It configured the build as following :<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; ./configure --enable-mpi-thread-multiple --enable-opal-multi-threads<br>
&gt;&gt; --with-threads=posix --disable-mpi-profile<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; I need emphasize that I do not have root acces on the system I am<br>
&gt;&gt; running my application.<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; Imran<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; _______________________________________________<br>
&gt;&gt; &gt;&gt; users mailing list<br>
&gt;&gt; &gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; &gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; --<br>
&gt;&gt; &gt; Jeff Squyres<br>
&gt;&gt; &gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt;&gt; &gt; For corporate legal information go to:<br>
&gt;&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; _______________________________________________<br>
&gt;&gt; &gt; users mailing list<br>
&gt;&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; ------------------------------<br>
&gt;&gt;<br>
&gt;&gt; Message: 6<br>
&gt;&gt; Date: Tue, 6 May 2014 13:34:38 +0000<br>
&gt;&gt; From: "Jeff Squyres (jsquyres)" &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>
&gt;&gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt;&gt; Subject: Re: [OMPI users] MPI File Open does not work<br>
&gt;&gt; Message-ID: &lt;<a href="mailto:2A933C0E-80F6-4DED-B44C-53B5F37EFC0C@cisco.com">2A933C0E-80F6-4DED-B44C-53B5F37EFC0C@cisco.com</a>&gt;<br>
&gt;&gt; Content-Type: text/plain; charset="us-ascii"<br>
&gt;&gt;<br>
&gt;&gt; On May 6, 2014, at 9:32 AM, Imran Ali &lt;<a href="mailto:imranal@student.matnat.uio.no">imranal@student.matnat.uio.no</a>&gt;<br>
&gt;&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; &gt; I will attempt that than. I read at<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; <a href="http://www.open-mpi.org/faq/?category=building#install-overwrite" target="_blank">http://www.open-mpi.org/faq/?category=building#install-overwrite</a><br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; that I should completely uninstall my previous version.<br>
&gt;&gt;<br>
&gt;&gt; Yes, that is best. &nbsp;OR: you can install into a whole separate tree and<br>
&gt;&gt; ignore the first installation.<br>
&gt;&gt;<br>
&gt;&gt; &gt; Could you recommend to me how I can go about doing it (without root<br>
&gt;&gt; access).<br>
&gt;&gt; &gt; I am uncertain where I can use make uninstall.<br>
&gt;&gt;<br>
&gt;&gt; If you don't have write access into the installation tree (i.e., it was<br>
&gt;&gt; installed via root and you don't have root access), then your best bet is<br>
&gt;&gt; simply to install into a new tree. &nbsp;E.g., if OMPI is installed into<br>
&gt;&gt; /opt/openmpi-1.6.2, try installing into /opt/openmpi-1.6.5, or even<br>
&gt;&gt; $HOME/installs/openmpi-1.6.5, or something like that.<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; Jeff Squyres<br>
&gt;&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt;&gt; For corporate legal information go to:<br>
&gt;&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; ------------------------------<br>
&gt;&gt;<br>
&gt;&gt; Message: 7<br>
&gt;&gt; Date: Tue, 6 May 2014 15:40:34 +0200<br>
&gt;&gt; From: Imran Ali &lt;<a href="mailto:imranal@student.matnat.uio.no">imranal@student.matnat.uio.no</a>&gt;<br>
&gt;&gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt;&gt; Subject: Re: [OMPI users] MPI File Open does not work<br>
&gt;&gt; Message-ID: &lt;<a href="mailto:14F0596C-C5C5-4B1A-A4A8-8849D44AB76A@math.uio.no">14F0596C-C5C5-4B1A-A4A8-8849D44AB76A@math.uio.no</a>&gt;<br>
&gt;&gt; Content-Type: text/plain; charset=us-ascii<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; 6. mai 2014 kl. 15:34 skrev Jeff Squyres (jsquyres) &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;:<br>
&gt;&gt;<br>
&gt;&gt; &gt; On May 6, 2014, at 9:32 AM, Imran Ali &lt;<a href="mailto:imranal@student.matnat.uio.no">imranal@student.matnat.uio.no</a>&gt;<br>
&gt;&gt; wrote:<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;&gt; I will attempt that than. I read at<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; <a href="http://www.open-mpi.org/faq/?category=building#install-overwrite" target="_blank">http://www.open-mpi.org/faq/?category=building#install-overwrite</a><br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; that I should completely uninstall my previous version.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Yes, that is best. &nbsp;OR: you can install into a whole separate tree and<br>
&gt;&gt; ignore the first installation.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;&gt; Could you recommend to me how I can go about doing it (without root<br>
&gt;&gt; access).<br>
&gt;&gt; &gt;&gt; I am uncertain where I can use make uninstall.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; If you don't have write access into the installation tree (i.e., it was<br>
&gt;&gt; installed via root and you don't have root access), then your best bet is<br>
&gt;&gt; simply to install into a new tree. &nbsp;E.g., if OMPI is installed into<br>
&gt;&gt; /opt/openmpi-1.6.2, try installing into /opt/openmpi-1.6.5, or even<br>
&gt;&gt; $HOME/installs/openmpi-1.6.5, or something like that.<br>
&gt;&gt;<br>
&gt;&gt; My install was in my user directory (i.e $HOME). I managed to locate the<br>
&gt;&gt; source directory and successfully run make uninstall.<br>
&gt;&gt;<br>
&gt;&gt; Will let you know how things went after installation.<br>
&gt;&gt;<br>
&gt;&gt; Imran<br>
&gt;&gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; --<br>
&gt;&gt; &gt; Jeff Squyres<br>
&gt;&gt; &gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt;&gt; &gt; For corporate legal information go to:<br>
&gt;&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; _______________________________________________<br>
&gt;&gt; &gt; users mailing list<br>
&gt;&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; ------------------------------<br>
&gt;&gt;<br>
&gt;&gt; Message: 8<br>
&gt;&gt; Date: Tue, 6 May 2014 14:42:52 +0000<br>
&gt;&gt; From: "Jeff Squyres (jsquyres)" &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>
&gt;&gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt;&gt; Subject: Re: [OMPI users] MPI File Open does not work<br>
&gt;&gt; Message-ID: &lt;<a href="mailto:710E3328-EDAA-4A13-9F07-B45FE319113D@cisco.com">710E3328-EDAA-4A13-9F07-B45FE319113D@cisco.com</a>&gt;<br>
&gt;&gt; Content-Type: text/plain; charset="us-ascii"<br>
&gt;&gt;<br>
&gt;&gt; On May 6, 2014, at 9:40 AM, Imran Ali &lt;<a href="mailto:imranal@student.matnat.uio.no">imranal@student.matnat.uio.no</a>&gt;<br>
&gt;&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; &gt; My install was in my user directory (i.e $HOME). I managed to locate<br>
&gt;&gt; the source directory and successfully run make uninstall.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; FWIW, I usually install Open MPI into its own subdir. &nbsp;E.g.,<br>
&gt;&gt; $HOME/installs/openmpi-x.y.z. &nbsp;Then if I don't want that install any more,<br>
&gt;&gt; I can just "rm -rf $HOME/installs/openmpi-x.y.z" -- no need to "make<br>
&gt;&gt; uninstall". &nbsp;Specifically: if there's nothing else installed in the same<br>
&gt;&gt; tree as Open MPI, you can just rm -rf its installation tree.<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; Jeff Squyres<br>
&gt;&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt;&gt; For corporate legal information go to:<br>
&gt;&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; ------------------------------<br>
&gt;&gt;<br>
&gt;&gt; Message: 9<br>
&gt;&gt; Date: Tue, 6 May 2014 14:50:34 +0000<br>
&gt;&gt; From: "Jeff Squyres (jsquyres)" &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>
&gt;&gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt;&gt; Subject: Re: [OMPI users] users Digest, Vol 2879, Issue 1<br>
&gt;&gt; Message-ID: &lt;<a href="mailto:C60AA7E1-96A7-4CCD-9B5B-11A38FB87772@cisco.com">C60AA7E1-96A7-4CCD-9B5B-11A38FB87772@cisco.com</a>&gt;<br>
&gt;&gt; Content-Type: text/plain; charset="us-ascii"<br>
&gt;&gt;<br>
&gt;&gt; Are you using TCP as the MPI transport?<br>
&gt;&gt;<br>
&gt;&gt; If so, another thing to try is to limit the IP interfaces that MPI uses<br>
&gt;&gt; for its traffic to see if there's some kind of problem with specific<br>
&gt;&gt; networks.<br>
&gt;&gt;<br>
&gt;&gt; For example:<br>
&gt;&gt;<br>
&gt;&gt; &nbsp; &nbsp;mpirun --mca btl_tcp_if_include eth0 ...<br>
&gt;&gt;<br>
&gt;&gt; If that works, then try adding in any/all other IP interfaces that you<br>
&gt;&gt; have on your machines.<br>
&gt;&gt;<br>
&gt;&gt; A sorta-wild guess: you have some IP interfaces that aren't working, or<br>
&gt;&gt; at least, don't work in the way that OMPI wants them to work. &nbsp;So the first<br>
&gt;&gt; barrier works because it flows across eth0 (or some other first network<br>
&gt;&gt; that, as far as OMPI is concerned, works just fine). &nbsp;But then the next<br>
&gt;&gt; barrier round-robin advances to the next IP interface, and it doesn't work<br>
&gt;&gt; for some reason.<br>
&gt;&gt;<br>
&gt;&gt; We've seen virtual machine bridge interfaces cause problems, for example.<br>
&gt;&gt; &nbsp;E.g., if a machine has a Xen virtual machine interface (vibr0, IIRC?),<br>
&gt;&gt; then OMPI will try to use it to communicate with peer MPI processes because<br>
&gt;&gt; it has a "compatible" IP address, and OMPI think it should be<br>
&gt;&gt; connected/reachable to peers. &nbsp;If this is the case, you might want to<br>
&gt;&gt; disable such interfaces and/or use btl_tcp_if_include or btl_tcp_if_exclude<br>
&gt;&gt; to select the interfaces that you want to use.<br>
&gt;&gt;<br>
&gt;&gt; Pro tip: if you use btl_tcp_if_exclude, remember to exclude the loopback<br>
&gt;&gt; interface, too. &nbsp;OMPI defaults to a btl_tcp_if_include="" (blank) and<br>
&gt;&gt; btl_tcp_if_exclude="lo0". So if you override btl_tcp_if_exclude, you need<br>
&gt;&gt; to be sure to *also* include lo0 in the new value. &nbsp;For example:<br>
&gt;&gt;<br>
&gt;&gt; &nbsp; &nbsp;mpirun --mca btl_tcp_if_exclude lo0,virb0 ...<br>
&gt;&gt;<br>
&gt;&gt; Also, if possible, try upgrading to Open MPI 1.8.1.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; On May 4, 2014, at 2:15 PM, Clay Kirkland &lt;<a href="mailto:clay.kirkland@versityinc.com">clay.kirkland@versityinc.com</a>&gt;<br>
&gt;&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; &gt; &nbsp;I am configuring with all defaults. &nbsp; Just doing a ./configure and then<br>
&gt;&gt; &gt; make and make install. &nbsp; I have used open mpi on several kinds of<br>
&gt;&gt; &gt; unix &nbsp;systems this way and have had no trouble before. &nbsp; I believe I<br>
&gt;&gt; &gt; last had success on a redhat version of linux.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; On Sat, May 3, 2014 at 11:00 AM, &lt;<a href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a>&gt; wrote:<br>
&gt;&gt; &gt; Send users mailing list submissions to<br>
&gt;&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; To subscribe or unsubscribe via the World Wide Web, visit<br>
&gt;&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; &gt; or, via email, send a message with subject or body 'help' to<br>
&gt;&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a><br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; You can reach the person managing the list at<br>
&gt;&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:users-owner@open-mpi.org">users-owner@open-mpi.org</a><br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; When replying, please edit your Subject line so it is more specific<br>
&gt;&gt; &gt; than "Re: Contents of users digest..."<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Today's Topics:<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; &nbsp; &nbsp;1. MPI_Barrier hangs on second attempt but only when multiple<br>
&gt;&gt; &gt; &nbsp; &nbsp; &nbsp; hosts used. (Clay Kirkland)<br>
&gt;&gt; &gt; &nbsp; &nbsp;2. Re: MPI_Barrier hangs on second attempt but only when<br>
&gt;&gt; &gt; &nbsp; &nbsp; &nbsp; multiple hosts used. (Ralph Castain)<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; ----------------------------------------------------------------------<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Message: 1<br>
&gt;&gt; &gt; Date: Fri, 2 May 2014 16:24:04 -0500<br>
&gt;&gt; &gt; From: Clay Kirkland &lt;<a href="mailto:clay.kirkland@versityinc.com">clay.kirkland@versityinc.com</a>&gt;<br>
&gt;&gt; &gt; To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; &gt; Subject: [OMPI users] MPI_Barrier hangs on second attempt but only<br>
&gt;&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; when &nbsp; &nbsp;multiple hosts used.<br>
&gt;&gt; &gt; Message-ID:<br>
&gt;&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; &lt;CAJDnjA8Wi=FEjz6Vz+Bc34b+nFE=<br>
&gt;&gt; <a href="mailto:TF4B7g0BQgMbeKg7H-pV%2BA@mail.gmail.com">TF4B7g0BQgMbeKg7H-pV+A@mail.gmail.com</a>&gt;<br>
&gt;&gt; &gt; Content-Type: text/plain; charset="utf-8"<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; I have been using MPI for many many years so I have very well debugged<br>
&gt;&gt; mpi<br>
&gt;&gt; &gt; tests. &nbsp; I am<br>
&gt;&gt; &gt; having trouble on either openmpi-1.4.5 &nbsp;or &nbsp;openmpi-1.6.5 versions<br>
&gt;&gt; though<br>
&gt;&gt; &gt; with getting the<br>
&gt;&gt; &gt; MPI_Barrier calls to work. &nbsp; It works fine when I run all processes on<br>
&gt;&gt; one<br>
&gt;&gt; &gt; machine but when<br>
&gt;&gt; &gt; I run with two or more hosts the second call to MPI_Barrier always<br>
&gt;&gt; hangs.<br>
&gt;&gt; &gt; Not the first one,<br>
&gt;&gt; &gt; but always the second one. &nbsp; I looked at FAQ's and such but found<br>
&gt;&gt; nothing<br>
&gt;&gt; &gt; except for a comment<br>
&gt;&gt; &gt; that MPI_Barrier problems were often problems with fire walls. &nbsp;Also<br>
&gt;&gt; &gt; mentioned as a problem<br>
&gt;&gt; &gt; was not having the same version of mpi on both machines. &nbsp;I turned<br>
&gt;&gt; &gt; firewalls off and removed<br>
&gt;&gt; &gt; and reinstalled the same version on both hosts but I still see the same<br>
&gt;&gt; &gt; thing. &nbsp; I then installed<br>
&gt;&gt; &gt; lam mpi on two of my machines and that works fine. &nbsp; I can call the<br>
&gt;&gt; &gt; MPI_Barrier function when run on<br>
&gt;&gt; &gt; one of two machines by itself &nbsp;many times with no hangs. &nbsp;Only hangs if<br>
&gt;&gt; two<br>
&gt;&gt; &gt; or more hosts are involved.<br>
&gt;&gt; &gt; These runs are all being done on CentOS release 6.4. &nbsp; Here is test<br>
&gt;&gt; program<br>
&gt;&gt; &gt; I used.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; main (argc, argv)<br>
&gt;&gt; &gt; int argc;<br>
&gt;&gt; &gt; char **argv;<br>
&gt;&gt; &gt; {<br>
&gt;&gt; &gt; &nbsp; &nbsp; char message[20];<br>
&gt;&gt; &gt; &nbsp; &nbsp; char hoster[256];<br>
&gt;&gt; &gt; &nbsp; &nbsp; char nameis[256];<br>
&gt;&gt; &gt; &nbsp; &nbsp; int fd, i, j, jnp, iret, myrank, np, ranker, recker;<br>
&gt;&gt; &gt; &nbsp; &nbsp; MPI_Comm comm;<br>
&gt;&gt; &gt; &nbsp; &nbsp; MPI_Status status;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; &nbsp; &nbsp; MPI_Init( &amp;argc, &amp;argv );<br>
&gt;&gt; &gt; &nbsp; &nbsp; MPI_Comm_rank( MPI_COMM_WORLD, &amp;myrank);<br>
&gt;&gt; &gt; &nbsp; &nbsp; MPI_Comm_size( MPI_COMM_WORLD, &amp;np);<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; gethostname(hoster,256);<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt;&gt; &gt; 1.\n",myrank,hoster);<br>
&gt;&gt; &gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt;&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt;&gt; &gt; 2.\n",myrank,hoster);<br>
&gt;&gt; &gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt;&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt;&gt; &gt; 3.\n",myrank,hoster);<br>
&gt;&gt; &gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt;&gt; &gt; &nbsp; &nbsp; MPI_Finalize();<br>
&gt;&gt; &gt; &nbsp; &nbsp; exit(0);<br>
&gt;&gt; &gt; }<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; &nbsp; Here are three runs of test program. &nbsp;First with two processes on one<br>
&gt;&gt; &gt; host, then with<br>
&gt;&gt; &gt; two processes on another host, and finally with one process on each of<br>
&gt;&gt; two<br>
&gt;&gt; &gt; hosts. &nbsp;The<br>
&gt;&gt; &gt; first two runs are fine but the last run hangs on the second<br>
&gt;&gt; MPI_Barrier.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host centos a.out<br>
&gt;&gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 1.<br>
&gt;&gt; &gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 1.<br>
&gt;&gt; &gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 2.<br>
&gt;&gt; &gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 3.<br>
&gt;&gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 2.<br>
&gt;&gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 3.<br>
&gt;&gt; &gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host RAID a.out<br>
&gt;&gt; &gt; /root/.bashrc: line 14: unalias: ls: not found<br>
&gt;&gt; &gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt;&gt; &gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt;&gt; &gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 3.<br>
&gt;&gt; &gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt;&gt; &gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt;&gt; &gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 3.<br>
&gt;&gt; &gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host centos,RAID a.out<br>
&gt;&gt; &gt; /root/.bashrc: line 14: unalias: ls: not found<br>
&gt;&gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 1.<br>
&gt;&gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 2.<br>
&gt;&gt; &gt; In rank 1 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt;&gt; &gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; &nbsp; Since it is such a simple test and problem and such a widely used MPI<br>
&gt;&gt; &gt; function, it must obviously<br>
&gt;&gt; &gt; be an installation or configuration problem. &nbsp; A pstack for each of the<br>
&gt;&gt; &gt; hung MPI_Barrier processes<br>
&gt;&gt; &gt; on the two machines shows this:<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; [root@centos ~]# pstack 31666<br>
&gt;&gt; &gt; #0 &nbsp;0x0000003baf0e8ee3 in __epoll_wait_nocancel () from /lib64/libc.so.6<br>
&gt;&gt; &gt; #1 &nbsp;0x00007f5de06125eb in epoll_dispatch () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; #2 &nbsp;0x00007f5de061475a in opal_event_base_loop () from<br>
&gt;&gt; &gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; #3 &nbsp;0x00007f5de0639229 in opal_progress () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; #4 &nbsp;0x00007f5de0586f75 in ompi_request_default_wait_all () from<br>
&gt;&gt; &gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; #5 &nbsp;0x00007f5ddc59565e in ompi_coll_tuned_sendrecv_actual () from<br>
&gt;&gt; &gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt;&gt; &gt; #6 &nbsp;0x00007f5ddc59d8ff in ompi_coll_tuned_barrier_intra_two_procs ()<br>
&gt;&gt; from<br>
&gt;&gt; &gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt;&gt; &gt; #7 &nbsp;0x00007f5de05941c2 in PMPI_Barrier () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; #8 &nbsp;0x0000000000400a43 in main ()<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; [root@RAID openmpi-1.6.5]# pstack 22167<br>
&gt;&gt; &gt; #0 &nbsp;0x00000030302e8ee3 in __epoll_wait_nocancel () from /lib64/libc.so.6<br>
&gt;&gt; &gt; #1 &nbsp;0x00007f7ee46885eb in epoll_dispatch () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; #2 &nbsp;0x00007f7ee468a75a in opal_event_base_loop () from<br>
&gt;&gt; &gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; #3 &nbsp;0x00007f7ee46af229 in opal_progress () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; #4 &nbsp;0x00007f7ee45fcf75 in ompi_request_default_wait_all () from<br>
&gt;&gt; &gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; #5 &nbsp;0x00007f7ee060b65e in ompi_coll_tuned_sendrecv_actual () from<br>
&gt;&gt; &gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt;&gt; &gt; #6 &nbsp;0x00007f7ee06138ff in ompi_coll_tuned_barrier_intra_two_procs ()<br>
&gt;&gt; from<br>
&gt;&gt; &gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt;&gt; &gt; #7 &nbsp;0x00007f7ee460a1c2 in PMPI_Barrier () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; #8 &nbsp;0x0000000000400a43 in main ()<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; &nbsp;Which looks exactly the same on each machine. &nbsp;Any thoughts or ideas<br>
&gt;&gt; would<br>
&gt;&gt; &gt; be greatly appreciated as<br>
&gt;&gt; &gt; I am stuck.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; &nbsp;Clay Kirkland<br>
&gt;&gt; &gt; -------------- next part --------------<br>
&gt;&gt; &gt; HTML attachment scrubbed and removed<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; ------------------------------<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Message: 2<br>
&gt;&gt; &gt; Date: Sat, 3 May 2014 06:39:20 -0700<br>
&gt;&gt; &gt; From: Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;<br>
&gt;&gt; &gt; To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt;&gt; &gt; Subject: Re: [OMPI users] MPI_Barrier hangs on second attempt but only<br>
&gt;&gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; when &nbsp; &nbsp;multiple hosts used.<br>
&gt;&gt; &gt; Message-ID: &lt;<a href="mailto:3CF53D73-15D9-40BB-A2DE-50BA3561A0F4@open-mpi.org">3CF53D73-15D9-40BB-A2DE-50BA3561A0F4@open-mpi.org</a>&gt;<br>
&gt;&gt; &gt; Content-Type: text/plain; charset="us-ascii"<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Hmmm...just testing on my little cluster here on two nodes, it works<br>
&gt;&gt; just fine with 1.8.2:<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; [rhc@bend001 v1.8]$ mpirun -n 2 --map-by node ./a.out<br>
&gt;&gt; &gt; &nbsp;In rank 0 and host= bend001 &nbsp;Do Barrier call 1.<br>
&gt;&gt; &gt; &nbsp;In rank 0 and host= bend001 &nbsp;Do Barrier call 2.<br>
&gt;&gt; &gt; &nbsp;In rank 0 and host= bend001 &nbsp;Do Barrier call 3.<br>
&gt;&gt; &gt; &nbsp;In rank 1 and host= bend002 &nbsp;Do Barrier call 1.<br>
&gt;&gt; &gt; &nbsp;In rank 1 and host= bend002 &nbsp;Do Barrier call 2.<br>
&gt;&gt; &gt; &nbsp;In rank 1 and host= bend002 &nbsp;Do Barrier call 3.<br>
&gt;&gt; &gt; [rhc@bend001 v1.8]$<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; How are you configuring OMPI?<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; On May 2, 2014, at 2:24 PM, Clay Kirkland &lt;<a href="mailto:clay.kirkland@versityinc.com">clay.kirkland@versityinc.com</a>&gt;<br>
&gt;&gt; wrote:<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; &gt; I have been using MPI for many many years so I have very well<br>
&gt;&gt; debugged mpi tests. &nbsp; I am<br>
&gt;&gt; &gt; &gt; having trouble on either openmpi-1.4.5 &nbsp;or &nbsp;openmpi-1.6.5 versions<br>
&gt;&gt; though with getting the<br>
&gt;&gt; &gt; &gt; MPI_Barrier calls to work. &nbsp; It works fine when I run all processes<br>
&gt;&gt; on one machine but when<br>
&gt;&gt; &gt; &gt; I run with two or more hosts the second call to MPI_Barrier always<br>
&gt;&gt; hangs. &nbsp; Not the first one,<br>
&gt;&gt; &gt; &gt; but always the second one. &nbsp; I looked at FAQ's and such but found<br>
&gt;&gt; nothing except for a comment<br>
&gt;&gt; &gt; &gt; that MPI_Barrier problems were often problems with fire walls. &nbsp;Also<br>
&gt;&gt; mentioned as a problem<br>
&gt;&gt; &gt; &gt; was not having the same version of mpi on both machines. &nbsp;I turned<br>
&gt;&gt; firewalls off and removed<br>
&gt;&gt; &gt; &gt; and reinstalled the same version on both hosts but I still see the<br>
&gt;&gt; same thing. &nbsp; I then installed<br>
&gt;&gt; &gt; &gt; lam mpi on two of my machines and that works fine. &nbsp; I can call the<br>
&gt;&gt; MPI_Barrier function when run on<br>
&gt;&gt; &gt; &gt; one of two machines by itself &nbsp;many times with no hangs. &nbsp;Only hangs<br>
&gt;&gt; if two or more hosts are involved.<br>
&gt;&gt; &gt; &gt; These runs are all being done on CentOS release 6.4. &nbsp; Here is test<br>
&gt;&gt; program I used.<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; main (argc, argv)<br>
&gt;&gt; &gt; &gt; int argc;<br>
&gt;&gt; &gt; &gt; char **argv;<br>
&gt;&gt; &gt; &gt; {<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; char message[20];<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; char hoster[256];<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; char nameis[256];<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; int fd, i, j, jnp, iret, myrank, np, ranker, recker;<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; MPI_Comm comm;<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; MPI_Status status;<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; MPI_Init( &amp;argc, &amp;argv );<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; MPI_Comm_rank( MPI_COMM_WORLD, &amp;myrank);<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; MPI_Comm_size( MPI_COMM_WORLD, &amp;np);<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; gethostname(hoster,256);<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt;&gt; 1.\n",myrank,hoster);<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt;&gt; 2.\n",myrank,hoster);<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; &nbsp; &nbsp; printf(" In rank %d and host= %s &nbsp;Do Barrier call<br>
&gt;&gt; 3.\n",myrank,hoster);<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; MPI_Barrier(MPI_COMM_WORLD);<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; MPI_Finalize();<br>
&gt;&gt; &gt; &gt; &nbsp; &nbsp; exit(0);<br>
&gt;&gt; &gt; &gt; }<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; &nbsp; Here are three runs of test program. &nbsp;First with two processes on<br>
&gt;&gt; one host, then with<br>
&gt;&gt; &gt; &gt; two processes on another host, and finally with one process on each<br>
&gt;&gt; of two hosts. &nbsp;The<br>
&gt;&gt; &gt; &gt; first two runs are fine but the last run hangs on the second<br>
&gt;&gt; MPI_Barrier.<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host centos a.out<br>
&gt;&gt; &gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 1.<br>
&gt;&gt; &gt; &gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 1.<br>
&gt;&gt; &gt; &gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 2.<br>
&gt;&gt; &gt; &gt; &nbsp;In rank 1 and host= centos &nbsp;Do Barrier call 3.<br>
&gt;&gt; &gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 2.<br>
&gt;&gt; &gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 3.<br>
&gt;&gt; &gt; &gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host RAID a.out<br>
&gt;&gt; &gt; &gt; /root/.bashrc: line 14: unalias: ls: not found<br>
&gt;&gt; &gt; &gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt;&gt; &gt; &gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt;&gt; &gt; &gt; &nbsp;In rank 0 and host= RAID &nbsp;Do Barrier call 3.<br>
&gt;&gt; &gt; &gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt;&gt; &gt; &gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt;&gt; &gt; &gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 3.<br>
&gt;&gt; &gt; &gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host centos,RAID<br>
&gt;&gt; a.out<br>
&gt;&gt; &gt; &gt; /root/.bashrc: line 14: unalias: ls: not found<br>
&gt;&gt; &gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 1.<br>
&gt;&gt; &gt; &gt; &nbsp;In rank 0 and host= centos &nbsp;Do Barrier call 2.<br>
&gt;&gt; &gt; &gt; In rank 1 and host= RAID &nbsp;Do Barrier call 1.<br>
&gt;&gt; &gt; &gt; &nbsp;In rank 1 and host= RAID &nbsp;Do Barrier call 2.<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; &nbsp; Since it is such a simple test and problem and such a widely used<br>
&gt;&gt; MPI function, it must obviously<br>
&gt;&gt; &gt; &gt; be an installation or configuration problem. &nbsp; A pstack for each of<br>
&gt;&gt; the hung MPI_Barrier processes<br>
&gt;&gt; &gt; &gt; on the two machines shows this:<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; [root@centos ~]# pstack 31666<br>
&gt;&gt; &gt; &gt; #0 &nbsp;0x0000003baf0e8ee3 in __epoll_wait_nocancel () from<br>
&gt;&gt; /lib64/libc.so.6<br>
&gt;&gt; &gt; &gt; #1 &nbsp;0x00007f5de06125eb in epoll_dispatch () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; &gt; #2 &nbsp;0x00007f5de061475a in opal_event_base_loop () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; &gt; #3 &nbsp;0x00007f5de0639229 in opal_progress () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; &gt; #4 &nbsp;0x00007f5de0586f75 in ompi_request_default_wait_all () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; &gt; #5 &nbsp;0x00007f5ddc59565e in ompi_coll_tuned_sendrecv_actual () from<br>
&gt;&gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt;&gt; &gt; &gt; #6 &nbsp;0x00007f5ddc59d8ff in ompi_coll_tuned_barrier_intra_two_procs ()<br>
&gt;&gt; from /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt;&gt; &gt; &gt; #7 &nbsp;0x00007f5de05941c2 in PMPI_Barrier () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; &gt; #8 &nbsp;0x0000000000400a43 in main ()<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; [root@RAID openmpi-1.6.5]# pstack 22167<br>
&gt;&gt; &gt; &gt; #0 &nbsp;0x00000030302e8ee3 in __epoll_wait_nocancel () from<br>
&gt;&gt; /lib64/libc.so.6<br>
&gt;&gt; &gt; &gt; #1 &nbsp;0x00007f7ee46885eb in epoll_dispatch () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; &gt; #2 &nbsp;0x00007f7ee468a75a in opal_event_base_loop () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; &gt; #3 &nbsp;0x00007f7ee46af229 in opal_progress () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; &gt; #4 &nbsp;0x00007f7ee45fcf75 in ompi_request_default_wait_all () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; &gt; #5 &nbsp;0x00007f7ee060b65e in ompi_coll_tuned_sendrecv_actual () from<br>
&gt;&gt; /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt;&gt; &gt; &gt; #6 &nbsp;0x00007f7ee06138ff in ompi_coll_tuned_barrier_intra_two_procs ()<br>
&gt;&gt; from /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt;&gt; &gt; &gt; #7 &nbsp;0x00007f7ee460a1c2 in PMPI_Barrier () from<br>
&gt;&gt; /usr/local/lib/libmpi.so.1<br>
&gt;&gt; &gt; &gt; #8 &nbsp;0x0000000000400a43 in main ()<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; &nbsp;Which looks exactly the same on each machine. &nbsp;Any thoughts or ideas<br>
&gt;&gt; would be greatly appreciated as<br>
&gt;&gt; &gt; &gt; I am stuck.<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; &nbsp;Clay Kirkland<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt;<br>
&gt;&gt; &gt; &gt; _______________________________________________<br>
&gt;&gt; &gt; &gt; users mailing list<br>
&gt;&gt; &gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; &gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; -------------- next part --------------<br>
&gt;&gt; &gt; HTML attachment scrubbed and removed<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; ------------------------------<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Subject: Digest Footer<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; _______________________________________________<br>
&gt;&gt; &gt; users mailing list<br>
&gt;&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; ------------------------------<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; End of users Digest, Vol 2879, Issue 1<br>
&gt;&gt; &gt; **************************************<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; _______________________________________________<br>
&gt;&gt; &gt; users mailing list<br>
&gt;&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; Jeff Squyres<br>
&gt;&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt;&gt; For corporate legal information go to:<br>
&gt;&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; ------------------------------<br>
&gt;&gt;<br>
&gt;&gt; Subject: Digest Footer<br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;<br>
&gt;&gt; ------------------------------<br>
&gt;&gt;<br>
&gt;&gt; End of users Digest, Vol 2881, Issue 1<br>
&gt;&gt; **************************************<br>
&gt;&gt;<br>
&gt;<br>
&gt;<br>
-------------- next part --------------<br>
HTML attachment scrubbed and removed<br>
<br>
------------------------------<br>
<br>
Subject: Digest Footer<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
------------------------------<br>
<br>
End of users Digest, Vol 2881, Issue 2<br>
**************************************<br>
</blockquote></div><br></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
