<?
$subject_val = "[OMPI users] OpenMPI 1.2.5 race condition / core dump with MPI_Reduce and MPI_Gather";
include("../../include/msg-header.inc");
?>
<!-- received="Fri Feb 22 09:48:38 2008" -->
<!-- isoreceived="20080222144838" -->
<!-- sent="Fri, 22 Feb 2008 14:48:26 +0000" -->
<!-- isosent="20080222144826" -->
<!-- name="John Markus Bj&#248;rndalen" -->
<!-- email="jmb_at_[hidden]" -->
<!-- subject="[OMPI users] OpenMPI 1.2.5 race condition / core dump with MPI_Reduce and MPI_Gather" -->
<!-- id="47BEE0BA.60808_at_cs.uit.no" -->
<!-- charset="ISO-8859-1" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>Subject:</strong> [OMPI users] OpenMPI 1.2.5 race condition / core dump with MPI_Reduce and MPI_Gather<br>
<strong>From:</strong> John Markus Bj&#248;rndalen (<em>jmb_at_[hidden]</em>)<br>
<strong>Date:</strong> 2008-02-22 09:48:26
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5067.php">Brian W. Barrett: "Re: [OMPI users] mpi.h macro naming"</a>
<li><strong>Previous message:</strong> <a href="5065.php">Jeff Squyres: "Re: [OMPI users] openmpi/openib problems"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5080.php">Jeff Squyres: "Re: [OMPI users] OpenMPI 1.2.5 race condition / core dump with MPI_Reduce and MPI_Gather"</a>
<li><strong>Reply:</strong> <a href="5080.php">Jeff Squyres: "Re: [OMPI users] OpenMPI 1.2.5 race condition / core dump with MPI_Reduce and MPI_Gather"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
Hi,
<br>
<p>I ran into a bug when running a few microbenchmarks for OpenMPI. I had 
<br>
thrown in Reduce and Gather for sanity checking, but OpenMPI crashed 
<br>
when running those operations. Usually, this would happen when I reached 
<br>
around 12-16 nodes.
<br>
<p>My current crash-test code looks like this (I've removed a few lines 
<br>
that were commented out):
<br>
<p>----------- snip-------------
<br>
#include &lt;mpi.h&gt;
<br>
#include &lt;stdlib.h&gt;
<br>
#include &lt;stdio.h&gt;
<br>
#include &lt;assert.h&gt;
<br>
#include &lt;unistd.h&gt;
<br>
<p>int main(int argc, char *argv[])
<br>
{
<br>
&nbsp;&nbsp;&nbsp;&nbsp;int rank, size, count = 1;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;MPI_Init(&amp;argc, &amp;argv);
<br>
&nbsp;&nbsp;&nbsp;&nbsp;MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);
<br>
&nbsp;&nbsp;&nbsp;&nbsp;MPI_Comm_size(MPI_COMM_WORLD, &amp;size);
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;if (argc &gt; 1)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;count = atoi(argv[1]);
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;int n, i;
<br>
&nbsp;&nbsp;&nbsp;&nbsp;// Just make sure we have plenty of buffer for any operation
<br>
&nbsp;&nbsp;&nbsp;&nbsp;int *sbuf = malloc(sizeof(int) * 2 * count);
<br>
&nbsp;&nbsp;&nbsp;&nbsp;int *rbuf = malloc(sizeof(int) * 2 * count);
<br>
&nbsp;&nbsp;&nbsp;&nbsp;assert(sbuf);
<br>
&nbsp;&nbsp;&nbsp;&nbsp;assert(rbuf);
<br>
<p>&nbsp;&nbsp;&nbsp;&nbsp;for (n = 1; n &lt;= 10000; n += 100) {
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf(&quot;N = %d\n&quot;, n);
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fflush(stdout);
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for (i = 0; i &lt; n; i++) {
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MPI_Reduce(sbuf, rbuf, count, MPI_INT, MPI_SUM, 0, 
<br>
MPI_COMM_WORLD);
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MPI_Barrier(MPI_COMM_WORLD);   
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf(&quot; -- DONE\n&quot;);
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fflush(stdout);
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MPI_Barrier(MPI_COMM_WORLD);   
<br>
&nbsp;&nbsp;&nbsp;&nbsp;}
<br>
&nbsp;&nbsp;&nbsp;&nbsp;MPI_Finalize();
<br>
&nbsp;&nbsp;&nbsp;&nbsp;return 0;
<br>
}
<br>
----------- snip-------------
<br>
<p><p>I can usually trigger a crash with count=1, and n=1000 using 16+ nodes, 
<br>
but I can also trigger it with  44 nodes and larger packets (around 32k 
<br>
ints I think). I can also crash it on a single host using 19 processes, 
<br>
but then it usually doesn't crash until I reach somewhere between 
<br>
1200-3000 iterations. Gather seems to have the same problems as Reduce.
<br>
<p>The output from running gdb on the coredump looks like this:
<br>
<p>----------- snip-------------
<br>
Using host libthread_db library &quot;/lib/tls/libthread_db.so.1&quot;.
<br>
Core was generated by `./ompi-crash2'.
<br>
Program terminated with signal 11, Segmentation fault.
<br>
#0  0x00434184 in sysconf () from /lib/tls/libc.so.6
<br>
#0  0x00434184 in sysconf () from /lib/tls/libc.so.6
<br>
#1  0xb7e78b59 in _int_malloc () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#2  0xb7e799ce in malloc () from /home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#3  0xb7f04852 in ompi_free_list_grow () from 
<br>
/home/johnm/local/ompi/lib/libmpi.so.0
<br>
#4  0xb7d74e70 in mca_btl_tcp_endpoint_recv_handler () from 
<br>
/home/johnm/local/ompi/lib/openmpi/mca_btl_tcp.so
<br>
#5  0xb7e62b44 in opal_event_base_loop () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#6  0xb7e62cff in opal_event_loop () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#7  0xb7e5d284 in opal_progress () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#8  0xb7d74f08 in mca_btl_tcp_endpoint_recv_handler () from 
<br>
/home/johnm/local/ompi/lib/openmpi/mca_btl_tcp.so
<br>
#9  0xb7e62b44 in opal_event_base_loop () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#10 0xb7e62cff in opal_event_loop () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#11 0xb7e5d284 in opal_progress () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#12 0xb7d74f08 in mca_btl_tcp_endpoint_recv_handler () from 
<br>
/home/johnm/local/ompi/lib/openmpi/mca_btl_tcp.so
<br>
#13 0xb7e62b44 in opal_event_base_loop () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#14 0xb7e62cff in opal_event_loop () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
<p>... and then continues until...
<br>
<p>#1356848 0xb7e5d284 in opal_progress () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#1356849 0xb7d8f389 in mca_pml_ob1_recv_frag_match () from 
<br>
/home/johnm/local/ompi/lib/openmpi/mca_pml_ob1.so
<br>
#1356850 0xb7d74a7d in mca_btl_tcp_endpoint_recv_handler () from 
<br>
/home/johnm/local/ompi/lib/openmpi/mca_btl_tcp.so
<br>
#1356851 0xb7e62b44 in opal_event_base_loop () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#1356852 0xb7e62cff in opal_event_loop () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#1356853 0xb7e5d284 in opal_progress () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#1356854 0xb7d8f389 in mca_pml_ob1_recv_frag_match () from 
<br>
/home/johnm/local/ompi/lib/openmpi/mca_pml_ob1.so
<br>
#1356855 0xb7d74a7d in mca_btl_tcp_endpoint_recv_handler () from 
<br>
/home/johnm/local/ompi/lib/openmpi/mca_btl_tcp.so
<br>
#1356856 0xb7e62b44 in opal_event_base_loop () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#1356857 0xb7e62cff in opal_event_loop () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#1356858 0xb7e5d284 in opal_progress () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#1356859 0xb7d8f389 in mca_pml_ob1_recv_frag_match () from 
<br>
/home/johnm/local/ompi/lib/openmpi/mca_pml_ob1.so
<br>
#1356860 0xb7d74a7d in mca_btl_tcp_endpoint_recv_handler () from 
<br>
/home/johnm/local/ompi/lib/openmpi/mca_btl_tcp.so
<br>
#1356861 0xb7e62b44 in opal_event_base_loop () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#1356862 0xb7e62cff in opal_event_loop () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#1356863 0xb7e5d284 in opal_progress () from 
<br>
/home/johnm/local/ompi/lib/libopen-pal.so.0
<br>
#1356864 0xb7d8cb69 in mca_pml_ob1_recv () from 
<br>
/home/johnm/local/ompi/lib/openmpi/mca_pml_ob1.so
<br>
#1356865 0xb7d5bb1c in ompi_coll_tuned_reduce_intra_basic_linear () from 
<br>
/home/johnm/local/ompi/lib/openmpi/mca_coll_tuned.so
<br>
#1356866 0xb7d55913 in ompi_coll_tuned_reduce_intra_dec_fixed () from 
<br>
/home/johnm/local/ompi/lib/openmpi/mca_coll_tuned.so
<br>
#1356867 0xb7f3db6c in PMPI_Reduce () from 
<br>
/home/johnm/local/ompi/lib/libmpi.so.0
<br>
#1356868 0x0804899e in main (argc=1, argv=0xbfba8a84) at ompi-crash2.c:58
<br>
----------- snip-------------
<br>
<p>I poked around in the code, and it looks like the culprit might be in 
<br>
the macros that try to allocate fragments in 
<br>
mca_pml_ob1_recv_frag_match: MCA_PML_OB1_RECV_FRAG_ALLOC and 
<br>
MCA_PML_OB1_RECV_FRAG_INIT use OMPI_FREE_LIST_WAIT, which again can end 
<br>
up calling opal_condition_wait(). opal_condition_wait() calls 
<br>
opal_progress() to &quot;block&quot;, which looks like it leads to infinite 
<br>
recursion in this case.
<br>
<p>I guess the problem is a race condition when one node is hammered with 
<br>
incoming packets.
<br>
<p>The stack trace contains about 1.35 million lines, so I won't include 
<br>
all of it here, but here's some statistics to verify that not much else 
<br>
is happening in that stack (I can make the full trace available if 
<br>
anybody needs it):
<br>
<p>----------- snip-------------
<br>
Number of callframes:  1356870
<br>
Called function statistics (how often in stackdump):
<br>
&nbsp;&nbsp;PMPI_Reduce                                        1
<br>
&nbsp;&nbsp;_int_malloc                                        1
<br>
&nbsp;&nbsp;main                                               1
<br>
&nbsp;&nbsp;malloc                                             1
<br>
&nbsp;&nbsp;mca_btl_tcp_endpoint_recv_handler             339197
<br>
&nbsp;&nbsp;mca_pml_ob1_recv                                   1
<br>
&nbsp;&nbsp;mca_pml_ob1_recv_frag_match                       72
<br>
&nbsp;&nbsp;ompi_coll_tuned_reduce_intra_basic_linear           1
<br>
&nbsp;&nbsp;ompi_coll_tuned_reduce_intra_dec_fixed             1
<br>
&nbsp;&nbsp;ompi_free_list_grow                                1
<br>
&nbsp;&nbsp;opal_event_base_loop                          339197
<br>
&nbsp;&nbsp;opal_event_loop                               339197
<br>
&nbsp;&nbsp;opal_progress                                 339197
<br>
&nbsp;&nbsp;sysconf                                            2
<br>
Address statistics (how often in stackdump), plus functions with that addr
<br>
(sanity check):
<br>
&nbsp;&nbsp;0x00434184                                         2 set(['sysconf'])
<br>
&nbsp;&nbsp;0x0804899e                                         1 set(['main'])
<br>
&nbsp;&nbsp;0xb7d55913                                         1 
<br>
set(['ompi_coll_tuned_reduce_intra_dec_fixed'])
<br>
&nbsp;&nbsp;0xb7d5bb1c                                         1 
<br>
set(['ompi_coll_tuned_reduce_intra_basic_linear'])
<br>
&nbsp;&nbsp;0xb7d74a7d                                        72 
<br>
set(['mca_btl_tcp_endpoint_recv_handler'])
<br>
&nbsp;&nbsp;0xb7d74e70                                         1 
<br>
set(['mca_btl_tcp_endpoint_recv_handler'])
<br>
&nbsp;&nbsp;0xb7d74f08                                    339124 
<br>
set(['mca_btl_tcp_endpoint_recv_handler'])
<br>
&nbsp;&nbsp;0xb7d8cb69                                         1 
<br>
set(['mca_pml_ob1_recv'])
<br>
&nbsp;&nbsp;0xb7d8f389                                        72 
<br>
set(['mca_pml_ob1_recv_frag_match'])
<br>
&nbsp;&nbsp;0xb7e5d284                                    339197 
<br>
set(['opal_progress'])
<br>
&nbsp;&nbsp;0xb7e62b44                                    339197 
<br>
set(['opal_event_base_loop'])
<br>
&nbsp;&nbsp;0xb7e62cff                                    339197 
<br>
set(['opal_event_loop'])
<br>
&nbsp;&nbsp;0xb7e78b59                                         1 set(['_int_malloc'])
<br>
&nbsp;&nbsp;0xb7e799ce                                         1 set(['malloc'])
<br>
&nbsp;&nbsp;0xb7f04852                                         1 
<br>
set(['ompi_free_list_grow'])
<br>
&nbsp;&nbsp;0xb7f3db6c                                         1 set(['PMPI_Reduce'])
<br>
----------- snip-------------
<br>
<p>I don't have any suggestions for a fix though, since this is the first 
<br>
time I've looked into the OpenMPI code.
<br>
<p>Btw. In case it makes a difference for triggering the bug: I'm running 
<br>
this on a cluster with 1 frontend and 44 nodes. The cluster runs Rocks 
<br>
4.1, and each of the nodes are 3.2GHz P4 Prescott machines with 2GB RAM, 
<br>
connected with gigabit Ethernet.
<br>
<p><p>Regards,
<br>
<p><pre>
-- 
// John Markus Bj&#248;rndalen
// <a href="http://www.cs.uit.no/~johnm/">http://www.cs.uit.no/~johnm/</a>
</pre>
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="5067.php">Brian W. Barrett: "Re: [OMPI users] mpi.h macro naming"</a>
<li><strong>Previous message:</strong> <a href="5065.php">Jeff Squyres: "Re: [OMPI users] openmpi/openib problems"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="5080.php">Jeff Squyres: "Re: [OMPI users] OpenMPI 1.2.5 race condition / core dump with MPI_Reduce and MPI_Gather"</a>
<li><strong>Reply:</strong> <a href="5080.php">Jeff Squyres: "Re: [OMPI users] OpenMPI 1.2.5 race condition / core dump with MPI_Reduce and MPI_Gather"</a>
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
