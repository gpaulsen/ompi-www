Hi Tom,<br>        sorry to add something in the same vein as Eugene&#39;s reply. i think this is an excellent resource <br><a href="http://ci-tutor.ncsa.illinois.edu/login.php">http://ci-tutor.ncsa.illinois.edu/login.php</a>. It&#39;s a great online course and detailed! Before I took proper classes, this helped me a lot!!<br>
<br><div class="gmail_quote">On Thu, Jan 28, 2010 at 7:05 PM, Tim <span dir="ltr">&lt;<a href="mailto:timlee126@yahoo.com">timlee126@yahoo.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Thanks, Eugene.<br>
<br>
I admit I am not that smart to understand well how to use MPI, but I did read some basic materials about it and understand how some simple problems are solved by MPI.<br>
<br>
But dealing with an array in my case, I am not certain about how to apply MPI to it. Are you saying to use send and recieve to transfer the value computed for each element from child process to parent process? Do you allocate a copy of the array for each process?<br>

<br>
Also I only need the loop that computes every element of the array to be parallelized. Someone said that the parallel part begins with MPI_Init and ends with MPI_Finilize, and one can do any serial computations before and/or after these calls. But I have wrote some MPI programs, and found that the parallel part is not restricted between MPI_Init and MPI_Finilize, but instead the whole program. If the rest part of the code has to be wrapped for process with ID 0, I have little idea about how to apply that to my case since the rest part would be the parts before and after the loop in the function and the whole in main().<br>

<br>
If someone could give a sample of how to apply MPI in my case, it will clarify a lot of my questions. Usually I can learn a lot from good examples.<br>
<br>
Thanks!<br>
<br>
--- On Thu, 1/28/10, Eugene Loh &lt;<a href="mailto:Eugene.Loh@sun.com">Eugene.Loh@sun.com</a>&gt; wrote:<br>
<br>
&gt; From: Eugene Loh &lt;<a href="mailto:Eugene.Loh@sun.com">Eugene.Loh@sun.com</a>&gt;<br>
&gt; Subject: Re: [OMPI users] speed up this problem by MPI<br>
&gt; To: &quot;Open MPI Users&quot; &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
&gt; Date: Thursday, January 28, 2010, 7:30 PM<br>
<div><div></div><div class="h5">&gt; Take a look at some introductory MPI<br>
&gt; materials to learn how to use MPI and what it&#39;s about. <br>
&gt; There should be resources on-line... take a look around.<br>
&gt;<br>
&gt; The main idea is that you would have many processes, each<br>
&gt; process would have part of the array.  Thereafter, if a<br>
&gt; process needs data or results from any other process, such<br>
&gt; data would have to be exchanged between the processes<br>
&gt; explicitly.<br>
&gt;<br>
&gt; Many codes have both OpenMP and MPI parallelization, but<br>
&gt; you should first familiarize yourself with the basics of MPI<br>
&gt; before dealing with &quot;hybrid&quot; codes.<br>
&gt;<br>
&gt; Tim wrote:<br>
&gt;<br>
&gt; &gt; Hi,<br>
&gt; &gt;<br>
&gt; &gt; (1). I am wondering how I can speed up the<br>
&gt; time-consuming computation in the loop of my code below<br>
&gt; using MPI?<br>
&gt; &gt;       int main(int argc, char<br>
&gt; ** argv)       {   <br>
&gt;    // some operations     <br>
&gt;          f(size);   <br>
&gt;            // some<br>
&gt; operations         <br>
&gt;    return 0;   <br>
&gt;    }           <br>
&gt;   void f(int size)   <br>
&gt;    {       // some<br>
&gt; operations             <br>
&gt; int i;           <br>
&gt;    double * array =  new double<br>
&gt; [size];           <br>
&gt;    for (i = 0; i &lt; size; i++) // how can I<br>
&gt; use MPI to speed up this loop to compute all elements in the<br>
&gt; array?       {   <br>
&gt;    array[i] = complicated_computation(); //<br>
&gt; time comsuming computation   <br>
&gt;    }           <br>
&gt;    // some operations using all elements in<br>
&gt; array           <br>
&gt;    delete [] array;      }<br>
&gt; &gt;<br>
&gt; &gt; As shown in the code, I want to do some operations<br>
&gt; before and after the part to be paralleled with MPI, but I<br>
&gt; don&#39;t know how to specify where the parallel part begins and<br>
&gt; ends.<br>
&gt; &gt;<br>
&gt; &gt; (2) My current code is using OpenMP to speed up the<br>
&gt; comutation.<br>
&gt; &gt;     void f(int size)   <br>
&gt;    {       // some<br>
&gt; operations           <br>
&gt;    int i;         <br>
&gt;      double * array =  new double<br>
&gt; [size];   <br>
&gt;    omp_set_num_threads(_nb_threads); <br>
&gt;     #pragma omp parallel shared(array)<br>
&gt; private(i)      {<br>
&gt; &gt;     #pragma omp for<br>
&gt; schedule(dynamic) nowait         <br>
&gt;     for (i = 0; i &lt; size; i++) // how can I use<br>
&gt; MPI to speed up this loop to compute all elements in the<br>
&gt; array?       {   <br>
&gt;    array[i] = complicated_computation(); //<br>
&gt; time comsuming computation   <br>
&gt;    }           <br>
&gt;   }     // some operations using<br>
&gt; all elements in array         <br>
&gt;      }<br>
&gt; &gt;<br>
&gt; &gt; I wonder if I change to use MPI, is it possible to<br>
&gt; have the code written both for OpenMP and MPI? If it is<br>
&gt; possible, how to write the code and how to compile and run<br>
&gt; the code?<br>
&gt; &gt; <br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
<br>
<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

