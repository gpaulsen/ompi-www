<div dir="ltr">Instead of posting a single, big MPI_Igather, why not post and simultaneously progress multiple, small MPI_Igathers? In this way, you can pipeline multiple outstanding collectives and do post collective processing as soon as requests complete. Without network hardware offload capability, <span style="font-size:12.8px">Gilles&#39; observation about the perceived overlap not being truly asynchronous is true; however, you will be able to at least leverage more scalable logarithmic algorithms (depending on which library implementation you use), and make more effective use of CPU cycles. </span><div><br></div><div>Josh </div></div><div class="gmail_extra"><br><div class="gmail_quote">On Wed, Oct 21, 2015 at 8:31 PM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles@rist.or.jp" target="_blank">gilles@rist.or.jp</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
  
    
  
  <div bgcolor="#FFFFFF" text="#000000">
    Eric,<br>
    <br>
    #1 MPI_Igather uses only one MPI_Request, and it is marked as
    completed when all data has been received.<br>
    so no, you cannot process data as it is received.<br>
    (and btw, that would require extra subroutines to ask which piece of
    data has already been received)<br>
    <br>
    #2 maybe not ...<br>
    a tree based approach has O(log(n)) scaling<br>
    (compared to O(n) scaling with your linear method.<br>
    so at scale, MPI_Igather will hopefully scale better (and especially
    if you are transmitting small messages)<br>
    <br>
    #3 difficult question ...<br>
    first, keep in mind there is currently no progress thread in Open
    MPI. that means messages can be received only when MPI_Wait* or
    MPI_Test* is invoked. you might hope messages are received when
    doing some computation (overlap of computation and communication)
    but unfortunatly, that does not happen most of the time.<br>
    <br>
    linear gather does not scale well (see my previous comment) plus you
    openmpi might malloc some space &quot;under the hood&quot; so MPI_Igather will
    hopefully scale better.<br>
    is there any hard reason why you are using non blocking collective ?<br>
    if your application is known to be highly asynchronous and some
    message might arrive (way) later than others, and computation is
    quite expensive, then your approach might be a good fit.<br>
    if your application is pretty synchronous, and cost of computation
    that might overlap with communication is not significant, your
    approach might have little benefits and poor scalability, so
    MPI_Gather (not MPI_Igather since you might have no computation that
    could overlap with communication) might be a better choice.<br>
    <br>
    Cheers,<br>
    <br>
    Gilles<div><div class="h5"><br>
    <br>
    <div>On 10/22/2015 4:46 AM, Eric Chamberland
      wrote:<br>
    </div>
    </div></div><blockquote type="cite"><div><div class="h5">Hi,
      <br>
      <br>
      A long time ago (in 2002) we programmed here a non-blocking
      MPI_Igather with equivalent calls to MPI_Isend/MPI_Irecv (see the
      2 attached files).
      <br>
      <br>
      A very convenient advantage of this version, is that I can do some
      work on the root process as soon as it start receiving data... 
      Then, it wait for the next communication to terminate and process
      the received data.
      <br>
      <br>
      Now, I am looking at MPI_Igather (and all non-blocking collective
      MPI calls), and I am somewhat surprised (or ignorant) that I
      cannot have the root rank receive some data, then process it, then
      wait until the next &quot;MPI_irecv&quot; terminate...
      <br>
      <br>
      In other words, a MPI_Igather generate only 1 MPI_Request but I
      would like to have either &quot;p&quot; (with p = size of communicator)
      MPI_Request generated or be able to call &quot;p&quot; times MPI_WaitAny
      with the same MPI_Request...  Am I normal? :)
      <br>
      <br>
      So my 3 questions are:
      <br>
      <br>
      #1- Is there a way to use MPI_Igather with MPI_WaitAny (or
      something else?) to process data as it is received?
      <br>
      <br>
      #2- Big question: will our implementation with MPI_Isend/MPI_Irecv
      scale to a large number of processes?  What are the possible
      drawbacks of doing it like we did?
      <br>
      <br>
      #3- Why should I replace our implementation by the native
      MPI_Igather?
      <br>
      <br>
      Thanks!
      <br>
      <br>
      Eric
      <br>
      <br>
      <br>
      <fieldset></fieldset>
      <br>
      </div></div><pre>_______________________________________________
users mailing list
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/10/27914.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/10/27914.php</a></pre>
    </blockquote>
    <br>
  </div>

<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/10/27915.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/10/27915.php</a><br></blockquote></div><br></div>

