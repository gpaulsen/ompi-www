Thanks&nbsp; for the feedback.<br><br>Rui<br><br><div class="gmail_quote">2010/6/29 Changsheng Jiang <span dir="ltr">&lt;<a href="mailto:jiangzuoyan@gmail.com">jiangzuoyan@gmail.com</a>&gt;</span><br><blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">
I am a learner, too, please correct me.<br><br clear="all"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Changsheng Jiang<br>
<br><br><div class="gmail_quote"><div class="im">On Tue, Jun 29, 2010 at 15:44, Íõî£ <span dir="ltr">&lt;<a href="mailto:wangraying@gmail.com" target="_blank">wangraying@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">


Hi, all<br><br>I&#39;m now learning MPI, but I&#39;m not clear 
with the following questions, <br><br>1, suppose a
 MPI program involves several nodes, if one node dead, will the program 
terminate? <br></blockquote></div><div>yes.&nbsp; If you program killed in one node, all another nodes will note that and abort. But if you server halt, or, you killed the parent of your program(which luanched by mpi runtime) first, then another nodes will wait a very long time before exit.<br>


</div><div class="im"><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<br>2, Is there any possibility to extend or shrink the size of MPI 
communicator size? If so, we can use spare node to replace the dead 
node?&nbsp; <br></blockquote></div><div>I think no. the message passing model prevent you doing this. the distributed program status depends on the dead nodes. <br></div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<div class="im">

<br>Thanks in advance and your reply will be highly appreciated!<br>
<br>Best Regards,<br>Rui Wang<br><br>
<br></div>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br><br clear="all"><br>-- <br>Rui Wang<br>Institute of Computing Technology, CAS, &nbsp;Beijing, P.R.China<br>


