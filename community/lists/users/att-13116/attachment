Hello Jeff,<br><br>thanks for your detailed answer.<br><br><div class="gmail_quote">2010/5/20 Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
You&#39;re basically talking about implementing some kind of application-specific protocol.  A few tips that may help in your design:<br>
<br>
1. Look into MPI_Isend / MPI_Irecv for non-blocking sends and receives.  These may be particularly useful in the server side, so that it can do other stuff while sends and receives are progressing.<br></blockquote><div><br>
-&gt; You are definitively right, I have to switch to non blocking sends and receives.<br> </div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">

<br>
2. You probably already noticed that collective operations (broadcasts and the link) need to be invoked by all members of the communicator.  So if you want to do a broadcast, everyone needs to know.  That being said, you can send a short message to everyone alerting them that a longer broadcast is coming -- then they can execute MPI_BCAST, etc.  That works best if your broadcasts are large messages (i.e., you benefit from scalable implementations of broadcast) -- otherwise you&#39;re individually sending short messages followed by a short broadcast.  There might not be much of a &quot;win&quot; there.<br>
</blockquote><div><br>-&gt; That is what I was thinking about to implement. As you mentioned, and specifically for my case where I mainly send short messages, there might not be much win. By the way, are their some benchmarks testing sequential MPI_ISend versus MPI_BCAST for instance ? The aim is to determine up which buffer size a MPI_BCast is worth to be used for my case. You can answer that the test is easy to do and that I can test it by myself :)<br>
 </div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<br>
3. FWIW, the MPI Forum has introduced the concept of non-blocking collective operations for the upcoming MPI-3 spec.  These may help; google for libnbc for a (non-optimized) implementation that may be of help to you.  MPI implementations (like Open MPI) will feature non-blocking collectives someday in the future.<br>

<div><div></div><div class="h5"><br></div></div></blockquote><div><br>-&gt; Interesting to know and to keep in mind. Sometimes the future is really near.<br> <br>Thanks again for your answer and info.<br><br>Olivier<br><br>
<br></div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div><div class="h5">
<br>
On May 20, 2010, at 5:30 AM, Olivier Riff wrote:<br>
<br>
&gt; Hello,<br>
&gt;<br>
&gt; I have a general question about the best way to implement an openmpi application, i.e the design of the application.<br>
&gt;<br>
&gt; A machine (I call it the &quot;server&quot;) should send to a cluster containing a lot of processors (the &quot;clients&quot;) regularly task to do (byte buffers from very various size).<br>
&gt; The server should send to each client a different buffer, then wait for each client answers (buffer sent by each client after some processing), and retrieve the result data.<br>
&gt;<br>
&gt; First I made something looking like this.<br>
&gt; On the server side: Send sequentially to each client buffers using MPI_Send.<br>
&gt; On each client side: loop which waits a buffer using MPI_Recv, then process the buffer and sends the result using MPI_Send<br>
&gt; This is really not efficient because a lot of time is lost due to the fact that the server sends and receives sequentially the buffers.<br>
&gt; It only has the advantage to have on the client size a pretty easy scheduler:<br>
&gt; Wait for buffer (MPI_Recv) -&gt; Analyse it -&gt; Send result (MPI_Send)<br>
&gt;<br>
&gt; My wish is to mix MPI_Send/MPI_Recv and other mpi functions like MPI_BCast/MPI_Scatter/MPI_Gather... (like I imagine every mpi application does).<br>
&gt; The problem is that I cannot find a easy solution in order that each client knows which kind of mpi function is currently called by the server. If the server calls MPI_BCast the client should do the same. Sending at each time a first message to indicate the function the server will call next does not look very nice. Though I do not see an easy/best way to implement an &quot;adaptative&quot; scheduler on the client side.<br>

&gt;<br>
&gt; Any tip, advice, help would be appreciate.<br>
&gt;<br>
&gt;<br>
&gt; Thanks,<br>
&gt;<br>
&gt; Olivier<br>
&gt;<br>
</div></div><div class="im">&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
</div><div class="im">--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to:<br>
<a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
<br>
_______________________________________________<br>
</div><div><div></div><div class="h5">users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

