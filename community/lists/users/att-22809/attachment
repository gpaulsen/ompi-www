<div dir="ltr">On 21 October 2013 15:19, Andreas Schäfer <span dir="ltr">&lt;<a href="mailto:gentryx@gmx.de" target="_blank">gentryx@gmx.de</a>&gt;</span> wrote:<br><div class="gmail_extra"><div class="gmail_quote"><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">
Hi,<br>
<br>
the solution depends on the details of your code. Will all clients<br>
send their progress updates simultaneously? Are you planning for few<br>
or many nodes?<br>
<br>
For few nodes and non-simultaneous updates you could loop on the root<br>
while receiving from MPI_ANY. Clients could send out their updates via<br>
MPI_Isend().<br>
<br>
If you&#39;re expecting many nodes, this 1-n schema will eventually<br>
overwhelm the root node. In that case MPI_Gather() or MPI_Reduce()<br>
will perform better. But those require all nodes to participate.<br>
<br>
Things get complicated if you want non-simultaneous updates from many<br>
nodes...<br>
<br>
HTH<br>
-Andreas</blockquote><div><br></div><div>Thanks, currently I run a prototype with 32 mpi processes or so. But I would deploy to a larger set later.</div><div><br></div><div>===&gt; root process code:</div><div>I) mpi thread</div>
<div>1. list all n-tuples<br></div><div>2. split list equally for 32 processes</div><div>3. scatter</div><div>4. loop to evaluate locally f for my section of space</div><div>5. reduce</div><div><br></div><div>II) UI thread</div>
<div><br></div><div>===&gt; compute mpi process node<br></div><div><div>3. scatter list of ntuples<br></div><div>4. loop to evaluate locally f for my section of space</div><div>5. reduce</div></div><div><br></div><div><br>
</div><div>The loops 4 are not naturally in sync.</div><div>Would you suggest to modify the loop to do a MPI_ISend after x iterations (for the clients) and MPI_IRecv on the root?</div><div><br></div><div>Thanks MM</div></div>
</div></div>

