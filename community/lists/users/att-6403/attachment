<table cellspacing="0" cellpadding="0" border="0" ><tr><td valign="top" style="font: inherit;">George,<br><br>Thank you for taking the time and composing the detailed explanation. This gives me a bit more understanding with respect to the underlying plumbing, which I appreciate.<br><br>Bottom line the update to r19377 has appeared to have resolved the truncate problem. While I have tested in only a limited number of hosts, it seems to behave as expected.&nbsp; Thanks!! Tom<br><br>--- On <b>Mon, 8/18/08, George Bosilca <i>&lt;bosilca@eecs.utk.edu&gt;</i></b> wrote:<br><blockquote style="border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;">From: George Bosilca &lt;bosilca@eecs.utk.edu&gt;<br>Subject: Re: [OMPI users] MPI_ERR_TRUNCATE with MPI_Revc without Infinipath<br>To: "Open MPI Users" &lt;users@open-mpi.org&gt;<br>Cc: "Tom Riddle" &lt;rarebitusa@yahoo.com&gt;<br>Date: Monday, August 18, 2008, 4:16
 PM<br><br><pre>Tom,<br><br>This make perfect sense. However, the fact that one of the network  <br>devices (BTL in Open MPi terms) is not available at runtime should not  <br>modify the behavior of the application. At least this is the theory :)  <br>Changing from named receives to unnamed one, definitively modify the  <br>signature (i.e. communication pattern) of the application, and might  <br>in most cases introduce mismatching if the same tag is used. However,  <br>with the osu_latency there are only two ranks involved in the  <br>communication (rank 0 and 1) so the communication pattern should stay  <br>the same whatever you use ANY_SOURCE or not, as the MPI standard  <br>enforce the message ordering.<br><br>Now, let me explain a little bit of internal black magic behind of  <br>Open MPI. When we discover that a BTL is overcharged, we reroute the  <br>new messages into a local "pending" queue, until some space on the  <br>device became available.
 Once we start book-keeping messages, we still  <br>have to enforce the MPI logical ordering, so all new messages will  <br>follow into the "pending" queue, until the device is capable of  <br>sending data again, and then the messages will be delivered in-order  <br>to their respective destination. What might happens, and this is only  <br>speculation at this point, is that somehow a message bypass this  <br>"pending" queue and goes into the wire too early. As this message<br>will  <br>have the same tag, Open MPI might match it when the message arrive at  <br>the destination, and can generate a TRUNCATE error if this message  <br>belong to the next loop in the osu_latency benchmark. As you can see,  <br>there are many ifs in the previous paragraph, so let's assume by now  <br>that this is just pure speculation. Please upgrade to the latest  <br>version of Open MPI, and if you encounter the same problem then we  <br>will try to dig a little bit deeper
 into this "speculation".<br><br>   Thanks,<br>     george.<br><br>On Aug 19, 2008, at 12:36 AM, Tom Riddle wrote:<br><br>&gt; Thanks George, I will update and try the latest repo. However I'd  <br>&gt; like to describe our usage case a bit more to see if there is  <br>&gt; something that may not be proper in our development approach.  <br>&gt; Forgive me if this is repetitious...<br>&gt;<br>&gt; We have configured and built OpenMPI originally on a machine with  <br>&gt; Infinipath / PSM installed. Since we desire a flexible software  <br>&gt; development environment across a number of machines (most of them  <br>&gt; are without the Infinipath hw), we run these same OpenMPI bins in a  <br>&gt; shared user area. That means other developer's machines, which do  <br>&gt; not have Infinipath / PSM installed locally, will simulate the  <br>&gt; multiple machine communication by running in shared memory mode.   <br>&gt; But again these OpenMPI bins have been
 configured with Infinipath  <br>&gt; support.<br>&gt;<br>&gt; So we see the error when running in shared memory mode on machines  <br>&gt; that don't have Infinipath, so is there a way at runtime that you  <br>&gt; can force shared memory mode exclusively? We are wondering if  <br>&gt; designating MPI_ANY_SOURCE may then direct OpenMPI to look at every  <br>&gt; possible communications mode and that probably would cause conflicts  <br>&gt; if there wasn't psm libs present.<br>&gt;<br>&gt; Hope this makes sense, Tom<br>&gt;<br>&gt;<br>&gt;<br>&gt; Things were working without issue until we went to the wildcard  <br>&gt; MPI_ANY_SOURCE on our receives but only on machines without . I  <br>&gt; guess I wonder what is the mechanism when in a wildcard mode.<br>&gt;<br>&gt; --- On Sun, 8/17/08, George Bosilca &lt;bosilca@eecs.utk.edu&gt; wrote:<br>&gt; From: George Bosilca &lt;bosilca@eecs.utk.edu&gt;<br>&gt; Subject: Re: [OMPI users] MPI_ERR_TRUNCATE with
 MPI_Revc without  <br>&gt; Infinipath<br>&gt; To: rarebitusa@yahoo.com, "Open MPI Users"<br>&lt;users@open-mpi.org&gt;<br>&gt; Date: Sunday, August 17, 2008, 2:42 PM<br>&gt;<br>&gt; Tom,<br>&gt;<br>&gt; I did the same modification as you on the osu_latency and the<br>&gt; resulting application run to completion. I don't get any TRUNCATE<br>&gt; error messages. I'm using the latest version of Open MPI  <br>&gt; (1.4a1r19313).<br>&gt;<br>&gt; There was a bug that might be related to your problem but our commit<br>&gt; log shows it was fixed by commit 18830 on July 9.<br>&gt;<br>&gt;    george.<br>&gt;<br>&gt; On Aug 13, 2008, at 5:49 PM, Tom Riddle wrote:<br>&gt;<br>&gt; &gt; Hi,<br>&gt; &gt;<br>&gt; &gt; A bit more info wrt the question below. I have run other releases of<br>&gt; &gt; OpenMPI and they seem to be fine. The reason I need to run the<br>&gt; &gt; latest is because it supports valgrind fully.<br>&gt; &gt;<br>&gt; &gt; openmpi-1.2.4<br>&gt;
 &gt; openmpi-1.3ar18303<br>&gt; &gt;<br>&gt; &gt; TIA, Tom<br>&gt; &gt;<br>&gt; &gt; --- On Tue, 8/12/08, Tom Riddle &lt;rarebitusa@yahoo.com&gt; wrote:<br>&gt; &gt;<br>&gt; &gt; Hi,<br>&gt; &gt;<br>&gt; &gt; I am getting a curious error on a simple communications test. I have<br>&gt; &gt; altered the std<br>&gt;  mvapich osu_latency test to accept receives from any<br>&gt; &gt; source and I get the following error<br>&gt; &gt;<br>&gt; &gt; [d013.sc.net:15455] *** An error occurred in MPI_Recv<br>&gt; &gt; [d013.sc.net:15455] *** on communicator MPI_COMM_WORLD<br>&gt; &gt; [d013.sc.net:15455] *** MPI_ERR_TRUNCATE: message truncated<br>&gt; &gt; [d013.sc.net:15455] *** MPI_ERRORS_ARE_FATAL (goodbye)<br>&gt; &gt;<br>&gt; &gt; the code change was...<br>&gt; &gt;<br>&gt; &gt;  MPI_Recv(r_buf, size, MPI_CHAR, MPI_ANY_SOURCE, 1, MPI_COMM_WORLD,<br>&gt; &gt; &amp;reqstat);<br>&gt; &gt;<br>&gt; &gt; the command line I run was<br>&gt; &gt;<br>&gt; &gt; &gt;
 mpirun -np 2 ./osu_latency<br>&gt; &gt;<br>&gt; &gt; Now I run this on 2 types of host machine configurations. One that<br>&gt; &gt; has Infinipath HCAs  installed and another that doesn't. I run<br>both<br>&gt; &gt; of these in shared memory mode ie: dual processes on the same node.<br>&gt; &gt; I have verified that when I am on the host with Infinipath I am<br>&gt; &gt; actually running the OpenMPI mpirun, not<br>&gt;  the mpi that comes with the<br>&gt; &gt; HCA.<br>&gt; &gt;<br>&gt; &gt; I have built OpenMPI with psm support from a fairly recent svn pull<br>&gt; &gt; and run the same bins on both host machines... The config was as<br>&gt; &gt; follows:<br>&gt; &gt; &gt; $ ../configure --prefix=/opt/wkspace/openmpi-1.3 CC=gcc CXX=g++<br>&gt; &gt; &gt; --disable-mpi-f77 --enable-debug --enable-memchecker<br>&gt; &gt; &gt; --with-psm=/usr/include --with-valgrind=/opt/wkspace/ <br>&gt; valgrind-3.3.0/<br>&gt; &gt; &gt; mpirun --version<br>&gt; &gt;
 mpirun (Open MPI) 1.4a1r18908<br>&gt; &gt;<br>&gt; &gt; The error presents itself only on the host that does not have<br>&gt; &gt; Infinipath installed. I have combed through the mca args to see if<br>&gt; &gt; there is a setting I am missing but I cannot see anything obvious.<br>&gt; &gt;<br>&gt; &gt; Any input would be appreciated. Thanks. Tom<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt; _______________________________________________<br>&gt; &gt; users mailing list<br>&gt; &gt; users@open-mpi.org<br>&gt; &gt;<br>&gt;  http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt;<br>&gt;<br><br></pre></blockquote></td></tr></table><br>

      
