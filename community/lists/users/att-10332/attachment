Well, it is getting better! :-)<br><br>On your cmd line, what btl&#39;s are you specifying? You should try -mca btl sm,tcp,self for this to work. Reason: sometimes systems block tcp loopback on the node. What I see below indicates that inter-node comm was fine, but the two procs that share a node couldn&#39;t communicate. Including shared memory should remove that problem.<br>
<br>The port numbers are fine and can be different or the same - it is totally random. The procs exchange their respective port info during wireup.<br><br><br><div class="gmail_quote">On Wed, Aug 12, 2009 at 12:51 PM, Jody Klymak <span dir="ltr">&lt;<a href="mailto:jklymak@uvic.ca">jklymak@uvic.ca</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div style="">Hi Ralph,<div><br></div><div>That gives me something more to work with...</div>
<div><br></div><div><br><div><div class="im"><div>On Aug 12, 2009, at  9:44 AM, Ralph Castain wrote:</div><br><blockquote type="cite">I believe TCP works fine, Jody, as it is used on Macs fairly widely. I suspect this is something funny about your installation.<br>
<br>One thing I have found is that you can get this error message when you have multiple NICs installed, each with a different subnet, and the procs try to connect across different ones. Do you by chance have multiple NICs?<br>
</blockquote><div><br></div></div><div>The head node has two active NICs:</div><div>en0: public</div><div>en1: private</div><div><br></div><div>The server nodes only have one connection </div><div>en0:private</div><div class="im">
<br><blockquote type="cite"> <br>Have you tried telling OMPI which TCP interface to use? You can do so with -mca btl_tcp_if_include eth0 (or whatever you want to use).<br></blockquote><div><br></div></div><div>If I try this, I get the same results. (though I need to use &quot;en0&quot; on my machine)...</div>
<div><br></div><div><div>If I include -mca btl_base_verbose 30 I get for n=2:</div><div><br></div><div><div>++++++++++</div><div></div><div>[xserve03.local:00841] select: init of component tcp returned success</div><div>Done MPI init</div>
<div>checking connection between rank 0 on xserve02.local and rank 1   </div><div>Done MPI init</div><div>[xserve02.local:01094] btl: tcp: attempting to connect() to address 192.168.2.103 on port 4</div><div>Done checking connection between rank 0 on xserve02.local and rank 1   </div>
<div>Connectivity test on 2 processes PASSED.</div><div><div>++++++++++</div><div></div></div><div><br></div><div>If I try n=3 the job hangs and I have to kill:</div><div><br></div><div><div>++++++++++</div><div></div><div>
Done MPI init</div><div>checking connection between rank 0 on xserve02.local and rank 1   </div><div>[xserve02.local:01110] btl: tcp: attempting to connect() to address 192.168.2.103 on port 4</div><div>Done MPI init</div>
<div>Done MPI init</div><div>checking connection between rank 1 on xserve03.local and rank 2   </div><div>[xserve03.local:00860] btl: tcp: attempting to connect() to address 192.168.2.102 on port 4</div><div>Done checking connection between rank 0 on xserve02.local and rank 1   </div>
<div>checking connection between rank 0 on xserve02.local and rank 2   </div><div>Done checking connection between rank 0 on xserve02.local and rank 2   </div><div>mpirun: killing job...</div><div><div>++++++++++</div><div>
</div></div></div></div><div><br></div><div>Those ip addresses are correct, no idea if port 4 make sense.  Sometimes I get port 260.  Should xserve03 and xserve02 be trying to use the same port for these comms? </div><div>
<br></div><div><br></div><div>Thanks,  Jody</div><div><br></div><div><br></div></div><div><div></div><div class="h5"><br><blockquote type="cite"><br><br><div class="gmail_quote">On Wed, Aug 12, 2009 at 10:01 AM, Jody Klymak <span dir="ltr">&lt;<a href="mailto:jklymak@uvic.ca" target="_blank">jklymak@uvic.ca</a>&gt;</span> wrote:<br>
 <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><br> On Aug 11, 2009, at  18:55 PM, Gus Correa wrote:<br> <br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
 <br> Did you wipe off the old directories before reinstalling?<br> </blockquote> <br> Check.<br> <br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
 I prefer to install on a NFS mounted directory,<br> </blockquote> <br> Check<br> <br> <br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
 Have you tried to ssh from node to node on all possible pairs?<br> </blockquote> <br> check - fixed this today, works fine with the spawning user...<br> <br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
 How could you roll back to 1.1.5,<br> now that you overwrote the directories?<br> </blockquote> <br> Oh, I still have it on another machine off the cluster in /usr/local/openmpi.  Will take just 5 mintues to reinstall.<br>
 <br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> Launching jobs with Torque is way much better than<br> using barebones mpirun.<br> </blockquote>
 <br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> And you don&#39;t want to stay behind with the OpenMPI versions<br> and improvements either.<br>
 </blockquote> <br> Sure, but I&#39;d like the jobs to be able to run at all..<br> <br> Is there any sense in rolling back to to 1.2.3 since that is known to work with OS X (its the one that comes with 10.5)?  My only guess at this point is other OS X users are using non-tcpip communication, and the tcp stuff just doesn&#39;t work in 1.3.3.<br>
 <br> Thanks,  Jody<br> <br> --<br> Jody Klymak<br> <a href="http://web.uvic.ca/%7Ejklymak/" target="_blank">http://web.uvic.ca/~jklymak/</a><br> <br> <br> <br> <br> _______________________________________________<br> users mailing list<br>
 <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> </blockquote>
</div><br> _______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div></div></div><div><div></div><div class="h5"><br><div> <span style="font-size: 12px;"><div style=""><span style="border-collapse: separate; border-spacing: 0px; color: rgb(0, 0, 0); font-family: Lucida Sans Typewriter; font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;"><div>
--</div><div>Jody Klymak    </div><div><a href="http://web.uvic.ca/%7Ejklymak/" target="_blank">http://web.uvic.ca/~jklymak/</a></div><div><br></div><div><br></div><br></span></div></span> </div><br></div></div></div></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

