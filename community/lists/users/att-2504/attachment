<HTML>
<HEAD>
<TITLE>Re: [OMPI users] Can't start more than one process in a node as normal user</TITLE>
</HEAD>
<BODY>
<FONT FACE="Verdana, Helvetica, Arial"><SPAN STYLE='font-size:12.0px'>What that parameter does is turn &#8220;off&#8221; all of the transports except tcp &#8211; so the problem you&#8217;re seeing goes away because we no longer try to create the shared memory file. This will somewhat hurt your performance, but it will work.<BR>
<BR>
Alternatively, you could use &#8220;--mca btl ^sm&#8221;, which would allow you to use whatever high speed interconnects are on your system while still turning &#8220;off&#8221; the shared memory file.<BR>
<BR>
I&#8217;m not sure why your tmp directory is getting its permissions wrong. It sounds like there is something in your environment that is doing something unexpected. You might just write a script and execute it that creates a file and lists its permissions &#8211; would be interesting to see if the user or access permissions are different than what you would normally expect.<BR>
<BR>
Ralph<BR>
<BR>
<BR>
On 1/18/07 8:30 PM, &quot;eddie168&quot; &lt;eddie168+ompi_user@gmail.com&gt; wrote:<BR>
<BR>
</SPAN></FONT><BLOCKQUOTE><FONT FACE="Verdana, Helvetica, Arial"><SPAN STYLE='font-size:12.0px'>Just to answer my own question, after I explicitly specify the &quot;--mca btl tcp&quot; parameter, the program works. So I will need to issue command like this:<BR>
&nbsp;<BR>
$ mpirun --mca btl tcp -np 2 tut01<BR>
oceanus:Hello world from 0<BR>
oceanus:Hello world from 1<BR>
&nbsp;<BR>
Regards,<BR>
&nbsp;<BR>
Eddie.<BR>
<BR>
&nbsp;<BR>
On 1/18/07, <B>eddie168</B> &lt;eddie168+ompi_user@gmail.com&gt; wrote: <BR>
</SPAN></FONT><BLOCKQUOTE><FONT FACE="Verdana, Helvetica, Arial"><SPAN STYLE='font-size:12.0px'>Hi Ralph and Brian,<BR>
&nbsp;<BR>
Thanks for the advice, I have checked the permission to /tmp<BR>
&nbsp;<BR>
drwxrwxrwt &nbsp;&nbsp;19 root &nbsp;root &nbsp;4096 Jan 18 11:38 tmp<BR>
&nbsp;<BR>
which I think there shouldn't be any problem to create files there, so option (a) still not work for me.<BR>
&nbsp;<BR>
I tried option (b) which set --tmpdir on command line and run as normal user, it works for -np 1, however it gives the same error for -np 2.<BR>
&nbsp;<BR>
Option (c) also tested by setting &quot;OMPI_MCA_tmpdir_base = /home2/mpi_tut/tmp&quot; in &quot;~/.openmpi/mca-params.conf&quot;, however error still occurred.<BR>
&nbsp;<BR>
I included the debug output of what I ran (with IP masked), I noticed that the optional tmp directory set in the beginning of the process, however it changed back to &quot;/tmp&quot; after executing orted. Could the error I got related to SSH setting? <BR>
&nbsp;<BR>
Many thanks,<BR>
&nbsp;<BR>
Eddie.<BR>
&nbsp;<BR>
<BR>
<BR>
[eddie@oceanus:~/home2/mpi_tut]$ mpirun -d --tmpdir /home2/mpi_tut/tmp -np 2 tut01<BR>
[oceanus:129119] [0,0,0] setting up session dir with<BR>
[oceanus:129119] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tmpdir /home2/mpi_tut/tmp<BR>
[oceanus:129119] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;universe default-universe <BR>
[oceanus:129119] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user eddie<BR>
[oceanus:129119] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;host oceanus<BR>
[oceanus:129119] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;jobid 0<BR>
[oceanus:129119] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;procid 0<BR>
[oceanus:129119] procdir: /home2/mpi_tut/tmp/openmpi-sessions-eddie@oceanus_0/default-universe/0/0 <BR>
[oceanus:129119] jobdir: /home2/mpi_tut/tmp/openmpi-sessions-eddie@oceanus_0/default-universe/0<BR>
[oceanus:129119] unidir: /home2/mpi_tut/tmp/openmpi-sessions-eddie@oceanus_0/default-universe<BR>
[oceanus:129119] top: openmpi-sessions-eddie@oceanus_0<BR>
[oceanus:129119] tmp: /home2/mpi_tut/tmp<BR>
[oceanus:129119] [0,0,0] contact_file /home2/mpi_tut/tmp/openmpi-sessions-eddie@oceanus_0/default-universe/universe-setup.txt <BR>
[oceanus:129119] [0,0,0] wrote setup file<BR>
[oceanus:129119] pls:rsh: local csh: 0, local bash: 1<BR>
[oceanus:129119] pls:rsh: assuming same remote shell as local shell <BR>
[oceanus:129119] pls:rsh: remote csh: 0, remote bash: 1 <BR>
[oceanus:129119] pls:rsh: final template argv:<BR>
[oceanus:129119] pls:rsh: &nbsp;&nbsp;&nbsp;&nbsp;/usr/bin/ssh &lt;template&gt; orted --debug --bootproxy 1 --name &lt;template&gt; --num_procs 2 --vpid_start 0 --nodename &lt;template&gt; --universe eddie@oceanus:default-universe --nsreplica &quot;0.0.0;tcp://xxx.xxx.xxx.xxx:52428&quot; --gprreplica &quot; 0.0.0;tcp://xxx.xxx.xxx.xxx:52428&quot; --mpi-call-yield 0<BR>
[oceanus:129119] pls:rsh: launching on node localhost<BR>
[oceanus:129119] pls:rsh: oversubscribed -- setting mpi_yield_when_idle to 1 (1 2)<BR>
[oceanus:129119] pls:rsh: localhost is a LOCAL node <BR>
[oceanus:129119] pls:rsh: changing to directory /home/eddie <BR>
[oceanus:129119] pls:rsh: executing: orted --debug --bootproxy 1 --name 0.0.1 --num_procs 2 --vpid_start 0 --nodename localhost --universe eddie@oceanus:default-universe &nbsp;<a href="mailto:eddie@oceanus:default-universe">&lt;mailto:eddie@oceanus:default-universe&gt;</a> --nsreplica &quot;0.0.0;tcp://xxx.xxx.xxx.xxx:52428&quot; --gprreplica &quot;0.0.0;tcp://xxx.xxx.xxx.xxx:52428&quot; --mpi-call-yield 1<BR>
[oceanus:129120] [0,0,1] setting up session dir with <BR>
[oceanus:129120] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;universe default-universe <BR>
[oceanus:129120] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user eddie<BR>
[oceanus:129120] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;host localhost<BR>
[oceanus:129120] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;jobid 0<BR>
[oceanus:129120] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;procid 1<BR>
[oceanus:129120] procdir: /tmp/openmpi-sessions-eddie@localhost_0/default-universe/0/1 <BR>
[oceanus:129120] jobdir: /tmp/openmpi-sessions-eddie@localhost_0/default-universe/0<BR>
[oceanus:129120] unidir: /tmp/openmpi-sessions-eddie@localhost_0/default-universe <BR>
[oceanus:129120] top: openmpi-sessions-eddie@localhost_0<BR>
[oceanus:129120] tmp: /tmp <BR>
[oceanus:129121] [0,1,0] setting up session dir with<BR>
[oceanus:129121] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;universe default-universe<BR>
[oceanus:129121] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user eddie<BR>
[oceanus:129121] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;host localhost <BR>
[oceanus:129121] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;jobid 1 <BR>
[oceanus:129121] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;procid 0<BR>
[oceanus:129121] procdir: /tmp/openmpi-sessions-eddie@localhost_0/default-universe/1/0<BR>
[oceanus:129121] jobdir: /tmp/openmpi-sessions-eddie@localhost_0/default-universe/1 <BR>
[oceanus:129121] unidir: /tmp/openmpi-sessions-eddie@localhost_0/default-universe<BR>
[oceanus:129121] top: openmpi-sessions-eddie@localhost_0<BR>
[oceanus:129121] tmp: /tmp <BR>
[oceanus:129122] [0,1,1] setting up session dir with<BR>
[oceanus:129122] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;universe default-universe<BR>
[oceanus:129122] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user eddie<BR>
[oceanus:129122] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;host localhost <BR>
[oceanus:129122] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;jobid 1 <BR>
[oceanus:129122] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;procid 1<BR>
[oceanus:129122] procdir: /tmp/openmpi-sessions-eddie@localhost_0/default-universe/1/1<BR>
[oceanus:129122] jobdir: /tmp/openmpi-sessions-eddie@localhost_0/default-universe/1 <BR>
[oceanus:129122] unidir: /tmp/openmpi-sessions-eddie@localhost_0/default-universe<BR>
[oceanus:129122] top: openmpi-sessions-eddie@localhost_0<BR>
[oceanus:129122] tmp: /tmp <BR>
[oceanus:129119] spawn: in job_state_callback(jobid = 1, state = 0x4)<BR>
[oceanus:129119] Info: Setting up debugger process table for applications<BR>
&nbsp;&nbsp;MPIR_being_debugged = 0<BR>
&nbsp;&nbsp;MPIR_debug_gate = 0<BR>
&nbsp;&nbsp;MPIR_debug_state = 1 <BR>
&nbsp;&nbsp;MPIR_acquired_pre_main = 0<BR>
&nbsp;&nbsp;MPIR_i_am_starter = 0<BR>
&nbsp;&nbsp;MPIR_proctable_size = 2<BR>
&nbsp;&nbsp;MPIR_proctable:<BR>
&nbsp;&nbsp;&nbsp;&nbsp;(i, host, exe, pid) = (0, localhost, tut01, 129121) <BR>
&nbsp;&nbsp;&nbsp;&nbsp;(i, host, exe, pid) = (1, localhost, tut01, 129122) <BR>
[oceanus:129121] mca_common_sm_mmap_init: ftruncate failed with errno=13<BR>
[oceanus:129121] mca_mpool_sm_init: unable to create shared memory mapping (/tmp/openmpi-sessions-eddie@localhost_0/default-universe/1/shared_mem_pool.localhost )<BR>
--------------------------------------------------------------------------<BR>
It looks like MPI_INIT failed for some reason; your parallel process is<BR>
likely to abort. &nbsp;There are many reasons that a parallel process can <BR>
fail during MPI_INIT; some of which are due to configuration or environment<BR>
problems. &nbsp;This failure appears to be an internal failure; here's some<BR>
additional information (which may only be relevant to an Open MPI <BR>
developer):<BR>
<BR>
&nbsp;&nbsp;PML add procs failed<BR>
&nbsp;&nbsp;--&gt; Returned &quot;Out of resource&quot; (-2) instead of &quot;Success&quot; (0)<BR>
--------------------------------------------------------------------------<BR>
*** An error occurred in MPI_Init <BR>
*** before MPI was initialized<BR>
*** MPI_ERRORS_ARE_FATAL (goodbye)<BR>
[oceanus:129120] sess_dir_finalize: found proc session dir empty - deleting<BR>
[oceanus:129120] sess_dir_finalize: job session dir not empty - leaving <BR>
[oceanus:129120] sess_dir_finalize: found proc session dir empty - deleting<BR>
[oceanus:129120] sess_dir_finalize: found job session dir empty - deleting<BR>
[oceanus:129120] sess_dir_finalize: univ session dir not empty - leaving <BR>
[oceanus:129120] orted: job_state_callback(jobid = 1, state = ORTE_PROC_STATE_TERMINATED)<BR>
[oceanus:129120] sess_dir_finalize: job session dir not empty - leaving<BR>
[oceanus:129120] sess_dir_finalize: found proc session dir empty - deleting <BR>
[oceanus:129120] sess_dir_finalize: found job session dir empty - deleting<BR>
[oceanus:129120] sess_dir_finalize: found univ session dir empty - deleting<BR>
[oceanus:129120] sess_dir_finalize: found top session dir empty - deleting <BR>
[eddie@oceanus:~/home2/mpi_tut]$<BR>
<BR>
&nbsp;<BR>
On 1/18/07, <B>Ralph H Castain</B> &lt;rhc@lanl.gov&gt; wrote: &nbsp;<BR>
</SPAN></FONT><BLOCKQUOTE><FONT FACE="Verdana, Helvetica, Arial"><SPAN STYLE='font-size:12.0px'>Hi Eddie<BR>
<BR>
Open MPI needs to create a temporary file system &#8211; what we call our &quot;session directory&quot; - where it stores things like the shared memory file. From this output, it appears that your /tmp directory is &quot;locked&quot; to root access only. <BR>
<BR>
You have three options for resolving this problem:<BR>
<BR>
(a) you could make /tmp accessible to general users;<BR>
<BR>
(b) you could use the &#8212;tmpdir xxx command line option to point Open MPI at another directory that is accessible to the user (for example, you could use a &quot;tmp&quot; directory under the user's home directory); or <BR>
<BR>
(c) you could set an MCA parameter OMPI_MCA_tmpdir_base to identify a directory we can use instead of /tmp.<BR>
<BR>
&nbsp;If you select options (b) or (c), the only requirement is that this location must be accessible on every node being used. Let me be clear on this: the tmp directory <B>must not</B> be NSF mounted and therefore shared across all nodes. However, each node must be able to access a location of the given name &#8211; that location should be strictly local to each node.<BR>
<BR>
Hope that helps<BR>
Ralph <BR>
<BR>
<BR>
<BR>
<BR>
On 1/17/07 12:25 AM, &quot;eddie168&quot; &lt; eddie168+ompi_user@gmail.com <a href="mailto:eddie168+ompi_user@gmail.com">&lt;mailto:eddie168+ompi_user@gmail.com&gt;</a> &gt; wrote:<BR>
<BR>
</SPAN></FONT><BLOCKQUOTE><FONT FACE="Verdana, Helvetica, Arial"><SPAN STYLE='font-size:12.0px'>Dear all,<BR>
&nbsp;<BR>
I have recently installed OpenMPI 1.1.2 on a OpenSSI cluster running Fedora core 3. I tested a simple hello world mpi program (attached) and it runs ok as root. However, if I run the same program under normal user, it gives the following error: <BR>
&nbsp;<BR>
</SPAN></FONT><SPAN STYLE='font-size:12.0px'><FONT FACE="Courier New">[eddie@oceanus:~/home2/mpi_tut]$ mpirun -np 2 tut01<BR>
[oceanus:125089] mca_common_sm_mmap_init: ftruncate failed with errno=13<BR>
[oceanus:125089] mca_mpool_sm_init: unable to create shared memory mapping ( /tmp/openmpi- sessions-eddie@localhost_0/default-universe/1/shared_mem_pool.localhost)<BR>
-------------------------------------------------------------------------- <BR>
It looks like MPI_INIT failed for some reason; your parallel process is <BR>
likely to abort. &nbsp;There are many reasons that a parallel process can<BR>
fail during MPI_INIT; some of which are due to configuration or environment <BR>
problems. &nbsp;This failure appears to be an internal failure; here's some <BR>
additional information (which may only be relevant to an Open MPI<BR>
developer):<BR>
&nbsp;&nbsp;PML add procs failed<BR>
&nbsp;&nbsp;--&gt; Returned &quot;Out of resource&quot; (-2) instead of &quot;Success&quot; (0)<BR>
-------------------------------------------------------------------------- <BR>
*** An error occurred in MPI_Init<BR>
*** before MPI was initialized<BR>
*** MPI_ERRORS_ARE_FATAL (goodbye)<BR>
[eddie@oceanus:~/home2/mpi_tut]$<BR>
</FONT><FONT FACE="Verdana, Helvetica, Arial"><BR>
Am I need to give certain permission to the user in order to oversubscribe processes?<BR>
<BR>
Thanks in advance,<BR>
<BR>
Eddie.<BR>
<BR>
&nbsp;<BR>
<BR>
<HR ALIGN=CENTER SIZE="3" WIDTH="95%"></FONT><FONT FACE="Monaco, Courier New">_______________________________________________<BR>
users mailing list<BR>
users@open-mpi.org<BR>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><BR>
</FONT></SPAN></BLOCKQUOTE><SPAN STYLE='font-size:12.0px'><FONT FACE="Monaco, Courier New"><BR>
</FONT><FONT FACE="Verdana, Helvetica, Arial"><BR>
_______________________________________________<BR>
users mailing list<BR>
users@open-mpi.org<BR>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><BR>
<BR>
</FONT></SPAN></BLOCKQUOTE><SPAN STYLE='font-size:12.0px'><FONT FACE="Verdana, Helvetica, Arial"><BR>
</FONT></SPAN></BLOCKQUOTE><SPAN STYLE='font-size:12.0px'><FONT FACE="Verdana, Helvetica, Arial"><BR>
<BR>
<HR ALIGN=CENTER SIZE="3" WIDTH="95%"></FONT></SPAN><FONT SIZE="2"><FONT FACE="Monaco, Courier New"><SPAN STYLE='font-size:10.0px'>_______________________________________________<BR>
users mailing list<BR>
users@open-mpi.org<BR>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><BR>
</SPAN></FONT></FONT></BLOCKQUOTE><FONT SIZE="2"><FONT FACE="Monaco, Courier New"><SPAN STYLE='font-size:10.0px'><BR>
</SPAN></FONT></FONT>
</BODY>
</HTML>


