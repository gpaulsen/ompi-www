I&#39;m having a problem trying to use OpenMPI on some multicore machines I have. The code I am running was giving me errors which suggested that MPI was assigning multiple processes to the same core (which I do not want). So, I tried launching my job using the -nooversubscribe option, and I get this error:<br>
<br>bash-3.2$ mpirun -np 2 -nooversubscribe &lt;executable and options here&gt;<br>--------------------------------------------------------------------------<br>There are not enough slots available in the system to satisfy the 2 slots <br>
that were requested by the application:<br>  &lt;executable name&gt;<br><br>Either request fewer slots for your application, or make more slots available<br>for use.<br>--------------------------------------------------------------------------<br>
--------------------------------------------------------------------------<br>A daemon (pid unknown) died unexpectedly on signal 1  while attempting to<br>launch so we are aborting.<br><br>There may be more information reported by the environment (see above).<br>
<br>This may be because the daemon was unable to find all the needed shared<br>libraries on the remote node. You may set your LD_LIBRARY_PATH to have the<br>location of the shared libraries on the remote nodes and this will<br>
automatically be forwarded to the remote nodes.<br>--------------------------------------------------------------------------<br>--------------------------------------------------------------------------<br>mpirun noticed that the job aborted, but has no info as to the process<br>
that caused that situation.<br>--------------------------------------------------------------------------<br>mpirun: clean termination accomplished<br><br>I am just trying to run on the localhost, not on any remote machines. This happens on both my 8 (2*4) core and 24 (4*6) core machines. Relevant info: I am not using any type of scheduler here, although from the searching I&#39;ve done that doesn&#39;t seem like a requirement. The only thing I can think is there must be some type of configuration or option I&#39;m not setting for using on shared memory machines (either at compile or run time), but I can&#39;t find anyone else who has come across this error. Any thoughts?<br>
<br>Thanks,<br><br>Kyle<br>

