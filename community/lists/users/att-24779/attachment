
<HTML><BODY><p>Here it is:<br></p><p><br>$ LD_PRELOAD=/mnt/data/users/dm2/vol3/semenov/_scratch/mxm/mxm-3.0/lib/libmxm.so&nbsp; mpirun&nbsp; -x LD_PRELOAD --mca plm_base_verbose 10 --debug-daemons -np 1 hello_c<br><br>[access1:29064] mca: base: components_register: registering plm components<br>[access1:29064] mca: base: components_register: found loaded component isolated<br>[access1:29064] mca: base: components_register: component isolated has no register or open function<br>[access1:29064] mca: base: components_register: found loaded component rsh<br>[access1:29064] mca: base: components_register: component rsh register function successful<br>[access1:29064] mca: base: components_register: found loaded component slurm<br>[access1:29064] mca: base: components_register: component slurm register function successful<br>[access1:29064] mca: base: components_open: opening plm components<br>[access1:29064] mca: base: components_open: found loaded component isolated<br>[access1:29064] mca: base: components_open: component isolated open function successful<br>[access1:29064] mca: base: components_open: found loaded component rsh<br>[access1:29064] mca: base: components_open: component rsh open function successful<br>[access1:29064] mca: base: components_open: found loaded component slurm<br>[access1:29064] mca: base: components_open: component slurm open function successful<br>[access1:29064] mca:base:select: Auto-selecting plm components<br>[access1:29064] mca:base:select:(&nbsp; plm) Querying component [isolated]<br>[access1:29064] mca:base:select:(&nbsp; plm) Query of component [isolated] set priority to 0<br>[access1:29064] mca:base:select:(&nbsp; plm) Querying component [rsh]<br>[access1:29064] mca:base:select:(&nbsp; plm) Query of component [rsh] set priority to 10<br>[access1:29064] mca:base:select:(&nbsp; plm) Querying component [slurm]<br>[access1:29064] mca:base:select:(&nbsp; plm) Query of component [slurm] set priority to 75<br>[access1:29064] mca:base:select:(&nbsp; plm) Selected component [slurm]<br>[access1:29064] mca: base: close: component isolated closed<br>[access1:29064] mca: base: close: unloading component isolated<br>[access1:29064] mca: base: close: component rsh closed<br>[access1:29064] mca: base: close: unloading component rsh<br>Daemon was launched on node1-128-17 - beginning to initialize<br>Daemon was launched on node1-128-18 - beginning to initialize<br>Daemon [[63607,0],2] checking in as pid 24538 on host node1-128-18<br>[node1-128-18:24538] [[63607,0],2] orted: up and running - waiting for commands!<br>Daemon [[63607,0],1] checking in as pid 17192 on host node1-128-17<br>[node1-128-17:17192] [[63607,0],1] orted: up and running - waiting for commands!<br>srun: error: node1-128-18: task 1: Exited with exit code 1<br>srun: Terminating job step 645191.1<br>srun: error: node1-128-17: task 0: Exited with exit code 1<br>--------------------------------------------------------------------------<br>An ORTE daemon has unexpectedly failed after launch and before<br>communicating back to mpirun. This could be caused by a number<br>of factors, including an inability to create a connection back<br>to mpirun due to a lack of common network interfaces and/or no<br>route found between them. Please check network connectivity<br>(including firewalls and network routing requirements).<br>--------------------------------------------------------------------------<br>[access1:29064] [[63607,0],0] orted_cmd: received halt_vm cmd<br>[access1:29064] mca: base: close: component slurm closed<br>[access1:29064] mca: base: close: unloading component slurm<br><br><br>Wed, 16 Jul 2014 14:20:33 +0300 от Mike Dubman &lt;miked@dev.mellanox.co.il&gt;:<br>
</p><blockquote style="border-left:1px solid #0857A6; margin:10px; padding:0 0 0 10px;">
	<div id="">
	



    









	
	


	
	
	
	
	

	
	

	
	



<div class="js-helper js-readmsg-msg">
	<style type="text/css"></style>
 	<div>
		<base target="_self" href="https://e.mail.ru/">
		
			<div id="style_14055096360000000171_BODY"><div dir="ltr">please add following flags to mpirun "--mca plm_base_verbose 10 --debug-daemons" and attach output.<div>Thx</div></div><div><br><br><div>On Wed, Jul 16, 2014 at 11:12 AM, Timur Ismagilov <span dir="ltr">&lt;<a href="//e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span> wrote:<br>
<blockquote style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div><p>Hello!<br>I have&nbsp;Open MPI v1.9a1r32142 and slurm 2.5.6.<br><br>I can not use mpirun after salloc:<br><br>$salloc -N2 --exclusive -p test -J ompi<br>$LD_PRELOAD=/mnt/data/users/dm2/vol3/semenov/_scratch/mxm/mxm-3.0/lib/libmxm.so mpirun -np 1 hello_c<br>
-----------------------------------------------------------------------------------------------------<br>An ORTE daemon has unexpectedly failed after launch and before<br>communicating back to mpirun. This could be caused by a number<br>
of factors, including an inability to create a connection back<br>to mpirun due to a lack of common network interfaces and/or no<br>route found between them. Please check network connectivity<br>(including firewalls and network routing requirements).<br>
 ------------------------------------------------------------------------------------------------------<br>But if i use mpirun in sbutch script it looks correct:<br>$cat ompi_mxm3.0<br>#!/bin/sh<br>LD_PRELOAD=/mnt/data/users/dm2/vol3/semenov/_scratch/mxm/mxm-3.0/lib/libmxm.so&nbsp; mpirun&nbsp; -x LD_PRELOAD -x MXM_SHM_KCOPY_MODE=off --map-by slot:pe=8 "$@"<br>
<br>$sbatch -N2&nbsp; --exclusive -p test -J ompi&nbsp; ompi_mxm3.0 ./hello_c<br>Submitted batch job 645039<br>$cat slurm-645039.out <br>[warn] Epoll ADD(1) on fd 0 failed.&nbsp; Old events were 0; read change was 1 (add); write change was 0 (none): Operation not permitted<br>
[warn] Epoll ADD(4) on fd 1 failed.&nbsp; Old events were 0; read change was 0 (none); write change was 1 (add): Operation not permitted<br>Hello, world, I am 0 of 2, (Open MPI v1.9a1, package: Open MPI semenov@compiler-2 Distribution, ident: 1.9a1r32142, repo rev: r32142, Jul 04, 2014 (nightly snapshot tarball), 146)<br>
Hello, world, I am 1 of 2, (Open MPI v1.9a1, package: Open MPI semenov@compiler-2 Distribution, ident: 1.9a1r32142, repo rev: r32142, Jul 04, 2014 (nightly snapshot tarball), 146)<br><br>Regards,<br>Timur</p></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="//e.mail.ru/compose/?mailto=mailto%3ausers@open%2dmpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/07/24777.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/07/24777.php</a><br></blockquote></div><br></div>

</div>
			
		
		<base target="_self" href="https://e.mail.ru/">
	</div>

	
</div>


</div>
</blockquote>
<br>
<br><br></BODY></HTML>
