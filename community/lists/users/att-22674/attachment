<div dir="ltr"><div><br></div><div>I&#39;m new to OPEN MPI and have a question in regards to the error I&#39;m seeing after compiling the OFED stack to facilitate RDMA and OpenMPI and specified the install path of OFED stack and installed Intel MPI Benchmark.  I was able to run tcp but when running openib we could not run succesfully we are see the error below: OFED version 3.5</div>
<div><br></div><div>[root@dhcp-8-168 imb]# mpirun --mca btl openib,sm,self --mca btl_openib_cpc_include rdmacm --mca btl_openib_if_include p2p1  --mca btl_openib_verbose 2 -np 2 -hostfile hosts ./3.2.4/src/IMB-MPI1 -npmin 2 -iter 10 PingPong</div>
<div>--------------------------------------------------------------------------</div><div>WARNING: One or more nonexistent OpenFabrics devices/ports were</div><div>specified:</div><div><br></div><div>  Host:                 dhcp-8-168</div>
<div>  MCA parameter:        mca_btl_if_include</div><div>  Nonexistent entities: p2p1</div><div><br></div><div>These entities will be ignored.  You can disable this warning by</div><div>setting the btl_openib_warn_nonexistent_if MCA parameter to 0.</div>
<div>--------------------------------------------------------------------------</div><div>--------------------------------------------------------------------------</div><div>At least one pair of MPI processes are unable to reach each other for</div>
<div>MPI communications.  This means that no Open MPI device has indicated</div><div>that it can be used to communicate between these processes.  This is</div><div>an error; Open MPI requires that all MPI processes be able to reach</div>
<div>each other.  This error can sometimes be the result of forgetting to</div><div>specify the &quot;self&quot; BTL.</div><div><br></div><div>  Process 1 ([[60771,1],0]) is on host: dhcp-8-168</div><div>  Process 2 ([[60771,1],1]) is on host: 169</div>
<div>  BTLs attempted: self sm</div><div><br></div><div>Your MPI job is now going to abort; sorry.</div><div>--------------------------------------------------------------------------</div><div>--------------------------------------------------------------------------</div>
<div>MPI_INIT has failed because at least one MPI process is unreachable</div><div>from another.  This *usually* means that an underlying communication</div><div>plugin -- such as a BTL or an MTL -- has either not loaded or not</div>
<div>allowed itself to be used.  Your MPI job will now abort.</div><div><br></div><div>You may wish to try to narrow down the problem;</div><div><br></div><div> * Check the output of ompi_info to see which BTL/MTL plugins are</div>
<div>   available.</div><div> * Run your application with MPI_THREAD_SINGLE.</div><div> * Set the MCA parameter btl_base_verbose to 100 (or mtl_base_verbose,</div><div>   if using MTL-based communications) to see exactly which</div>
<div>   communication plugins were considered and/or discarded.</div><div>--------------------------------------------------------------------------</div><div>[dhcp-8-168:3503] *** An error occurred in MPI_Init</div><div>
[dhcp-8-168:3503] *** on a NULL communicator</div><div>[dhcp-8-168:3503] *** Unknown error</div><div>[dhcp-8-168:3503] *** MPI_ERRORS_ARE_FATAL: your MPI job will now abort</div><div>--------------------------------------------------------------------------</div>
<div>An MPI process is aborting at a time when it cannot guarantee that all</div><div>of its peer processes in the job will be killed properly.  You should</div><div>double check that everything has shut down cleanly.</div>
<div><br></div><div>  Reason:     Before MPI_INIT completed</div><div>  Local host: dhcp-8-168</div><div>  PID:        3503</div><div>--------------------------------------------------------------------------</div><div>--------------------------------------------------------------------------</div>
<div>mpirun has exited due to process rank 0 with PID 3503 on</div><div>node dhcp-8-168 exiting improperly. There are two reasons this could occur:</div><div><br></div><div>1. this process did not call &quot;init&quot; before exiting, but others in</div>
<div>the job did. This can cause a job to hang indefinitely while it waits</div><div>for all processes to call &quot;init&quot;. By rule, if one process calls &quot;init&quot;,</div><div>then ALL processes must call &quot;init&quot; prior to termination.</div>
<div><br></div><div>2. this process called &quot;init&quot;, but exited without calling &quot;finalize&quot;.</div><div>By rule, all processes that call &quot;init&quot; MUST call &quot;finalize&quot; prior to</div><div>exiting or it will be considered an &quot;abnormal termination&quot;</div>
<div><br></div><div>This may have caused other processes in the application to be</div><div>terminated by signals sent by mpirun (as reported here).</div><div>--------------------------------------------------------------------------</div>
<div>[dhcp-8-168:03501] 1 more process has sent help message help-mpi-btl-openib.txt / nonexistent port</div><div>[dhcp-8-168:03501] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages</div>
<div>[dhcp-8-168:03501] 1 more process has sent help message help-mca-bml-r2.txt / unreachable proc</div><div>[dhcp-8-168:03501] 1 more process has sent help message help-mpi-runtime / mpi_init:startup:pml-add-procs-fail</div>
<div>[dhcp-8-168:03501] 1 more process has sent help message help-mpi-errors.txt / mpi_errors_are_fatal unknown handle</div><div>[dhcp-8-168:03501] 1 more process has sent help message help-mpi-runtime.txt / ompi mpi abort:cannot guarantee all killed</div>
<div><br></div><div>--Tester</div><div><br></div></div>

