<table cellspacing="0" cellpadding="0" border="0" ><tr><td valign="top" style="font: inherit;">Thanx for the reply.&nbsp; <br><br>I guess I should go back a step:&nbsp; I had used the openmpi version on my system which is simply:<br>"mpirun -machinefile $PBS_NODEFILE -np $NPROCS ${CODE} &gt;/ul/tedhyu/fuelcell/HOH/test/HH.out"<br><br>This did not work because I was just getting a blank output.<br><br>I tried this older version because at least i was getting an output.<br>"/opt/mpich-1.2.5.10-ch_p4-gcc/bin/mpirun -machinefile $PBS_NODEFILE -np<br><pre>$NPROCS ${CODE} &gt;/ul/tedhyu/fuelcell/HOH/test/HH.out"</pre>
I think this older version is failing me for whatever reason.&nbsp; Do you have any clue?&nbsp; I read somewhere that new versions of mpirun adds extra commandline arguments to the end of the line.&nbsp; Therefore the newer version of mpirun may be not be giving an output because it sees all extra commandline arguments after my output file &gt;/ul/tedhyu/fuelcell/HOH/test/HH.out<br><br>This is where I'm reading that there are extra commandline arguments for a version of mpirun:<br>https://lists.sdsc.edu/pipermail/npaci-rocks-discussion/2008-February/029333.html<br><br>Again, I'm new at this, and I'm just guessing.&nbsp; Any ideas of where to turn would be helpful!<br><br>Ted<br><br>--- On <b>Thu, 2/5/09, doriankrause <i>&lt;doriankrause@web.de&gt;</i></b> wrote:<br><blockquote style="border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;">From: doriankrause &lt;doriankrause@web.de&gt;<br>Subject: Re: [OMPI users] Global
 Communicator<br>To: tedhyu@wag.caltech.edu, "Open MPI Users" &lt;users@open-mpi.org&gt;<br>Date: Thursday, February 5, 2009, 11:14 PM<br><br><pre>Ted Yu wrote:<br>&gt; I'm trying to run a job based on openmpi.  For some reason, the<br>program and the global communicator are not in sync and it reads that there is<br>only one processors, whereas, there should be 2 or more.  Any advice on where to<br>look?  Here is my PBS script.  Thanx!<br>&gt;<br>&gt; PBS SCRIPT:<br>&gt; #!/bin/sh<br>&gt; ### Set the job name<br>&gt; #PBS -N HH<br>&gt; ### Declare myprogram non-rerunable<br>&gt; #PBS -r n<br>&gt; ### Combine standard error and standard out to one file.<br>&gt; #PBS -j oe<br>&gt; ### Have PBS mail you results<br>&gt; #PBS -m ae<br>&gt; #PBS -M tedhyu@wag.caltech.edu<br>&gt; ### Set the queue name, given to you when you get a reservation.<br>&gt; #PBS -q workq<br>&gt; ### Specify the number of cpus for your job.  This example will run on 32<br>cpus<br>&gt;
 ### using 8 nodes with 4 processes per node.<br>&gt; #PBS -l nodes=1:ppn=2,walltime=70:00:00<br>&gt; # Switch to the working directory; by default PBS launches processes from<br>your home directory.<br>&gt; # Jobs should only be run from /home, /project, or /work; PBS returns<br>results via NFS.<br>&gt; PBS_O_WORKDIR=/temp1/tedhyu/HH<br>&gt; export<br>CODE=/project/source/seqquest/seqquest_source_v261j/hive_CentOS4.5_parallel/build_261j/quest_ompi.x<br>&gt;<br>&gt; echo Working directory is $PBS_O_WORKDIR<br>&gt; mkdir -p $PBS_O_WORKDIR<br>&gt; cd $PBS_O_WORKDIR<br>&gt; rm -rf *<br>&gt; cp /ul/tedhyu/fuelcell/HOH/test/HH.in ./lcao.in<br>&gt; cp /ul/tedhyu/atom_pbe/* .<br>&gt; echo Running on host `hostname`<br>&gt; echo Time is `date`<br>&gt; echo Directory is `pwd`<br>&gt; echo This jobs runs on the following processors:<br>&gt; echo `cat $PBS_NODEFILE`<br>&gt; Number=`wc -l $PBS_NODEFILE | awk '{print $1}'`<br>&gt;<br>&gt; export Number<br>&gt; echo
 ${Number}<br>&gt; # Define number of processors<br>&gt; NPROCS=`wc -l &lt; $PBS_NODEFILE`<br>&gt; # And the number or hosts<br>&gt; NHOSTS=`cat $PBS_NODEFILE|uniq|wc -l`<br>&gt; echo This job has allocated $NPROCS cpus<br>&gt; echo NHOSTS<br>&gt; #mpirun  -machinefile $PBS_NODEFILE  ${CODE}<br>&gt;/ul/tedhyu/fuelcell/HOH/test/HH.out<br>&gt; #mpiexec -np 2  ${CODE} &gt;/ul/tedhyu/fuelcell/HOH/test/HH.out<br>&gt; /opt/mpich-1.2.5.10-ch_p4-gcc/bin/mpirun -machinefile $PBS_NODEFILE -np<br>$NPROCS ${CODE} &gt;/ul/tedhyu/fuelcell/HOH/test/HH.out<br>&gt; cd ..<br>&gt; rm -rf HH<br>&gt;<br>&gt;<br>&gt;   <br><br>Please note, that you are mixing Open MPI (API/Library) with MPICH <br>(mpirun). This is a mistake I like to make, too. If you use<br>the ompi mpiexec program, it probably works.<br><br>Dorian<br><br>&gt;<br>&gt;       <br>&gt;   <br>&gt; ------------------------------------------------------------------------<br>&gt;<br>&gt;
 _______________________________________________<br>&gt; users mailing list<br>&gt; users@open-mpi.org<br>&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br><br></pre></blockquote></td></tr></table><br>



      
