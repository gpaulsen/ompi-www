<div dir="ltr"><div><div><div>OMP_NUM_THREADS=1 mpiexec -n 1 gnu_openmpi_a/one_c_prof.exe : 113 iterations<br>OMP_NUM_THREADS=6 mpiexec -n 1 --map-by node:PE=6 : 639 iterations<br>OMP_NUM_THREADS=6 mpiexec -n 2 --map-by node:PE=6 : 639 iterations<br>OMP_NUM_THREADS=12 mpiexec -n 1 --map-by node:PE=12 : 1000 iterations<br>OMP_NUM_THREADS=12 mpiexec -n 2 --use-hwthread-cpus --map-by node:PE=12 : 646 iterations<br><br></div>that&#39;s looking better, with limited gain for 1 process on 2 chips. Thanks. I am testing Allineas profiler, and our goal is to point out bad practice, so I need to run all sorts of pathological cases. Now to see what our software thinks<br><br></div>Thanks for your help<br><br></div>John<br></div><div class="gmail_extra"><br><div class="gmail_quote">On 8 December 2014 at 15:57, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Thanks for sending that lstopo output - helped clarify things for me. I think I now understand the issue. Mostly a problem of my being rather dense when reading your earlier note.<br>
<br>
Try using —map-by node:PE=N to your cmd line. I think the problem is that we default to —map-by numa if you just give cpus-per-proc and no mapping directive as we know that having threads that span multiple numa regions is bad for performance<br>
<div><div class="h5"><br>
<br>
&gt; On Dec 5, 2014, at 9:07 AM, John Bray &lt;<a href="mailto:jbray@allinea.com">jbray@allinea.com</a>&gt; wrote:<br>
&gt;<br>
&gt; Hi Ralph<br>
&gt;<br>
&gt; I have a motherboard with 2 X6580 chips, each with 6 cores 2 way hyperthreading, so /proc/cpuinfo reports 24 cores<br>
&gt;<br>
&gt; Doing a pure compute OpenMP loop where I&#39;d expect the number of iterations in 10s to rise with number of threads<br>
&gt; with gnu and mpich<br>
&gt; OMP_NUM_THREADS=1 -n 1 : 112 iterations<br>
&gt; OMP_NUM_THREADS=2 -n 1 : 224 iterations<br>
&gt; OMP_NUM_THREADS=6 -n 1 : 644 iterations<br>
&gt; OMP_NUM_THREADS=12 -n 1 : 1287 iterations<br>
&gt; OMP_NUM_THREADS=22 -n 1 : 1182 iterations<br>
&gt; OMP_NUM_THREADS=24 -n 1 : 454 iterations<br>
&gt;<br>
&gt; which shows that mpich is spreading across the cores, but hyperthreading is not useful, and using the whole node counterproductive<br>
&gt;<br>
&gt; with gnu and openmpi 1.8.3<br>
&gt; OMP_NUM_THREADS=1 mpiexec -n 1 : 112<br>
&gt; OMP_NUM_THREADS=2 mpiexec -n 1 : 113<br>
&gt; which suggests you aren&#39;t allowing the threads to spread across cores<br>
&gt;<br>
&gt; adding --cpus-per-node I gain access to the resources on one chip<br>
&gt;<br>
&gt; OMP_NUM_THREADS=1 mpiexec --cpus-per-proc 1 -n 1 : 112<br>
&gt; OMP_NUM_THREADS=2 mpiexec --cpus-per-proc 2 -n 1 : 224<br>
&gt; OMP_NUM_THREADS=6 mpiexec --cpus-per-proc 2 -n 1 : 644<br>
&gt; then<br>
&gt; OMP_NUM_THREADS=12 mpiexec --cpus-per-proc 12 -n 1<br>
&gt;<br>
&gt; A request for multiple cpus-per-proc was given, but a directive<br>
&gt; was also give to map to an object level that has less cpus than<br>
&gt; requested ones:<br>
&gt;<br>
&gt;   #cpus-per-proc:  12<br>
&gt;   number of cpus:  6<br>
&gt;   map-by:          BYNUMA<br>
&gt;<br>
&gt; So you aren&#39;t happy using both chips for one process<br>
&gt;<br>
&gt; OMP_NUM_THREADS=1 mpiexec -n 1 --cpus-per-proc 1 --use-hwthread-cpus : 112<br>
&gt; OMP_NUM_THREADS=2 mpiexec -n 1 --cpus-per-proc 2 --use-hwthread-cpus : 112<br>
&gt; OMP_NUM_THREADS=4 mpiexec -n 1 --cpus-per-proc 4 --use-hwthread-cpus : 224<br>
&gt; OMP_NUM_THREADS=6 mpiexec -n 1 --cpus-per-proc 6 --use-hwthread-cpus : 324<br>
&gt; OMP_NUM_THREADS=6 mpiexec -n 1 --cpus-per-proc 12 --use-hwthread-cpus : 631<br>
&gt; OMP_NUM_THREADS=12 mpiexec -n 1 --cpus-per-proc 12 --use-hwthread-cpus : 647<br>
&gt;<br>
&gt; OMP_NUM_THREADS=24 mpiexec -n 1 --cpus-per-proc 12 --use-hwthread-cpus<br>
&gt;<br>
&gt; A request for multiple cpus-per-proc was given, but a directive<br>
&gt; was also give to map to an object level that has less cpus than<br>
&gt; requested ones:<br>
&gt;<br>
&gt;   #cpus-per-proc:  24<br>
&gt;   number of cpus:  12<br>
&gt;   map-by:          BYNUMA<br>
&gt;<br>
&gt; OMP_NUM_THREADS=1 mpiexec -n 1 --cpus-per-proc 2 --use-hwthread-cpus : 112<br>
&gt; OMP_NUM_THREADS=2 mpiexec -n 1 --cpus-per-proc 4 --use-hwthread-cpus : 224<br>
&gt; OMP_NUM_THREADS=6 mpiexec -n 1 --cpus-per-proc 12 --use-hwthread-cpus :: 644<br>
&gt;<br>
&gt; OMP_NUM_THREADS=12 mpiexec -n 1 --cpus-per-proc 24 --use-hwthread-cpus :: 644<br>
&gt;<br>
&gt; A request for multiple cpus-per-proc was given, but a directive<br>
&gt; was also give to map to an object level that has less cpus than<br>
&gt; requested ones:<br>
&gt;<br>
&gt;   #cpus-per-proc:  24<br>
&gt;   number of cpus:  12<br>
&gt;   map-by:          BYNUMA<br>
&gt;<br>
&gt; So it seems that --use-hwthread-cpus means that --cpus-per-proc changes from physical cores to hyperthreaded cores, but I can&#39;t get both chips working on the problem in way mpich can<br>
&gt;<br>
&gt; John<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
</div></div><span class="">&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</span>&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/12/25919.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/12/25919.php</a><br>
<span class=""><br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</span>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/12/25927.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/12/25927.php</a></blockquote></div><br></div>

