<div dir="ltr">Hi,<div>I would suggest use MXM (part of mofed, can be downloaded as standalone rpm from <a href="http://mellanox.com/products/mxm">http://mellanox.com/products/mxm</a> for ofed)</div><div><br></div><div>It uses UD (constant memory footprint) and should provide good performance.</div>
<div>The next MXM v2.0 will support RC and DC (reliable UD) as well.</div><div><br></div><div>Once mxm is installed from rpm (or extracted elsewhere from rpm-&gt;tarball) - you can point ompi configure with &quot;--with-mxm=/path/to/mxm&quot;)</div>
<div>Regards</div><div>M</div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Fri, Jul 5, 2013 at 10:33 PM, Ben <span dir="ltr">&lt;<a href="mailto:Benjamin.M.Auer@nasa.gov" target="_blank">Benjamin.M.Auer@nasa.gov</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">I&#39;m part of a team that maintains a global climate model running under mpi. Recently we have been trying it out with different mpi stacks<br>

at high resolution/processor counts.<br>
At one point in the code there is a large number of mpi_isends/mpi_recv (tens to hundreds of thousands) when data distributed across all mpi processes must be collective on a particular processor or processors be transformed to a new resolution before writing. At first the model was<br>

crashing with a message:<br>
&quot;A process failed to create a queue pair. This usually means either the device has run out of queue pairs (too many connections) or there are insufficient resources available to allocate a queue pair (out of memory). The latter can happen if either 1) insufficient memory is available, or 2) no more physical memory can be registered with the device.&quot;<br>

when it hit the part of code with the send/receives. Watching the node memory in an xterm I could see the memory skyrocket and fill the node.<br>
<br>
Somewhere we found a suggestion try using the xrc queues (<a href="http://www.open-mpi.org/faq/?category=openfabrics#ib-xrc" target="_blank">http://www.open-mpi.org/faq/?<u></u>category=openfabrics#ib-xrc</a>) to get around this problem and indeed running with<br>

<br>
setenv OMPI_MCA_btl_openib_receive_<u></u>queues &quot;X,128,256,192,128:X,2048,256,<u></u>128,32:X,12288,256,128,32:X,<u></u>65536,256,128,32&quot;<br>
mpirun --bind-to-core -np numproc ./app<br>
<br>
allowed the model to successfully run. It still seems to use a large amount of memory when it writes (on the order of several Gb). Does anyone have any  suggestions on how to perhaps tweak the settings to help with memory use.<span class="HOEnZb"><font color="#888888"><br>

<br>
-- <br>
Ben Auer, PhD   SSAI, Scientific Programmer/Analyst<br>
NASA GSFC,  Global Modeling and Assimilation Office<br>
Code 610.1, 8800 Greenbelt Rd, Greenbelt, MD  20771<br>
Phone: <a href="tel:301-286-9176" value="+13012869176" target="_blank">301-286-9176</a>               Fax: <a href="tel:301-614-6246" value="+13016146246" target="_blank">301-614-6246</a><br>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
</font></span></blockquote></div><br></div>

