<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">Hi,<div><br></div><div>QP = Queue Pair</div><div><br></div><div>Here are a couple of nice FAQ entries that I find useful.</div><div><br></div><div><a href="http://www.open-mpi.org/faq/?category=openfabrics">http://www.open-mpi.org/faq/?category=openfabrics</a></div><div><br></div><div>And videos:</div><div><br></div><div><a href="http://www.open-mpi.org/video/?category=openfabrics">http://www.open-mpi.org/video/?category=openfabrics</a></div><div><br></div><div><br></div><div>--</div><div>Samuel K. Gutierrez<br>Los Alamos National Laboratory</div><div><a href="http://www.open-mpi.org/faq/?category=openfabrics"></a><br><div><div>On Jun 23, 2011, at 8:22 AM, Mathieu Gontier wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite">
<div bgcolor="#ffffff" text="#000000">
    Hi,<br>
    <br>
    Thanks for your answer. It makes sense. <br>
    Sorry if my question seems silly, but what does QP mean? It is
    difficult to read the FAQ without knowing that!<br>
    <br>
    Thanks. <br>
    <br>
    On 06/23/2011 04:00 PM, Ralph Castain wrote:
    <blockquote cite="mid:C4FF029C-FA78-4DDF-B5A8-9E92AEA16279@open-mpi.org" type="cite">One possibility: if you increase the number of
      processes in the job, and they all interconnect, then the IB
      interface can (I believe) run out of memory at some point. IIRC,
      the answer was to reduce the size of the QPs so that you could
      support a larger number of them.
      <div><br>
      </div>
      <div>You should find info about controlling QP size in the IB FAQ
        area on the OMPI web site, I believe.</div>
      <div><br>
        <div>
          <div>On Jun 23, 2011, at 7:56 AM, Mathieu Gontier wrote:</div>
          <br class="Apple-interchange-newline">
          <blockquote type="cite">
            <div bgcolor="#ffffff" text="#000000"> Hello, <br>
              <br>
              Thank for the answer.<br>
              I am testing with OpenMPI-1.4.3: my computation is
              queuing. But I did not read anything obvious related to my
              issue. Have you read something which could solve it? <br>
              I am going to submit my computation with <tt>--mca
                mpi_leave_pinned 0</tt>, but do you have any idea how it
              affect the performance? Compared to using Ethernet? <br>
              <br>
              Many thanks for your support. <br>
              <br>
              On 06/23/2011 03:01 PM, Josh Hursey wrote:
              <blockquote cite="mid:BANLkTin3fi=TjH4nDqbTtxL3ds9VAaBJig@mail.gmail.com" type="cite">
                <pre wrap="">I wonder if this is related to memory pinning. Can you try turning off
the leave pinned, and see if the problem persists (this may affect
performance, but should avoid the crash):
  mpirun ... --mca mpi_leave_pinned 0 ...

Also it looks like Smoky has a slightly newer version of the 1.4
branch that you should try to switch to if you can. The following
command will show you all of the available installs on that machine:
  shell$ module avail ompi

For a list of supported compilers for that version try the 'show' option:
shell$ module show ompi/1.4.3
-------------------------------------------------------------------
/sw/smoky/modulefiles-centos/ompi/1.4.3:

module-whatis	 This module configures your environment to make Open
MPI 1.4.3 available.
Supported Compilers:
     pathscale/3.2.99
     pathscale/3.2
     pgi/10.9
     pgi/10.4
     intel/11.1.072
     gcc/4.4.4
     gcc/4.4.3
-------------------------------------------------------------------

Let me know if that helps.

Josh


On Wed, Jun 22, 2011 at 4:16 AM, Mathieu Gontier
<a moz-do-not-send="true" class="moz-txt-link-rfc2396E" href="mailto:mathieu.gontier@gmail.com">&lt;mathieu.gontier@gmail.com&gt;</a> wrote:
</pre>
                <blockquote type="cite">
                  <pre wrap="">Dear all,

First of all, all my apologies because I post this message to both the bug
and user mailing list. But for the moment, I do not know if it is a bug!

I am running a CFD structured flow solver at ORNL, and I have an access to a
small cluster (Smoky) using OpenMPI-1.4.2 with Infiniband by default.
Recently we increased the size of our models, and since that time we have
run into many infiniband related problems.&nbsp; The most serious problem is a
hard crash with the following error message:

[smoky45][[60998,1],32][/sw/sources/ompi/1.4.2/ompi/mca/btl/openib/connect/btl_openib_connect_oob.c:464:qp_create_one]
error creating qp errno says Cannot allocate memory

If we force the solver to use ethernet (mpirun -mca btl ^openib) the
computations works correctly, although very slowly (a single iteration take
ages). Do you have any idea what could be causing these problems?

If it is due to a bug or a limitation into OpenMPI, do you think the version
1.4.3, the coming 1.4.4 or any 1.5 version could solve the problem? I read
the release notes, but I did not read any obvious patch which could fix my
problem. The system administrator is ready to compile a new package for us,
but I do not want to ask to install to many of them.

Thanks.
--

Mathieu Gontier
skype: mathieu_gontier
_______________________________________________
users mailing list
<a moz-do-not-send="true" class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
<a moz-do-not-send="true" class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>

</pre>
                </blockquote>
                <pre wrap=""></pre>
              </blockquote>
              <br>
              <div class="moz-signature">-- <br>
                <i><small><br>
                    Mathieu Gontier <br>
                    skype: mathieu_gontier </small></i></div>
            </div>
            _______________________________________________<br>
            users mailing list<br>
            <a moz-do-not-send="true" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
            <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
        </div>
        <br>
      </div>
      <pre wrap=""><fieldset class="mimeAttachmentHeader"></fieldset>
_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></pre>
    </blockquote>
    <br>
    <div class="moz-signature">-- <br>
      <i><small><br>
          Mathieu Gontier
          <br>
          skype: mathieu_gontier
        </small></i></div>
  </div>

_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div><br><br><div><span class="Apple-style-span" style="border-collapse: separate; color: rgb(0, 0, 0); font-family: Helvetica; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; "><br><br></span>
</div>
<br></body></html>
