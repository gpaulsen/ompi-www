Ronald,<div><br></div><div>first, can you make sure tm was built ?</div><div>the easiest way us to</div><div>configure --with-tm ...</div><div>it will crash if tm is not found</div><div>if pbs/torque is not installed in a standard location, then you have to</div><div>configure --with-tm=&lt;dir&gt;</div><div><br></div><div>then you can omit -hostfile from your mpirun command line</div><div><br></div>hpl is known to scale, assuming the data is big enough, you use an optimized blas, and the right number of openmp threads<div>(e.g. if you run 8 tasks per node, the you can have up to 2 openmp threads, but if you use 8 or 16 threads, then performance will be worst)</div><div>first run xhpl one node, and when you get 80% of the peak performance, then you can run on two nodes.</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<br><div><br>On Wednesday, March 23, 2016, Ronald Cohen &lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;recohen3@gmail.com&#39;);" target="_blank">recohen3@gmail.com</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">The configure line was simply:<br>
<br>
 ./configure --prefix=/home/rcohen<br>
<br>
when I run:<br>
<br>
mpirun --mca btl self,vader,openib ...<br>
<br>
I get the same lousy results: 1.5 GFLOPS<br>
<br>
The output of the grep is:<br>
<br>
Cpus_allowed_list:      0-7<br>
Cpus_allowed_list:      8-15<br>
Cpus_allowed_list:      0-7<br>
Cpus_allowed_list:      8-15<br>
Cpus_allowed_list:      0-7<br>
Cpus_allowed_list:      8-15<br>
Cpus_allowed_list:      0-7<br>
Cpus_allowed_list:      8-15<br>
Cpus_allowed_list:      0-7<br>
Cpus_allowed_list:      8-15<br>
Cpus_allowed_list:      0-7<br>
Cpus_allowed_list:      8-15<br>
Cpus_allowed_list:      0-7<br>
Cpus_allowed_list:      8-15<br>
Cpus_allowed_list:      0-7<br>
Cpus_allowed_list:      8-15<br>
<br>
<br>
linpack *HPL) certainly is known to scale fine.<br>
<br>
I am running a standard benchmark--HPL--linpack.<br>
<br>
I think it is not the compiler, but I could try that.<br>
<br>
Ron<br>
<br>
<br>
<br>
<br>
---<br>
Ron Cohen<br>
<a>recohen3@gmail.com</a><br>
skypename: ronaldcohen<br>
twitter: @recohen3<br>
<br>
<br>
On Wed, Mar 23, 2016 at 9:32 AM, Gilles Gouaillardet<br>
&lt;<a>gilles.gouaillardet@gmail.com</a>&gt; wrote:<br>
&gt; Ronald,<br>
&gt;<br>
&gt; the fix I mentioned landed into the v1.10 branch<br>
&gt; <a href="https://github.com/open-mpi/ompi-release/commit/c376994b81030cfa380c29d5b8f60c3e53d3df62" target="_blank">https://github.com/open-mpi/ompi-release/commit/c376994b81030cfa380c29d5b8f60c3e53d3df62</a><br>
&gt;<br>
&gt; can you please post your configure command line ?<br>
&gt;<br>
&gt; you can also try to<br>
&gt; mpirun --mca btl self,vader,openib ...<br>
&gt; to make sure your run will abort instead of falling back to tcp<br>
&gt;<br>
&gt; then you can<br>
&gt; mpirun ... grep Cpus_allowed_list /proc/self/status<br>
&gt; to confirm your tasks do not end up bound to the same cores when running on<br>
&gt; two nodes.<br>
&gt;<br>
&gt; is your application known to scale on infiniband network ?<br>
&gt; or did you naively hope it would scale ?<br>
&gt;<br>
&gt; at first, I recommend you run standard benchmark to make sure you get the<br>
&gt; performance you expect from your infiniband network<br>
&gt; (for example IMB or OSU benchmark)<br>
&gt; and run this test in the same environment than your app (e.g. via a batch<br>
&gt; manager if applicable)<br>
&gt;<br>
&gt; if you do not get the performance you expect, then I suggest you try the<br>
&gt; stock gcc compiler shipped with your distro and see if it helps.<br>
&gt;<br>
&gt; Cheers,<br>
&gt;<br>
&gt; Gilles<br>
&gt;<br>
&gt; On Wednesday, March 23, 2016, Ronald Cohen &lt;<a>recohen3@gmail.com</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; Thank  you! Here are the answers:<br>
&gt;&gt;<br>
&gt;&gt; I did not try a previous release of gcc.<br>
&gt;&gt; I built from a tarball.<br>
&gt;&gt; What should I do about the iirc issue--how should I check?<br>
&gt;&gt; Are there any flags I should be using for infiniband? Is this a<br>
&gt;&gt; problem with latency?<br>
&gt;&gt;<br>
&gt;&gt; Ron<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; ---<br>
&gt;&gt; Ron Cohen<br>
&gt;&gt; <a>recohen3@gmail.com</a><br>
&gt;&gt; skypename: ronaldcohen<br>
&gt;&gt; twitter: @recohen3<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; On Wed, Mar 23, 2016 at 8:13 AM, Gilles Gouaillardet<br>
&gt;&gt; &lt;<a>gilles.gouaillardet@gmail.com</a>&gt; wrote:<br>
&gt;&gt; &gt; Ronald,<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; did you try to build openmpi with a previous gcc release ?<br>
&gt;&gt; &gt; if yes, what about the performance ?<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; did you build openmpi from a tarball or from git ?<br>
&gt;&gt; &gt; if from git and without VPATH, then you need to<br>
&gt;&gt; &gt; configure with --disable-debug<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; iirc, one issue was identified previously<br>
&gt;&gt; &gt; (gcc optimization that prevents the memory wrapper from behaving as<br>
&gt;&gt; &gt; expected) and I am not sure the fix landed in v1.10 branch nor master<br>
&gt;&gt; &gt; ...<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; thanks for the info about gcc 6.0.0<br>
&gt;&gt; &gt; now this is supported on a free compiler<br>
&gt;&gt; &gt; (cray and intel already support that, but they are commercial<br>
&gt;&gt; &gt; compilers),<br>
&gt;&gt; &gt; I will resume my work on supporting this<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Cheers,<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Gilles<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; On Wednesday, March 23, 2016, Ronald Cohen &lt;<a>recohen3@gmail.com</a>&gt; wrote:<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; I get 100 GFLOPS for 16 cores on one node, but 1 GFLOP running 8 cores<br>
&gt;&gt; &gt;&gt; on two nodes. It seems that quad-infiniband should do better than<br>
&gt;&gt; &gt;&gt; this. I built openmpi-1.10.2g with gcc version 6.0.0 20160317 . Any<br>
&gt;&gt; &gt;&gt; ideas of what to do to get usable performance? Thank you!<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; bstatus<br>
&gt;&gt; &gt;&gt; Infiniband device &#39;mlx4_0&#39; port 1 status:<br>
&gt;&gt; &gt;&gt;         default gid:     fe80:0000:0000:0000:0002:c903:00ec:9301<br>
&gt;&gt; &gt;&gt;         base lid:        0x1<br>
&gt;&gt; &gt;&gt;         sm lid:          0x1<br>
&gt;&gt; &gt;&gt;         state:           4: ACTIVE<br>
&gt;&gt; &gt;&gt;         phys state:      5: LinkUp<br>
&gt;&gt; &gt;&gt;         rate:            56 Gb/sec (4X FDR)<br>
&gt;&gt; &gt;&gt;         link_layer:      InfiniBand<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; Ron<br>
&gt;&gt; &gt;&gt; --<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; Professor Dr. Ronald Cohen<br>
&gt;&gt; &gt;&gt; Ludwig Maximilians Universität<br>
&gt;&gt; &gt;&gt; Theresienstrasse 41 Room 207<br>
&gt;&gt; &gt;&gt; Department für Geo- und Umweltwissenschaften<br>
&gt;&gt; &gt;&gt; München<br>
&gt;&gt; &gt;&gt; 80333<br>
&gt;&gt; &gt;&gt; Deutschland<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; <a>ronald.cohen@min.uni-muenchen.de</a><br>
&gt;&gt; &gt;&gt; skype: ronaldcohen<br>
&gt;&gt; &gt;&gt; +49 (0) 89 74567980<br>
&gt;&gt; &gt;&gt; ---<br>
&gt;&gt; &gt;&gt; Ronald Cohen<br>
&gt;&gt; &gt;&gt; Geophysical Laboratory<br>
&gt;&gt; &gt;&gt; Carnegie Institution<br>
&gt;&gt; &gt;&gt; 5251 Broad Branch Rd., N.W.<br>
&gt;&gt; &gt;&gt; Washington, D.C. 20015<br>
&gt;&gt; &gt;&gt; <a>rcohen@carnegiescience.edu</a><br>
&gt;&gt; &gt;&gt; office: 202-478-8937<br>
&gt;&gt; &gt;&gt; skype: ronaldcohen<br>
&gt;&gt; &gt;&gt; <a href="https://twitter.com/recohen3" target="_blank">https://twitter.com/recohen3</a><br>
&gt;&gt; &gt;&gt; <a href="https://www.linkedin.com/profile/view?id=163327727" target="_blank">https://www.linkedin.com/profile/view?id=163327727</a><br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; ---<br>
&gt;&gt; &gt;&gt; Ron Cohen<br>
&gt;&gt; &gt;&gt; <a>recohen3@gmail.com</a><br>
&gt;&gt; &gt;&gt; skypename: ronaldcohen<br>
&gt;&gt; &gt;&gt; twitter: @recohen3<br>
&gt;&gt; &gt;&gt; _______________________________________________<br>
&gt;&gt; &gt;&gt; users mailing list<br>
&gt;&gt; &gt;&gt; <a>users@open-mpi.org</a><br>
&gt;&gt; &gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; &gt;&gt; Link to this post:<br>
&gt;&gt; &gt;&gt; <a href="http://www.open-mpi.org/community/lists/users/2016/03/28791.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/03/28791.php</a><br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; _______________________________________________<br>
&gt;&gt; &gt; users mailing list<br>
&gt;&gt; &gt; <a>users@open-mpi.org</a><br>
&gt;&gt; &gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; &gt; Link to this post:<br>
&gt;&gt; &gt; <a href="http://www.open-mpi.org/community/lists/users/2016/03/28793.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/03/28793.php</a><br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a>users@open-mpi.org</a><br>
&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; Link to this post:<br>
&gt;&gt; <a href="http://www.open-mpi.org/community/lists/users/2016/03/28794.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/03/28794.php</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a>users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post:<br>
&gt; <a href="http://www.open-mpi.org/community/lists/users/2016/03/28796.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/03/28796.php</a><br>
_______________________________________________<br>
users mailing list<br>
<a>users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/03/28800.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/03/28800.php</a></blockquote></div>
</div>

