Hmm, I didn&#39;t know that.  Is OS X&#39;s small stack something that can be alleviated with &quot;ulimit&quot; in bash?  Right now, I have my ulimit set to unlimited.  Does this still work with OpenMPI?  (I might be wrong, but doesn&#39;t MPI work over TCP, such that new spawned processes on my host wouldn&#39;t be affected by my bash settings?)<div>
<br></div><div>What is discouraging and possibly related is, one member of my research group has to set, unset and reset her ulimit on OS X (Snow Leopard) when running this model statically.  I haven&#39;t experienced the same, but it gives me an impression that something on her computer of OS is very finicky.<br>
<br></div><div>I&#39;m going to try re-building OpenMPI to ensure that everything I am using is built with the same compiler (PGI), and then if (when) I run into this error again, I&#39;ll run the debugger as you suggested below.</div>
<div><br></div><div>Thanks!</div><div><br><div class="gmail_quote">On Tue, Aug 9, 2011 at 5:27 PM, Barrett, Brian W <span dir="ltr">&lt;<a href="mailto:bwbarre@sandia.gov">bwbarre@sandia.gov</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
The error message looks like it&#39;s no where near an MPI function; I would<br>
guess that this is not an Open MPI problem but, particularly given your<br>
statements about Snow Leopard) a CMAQ problem.  The easiest way to debug<br>
on OS X is to launch the application code in a debugger, something like:<br>
<br>
  mpirun -np 2 xterm -e gdb &lt;app&gt;<br>
<br>
One thing that can get people on OS X is that the maximum stack size is<br>
extremely small compared to Linux.  Fortran apps, in particular, can end<br>
up putting things on the stack which cause an overrun and all kinds of fun.<br>
<br>
Brian<br>
<div><div></div><div class="h5"><br>
On 8/9/11 3:18 PM, &quot;Ralph Castain&quot; &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
<br>
&gt;Also, please be aware that we haven&#39;t done any testing of OMPI on Lion,<br>
&gt;so this is truly new ground.<br>
&gt;<br>
&gt;On Aug 9, 2011, at 3:00 PM, Doug Reeder wrote:<br>
&gt;<br>
&gt;<br>
&gt;Matt,<br>
&gt;Are you sure you are building against your macports version of openmpi<br>
&gt;and not the one that ships w/ lion. In the trace back are items 4-9, that<br>
&gt;end w/ x86_64pg from the pgi compiler. You said you are using pgf90 and<br>
&gt;pgcc but in the configure input it looks like gcc is being used on lion.<br>
&gt;<br>
&gt;Doug Reeder<br>
&gt;On Aug 9, 2011, at 1:49 PM, Matthew Russell wrote:<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;Hi,<br>
&gt;I&#39;m trying to run CMAQ - an air quality model developed by the US EPA -<br>
&gt;on a Mac (Lion) using OpenMPI (1.5.3) installed with MacPorts.<br>
&gt;<br>
&gt;I am able to run CMAQ in parallel, and am able to run small programs that<br>
&gt;use OpenMPI.<br>
&gt;<br>
&gt;I set the OpenMPI environment variables to use pgf90/pgcc (10.9) as my<br>
&gt;compiler.  Using PGI because some of the code I need to build is fortran<br>
&gt;77 ( *sigh* ), and for some other reasons.<br>
&gt;<br>
&gt;<br>
&gt;The error I get is:<br>
&gt;<br>
&gt;/opt/local/lib/openmpi/bin/mpirun -v -machinefile<br>
&gt;/Users/matt/cmaq/darwin11/scripts/cctm/machines8 -np 2<br>
&gt;/Users/matt/cmaq/darwin11/scripts/cctm/CCTM_e1a_Darwin11_x86_64pg<br>
&gt;[pontus:72547] *** Process received signal ***<br>
&gt;[pontus:72547] Signal: Segmentation fault: 11 (11)<br>
&gt;[pontus:72547] Signal code: Address not mapped (1)<br>
&gt;[pontus:72547] Failing at address: 0x0<br>
&gt;[pontus:72547] [ 0] 2   libsystem_c.dylib<br>
&gt;0x00007fff91065cfa _sigtramp + 26<br>
&gt;[pontus:72547] [ 1] 3   ???<br>
&gt;0x00007fff5fbe58ab 0x0 + 140734799698091<br>
&gt;[pontus:72547] [ 2] 4   CCTM_e1a_Darwin11_x86_64pg<br>
&gt;0x000000010003c89b distr_env_ + 971<br>
&gt;[pontus:72547] [ 3] 5   CCTM_e1a_Darwin11_x86_64pg<br>
&gt;0x000000010003cbe5 par_init_ + 565<br>
&gt;[pontus:72547] [ 4] 6   CCTM_e1a_Darwin11_x86_64pg<br>
&gt;0x0000000100032e1b MAIN_ + 219<br>
&gt;[pontus:72547] [ 5] 7   CCTM_e1a_Darwin11_x86_64pg<br>
&gt;0x00000001000016f6 main + 70<br>
&gt;[pontus:72547] [ 6] 8   CCTM_e1a_Darwin11_x86_64pg<br>
&gt;0x000000010000163a _start + 248<br>
&gt;[pontus:72547] [ 7] 9   CCTM_e1a_Darwin11_x86_64pg<br>
&gt;0x0000000100001541 start + 33<br>
&gt;[pontus:72547] [ 8] 10  ???<br>
&gt;0x0000000000000001 0x0 + 1<br>
&gt;[pontus:72547] *** End of error message ***<br>
&gt;--------------------------------------------------------------------------<br>
&gt;mpirun noticed that process rank 1 with PID 72547 on node<br>
</div></div>&gt;<a href="http://pontus.cee.carleton.ca" target="_blank">pontus.cee.carleton.ca</a> &lt;<a href="http://pontus.cee.carleton.ca/" target="_blank">http://pontus.cee.carleton.ca/</a>&gt; exited on signal<br>
<div><div></div><div class="h5">&gt;11 (Segmentation fault: 11).<br>
&gt;--------------------------------------------------------------------------<br>
&gt;<br>
&gt;<br>
&gt;I don&#39;t expect anyone to know the solution from this brief error message,<br>
&gt;however I was wondering if anyone has insight on how I might debug this?<br>
&gt;I am too new to both OpenMPI and CMAQ to be served that well from this<br>
&gt;traceback.<br>
&gt;<br>
&gt;I&#39;m told by others in my research group that CMAQ with OpenMPI on Linux<br>
&gt;works fine, and that the error I&#39;m getting is very similar to the error<br>
&gt;others got when trying this on a Mac (Snow Leopard) with ifort.. before<br>
&gt;they gave up...<br>
&gt;<br>
&gt;OpenMPI was configured with:<br>
&gt;configure.args  --sysconfdir=${prefix}/etc/${name} \<br>
&gt;<br>
&gt;                --includedir=${prefix}/include/${name} \<br>
&gt;                --bindir=${prefix}/lib/${name}/bin \<br>
&gt;                --mandir=${prefix}/share/man \<br>
&gt;                --with-memory-manager=none<br>
&gt;<br>
&gt;# enable build on Lion<br>
&gt;if {$os.major} &gt;= 11} {<br>
&gt;        configure.compiler       gcc-4.2<br>
&gt;}<br>
&gt;<br>
&gt;<br>
&gt;The --with-memory-manager is there because I saw it fix potentially<br>
&gt;similar problems in other postings to this Mailing list.  It didn&#39;t make<br>
&gt;a difference though.<br>
&gt;<br>
&gt;Thanks!<br>
&gt;<br>
&gt;<br>
&gt;_______________________________________________<br>
&gt;users mailing list<br>
&gt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;_______________________________________________<br>
&gt;users mailing list<br>
&gt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;_______________________________________________<br>
&gt;users mailing list<br>
&gt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
</div></div><font color="#888888">--<br>
  Brian W. Barrett<br>
  Dept. 1423: Scalable System Software<br>
  Sandia National Laboratories<br>
</font><div><div></div><div class="h5"><br>
<br>
<br>
<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div>

