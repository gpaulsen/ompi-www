You have version confusion somewhere - the error message indicates that mpirun is looking for a component that only exists in the 1.2.x series, not in 1.3.x. Check that your LD_LIBRARY_PATH is pointing to the 1.3.2 location, along with your PATH.<br>
<br><br><br><div class="gmail_quote">On Fri, May 29, 2009 at 12:52 PM, Jeff Layton <span dir="ltr">&lt;<a href="mailto:laytonjb@att.net">laytonjb@att.net</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
I&#39;ve got some more information (after rebuilding Open MPI and the<br>
application a few times). I put<br>
<br>
-mca mpi_show_mca_params enviro<br>
<br>
<br>
in my mpirun line to get some of the parameter information back.<br>
I get the following information back (warning - it&#39;s long).<br>
<br>
--------------------------------------------------------------------------<br>
A requested component was not found, or was unable to be opened.  This<br>
means that this component is either not installed or is unable to be<br>
used on your system (e.g., sometimes this means that shared libraries<br>
that the component requires are unable to be found/loaded).  Note that<br>
Open MPI stopped checking at the first component that it did not find.<br>
<br>
Host:      compute-0-0.local<br>
Framework: ras<br>
Component: proxy<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01564] [[58307,0],0] ORTE_ERROR_LOG: Error in file ess_hnp_module.c at line 199<br>
--------------------------------------------------------------------------<br>
A requested component was not found, or was unable to be opened.  This<br>
means that this component is either not installed or is unable to be<br>
used on your system (e.g., sometimes this means that shared libraries<br>
that the component requires are unable to be found/loaded).  Note that<br>
Open MPI stopped checking at the first component that it did not find.<br>
<br>
Host:      compute-0-0.local<br>
Framework: ras<br>
Component: proxy<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01565] [[58306,0],0] ORTE_ERROR_LOG: Error in file ess_hnp_module.c at line 199<br>
--------------------------------------------------------------------------<br>
A requested component was not found, or was unable to be opened.  This<br>
means that this component is either not installed or is unable to be<br>
used on your system (e.g., sometimes this means that shared libraries<br>
that the component requires are unable to be found/loaded).  Note that<br>
Open MPI stopped checking at the first component that it did not find.<br>
<br>
Host:      compute-0-0.local<br>
Framework: ras<br>
Component: proxy<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01562] [[58309,0],0] ORTE_ERROR_LOG: Error in file ess_hnp_module.c at line 199<br>
--------------------------------------------------------------------------<br>
A requested component was not found, or was unable to be opened.  This<br>
means that this component is either not installed or is unable to be<br>
used on your system (e.g., sometimes this means that shared libraries<br>
that the component requires are unable to be found/loaded).  Note that<br>
Open MPI stopped checking at the first component that it did not find.<br>
<br>
Host:      compute-0-0.local<br>
Framework: ras<br>
Component: proxy<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01560] [[58311,0],0] ORTE_ERROR_LOG: Error in file ess_hnp_module.c at line 199<br>
--------------------------------------------------------------------------<br>
A requested component was not found, or was unable to be opened.  This<br>
means that this component is either not installed or is unable to be<br>
used on your system (e.g., sometimes this means that shared libraries<br>
that the component requires are unable to be found/loaded).  Note that<br>
Open MPI stopped checking at the first component that it did not find.<br>
<br>
Host:      compute-0-0.local<br>
Framework: ras<br>
Component: proxy<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01566] [[58305,0],0] ORTE_ERROR_LOG: Error in file ess_hnp_module.c at line 199<br>
--------------------------------------------------------------------------<br>
A requested component was not found, or was unable to be opened.  This<br>
means that this component is either not installed or is unable to be<br>
used on your system (e.g., sometimes this means that shared libraries<br>
that the component requires are unable to be found/loaded).  Note that<br>
Open MPI stopped checking at the first component that it did not find.<br>
<br>
Host:      compute-0-0.local<br>
Framework: ras<br>
Component: proxy<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01563] [[58308,0],0] ORTE_ERROR_LOG: Error in file ess_hnp_module.c at line 199<br>
--------------------------------------------------------------------------<br>
A requested component was not found, or was unable to be opened.  This<br>
means that this component is either not installed or is unable to be<br>
used on your system (e.g., sometimes this means that shared libraries<br>
that the component requires are unable to be found/loaded).  Note that<br>
Open MPI stopped checking at the first component that it did not find.<br>
<br>
Host:      compute-0-0.local<br>
Framework: ras<br>
Component: proxy<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01559] [[58312,0],0] ORTE_ERROR_LOG: Error in file ess_hnp_module.c at line 199<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
 orte_ras_base_open failed<br>
 --&gt; Returned value Error (-1) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01565] [[58306,0],0] ORTE_ERROR_LOG: Error in file runtime/orte_init.c at line 132<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
 orte_ras_base_open failed<br>
 --&gt; Returned value Error (-1) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01564] [[58307,0],0] ORTE_ERROR_LOG: Error in file runtime/orte_init.c at line 132<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
 orte_ras_base_open failed<br>
 --&gt; Returned value Error (-1) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01562] [[58309,0],0] ORTE_ERROR_LOG: Error in file runtime/orte_init.c at line 132<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
 orte_ras_base_open failed<br>
 --&gt; Returned value Error (-1) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01566] [[58305,0],0] ORTE_ERROR_LOG: Error in file runtime/orte_init.c at line 132<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
 orte_ras_base_open failed<br>
 --&gt; Returned value Error (-1) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01560] [[58311,0],0] ORTE_ERROR_LOG: Error in file runtime/orte_init.c at line 132<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
 orte_ras_base_open failed<br>
 --&gt; Returned value Error (-1) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01563] [[58308,0],0] ORTE_ERROR_LOG: Error in file runtime/orte_init.c at line 132<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
 orte_ras_base_open failed<br>
 --&gt; Returned value Error (-1) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01559] [[58312,0],0] ORTE_ERROR_LOG: Error in file runtime/orte_init.c at line 132<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
 orte_ess_set_name failed<br>
 --&gt; Returned value Error (-1) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01566] [[58305,0],0] ORTE_ERROR_LOG: Error in file orted/orted_main.c at line 323<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
 orte_ess_set_name failed<br>
 --&gt; Returned value Error (-1) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01564] [[58307,0],0] ORTE_ERROR_LOG: Error in file orted/orted_main.c at line 323<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
 orte_ess_set_name failed<br>
 --&gt; Returned value Error (-1) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01565] [[58306,0],0] ORTE_ERROR_LOG: Error in file orted/orted_main.c at line 323<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
 orte_ess_set_name failed<br>
 --&gt; Returned value Error (-1) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01562] [[58309,0],0] ORTE_ERROR_LOG: Error in file orted/orted_main.c at line 323<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
 orte_ess_set_name failed<br>
 --&gt; Returned value Error (-1) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01560] [[58311,0],0] ORTE_ERROR_LOG: Error in file orted/orted_main.c at line 323<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
 orte_ess_set_name failed<br>
 --&gt; Returned value Error (-1) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01563] [[58308,0],0] ORTE_ERROR_LOG: Error in file orted/orted_main.c at line 323<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
 orte_ess_set_name failed<br>
 --&gt; Returned value Error (-1) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01559] [[58312,0],0] ORTE_ERROR_LOG: Error in file orted/orted_main.c at line 323<br>
[compute-0-0.local:01556] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file ess_singleton_module.c at line 381<br>
[compute-0-0.local:01556] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file ess_singleton_module.c at line 143<br>
[compute-0-0.local:01556] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file runtime/orte_init.c at line 132<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
 orte_ess_set_name failed<br>
 --&gt; Returned value Unable to start a daemon on the local node (-128) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
[compute-0-0.local:01555] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file ess_singleton_module.c at line 381<br>
[compute-0-0.local:01555] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file ess_singleton_module.c at line 143<br>
[compute-0-0.local:01551] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file ess_singleton_module.c at line 381<br>
[compute-0-0.local:01551] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file ess_singleton_module.c at line 143<br>
[compute-0-0.local:01551] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file runtime/orte_init.c at line 132<br>
[compute-0-0.local:01552] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file ess_singleton_module.c at line 381<br>
[compute-0-0.local:01552] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file ess_singleton_module.c at line 143<br>
[compute-0-0.local:01552] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file runtime/orte_init.c at line 132<br>
[compute-0-0.local:01554] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file ess_singleton_module.c at line 381<br>
[compute-0-0.local:01554] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file ess_singleton_module.c at line 143<br>
[compute-0-0.local:01554] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file runtime/orte_init.c at line 132<br>
[compute-0-0.local:01555] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file runtime/orte_init.c at line 132<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
 orte_ess_set_name failed<br>
 --&gt; Returned value Unable to start a daemon on the local node (-128) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
--------------------------------------------------------------------------<br>
It looks like MPI_INIT failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during MPI_INIT; some of which are due to configuration or environment<br>
problems.  This failure appears to be an internal failure; here&#39;s some<br>
additional information (which may only be relevant to an Open MPI<br>
developer):<br>
<br>
 ompi_mpi_init: orte_init failed<br>
 --&gt; Returned &quot;Unable to start a daemon on the local node&quot; (-128) instead of &quot;Success&quot; (0)<br>
--------------------------------------------------------------------------<br>
*** An error occurred in MPI_Init<br>
*** before MPI was initialized<br>
*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>
[compute-0-0.local:1556] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>
[compute-0-0.local:01557] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file ess_singleton_module.c at line 381<br>
[compute-0-0.local:01557] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file ess_singleton_module.c at line 143<br>
[compute-0-0.local:01557] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file runtime/orte_init.c at line 132<br>
[compute-0-0.local:01558] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file ess_singleton_module.c at line 381<br>
[compute-0-0.local:01558] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file ess_singleton_module.c at line 143<br>
[compute-0-0.local:01558] [[INVALID],INVALID] ORTE_ERROR_LOG: Unable to start a daemon on the local node in file runtime/orte_init.c at line 132<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
 orte_ess_set_name failed<br>
 --&gt; Returned value Unable to start a daemon on the local node (-128) instead of ORTE_SUCCESS<br>
--------------------------------------------------------------------------<br>
--------------------------------------------------------------------------<br>
It looks like orte_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during orte_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
<br>
(and on and on).<br>
<br>
Does anyone have any ideas? Google let me down on this one.<br>
<br>
TIA!<br><font color="#888888">
<br>
Jeff</font><div><div></div><div class="h5"><br>
<br>
<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Good morning,<br>
<br>
I just built 1.3.2 on a ROCKS 4.something system. I built my code<br>
(CFL3D) with the Intel 10.1 compilers. I also linked in the<br>
OpenMPI libs and the Intel libraries to make sure I had the paths<br>
correct. When I try running my code, I get the following,<br>
<br>
<br>
error: executing task of job 2951 failed: execution daemon on host &quot;compute-2-3.local&quot; didn&#39;t accept task<br>
-------------------------------------------------------------------------- <br>
A daemon (pid 12015) died unexpectedly with status 1 while attempting<br>
to launch so we are aborting.<br>
<br>
There may be more information reported by the environment (see above).<br>
<br>
This may be because the daemon was unable to find all the needed shared<br>
libraries on the remote node. You may set your LD_LIBRARY_PATH to have the<br>
location of the shared libraries on the remote nodes and this will<br>
automatically be forwarded to the remote nodes.<br>
-------------------------------------------------------------------------- <br>
-------------------------------------------------------------------------- <br>
mpirun noticed that the job aborted, but has no info as to the process<br>
that caused that situation.<br>
-------------------------------------------------------------------------- <br>
mpirun: clean termination accomplished<br>
<br>
<br>
<br>
Everything seems correct. I checked that the mpirun was correct<br>
and the binary has the correct libraries (checked using ldd).<br>
<br>
Can anyone tell me what the &quot;status 1&quot; means? Any tips on debugging<br>
the problem?<br>
<br>
Thanks!<br>
<br>
Jeff<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
</blockquote>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

