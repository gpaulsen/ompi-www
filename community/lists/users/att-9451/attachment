<HTML dir=ltr><HEAD>
<META http-equiv=Content-Type content="text/html; charset=unicode">
<STYLE type=text/css><!-- DIV {margin:0px;} --></STYLE>

<META content="MSHTML 6.00.2900.5726" name=GENERATOR></HEAD>
<BODY>
<DIV id=idOWAReplyText96960 dir=ltr>
<DIV dir=ltr><FONT face=Arial color=#000000 size=2>MK,</FONT></DIV>
<DIV dir=ltr><FONT face=Arial size=2></FONT>&nbsp;</DIV>
<DIV dir=ltr><FONT face=Arial size=2>Hmm.. What if you put CC=<FONT size=3>/usr/local/intel/Compiler/11.0/083/bin/intel64/icc</FONT></FONT></DIV>
<DIV dir=ltr>on the build line.</DIV>
<DIV dir=ltr>&nbsp;</DIV>
<DIV dir=ltr>Joe</DIV>
<DIV dir=ltr>&nbsp;</DIV></DIV>
<DIV dir=ltr><BR>
<HR tabIndex=-1>
<FONT face=Tahoma size=2><B>From:</B> users-bounces@open-mpi.org on behalf of Michael Kuklik<BR><B>Sent:</B> Wed 5/27/2009 5:05 PM<BR><B>To:</B> users@open-mpi.org<BR><B>Subject:</B> Re: [OMPI users] problem with installing openmpi with intelcompiler onubuntu<BR></FONT><BR></DIV>
<DIV>
<DIV style="FONT-SIZE: 12pt; FONT-FAMILY: verdana,helvetica,sans-serif">
<DIV>Joe</DIV>
<DIV style="FONT-SIZE: 12pt; FONT-FAMILY: verdana,helvetica,sans-serif"><BR>'which icc' returns the path to icc<BR>/usr/local/intel/Compiler/11.0/083/bin/intel64/icc<BR><BR>and I used the env variable script provided by intel.<BR>so my shell env is ok and I think libtool should inherit my shell environment<BR><BR>just in case I send you the env printout<BR><BR>MKLROOT=/usr/local/intel/Compiler/11.0/083/mkl<BR>MANPATH=/usr/local/intel/Compiler/11.0/083/man:/usr/local/intel/Compiler/11.0/083/mkl/man/en_US:/usr/local/intel/Compiler/11.0/083/man:/usr/local/intel/Compiler/11.0/083/mkl/man/en_US:/usr/local/man:/usr/local/share/man:/usr/share/man<BR>INTEL_LICENSE_FILE=/usr/local/intel/Compiler/11.0/083/licenses:/opt/intel/licenses:/home/mkuklik/intel/licenses:/usr/local/intel/Compiler/11.0/083/licenses:/opt/intel/licenses:/home/mkuklik/intel/licenses<BR>IPPROOT=/usr/local/intel/Compiler/11.0/083/ipp/em64t<BR>TERM=xterm-color<BR>SHELL=/bin/bash<BR>XDG_SESSION_COOKIE=d03e782e0b3c90f7ce8380174a15d9d2-1243468120.315267-1057427925<BR>SSH_CLIENT=128.151.210.198 54616 22<BR>LIBRARY_PATH=/usr/local/intel/Compiler/11.0/083/ipp/em64t/lib:/usr/local/intel/Compiler/11.0/083/mkl/lib/em64t:/usr/local/intel/Compiler/11.0/083/tbb/em64t/cc4.1.0_libc2.4_kernel2.6.16.21/lib:/usr/local/intel/Compiler/11.0/083/ipp/em64t/lib:/usr/local/intel/Compiler/11.0/083/mkl/lib/em64t:/usr/local/intel/Compiler/11.0/083/tbb/em64t/cc4.1.0_libc2.4_kernel2.6.16.21/lib<BR>FPATH=/usr/local/intel/Compiler/11.0/083/mkl/include:/usr/local/intel/Compiler/11.0/083/mkl/include<BR>SSH_TTY=/dev/pts/4<BR>LC_ALL=C<BR>USER=mkuklik<BR>LD_LIBRARY_PATH=/usr/local/intel/Compiler/11.0/083/lib/intel64:/usr/local/intel/Compiler/11.0/083/ipp/em64t/sharedlib:/usr/local/intel/Compiler/11.0/083/mkl/lib/em64t:/usr/local/intel/Compiler/11.0/083/tbb/em64t/cc4.1.0_libc2.4_kernel2.6.16.21/lib:/usr/local/intel/Compiler/11.0/083/lib/intel64:/usr/local/intel/Compiler/11.0/083/ipp/em64t/sharedlib:/usr/local/intel/Compiler/11.0/083/mkl/lib/em64t:/usr/local/intel/Compiler/11.0/083 /tbb/em64t/cc4.1.0_libc2.4_kernel2.6.16.21/lib<BR>LIB=/usr/local/intel/Compiler/11.0/083/ipp/em64t/lib:/usr/local/intel/Compiler/11.0/083/ipp/em64t/lib:<BR>CPATH=/usr/local/intel/Compiler/11.0/083/ipp/em64t/include:/usr/local/intel/Compiler/11.0/083/mkl/include:/usr/local/intel/Compiler/11.0/083/tbb/include:/usr/local/intel/Compiler/11.0/083/ipp/em64t/include:/usr/local/intel/Compiler/11.0/083/mkl/include:/usr/local/intel/Compiler/11.0/083/tbb/include<BR>NLSPATH=/usr/local/intel/Compiler/11.0/083/lib/intel64/locale/%l_%t/%N:/usr/local/intel/Compiler/11.0/083/ipp/em64t/lib/locale/%l_%t/%N:/usr/local/intel/Compiler/11.0/083/mkl/lib/em64t/locale/%l_%t/%N:/usr/local/intel/Compiler/11.0/083/idb/intel64/locale/%l_%t/%N:/usr/local/intel/Compiler/11.0/083/lib/intel64/locale/%l_%t/%N:/usr/local/intel/Compiler/11.0/083/ipp/em64t/lib/locale/%l_%t/%N:/usr/local/intel/Compiler/11.0/083/mkl/lib/em64t/locale/%l_%t/%N:/usr/local/intel/Compiler/11.0/083/idb/intel64/loca le/%l_%t/%N<BR>MAIL=/var/mail/mkuklik<BR>PATH=/usr/local/intel/Compiler/11.0/083/bin/intel64:/usr/local/intel/Compiler/11.0/083/bin/intel64:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games<BR>PWD=/home/mkuklik<BR>LANG=en_US<BR>SHLVL=1<BR>HOME=/home/mkuklik<BR>DYLD_LIBRARY_PATH=/usr/local/intel/Compiler/11.0/083/tbb/em64t/cc4.1.0_libc2.4_kernel2.6.16.21/lib:/usr/local/intel/Compiler/11.0/083/tbb/em64t/cc4.1.0_libc2.4_kernel2.6.16.21/lib<BR>LOGNAME=mkuklik<BR>SSH_CONNECTION=128.151.210.198 54616 128.151.210.190 22<BR>INCLUDE=/usr/local/intel/Compiler/11.0/083/ipp/em64t/include:/usr/local/intel/Compiler/11.0/083/mkl/include:/usr/local/intel/Compiler/11.0/083/ipp/em64t/include:/usr/local/intel/Compiler/11.0/083/mkl/include<BR>_=/usr/bin/env<BR><BR>Thanks,<BR><BR>mk<BR><BR>
<DIV style="FONT-SIZE: 13px; FONT-FAMILY: arial,helvetica,sans-serif"><FONT face=Tahoma size=2>
<HR SIZE=1>
<B><SPAN style="FONT-WEIGHT: bold"></SPAN></B></FONT><BR>----------------------------------------------------------------------<BR><BR>Message: 1<BR>Date: Tue, 26 May 2009 19:51:48 -0700<BR>From: "Joe Griffin" &lt;<A href="mailto:joe.griffin@mscsoftware.com">joe.griffin@mscsoftware.com</A>&gt;<BR>Subject: Re: [OMPI users] problem with installing openmpi with intel<BR>&nbsp;&nbsp;&nbsp; compiler onubuntu<BR>To: "Open MPI Users" &lt;<A href="mailto:users@open-mpi.org">users@open-mpi.org</A>&gt;<BR>Message-ID:<BR>&nbsp;&nbsp;&nbsp; &lt;<A href="mailto:1D367926756E9848BABD800E249AA5E04BFF84@NASCMEX01.na.mscsoftware.com">1D367926756E9848BABD800E249AA5E04BFF84@NASCMEX01.na.mscsoftware.com</A>&gt;<BR>Content-Type: text/plain; charset="iso-8859-1"<BR><BR>MK,<BR><BR>Is "icc" in your path?<BR><BR>What if you type "which icc"?<BR><BR>Joe<BR><BR><BR>________________________________<BR><BR>From: <A href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</A> on behalf of Michael Kuklik<BR>Sent: Tue 5/26/2009 7:05 PM<BR>To: <A href="mailto:users@open-mpi.org">users@open-mpi.org</A><BR>Subject: [OMPI users] problem with installing openmpi with intel compiler onubuntu<BR><BR><BR>Hi everybody,<BR><BR>I try to compile openmpi with intel compiler on ubuntu 9.04.<BR>I compiled openmpi on Redhat and os x many times and I could always find a problem. But the error that I'm getting now, gives me no clues where to even search for the problem.<BR><BR>my config line is a follows:<BR>./configure CC=icc CXX=icpc --prefix=/usr/local/intel/openmpi<BR><BR>Everything configures and compiles OK. But then when I try to install I get this error<BR><BR>Making install in etc<BR>make[2]: Entering directory `/tmp/openmpi-1.3.2/orte/etc'<BR>make[3]: Entering directory `/tmp/openmpi-1.3.2/orte/etc'<BR>make[3]: Nothing to be done for `install-exec-am'.<BR>/bin/mkdir -p /usr/local/intel/openmpi/etc<BR>******************************* WARNING ************************************<BR>*** Not installing new openmpi-default-hostfile over existing file in:<BR>***&nbsp; /usr/local/intel/openmpi/etc/openmpi-default-hostfile<BR>******************************* WARNING ************************************<BR>make[3]: Leaving directory `/tmp/openmpi-1.3.2/orte/etc'<BR>make[2]: Leaving directory `/tmp/openmpi-1.3.2/orte/etc'<BR>Making install in .<BR>make[2]: Entering directory `/tmp/openmpi-1.3.2/orte'<BR>make[3]: Entering directory `/tmp/openmpi-1.3.2/orte'<BR>test -z "/usr/local/intel/openmpi/lib" || /bin/mkdir -p "/usr/local/intel/openmpi/lib"<BR>/bin/bash ../libtool&nbsp; --mode=install /usr/bin/install -c&nbsp; '<A href="http://libopen-rte.la/" target=_blank>libopen-rte.la</A>' '/usr/local/intel/openmpi/lib/libopen-rte.la'<BR>libtool: install: warning: relinking `libopen-rte.la'<BR>libtool: install: (cd /tmp/openmpi-1.3.2/orte; /bin/bash /tmp/openmpi-1.3.2/libtool&nbsp; --tag CC --mode=relink icc -O3 -DNDEBUG -finline-functions -fno-strict-aliasing ................ )<BR>libtool: relink: icc -shared&nbsp; runtime/.libs/orte_finalize.o runtime/.libs/orte_init.o runtime/.libs/orte_locks.o runtime/.libs/orte_globals.o runtime/data_type_support/.libs/orte_dt_compare_fns.o runtime/data_type_support/.libs/orte_dt_copy_fns.o runtime/data_type_support/.libs/orte_dt_print_fns.o runtime/data_type_support/.libs/orte_dt_release_fns.o runtime/data_type_support/.libs/orte_dt_size_fns.o runtime/data_type_support/.libs/orte_dt_packing_fns.o runtime/data_type_support/.libs/orte_dt_unpacking_fns.o runtime/.libs/orte_mca_params.o runtime/.libs/orte_wait.o runtime/.libs/orte_cr.o runtime/.libs/..................................... -Wl,<A href="http://libopen-rte.so/" target=_blank>libopen-rte.so</A>.0 -o .libs/libopen-rte.so.0.0.0<BR>/tmp/openmpi-1.3.2/libtool: line 7847: icc: command not found<BR>libtool: install: error: relink `libopen-rte.la' with the above command before installing it<BR>make[3]: *** [install-libLTLIBRARIES] Error 1<BR>make[3]: Leaving directory `/tmp/openmpi-1.3.2/orte'<BR>make[2]: *** [install-am] Error 2<BR>make[2]: Leaving directory `/tmp/openmpi-1.3.2/orte'<BR>make[1]: *** [install-recursive] Error 1<BR>make[1]: Leaving directory `/tmp/openmpi-1.3.2/orte'<BR>make: *** [install-recursive] Error 1<BR><BR>libtool is the one from ubuntu repository i.e. 2.2.6a-1<BR>icc and icpc are the newest ones i.e. 11.083 <BR><BR>Ouputs of configure make and install are attached.<BR><BR>Any clues what's wrong?<BR><BR>Thanks for help<BR><BR>mk<BR><BR><BR>-------------- next part --------------<BR>A non-text attachment was scrubbed...<BR>Name: not available<BR>Type: application/ms-tnef<BR>Size: 5414 bytes<BR>Desc: not available<BR><SPAN>URL: &lt;<A href="http://www.open-mpi.org/MailArchives/users/attachments/20090526/9737163d/attachment.bin" target=_blank>http://www.open-mpi.org/MailArchives/users/attachments/20090526/9737163d/attachment.bin</A>&gt;</SPAN><BR><BR>------------------------------<BR><BR>Message: 2<BR>Date: Wed, 27 May 2009 13:09:27 +0300<BR>From: vasilis &lt;<A href="mailto:gkanis@ipta.demokritos.gr">gkanis@ipta.demokritos.gr</A>&gt;<BR>Subject: Re: [OMPI users] "An error occurred in MPI_Recv" with more<BR>&nbsp;&nbsp;&nbsp; than 2&nbsp;&nbsp;&nbsp; CPU<BR>To: Open MPI Users &lt;<A href="mailto:users@open-mpi.org">users@open-mpi.org</A>&gt;<BR>Message-ID: &lt;<A href="mailto:200905271309.27914.gkanis@ipta.demokritos.gr">200905271309.27914.gkanis@ipta.demokritos.gr</A>&gt;<BR>Content-Type: Text/Plain;&nbsp; charset="iso-8859-1"<BR><BR>Thank you Eugene for your suggestion. I used different tags for each variable, <BR>and now I do not get this error. <BR>The problem now is that I am getting a different solution when I use more than <BR>2 CPUs. I checked the matrices and I found that they differ by a very small <BR>amount&nbsp; of the order 10^(-10). Actually, I am getting a different solution if I <BR>use 4CPUs or 16CPUs!!!<BR>Do you have any idea what could cause this behavior?<BR><BR>Thank you,<BR>Vasilis<BR><BR>On Tuesday 26 of May 2009 7:21:32 pm you wrote:<BR>&gt; vasilis wrote:<BR>&gt; &gt;Dear openMpi users,<BR>&gt; &gt;<BR>&gt; &gt;I am trying to develop a code that runs in parallel mode with openMPI<BR>&gt; &gt; (1.3.2 version). The code is written in Fortran 90, and I am running on <BR>&gt; &gt; a cluster<BR>&gt; &gt;<BR>&gt; &gt;If I use 2 CPU the program runs fine, but for a larger number of CPUs I<BR>&gt; &gt; get the following error:<BR>&gt; &gt;<BR>&gt; &gt;[compute-2-6.local:18491] *** An error occurred in MPI_Recv<BR>&gt; &gt;[compute-2-6.local:18491] *** on communicator MPI_COMM_WORLD<BR>&gt; &gt;[compute-2-6.local:18491] *** MPI_ERR_TRUNCATE: message truncated<BR>&gt; &gt;[compute-2-6.local:18491] *** MPI_ERRORS_ARE_FATAL (your MPI job will now<BR>&gt; &gt;abort)<BR>&gt; &gt;<BR>&gt; &gt;Here is the part of the code that this error refers to:<BR>&gt; &gt;if( mumps_par%MYID .eq. 0 ) THEN<BR>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; res=res+res_cpu<BR>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; do iw=1,total_elem_cpu*unique<BR>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; jacob(iw)=jacob(iw)+jacob_cpu(iw)<BR>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; position_col(iw)=position_col(iw)+col_cpu(iw)<BR>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; position_row(iw)=position_row(iw)+row_cpu(iw)<BR>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; end do<BR>&gt; &gt;<BR>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; do jw=1,nsize-1<BR>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; call<BR>&gt; &gt;MPI_recv(jacob_cpu,total_elem_cpu*unique,MPI_DOUBLE_PRECISION,MPI_ANY_SOUR<BR>&gt; &gt;CE,MPI_ANY_TAG,MPI_COMM_WORLD,status1,ierr) call<BR>&gt; &gt;MPI_recv(res_cpu,total_unknowns,MPI_DOUBLE_PRECISION,MPI_ANY_SOURCE,MPI_AN<BR>&gt; &gt;Y_TAG,MPI_COMM_WORLD,status2,ierr) call<BR>&gt; &gt;MPI_recv(row_cpu,total_elem_cpu*unique,MPI_INTEGER,MPI_ANY_SOURCE,MPI_ANY_<BR>&gt; &gt;TAG,MPI_COMM_WORLD,status3,ierr) call<BR>&gt; &gt;MPI_recv(col_cpu,total_elem_cpu*unique,MPI_INTEGER,MPI_ANY_SOURCE,MPI_ANY_<BR>&gt; &gt;TAG,MPI_COMM_WORLD,status4,ierr)<BR>&gt; &gt;<BR>&gt; &gt;&nbsp; res=res+res_cpu<BR>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; do iw=1,total_elem_cpu*unique<BR>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <BR>&gt; &gt; jacob(status1(MPI_SOURCE)*total_elem_cpu*unique+iw)=&amp;<BR>&gt; &gt; jacob(status1(MPI_SOURCE)*total_elem_cpu*unique+iw)+jacob_cpu(iw)<BR>&gt; &gt; position_col(status4(MPI_SOURCE)*total_elem_cpu*unique+iw)=&amp;<BR>&gt; &gt; position_col(status4(MPI_SOURCE)*total_elem_cpu*unique+iw)+col_cpu(iw)<BR>&gt; &gt; position_row(status3(MPI_SOURCE)*total_elem_cpu*unique+iw)=&amp;<BR>&gt; &gt; position_row(status3(MPI_SOURCE)*total_elem_cpu*unique+iw)+row_cpu(iw)<BR>&gt; &gt; end do<BR>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; end do<BR>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; else<BR>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; call<BR>&gt; &gt;MPI_Isend(jacob_cpu,total_elem_cpu*unique,MPI_DOUBLE_PRECISION,0,mumps_par<BR>&gt; &gt;%MYID,MPI_COMM_WORLD,request1,ierr) call<BR>&gt; &gt;MPI_Isend(res_cpu,total_unknowns,MPI_DOUBLE_PRECISION,0,mumps_par%MYID,MPI<BR>&gt; &gt;_COMM_WORLD,request2,ierr) call<BR>&gt; &gt;MPI_Isend(row_cpu,total_elem_cpu*unique,MPI_INTEGER,0,mumps_par%MYID,MPI_C<BR>&gt; &gt;OMM_WORLD,request3,ierr) call<BR>&gt; &gt;MPI_Isend(col_cpu,total_elem_cpu*unique,MPI_INTEGER,0,mumps_par%MYID,MPI_C<BR>&gt; &gt;OMM_WORLD,request4,ierr) call MPI_Wait(request1, status1, ierr)<BR>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; call MPI_Wait(request2, status2, ierr)<BR>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; call MPI_Wait(request3, status3, ierr)<BR>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; call MPI_Wait(request4, status4, ierr)<BR>&gt; &gt;&nbsp; &nbsp; &nbsp; &nbsp; end if<BR>&gt; &gt;<BR>&gt; &gt;<BR>&gt; &gt;I am also using the MUMPS library<BR>&gt; &gt;<BR>&gt; &gt;Could someone help to track this error down. Is really annoying to use<BR>&gt; &gt; only two processors.<BR>&gt; &gt;The cluster has about 8 nodes and each has 4 dual core CPU. I tried to run<BR>&gt; &gt; the code on a single node with more than 2 CPU but I got the same error!!<BR>&gt;<BR>&gt; I think the error message means that the received message was longer<BR>&gt; than the receive buffer that was specified.&nbsp; If I look at your code and<BR>&gt; try to reason about its correctness, I think of the message-passing<BR>&gt; portion as looking like this:<BR>&gt;<BR>&gt; if( mumps_par%MYID .eq. 0 ) THEN<BR>&gt;&nbsp; &nbsp; do jw=1,nsize-1<BR>&gt;&nbsp; &nbsp; &nbsp; &nbsp; call<BR>&gt; MPI_recv(jacob_cpu,total_elem_cpu*unique,MPI_DOUBLE_PRECISION,MPI_ANY_SOURC<BR>&gt;E,MPI_ANY_TAG,MPI_COMM_WORLD,status1,ierr) call MPI_recv( <BR>&gt; res_cpu,total_unknowns<BR>&gt; ,MPI_DOUBLE_PRECISION,MPI_ANY_SOURCE,MPI_ANY_TAG,MPI_COMM_WORLD,status2,ier<BR>&gt;r) call MPI_recv(<BR>&gt; row_cpu,total_elem_cpu*unique,MPI_INTEGER<BR>&gt; ,MPI_ANY_SOURCE,MPI_ANY_TAG,MPI_COMM_WORLD,status3,ierr)<BR>&gt;&nbsp; &nbsp; &nbsp; &nbsp; call MPI_recv(<BR>&gt; col_cpu,total_elem_cpu*unique,MPI_INTEGER<BR>&gt; ,MPI_ANY_SOURCE,MPI_ANY_TAG,MPI_COMM_WORLD,status4,ierr)<BR>&gt;&nbsp; &nbsp; end do<BR>&gt; else<BR>&gt;&nbsp; &nbsp; call<BR>&gt; MPI_Send(jacob_cpu,total_elem_cpu*unique,MPI_DOUBLE_PRECISION,0,mumps_par%M<BR>&gt;YID,MPI_COMM_WORLD,ierr) call MPI_Send(&nbsp; res_cpu,total_unknowns<BR>&gt; ,MPI_DOUBLE_PRECISION,0,mumps_par%MYID,MPI_COMM_WORLD,ierr)<BR>&gt;&nbsp; &nbsp; call MPI_Send(&nbsp; row_cpu,total_elem_cpu*unique,MPI_INTEGER<BR>&gt; ,0,mumps_par%MYID,MPI_COMM_WORLD,ierr)<BR>&gt;&nbsp; &nbsp; call MPI_Send(&nbsp; col_cpu,total_elem_cpu*unique,MPI_INTEGER<BR>&gt; ,0,mumps_par%MYID,MPI_COMM_WORLD,ierr)<BR>&gt; end if<BR>&gt;<BR>&gt; If you're running on two processes, then the messages you receive are in<BR>&gt; the order you expect.&nbsp; If there are more than two processes, however,<BR>&gt; certainly messages will start appearing "out of order" and your<BR>&gt; indiscriminate use of MPI_ANY_SOURCE and MPI_ANY_TAG will start getting<BR>&gt; them mixed up.&nbsp; You won't just get all messages from one rank and then<BR>&gt; all from another and then all from another.&nbsp; Rather, the messages from<BR>&gt; all these other processes will come interwoven, but you interpret them<BR>&gt; in a fixed order.<BR>&gt;<BR>&gt; Here is what I mean.&nbsp; Let's say you have 3 processes.&nbsp; So, rank 0 will<BR>&gt; receive 8 messages:&nbsp; 4 from rank 1and 4 from rank 2.&nbsp; Correspondingly,<BR>&gt; rank 1 and rank 2 will each send 4 messages to rank 0.&nbsp; Here is a<BR>&gt; possibility for the order in which messages are received:<BR>&gt;<BR>&gt; jacob_cpu from rank 1<BR>&gt; jacob_cpu from rank 2<BR>&gt; res_cpu from rank 1<BR>&gt; row_cpu from rank 1<BR>&gt; res_cpu from rank 2<BR>&gt; row_cpu from rank 2<BR>&gt; col_cpu from rank 2<BR>&gt; col_cpu from rank 1<BR>&gt;<BR>&gt; Rank 0, however, is trying to unpack these in the order you prescribed<BR>&gt; in your code.&nbsp; Data will get misinterpreted.&nbsp; More to the point here,<BR>&gt; you will be trying to receive data into buffers of the wrong size (some<BR>&gt; of the time).<BR>&gt;<BR>&gt; Maybe you should use tags to distinguish between the different types of<BR>&gt; messages you're trying to send.<BR>&gt; _______________________________________________<BR>&gt; users mailing list<BR>&gt; <A href="mailto:users@open-mpi.org">users@open-mpi.org</A><BR>&gt; <A href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target=_blank>http://www.open-mpi.org/mailman/listinfo.cgi/users</A><BR><BR><BR><BR>------------------------------<BR><BR>Message: 3<BR>Date: Wed, 27 May 2009 07:41:08 -0700<BR>From: Eugene Loh &lt;<A href="mailto:Eugene.Loh@Sun.COM">Eugene.Loh@Sun.COM</A>&gt;<BR>Subject: Re: [OMPI users] "An error occurred in MPI_Recv" with more<BR>&nbsp;&nbsp;&nbsp; than 2&nbsp;&nbsp;&nbsp; CPU<BR>To: Open MPI Users &lt;<A href="mailto:users@open-mpi.org">users@open-mpi.org</A>&gt;<BR>Message-ID: &lt;<A href="mailto:4A1D5104.7090501@sun.com">4A1D5104.7090501@sun.com</A>&gt;<BR>Content-Type: text/plain; CHARSET=US-ASCII; format=flowed<BR><BR>vasilis wrote:<BR><BR>&gt;Thank you Eugene for your suggestion. I used different tags for each variable, <BR>&gt;and now I do not get this error. <BR>&gt;The problem now is that I am getting a different solution when I use more than <BR>&gt;2 CPUs. I checked the matrices and I found that they differ by a very small <BR>&gt;amount&nbsp; of the order 10^(-10). Actually, I am getting a different solution if I <BR>&gt;use 4CPUs or 16CPUs!!!<BR>&gt;Do you have any idea what could cause this behavior?<BR>&gt;&nbsp; <BR>&gt;<BR>Sure.<BR><BR>Rank 0 accumulates all the res_cpu values into a single array, res.&nbsp; It <BR>starts with its own res_cpu and then adds all other processes.&nbsp; When <BR>np=2, that means the order is prescribed.&nbsp; When np&gt;2, the order is no <BR>longer prescribed and some floating-point rounding variations can start <BR>to occur.<BR><BR>If you want results to be more deterministic, you need to fix the order <BR>in which res is aggregated.&nbsp; E.g., instead of using MPI_ANY_SOURCE, loop <BR>over the peer processes in a specific order.<BR><BR><BR><BR>P.S.&nbsp; It seems to me that you could use MPI collective operations to <BR>implement what you're doing.&nbsp; E.g., something like:<BR><BR>call MPI_Reduce(res_cpu, res, total_unknown, MPI_DOUBLE_PRECISION, <BR>MPI_SUM, 0, MPI_COMM_WORLD, ierr)<BR><BR>call MPI_Gather(jacob_cpu, total_elem_cpu * unique, MPI_DOUBLE_PRECISION, &amp;<BR>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; jacob&nbsp; &nbsp; , total_elem_cpu * unique, <BR>MPI_DOUBLE_PRECISION, 0, MPI_COMM_WORLD, ierr)<BR>call MPI_Gather(&nbsp; row_cpu, total_elem_cpu * unique, MPI_INTEGER&nbsp; &nbsp; &nbsp; &nbsp; , &amp;<BR>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; row&nbsp; &nbsp; , total_elem_cpu * unique, MPI_INTEGER&nbsp; &nbsp; &nbsp; &nbsp; <BR>, 0, MPI_COMM_WORLD, ierr)<BR>call MPI_Gather(&nbsp; col_cpu, total_elem_cpu * unique, MPI_INTEGER&nbsp; &nbsp; &nbsp; &nbsp; , &amp;<BR>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; col&nbsp; &nbsp; , total_elem_cpu * unique, MPI_INTEGER&nbsp; &nbsp; &nbsp; &nbsp; <BR>, 0, MPI_COMM_WORLD, ierr)<BR><BR>I think the res part is right.&nbsp; The jacob/row/col parts are not quite <BR>right since you don't just want to gather the elements, but add them <BR>into particular arrays.&nbsp; Not sure if you really want to allocate a new <BR>scratch array to use for this purpose or what.&nbsp; Nor would this solve the <BR>res_cpu indeterministic problem you had.&nbsp; I just wanted to make sure you <BR>knew about the MPI collective operations as an alternative to your <BR>point-to-point implementation.<BR><BR><BR>------------------------------<BR><BR>Message: 4<BR>Date: Wed, 27 May 2009 10:28:42 -0400<BR>From: Jeff Squyres &lt;<A href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</A>&gt;<BR>Subject: Re: [OMPI users] How to use Multiple links with<BR>&nbsp;&nbsp;&nbsp; OpenMPI??????????????????<BR>To: "Open MPI Users" &lt;<A href="mailto:users@open-mpi.org">users@open-mpi.org</A>&gt;<BR>Message-ID: &lt;<A href="mailto:8864ED55-66A8-424E-B1B9-249F033816DE@cisco.com">8864ED55-66A8-424E-B1B9-249F033816DE@cisco.com</A>&gt;<BR>Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes<BR><BR>Open MPI considers hosts differently than network links.<BR><BR>So you should only list the actual hostname in the hostfile, with&nbsp; <BR>slots equal to the number of processors (4 in your case, I think?).<BR><BR>Once the MPI processes are launched, they each look around on the host&nbsp; <BR>that they're running and find network paths to each of their peers.&nbsp; <BR>If they are multiple paths between pairs of peers, Open MPI will round- <BR>robin stripe messages across each of the links.&nbsp; We don't really have&nbsp; <BR>an easy setting for each peer pair only using 1 link.&nbsp; Indeed, since&nbsp; <BR>connectivity is bidirectional, the traffic patterns become less&nbsp; <BR>obvious if you want MPI_COMM_WORLD rank X to only use link Y -- what&nbsp; <BR>does that mean to the other 4 MPI processes on the other host (with&nbsp; <BR>whom you have assumedly assigned their own individual links as well)?<BR><BR><BR>On May 26, 2009, at 12:24 AM, shan axida wrote:<BR><BR>&gt; Hi everyone,<BR>&gt; I want to ask how to use multiple links (multiple NICs) with OpenMPI.<BR>&gt; For example, how can I assign a link to each process, if there are 4&nbsp; <BR>&gt; links<BR>&gt; and 4 processors on each node in our cluster?<BR>&gt; Is this a correct way?<BR>&gt; hostfile:<BR>&gt; ----------------------<BR>&gt; host1-eth0 slots=1<BR>&gt; host1-eth1 slots=1<BR>&gt; host1-eth2 slots=1<BR>&gt; host1-eth3 slots=1<BR>&gt; host2-eth0 slots=1<BR>&gt; host2-eth1 slots=1<BR>&gt; host2-eth2 slots=1<BR>&gt; host2-eth3 slots=1<BR>&gt; ...&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ...<BR>&gt; ...&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; ...<BR>&gt; host16-eth0 slots=1<BR>&gt; host16-eth1 slots=1<BR>&gt; host16-eth2 slots=1<BR>&gt; host16-eth3 slots=1<BR>&gt; ------------------------<BR>&gt;<BR>&gt;<BR>&gt;<BR>&gt;<BR>&gt;<BR>&gt;<BR>&gt;<BR>&gt;<BR>&gt;<BR>&gt;<BR>&gt; _______________________________________________<BR>&gt; users mailing list<BR>&gt; <A href="mailto:users@open-mpi.org">users@open-mpi.org</A><BR>&gt; <A href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target=_blank>http://www.open-mpi.org/mailman/listinfo.cgi/users</A><BR><BR><BR>-- <BR>Jeff Squyres<BR>Cisco Systems<BR><BR><BR><BR>------------------------------<BR><BR>_______________________________________________<BR>users mailing list<BR><A href="mailto:users@open-mpi.org">users@open-mpi.org</A><BR><A href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target=_blank>http://www.open-mpi.org/mailman/listinfo.cgi/users</A><BR><BR>End of users Digest, Vol 1242, Issue 1<BR>**************************************<BR></DIV></DIV></DIV><BR></DIV></BODY></HTML>
