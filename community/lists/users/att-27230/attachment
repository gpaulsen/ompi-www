<html dir="ltr">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<style id="owaParaStyle" type="text/css">P {margin-top:0;margin-bottom:0;}</style>
</head>
<body ocsi="0" fpstyle="1">
<div style="direction: ltr;font-family: Tahoma;color: #000000;font-size: 10pt;">Howard,<br>
<br>
I have one more question. Is it possible to use MPI_Comm_spawn when <br>
launching an OpenMPI job with aprun? I'm getting this error when I try:<br>
<br>
nradclif@kay:/lus/scratch/nradclif&gt; aprun -n 1 -N 1 ./manager<br>
[nid00036:21772] [[14952,0],0] ORTE_ERROR_LOG: Not available in file dpm_orte.c at line 1190<br>
[36:21772] *** An error occurred in MPI_Comm_spawn<br>
[36:21772] *** reported by process [979894272,0]<br>
[36:21772] *** on communicator MPI_COMM_SELF<br>
[36:21772] *** MPI_ERR_UNKNOWN: unknown error<br>
[36:21772] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>
[36:21772] ***&nbsp;&nbsp;&nbsp; and potentially your MPI job)<br>
aborting job:<br>
N/A<br>
<br>
<div><br>
<div style="font-family:Tahoma; font-size:13px"><font size="2"><span style="font-size:10pt"><font size="2">Nick Radcliffe</font><br>
Software Engineer<br>
Cray, Inc.</span></font></div>
</div>
<div style="font-family: Times New Roman; color: #000000; font-size: 16px">
<hr tabindex="-1">
<div style="direction: ltr;" id="divRpF610822"><font color="#000000" face="Tahoma" size="2"><b>From:</b> users [users-bounces@open-mpi.org] on behalf of Howard Pritchard [hppritcha@gmail.com]<br>
<b>Sent:</b> Thursday, June 25, 2015 11:00 PM<br>
<b>To:</b> Open MPI Users<br>
<b>Subject:</b> Re: [OMPI users] Running with native ugni on a Cray XC<br>
</font><br>
</div>
<div></div>
<div>
<div dir="ltr">Hi Nick,
<div><br>
</div>
<div>I will endeavor to put together a wiki for the master/v2.x series specific to Cray systems</div>
<div>(sans those customers who choose to neither 1) use Cray supported eslogin</div>
<div>setup nor 2) &nbsp;permit users to directly log in to and build apps on service nodes) &nbsp;that explains best practices for&nbsp;</div>
<div>using Open MPI on Cray XE/XK/XC systems.</div>
<div><br>
</div>
<div>A significant &nbsp;amount of work went in to master, and now the v2.x release</div>
<div>stream to rationalize support for Open MPI on Cray XE/XK/XC systems using either aprun&nbsp;</div>
<div>or native slurm launch.</div>
<div><br>
</div>
<div>General advice for all on this mailing list, do not use the Open MPI 1.8.X release</div>
<div>series with direct ugni access enabled on Cray XE/XK/XC .&nbsp; Rather use master, or as soon as</div>
<div>a release is available, from v2.x. &nbsp; Note that if you are using CCM, &nbsp;the performance</div>
<div>of Open MPI 1.8.X over the Cray IAA (simulated ibverbs) is pretty good.&nbsp; I suggest this</div>
<div>as the preferred route for using the 1.8.X release stream on Cray XE/XK/XC.</div>
<div><br>
</div>
<div>Howard</div>
<div><br>
</div>
</div>
<div class="gmail_extra"><br>
<div class="gmail_quote">2015-06-25 19:35 GMT-06:00 Nick Radcliffe <span dir="ltr">
&lt;<a href="mailto:nradclif@cray.com" target="_blank">nradclif@cray.com</a>&gt;</span>:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex; border-left:1px #ccc solid; padding-left:1ex">
<div>
<div style="direction:ltr; font-family:Tahoma; color:#000000; font-size:10pt">Thanks Howard, using master worked for me.<br>
<div><br>
<div style="font-family:Tahoma; font-size:13px"><font size="2"><span style="font-size:10pt"><font size="2">Nick Radcliffe</font><br>
Software Engineer<br>
Cray, Inc.</span></font></div>
</div>
<div style="font-family:Times New Roman; color:#000000; font-size:16px">
<hr>
<div style="direction:ltr"><font color="#000000" face="Tahoma" size="2"><b>From:</b> users [<a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a>] on behalf of Howard Pritchard [<a href="mailto:hppritcha@gmail.com" target="_blank">hppritcha@gmail.com</a>]<br>
<b>Sent:</b> Thursday, June 25, 2015 5:11 PM<br>
<b>To:</b> Open MPI Users<br>
<b>Subject:</b> Re: [OMPI users] Running with native ugni on a Cray XC<br>
</font><br>
</div>
<div></div>
<div>
<p dir="ltr">Hi Nick</p>
<p dir="ltr">use master not 1.8.x. for cray xc.&nbsp; also for config do not pay attention to cray/lanl platform files.&nbsp; just do config.&nbsp; also if using nativized slurm launch with srun not mpirun.</p>
<p dir="ltr">howard</p>
<p dir="ltr">----------</p>
<p dir="ltr">sent from my smart phonr so no good type.</p>
<p dir="ltr">Howard</p>
<div class="gmail_quote">On Jun 25, 2015 2:56 PM, &quot;Nick Radcliffe&quot; &lt;<a href="mailto:nradclif@cray.com" target="_blank">nradclif@cray.com</a>&gt; wrote:<br type="attribution">
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex; border-left:1px #ccc solid; padding-left:1ex">
Hi,<br>
<br>
I'm trying to build and run Open MPI 1.8.5 with native ugni on a Cray XC. The build works, but I'm getting this error when I run:<br>
<br>
nradclif@kay:/lus/scratch/nradclif&gt; aprun -n 2 -N 1 ./osu_latency<br>
[nid00014:28784] [db_pmi.c:174:pmi_commit_packed] PMI_KVS_Put: Operation failed<br>
[nid00014:28784] [db_pmi.c:457:commit] PMI_KVS_Commit: Operation failed<br>
[nid00012:12788] [db_pmi.c:174:pmi_commit_packed] PMI_KVS_Put: Operation failed<br>
[nid00012:12788] [db_pmi.c:457:commit] PMI_KVS_Commit: Operation failed<br>
# OSU MPI Latency Test<br>
# Size&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Latency (us)<br>
osu_latency: btl_ugni_endpoint.c:87: mca_btl_ugni_ep_connect_start: Assertion `0' failed.<br>
[nid00012:12788] *** Process received signal ***<br>
[nid00012:12788] Signal: Aborted (6)<br>
[nid00012:12788] Signal code:&nbsp; (-6)<br>
[nid00012:12788] [ 0] /lib64/libpthread.so.0(&#43;0xf850)[0x2aaaab42b850]<br>
[nid00012:12788] [ 1] /lib64/libc.so.6(gsignal&#43;0x35)[0x2aaaab66b885]<br>
[nid00012:12788] [ 2] /lib64/libc.so.6(abort&#43;0x181)[0x2aaaab66ce61]<br>
[nid00012:12788] [ 3] /lib64/libc.so.6(__assert_fail&#43;0xf0)[0x2aaaab664740]<br>
[nid00012:12788] [ 4] /lus/scratch/nradclif/openmpi_install/lib/libmpi.so.1(mca_btl_ugni_ep_connect_progress&#43;0x6c9)[0x2aaaaaff9869]<br>
[nid00012:12788] [ 5] /lus/scratch/nradclif/openmpi_install/lib/libmpi.so.1(&#43;0x5ae32)[0x2aaaaaf46e32]<br>
[nid00012:12788] [ 6] /lus/scratch/nradclif/openmpi_install/lib/libmpi.so.1(mca_btl_ugni_sendi&#43;0x8bd)[0x2aaaaaffaf7d]<br>
[nid00012:12788] [ 7] /lus/scratch/nradclif/openmpi_install/lib/libmpi.so.1(&#43;0x1f0c17)[0x2aaaab0dcc17]<br>
[nid00012:12788] [ 8] /lus/scratch/nradclif/openmpi_install/lib/libmpi.so.1(mca_pml_ob1_isend&#43;0xa8)[0x2aaaab0dd488]<br>
[nid00012:12788] [ 9] /lus/scratch/nradclif/openmpi_install/lib/libmpi.so.1(ompi_coll_tuned_barrier_intra_two_procs&#43;0x11b)[0x2aaaab07e84b]<br>
[nid00012:12788] [10] /lus/scratch/nradclif/openmpi_install/lib/libmpi.so.1(PMPI_Barrier&#43;0xb6)[0x2aaaaaf8a7c6]<br>
[nid00012:12788] [11] ./osu_latency[0x401114]<br>
[nid00012:12788] [12] /lib64/libc.so.6(__libc_start_main&#43;0xe6)[0x2aaaab657c36]<br>
[nid00012:12788] [13] ./osu_latency[0x400dd9]<br>
[nid00012:12788] *** End of error message ***<br>
osu_latency: btl_ugni_endpoint.c:87: mca_btl_ugni_ep_connect_start: Assertion `0' failed.<br>
<br>
<br>
Here's how I build:<br>
<br>
export FC=ftn&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(I'm not using Fortran, but the configure fails if it can't find a Fortran compiler)<br>
./configure --prefix=/lus/scratch/nradclif/openmpi_install --enable-mpi-fortran=none --with-platform=contrib/platform/lanl/cray_xe6/debug-lustre<br>
make install<br>
<br>
I didn't modify the debug-lustre file, but I did change cray-common to remove the hard-coding, e.g., rather than using the gemini-specific path &quot;with_pmi=/opt/cray/pmi/2.1.4-1.0000.8596.8.9.gem&quot;, I used &quot;with_pmi=/opt/cray/pmi/default&quot;.<br>
<br>
I've tried running different executables with different numbers of ranks/nodes, but they all seem to run into problems with PMI_KVS_Put.<br>
<br>
Any ideas what could be going wrong?<br>
<br>
Thanks for any help,<br>
Nick<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/06/27197.php" rel="noreferrer" target="_blank">
http://www.open-mpi.org/community/lists/users/2015/06/27197.php</a><br>
</blockquote>
</div>
</div>
</div>
</div>
</div>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/06/27199.php" rel="noreferrer" target="_blank">
http://www.open-mpi.org/community/lists/users/2015/06/27199.php</a><br>
</blockquote>
</div>
<br>
</div>
</div>
</div>
</div>
</body>
</html>

