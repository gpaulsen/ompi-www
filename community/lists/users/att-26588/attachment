<div dir="ltr">The warning about binding to memory is due to not having numactl-devel installed on the system. The job would still run, but we are warning you that we cannot bind memory to the same domain as the core where we bind the process. Can cause poor performance, but not fatal. I forget the name of the param, but you can tell us to &quot;shut up&quot; :-)<div><br></div><div>The other warning/error indicates that we aren&#39;t seeing enough cores on the allocation you gave us via the hostile to support one proc/core - i.e., we didn&#39;t at least 128 cores in the sum of the nodes you told us about. I take it you were expecting that there were that many or more?</div><div><br></div><div>Ralph</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Wed, Apr 1, 2015 at 12:54 AM, Lane, William <span dir="ltr">&lt;<a href="mailto:William.Lane@cshs.org" target="_blank">William.Lane@cshs.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">




<div>
<div style="direction:ltr;font-family:Tahoma;color:#000000;font-size:10pt">I&#39;m having problems running OpenMPI jobs<br>
(using a hostfile) on an HPC cluster running<br>
ROCKS on CentOS 6.3. I&#39;m running OpenMPI<br>
outside of Sun Grid Engine (i.e. it is not submitted<br>
as a job to SGE). The program being run is a LAPACK<br>
benchmark. The commandline parameter I&#39;m <br>
using to run the jobs is:<br>
<br>
$MPI_DIR/bin/mpirun -np $NSLOTS -bind-to-core -report-bindings --hostfile hostfile --mca btl_tcp_if_include eth0 --prefix $MPI_DIR $BENCH_DIR/$APP_DIR/$APP_BIN<br>
<br>
Where MPI_DIR=/hpc/apps/mpi/openmpi/1.8.2/<br>
NSLOTS=128<br>
<br>
I&#39;m getting errors of the form and OpenMPI never runs the LAPACK benchmark:<br>
<br>
   --------------------------------------------------------------------------<br>
   WARNING: a request was made to bind a process. While the system<br>
   supports binding the process itself, at least one node does NOT<br>
   support binding memory to the process location.<br>
<br>
    Node:  csclprd3-0-11<br>
<br>
   This usually is due to not having the required NUMA support installed<br>
   on the node. In some Linux distributions, the required support is<br>
   contained in the libnumactl and libnumactl-devel packages.<br>
   This is a warning only; your job will continue, though performance may be degraded.<br>
   --------------------------------------------------------------------------<br>
<br>
   --------------------------------------------------------------------------<br>
   A request was made to bind to that would result in binding more<br>
   processes than cpus on a resource:<br>
<br>
      Bind to:     CORE<br>
      Node:        csclprd3-0-11<br>
      #processes:  2<br>
      #cpus:       1<br>
<br>
   You can override this protection by adding the &quot;overload-allowed&quot;<br>
   option to your binding directive.<br>
   --------------------------------------------------------------------------<br>
<br>
The only installed numa packages are:<br>
numactl.x86_64                                                2.0.7-3.el6                        @centos6.3-x86_64-0/$<br>
<br>
When I search for the available NUMA packages I find:<br>
<br>
yum search numa | less<br>
<br>
        Loaded plugins: fastestmirror<br>
        Loading mirror speeds from cached hostfile<br>
        ============================== N/S Matched: numa ===============================<br>
        numactl-devel.i686 : Development package for building Applications that use numa<br>
        numactl-devel.x86_64 : Development package for building Applications that use<br>
                             : numa<br>
        numad.x86_64 : NUMA user daemon<br>
        numactl.i686 : Library for tuning for Non Uniform Memory Access machines<br>
        numactl.x86_64 : Library for tuning for Non Uniform Memory Access machines<br>
<br>
Do I need to install additional and/or different NUMA packages in order to get OpenMPI to work<br>
on this cluster?<br>
<br>
-Bill Lane<br>
</div>
IMPORTANT WARNING: This message is intended for the use of the person or entity to which it is addressed and may contain information that is privileged and confidential, the disclosure of which is governed by applicable law. If the reader of this message is
 not the intended recipient, or the employee or agent responsible for delivering it to the intended recipient, you are hereby notified that any dissemination, distribution or copying of this information is strictly prohibited. Thank you for your cooperation.
</div>

<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Searchable archives: <a href="http://www.open-mpi.org/community/lists/users/2015/04/index.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/04/index.php</a><br></blockquote></div><br></div>

