<div dir="ltr">A ha! The Gurus know all. The map-by was the magic sauce:<div><br></div><div><div><font face="monospace, monospace">(1176) $ env OMP_NUM_THREADS=7 KMP_AFFINITY=compact mpirun -np 4 -map-by ppr:2:socket:pe=7 ./hello-hybrid.x | sort -g -k 18</font></div><div><font face="monospace, monospace">Hello from thread 0 out of 7 from process 0 out of 4 on borgo035 on CPU 0</font></div><div><font face="monospace, monospace">Hello from thread 1 out of 7 from process 0 out of 4 on borgo035 on CPU 1</font></div><div><font face="monospace, monospace">Hello from thread 2 out of 7 from process 0 out of 4 on borgo035 on CPU 2</font></div><div><font face="monospace, monospace">Hello from thread 3 out of 7 from process 0 out of 4 on borgo035 on CPU 3</font></div><div><font face="monospace, monospace">Hello from thread 4 out of 7 from process 0 out of 4 on borgo035 on CPU 4</font></div><div><font face="monospace, monospace">Hello from thread 5 out of 7 from process 0 out of 4 on borgo035 on CPU 5</font></div><div><font face="monospace, monospace">Hello from thread 6 out of 7 from process 0 out of 4 on borgo035 on CPU 6</font></div><div><font face="monospace, monospace">Hello from thread 0 out of 7 from process 1 out of 4 on borgo035 on CPU 7</font></div><div><font face="monospace, monospace">Hello from thread 1 out of 7 from process 1 out of 4 on borgo035 on CPU 8</font></div><div><font face="monospace, monospace">Hello from thread 2 out of 7 from process 1 out of 4 on borgo035 on CPU 9</font></div><div><font face="monospace, monospace">Hello from thread 3 out of 7 from process 1 out of 4 on borgo035 on CPU 10</font></div><div><font face="monospace, monospace">Hello from thread 4 out of 7 from process 1 out of 4 on borgo035 on CPU 11</font></div><div><font face="monospace, monospace">Hello from thread 5 out of 7 from process 1 out of 4 on borgo035 on CPU 12</font></div><div><font face="monospace, monospace">Hello from thread 6 out of 7 from process 1 out of 4 on borgo035 on CPU 13</font></div><div><font face="monospace, monospace">Hello from thread 0 out of 7 from process 2 out of 4 on borgo035 on CPU 14</font></div><div><font face="monospace, monospace">Hello from thread 1 out of 7 from process 2 out of 4 on borgo035 on CPU 15</font></div><div><font face="monospace, monospace">Hello from thread 2 out of 7 from process 2 out of 4 on borgo035 on CPU 16</font></div><div><font face="monospace, monospace">Hello from thread 3 out of 7 from process 2 out of 4 on borgo035 on CPU 17</font></div><div><font face="monospace, monospace">Hello from thread 4 out of 7 from process 2 out of 4 on borgo035 on CPU 18</font></div><div><font face="monospace, monospace">Hello from thread 5 out of 7 from process 2 out of 4 on borgo035 on CPU 19</font></div><div><font face="monospace, monospace">Hello from thread 6 out of 7 from process 2 out of 4 on borgo035 on CPU 20</font></div><div><font face="monospace, monospace">Hello from thread 0 out of 7 from process 3 out of 4 on borgo035 on CPU 21</font></div><div><font face="monospace, monospace">Hello from thread 1 out of 7 from process 3 out of 4 on borgo035 on CPU 22</font></div><div><font face="monospace, monospace">Hello from thread 2 out of 7 from process 3 out of 4 on borgo035 on CPU 23</font></div><div><font face="monospace, monospace">Hello from thread 3 out of 7 from process 3 out of 4 on borgo035 on CPU 24</font></div><div><font face="monospace, monospace">Hello from thread 4 out of 7 from process 3 out of 4 on borgo035 on CPU 25</font></div><div><font face="monospace, monospace">Hello from thread 5 out of 7 from process 3 out of 4 on borgo035 on CPU 26</font></div><div><font face="monospace, monospace">Hello from thread 6 out of 7 from process 3 out of 4 on borgo035 on CPU 27</font></div></div><div><br></div><div>So, a question: what does &quot;ppr&quot; mean? The man page seems to accept it as an axiom of Open MPI:</div><div><br></div><div><div>       --map-by &lt;foo&gt;</div><div>              Map  to  the specified object, defaults to socket. Supported options include slot, hwthread, core, L1cache, L2cache, L3cache, socket, numa,</div><div>              board, node, sequential, distance, and ppr. Any object can include modifiers by adding a : and any combination of PE=n (bind  n  processing</div><div>              elements  to  each  proc), SPAN (load balance the processes across the allocation), OVERSUBSCRIBE (allow more processes on a node than pro‐</div><div>              cessing elements), and NOOVERSUBSCRIBE.  This includes PPR, where the pattern would be terminated by another colon to separate it from  the</div><div>              modifiers.</div></div><div><br></div><div>Is it an acronym/initialism? From some experimenting it seems to be ppr:2:socket means 2 processes per socket? And pe=7 means leave 7 processes between them? Is that about right?</div><div><br></div><div>Matt</div></div><div class="gmail_extra"><br><div class="gmail_quote">On Wed, Jan 6, 2016 at 3:19 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">I believe he wants two procs/socket, so you’d need ppr:2:socket:pe=7<div><br></div><div><br><div><blockquote type="cite"><div><div class="h5"><div>On Jan 6, 2016, at 12:14 PM, Nick Papior &lt;<a href="mailto:nickpapior@gmail.com" target="_blank">nickpapior@gmail.com</a>&gt; wrote:</div><br></div></div><div><div><div class="h5"><div dir="ltr">I do not think KMP_AFFINITY should affect anything in OpenMPI, it is an MKL env setting? Or am I wrong?<div><br></div><div>Note that these are used in an environment where openmpi automatically gets the host-file. Hence they are not present.</div><div>With intel mkl and openmpi I got the best performance using these, rather long flags:</div><div><br></div><div><div>export KMP_AFFINITY=verbose,compact,granularity=core<br></div><div>export KMP_STACKSIZE=62M</div><div>export KMP_SETTINGS=1</div><div><br></div><div>def_flags=&quot;--bind-to core -x OMP_PROC_BIND=true --report-bindings&quot;</div><div>def_flags=&quot;$def_flags -x KMP_AFFINITY=$KMP_AFFINITY&quot;</div><div><br></div><div># in your case 7:</div><div>ONP=7</div><div>flags=&quot;$def_flags -x MKL_NUM_THREADS=$ONP -x MKL_DYNAMIC=FALSE&quot;<br></div><div>flags=&quot;$flags -x OMP_NUM_THREADS=$ONP -x OMP_DYNAMIC=FALSE&quot;</div><div>flags=&quot;$flags -x KMP_STACKSIZE=$KMP_STACKSIZE&quot;</div><div>flags=&quot;$flags --map-by ppr:1:socket:pe=7&quot;</div></div><div><br></div><div>then run your program:</div><div><br></div><div>mpirun $flags &lt;app&gt; </div><div><br></div><div>A lot of the option flags are duplicated (and strictly not needed), but I provide them for easy testing changes.</div><div>Surely this is application dependent, but for my case it was performing really well. </div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">2016-01-06 20:48 GMT+01:00 Erik Schnetter <span dir="ltr">&lt;<a href="mailto:schnetter@gmail.com" target="_blank">schnetter@gmail.com</a>&gt;</span>:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Setting KMP_AFFINITY will probably override anything that OpenMPI<br>
sets. Can you try without?<br>
<br>
-erik<br>
<div><div><br>
On Wed, Jan 6, 2016 at 2:46 PM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com" target="_blank">fortran@gmail.com</a>&gt; wrote:<br>
&gt; Hello Open MPI Gurus,<br>
&gt;<br>
&gt; As I explore MPI-OpenMP hybrid codes, I&#39;m trying to figure out how to do<br>
&gt; things to get the same behavior in various stacks. For example, I have a<br>
&gt; 28-core node (2 14-core Haswells), and I&#39;d like to run 4 MPI processes and 7<br>
&gt; OpenMP threads. Thus, I&#39;d like the processes to be 2 processes per socket<br>
&gt; with the OpenMP threads laid out on them. Using a &quot;hybrid Hello World&quot;<br>
&gt; program, I can achieve this with Intel MPI (after a lot of testing):<br>
&gt;<br>
&gt; (1097) $ env OMP_NUM_THREADS=7 KMP_AFFINITY=compact mpirun -np 4<br>
&gt; ./hello-hybrid.x | sort -g -k 18<br>
&gt; srun.slurm: cluster configuration lacks support for cpu binding<br>
&gt; Hello from thread 0 out of 7 from process 2 out of 4 on borgo035 on CPU 0<br>
&gt; Hello from thread 1 out of 7 from process 2 out of 4 on borgo035 on CPU 1<br>
&gt; Hello from thread 2 out of 7 from process 2 out of 4 on borgo035 on CPU 2<br>
&gt; Hello from thread 3 out of 7 from process 2 out of 4 on borgo035 on CPU 3<br>
&gt; Hello from thread 4 out of 7 from process 2 out of 4 on borgo035 on CPU 4<br>
&gt; Hello from thread 5 out of 7 from process 2 out of 4 on borgo035 on CPU 5<br>
&gt; Hello from thread 6 out of 7 from process 2 out of 4 on borgo035 on CPU 6<br>
&gt; Hello from thread 0 out of 7 from process 3 out of 4 on borgo035 on CPU 7<br>
&gt; Hello from thread 1 out of 7 from process 3 out of 4 on borgo035 on CPU 8<br>
&gt; Hello from thread 2 out of 7 from process 3 out of 4 on borgo035 on CPU 9<br>
&gt; Hello from thread 3 out of 7 from process 3 out of 4 on borgo035 on CPU 10<br>
&gt; Hello from thread 4 out of 7 from process 3 out of 4 on borgo035 on CPU 11<br>
&gt; Hello from thread 5 out of 7 from process 3 out of 4 on borgo035 on CPU 12<br>
&gt; Hello from thread 6 out of 7 from process 3 out of 4 on borgo035 on CPU 13<br>
&gt; Hello from thread 0 out of 7 from process 0 out of 4 on borgo035 on CPU 14<br>
&gt; Hello from thread 1 out of 7 from process 0 out of 4 on borgo035 on CPU 15<br>
&gt; Hello from thread 2 out of 7 from process 0 out of 4 on borgo035 on CPU 16<br>
&gt; Hello from thread 3 out of 7 from process 0 out of 4 on borgo035 on CPU 17<br>
&gt; Hello from thread 4 out of 7 from process 0 out of 4 on borgo035 on CPU 18<br>
&gt; Hello from thread 5 out of 7 from process 0 out of 4 on borgo035 on CPU 19<br>
&gt; Hello from thread 6 out of 7 from process 0 out of 4 on borgo035 on CPU 20<br>
&gt; Hello from thread 0 out of 7 from process 1 out of 4 on borgo035 on CPU 21<br>
&gt; Hello from thread 1 out of 7 from process 1 out of 4 on borgo035 on CPU 22<br>
&gt; Hello from thread 2 out of 7 from process 1 out of 4 on borgo035 on CPU 23<br>
&gt; Hello from thread 3 out of 7 from process 1 out of 4 on borgo035 on CPU 24<br>
&gt; Hello from thread 4 out of 7 from process 1 out of 4 on borgo035 on CPU 25<br>
&gt; Hello from thread 5 out of 7 from process 1 out of 4 on borgo035 on CPU 26<br>
&gt; Hello from thread 6 out of 7 from process 1 out of 4 on borgo035 on CPU 27<br>
&gt;<br>
&gt; Other than the odd fact that Process #0 seemed to start on Socket #1 (this<br>
&gt; might be an artifact of how I&#39;m trying to detect the CPU I&#39;m on), this looks<br>
&gt; reasonable. 14 threads on each socket and each process is laying out its<br>
&gt; threads in a nice orderly fashion.<br>
&gt;<br>
&gt; I&#39;m trying to figure out how to do this with Open MPI (version 1.10.0) and<br>
&gt; apparently I am just not quite good enough to figure it out. The closest<br>
&gt; I&#39;ve gotten is:<br>
&gt;<br>
&gt; (1155) $ env OMP_NUM_THREADS=7 KMP_AFFINITY=compact mpirun -np 4 -map-by<br>
&gt; ppr:2:socket ./hello-hybrid.x | sort -g -k 18<br>
&gt; Hello from thread 0 out of 7 from process 0 out of 4 on borgo035 on CPU 0<br>
&gt; Hello from thread 0 out of 7 from process 1 out of 4 on borgo035 on CPU 0<br>
&gt; Hello from thread 1 out of 7 from process 0 out of 4 on borgo035 on CPU 1<br>
&gt; Hello from thread 1 out of 7 from process 1 out of 4 on borgo035 on CPU 1<br>
&gt; Hello from thread 2 out of 7 from process 0 out of 4 on borgo035 on CPU 2<br>
&gt; Hello from thread 2 out of 7 from process 1 out of 4 on borgo035 on CPU 2<br>
&gt; Hello from thread 3 out of 7 from process 0 out of 4 on borgo035 on CPU 3<br>
&gt; Hello from thread 3 out of 7 from process 1 out of 4 on borgo035 on CPU 3<br>
&gt; Hello from thread 4 out of 7 from process 0 out of 4 on borgo035 on CPU 4<br>
&gt; Hello from thread 4 out of 7 from process 1 out of 4 on borgo035 on CPU 4<br>
&gt; Hello from thread 5 out of 7 from process 0 out of 4 on borgo035 on CPU 5<br>
&gt; Hello from thread 5 out of 7 from process 1 out of 4 on borgo035 on CPU 5<br>
&gt; Hello from thread 6 out of 7 from process 0 out of 4 on borgo035 on CPU 6<br>
&gt; Hello from thread 6 out of 7 from process 1 out of 4 on borgo035 on CPU 6<br>
&gt; Hello from thread 0 out of 7 from process 2 out of 4 on borgo035 on CPU 14<br>
&gt; Hello from thread 0 out of 7 from process 3 out of 4 on borgo035 on CPU 14<br>
&gt; Hello from thread 1 out of 7 from process 2 out of 4 on borgo035 on CPU 15<br>
&gt; Hello from thread 1 out of 7 from process 3 out of 4 on borgo035 on CPU 15<br>
&gt; Hello from thread 2 out of 7 from process 2 out of 4 on borgo035 on CPU 16<br>
&gt; Hello from thread 2 out of 7 from process 3 out of 4 on borgo035 on CPU 16<br>
&gt; Hello from thread 3 out of 7 from process 2 out of 4 on borgo035 on CPU 17<br>
&gt; Hello from thread 3 out of 7 from process 3 out of 4 on borgo035 on CPU 17<br>
&gt; Hello from thread 4 out of 7 from process 2 out of 4 on borgo035 on CPU 18<br>
&gt; Hello from thread 4 out of 7 from process 3 out of 4 on borgo035 on CPU 18<br>
&gt; Hello from thread 5 out of 7 from process 2 out of 4 on borgo035 on CPU 19<br>
&gt; Hello from thread 5 out of 7 from process 3 out of 4 on borgo035 on CPU 19<br>
&gt; Hello from thread 6 out of 7 from process 2 out of 4 on borgo035 on CPU 20<br>
&gt; Hello from thread 6 out of 7 from process 3 out of 4 on borgo035 on CPU 20<br>
&gt;<br>
&gt; Obviously not right. Any ideas on how to help me learn? The man mpirun page<br>
&gt; is a bit formidable in the pinning part, so maybe I&#39;ve missed an obvious<br>
&gt; answer.<br>
&gt;<br>
&gt; Matt<br>
&gt; --<br>
&gt; Matt Thompson<br>
&gt;<br>
&gt; Man Among Men<br>
&gt; Fulcrum of History<br>
&gt;<br>
&gt;<br>
</div></div>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post:<br>
&gt; <a href="http://www.open-mpi.org/community/lists/users/2016/01/28217.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/01/28217.php</a><br>
<span><font color="#888888"><br>
<br>
<br>
--<br>
Erik Schnetter &lt;<a href="mailto:schnetter@gmail.com" target="_blank">schnetter@gmail.com</a>&gt;<br>
<a href="http://www.perimeterinstitute.ca/personal/eschnetter/" rel="noreferrer" target="_blank">http://www.perimeterinstitute.ca/personal/eschnetter/</a><br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/01/28218.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/01/28218.php</a><br>
</font></span></blockquote></div><br><br clear="all"><div><br></div>-- <br><div><div dir="ltr"><div>Kind regards Nick</div></div></div>
</div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/01/28219.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/01/28219.php</a></div></blockquote></div><br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/01/28221.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/01/28221.php</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br><div class="gmail_signature"><div dir="ltr"><div><div dir="ltr"><div>Matt Thompson</div></div></div><blockquote style="margin:0 0 0 40px;border:none;padding:0px"><div><div><div>Man Among Men</div></div></div><div><div><div>Fulcrum of History</div></div></div></blockquote></div></div>
</div>

