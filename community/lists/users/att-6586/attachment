<html><body>
<p>Wow.  I am indeed on IB.<br>
<br>
So a program that calls an MPI_Bcast, then does a bunch of setup work that should be done in parallel before re-synchronizing, in fact serializes the setup work?  I see its not quite that bad - If I run my little program on 5 nodes, I get 0 immediately, 1,2 and 4 after 5 seconds and 3 after 10, revealing, I guess, the tree distribution.<br>
<br>
Ticket 1224 isn't terribly clear - is this patch already in 1.2.6 or 1.2.7, or do I have to download source, patch and build?<br>
<br>
Greg<br>
<br>
<br>
<img width="16" height="16" src="cid:1__=0ABBFE54DFCB47718f9e8a93df938@us.ibm.com" border="0" alt="Inactive hide details for Jeff Squyres ---09/17/2008 12:03:21 PM---Are you using IB, perchance?"><font color="#424282">Jeff Squyres ---09/17/2008 12:03:21 PM---Are you using IB, perchance?</font><br>
<br>

<table width="100%" border="0" cellspacing="0" cellpadding="0">
<tr valign="top"><td style="background-image:url(cid:2__=0ABBFE54DFCB47718f9e8a93df938@us.ibm.com); background-repeat: no-repeat; " width="40%">
<ul>
<ul>
<ul>
<ul><b><font size="2">Jeff Squyres &lt;jsquyres@cisco.com&gt;</font></b><font size="2"> </font><br>
<font size="2">Sent by: users-bounces@open-mpi.org</font>
<p><font size="2">09/17/08 11:55 AM</font>
<table border="1">
<tr valign="top"><td width="168" bgcolor="#FFFFFF"><div align="center"><font size="2">Please respond to<br>
Open MPI Users &lt;users@open-mpi.org&gt;</font></div></td></tr>
</table>
</ul>
</ul>
</ul>
</ul>
</td><td width="60%">
<table width="100%" border="0" cellspacing="0" cellpadding="0">
<tr valign="top"><td width="1%"><img width="58" height="1" src="cid:3__=0ABBFE54DFCB47718f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<div align="right"><font size="2">To</font></div></td><td width="100%"><img width="1" height="1" src="cid:3__=0ABBFE54DFCB47718f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<font size="2">Open MPI Users &lt;users@open-mpi.org&gt;</font></td></tr>

<tr valign="top"><td width="1%"><img width="58" height="1" src="cid:3__=0ABBFE54DFCB47718f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<div align="right"><font size="2">cc</font></div></td><td width="100%"><img width="1" height="1" src="cid:3__=0ABBFE54DFCB47718f9e8a93df938@us.ibm.com" border="0" alt=""><br>
</td></tr>

<tr valign="top"><td width="1%"><img width="58" height="1" src="cid:3__=0ABBFE54DFCB47718f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<div align="right"><font size="2">Subject</font></div></td><td width="100%"><img width="1" height="1" src="cid:3__=0ABBFE54DFCB47718f9e8a93df938@us.ibm.com" border="0" alt=""><br>
<font size="2">Re: [OMPI users] Odd MPI_Bcast behavior</font></td></tr>
</table>

<table border="0" cellspacing="0" cellpadding="0">
<tr valign="top"><td width="58"><img width="1" height="1" src="cid:3__=0ABBFE54DFCB47718f9e8a93df938@us.ibm.com" border="0" alt=""></td><td width="336"><img width="1" height="1" src="cid:3__=0ABBFE54DFCB47718f9e8a93df938@us.ibm.com" border="0" alt=""></td></tr>
</table>
</td></tr>
</table>
<br>
<tt>Are you using IB, perchance?<br>
<br>
We have an &quot;early completion&quot; optimization in the 1.2 series that can &nbsp;<br>
cause this kind of behavior. &nbsp;For apps that dip into the MPI layer &nbsp;<br>
frequently, it doesn't matter. &nbsp;But for those that do not dip into the &nbsp;<br>
MPI layer frequently, it can cause delays like this. &nbsp;See </tt><tt><a href="http://www.open-mpi.org/faq/?category=openfabrics#v1.2-use-early-completion">http://www.open-mpi.org/faq/?category=openfabrics#v1.2-use-early-completion</a></tt><tt>&nbsp;<br>
 &nbsp;for a few more details.<br>
<br>
If you're not using IB, let us know.<br>
<br>
<br>
On Sep 17, 2008, at 10:34 AM, Gregory D Abram wrote:<br>
<br>
&gt; I have a little program which initializes, calls MPI_Bcast, prints a &nbsp;<br>
&gt; message, waits five seconds, and finalized. I sure thought that each &nbsp;<br>
&gt; participating process would print the message immediately, then all &nbsp;<br>
&gt; would wait and exit - thats what happens with mvapich 1.0.0. &nbsp;On &nbsp;<br>
&gt; OpenMPI 1.2.5, though, I get the message immediately from proc 0, &nbsp;<br>
&gt; then 5 seconds later, from proc 1, and then 5 seconds later, it &nbsp;<br>
&gt; exits- as if MPI_Finalize on proc 0 flushed the MPI_Bcast. If I add &nbsp;<br>
&gt; a MPI_Barrier after the MPI_Bcast, it works as I'd expect. Is this &nbsp;<br>
&gt; behavior correct? If so, I so I have a bunch of code to change in &nbsp;<br>
&gt; order to work correctly on OpenMPI.<br>
&gt;<br>
&gt; Greg<br>
&gt;<br>
&gt; Here's the code:<br>
&gt;<br>
&gt; #include &lt;stdlib.h&gt;<br>
&gt; #include &lt;stdio.h&gt;<br>
&gt; #include &lt;mpi.h&gt;<br>
&gt;<br>
&gt; main(int argc, char *argv[])<br>
&gt; {<br>
&gt; char hostname[256]; int r, s;<br>
&gt; MPI_Init(&amp;argc, &amp;argv);<br>
&gt;<br>
&gt; gethostname(hostname, sizeof(hostname));<br>
&gt;<br>
&gt; MPI_Comm_rank(MPI_COMM_WORLD, &amp;r);<br>
&gt; MPI_Comm_size(MPI_COMM_WORLD, &amp;s);<br>
&gt;<br>
&gt; fprintf(stderr, &quot;%d of %d: %s\n&quot;, r, s, hostname);<br>
&gt;<br>
&gt; int i = 99999;<br>
&gt; MPI_Bcast(&amp;i, sizeof(i), MPI_UNSIGNED_CHAR, 0, MPI_COMM_WORLD);<br>
&gt; // MPI_Barrier(MPI_COMM_WORLD);<br>
&gt;<br>
&gt; fprintf(stderr, &quot;%d: got it\n&quot;, r);<br>
&gt;<br>
&gt; sleep(5);<br>
&gt;<br>
&gt; MPI_Finalize();<br>
&gt; }<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; users@open-mpi.org<br>
&gt; </tt><tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></tt><tt><br>
<br>
<br>
-- <br>
Jeff Squyres<br>
Cisco Systems<br>
<br>
_______________________________________________<br>
users mailing list<br>
users@open-mpi.org<br>
</tt><tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></tt><tt><br>
</tt><br>
</body></html>

