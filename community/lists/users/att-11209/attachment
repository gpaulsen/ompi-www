<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><div>Hi David,</div><div><br></div>Hmm, your demo is well-chosen and crystal-clear, yet the output is unexpected. &nbsp;I do not see environment vars passed by default here:<div><br></div><div><br></div>login3$ qsub -l nodes=2:ppn=1 -I<div>qsub: waiting for job 34683.mds01 to start<br>qsub: job 34683.mds01 ready<br><br>n102$ mpirun -n 2 -machinefile $PBS_NODEFILE hostname<br>n102<br>n085<br>n102$ mpirun -n 2 -machinefile $PBS_NODEFILE env | grep FOO<br>n102$ export FOO=BAR<br>n102$ mpirun -n 2 -machinefile $PBS_NODEFILE env | grep FOO<br>FOO=BAR<br><div>n102$ type mpirun</div><div>mpirun is hashed (/opt/soft/openmpi-1.3.2-intel10-1/bin/mpirun)</div><div><br></div><div><br></div><div><div>Curious, what do you get upon:</div><div><br></div><div><span class="Apple-tab-span" style="white-space: pre; ">	</span>where mpirun</div><div><br></div></div><div><br><div>I built OpenMPI-1.3.2 here from source with:<div><div><br></div><div>&nbsp;&nbsp; &nbsp;CC=icc &nbsp;CXX=icpc &nbsp;FC=ifort &nbsp;F77=ifort \</div><div>&nbsp;&nbsp; &nbsp;LDFLAGS='-Wl,-z,noexecstack' \</div><div>&nbsp;&nbsp; &nbsp;CFLAGS='-O2 -g -fPIC' \</div><div>&nbsp;&nbsp; &nbsp;CXXFLAGS='-O2 -g&nbsp;-fPIC' \</div><div>&nbsp;&nbsp; &nbsp;FFLAGS='-O2 -g -fPIC' \</div><div>&nbsp;&nbsp; &nbsp;./configure --prefix=$prefix \</div><div>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--with-libnuma=/usr \<br>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--with-openib=/usr \<br>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--with-udapl \<br>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--enable-mpirun-prefix-by-default \<br>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--without-tm<br><br><div><div><br></div><div>I did't find the behavior I saw strange, given that orterun(1) talks only about $OPMI_* and inheritance from the remote shell. &nbsp;It also mentions a&nbsp;"boot MCA module",&nbsp;about which I couldn't find much on <a href="http://open-mpi.org">open-mpi.org</a> - hmm.</div><div><br></div><div><br></div><div>In the meantime, I did find a possible solution,&nbsp;namely, to tell ssh to pass a variable using SendEnv/AcceptEnv. &nbsp;That variable is then seen by and can be interpreted (cautiously) in /etc/profile.d/ scripts. &nbsp;A user could set it in the job file (or even qalter it post submission):</div><div><br></div><div><span class="Apple-tab-span" style="white-space:pre">	</span>#PBS -v VARNAME=foo:bar:baz</div><div><br></div><div>For VARNAME, I think simply "MODULES" or "EXTRAMODULES" could do.</div><div><br></div><div><br></div><div><div>With best regards,</div><div>Michael</div></div><div><br></div><div><br></div><div><br></div><div>On Nov 17, 2009, at 4:29 , David Singleton wrote:</div><blockquote type="cite"><div><font class="Apple-style-span" color="#000000"><br></font>I'm not sure why you dont see Open MPI behaving like other MPI's w.r.t.<br>modules/environment on remote MPI tasks - we do.<br><br>xe:~ &gt; qsub -q express -lnodes=2:ppn=8,walltime=10:00,vmem=2gb -I<br>qsub: waiting for job 376366.xepbs to start<br>qsub: job 376366.xepbs ready<br><br>[dbs900@x27 ~]$ module load openmpi<br>[dbs900@x27 ~]$ mpirun -n 2 --bynode hostname<br>x27<br>x28<br>[dbs900@x27 ~]$ mpirun -n 2 --bynode env | grep FOO<br>[dbs900@x27 ~]$ setenv FOO BAR<br>[dbs900@x27 ~]$ mpirun -n 2 --bynode env | grep FOO<br>FOO=BAR<br>FOO=BAR<br>[dbs900@x27 ~]$ mpirun -n 2 --bynode env | grep amber<br>[dbs900@x27 ~]$ module load amber<br>[dbs900@x27 ~]$ mpirun -n 2 --bynode env | grep amber<br>LOADEDMODULES=openmpi/1.3.3:amber/9<br>PATH=/apps/openmpi/1.3.3/bin:/home/900/dbs900/bin:/bin:/usr/bin::/opt/bin:/usr/X11R6/bin:/opt/pbs/bin:/sbin:/usr/sbin:/apps/amber/9/exe<br>_LMFILES_=/apps/Modules/modulefiles/openmpi/1.3.3:/apps/Modules/modulefiles/amber/9<br>AMBERHOME=/apps/amber/9<br>LOADEDMODULES=openmpi/1.3.3:amber/9<br>PATH=/apps/openmpi/1.3.3/bin:/home/900/dbs900/bin:/bin:/usr/bin:/opt/bin:/usr/X11R6/bin:/opt/pbs/bin:/sbin:/usr/sbin:/apps/amber/9/exe<br>_LMFILES_=/apps/Modules/modulefiles/openmpi/1.3.3:/apps/Modules/modulefiles/amber/9<br>AMBERHOME=/apps/amber/9<br><br>David<br><br><br>Michael Sternberg wrote:<br><blockquote type="cite">Dear readers,<br></blockquote><blockquote type="cite">With OpenMPI, how would one go about requesting to load environment modules (of the <a href="http://modules.sourceforge.net/">http://modules.sourceforge.net/</a> kind) on remote nodes, augmenting those &nbsp;normally loaded there by shell dotfiles?<br></blockquote><blockquote type="cite">Background:<br></blockquote><blockquote type="cite">I run a RHEL-5/CentOS-5 cluster. &nbsp;I load a bunch of default modules through /etc/profile.d/ and recommend to users to customize modules in ~/.bashrc. &nbsp;A problem arises for PBS jobs which might need job-specific modules, e.g., to pick a specific flavor of an application. &nbsp;With other MPI implementations (ahem) which export all (or judiciously nearly all) environment variables by default, you can say:<br></blockquote><blockquote type="cite"><span class="Apple-tab-span" style="white-space:pre">	</span>#PBS ...<br></blockquote><blockquote type="cite"><span class="Apple-tab-span" style="white-space:pre">	</span>module load foo<span class="Apple-tab-span" style="white-space:pre">	</span><span class="Apple-tab-span" style="white-space:pre">	</span># not for OpenMPI<br></blockquote><blockquote type="cite"><span class="Apple-tab-span" style="white-space:pre">	</span>mpirun -np 42 ... \<br></blockquote><blockquote type="cite"><span class="Apple-tab-span" style="white-space:pre">	</span><span class="Apple-tab-span" style="white-space:pre">	</span>bar-app<br></blockquote><blockquote type="cite">Not so with OpenMPI - any such customization is only effective for processes on the master (=local) node of the job, and any variables changed by a given module would have to be specifically passed via mpirun -x VARNAME. &nbsp;&nbsp;On the remote nodes, those variables are not available in the dotfiles because they are passed only once orted is live (after dotfile processing by the shell), which then immediately spawns the application binaries (right?)<br></blockquote><blockquote type="cite">I thought along the following lines:<br></blockquote><blockquote type="cite">(1) I happen to run Lustre, which would allow writing a file coherently across nodes prior to mpirun, and thus hook into the shell dotfile processing, but that seems rather crude.<br></blockquote><blockquote type="cite">(2) "mpirun -x PATH -x LD_LIBRARY_PATH …" would take care of a lot, but is not really general.<br></blockquote><blockquote type="cite">Is there a recommended way?<br></blockquote><blockquote type="cite">regards,<br></blockquote><blockquote type="cite">Michael<br></blockquote>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></div></blockquote></div><br></div></div></div></div></div></body></html>
