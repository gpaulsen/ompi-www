<html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"></head><body dir="auto"><div>Manumachu,</div><div><br></div><div>Both nodes have the same IP for their Phi (mic0 and mic1). This is OK as long as they don't try to connect to each other using these addresses. A simple fix is to prevent OMPI from using the supposedly local mic0 and mic1 IP. Add --mca btl_tcp_if_exclude mic0,mic1 to your mpirun command and things should start working better.</div><div><br></div><div>George.<br><br><br></div><div><br>On Apr 24, 2015, at 03:32, Manumachu Reddy &lt;<a href="mailto:manumachu.reddy@gmail.com">manumachu.reddy@gmail.com</a>&gt; wrote:<br><br></div><blockquote type="cite"><div><div dir="ltr"><br>Dear OpenMPI Users,<br><br>I request your help to resolve a hang in my OpenMPI application.<br><br>My OpenMPI application hangs in MPI_Comm_split() operation. The code for this simple application is at the end of this email. Broadcast works fine.<br><br>My experimental setup comprises of two RHEL6.4 Linux nodes. Each node has 2 mic cards. Please note that although there are mic cards, I do not use mic cards in my OpenMPI application.<br><br>I have tested with two OpenMPI versions (1.6.5, 1.8.4). I see the hang in both the versions. OpenMPI is installed using the following command:<br><br>./configure --prefix=/home/manumachu/OpenMPI/openmpi-1.8.4/OPENMPI_INSTALL_ICC CC="icc -fPIC" CXX="icpc -fPIC"<br><br>I have made sure I have turned off the firewall using the following commands:<br><br>sudo service iptables save<br>sudo service iptables stop<br>sudo chkconfig iptables off<br><br>I made sure the mic cards are online and healthy. I am able to login to the mic cards.<br><br>I use an appfile to launch 2 processes on each node.<br><br>I have also attached the "ifconfig" list for each node. Could this problem be related to multiple network interfaces (from the application output also shown at the end of the email)?<br><br>Please let me know if you need further information and I look forward to your suggestions.<br><br>Best Regards<br>Manumachu<br><br><u><b>Application</b></u><br><br>#include &lt;stdio.h&gt;<br>#include &lt;mpi.h&gt;<br><br>int main(int argc, char** argv)<br>{<br>&nbsp;&nbsp;&nbsp; int me, hostnamelen;<br>&nbsp;&nbsp;&nbsp; char hostname[MPI_MAX_PROCESSOR_NAME];<br><br>&nbsp;&nbsp;&nbsp; MPI_Init(&amp;argc, &amp;argv);<br><br>&nbsp;&nbsp;&nbsp; MPI_Get_processor_name(hostname, &amp;hostnamelen);<br><br>&nbsp;&nbsp;&nbsp; MPI_Comm_rank(MPI_COMM_WORLD, &amp;me);<br>&nbsp;&nbsp;&nbsp; printf("Hostname %s: Me is %d.\n", hostname, me);<br><br>&nbsp;&nbsp;&nbsp; int a;<br>&nbsp;&nbsp;&nbsp; MPI_Bcast(&amp;a, 1, MPI_INT, 0, MPI_COMM_WORLD);<br><br>&nbsp;&nbsp;&nbsp; printf("Hostname %s: Me %d broadcasted.\n", hostname, me);<br><br>&nbsp;&nbsp;&nbsp; MPI_Comm intraNodeComm;<br>&nbsp;&nbsp;&nbsp; int rc = MPI_Comm_split(<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MPI_COMM_WORLD,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; me,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; me,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &amp;intraNodeComm<br>&nbsp;&nbsp;&nbsp; );<br><br>&nbsp;&nbsp;&nbsp; if (rc != MPI_SUCCESS)<br>&nbsp;&nbsp;&nbsp; {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; printf("MAIN: Problems MPI_Comm_split...Exiting...\n");<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return -1;<br>&nbsp;&nbsp;&nbsp; }<br><br>&nbsp;&nbsp;&nbsp; printf("Hostname %s: Me %d after comm split.\n", hostname, me);<br>&nbsp;&nbsp;&nbsp; MPI_Comm_free(&amp;intraNodeComm);<br>&nbsp;&nbsp;&nbsp; MPI_Finalize();<br><br>&nbsp;&nbsp;&nbsp; return 0;<br>}<br><br><u><b>Application output</b></u><br><br>Hostname server5: Me is 0.<br>Hostname server5: Me is 1.<br>Hostname server5: Me 1 broadcasted.<br>Hostname server5: Me 0 broadcasted.<br>[server5][[50702,1],0][btl_tcp_endpoint.c:655:mca_btl_tcp_endpoint_complete_connect] connect() to 172.31.1.254 failed: Connection refused (111)<br>[server5][[50702,1],1][btl_tcp_endpoint.c:655:mca_btl_tcp_endpoint_complete_connect] connect() to 172.31.1.254 failed: Connection refused (111)<br>Hostname server2: Me is 2.<br>Hostname server2: Me 2 broadcasted.<br>Hostname server2: Me is 3.<br>Hostname server2: Me 3 broadcasted.<br><br><u><b>server2 ifconfig</b></u><br><br>eth0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Ethernet...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UP BROADCAST MULTICAST&nbsp; MTU:1500&nbsp; Metric:1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br>eth1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Ethernet...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UP BROADCAST MULTICAST&nbsp; MTU:1500&nbsp; Metric:1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br>eth2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Ethernet...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UP BROADCAST MULTICAST&nbsp; MTU:1500&nbsp; Metric:1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br>eth3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Ethernet...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inet addr:172.17.27.17&nbsp; Bcast:172.17.27.255&nbsp; Mask:255.255.255.0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inet6 addr: fe80::921b:eff:fe42:a5ba/64 Scope:Link<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UP BROADCAST RUNNING MULTICAST&nbsp; MTU:1500&nbsp; Metric:1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br>lo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Local Loopback&nbsp; <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inet addr:127.0.0.1&nbsp; Mask:255.0.0.0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UP LOOPBACK RUNNING&nbsp; MTU:65536&nbsp; Metric:1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br><br>mic0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Ethernet<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inet addr:172.31.1.254&nbsp; Bcast:172.31.1.255&nbsp; Mask:255.255.255.0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br><br>mic1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Ethernet...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inet addr:172.31.2.254&nbsp; Bcast:172.31.2.255&nbsp; Mask:255.255.255.0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br><br><u><b>server5 ifconfig</b></u><br>eth0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Ethernet...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UP BROADCAST MULTICAST&nbsp; MTU:1500&nbsp; Metric:1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br><br>eth1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Ethernet...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UP BROADCAST MULTICAST&nbsp; MTU:1500&nbsp; Metric:1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br><br>eth2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Ethernet...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UP BROADCAST MULTICAST&nbsp; MTU:1500&nbsp; Metric:1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br><br>eth3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Ethernet...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inet addr:172.17.27.14&nbsp; Bcast:172.17.27.255&nbsp; Mask:255.255.255.0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UP BROADCAST RUNNING MULTICAST&nbsp; MTU:1500&nbsp; Metric:1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br><br>lo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Local Loopback&nbsp; <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inet addr:127.0.0.1&nbsp; Mask:255.0.0.0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br><br>mic0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Ethernet...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inet addr:172.31.1.254&nbsp; Bcast:172.31.1.255&nbsp; Mask:255.255.255.0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UP BROADCAST RUNNING&nbsp; MTU:64512&nbsp; Metric:1<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br><br>mic1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Ethernet...<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inet addr:172.31.2.254&nbsp; Bcast:172.31.2.255&nbsp; Mask:255.255.255.0<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br><br></div>
</div></blockquote><blockquote type="cite"><div><span>_______________________________________________</span><br><span>users mailing list</span><br><span><a href="mailto:users@open-mpi.org">users@open-mpi.org</a></span><br><span>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></span><br><span>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/04/26780.php">http://www.open-mpi.org/community/lists/users/2015/04/26780.php</a></span></div></blockquote></body></html>
