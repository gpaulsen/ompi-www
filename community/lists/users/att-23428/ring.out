Sender: LSF System <lsfadmin@bl1203>
Subject: Job 902743: <mpirun.lsf ring_c_165> Exited

Job <mpirun.lsf ring_c_165> was submitted from host <lxlogin2> by user <fischega> in cluster <ec_cluster>.
Job was executed on host(s) <bl1203>, in queue <sles10>, as user <fischega> in cluster <ec_cluster>.
</home/fischega> was used as the home directory.
</data/fischega/casl_mpi_test/openmpi_1.6.5> was used as the working directory.
Started at Wed Jan 22 09:54:41 2014
Results reported at Wed Jan 22 09:54:49 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpirun.lsf ring_c_165
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time   :      0.14 sec.
    Max Memory :         4 MB
    Max Swap   :        22 MB

    Max Processes  :         1
    Max Threads    :         1

The output (if any) follows:

--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              bl1203
  Registerable memory:     32768 MiB
  Total memory:            64618 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Process 0 sending 10 to 0, tag 201 (1 processes in ring)
Process 0 sent to 0
Process 0 decremented value: 9
Process 0 decremented value: 8
Process 0 decremented value: 7
Process 0 decremented value: 6
Process 0 decremented value: 5
Process 0 decremented value: 4
Process 0 decremented value: 3
Process 0 decremented value: 2
Process 0 decremented value: 1
Process 0 decremented value: 0
Process 0 exiting
[bl1203:26854] *** Process received signal ***
[bl1203:26854] Signal: Segmentation fault (11)
[bl1203:26854] Signal code:  (128)
[bl1203:26854] Failing at address: (nil)
[bl1203:26854] [ 0] /lib64/libpthread.so.0 [0x2abf8fd46c00]
[bl1203:26854] [ 1] /lib64/ld-linux-x86-64.so.2 [0x2abf8f2f6646]
[bl1203:26854] [ 2] /lib64/ld-linux-x86-64.so.2 [0x2abf8f2f6a77]
[bl1203:26854] [ 3] /lib64/ld-linux-x86-64.so.2 [0x2abf8f2fa0a5]
[bl1203:26854] [ 4] /lib64/ld-linux-x86-64.so.2 [0x2abf8f2ff4c2]
[bl1203:26854] [ 5] /usr/lib64/libibverbs.so.1(ibv_dereg_mr+0x32) [0x2abf91883192]
[bl1203:26854] [ 6] /tools/casl_sles10/vera/gcc-4.6.1/toolset/openmpi-1.6.5/lib/openmpi/mca_btl_openib.so [0x2abf91992942]
[bl1203:26854] [ 7] /tools/casl_sles10/vera/gcc-4.6.1/toolset/openmpi-1.6.5/lib/openmpi/mca_mpool_rdma.so(mca_mpool_rdma_deregister+0xb1) [0x2abf90cfdba1]
[bl1203:26854] [ 8] /tools/casl_sles10/vera/gcc-4.6.1/toolset/openmpi-1.6.5/lib/openmpi/mca_mpool_rdma.so(mca_mpool_rdma_free+0xd) [0x2abf90cfdc2d]
[bl1203:26854] [ 9] /tools/casl_sles10/vera/gcc-4.6.1/toolset/openmpi-1.6.5/lib/libmpi.so.1 [0x2abf8f454d6d]
[bl1203:26854] [10] /tools/casl_sles10/vera/gcc-4.6.1/toolset/openmpi-1.6.5/lib/openmpi/mca_btl_openib.so [0x2abf919920d1]
[bl1203:26854] [11] /tools/casl_sles10/vera/gcc-4.6.1/toolset/openmpi-1.6.5/lib/openmpi/mca_btl_openib.so(mca_btl_openib_finalize+0x419) [0x2abf9198eea9]
[bl1203:26854] [12] /tools/casl_sles10/vera/gcc-4.6.1/toolset/openmpi-1.6.5/lib/libmpi.so.1(mca_btl_base_close+0x5f) [0x2abf8f4976cf]
[bl1203:26854] [13] /tools/casl_sles10/vera/gcc-4.6.1/toolset/openmpi-1.6.5/lib/openmpi/mca_pml_ob1.so [0x2abf91342539]
[bl1203:26854] [14] /tools/casl_sles10/vera/gcc-4.6.1/toolset/openmpi-1.6.5/lib/libmpi.so.1(mca_base_components_close+0x72) [0x2abf8f50a382]
[bl1203:26854] [15] /tools/casl_sles10/vera/gcc-4.6.1/toolset/openmpi-1.6.5/lib/libmpi.so.1(mca_pml_base_close+0xcd) [0x2abf8f4a61dd]
[bl1203:26854] [16] /tools/casl_sles10/vera/gcc-4.6.1/toolset/openmpi-1.6.5/lib/libmpi.so.1(ompi_mpi_finalize+0x2b3) [0x2abf8f46b1d3]
[bl1203:26854] [17] ring_c_165(main+0x1c1) [0x400bb5]
[bl1203:26854] [18] /lib64/libc.so.6(__libc_start_main+0xf4) [0x2abf8fe6f184]
[bl1203:26854] [19] ring_c_165 [0x400939]
[bl1203:26854] *** End of error message ***
--------------------------------------------------------------------------
mpirun has exited due to process rank 0 with PID 26853 on
node bl1203 exiting improperly. There are two reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

This may have caused other processes in the application to be
terminated by signals sent by mpirun (as reported here).
--------------------------------------------------------------------------
Job  /tools/lsf/7.0.6.EC/7.0/linux2.6-glibc2.3-x86_64/bin/openmpi_wrapper ring_c_165

TID   HOST_NAME   COMMAND_LINE            STATUS            TERMINATION_TIME
===== ========== ================  =======================  ===================
00000 bl1203     ring_c_165        Signaled (SIGSEGV)       01/22/2014 09:54:45
