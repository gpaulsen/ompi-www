<div dir="ltr"><br>Hi guys,<br><br>[From Eugene Loh:]<br><div class="gmail_quote"><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div><div class="Wj3C7c">
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">OpenMPI - 25 m 39 s.<br>
MPICH2 &nbsp;- &nbsp;15 m 53 s.<br>
</blockquote>
</div></div></blockquote><div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">With
regards to your issue, do you have any indication when you get that
25m39s timing if there is a grotesque amount of time being spent in MPI
calls? &nbsp;Or, is the slowdown due to non-MPI portions?</blockquote><br>&nbsp; Just to add my two cents: if this job <i>can</i> be run on less than 8 processors (ideally, even on just 1), then I&#39;d recommend doing so.&nbsp; That is, run it with OpenMPI and with MPICH2 on 1, 2 and 4 processors as well.&nbsp; If the single-processor jobs still give vastly different timings, then perhaps Eugene is on the right track and it comes down to various computational optimizations and not so much the message-passing that&#39;s make a difference.&nbsp; Timings from 2 and 4 process runs might be interesting as well to see how this difference changes with process counts.<br>
<br>&nbsp; I&#39;ve seen differences between various MPI libraries before, but nothing quite this severe either.&nbsp; If I get the time, maybe I&#39;ll try to set up Gromacs tonight -- I&#39;ve got both MPICH2 and OpenMPI installed here and can try to duplicate the runs.&nbsp;&nbsp; Sangamesh, is this a standard benchmark case that anyone can download and run?<br>
<br>&nbsp; Cheers,<br>&nbsp; - Brian<br><br><br>Brian Dobbins<br>Yale Engineering HPC</div></div><br></div>

