<div dir="ltr">Ralph,<div><br></div><div>For 1.8.2rc4 I get:</div><div><br></div><div><div><font face="courier new, monospace">(1003) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2rc4/bin/mpirun --leave-session-attached --debug-daemons -np 8 ./helloWorld.182.x</font></div>

<div><font face="courier new, monospace">srun.slurm: cluster configuration lacks support for cpu binding</font></div><div><font face="courier new, monospace">srun.slurm: cluster configuration lacks support for cpu binding</font></div>

<div><font face="courier new, monospace">Daemon [[47143,0],5] checking in as pid 10990 on host borg01x154</font></div><div><font face="courier new, monospace">[borg01x154:10990] [[47143,0],5] orted: up and running - waiting for commands!</font></div>

<div><font face="courier new, monospace">Daemon [[47143,0],1] checking in as pid 23473 on host borg01x143</font></div><div><font face="courier new, monospace">Daemon [[47143,0],2] checking in as pid 8250 on host borg01x144</font></div>

<div><font face="courier new, monospace">[borg01x144:08250] [[47143,0],2] orted: up and running - waiting for commands!</font></div><div><font face="courier new, monospace">[borg01x143:23473] [[47143,0],1] orted: up and running - waiting for commands!</font></div>

<div><font face="courier new, monospace">Daemon [[47143,0],3] checking in as pid 12320 on host borg01x145</font></div><div><font face="courier new, monospace">Daemon [[47143,0],4] checking in as pid 10902 on host borg01x153</font></div>

<div><font face="courier new, monospace">[borg01x153:10902] [[47143,0],4] orted: up and running - waiting for commands!</font></div><div><font face="courier new, monospace">[borg01x145:12320] [[47143,0],3] orted: up and running - waiting for commands!</font></div>

<div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_cmd: received add_local_procs</font></div><div><font face="courier new, monospace">[borg01x144:08250] [[47143,0],2] orted_cmd: received add_local_procs</font></div>

<div><font face="courier new, monospace">[borg01x153:10902] [[47143,0],4] orted_cmd: received add_local_procs</font></div><div><font face="courier new, monospace">[borg01x143:23473] [[47143,0],1] orted_cmd: received add_local_procs</font></div>

<div><font face="courier new, monospace">[borg01x145:12320] [[47143,0],3] orted_cmd: received add_local_procs</font></div><div><font face="courier new, monospace">[borg01x154:10990] [[47143,0],5] orted_cmd: received add_local_procs</font></div>

<div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_recv: received sync+nidmap from local proc [[47143,1],0]</font></div><div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_recv: received sync+nidmap from local proc [[47143,1],2]</font></div>

<div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_recv: received sync+nidmap from local proc [[47143,1],3]</font></div><div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_recv: received sync+nidmap from local proc [[47143,1],1]</font></div>

<div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_recv: received sync+nidmap from local proc [[47143,1],5]</font></div><div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_recv: received sync+nidmap from local proc [[47143,1],4]</font></div>

<div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_recv: received sync+nidmap from local proc [[47143,1],6]</font></div><div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_recv: received sync+nidmap from local proc [[47143,1],7]</font></div>

<div><font face="courier new, monospace">  MPIR_being_debugged = 0</font></div><div><font face="courier new, monospace">  MPIR_debug_state = 1</font></div><div><font face="courier new, monospace">  MPIR_partial_attach_ok = 1</font></div>

<div><font face="courier new, monospace">  MPIR_i_am_starter = 0</font></div><div><font face="courier new, monospace">  MPIR_forward_output = 0</font></div><div><font face="courier new, monospace">  MPIR_proctable_size = 8</font></div>

<div><font face="courier new, monospace">  MPIR_proctable:</font></div><div><font face="courier new, monospace">    (i, host, exe, pid) = (0, borg01x142, /home/mathomp4/HelloWorldTest/./helloWorld.182.x, 1647)</font></div>

<div><font face="courier new, monospace">    (i, host, exe, pid) = (1, borg01x142, /home/mathomp4/HelloWorldTest/./helloWorld.182.x, 1648)</font></div><div><font face="courier new, monospace">    (i, host, exe, pid) = (2, borg01x142, /home/mathomp4/HelloWorldTest/./helloWorld.182.x, 1650)</font></div>

<div><font face="courier new, monospace">    (i, host, exe, pid) = (3, borg01x142, /home/mathomp4/HelloWorldTest/./helloWorld.182.x, 1652)</font></div><div><font face="courier new, monospace">    (i, host, exe, pid) = (4, borg01x142, /home/mathomp4/HelloWorldTest/./helloWorld.182.x, 1654)</font></div>

<div><font face="courier new, monospace">    (i, host, exe, pid) = (5, borg01x142, /home/mathomp4/HelloWorldTest/./helloWorld.182.x, 1656)</font></div><div><font face="courier new, monospace">    (i, host, exe, pid) = (6, borg01x142, /home/mathomp4/HelloWorldTest/./helloWorld.182.x, 1658)</font></div>

<div><font face="courier new, monospace">    (i, host, exe, pid) = (7, borg01x142, /home/mathomp4/HelloWorldTest/./helloWorld.182.x, 1660)</font></div><div><font face="courier new, monospace">MPIR_executable_path: NULL</font></div>

<div><font face="courier new, monospace">MPIR_server_arguments: NULL</font></div><div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_cmd: received message_local_procs</font></div><div><font face="courier new, monospace">[borg01x144:08250] [[47143,0],2] orted_cmd: received message_local_procs</font></div>

<div><font face="courier new, monospace">[borg01x143:23473] [[47143,0],1] orted_cmd: received message_local_procs</font></div><div><font face="courier new, monospace">[borg01x153:10902] [[47143,0],4] orted_cmd: received message_local_procs</font></div>

<div><font face="courier new, monospace">[borg01x154:10990] [[47143,0],5] orted_cmd: received message_local_procs</font></div><div><font face="courier new, monospace">[borg01x145:12320] [[47143,0],3] orted_cmd: received message_local_procs</font></div>

<div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_cmd: received message_local_procs</font></div><div><font face="courier new, monospace">[borg01x143:23473] [[47143,0],1] orted_cmd: received message_local_procs</font></div>

<div><font face="courier new, monospace">[borg01x144:08250] [[47143,0],2] orted_cmd: received message_local_procs</font></div><div><font face="courier new, monospace">[borg01x153:10902] [[47143,0],4] orted_cmd: received message_local_procs</font></div>

<div><font face="courier new, monospace">[borg01x145:12320] [[47143,0],3] orted_cmd: received message_local_procs</font></div><div><font face="courier new, monospace">Process    2 of    8 is on borg01x142</font></div><div>

<font face="courier new, monospace">Process    5 of    8 is on borg01x142</font></div><div><font face="courier new, monospace">Process    4 of    8 is on borg01x142</font></div><div><font face="courier new, monospace">Process    1 of    8 is on borg01x142</font></div>

<div><font face="courier new, monospace">Process    0 of    8 is on borg01x142</font></div><div><font face="courier new, monospace">Process    3 of    8 is on borg01x142</font></div><div><font face="courier new, monospace">Process    6 of    8 is on borg01x142</font></div>

<div><font face="courier new, monospace">Process    7 of    8 is on borg01x142</font></div><div><font face="courier new, monospace">[borg01x154:10990] [[47143,0],5] orted_cmd: received message_local_procs</font></div><div>

<font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_cmd: received message_local_procs</font></div><div><font face="courier new, monospace">[borg01x144:08250] [[47143,0],2] orted_cmd: received message_local_procs</font></div>

<div><font face="courier new, monospace">[borg01x143:23473] [[47143,0],1] orted_cmd: received message_local_procs</font></div><div><font face="courier new, monospace">[borg01x153:10902] [[47143,0],4] orted_cmd: received message_local_procs</font></div>

<div><font face="courier new, monospace">[borg01x154:10990] [[47143,0],5] orted_cmd: received message_local_procs</font></div><div><font face="courier new, monospace">[borg01x145:12320] [[47143,0],3] orted_cmd: received message_local_procs</font></div>

<div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_recv: received sync from local proc [[47143,1],2]</font></div><div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_recv: received sync from local proc [[47143,1],1]</font></div>

<div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_recv: received sync from local proc [[47143,1],3]</font></div><div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_recv: received sync from local proc [[47143,1],0]</font></div>

<div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_recv: received sync from local proc [[47143,1],4]</font></div><div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_recv: received sync from local proc [[47143,1],6]</font></div>

<div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_recv: received sync from local proc [[47143,1],5]</font></div><div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_recv: received sync from local proc [[47143,1],7]</font></div>

<div><font face="courier new, monospace">[borg01x142:01629] [[47143,0],0] orted_cmd: received exit cmd</font></div><div><font face="courier new, monospace">[borg01x144:08250] [[47143,0],2] orted_cmd: received exit cmd</font></div>

<div><font face="courier new, monospace">[borg01x144:08250] [[47143,0],2] orted_cmd: all routes and children gone - exiting</font></div><div><font face="courier new, monospace">[borg01x153:10902] [[47143,0],4] orted_cmd: received exit cmd</font></div>

<div><font face="courier new, monospace">[borg01x153:10902] [[47143,0],4] orted_cmd: all routes and children gone - exiting</font></div><div><font face="courier new, monospace">[borg01x143:23473] [[47143,0],1] orted_cmd: received exit cmd</font></div>

<div><font face="courier new, monospace">[borg01x154:10990] [[47143,0],5] orted_cmd: received exit cmd</font></div><div><font face="courier new, monospace">[borg01x154:10990] [[47143,0],5] orted_cmd: all routes and children gone - exiting</font></div>

<div><font face="courier new, monospace">[borg01x145:12320] [[47143,0],3] orted_cmd: received exit cmd</font></div><div><font face="courier new, monospace">[borg01x145:12320] [[47143,0],3] orted_cmd: all routes and children gone - exiting</font></div>

</div><div><br></div><div>Using the 1.8.2 mpirun:</div><div><br></div><div><div><font face="courier new, monospace">(1004) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2/bin/mpirun --leave-session-attached --debug-daemons -np 8 ./helloWorld.182.x</font></div>

<div><font face="courier new, monospace">srun.slurm: cluster configuration lacks support for cpu binding</font></div><div><font face="courier new, monospace">srun.slurm: cluster configuration lacks support for cpu binding</font></div>

<div><font face="courier new, monospace">[borg01x143:23494] [[47330,0],1] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161</font></div><div><font face="courier new, monospace">[borg01x143:23494] [[47330,0],1] ORTE_ERROR_LOG: Bad parameter in file routed_binomial.c at line 498</font></div>

<div><font face="courier new, monospace">[borg01x143:23494] [[47330,0],1] ORTE_ERROR_LOG: Bad parameter in file base/ess_base_std_orted.c at line 539</font></div><div><font face="courier new, monospace">srun.slurm: error: borg01x143: task 0: Exited with exit code 213</font></div>

<div><font face="courier new, monospace">srun.slurm: Terminating job step 2332583.4</font></div><div><font face="courier new, monospace">[borg01x153:10915] [[47330,0],4] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161</font></div>

<div><font face="courier new, monospace">[borg01x153:10915] [[47330,0],4] ORTE_ERROR_LOG: Bad parameter in file routed_binomial.c at line 498</font></div><div><font face="courier new, monospace">[borg01x153:10915] [[47330,0],4] ORTE_ERROR_LOG: Bad parameter in file base/ess_base_std_orted.c at line 539</font></div>

<div><font face="courier new, monospace">[borg01x144:08263] [[47330,0],2] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161</font></div><div><font face="courier new, monospace">[borg01x144:08263] [[47330,0],2] ORTE_ERROR_LOG: Bad parameter in file routed_binomial.c at line 498</font></div>

<div><font face="courier new, monospace">[borg01x144:08263] [[47330,0],2] ORTE_ERROR_LOG: Bad parameter in file base/ess_base_std_orted.c at line 539</font></div><div><font face="courier new, monospace">srun.slurm: Job step aborted: Waiting up to 2 seconds for job step to finish.</font></div>

<div><font face="courier new, monospace">slurmd[borg01x145]: *** STEP 2332583.4 KILLED AT 2014-08-29T07:16:20 WITH SIGNAL 9 ***</font></div><div><font face="courier new, monospace">slurmd[borg01x154]: *** STEP 2332583.4 KILLED AT 2014-08-29T07:16:20 WITH SIGNAL 9 ***</font></div>

<div><font face="courier new, monospace">slurmd[borg01x153]: *** STEP 2332583.4 KILLED AT 2014-08-29T07:16:20 WITH SIGNAL 9 ***</font></div><div><font face="courier new, monospace">slurmd[borg01x153]: *** STEP 2332583.4 KILLED AT 2014-08-29T07:16:20 WITH SIGNAL 9 ***</font></div>

<div><font face="courier new, monospace">srun.slurm: error: borg01x144: task 1: Exited with exit code 213</font></div><div><font face="courier new, monospace">slurmd[borg01x144]: *** STEP 2332583.4 KILLED AT 2014-08-29T07:16:20 WITH SIGNAL 9 ***</font></div>

<div><font face="courier new, monospace">slurmd[borg01x144]: *** STEP 2332583.4 KILLED AT 2014-08-29T07:16:20 WITH SIGNAL 9 ***</font></div><div><font face="courier new, monospace">srun.slurm: error: borg01x153: task 3: Exited with exit code 213</font></div>

<div><font face="courier new, monospace">slurmd[borg01x154]: *** STEP 2332583.4 KILLED AT 2014-08-29T07:16:20 WITH SIGNAL 9 ***</font></div><div><font face="courier new, monospace">slurmd[borg01x145]: *** STEP 2332583.4 KILLED AT 2014-08-29T07:16:20 WITH SIGNAL 9 ***</font></div>

<div><font face="courier new, monospace">srun.slurm: error: borg01x154: task 4: Killed</font></div><div><font face="courier new, monospace">srun.slurm: error: borg01x145: task 2: Killed</font></div><div><font face="courier new, monospace">sh: tcp://<a href="http://10.1.25.142">10.1.25.142</a>,172.31.1.254,<a href="http://10.12.25.142:34169">10.12.25.142:34169</a>: No such file or directory</font></div>

</div><div><br></div><div><br></div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Thu, Aug 28, 2014 at 7:17 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">I&#39;m unaware of any changes to the Slurm integration between rc4 and final release. It sounds like this might be something else going on - try adding &quot;--leave-session-attached --debug-daemons&quot; to your 1.8.2 command line and let&#39;s see if any errors get reported.<div>

<br></div><div><br><div><div><div class="h5"><div>On Aug 28, 2014, at 12:20 PM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com" target="_blank">fortran@gmail.com</a>&gt; wrote:</div><br></div></div><blockquote type="cite">

<div><div class="h5"><div dir="ltr">Open MPI List,<div><br></div><div>I recently encountered an odd bug with Open MPI 1.8.1 and GCC 4.9.1 on our cluster (reported on this list), and decided to try it with 1.8.2. However, we seem to be having an issue with Open MPI 1.8.2 and SLURM. Even weirder, Open MPI 1.8.2rc4 doesn&#39;t show the bug. And the bug is: I get no stdout with Open MPI 1.8.2. That is, HelloWorld doesn&#39;t work.</div>



<div><div><br></div><div>To wit, our sysadmin has two tarballs:</div><div><br></div><div><div>(1441) $ sha1sum openmpi-1.8.2rc4.tar.bz2</div><div>7e7496913c949451f546f22a1a159df25f8bb683  openmpi-1.8.2rc4.tar.bz2</div><div>



(1442) $ sha1sum openmpi-1.8.2.tar.gz</div><div>cf2b1e45575896f63367406c6c50574699d8b2e1  openmpi-1.8.2.tar.gz</div></div><div><br></div><div>I then build each with a script in the method our sysadmin usually does:</div>


<div>
<br></div></div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">#!/bin/sh <br>set -x<br>export PREFIX=/discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2<br>



export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/nlocal/slurm/2.6.3/lib64<br>build() {<br>  echo `pwd`<br>  ./configure --with-slurm --disable-wrapper-rpath --enable-shared --enable-mca-no-build=btl-usnic \<br>      CC=gcc CXX=g++ F77=gfortran FC=gfortran \<br>



      CFLAGS=&quot;-mtune=generic -fPIC -m64&quot; CXXFLAGS=&quot;-mtune=generic -fPIC -m64&quot; FFLAGS=&quot;-mtune=generic -fPIC -m64&quot; \<br>      F77FLAGS=&quot;-mtune=generic -fPIC -m64&quot; FCFLAGS=&quot;-mtune=generic -fPIC -m64&quot; F90FLAGS=&quot;-mtune=generic -fPIC -m64&quot; \<br>



      LDFLAGS=&quot;-L/usr/nlocal/slurm/2.6.3/lib64&quot; CPPFLAGS=&quot;-I/usr/nlocal/slurm/2.6.3/include&quot; LIBS=&quot;-lpciaccess&quot; \<br>     --prefix=${PREFIX} 2&gt;&amp;1 | tee configure.1.8.2.log<br>  make 2&gt;&amp;1 | tee make.1.8.2.log<br>



  make check 2&gt;&amp;1 | tee makecheck.1.8.2.log<br>  make install 2&gt;&amp;1 | tee makeinstall.1.8.2.log<br>}<br>echo &quot;calling build&quot;<br>build<br>echo &quot;exiting&quot;</blockquote><div><div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">



</blockquote><blockquote class="gmail_quote" style="margin:0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;border-right-width:1px;border-right-color:rgb(204,204,204);border-right-style:solid;padding-left:1ex;padding-right:1ex">



</blockquote><blockquote class="gmail_quote" style="margin:0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;border-right-width:1px;border-right-color:rgb(204,204,204);border-right-style:solid;padding-left:1ex;padding-right:1ex">



</blockquote><blockquote class="gmail_quote" style="margin:0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;border-right-width:1px;border-right-color:rgb(204,204,204);border-right-style:solid;padding-left:1ex;padding-right:1ex">



</blockquote><blockquote class="gmail_quote" style="margin:0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;border-right-width:1px;border-right-color:rgb(204,204,204);border-right-style:solid;padding-left:1ex;padding-right:1ex">



</blockquote><blockquote class="gmail_quote" style="margin:0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;border-right-width:1px;border-right-color:rgb(204,204,204);border-right-style:solid;padding-left:1ex;padding-right:1ex">



</blockquote><blockquote class="gmail_quote" style="margin:0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;border-right-width:1px;border-right-color:rgb(204,204,204);border-right-style:solid;padding-left:1ex;padding-right:1ex">



</blockquote><blockquote class="gmail_quote" style="margin:0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;border-right-width:1px;border-right-color:rgb(204,204,204);border-right-style:solid;padding-left:1ex;padding-right:1ex">



</blockquote><blockquote class="gmail_quote" style="margin:0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;border-right-width:1px;border-right-color:rgb(204,204,204);border-right-style:solid;padding-left:1ex;padding-right:1ex">



</blockquote><blockquote class="gmail_quote" style="margin:0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;border-right-width:1px;border-right-color:rgb(204,204,204);border-right-style:solid;padding-left:1ex;padding-right:1ex">



</blockquote><blockquote class="gmail_quote" style="margin:0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;border-right-width:1px;border-right-color:rgb(204,204,204);border-right-style:solid;padding-left:1ex;padding-right:1ex">



</blockquote><blockquote class="gmail_quote" style="margin:0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;border-right-width:1px;border-right-color:rgb(204,204,204);border-right-style:solid;padding-left:1ex;padding-right:1ex">



</blockquote><blockquote class="gmail_quote" style="margin:0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;border-right-width:1px;border-right-color:rgb(204,204,204);border-right-style:solid;padding-left:1ex;padding-right:1ex">



</blockquote><blockquote class="gmail_quote" style="margin:0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;border-right-width:1px;border-right-color:rgb(204,204,204);border-right-style:solid;padding-left:1ex;padding-right:1ex">



</blockquote><blockquote class="gmail_quote" style="margin:0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;border-right-width:1px;border-right-color:rgb(204,204,204);border-right-style:solid;padding-left:1ex;padding-right:1ex">



</blockquote><blockquote class="gmail_quote" style="margin:0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;border-right-width:1px;border-right-color:rgb(204,204,204);border-right-style:solid;padding-left:1ex;padding-right:1ex">



</blockquote><blockquote class="gmail_quote" style="margin:0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;border-right-width:1px;border-right-color:rgb(204,204,204);border-right-style:solid;padding-left:1ex;padding-right:1ex">



</blockquote><blockquote class="gmail_quote" style="margin:0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;border-right-width:1px;border-right-color:rgb(204,204,204);border-right-style:solid;padding-left:1ex;padding-right:1ex">



</blockquote></div><div><br></div><div>The only difference between the two is &#39;1.8.2&#39; or &#39;1.8.2rc4&#39; in the PREFIX and log file tees.  Now, let us test. First, I grab some nodes with slurm:</div><div><br></div>



</div><div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">$ salloc --nodes=6 --ntasks-per-node=16 --constraint=sand --time=09:00:00 --account=g0620 --mail-type=BEGIN</blockquote>



</div><div><div><br></div><div>Once I get my nodes, I run with 1.8.2rc4:</div><div><br></div><div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">



(1142) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2rc4/bin/mpifort -o helloWorld.182rc4.x helloWorld.F90<br>(1143) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2rc4/bin/mpirun -np 8 ./helloWorld.182rc4.x<br>



Process    0 of    8 is on borg01w044<br>Process    5 of    8 is on borg01w044<br>Process    3 of    8 is on borg01w044<br>Process    7 of    8 is on borg01w044<br>Process    1 of    8 is on borg01w044<br>Process    2 of    8 is on borg01w044<br>



Process    4 of    8 is on borg01w044<br>Process    6 of    8 is on borg01w044</blockquote><div><br></div><div>Now 1.8.2:</div><div><br></div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">



(1144) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2/bin/mpifort -o helloWorld.182.x helloWorld.F90<br>(1145) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2/bin/mpirun -np 8 ./helloWorld.182.x<br>(1146) $</blockquote>



<div><br></div><div>No output at all. But, if I take the helloWorld.x from 1.8.2 and run it with 1.8.2rc4&#39;s mpirun:</div><div><br></div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">



(1146) $ /discover/nobackup/mathomp4/MPI/gcc_4.9.1-openmpi_1.8.2rc4/bin/mpirun -np 8 ./helloWorld.182.x<br>Process    5 of    8 is on borg01w044<br>Process    7 of    8 is on borg01w044<br>Process    2 of    8 is on borg01w044<br>



Process    4 of    8 is on borg01w044<br>Process    1 of    8 is on borg01w044<br>Process    3 of    8 is on borg01w044<br>Process    6 of    8 is on borg01w044<br>Process    0 of    8 is on borg01w044</blockquote></div>


<div>
<br></div><div>So...any idea what is happening here? There did seem to be a few SLURM related changes between the two tarballs involving /dev/null but it&#39;s a bit above me to decipher.</div><div><br></div><div>You can find the ompi_info, build, make, config, etc logs at these links (they are ~300kB which is over the mailing list limit according to the Open MPI web page):</div>



<div><br></div><div><a href="https://dl.dropboxusercontent.com/u/61696/OMPI-1.8.2rc4-Output.tar.bz2" target="_blank">https://dl.dropboxusercontent.com/u/61696/OMPI-1.8.2rc4-Output.tar.bz2</a><br></div><div><a href="https://dl.dropboxusercontent.com/u/61696/OMPI-1.8.2-Output.tar.bz2" target="_blank">https://dl.dropboxusercontent.com/u/61696/OMPI-1.8.2-Output.tar.bz2</a><br>



</div><div><br></div><div>Thank you for any help and please let me know if you need more information,</div><div>Matt</div><div><br></div>-- <br><div dir="ltr"><div>&quot;And, isn&#39;t sanity really just a one-trick pony anyway? I mean all you</div>



<div> get is one trick: rational thinking. But when you&#39;re good and crazy, </div><div> oooh, oooh, oooh, the sky is the limit!&quot; -- The Tick</div><div><br></div></div>
</div></div></div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>

Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25182.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25182.php</a></blockquote></div><br></div></div><br>_______________________________________________<br>


users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/25184.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25184.php</a><br></blockquote></div><br><br clear="all"><div>

<br></div>-- <br><div dir="ltr"><div>&quot;And, isn&#39;t sanity really just a one-trick pony anyway? I mean all you</div><div> get is one trick: rational thinking. But when you&#39;re good and crazy, </div><div> oooh, oooh, oooh, the sky is the limit!&quot; -- The Tick</div>

<div><br></div></div>
</div>

