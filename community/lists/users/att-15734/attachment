I guess I wasn&#39;t clear earlier - I don&#39;t know anything about how HP-MPI works. I was only theorizing that perhaps they did something different that results in some other slurm vars showing up in Brent&#39;s tests. From Brent&#39;s comments, I guess they don&#39;t - but they launch jobs in a different manner that results in some difference in the slurm envars seen by application procs.<div>
<br></div><div>I don&#39;t believe we have a bug in OMPI. What we have is behavior that reflects how the proc is launched. If an app has integrated itself tightly with slurm, then OMPI may not be a good choice - or they can try the &quot;slurm-direct&quot; launch method in 1.5 and see if that meets their needs.</div>
<div><br></div><div>There may be something going on with slurm 2.2.x - as I&#39;ve said before, slurm makes major changes in even minor releases, and trying to track them is a nearly impossible task, especially as many of these features are configuration dependent. What we have in OMPI is the level of slurm integration required by the three DOE weapons labs as (a) they represent the largest component of the very small slurm community, and (b) in the past, they provided the majority of the slurm integration effort within ompi. It works as they need it to, given the way they configure slurm (which may not be how others do).</div>
<div><br></div><div>I&#39;m always willing to help other slurm users, but within the guidelines expressed in an earlier thread - the result must be driven by the DOE weapons lab&#39;s requirements, and cannot interfere with their usage models.</div>
<div><br></div><div>As for slurm_procid - if an application is looking for it, it sounds like that OMPI may not be a good choice for them. Under OMPI, slurm does not see the application procs and has no idea they exist. Slurm&#39;s knowledge of an OMPI job is limited solely to the daemons. This has tradeoffs, as most design decisions do - in the case of the DOE labs, the tradeoffs were judged favorable...at least, as far as LANL was concerned, and they were my boss when I wrote the code :-) At LLNL&#39;s request, I did create the ability to run jobs directly under srun - but as Jeff noted, with reduced capability.</div>
<div><br></div><div>Hope that helps clarify what is in the code, and why. I&#39;m not sure what motivated the original question, but hopefully ompi&#39;s slurm support is a little bit clearer?</div><div><br></div><div>Ralph</div>
<div><br></div><div><br></div><div><br><div><br><div class="gmail_quote">On Thu, Feb 24, 2011 at 2:08 PM, Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
On Feb 24, 2011, at 2:59 PM, Henderson, Brent wrote:<br>
<br>
&gt; [snip]<br>
<div class="im">&gt; They really can&#39;t be all SLURM_PROCID=0 - that is supposed to be unique for the job - right?  It appears that the SLURM_PROCID is inherited from the orted parent - which makes a fair amount of sense given how things are launched.<br>

<br>
</div>That&#39;s correct, and I can agree with your sentiment.<br>
<br>
However, our design goals were to provide a consistent *Open MPI* experience across different launchers. Providing native access to the actual underlying launcher was a secondary goal.  Balancing those two, you can see why we chose the model we did: our orted provides  (nearly) the same functionality across all environments.<br>

<br>
In SLURM&#39;s case, we propagate a [seemingly] non-sensical SLURM_PROCID values to the individual processes, but only if you are making an assumption about how Open MPI is using SLURM&#39;s launcher.<br>
<br>
More specifically, our goal is to provide consistent *Open MPI information* (e.g., through the OMPI_COMM_WORLD* env variables) -- not emulate what SLURM would have done if MPI processes had been launched individually through srun.  Even more specifically: we don&#39;t think that the exact underlying launching mechanism that OMPI uses is of interest to most users; we encourage them to use our portable mechanisms that work even if they move to another cluster with a different launcher.  Admittedly, that does make it a little more challenging if you have to support multiple MPI implementations, and although that&#39;s an important consideration to us, it&#39;s not our first priority.<br>

<div class="im"><br>
&gt; Now to answer the other question - why are there some variables missing.  It appears that when the orted processes are launched - via srun but only one per node, it is a subset of the main allocation and thus some of the environment variables are not the same (or missing entirely) as compared to launching them directly with srun on the full allocation.  This also makes sense to me at some level, so I&#39;m at peace with it now.  :)<br>

<br>
</div>Ah, good.<br>
<div class="im"><br>
&gt; Last thing before I go.  Please let me apologize for not being clear on what I disagreed with Ralph about in my last note.  Clearly he nailed the orted launching process and spelled it out very clearly, but I don&#39;t believe that HP-MPI is not doing anything special to copy/fix up the SLURM environment variables.  Hopefully that was clear by the body of that message.<br>

<br>
</div>No worries; you were perfectly clear.  Thanks!<br>
<div class="im"><br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to:<br>
<a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
<br>
_______________________________________________<br>
</div><div><div></div><div class="h5">users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div></div>

