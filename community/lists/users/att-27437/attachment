basically, without --hetero-nodes, ompi assumes all nodes have the same topology (fast startup)<div>with --hetero-nodes, ompi does not assume anything and request node topology (slower startup)</div><div><br></div><div>I am nor sure if this is still 100% true on all versions.</div><div>iirc, at least on master, a hwloc signature is checked and ompi transparently fall back to --heyero-nodes if needed</div><div><br></div><div>bottom line, on a heterogeneous cluster, it is required or safer to use the --hetero-nodes option</div><div><br></div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<br><br>On Wednesday, August 12, 2015, Dave Love &lt;<a href="mailto:d.love@liverpool.ac.uk">d.love@liverpool.ac.uk</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">&quot;Lane, William&quot; &lt;<a href="javascript:;" onclick="_e(event, &#39;cvml&#39;, &#39;William.Lane@cshs.org&#39;)">William.Lane@cshs.org</a>&gt; writes:<br>
<br>
&gt; I can successfully run my OpenMPI 1.8.7 jobs outside of Son-of-Gridengine but not via qrsh. We&#39;re<br>
&gt; using CentOS 6.3 and a heterogeneous cluster of hyperthreaded and non-hyperthreaded blades<br>
&gt; and x3550 chassis. OpenMPI 1.8.7 has been built w/the debug switch as well.<br>
<br>
I think you want to explain exactly why you need this world of pain.  It<br>
seems unlikely that MPI programs will run efficiently in it.  Our Intel<br>
nodes mostly have hyperthreading on in BIOS -- or what passes for BIOS<br>
on them -- but disabled at startup, and we only run MPI across identical<br>
nodes in the heterogeneous system.<br>
<br>
&gt; Here&#39;s my latest errors:<br>
&gt; qrsh -V -now yes -pe mpi 209 mpirun -np 209 -display-devel-map --prefix /hpc/apps/mpi/openmpi/1.8.7/ --mca btl ^sm --hetero-nodes --bind-to core /hpc/home/lanew/mpi/openmpi/ProcessColors3<br>
<br>
[What does --hetero-nodes do?  It&#39;s undocumented as far as I can tell.]<br>
<br>
&gt; error: executing task of job 211298 failed: execution daemon on host &quot;csclprd3-0-4&quot; didn&#39;t accept task<br>
&gt; error: executing task of job 211298 failed: execution daemon on host &quot;csclprd3-4-1&quot; didn&#39;t accept task<br>
<br>
So you need to find out why that was (probably lack of slots on the exec<br>
host, which might be explained in the execd messages).<br>
<br>
&gt; [...]<br>
<br>
&gt; NOTE: the hosts that &quot;didn&#39;t accept task&quot; were different in two different runs but the errors were the same.<br>
&gt;<br>
&gt; Here&#39;s the definition of the mpi Parallel Environment on our Son-of-Gridengine cluster:<br>
&gt;<br>
&gt; pe_name            mpi<br>
&gt; slots              9999<br>
&gt; user_lists         NONE<br>
&gt; xuser_lists        NONE<br>
&gt; start_proc_args    /opt/sge/mpi/startmpi.sh $pe_hostfile<br>
&gt; stop_proc_args     /opt/sge/mpi/stopmpi.sh<br>
<br>
Why are those two not NONE?<br>
<br>
&gt; allocation_rule    $fill_up<br>
<br>
As I said, that doesn&#39;t seem wise (unless you use -l exclusive).<br>
<br>
&gt; control_slaves     FALSE<br>
&gt; job_is_first_task  TRUE<br>
&gt; urgency_slots      min<br>
&gt; accounting_summary TRUE<br>
&gt; qsort_args         NONE<br>
&gt;<br>
&gt; Qsort_args is set to NONE, but it&#39;s supposed to be set to TRUE right?<br>
<br>
No see sge_pe(5).  (I think the text I supplied for the FAQ is accurate,<br>
but reuti might confirm if he&#39;s reading this.)<br>
<br>
&gt; -Bill L.<br>
&gt;<br>
&gt; If I can run my OpenMPI 1.8.7 jobs outside of Son-of-Gridengine w/no issues it has to be Son-of-Gridengine that&#39;s<br>
&gt; the issue right?<br>
<br>
I don&#39;t see any evidence of an SGE bug, if that&#39;s what you mean, but<br>
clearly you have a problem if execds won&#39;t accept the jobs, and this<br>
isn&#39;t the place to discuss it.  I asked about SGE core binding, and it&#39;s<br>
presumably also relevant how slots are defined on the compute nodes, but<br>
I&#39;d just say &quot;Don&#39;t do that&quot; without a pressing reason.<br>
_______________________________________________<br>
users mailing list<br>
<a href="javascript:;" onclick="_e(event, &#39;cvml&#39;, &#39;users@open-mpi.org&#39;)">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/08/27436.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/08/27436.php</a><br>
</blockquote></div>

