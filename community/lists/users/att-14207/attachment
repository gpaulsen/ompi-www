
<br><tt><font size=2>Ashley</font></tt>
<br>
<br><font size=2 face="sans-serif">Can you provide an example of a situation
in which these semantically redundant barriers help?</font>
<br>
<br><font size=2 face="sans-serif">I may be missing something but my statement
for the text book would be </font>
<br>
<br><font size=2 face="sans-serif">&quot;If adding a barrier to your MPI
program makes it run faster, there is almost certainly a flaw in it that
is better solved another way.&quot;</font>
<br>
<br><font size=2 face="sans-serif">The only exception I can think of is
some sort of one direction data dependancy with messages small enough to
go eagerly. &nbsp;A program that calls MPI_Reduce with a small message
and the same root every iteration and &nbsp;calls no other collective would
be an example.</font>
<br>
<br><font size=2 face="sans-serif">In that case, fast tasks at leaf positions
would run free and a slow task near the root could pile up early arrivals
and end up with some additional slowing. Unless it was driven into paging
I cannot imagine the slowdown would be significant though.</font>
<br>
<br><font size=2 face="sans-serif">&nbsp;Even that should not be a problem
for an MPI implementation that backs off on eager send before it floods
early arrival buffers.</font>
<br>
<br>
<br><font size=2 face="sans-serif">Dick Treumann &nbsp;- &nbsp;MPI Team
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br>
IBM Systems &amp; Technology Group<br>
Dept X2ZA / MS P963 -- 2455 South Road -- Poughkeepsie, NY 12601<br>
Tele (845) 433-7846 &nbsp; &nbsp; &nbsp; &nbsp; Fax (845) 433-8363<br>
</font>
