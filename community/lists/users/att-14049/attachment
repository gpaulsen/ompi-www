Hi Eloi,<br>&gt;Do you think that a thread race condition could explain the hdr-&gt;tag value ?<br>Are there multiple threads invoking MPI functions in your application? The openib BTL is not yet thread safe in the 1.4 release series. There have been improvements to openib BTL thread safety in 1.5, but it is still not officially supported.<br>
<br>--Nysal  <br><br><div class="gmail_quote">On Tue, Aug 17, 2010 at 1:06 PM, Eloi Gaudry <span dir="ltr">&lt;<a href="mailto:eg@fft.be">eg@fft.be</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">
Hi Nysal,<br>
<br>
This is what I was wondering, it hdr-&gt;tag was expected to be null or not. I&#39;ll soon send a valgrind output to the list, hoping this could help to locate an invalid<br>
memory access allowing to understand why reg-&gt;cbfunc / hdr-&gt;tag are null.<br>
<br>
Do you think that a thread race condition could explain the hdr-&gt;tag value ?<br>
<div class="im"><br>
Thanks for your help,<br>
Eloi<br>
<br>
</div><div><div></div><div class="h5">On Monday 16 August 2010 20:46:39 Nysal Jan wrote:<br>
&gt; The value of hdr-&gt;tag seems wrong.<br>
&gt;<br>
&gt; In ompi/mca/pml/ob1/pml_ob1_hdr.h<br>
&gt; #define MCA_PML_OB1_HDR_TYPE_MATCH     (MCA_BTL_TAG_PML + 1)<br>
&gt; #define MCA_PML_OB1_HDR_TYPE_RNDV      (MCA_BTL_TAG_PML + 2)<br>
&gt; #define MCA_PML_OB1_HDR_TYPE_RGET      (MCA_BTL_TAG_PML + 3)<br>
&gt; #define MCA_PML_OB1_HDR_TYPE_ACK       (MCA_BTL_TAG_PML + 4)<br>
&gt; #define MCA_PML_OB1_HDR_TYPE_NACK      (MCA_BTL_TAG_PML + 5)<br>
&gt; #define MCA_PML_OB1_HDR_TYPE_FRAG      (MCA_BTL_TAG_PML + 6)<br>
&gt; #define MCA_PML_OB1_HDR_TYPE_GET       (MCA_BTL_TAG_PML + 7)<br>
&gt; #define MCA_PML_OB1_HDR_TYPE_PUT       (MCA_BTL_TAG_PML + 8)<br>
&gt; #define MCA_PML_OB1_HDR_TYPE_FIN       (MCA_BTL_TAG_PML + 9)<br>
&gt;<br>
&gt; and in ompi/mca/btl/btl.h<br>
&gt; #define MCA_BTL_TAG_PML             0x40<br>
&gt;<br>
&gt; So hdr-&gt;tag should be a value &gt;= 65<br>
&gt; Since the tag is incorrect you are not getting the proper callback function<br>
&gt; pointer and hence the SEGV.<br>
&gt; I&#39;m not sure at this point as to why you are getting an invalid/corrupt<br>
&gt; message header ?<br>
&gt;<br>
&gt; --Nysal<br>
&gt;<br>
&gt; On Tue, Aug 10, 2010 at 7:45 PM, Eloi Gaudry &lt;<a href="mailto:eg@fft.be">eg@fft.be</a>&gt; wrote:<br>
&gt; &gt; Hi,<br>
&gt; &gt;<br>
&gt; &gt; sorry, i just forgot to add the values of the function parameters:<br>
&gt; &gt; (gdb) print reg-&gt;cbdata<br>
&gt; &gt; $1 = (void *) 0x0<br>
&gt; &gt; (gdb) print openib_btl-&gt;super<br>
&gt; &gt; $2 = {btl_component = 0x2b341edd7380, btl_eager_limit = 12288,<br>
&gt; &gt; btl_rndv_eager_limit = 12288, btl_max_send_size = 65536,<br>
&gt; &gt; btl_rdma_pipeline_send_length = 1048576,<br>
&gt; &gt;<br>
&gt; &gt;  btl_rdma_pipeline_frag_size = 1048576, btl_min_rdma_pipeline_size =<br>
&gt; &gt;<br>
&gt; &gt; 1060864, btl_exclusivity = 1024, btl_latency = 10, btl_bandwidth = 800,<br>
&gt; &gt; btl_flags = 310,<br>
&gt; &gt;<br>
&gt; &gt;  btl_add_procs = 0x2b341eb8ee47 &lt;mca_btl_openib_add_procs&gt;, btl_del_procs<br>
&gt; &gt;  =<br>
&gt; &gt;<br>
&gt; &gt; 0x2b341eb90156 &lt;mca_btl_openib_del_procs&gt;, btl_register = 0, btl_finalize<br>
&gt; &gt; = 0x2b341eb93186 &lt;mca_btl_openib_finalize&gt;,<br>
&gt; &gt;<br>
&gt; &gt;  btl_alloc = 0x2b341eb90a3e &lt;mca_btl_openib_alloc&gt;, btl_free =<br>
&gt; &gt;<br>
&gt; &gt; 0x2b341eb91400 &lt;mca_btl_openib_free&gt;, btl_prepare_src = 0x2b341eb91813<br>
&gt; &gt; &lt;mca_btl_openib_prepare_src&gt;,<br>
&gt; &gt;<br>
&gt; &gt;  btl_prepare_dst = 0x2b341eb91f2e &lt;mca_btl_openib_prepare_dst&gt;, btl_send<br>
&gt; &gt;  =<br>
&gt; &gt;<br>
&gt; &gt; 0x2b341eb94517 &lt;mca_btl_openib_send&gt;, btl_sendi = 0x2b341eb9340d<br>
&gt; &gt; &lt;mca_btl_openib_sendi&gt;,<br>
&gt; &gt;<br>
&gt; &gt;  btl_put = 0x2b341eb94660 &lt;mca_btl_openib_put&gt;, btl_get = 0x2b341eb94c4e<br>
&gt; &gt;<br>
&gt; &gt; &lt;mca_btl_openib_get&gt;, btl_dump = 0x2b341acd45cb &lt;mca_btl_base_dump&gt;,<br>
&gt; &gt; btl_mpool = 0xf3f4110,<br>
&gt; &gt;<br>
&gt; &gt;  btl_register_error = 0x2b341eb90565 &lt;mca_btl_openib_register_error_cb&gt;,<br>
&gt; &gt;<br>
&gt; &gt; btl_ft_event = 0x2b341eb952e7 &lt;mca_btl_openib_ft_event&gt;}<br>
&gt; &gt; (gdb) print hdr-&gt;tag<br>
&gt; &gt; $3 = 0 &#39;\0&#39;<br>
&gt; &gt; (gdb) print des<br>
&gt; &gt; $4 = (mca_btl_base_descriptor_t *) 0xf4a6700<br>
&gt; &gt; (gdb) print reg-&gt;cbfunc<br>
&gt; &gt; $5 = (mca_btl_base_module_recv_cb_fn_t) 0<br>
&gt; &gt;<br>
&gt; &gt; Eloi<br>
&gt; &gt;<br>
&gt; &gt; On Tuesday 10 August 2010 16:04:08 Eloi Gaudry wrote:<br>
&gt; &gt; &gt; Hi,<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; Here is the output of a core file generated during a segmentation fault<br>
&gt; &gt; &gt; observed during a collective call (using openib):<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; #0  0x0000000000000000 in ?? ()<br>
&gt; &gt; &gt; (gdb) where<br>
&gt; &gt; &gt; #0  0x0000000000000000 in ?? ()<br>
&gt; &gt; &gt; #1  0x00002aedbc4e05f4 in btl_openib_handle_incoming<br>
&gt; &gt; &gt; (openib_btl=0x1902f9b0, ep=0x1908a1c0, frag=0x190d9700, byte_len=18) at<br>
&gt; &gt; &gt; btl_openib_component.c:2881 #2  0x00002aedbc4e25e2 in handle_wc<br>
&gt; &gt; &gt; (device=0x19024ac0, cq=0, wc=0x7ffff279ce90) at<br>
&gt; &gt; &gt; btl_openib_component.c:3178 #3  0x00002aedbc4e2e9d in poll_device<br>
&gt; &gt; &gt; (device=0x19024ac0, count=2) at btl_openib_component.c:3318 #4<br>
&gt; &gt; &gt; 0x00002aedbc4e34b8 in progress_one_device (device=0x19024ac0) at<br>
&gt; &gt; &gt; btl_openib_component.c:3426 #5  0x00002aedbc4e3561 in<br>
&gt; &gt; &gt; btl_openib_component_progress () at btl_openib_component.c:3451 #6<br>
&gt; &gt; &gt; 0x00002aedb8b22ab8 in opal_progress () at runtime/opal_progress.c:207<br>
&gt; &gt; &gt; #7 0x00002aedb859f497 in opal_condition_wait (c=0x2aedb888ccc0,<br>
&gt; &gt; &gt; m=0x2aedb888cd20) at ../opal/threads/condition.h:99 #8<br>
&gt; &gt;<br>
&gt; &gt;  0x00002aedb859fa31<br>
&gt; &gt;<br>
&gt; &gt; &gt; in ompi_request_default_wait_all (count=2, requests=0x7ffff279d0e0,<br>
&gt; &gt; &gt; statuses=0x0) at request/req_wait.c:262 #9  0x00002aedbd7559ad in<br>
&gt; &gt; &gt; ompi_coll_tuned_allreduce_intra_recursivedoubling (sbuf=0x7ffff279d444,<br>
&gt; &gt; &gt; rbuf=0x7ffff279d440, count=1, dtype=0x6788220, op=0x6787a20,<br>
&gt; &gt; &gt; comm=0x19d81ff0, module=0x19d82b20) at coll_tuned_allreduce.c:223<br>
&gt; &gt; &gt; #10 0x00002aedbd7514f7 in ompi_coll_tuned_allreduce_intra_dec_fixed<br>
&gt; &gt; &gt; (sbuf=0x7ffff279d444, rbuf=0x7ffff279d440, count=1, dtype=0x6788220,<br>
&gt; &gt; &gt; op=0x6787a20, comm=0x19d81ff0, module=0x19d82b20) at<br>
&gt; &gt; &gt; coll_tuned_decision_fixed.c:63<br>
&gt; &gt; &gt; #11 0x00002aedb85c7792 in PMPI_Allreduce (sendbuf=0x7ffff279d444,<br>
&gt; &gt; &gt; recvbuf=0x7ffff279d440, count=1, datatype=0x6788220, op=0x6787a20,<br>
&gt; &gt; &gt; comm=0x19d81ff0) at pallreduce.c:102 #12 0x0000000004387dbf in<br>
&gt; &gt; &gt; FEMTown::MPI::Allreduce (sendbuf=0x7ffff279d444,<br>
&gt; &gt; &gt; recvbuf=0x7ffff279d440, count=1, datatype=0x6788220, op=0x6787a20,<br>
&gt; &gt; &gt; comm=0x19d81ff0) at stubs.cpp:626 #13 0x0000000004058be8 in<br>
&gt; &gt; &gt; FEMTown::Domain::align (itf=<br>
&gt; &gt;<br>
&gt; &gt; {&lt;FEMTown::Boost::shared_base_ptr&lt;FEMTown::Domain::Interface&gt;&gt;<br>
&gt; &gt;<br>
&gt; &gt; &gt; = {_vptr.shared_base_ptr = 0x7ffff279d620, ptr_ = {px = 0x199942a4, pn<br>
&gt; &gt; &gt; = {pi_ = 0x6}}}, &lt;No data fields&gt;}) at interface.cpp:371<br>
&gt; &gt; &gt; #14 0x00000000040cb858 in<br>
&gt; &gt;<br>
&gt; &gt; FEMTown::Field::detail::align_itfs_and_neighbhors<br>
&gt; &gt;<br>
&gt; &gt; &gt; (dim=2, set={px = 0x7ffff279d780, pn = {pi_ = 0x2f279d640}},<br>
&gt; &gt; &gt; check_info=@0x7ffff279d7f0) at check.cpp:63 #15 0x00000000040cbfa8 in<br>
&gt; &gt; &gt; FEMTown::Field::align_elements (set={px = 0x7ffff279d950, pn = {pi_ =<br>
&gt; &gt; &gt; 0x66e08d0}}, check_info=@0x7ffff279d7f0) at check.cpp:159 #16<br>
&gt; &gt; &gt; 0x00000000039acdd4 in PyField_align_elements (self=0x0,<br>
&gt; &gt; &gt; args=0x2aaab0765050, kwds=0x19d2e950) at check.cpp:31 #17<br>
&gt; &gt; &gt; 0x0000000001fbf76d in FEMTown::Main::ExErrCatch&lt;_object* (*)(_object*,<br>
&gt; &gt; &gt; _object*, _object*)&gt;::exec&lt;_object&gt; (this=0x7ffff279dc20, s=0x0,<br>
&gt; &gt; &gt; po1=0x2aaab0765050, po2=0x19d2e950) at<br>
&gt; &gt; &gt; /home/qa/svntop/femtown/modules/main/py/exception.hpp:463<br>
&gt; &gt; &gt; #18 0x00000000039acc82 in PyField_align_elements_ewrap (self=0x0,<br>
&gt; &gt; &gt; args=0x2aaab0765050, kwds=0x19d2e950) at check.cpp:39 #19<br>
&gt; &gt; &gt; 0x00000000044093a0 in PyEval_EvalFrameEx (f=0x19b52e90,<br>
&gt; &gt; &gt; throwflag=&lt;value optimized out&gt;) at Python/ceval.c:3921 #20<br>
&gt; &gt; &gt; 0x000000000440aae9 in PyEval_EvalCodeEx (co=0x2aaab754ad50,<br>
&gt; &gt; &gt; globals=&lt;value optimized out&gt;, locals=&lt;value optimized out&gt;, args=0x3,<br>
&gt; &gt; &gt; argcount=1, kws=0x19ace4a0, kwcount=2, defs=0x2aaab75e4800,<br>
&gt; &gt; &gt; defcount=2, closure=0x0) at<br>
&gt; &gt; &gt; Python/ceval.c:2968<br>
&gt; &gt; &gt; #21 0x0000000004408f58 in PyEval_EvalFrameEx (f=0x19ace2d0,<br>
&gt; &gt; &gt; throwflag=&lt;value optimized out&gt;) at Python/ceval.c:3802 #22<br>
&gt; &gt; &gt; 0x000000000440aae9 in PyEval_EvalCodeEx (co=0x2aaab7550120,<br>
&gt; &gt;<br>
&gt; &gt; globals=&lt;value<br>
&gt; &gt;<br>
&gt; &gt; &gt; optimized out&gt;, locals=&lt;value optimized out&gt;, args=0x7, argcount=1,<br>
&gt; &gt; &gt; kws=0x19acc418, kwcount=3, defs=0x2aaab759e958, defcount=6,<br>
&gt; &gt; &gt; closure=0x0) at Python/ceval.c:2968<br>
&gt; &gt; &gt; #23 0x0000000004408f58 in PyEval_EvalFrameEx (f=0x19acc1c0,<br>
&gt; &gt; &gt; throwflag=&lt;value optimized out&gt;) at Python/ceval.c:3802 #24<br>
&gt; &gt; &gt; 0x000000000440aae9 in PyEval_EvalCodeEx (co=0x2aaab8b5e738,<br>
&gt; &gt;<br>
&gt; &gt; globals=&lt;value<br>
&gt; &gt;<br>
&gt; &gt; &gt; optimized out&gt;, locals=&lt;value optimized out&gt;, args=0x6, argcount=1,<br>
&gt; &gt; &gt; kws=0x19abd328, kwcount=5, defs=0x2aaab891b7e8, defcount=3,<br>
&gt; &gt; &gt; closure=0x0) at Python/ceval.c:2968<br>
&gt; &gt; &gt; #25 0x0000000004408f58 in PyEval_EvalFrameEx (f=0x19abcea0,<br>
&gt; &gt; &gt; throwflag=&lt;value optimized out&gt;) at Python/ceval.c:3802 #26<br>
&gt; &gt; &gt; 0x000000000440aae9 in PyEval_EvalCodeEx (co=0x2aaab3eb4198,<br>
&gt; &gt;<br>
&gt; &gt; globals=&lt;value<br>
&gt; &gt;<br>
&gt; &gt; &gt; optimized out&gt;, locals=&lt;value optimized out&gt;, args=0xb, argcount=1,<br>
&gt; &gt; &gt; kws=0x19a89df0, kwcount=10, defs=0x0, defcount=0, closure=0x0) at<br>
&gt; &gt; &gt; Python/ceval.c:2968<br>
&gt; &gt; &gt; #27 0x0000000004408f58 in PyEval_EvalFrameEx (f=0x19a89c40,<br>
&gt; &gt; &gt; throwflag=&lt;value optimized out&gt;) at Python/ceval.c:3802 #28<br>
&gt; &gt; &gt; 0x000000000440aae9 in PyEval_EvalCodeEx (co=0x2aaab3eb4288,<br>
&gt; &gt;<br>
&gt; &gt; globals=&lt;value<br>
&gt; &gt;<br>
&gt; &gt; &gt; optimized out&gt;, locals=&lt;value optimized out&gt;, args=0x1, argcount=0,<br>
&gt; &gt; &gt; kws=0x19a89330, kwcount=0, defs=0x2aaab8b66668, defcount=1,<br>
&gt; &gt; &gt; closure=0x0) at Python/ceval.c:2968<br>
&gt; &gt; &gt; #29 0x0000000004408f58 in PyEval_EvalFrameEx (f=0x19a891b0,<br>
&gt; &gt; &gt; throwflag=&lt;value optimized out&gt;) at Python/ceval.c:3802 #30<br>
&gt; &gt; &gt; 0x000000000440aae9 in PyEval_EvalCodeEx (co=0x2aaab8b6a738,<br>
&gt; &gt;<br>
&gt; &gt; globals=&lt;value<br>
&gt; &gt;<br>
&gt; &gt; &gt; optimized out&gt;, locals=&lt;value optimized out&gt;, args=0x0, argcount=0,<br>
&gt; &gt; &gt; kws=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0) at<br>
&gt; &gt; &gt; Python/ceval.c:2968<br>
&gt; &gt; &gt; #31 0x000000000440ac02 in PyEval_EvalCode (co=0x1902f9b0, globals=0x0,<br>
&gt; &gt; &gt; locals=0x190d9700) at Python/ceval.c:522 #32 0x000000000442853c in<br>
&gt; &gt; &gt; PyRun_StringFlags (str=0x192fd3d8 &quot;DIRECT.Actran.main()&quot;, start=&lt;value<br>
&gt; &gt; &gt; optimized out&gt;, globals=0x192213d0, locals=0x192213d0, flags=0x0) at<br>
&gt; &gt; &gt; Python/pythonrun.c:1335 #33 0x0000000004429690 in<br>
&gt; &gt; &gt; PyRun_SimpleStringFlags (command=0x192fd3d8 &quot;DIRECT.Actran.main()&quot;,<br>
&gt; &gt; &gt; flags=0x0) at<br>
&gt; &gt; &gt; Python/pythonrun.c:957 #34 0x0000000001fa1cf9 in<br>
&gt; &gt; &gt; FEMTown::Python::FEMPy::run_application (this=0x7ffff279f650) at<br>
&gt; &gt; &gt; fempy.cpp:873 #35 0x000000000434ce99 in FEMTown::Main::Batch::run<br>
&gt; &gt; &gt; (this=0x7ffff279f650) at batch.cpp:374 #36 0x0000000001f9aa25 in main<br>
&gt; &gt; &gt; (argc=8, argv=0x7ffff279fa48) at main.cpp:10 (gdb) f 1<br>
&gt; &gt; &gt; #1  0x00002aedbc4e05f4 in btl_openib_handle_incoming<br>
&gt; &gt; &gt; (openib_btl=0x1902f9b0, ep=0x1908a1c0, frag=0x190d9700, byte_len=18) at<br>
&gt; &gt; &gt; btl_openib_component.c:2881 2881            reg-&gt;cbfunc(<br>
&gt; &gt;<br>
&gt; &gt; &gt; &amp;openib_btl-&gt;super, hdr-&gt;tag, des, reg-&gt;cbdata ); Current language:<br>
&gt; &gt;  auto;<br>
&gt; &gt;<br>
&gt; &gt; &gt; currently c<br>
&gt; &gt; &gt; (gdb)<br>
&gt; &gt; &gt; #1  0x00002aedbc4e05f4 in btl_openib_handle_incoming<br>
&gt; &gt; &gt; (openib_btl=0x1902f9b0, ep=0x1908a1c0, frag=0x190d9700, byte_len=18) at<br>
&gt; &gt; &gt; btl_openib_component.c:2881 2881            reg-&gt;cbfunc(<br>
&gt; &gt; &gt; &amp;openib_btl-&gt;super, hdr-&gt;tag, des, reg-&gt;cbdata ); (gdb) l<br>
&gt; &gt; &gt; 2876<br>
&gt; &gt; &gt; 2877        if(OPAL_LIKELY(!(is_credit_msg = is_credit_message(frag))))<br>
&gt; &gt; &gt; { 2878            /* call registered callback */<br>
&gt; &gt; &gt; 2879            mca_btl_active_message_callback_t* reg;<br>
&gt; &gt; &gt; 2880            reg = mca_btl_base_active_message_trigger + hdr-&gt;tag;<br>
&gt; &gt; &gt; 2881            reg-&gt;cbfunc( &amp;openib_btl-&gt;super, hdr-&gt;tag, des,<br>
&gt; &gt;<br>
&gt; &gt; reg-&gt;cbdata<br>
&gt; &gt;<br>
&gt; &gt; &gt; ); 2882            if(MCA_BTL_OPENIB_RDMA_FRAG(frag)) {<br>
&gt; &gt; &gt; 2883                cqp = (hdr-&gt;credits &gt;&gt; 11) &amp; 0x0f;<br>
&gt; &gt; &gt; 2884                hdr-&gt;credits &amp;= 0x87ff;<br>
&gt; &gt; &gt; 2885            } else {<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; Regards,<br>
&gt; &gt; &gt; Eloi<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt; On Friday 16 July 2010 16:01:02 Eloi Gaudry wrote:<br>
&gt; &gt; &gt; &gt; Hi Edgar,<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; The only difference I could observed was that the segmentation fault<br>
&gt; &gt; &gt; &gt; appeared sometimes later during the parallel computation.<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; I&#39;m running out of idea here. I wish I could use the &quot;--mca coll<br>
&gt; &gt; &gt; &gt; tuned&quot; with &quot;--mca self,sm,tcp&quot; so that I could check that the issue<br>
&gt; &gt; &gt; &gt; is not somehow limited to the tuned collective routines.<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; Thanks,<br>
&gt; &gt; &gt; &gt; Eloi<br>
&gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; On Thursday 15 July 2010 17:24:24 Edgar Gabriel wrote:<br>
&gt; &gt; &gt; &gt; &gt; On 7/15/2010 10:18 AM, Eloi Gaudry wrote:<br>
&gt; &gt; &gt; &gt; &gt; &gt; hi edgar,<br>
&gt; &gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt; thanks for the tips, I&#39;m gonna try this option as well. the<br>
&gt; &gt; &gt; &gt; &gt; &gt; segmentation fault i&#39;m observing always happened during a<br>
&gt; &gt;<br>
&gt; &gt; collective<br>
&gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt; communication indeed... does it basically switch all collective<br>
&gt; &gt; &gt; &gt; &gt; &gt; communication to basic mode, right ?<br>
&gt; &gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt; sorry for my ignorance, but what&#39;s a NCA ?<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; sorry, I meant to type HCA (InifinBand networking card)<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; Thanks<br>
&gt; &gt; &gt; &gt; &gt; Edgar<br>
&gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt; thanks,<br>
&gt; &gt; &gt; &gt; &gt; &gt; éloi<br>
&gt; &gt; &gt; &gt; &gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt; On Thursday 15 July 2010 16:20:54 Edgar Gabriel wrote:<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt; you could try first to use the algorithms in the basic module,<br>
&gt; &gt;<br>
&gt; &gt; e.g.<br>
&gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt; mpirun -np x --mca coll basic ./mytest<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt; and see whether this makes a difference. I used to observe<br>
&gt; &gt;<br>
&gt; &gt; sometimes<br>
&gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt; a (similar ?) problem in the openib btl triggered from the tuned<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt; collective component, in cases where the ofed libraries were<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt; installed but no NCA was found on a node. It used to work<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt; however with the basic component.<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt; Thanks<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt; Edgar<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt; On 7/15/2010 3:08 AM, Eloi Gaudry wrote:<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt; hi Rolf,<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt; unfortunately, i couldn&#39;t get rid of that annoying segmentation<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt; fault when selecting another bcast algorithm. i&#39;m now going to<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt; replace MPI_Bcast with a naive implementation (using MPI_Send<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt; and MPI_Recv) and see if that helps.<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt; regards,<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt; éloi<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt; On Wednesday 14 July 2010 10:59:53 Eloi Gaudry wrote:<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; Hi Rolf,<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; thanks for your input. You&#39;re right, I miss the<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; coll_tuned_use_dynamic_rules option.<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; I&#39;ll check if I the segmentation fault disappears when using<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; the basic bcast linear algorithm using the proper command<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; line you provided.<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; Regards,<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; Eloi<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; On Tuesday 13 July 2010 20:39:59 Rolf vandeVaart wrote:<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; Hi Eloi:<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; To select the different bcast algorithms, you need to add an<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; extra mca parameter that tells the library to use dynamic<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; selection. --mca coll_tuned_use_dynamic_rules 1<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; One way to make sure you are typing this in correctly is to<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; use it with ompi_info.  Do the following:<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; ompi_info -mca coll_tuned_use_dynamic_rules 1 --param coll<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; You should see lots of output with all the different<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; algorithms that can be selected for the various collectives.<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; Therefore, you need this:<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; --mca coll_tuned_use_dynamic_rules 1 --mca<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; coll_tuned_bcast_algorithm 1<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; Rolf<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; On 07/13/10 11:28, Eloi Gaudry wrote:<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; Hi,<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; I&#39;ve found that &quot;--mca coll_tuned_bcast_algorithm 1&quot; allowed<br>
&gt; &gt;<br>
&gt; &gt; to<br>
&gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; switch to the basic linear algorithm. Anyway whatever the<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; algorithm used, the segmentation fault remains.<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; Does anyone could give some advice on ways to diagnose the<br>
&gt; &gt;<br>
&gt; &gt; issue<br>
&gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; I&#39;m facing ?<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; Regards,<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; Eloi<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; On Monday 12 July 2010 10:53:58 Eloi Gaudry wrote:<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; Hi,<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; I&#39;m focusing on the MPI_Bcast routine that seems to<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; randomly segfault when using the openib btl. I&#39;d like to<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; know if there is any way to make OpenMPI switch to a<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; different algorithm than the default one being selected<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; for MPI_Bcast.<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; Thanks for your help,<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; Eloi<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; On Friday 02 July 2010 11:06:52 Eloi Gaudry wrote:<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hi,<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; I&#39;m observing a random segmentation fault during an<br>
&gt; &gt;<br>
&gt; &gt; internode<br>
&gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; parallel computation involving the openib btl and<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; OpenMPI-1.4.2 (the same issue can be observed with<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; OpenMPI-1.3.3).<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    mpirun (Open MPI) 1.4.2<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    Report bugs to <a href="http://www.open-mpi.org/community/help/" target="_blank">http://www.open-mpi.org/community/help/</a><br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    [pbn08:02624] *** Process received signal ***<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    [pbn08:02624] Signal: Segmentation fault (11)<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    [pbn08:02624] Signal code: Address not mapped (1)<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    [pbn08:02624] Failing at address: (nil)<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    [pbn08:02624] [ 0] /lib64/libpthread.so.0<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    [0x349540e4c0] [pbn08:02624] *** End of error message<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    ***<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    sh: line 1:  2624 Segmentation fault<br>
&gt; &gt;<br>
&gt; &gt; \/share\/hpc3\/actran_suite\/Actran_11\.0\.rc2\.41872\/RedHatE<br>
&gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; L\ -5 \/ x 86 _6 4\ /bin\/actranpy_mp<br>
&gt; &gt;<br>
&gt; &gt; &#39;--apl=/share/hpc3/actran_suite/Actran_11.0.rc2.41872/RedHatEL<br>
&gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; -5 /x 86 _ 64 /A c tran_11.0.rc2.41872&#39;<br>
&gt; &gt;<br>
&gt; &gt; &#39;--inputfile=/work/st25652/LSF_130073_0_47696_0/Case1_3Dreal_m<br>
&gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 4_ n2 .d a t&#39;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#39;--scratch=/scratch/st25652/LSF_130073_0_47696_0/scratch&#39;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#39;--mem=3200&#39; &#39;--threads=1&#39; &#39;--errorlevel=FATAL&#39;<br>
&gt; &gt;<br>
&gt; &gt; &#39;--t_max=0.1&#39;<br>
&gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#39;--parallel=domain&#39;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; If I choose not to use the openib btl (by using --mca btl<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; self,sm,tcp on the command line, for instance), I don&#39;t<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; encounter any problem and the parallel computation runs<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; flawlessly.<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; I would like to get some help to be able:<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; - to diagnose the issue I&#39;m facing with the openib btl<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; - understand why this issue is observed only when using<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; the openib btl and not when using self,sm,tcp<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Any help would be very much appreciated.<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; The outputs of ompi_info and the configure scripts of<br>
&gt; &gt;<br>
&gt; &gt; OpenMPI<br>
&gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; are enclosed to this email, and some information on the<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; infiniband drivers as well.<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Here is the command line used when launching a parallel<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; computation<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; using infiniband:<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    path_to_openmpi/bin/mpirun -np $NPROCESS --hostfile<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    host.list --mca<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; btl openib,sm,self,tcp  --display-map --verbose --version<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; --mca mpi_warn_on_fork 0 --mca<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; btl_openib_want_fork_support<br>
&gt; &gt;<br>
&gt; &gt; 0<br>
&gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; [...]<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; and the command line used if not using infiniband:<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    path_to_openmpi/bin/mpirun -np $NPROCESS --hostfile<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;    host.list --mca<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; btl self,sm,tcp  --display-map --verbose --version --mca<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; mpi_warn_on_fork 0 --mca btl_openib_want_fork_support 0<br>
&gt; &gt;<br>
&gt; &gt; [...]<br>
&gt; &gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Thanks,<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Eloi<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; users mailing list<br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; &gt;<br>
&gt; &gt; --<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; Eloi Gaudry<br>
&gt; &gt;<br>
&gt; &gt; Free Field Technologies<br>
&gt; &gt; Company Website: <a href="http://www.fft.be" target="_blank">http://www.fft.be</a><br>
&gt; &gt; Company Phone:   +32 10 487 959<br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
</div></div>--<br>
<div><div></div><div class="h5"><br>
<br>
Eloi Gaudry<br>
<br>
Free Field Technologies<br>
Company Website: <a href="http://www.fft.be" target="_blank">http://www.fft.be</a><br>
Company Phone:   +32 10 487 959<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

