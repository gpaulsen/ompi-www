<div>Automatically striping large messages across multiple NICs is certainly a very nice feature; I was not aware that OpenMPI does this transparently. (I wonder if other MPI implementations do this or not). However, I have the following concern: Since the communication over an ethernet NIC is most likely over IP, does it take into account the route cost when striping messages? For example, host A and B in the MPD ring might be connected via two NICs, one direct and one via an intermediate router, or one with a large bandwidth and another with a small bandwidth. Does OpenMPI send a smaller chunk of data over a route with a higher cost?
</div>
<div>&nbsp;</div>
<div>Because of this concern, I think the channel bonding approach someone else suggested is more preferable; all these details will be taken care of at the hardware level instead of at the IP level.<br></div>
<div>Thanks</div>
<div>Durga<br></div>
<div>&nbsp;</div>
<div class="gmail_quote">On Dec 6, 2007 9:42 AM, Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br>
<blockquote class="gmail_quote" style="PADDING-LEFT: 1ex; MARGIN: 0px 0px 0px 0.8ex; BORDER-LEFT: #ccc 1px solid">Wow, that&#39;s quite a .sig. &nbsp;:-)<br><br>Open MPI will automatically stripe large messages across however many
<br>NICs you have. &nbsp;So you shouldn&#39;t need to use multiple threads.<br><br>The threading support in the OMPI v1.2 series is broken; it&#39;s not<br>worth using. There&#39;s a big warning in configure when you enable it. &nbsp;:-)
<br><br><br>On Dec 5, 2007, at 9:57 PM, Tee Wen Kai wrote:<br><br>&gt; Hi everyone,<br>&gt;<br>&gt; I have installed openmpi-1.2.3. My system has two ethernet ports.<br>&gt; Thus, I am trying to make use of both ports to speed up the
<br>&gt; communication process by using openmp to split into two threads.<br>&gt; However, this implementation always cause error. Then I realized<br>&gt; that I need to build openmpi using --enable-mpi-threads and use<br>
&gt; MPI_Init_thread to initialize. But, the initialization always return<br>&gt; MPI_THREAD_SINGLE no matter what value I set. Using ompi_info|grep<br>&gt; Thread, it shows that thread support has already been activated.
<br>&gt; Thus, I seek your help to teach me what other configurations I need<br>&gt; to set in order to use multi-threads and what are the parameters to<br>&gt; include in mpirun in order to use the two ethernet ports.<br>
&gt;<br>&gt; Thank you very much.<br>&gt;<br>&gt; Regards,<br>&gt; Tee<br>&gt;<br>&gt;<br>&gt;<br>&gt; _________________________________________________<br>&gt;<br>&gt;<br>&gt;<br>&gt; &nbsp; Many of us spend our time wishing for things we could have if we
<br>&gt; didn&#39;t spend half our time wishing.<br>&gt;<br>&gt; Looking for last minute shopping deals? Find them fast with Yahoo!<br>&gt; Search._______________________________________________<br>&gt; users mailing list
<br>&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><font color="#888888">
<br><br>--<br>Jeff Squyres<br>Cisco Systems<br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></font></blockquote></div><br><br clear="all"><br>-- <br>Its a battle between humans and communists;<br>Which side are you in?<br>. 

