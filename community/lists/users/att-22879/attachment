<div dir="ltr"><div><div><div><div><div><div><div><div><div><div><div>Jeff,<br></div>  Here&#39;s what I know:<br></div>1.  Checked FAQs.  Done<br></div>2.  Version 1.6.5<br></div>3. config.log file has been removed by the sysadmin...<br>
</div>4. ompi_info -a from head node is in attached as headnode.out<br></div>5. N/A<br></div>6. compute node info in attached as compute-x-yy.out<br></div>7. As discussed, local variables are being overwritten after calls to MPI_RECV from Fortran code<br>
</div>8. ifconfig output from head node and computes listed as *-ifconfig.out<br><br></div>Cheers,<br></div>--Jim<br></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Wed, Oct 30, 2013 at 5:29 PM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Can you send the information listed here:<br>
<br>
    <a href="http://www.open-mpi.org/community/help/" target="_blank">http://www.open-mpi.org/community/help/</a><br>
<div><div class="h5"><br>
<br>
On Oct 30, 2013, at 6:22 PM, Jim Parker &lt;<a href="mailto:jimparker96313@gmail.com">jimparker96313@gmail.com</a>&gt; wrote:<br>
<br>
&gt; Jeff and Ralph,<br>
&gt;   Ok, I downshifted to a helloWorld example (attached), bottom line after I hit the MPI_Recv call, my local variable (rank) gets borked.<br>
&gt;<br>
&gt; I have compiled with -m64 -fdefault-integer-8 and even have assigned kind=8 to the integers (which would be the preferred method in my case)<br>
&gt;<br>
&gt; Your help is appreciated.<br>
&gt;<br>
&gt; Cheers,<br>
&gt; --Jim<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; On Wed, Oct 30, 2013 at 4:49 PM, Jeff Squyres (jsquyres) &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br>
&gt; On Oct 30, 2013, at 4:35 PM, Jim Parker &lt;<a href="mailto:jimparker96313@gmail.com">jimparker96313@gmail.com</a>&gt; wrote:<br>
&gt;<br>
&gt; &gt;   I have recently built a cluster that uses the 64-bit indexing feature of OpenMPI following the directions at<br>
&gt; &gt; <a href="http://wiki.chem.vu.nl/dirac/index.php/How_to_build_MPI_libraries_for_64-bit_integers" target="_blank">http://wiki.chem.vu.nl/dirac/index.php/How_to_build_MPI_libraries_for_64-bit_integers</a><br>
&gt;<br>
&gt; That should be correct (i.e., passing -i8 in FFLAGS and FCFLAGS for OMPI 1.6.x).<br>
&gt;<br>
&gt; &gt; My question is what are the new prototypes for the MPI calls ?<br>
&gt; &gt; specifically<br>
&gt; &gt; MPI_RECV<br>
&gt; &gt; MPI_Allgathterv<br>
&gt;<br>
&gt; They&#39;re the same as they&#39;ve always been.<br>
&gt;<br>
&gt; The magic is that the -i8 flag tells the compiler &quot;make all Fortran INTEGERs be 8 bytes, not (the default) 4.&quot;  So Ralph&#39;s answer was correct in that all the MPI parameters are INTEGERs -- but you can tell the compiler that all INTEGERs are 8 bytes, not 4, and therefore get &quot;large&quot; integers.<br>

&gt;<br>
&gt; Note that this means that you need to compile your application with -i8, too.  That will make *your* INTEGERs also be 8 bytes, and then you&#39;ll match what Open MPI is doing.<br>
&gt;<br>
&gt; &gt; I&#39;m curious because some off my local variables get killed (set to null) upon my first call to MPI_RECV.  Typically, this is due (in Fortran) to someone not setting the &#39;status&#39; variable to an appropriate array size.<br>

&gt;<br>
&gt; If you didn&#39;t compile your application with -i8, this could well be because your application is treating INTEGERs as 4 bytes, but OMPI is treating INTEGERs as 8 bytes.  Nothing good can come from that.<br>
&gt;<br>
&gt; If you *did* compile your application with -i8 and you&#39;re seeing this kind of wonkyness, we should dig deeper and see what&#39;s going on.<br>
&gt;<br>
&gt; &gt; My review of mpif.h and mpi.h seem to indicate that the functions are defined as C int types and therefore , I assume, the coercion during the compile makes the library support 64-bit indexing.  ie. int -&gt; long int<br>

&gt;<br>
&gt; FWIW: We actually define a type MPI_Fint; its actual type is determined by configure (int or long int, IIRC).  When your Fortran code calls C, we use the MPI_Fint type for parameters, and so it will be either a 4 or 8 byte integer type.<br>

&gt;<br>
&gt; --<br>
&gt; Jeff Squyres<br>
&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
</div></div>&gt; &lt;mpi-test-64bit.tar.bz2&gt;_______________________________________________<br>
<div class="HOEnZb"><div class="h5">&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div>

