Diego,<div><br></div><div>about MPI_Allreduce, you should use MPI_IN_PLACE if you want the same buffer in send and recv</div><div><br></div><div>about the stack, I notice comm is NULL which is a bit surprising...</div><div>at first glance, type creation looks good.</div><div>that being said, you do not check MPIdata%iErr is MPI_SUCCESS after each MPI call.</div><div>I recommend you first do this, so you can catch the error as soon it happens, and hopefully understand why it occurs．</div><div><br></div>Cheers,<div><br></div><div>Gilles<br><div><br>On Wednesday, September 2, 2015, Diego Avesani &lt;<a href="mailto:diego.avesani@gmail.com">diego.avesani@gmail.com</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Dear all,<div><br></div><div>I have notice small difference between OPEN-MPI and intel MPI. </div><div>For example in MPI_ALLREDUCE in intel MPI is not allowed to use the same variable in send and receiving Buff.</div><div><br></div><div>I have written my code in OPEN-MPI, but unfortunately I have to run in on a intel-MPI cluster. </div><div>Now I have the following error:</div><div><br></div><div><div><i>atal error in MPI_Isend: Invalid communicator, error stack:</i></div><div><i>MPI_Isend(158): MPI_Isend(buf=0x1dd27b0, count=1, INVALID DATATYPE, dest=0, tag=0, comm=0x0, request=0x7fff9d7dd9f0) failed</i></div></div><div><br></div><div><br></div><div>This is ho I create my type:</div><div><br></div><div><div><i>  CALL  MPI_TYPE_VECTOR(1, Ncoeff_MLS, Ncoeff_MLS, MPI_DOUBLE_PRECISION, coltype, MPIdata%iErr) </i></div><div><i>  CALL  MPI_TYPE_COMMIT(coltype, MPIdata%iErr)</i></div><div><i>  !</i></div><div><i>  CALL  MPI_TYPE_VECTOR(1, nVar, nVar, coltype, MPI_WENO_TYPE, MPIdata%iErr) </i></div><div><i>  CALL  MPI_TYPE_COMMIT(MPI_WENO_TYPE, MPIdata%iErr)</i></div></div><div><br></div><div><br></div><div>do you believe that is here the problem?</div><div>Is also this the way how intel MPI create a datatype?</div><div><br></div><div>maybe I could also ask to intel MPI users</div><div>What do you think?</div><div><br clear="all"><div><div>Diego<br><br></div></div>
</div></div>
</blockquote></div></div>

