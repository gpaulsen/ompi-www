<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">When you say "stuck", what actually happens?<div><br></div><div>On Aug 10, 2011, at 2:09 PM, CB wrote:</div><div><div><br class="Apple-interchange-newline"><blockquote type="cite">Now I was able to run MPI hello world example up to 3096 processes across 129 nodes (24 cores per node).<br>However, it seems to get stuck with 3097 processes.<br><br>Any suggestions for troubleshooting?<br><br>Thanks,<br>
- Chansup<br><br><br><div class="gmail_quote">On Tue, Aug 9, 2011 at 2:02 PM, CB <span dir="ltr">&lt;<a href="mailto:cbalways@gmail.com">cbalways@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">
Hi Ralph,<br><br>Yes, you are right. Those nodes were still pointing to an old version.<br>I'll check the installation on all nodes and try to run it again.<br><br>Thanks,<br>- Chansup<div><div></div><div class="h5"><br>
<br><div class="gmail_quote">On Tue, Aug 9, 2011 at 1:48 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">That error makes no sense - line 335 is just a variable declaration. Sure you are not picking up a different version on that node?<br>


<div><div></div><div><br>
<br>
On Aug 9, 2011, at 11:37 AM, CB wrote:<br>
<br>
&gt; Hi,<br>
&gt;<br>
&gt; Currently I'm having trouble to scale an MPI job beyond a certain limit.<br>
&gt; So I'm running an MPI hello example to test beyond 1024 but it failed with the following error with 2048 processes.<br>
&gt; It worked fine with 1024 processes. &nbsp;I have enough file descriptor limit (65536) defined for each process.<br>
&gt;<br>
&gt; I appreciate if anyone gives me any suggestions.<br>
&gt; I'm running (Open MPI) 1.4.3<br>
&gt;<br>
&gt; [x-01-06-a:25989] [[37568,0],69] ORTE_ERROR_LOG: Data unpack had inadequate space in file base/odls_base_default_fns.c at line 335<br>
&gt; [x-01-06-b:09532] [[37568,0],74] ORTE_ERROR_LOG: Data unpack had inadequate space in file base/odls_base_default_fns.c at line 335<br>
&gt; --------------------------------------------------------------------------<br>
&gt; mpirun noticed that the job aborted, but has no info as to the process<br>
&gt; that caused that situation.<br>
&gt; --------------------------------------------------------------------------<br>
&gt; [x-03-20-b:23316] *** Process received signal ***<br>
&gt; [x-03-20-b:23316] Signal: Segmentation fault (11)<br>
&gt; [x-03-20-b:23316] Signal code: Address not mapped (1)<br>
&gt; [x-03-20-b:23316] Failing at address: 0x6c<br>
&gt; [x-03-20-b:23316] [ 0] /lib64/libpthread.so.0 [0x310860ee90]<br>
&gt; [x-03-20-b:23316] [ 1] /usr/local/MPI/openmpi-1.4.3/lib/libopen-rte.so.0(orte_plm_base_app_report_launch+0x230) [0x7f0dbe0c5010]<br>
&gt; [x-03-20-b:23316] [ 2] /usr/local/MPI/openmpi-1.4.3/lib/libopen-pal.so.0 [0x7f0dbde5c8f8]<br>
&gt; [x-03-20-b:23316] [ 3] mpirun [0x403bbe]<br>
&gt; [x-03-20-b:23316] [ 4] /usr/local/MPI/openmpi-1.4.3/lib/libopen-pal.so.0 [0x7f0dbde5c8f8]<br>
&gt; [x-03-20-b:23316] [ 5] /usr/local/MPI/openmpi-1.4.3/lib/libopen-pal.so.0(opal_progress+0x99) [0x7f0dbde50e49]<br>
&gt; [x-03-20-b:23316] [ 6] /usr/local/MPI/openmpi-1.4.3/lib/libopen-rte.so.0(orte_trigger_event+0x42) [0x7f0dbe0a7ca2]<br>
&gt; [x-03-20-b:23316] [ 7] /usr/local/MPI/openmpi-1.4.3/lib/libopen-rte.so.0(orte_plm_base_app_report_launch+0x22d) [0x7f0dbe0c500d]<br>
&gt; [x-03-20-b:23316] [ 8] /usr/local/MPI/openmpi-1.4.3/lib/libopen-pal.so.0 [0x7f0dbde5c8f8]<br>
&gt; [x-03-20-b:23316] [ 9] /usr/local/MPI/openmpi-1.4.3/lib/libopen-pal.so.0(opal_progress+0x99) [0x7f0dbde50e49]<br>
&gt; [x-03-20-b:23316] [10] /usr/local/MPI/openmpi-1.4.3/lib/libopen-rte.so.0(orte_plm_base_launch_apps+0x23d) [0x7f0dbe0c5ddd]<br>
&gt; [x-03-20-b:23316] [11] /usr/local/MPI/openmpi-1.4.3/lib/openmpi/mca_plm_rsh.so [0x7f0dbd41d679]<br>
&gt; [x-03-20-b:23316] [12] mpirun [0x40373f]<br>
&gt; [x-03-20-b:23316] [13] mpirun [0x402a1c]<br>
&gt; [x-03-20-b:23316] [14] /lib64/libc.so.6(__libc_start_main+0xfd) [0x3107e1ea2d]<br>
&gt; [x-03-20-b:23316] [15] mpirun [0x402939]<br>
&gt; [x-03-20-b:23316] *** End of error message ***<br>
&gt; [x-01-06-a:25989] [[37568,0],69]-[[37568,0],0] mca_oob_tcp_msg_recv: readv failed: Connection reset by peer (104)<br>
&gt; [x-01-06-b:09532] [[37568,0],74]-[[37568,0],0] mca_oob_tcp_msg_recv: readv failed: Connection reset by peer (104)<br>
&gt; ./sge_jsb.sh: line 9: 23316 Segmentation fault &nbsp; &nbsp; &nbsp;(core dumped) mpirun -np $NSLOTS ./hello_openmpi.exe<br>
&gt;<br>
&gt;<br>
</div></div>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br>
</div></div></blockquote></div><br>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
