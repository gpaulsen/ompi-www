<html><head><meta http-equiv="Content-Type" content="text/html charset=gb18030"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">Forgive me, but I have no idea what that output means. Why do you think only 3 processors are being used?<div><br><div style=""><div>On Dec 9, 2013, at 5:05 AM, 胡杨 &lt;<a href="mailto:781578278@qq.com">781578278@qq.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div>&nbsp;I have a&nbsp;server &nbsp;with&nbsp; 12 cores.when I run mpi program with 10 processors.only&nbsp; three processors works.Here are a picture about the problem</div>
<div><!--StartFragment -->
<div>&nbsp;</div></div>
<div>
<div><span>&lt;40F6DC95@E690AF16.27C0A552.jpg&gt;</span><br></div>
<div>&nbsp;</div>
<div>why?Is the problem with&nbsp;<span style="WHITE-SPACE: normal; TEXT-TRANSFORM: none; WORD-SPACING: 0px; FLOAT: none; COLOR: rgb(67,67,67); TEXT-ALIGN: left; FONT: bold 12px/19px Tahoma, Arial; DISPLAY: inline !important; LETTER-SPACING: normal; BACKGROUND-COLOR: rgb(242,242,242); TEXT-INDENT: 0px; -webkit-text-stroke-width: 0px">process schedule??&nbsp;&nbsp; </span><br></div>
<div style="FONT-SIZE: 12px; FONT-FAMILY: Arial Narrow; PADDING-BOTTOM: 2px; PADDING-TOP: 2px; PADDING-LEFT: 0px; PADDING-RIGHT: 0px">------------------&nbsp;原始邮件&nbsp;------------------</div>
<div style="FONT-SIZE: 12px; BACKGROUND: #efefef; PADDING-BOTTOM: 8px; PADDING-TOP: 8px; PADDING-LEFT: 8px; PADDING-RIGHT: 8px">
<div><b>发件人:</b>&nbsp;"Bruno Coutinho";&lt;<a href="mailto:coutinho@dcc.ufmg.br">coutinho@dcc.ufmg.br</a>&gt;;</div>
<div><b>发送时间:</b>&nbsp;2013年12月6日(星期五) 晚上11:14</div>
<div><b>收件人:</b>&nbsp;"Open MPI Users"&lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;; <wbr></div>
<div></div>
<div><b>主题:</b>&nbsp;Re: [OMPI users]回复： can you help me please ?thanks</div></div>
<div><br></div>
<div dir="ltr">Probably it was the changing from eager to rendezvous protocols as Jeff said. 
<div><br></div>
<div>If you don't know what are these, read this:</div>
<div><a href="https://computing.llnl.gov/tutorials/mpi_performance/#Protocols">https://computing.llnl.gov/tutorials/mpi_performance/#Protocols</a><br></div>
<div><a href="http://blogs.cisco.com/performance/what-is-an-mpi-eager-limit/">http://blogs.cisco.com/performance/what-is-an-mpi-eager-limit/</a><br></div>
<div><a href="http://blogs.cisco.com/performance/eager-limits-part-2/">http://blogs.cisco.com/performance/eager-limits-part-2/</a><br></div>
<div><br></div>
<div>You can tune eager limit chaning mca parameters btl_tcp_eager_limit (for tcp), btl_self_eager_limit (comunication fron one process to itself), btl_sm_eager_limit (shared memory) and btl_udapl_eager_limit or btl_openib_eager_limit (if you use infiniband).</div>
<div class="gmail_extra"><br><br>
<div class="gmail_quote">2013/12/6 Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span><br>
<blockquote class="gmail_quote" style="PADDING-LEFT: 1ex; MARGIN: 0px 0px 0px 0.8ex; BORDER-LEFT: #ccc 1px solid">I sent you some further questions yesterday:<br><br>&nbsp; &nbsp; <a href="http://www.open-mpi.org/community/lists/users/2013/12/23158.php" target="_blank">http://www.open-mpi.org/community/lists/users/2013/12/23158.php</a><br>
<div class="HOEnZb">
<div class="h5"><br><br>On Dec 6, 2013, at 1:35 AM, 胡杨 &lt;<a href="mailto:781578278@qq.com">781578278@qq.com</a>&gt; wrote:<br><br>&gt; Here is &nbsp;my code:<br>&gt; int*a=(int*)malloc(sizeof(int)*number);<br>&gt; MPI_Send(a,number, MPI_INT, 1, 1,MPI_COMM_WORLD);<br>&gt;<br>&gt; int*b=(int*)malloc(sizeof(int)*number);<br>&gt; MPI_Recv(b, number, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status);<br>&gt;<br>&gt; number &nbsp;here is the size of my array(eg,a or b).<br>&gt; I &nbsp;have try it on my local compute and my rocks cluster.On rocks cluster, one processor &nbsp;on &nbsp;one frontend node &nbsp;use "MPI_Send" send a message ,other processors on compute nodes use "MPI_Recv" receive message .<br>&gt; when number is least than 10000,other processors can receive message fast;<br>&gt; but when &nbsp;number is more than 15000,other processors can receive message slowly<br>&gt; why?? &nbsp;becesue openmpi API ?? or other &nbsp;problems?<br>&gt;<br>&gt; it spends me a few days , I want your help,thanks for all readers. good luck for you<br>&gt;<br>&gt;<br>&gt;<br>&gt;<br>&gt; ------------------ 原始邮件 ------------------<br>&gt; 发件人: "Ralph Castain";&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;;<br>&gt; 发送时间: 2013年12月5日(星期四) 晚上6:52<br>&gt; 收件人: "Open MPI Users"&lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;;<br>&gt; 主题: Re: [OMPI users] can you help me please ?thanks<br>&gt;<br>&gt; You are running 15000 ranks on two nodes?? My best guess is that you are swapping like crazy as your memory footprint problem exceeds available physical memory.<br>&gt;<br>&gt;<br>&gt;<br>&gt; On Thu, Dec 5, 2013 at 1:04 AM, 胡杨 &lt;<a href="mailto:781578278@qq.com">781578278@qq.com</a>&gt; wrote:<br>&gt; My ROCKS cluster includes one frontend and two &nbsp;compute nodes.In my program,I have use the openmpi API &nbsp;such as &nbsp;MPI_Send and &nbsp;MPI_Recv . &nbsp;but &nbsp;when I &nbsp;run &nbsp;the progam with 3 processors . one processor &nbsp;send a message ,other receive message .here are some code.<br>&gt; int*a=(int*)malloc(sizeof(int)*number);<br>&gt; MPI_Send(a,number, MPI_INT, 1, 1,MPI_COMM_WORLD);<br>&gt;<br>&gt; int*b=(int*)malloc(sizeof(int)*number);<br>&gt; MPI_Recv(b, number, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status);<br>&gt;<br>&gt; when number is least than 10000,it runs fast.<br>&gt; but number is more than 15000,it runs slowly<br>&gt;<br>&gt; why?? &nbsp;becesue openmpi API ?? or other &nbsp;problems?<br>&gt; ------------------ 原始邮件 ------------------<br>&gt; 发件人: "Ralph Castain";&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;;<br>&gt; 发送时间: 2013年12月3日(星期二) 中午1:39<br>&gt; 收件人: "Open MPI Users"&lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;;<br>&gt; 主题: Re: [OMPI users] can you help me please ?thanks<br>&gt;<br>&gt;<br>&gt;<br>&gt;<br>&gt;<br>&gt; On Mon, Dec 2, 2013 at 9:23 PM, 胡杨 &lt;<a href="mailto:781578278@qq.com">781578278@qq.com</a>&gt; wrote:<br>&gt; A simple program at my 4-node ROCKS cluster runs fine with command:<br>&gt; /opt/openmpi/bin/mpirun -np 4 -machinefile machines ./sort_mpi6<br>&gt;<br>&gt;<br>&gt; Another bigger programs runs fine on the head node only with command:<br>&gt;<br>&gt; cd ./sphere; /opt/openmpi/bin/mpirun -np 4 ../bin/sort_mpi6<br>&gt;<br>&gt; But with the command:<br>&gt;<br>&gt; cd /sphere; /opt/openmpi/bin/mpirun -np 4 -machinefile ../machines<br>&gt; ../bin/sort_mpi6<br>&gt;<br>&gt; It gives output that:<br>&gt;<br>&gt; ../bin/sort_mpi6: error while loading shared libraries: libgdal.so.1: cannot open<br>&gt; shared object file: No such file or directory<br>&gt; ../bin/sort_mpi6: error while loading shared libraries: libgdal.so.1: cannot open<br>&gt; shared object file: No such file or directory<br>&gt; ../bin/sort_mpi6: error while loading shared libraries: libgdal.so.1: cannot open<br>&gt; shared object file: No such file or directory<br>&gt;<br>&gt;<br>&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<br>&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><br></div></div>
<div class="im HOEnZb">--<br>Jeff Squyres<br><a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br><br></div>
<div class="HOEnZb">
<div class="h5">_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></div></blockquote></div><br></div></div>
<div></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
