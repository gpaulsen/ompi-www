<div dir="ltr">Dear all,<div><br></div><div>I have notice small difference between OPEN-MPI and intel MPI. </div><div>For example in MPI_ALLREDUCE in intel MPI is not allowed to use the same variable in send and receiving Buff.</div><div><br></div><div>I have written my code in OPEN-MPI, but unfortunately I have to run in on a intel-MPI cluster. </div><div>Now I have the following error:</div><div><br></div><div><div><i>atal error in MPI_Isend: Invalid communicator, error stack:</i></div><div><i>MPI_Isend(158): MPI_Isend(buf=0x1dd27b0, count=1, INVALID DATATYPE, dest=0, tag=0, comm=0x0, request=0x7fff9d7dd9f0) failed</i></div></div><div><br></div><div><br></div><div>This is ho I create my type:</div><div><br></div><div><div><i>  CALL  MPI_TYPE_VECTOR(1, Ncoeff_MLS, Ncoeff_MLS, MPI_DOUBLE_PRECISION, coltype, MPIdata%iErr) </i></div><div><i>  CALL  MPI_TYPE_COMMIT(coltype, MPIdata%iErr)</i></div><div><i>  !</i></div><div><i>  CALL  MPI_TYPE_VECTOR(1, nVar, nVar, coltype, MPI_WENO_TYPE, MPIdata%iErr) </i></div><div><i>  CALL  MPI_TYPE_COMMIT(MPI_WENO_TYPE, MPIdata%iErr)</i></div></div><div><br></div><div><br></div><div>do you believe that is here the problem?</div><div>Is also this the way how intel MPI create a datatype?</div><div><br></div><div>maybe I could also ask to intel MPI users</div><div>What do you think?</div><div><br clear="all"><div><div class="gmail_signature">Diego<br><br></div></div>
</div></div>

