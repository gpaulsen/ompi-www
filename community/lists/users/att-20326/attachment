Well, not sure what I can advise. Check to ensure that your LD_LIBRARY_PATH is pointing to the same installation where your mpirun is located. For whatever reason, the processes think they are singletons - i.e., that they were not actually started by mpirun.<br>
<br>You might also want to ask the mpi4py folks - we aren&#39;t very familiar with that package over here. It could be that you need to configure it for OpenMPI as opposed to mpich.<br><br><br><div class="gmail_quote">On Tue, Sep 25, 2012 at 7:08 PM, Mariana Vargas Magana <span dir="ltr">&lt;<a href="mailto:mmarianav@yahoo.com.mx" target="_blank">mmarianav@yahoo.com.mx</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word"><div><br></div>Yes I am sure I read from a mpi4py guide I already check the examples if fact this an example extracted from a guide…!! Evenmore this example if I use with mpich2 it runs very nicely, even though for the other code I need openmpi working =s<span class="HOEnZb"><font color="#888888"><div>
<br></div></font></span><div><span class="HOEnZb"><font color="#888888">Mariana</font></span><div><div class="h5"><br><div><br></div><div><br><div><div>On Sep 25, 2012, at 8:00 PM, Ralph Castain &lt;<a href="mailto:rhc.openmpi@gmail.com" target="_blank">rhc.openmpi@gmail.com</a>&gt; wrote:</div>
<br><blockquote type="cite"><div bgcolor="#FFFFFF"><div>I don&#39;t think that is true, but I suggest you check the mpi4py examples. I believe all import does is import function definitions - it doesn&#39;t execute anything.<br>
<br>Sent from my iPad</div><div><br>On Sep 25, 2012, at 2:41 PM, mariana Vargas &lt;<a href="mailto:mmarianav@yahoo.com.mx" target="_blank">mmarianav@yahoo.com.mx</a>&gt; wrote:<br><br></div><div></div><blockquote type="cite">
MPI_init() is actually called when import MPI module from MPi package...<div><br></div><div><br><div><div>On Sep 25, 2012, at 5:17 PM, Ralph Castain wrote:</div><br><blockquote type="cite"><div style="word-wrap:break-word">
You forgot to call MPI_Init at the beginning of your program.<div><br><div><div>On Sep 25, 2012, at 2:08 PM, Mariana Vargas Magana &lt;<a href="mailto:mmarianav@yahoo.com.mx" target="_blank">mmarianav@yahoo.com.mx</a>&gt; wrote:</div>
<br><blockquote type="cite"><div style="word-wrap:break-word"><div><div><div>Hi</div><div>I think I&#39;am not understanding what you said , here is the hello.py and next the command mpirun…</div><div><br></div><div>Thanks!</div>
<div><br></div><div>#!/usr/bin/env python</div><div>&quot;&quot;&quot;</div></div><div>Parallel Hello World</div><div>&quot;&quot;&quot;</div><div><br></div><div>from mpi4py import MPI</div><div>import sys</div><div><br></div>
<div>size = MPI.COMM_WORLD.Get_size()</div><div>rank = MPI.COMM_WORLD.Get_rank()</div><div>name = MPI.Get_processor_name()</div><div><br></div><div>sys.stdout.write(</div><div>    &quot;Hello, World! I am process %d of %d on %s.\n&quot;</div>
<div>    % (rank, size, name))</div></div><div><br></div><div> ~/bin/mpirun -np 70 python2.7 helloworld.py </div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div>
<div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div>
<div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div>
<div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div><br></div><div><div>On Sep 25, 2012, at 4:46 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt; wrote:</div>
<br><blockquote type="cite"><div style="word-wrap:break-word">The usual reason for this is that you aren&#39;t launching these processes correctly. How are you starting your job? Are you using mpirun?<div><br></div><div><br>
<div><div>On Sep 25, 2012, at 1:43 PM, mariana Vargas &lt;<a href="mailto:mmarianav@yahoo.com.mx" target="_blank">mmarianav@yahoo.com.mx</a>&gt; wrote:</div><br><blockquote type="cite"><div style="word-wrap:break-word"><div>
Hi</div><div><br></div><div>I fact I found what is the origin of this problem and it is because all processes have rank 0, so I tested and in effect even when I send the clasical Hello.py give the same, how can I solved this?? Do I  re installed every again???</div>
<div><br></div><div>Help please...</div><div><br></div><div>Mariana</div><div><br></div><div><br></div><br><div><div>On Sep 24, 2012, at 9:13 PM, Mariana Vargas Magana wrote:</div><br><blockquote type="cite"><div style="word-wrap:break-word">
<div><br></div><div><br></div><div>Yes you are right this is what it says but if fact the weird thing is that not all times the error message appears….I send to 20 nodes and only one gives this message, is this normal…</div>
<div><br></div><div><br></div><div><br></div><br><div><div>On Sep 24, 2012, at 8:00 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt; wrote:</div><br><blockquote type="cite">
<div style="word-wrap:break-word">Well, as it says, your processes called MPI_Init, but at least one of them exited without calling MPI_Finalize. That violates the MPI rules and we therefore terminate the remaining processes.<div>
<br></div><div>Check your code and see how/why you are doing that - you probably have a code path whereby a process exits without calling finalize.</div><div><br></div><div><br><div><div>On Sep 24, 2012, at 4:37 PM, mariana Vargas &lt;<a href="mailto:mmarianav@yahoo.com.mx" target="_blank">mmarianav@yahoo.com.mx</a>&gt; wrote:</div>
<br><blockquote type="cite"><div style="word-wrap:break-word"><br><div><br><blockquote type="cite"></blockquote><font color="#144FAE"><br></font><blockquote type="cite"></blockquote>Hi all<br><blockquote type="cite"></blockquote>
<font color="#144FAE"><br></font><blockquote type="cite"></blockquote>I get this error when I run a paralelized python code in a cluster, could anyone give me an idea of what is happening? I&#39;am new in this Thanks...<br>
<blockquote type="cite"></blockquote><font color="#144FAE"><br></font><blockquote type="cite"></blockquote>mpirun has exited due to process rank 2 with PID 10259 on<br><blockquote type="cite"></blockquote>node f01 exiting improperly. There are two reasons this could occur:<br>
<blockquote type="cite"></blockquote><font color="#144FAE"><br></font><blockquote type="cite"></blockquote>1. this process did not call &quot;init&quot; before exiting, but others in<br><blockquote type="cite"></blockquote>
the job did. This can cause a job to hang indefinitely while it waits<br><blockquote type="cite"></blockquote>for all processes to call &quot;init&quot;. By rule, if one process calls &quot;init&quot;,<br><blockquote type="cite">
</blockquote>then ALL processes must call &quot;init&quot; prior to termination.<br><blockquote type="cite"></blockquote><font color="#144FAE"><br></font><blockquote type="cite"></blockquote>2. this process called &quot;init&quot;, but exited without calling &quot;finalize&quot;.<br>
<blockquote type="cite"></blockquote>By rule, all processes that call &quot;init&quot; MUST call &quot;finalize&quot; prior to<br><blockquote type="cite"></blockquote>exiting or it will be considered an &quot;abnormal termination&quot;<br>
<blockquote type="cite"></blockquote><font color="#144FAE"><br></font><blockquote type="cite"></blockquote>This may have caused other processes in the application to be<br><blockquote type="cite"></blockquote>terminated by signals sent by mpirun (as reported here).<br>
<blockquote type="cite"></blockquote><font color="#144FAE"><br></font><blockquote type="cite"></blockquote>Thanks!!<br><blockquote type="cite"></blockquote><font color="#144FAE"><br></font><blockquote type="cite"></blockquote>
<font color="#144FAE"><br></font><blockquote type="cite"></blockquote><font color="#144FAE"><br></font><blockquote type="cite"><br>Dr. Mariana Vargas Magana<br>Astroparticule et Cosmologie - Bureau 409B<br>PHD student- Université Denis Diderot-Paris 7<br>
10, rue Alice Domon et Léonie Duquet<br>75205 Paris Cedex - France<br>Tel. <a href="tel:%2B33%20%280%291%2057%2027%2070%2032" value="+33157277032" target="_blank">+33 (0)1 57 27 70 32</a><br>Fax. <a href="tel:%2B33%20%280%291%2057%2027%2060%2071" value="+33157276071" target="_blank">+33 (0)1 57 27 60 71</a><br>
<a href="mailto:mariana@apc.univ-paris7.fr" target="_blank">mariana@apc.univ-paris7.fr</a><br></blockquote></div><br></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote></div><br></div></div>_______________________________________________<br>users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div></blockquote><blockquote type="cite"><span>_______________________________________________</span><br><span>users mailing list</span><br><span><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a></span><br>
<span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></span></blockquote></div>_______________________________________________<br>users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div></div></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

