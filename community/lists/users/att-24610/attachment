<div dir="ltr"><div class="gmail_extra"><br><div class="gmail_quote">On Mon, Jun 9, 2014 at 3:21 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div style="word-wrap:break-word"><br><div><div class=""><div>
On Jun 9, 2014, at 2:41 PM, Vineet Rawat &lt;<a href="mailto:vineetrawat0@gmail.com" target="_blank">vineetrawat0@gmail.com</a>&gt; wrote:</div><br><blockquote type="cite"><div dir="ltr"><span style="font-family:arial,sans-serif;font-size:13px">Hi,</span><div style="font-family:arial,sans-serif;font-size:13px">
<br></div><div style="font-family:arial,sans-serif;font-size:13px">We&#39;ve deployed OpenMPI on a small cluster but get a SEGV in orted. Debug information is very limited as the cluster is at a remote customer site. They have a network card with which I&#39;m not familiar (Cisco Systems Inc VIC P81E PCIe Ethernet NIC) and it seems capable of using the usNIC BTL. I&#39;m suspicious that it might be at the root of the problem. They&#39;re also bonding the 2 ports.</div>
</div></blockquote><div><br></div></div>This shouldn&#39;t matter - the VIC should work fine.</div></div></blockquote><div><br></div><div>Great, glad to hear that.</div><div> </div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">
<div style="word-wrap:break-word"><div><div class=""><br><blockquote type="cite"><div dir="ltr">
<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">However, we&#39;re also doing a few unusual things which could be causing problems. Firstly, we built OpenMPI (I tried 1.6.4 and 1.8.1) without the ibverbs or usnic BTLs. Then, we only ship what (we think) we need: otrerun, orted, libmpi, libmpi_cxx, libopen-rte and libopen-pal. Could there be a dependency on some other binary executable or dlopen&#39;ed library? We also use a special plm_rsh_agent but we&#39;ve used this approach for some time without issue.</div>
</div></blockquote><div><br></div></div>Did you remember to include all the libraries under &lt;prefix&gt;/lib/openmpi? We need all of those or else the orted will fail.</div></div></blockquote><div><br></div><div>No, we only included what seemed necessary (from ldd output and experience on other clusters). The only things in my &lt;prefix&gt;/lib/openmpi are libompi_dbg_msgq*. Is that what you&#39;re referring to? In &lt;prefix&gt;/lib for 12.8.1 (ignoring the VampirTrace libs) I could add libmpi_mpifh, libmpi_usempi, libompitrace and/or liboshmem. Anything needed there?</div>
<div><br></div><div>Thanks for the help,</div><div>Vineet</div><div> </div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">
<div style="word-wrap:break-word"><div><br><blockquote type="cite"><div class=""><div dir="ltr">
<div style="font-family:arial,sans-serif;font-size:13px">I tried a few different MCA settings, the most restrictive of which led to the failure of this command:<br></div><div style="font-family:arial,sans-serif;font-size:13px">

<br>orted --debug --debug-daemons -mca ess env -mca orte_ess_jobid 1925054464 -mca orte_ess_vpid 1 -mca orte_ess_num_procs 2 -mca orte_hnp_uri \&quot;1925054464.0;<a>tcp://10.xxx.xxx.xxx:40547\</a>&quot; --tree-spawn --mca orte_base_help_aggregate 1 --mca plm_rsh_agent yyy --mca btl_tcp_port_min_v4 2000 --mca btl_tcp_port_range_v4 100 --mca btl tcp,self --mca btl_tcp_if_include bond0 --mca orte_create_session_dirs 0 --mca plm_rsh_assume_same_shell 0 -mca plm rsh -mca orte_debug_daemons 1 -mca orte_debug 1 -mca orte_tag_output 1<br>

</div><div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">It seems that the host is set up such that the core file is generated and immediately removed (&quot;ulimit -c&quot; is unlimited) but the abrt daemon is doing something weird. I&#39;ll be trying to get access to the system so I can use &quot;--mca orte orte_daemon_spin&quot; and attach a debugger (if that&#39;s how that&#39;s done). If I&#39;m able to debug or obtain a core file I&#39;ll provide more information. I&#39;ve attached some information regarding the hardware, OpenMPI&#39;s configuration and ompi_info output. Any thoughts?</div>

<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">Thanks,</div><div style="font-family:arial,sans-serif;font-size:13px">Vineet</div></div>
</div><span>&lt;orted_segv.tar.gz&gt;</span>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div></div><br></div>

