<div dir="ltr"><div><br></div>     I ran some aggregate bandwidth tests between 2 hosts connected by <div>both QDR InfiniBand and RoCE enabled 10 Gbps Mellanox cards.  The tests</div><div>measured the aggregate performance for 16 cores on one host communicating</div><div>with 16 on the second host.  I saw the same performance as with the QDR </div><div>InfiniBand alone, so it appears that the addition of the 10 Gbps RoCE cards is</div><div>not helping.</div><div><br></div><div>     Should OpenMPI be using both in this case by default, or is there something</div><div>I need to configure to allow for this?  I suppose this is the same question as </div><div>how to make use of 2 identical IB connections on each node, or is the system</div><div>simply ignoring the 10 Gbps cards because they are the slower option.</div><div><br></div><div>     Any clarification on this would be helpful.  The only posts I&#39;ve found are very</div><div>old and discuss mostly channel bonding of 1 Gbps cards.</div><div><br></div><div>                     Dave Turner<br clear="all"><div><br></div>-- <br><div class="gmail_signature"><div dir="ltr">Work:     <a href="mailto:DaveTurner@ksu.edu" target="_blank">DaveTurner@ksu.edu</a>     (785) 532-7791<div>             118 Nichols Hall, Manhattan KS  66502<br>Home:    <a href="mailto:DrDaveTurner@gmail.com" target="_blank">DrDaveTurner@gmail.com</a><br>              cell: (785) 770-5929<br></div></div></div>
</div></div>

