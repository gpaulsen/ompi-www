<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="Generator" content="Microsoft Exchange Server">
<!-- converted from rtf -->
<style><!-- .EmailQuote { margin-left: 1pt; padding-left: 4pt; border-left: #800000 2px solid; } --></style>
</head>
<body>
<font face="Calibri" size="2"><span style="font-size:11pt;">
<div style="margin-bottom:10pt;">Hi,</div>
<div style="margin-bottom:10pt;">I&#8217;m trying to compare the semantics of MPI RMA with those of ARMCI. I&#8217;ve written a small test program that writes data to a remote processor and then reads the data back to the original processor. In ARMCI, you should be able
to do this since operations to the same remote processor are completed in the same order that they are requested on the calling processor. I&#8217;ve implemented this two different ways using MPI RMA. The first is to call MPI_Win_lock to create a shared lock on the
remote processor, then MPI_Put/MPI_Get to initiate the data transfer and finally MPI_Win_unlock to force completion of the data transfer. My understanding is that this should allow you to write and then read data to the same process, since the first triplet</div>
<div>MPI_Win_lock</div>
<div>MPI_Put</div>
<div style="margin-bottom:10pt;">MPI_Win_unlock</div>
<div style="margin-bottom:10pt;">must be completed both locally and remotely before the unlock call completes. The calls in the second triplet</div>
<div>MPI_Win_lock</div>
<div>MPI_Get</div>
<div style="margin-bottom:10pt;">MPI_Win_unlock</div>
<div style="margin-bottom:10pt;">cannot start until the first triplet is done, so if both the put and the get refer to the same data on the same remote processor, then it should work.</div>
<div style="margin-bottom:10pt;">The second implementation uses request-based RMA and starts by calling MPI_Win_lock_all collectively on the window when it is created and MPI_Win_unlock_all when it is destroy so that the window is always in a passive synchronization
epoch. The put is implement by calling MPI_Rput followed by calling MPI_Wait on the handle returned from the MPI_Rput call. Similarly, get is implemented by calling MPI_Rget followed by MPI_Wait. The wait call guarantees that the operation is completed locally
and the data can then be used. However, from what I understand of the standard, it doesn&#8217;t say anything about the ordering of the operations so conceivably the put could execute remotely before the get. Inserting an MPI_Win_flush_all between the MPI_Rput and
MPI_Rget should guarantee that the operations are ordered.</div>
<div style="margin-bottom:10pt;">I&#8217;ve written the test program so that it can use either the lock or request-based implementations and I&#8217;ve also included an option that inserts a fence/flush plus barrier operation between put and get. The different configurations
can be set up by defining some preprocessor symbols at the top of the program.&nbsp; The program loops over the test repeatedly and the current number of loops is set at 2000. The results I get running on a Linux cluster with an Infiniband network using OpenMPI-1.10.1
on 2 processors on 2 different SMP nodes are as follows:</div>
<div style="margin-bottom:10pt;">Using OpenMPI-1.8.3:</div>
<div style="margin-bottom:10pt;">Request-based implementation without synchronization: 9 successes out of 10 runs</div>
<div style="margin-bottom:10pt;">Request-based implementation with synchronization: 19 successes out of 20 runs</div>
<div style="margin-bottom:10pt;">Lock-based implementation without synchronization: 1 success out of 10 runs</div>
<div style="margin-bottom:10pt;">Lock-based implementation with synchronization: 1 success out of 10 runs</div>
<div style="margin-bottom:10pt;">Using OpenMPI-1.10.1</div>
<div style="margin-bottom:10pt;">Request-based implementation without synchronization: 2 successes out of 10 runs</div>
<div style="margin-bottom:10pt;">Request-based implementation with synchronization: 8 successes out of 10 runs</div>
<div style="margin-bottom:10pt;">Lock-based implementation without synchronization: 4 successes out of 10 runs</div>
<div style="margin-bottom:10pt;">Lock-based implementation with synchronization: 2 successes out of 10 runs</div>
<div style="margin-bottom:10pt;">Except for the request-based implementation without synchronization (in this case a call to MPI_Win_flush_all), I would expect these to all succeed. Is there some fault to my thinking here? I&#8217;ve attached the test program</div>
<div style="margin-bottom:10pt;">Bruce Palmer</div>
<div style="margin-bottom:10pt;"> </div>
<div style="margin-bottom:10pt;"><font face="Times New Roman">&nbsp;</font></div>
</span></font>
</body>
</html>

