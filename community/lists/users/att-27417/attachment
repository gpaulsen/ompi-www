<html dir="ltr">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<style id="owaParaStyle" type="text/css">P {margin-top:0;margin-bottom:0;}</style>
</head>
<body ocsi="0" fpstyle="1">
<div style="direction: ltr;font-family: Tahoma;color: #000000;font-size: 10pt;">Here's a qrsh run of OpenMPI 1.8.7 that actually generated an error message, usually<br>
I get no output whatsoever (i.e. from stderr or stdout) from the job, and it eventually<br>
generates core dumps:<br>
<br>
qrsh -V -now yes -pe orte 209 mpirun -np 209 -display-devel-map --prefix /hpc/apps/mpi/openmpi/1.8.7/ --mca btl ^sm --hetero-nodes --bind-to core /hpc/home/lanew/mpi/openmpi/ProcessColors3<br>
--------------------------------------------------------------------------<br>
WARNING: a request was made to bind a process. While the system<br>
supports binding the process itself, at least one node does NOT<br>
support binding memory to the process location.<br>
<br>
&nbsp; Node:&nbsp; csclprd3-4-2<br>
<br>
This usually is due to not having the required NUMA support installed<br>
on the node. In some Linux distributions, the required support is<br>
contained in the libnumactl and libnumactl-devel packages.<br>
This is a warning only; your job will continue, though performance may be degraded.<br>
--------------------------------------------------------------------------<br>
--------------------------------------------------------------------------<br>
A request was made to bind to that would result in binding more<br>
processes than cpus on a resource:<br>
<br>
&nbsp;&nbsp; Bind to:&nbsp;&nbsp;&nbsp;&nbsp; CORE<br>
&nbsp;&nbsp; Node:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; csclprd3-4-2<br>
&nbsp;&nbsp; #processes:&nbsp; 2<br>
&nbsp;&nbsp; #cpus:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1<br>
<br>
You can override this protection by adding the &quot;overload-allowed&quot;<br>
option to your binding directive.<br>
--------------------------------------------------------------------------<br>
<br>
I'm using CentOS 6.3 and Son-of-Gridengine as my scheduling agent.<br>
<br>
The relevant NUMA libraries have been installed to the cluster:<br>
<br>
<blockquote>csclprd3-4-2 ~]$ yum list installed *numa*<br>
</blockquote>
<blockquote>Installed Packages<br>
numactl.x86_64&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.0.7-3.el6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; @centos6.3-x86_64-0/$releasever&nbsp;&nbsp;
<br>
numactl-devel.x86_64<br>
</blockquote>
<br>
Here's the lstopo output for the node in question (a x3550-M3 node w/6-core Westmere CPU's and hyperthreading):<br>
csclprd3-4-2 ~]$ lstopo<br>
Machine (96GB)<br>
&nbsp; NUMANode L#0 (P#0 48GB) &#43; Socket L#0 &#43; L3 L#0 (12MB)<br>
&nbsp;&nbsp;&nbsp; L2 L#0 (256KB) &#43; L1d L#0 (32KB) &#43; L1i L#0 (32KB) &#43; Core L#0<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#0 (P#0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#1 (P#12)<br>
&nbsp;&nbsp;&nbsp; L2 L#1 (256KB) &#43; L1d L#1 (32KB) &#43; L1i L#1 (32KB) &#43; Core L#1<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#2 (P#1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#3 (P#13)<br>
&nbsp;&nbsp;&nbsp; L2 L#2 (256KB) &#43; L1d L#2 (32KB) &#43; L1i L#2 (32KB) &#43; Core L#2<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#4 (P#2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#5 (P#14)<br>
&nbsp;&nbsp;&nbsp; L2 L#3 (256KB) &#43; L1d L#3 (32KB) &#43; L1i L#3 (32KB) &#43; Core L#3<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#6 (P#3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#7 (P#15)<br>
&nbsp;&nbsp;&nbsp; L2 L#4 (256KB) &#43; L1d L#4 (32KB) &#43; L1i L#4 (32KB) &#43; Core L#4<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#8 (P#4)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#9 (P#16)<br>
&nbsp;&nbsp;&nbsp; L2 L#5 (256KB) &#43; L1d L#5 (32KB) &#43; L1i L#5 (32KB) &#43; Core L#5<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#10 (P#5)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#11 (P#17)<br>
&nbsp; NUMANode L#1 (P#1 48GB) &#43; Socket L#1 &#43; L3 L#1 (12MB)<br>
&nbsp;&nbsp;&nbsp; L2 L#6 (256KB) &#43; L1d L#6 (32KB) &#43; L1i L#6 (32KB) &#43; Core L#6<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#12 (P#6)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#13 (P#18)<br>
&nbsp;&nbsp;&nbsp; L2 L#7 (256KB) &#43; L1d L#7 (32KB) &#43; L1i L#7 (32KB) &#43; Core L#7<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#14 (P#7)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#15 (P#19)<br>
&nbsp;&nbsp;&nbsp; L2 L#8 (256KB) &#43; L1d L#8 (32KB) &#43; L1i L#8 (32KB) &#43; Core L#8<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#16 (P#8)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#17 (P#20)<br>
&nbsp;&nbsp;&nbsp; L2 L#9 (256KB) &#43; L1d L#9 (32KB) &#43; L1i L#9 (32KB) &#43; Core L#9<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#18 (P#9)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#19 (P#21)<br>
&nbsp;&nbsp;&nbsp; L2 L#10 (256KB) &#43; L1d L#10 (32KB) &#43; L1i L#10 (32KB) &#43; Core L#10<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#20 (P#10)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#21 (P#22)<br>
&nbsp;&nbsp;&nbsp; L2 L#11 (256KB) &#43; L1d L#11 (32KB) &#43; L1i L#11 (32KB) &#43; Core L#11<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#22 (P#11)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PU L#23 (P#23)<br>
<br>
I'm going to setup a PE that has the appropriate parameters for OpenMPI<br>
as described here:<a href="https://www.open-mpi.org/faq/?category=sge" target="_blank">https://www.open-mpi.org/faq/?category=sge</a><br>
<font color="black" face="Tahoma" size="2"><span dir="ltr" style="font-size:10pt"><font face="Times New Roman" size="2"><span style="font-size:16px"><font face="Tahoma" size="2"><span style="font-size:10pt">
<pre style="margin-top:14pt; margin-bottom:14pt"><font face="Tahoma">and re-test</font> w/this PE a<font face="Tahoma">s well as using the </font>--leave-session-attached --mca plm_base_verbose 5<br><font face="Tahoma">mpirun switches.<br><br>-Bill L.<br></font></pre>
</span></font></span></font></span></font><br>
<div style="font-family: Times New Roman; color: #000000; font-size: 16px">
<hr tabindex="-1">
<div style="direction: ltr;" id="divRpF969576"><font color="#000000" face="Tahoma" size="2"><b>From:</b> users [users-bounces@open-mpi.org] on behalf of Ralph Castain [rhc@open-mpi.org]<br>
<b>Sent:</b> Wednesday, August 05, 2015 4:41 PM<br>
<b>To:</b> Open MPI Users<br>
<b>Subject:</b> Re: [OMPI users] Son of Grid Engine, Parallel Environments and OpenMPI 1.8.7<br>
</font><br>
</div>
<div></div>
<div>
<div dir="ltr">Well that stinks! Let me know what's going on and I'll take a look. FWIW, the best method is generally to configure OMPI with --enable-debug, and then run with &quot;--leave-session-attached --mca plm_base_verbose 5&quot;. That will tell us what the launcher
 thinks it is doing and what the daemons think is wrong.
<div><br>
</div>
</div>
<div class="gmail_extra"><br>
<div class="gmail_quote">On Wed, Aug 5, 2015 at 3:17 PM, Lane, William <span dir="ltr">
&lt;<a href="mailto:William.Lane@cshs.org" target="_blank">William.Lane@cshs.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex; border-left:1px #ccc solid; padding-left:1ex">
<div>
<div style="direction:ltr; font-family:Tahoma; color:#000000; font-size:10pt">Actually, we're still having problems submitting OpenMPI 1.8.7 jobs<br>
to the cluster thru SGE (which we need to do in order to track usage<br>
stats on the cluster). I suppose I'll make a PE w/the appropriate settings<br>
and see if that makes a difference.<br>
<br>
-Bill L<br>
<br>
<div style="font-family:Times New Roman; color:#000000; font-size:16px">
<hr>
<div style="direction:ltr"><font color="#000000" face="Tahoma" size="2"><b>From:</b> users [<a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a>] on behalf of Ralph Castain [<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>]<br>
<b>Sent:</b> Wednesday, August 05, 2015 1:18 PM<br>
<b>To:</b> Open MPI Users<br>
<b>Subject:</b> Re: [OMPI users] Son of Grid Engine, Parallel Environments and OpenMPI 1.8.7<br>
</font><br>
</div>
<div>
<div class="h5">
<div></div>
<div>
<div dir="ltr">You know, I honestly don't know - there is a patch in there for qsort, but I haven't checked it against SGE. Let us know if you hit a problem and we'll try to figure it out.
<div><br>
</div>
<div>Glad to hear your cluster is working - nice to have such challenges to shake the cobwebs out :-)</div>
</div>
<div class="gmail_extra"><br>
<div class="gmail_quote">On Wed, Aug 5, 2015 at 12:43 PM, Lane, William <span dir="ltr">
&lt;<a href="mailto:William.Lane@cshs.org" target="_blank">William.Lane@cshs.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex; border-left:1px #ccc solid; padding-left:1ex">
<div>
<div style="direction:ltr; font-family:Tahoma; color:#000000; font-size:10pt">I read @
<font color="black" face="Tahoma" size="2"><span dir="ltr" style="font-size:10pt"><font face="Times New Roman" size="2"><span style="font-size:16px"><font face="Tahoma" size="2"><span style="font-size:10pt">
<pre style="margin-top:14pt; margin-bottom:14pt"><a href="https://www.open-mpi.org/faq/?category=sge" target="_blank">https://www.open-mpi.org/faq/?category=sge</a><br><br><font face="Tahoma">that </font>for OpenMPI Parallel Environments there's
a special consideration for Son of Grid Engine:

   '&quot;qsort_args&quot; is necessary with the Son of Grid Engine distribution,
   version 8.1.1 and later, and probably only applicable to it.  For
   very old versions of SGE, omit &quot;accounting_summary&quot; too.'

<font face="Tahoma">Does this requirement still hold true for OpenMPI 1.8.7? Because<br>the webpage above only refers to much older versions of OpenMPI.<br><br>I also want to thank Ralph for all his help in debugging the manifold<br>problems w/our mixed bag cluster.<br><br>-Bill Lane<br><br><br></font>&nbsp;<font color="black" face="Tahoma" size="2"><span dir="ltr" style="font-size:10pt"><font face="Times New Roman" size="2"><span style="font-size:16px"><font face="Tahoma" size="2"><span style="font-size:10pt"></span></font></span></font></span></font><br></pre>
</span></font></span></font></span></font></div>
IMPORTANT WARNING: This message is intended for the use of the person or entity to which it is addressed and may contain information that is privileged and confidential, the disclosure of which is governed by applicable law. If the reader of this message is
 not the intended recipient, or the employee or agent responsible for delivering it to the intended recipient, you are hereby notified that any dissemination, distribution or copying of this information is strictly prohibited. Thank you for your cooperation.
</div>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/08/27397.php" rel="noreferrer" target="_blank">
http://www.open-mpi.org/community/lists/users/2015/08/27397.php</a><br>
</blockquote>
</div>
<br>
</div>
</div>
</div>
</div>
</div>
</div>
<div>
<div class="h5">IMPORTANT WARNING: This message is intended for the use of the person or entity to which it is addressed and may contain information that is privileged and confidential, the disclosure of which is governed by applicable law. If the reader of
 this message is not the intended recipient, or the employee or agent responsible for delivering it to the intended recipient, you are hereby notified that any dissemination, distribution or copying of this information is strictly prohibited. Thank you for
 your cooperation. </div>
</div>
</div>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/08/27402.php" rel="noreferrer" target="_blank">
http://www.open-mpi.org/community/lists/users/2015/08/27402.php</a><br>
</blockquote>
</div>
<br>
</div>
</div>
</div>
</div>
IMPORTANT WARNING: This message is intended for the use of the person or entity to which it is addressed and may contain information that is privileged and confidential, the disclosure of which is governed by applicable law. If the reader of this message is
 not the intended recipient, or the employee or agent responsible for delivering it to the intended recipient, you are hereby notified that any dissemination, distribution or copying of this information is strictly prohibited. Thank you for your cooperation.
</body>
</html>

