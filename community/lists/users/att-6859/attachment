<table cellspacing="0" cellpadding="0" border="0" ><tr><td valign="top" style="font: inherit;"><DIV>Doug Reeder,</DIV>
<DIV>Daniel is saying that the problem only occurs in openmpi when running more than 16 processes. So could that still be cause becasue openmpi does not support threads??!! <BR><BR>--- On <B>Fri, 10/3/08, Doug Reeder <I>&lt;dlr@rain.org&gt;</I></B> wrote:<BR></DIV>
<BLOCKQUOTE style="PADDING-LEFT: 5px; MARGIN-LEFT: 5px; BORDER-LEFT: rgb(16,16,255) 2px solid">From: Doug Reeder &lt;dlr@rain.org&gt;<BR>Subject: Re: [OMPI users] segfault issue - possible bug in openmpi<BR>To: "Open MPI Users" &lt;users@open-mpi.org&gt;<BR>Date: Friday, October 3, 2008, 2:40 PM<BR><BR>
<DIV id=yiv1658397218>Daniel,
<DIV><BR></DIV>
<DIV>Are you using threads. I don't think the opempi-1.2.x work with threads.</DIV>
<DIV><BR></DIV>
<DIV>Doug Reeder<BR>
<DIV>
<DIV>On Oct 3, 2008, at 2:30 PM, Daniel Hansen wrote:</DIV><BR class=Apple-interchange-newline>
<BLOCKQUOTE type="cite">
<DIV dir=ltr>Oh, by the way, here is the segfault:<BR><BR>[m4b-1-8:11481] *** Process received signal ***<BR>[m4b-1-8:11481] Signal: Segmentation fault (11)<BR>[m4b-1-8:11481] Signal code: Address not mapped (1)<BR>[m4b-1-8:11481] Failing at address: 0x2b91c69eed<BR>[m4b-1-8:11483] [ 0] /lib64/libpthread.so.0 [0x33e8c0de70]<BR>[m4b-1-8:11483] [ 1] /fslhome/dhansen7/openmpi/lib/libmpi.so.0 [0x2aaaaabea7c0]<BR>[m4b-1-8:11483] [ 2] /fslhome/dhansen7/openmpi/lib/libmpi.so.0 [0x2aaaaabea675]<BR>[m4b-1-8:11483] [ 3] /fslhome/dhansen7/openmpi/lib/libmpi.so.0(mca_pml_ob1_send+0x2da) [0x2aaaaabeaf55]<BR>[m4b-1-8:11483] [ 4] /fslhome/dhansen7/openmpi/lib/libmpi.so.0(MPI_Send+0x28e) [0x2aaaaab52c5a]<BR>[m4b-1-8:11483] [ 5] /fslhome/dhansen7/compute/for_DanielHansen/replica_mpi_marylou2/Openmpi_md_twham(twham_init+0x708) [0x42a8a8]<BR>[m4b-1-8:11483] [ 6] /fslhome/dhansen7/compute/for_DanielHansen/replica_mpi_marylou2/Openmpi_md_twham(repexch+0x73c)
 [0x425d5c]<BR>[m4b-1-8:11483] [ 7] /fslhome/dhansen7/compute/for_DanielHansen/replica_mpi_marylou2/Openmpi_md_twham(main+0x855) [0x4133a5]<BR>[m4b-1-8:11483] [ 8] /lib64/libc.so.6(__libc_start_main+0xf4) [0x33e841d8a4]<BR>[m4b-1-8:11483] [ 9] /fslhome/dhansen7/compute/for_DanielHansen/replica_mpi_marylou2/Openmpi_md_twham [0x4040b9]<BR>[m4b-1-8:11483] *** End of error message ***<BR><BR><BR><BR>
<DIV class=gmail_quote>On Fri, Oct 3, 2008 at 3:20 PM, Daniel Hansen <SPAN dir=ltr>&lt;<A href="mailto:dhansen@byu.net" target=_blank rel=nofollow>dhansen@byu.net</A>&gt;</SPAN> wrote:<BR>
<BLOCKQUOTE class=gmail_quote style="PADDING-LEFT: 1ex; MARGIN: 0pt 0pt 0pt 0.8ex; BORDER-LEFT: rgb(204,204,204) 1px solid">
<DIV dir=ltr>I have been testing some code against openmpi lately that always causes it to crash during certain mpi function calls.&nbsp; The code does not seem to be the problem, as it runs just fine against mpich.&nbsp; I have tested it against openmpi 1.2.5, 1.2.6, and 1.2.7 and they all exhibit the same problem.&nbsp; Also, the problem only occurs in openmpi when running more than 16 processes.&nbsp; I have posted this stack trace to the list before, but I am submitting it now as a potential bug report.&nbsp; I need some help debugging it and finding out exactly what is going on in openmpi when the segfault occurs.&nbsp; Are there any suggestions on how best to do this?&nbsp; Is there an easy way to attach gdb to one of the processes or something??&nbsp; I have already compiled openmpi with debugging, memory profiling, etc.&nbsp; How can I best take advantage of these features?<BR><BR>Thanks,<BR>Daniel Hansen<BR>Systems Administrator<BR>BYU Fulton
 Supercomputing Lab<BR></DIV></BLOCKQUOTE></DIV><BR></DIV>
<DIV style="MARGIN: 0px">_______________________________________________</DIV>
<DIV style="MARGIN: 0px">users mailing list</DIV>
<DIV style="MARGIN: 0px"><A href="mailto:users@open-mpi.org" target=_blank rel=nofollow>users@open-mpi.org</A></DIV>
<DIV style="MARGIN: 0px"><A href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target=_blank rel=nofollow>http://www.open-mpi.org/mailman/listinfo.cgi/users</A></DIV></BLOCKQUOTE></DIV><BR></DIV></DIV><PRE>_______________________________________________
users mailing list
users@open-mpi.org
http://www.open-mpi.org/mailman/listinfo.cgi/users</PRE></BLOCKQUOTE></td></tr></table><br>

      
