Stefan,<div><br></div><div>what if you</div><div>ulimit -c unlimited</div><div><br></div><div>do orted generate some core dump ?</div><div><br></div><div>Cheers</div><div><br></div><div>Gilles<br><br>On Tuesday, April 12, 2016, Stefan Friedel &lt;<a href="mailto:stefan.friedel@iwr.uni-heidelberg.de">stefan.friedel@iwr.uni-heidelberg.de</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">On Tue, Apr 12, 2016 at 05:11:59PM +0900, Gilles Gouaillardet wrote:<br>
Dear Gilles,<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
which version of OpenMPI are you using ?<br>
</blockquote>
as I wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
   openmpi-1.10.2, slurm-15.08.9; homes mounted via NFS/RDMA/ipoib, mpi<br>
</blockquote>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
when does the error occur ?<br>
is it before MPI_Init() completes ?<br>
is it in the middle of the job ? if yes, are you sure no task invoked MPI_Abort<br>
</blockquote>
During the setup of the job (in most cases) and there is no output from the<br>
application. I will build a minimal program to get some printf debugging ...I&#39;ll<br>
report...<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
also, you might want to check the system logs and make sure there was no OOM<br>
(Out Of Memory).<br>
</blockquote>
No OOM messages from the nodes. No relevant messages at all from the<br>
nodes...(remote syslog is running from all nodes to a central system)<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
mpirun --mca oob_tcp_if_include eth0 ...<br>
</blockquote>
I already tested this.<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
mpirun --mca btl tcp,vader,self --mca btl_tcp_if_include eth0 ...<br>
or<br>
mpirun --mca btl tcp,vader,self --mca btl_tcp_if_include ib0 ...<br>
</blockquote>
Just tested this on 350 nodes - two out of seven jobs spawned one after each<br>
other were successfull but subsequent jobs were failing again:<br>
<br>
*tcp,vader,self eth0 failed<br>
*tcp,sm,self eth0 failed<br>
*tcp,vader,self ib0 failed<br>
*tcp,sm,self ib0 success!<br>
*tcp,sm,self ib0 failed :-/<br>
*tcp,sm,self ib0 success again!<br>
*tcp,sm,self ib0 failed...<br>
<br>
hhmmm. tcp+sm is a little bit more reliable??<br>
<br>
For the sake of completeness - I forgot the ompi_info output:<br>
<br>
                Package: Open MPI root@dyaus Distribution<br>
               Open MPI: 1.10.2<br>
 Open MPI repo revision: v1.10.1-145-g799148f<br>
  Open MPI release date: Jan 21, 2016<br>
               Open RTE: 1.10.2<br>
 Open RTE repo revision: v1.10.1-145-g799148f<br>
  Open RTE release date: Jan 21, 2016<br>
                   OPAL: 1.10.2<br>
     OPAL repo revision: v1.10.1-145-g799148f<br>
      OPAL release date: Jan 21, 2016<br>
                MPI API: 3.0.0<br>
           Ident string: 1.10.2<br>
                 Prefix: /opt/openmpi/1.10.2/gcc/4.9.2<br>
Configured architecture: x86_64-pc-linux-gnu<br>
         Configure host: dyaus<br>
          Configured by: root<br>
          Configured on: Mon Apr 11 09:54:21 CEST 2016<br>
         Configure host: dyaus<br>
               Built by: root<br>
               Built on: Mon Apr 11 10:12:25 CEST 2016<br>
             Built host: dyaus<br>
             C bindings: yes<br>
           C++ bindings: yes<br>
            Fort mpif.h: yes (all)<br>
           Fort use mpi: yes (full: ignore TKR)<br>
      Fort use mpi size: deprecated-ompi-info-value<br>
       Fort use mpi_f08: yes<br>
Fort mpi_f08 compliance: The mpi_f08 module is available, but due to limitations in the gfortran compiler, does not support the following: array subsections, direct passthru (where possible) to underlying Open MPI&#39;s C functionality<br>
 Fort mpi_f08 subarrays: no<br>
          Java bindings: no<br>
 Wrapper compiler rpath: runpath<br>
             C compiler: gcc<br>
    C compiler absolute: /usr/bin/gcc<br>
 C compiler family name: GNU<br>
     C compiler version: 4.9.2<br>
           C++ compiler: g++<br>
  C++ compiler absolute: /usr/bin/g++<br>
          Fort compiler: gfortran<br>
      Fort compiler abs: /usr/bin/gfortran<br>
        Fort ignore TKR: yes (!GCC$ ATTRIBUTES NO_ARG_CHECK ::)<br>
  Fort 08 assumed shape: yes<br>
     Fort optional args: yes<br>
         Fort INTERFACE: yes<br>
   Fort ISO_FORTRAN_ENV: yes<br>
      Fort STORAGE_SIZE: yes<br>
     Fort BIND(C) (all): yes<br>
     Fort ISO_C_BINDING: yes<br>
Fort SUBROUTINE BIND(C): yes<br>
      Fort TYPE,BIND(C): yes<br>
Fort T,BIND(C,name=&quot;a&quot;): yes<br>
           Fort PRIVATE: yes<br>
         Fort PROTECTED: yes<br>
          Fort ABSTRACT: yes<br>
      Fort ASYNCHRONOUS: yes<br>
         Fort PROCEDURE: yes<br>
        Fort USE...ONLY: yes<br>
          Fort C_FUNLOC: yes<br>
Fort f08 using wrappers: yes<br>
        Fort MPI_SIZEOF: yes<br>
            C profiling: yes<br>
          C++ profiling: yes<br>
  Fort mpif.h profiling: yes<br>
 Fort use mpi profiling: yes<br>
  Fort use mpi_f08 prof: yes<br>
         C++ exceptions: no<br>
         Thread support: posix (MPI_THREAD_MULTIPLE: no, OPAL support: yes, OMPI progress: no, ORTE progress: yes, Event lib: yes)<br>
          Sparse Groups: no<br>
 Internal debug support: no<br>
 MPI interface warnings: yes<br>
    MPI parameter check: runtime<br>
Memory profiling support: no<br>
Memory debugging support: no<br>
             dl support: yes<br>
  Heterogeneous support: no<br>
mpirun default --prefix: no<br>
        MPI I/O support: yes<br>
      MPI_WTIME support: gettimeofday<br>
    Symbol vis. support: yes<br>
  Host topology support: yes<br>
         MPI extensions:   FT Checkpoint support: no (checkpoint thread: no)<br>
  C/R Enabled Debugging: no<br>
    VampirTrace support: yes<br>
 MPI_MAX_PROCESSOR_NAME: 256<br>
   MPI_MAX_ERROR_STRING: 256<br>
    MPI_MAX_OBJECT_NAME: 64<br>
       MPI_MAX_INFO_KEY: 36<br>
       MPI_MAX_INFO_VAL: 256<br>
      MPI_MAX_PORT_NAME: 1024<br>
 MPI_MAX_DATAREP_STRING: 128<br>
          MCA backtrace: execinfo (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
           MCA compress: gzip (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
           MCA compress: bzip (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA crs: none (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                 MCA db: hash (MCA v2.0.0, API v1.0.0, Component v1.10.2)<br>
                 MCA db: print (MCA v2.0.0, API v1.0.0, Component v1.10.2)<br>
                 MCA dl: dlopen (MCA v2.0.0, API v1.0.0, Component v1.10.2)<br>
              MCA event: libevent2021 (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA hwloc: hwloc191 (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                 MCA if: posix_ipv4 (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                 MCA if: linux_ipv6 (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
        MCA installdirs: env (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
        MCA installdirs: config (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
             MCA memory: linux (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA pstat: linux (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA sec: basic (MCA v2.0.0, API v1.0.0, Component v1.10.2)<br>
              MCA shmem: posix (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA shmem: sysv (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA shmem: mmap (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA timer: linux (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA dfs: app (MCA v2.0.0, API v1.0.0, Component v1.10.2)<br>
                MCA dfs: test (MCA v2.0.0, API v1.0.0, Component v1.10.2)<br>
                MCA dfs: orted (MCA v2.0.0, API v1.0.0, Component v1.10.2)<br>
             MCA errmgr: default_tool (MCA v2.0.0, API v3.0.0, Component v1.10.2)<br>
             MCA errmgr: default_hnp (MCA v2.0.0, API v3.0.0, Component v1.10.2)<br>
             MCA errmgr: default_app (MCA v2.0.0, API v3.0.0, Component v1.10.2)<br>
             MCA errmgr: default_orted (MCA v2.0.0, API v3.0.0, Component v1.10.2)<br>
                MCA ess: slurm (MCA v2.0.0, API v3.0.0, Component v1.10.2)<br>
                MCA ess: tool (MCA v2.0.0, API v3.0.0, Component v1.10.2)<br>
                MCA ess: singleton (MCA v2.0.0, API v3.0.0, Component v1.10.2)<br>
                MCA ess: env (MCA v2.0.0, API v3.0.0, Component v1.10.2)<br>
                MCA ess: hnp (MCA v2.0.0, API v3.0.0, Component v1.10.2)<br>
              MCA filem: raw (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
            MCA grpcomm: bad (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA iof: orted (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA iof: hnp (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA iof: mr_hnp (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA iof: tool (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA iof: mr_orted (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
               MCA odls: default (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA oob: tcp (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA plm: isolated (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA plm: slurm (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA plm: rsh (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA ras: simulator (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA ras: slurm (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA ras: loadleveler (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA rmaps: round_robin (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA rmaps: ppr (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA rmaps: resilient (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA rmaps: rank_file (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA rmaps: mindist (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA rmaps: staged (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA rmaps: seq (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA rml: oob (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
             MCA routed: binomial (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
             MCA routed: radix (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
             MCA routed: direct (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
             MCA routed: debruijn (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA state: staged_orted (MCA v2.0.0, API v1.0.0, Component v1.10.2)<br>
              MCA state: dvm (MCA v2.0.0, API v1.0.0, Component v1.10.2)<br>
              MCA state: orted (MCA v2.0.0, API v1.0.0, Component v1.10.2)<br>
              MCA state: tool (MCA v2.0.0, API v1.0.0, Component v1.10.2)<br>
              MCA state: app (MCA v2.0.0, API v1.0.0, Component v1.10.2)<br>
              MCA state: novm (MCA v2.0.0, API v1.0.0, Component v1.10.2)<br>
              MCA state: staged_hnp (MCA v2.0.0, API v1.0.0, Component v1.10.2)<br>
              MCA state: hnp (MCA v2.0.0, API v1.0.0, Component v1.10.2)<br>
          MCA allocator: bucket (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
          MCA allocator: basic (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
               MCA bcol: basesmuma (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
               MCA bcol: ptpcoll (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA bml: r2 (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA btl: openib (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA btl: self (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA btl: tcp (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA btl: sm (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA btl: vader (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
               MCA coll: tuned (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
               MCA coll: self (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
               MCA coll: basic (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
               MCA coll: sm (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
               MCA coll: inter (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
               MCA coll: ml (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
               MCA coll: libnbc (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
               MCA coll: hierarch (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA dpm: orte (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
               MCA fbtl: posix (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA fcoll: individual (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA fcoll: two_phase (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA fcoll: static (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA fcoll: dynamic (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA fcoll: ylib (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                 MCA fs: ufs (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                 MCA io: romio (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                 MCA io: ompio (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA mpool: grdma (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
              MCA mpool: sm (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA mtl: psm (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA osc: pt2pt (MCA v2.0.0, API v3.0.0, Component v1.10.2)<br>
                MCA osc: sm (MCA v2.0.0, API v3.0.0, Component v1.10.2)<br>
                MCA pml: v (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA pml: ob1 (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA pml: cm (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA pml: bfo (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
             MCA pubsub: orte (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
             MCA rcache: vma (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
                MCA rte: orte (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
               MCA sbgp: basesmuma (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
               MCA sbgp: p2p (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
               MCA sbgp: basesmsocket (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
           MCA sharedfp: lockedfile (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
           MCA sharedfp: sm (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
           MCA sharedfp: individual (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
               MCA topo: basic (MCA v2.0.0, API v2.1.0, Component v1.10.2)<br>
          MCA vprotocol: pessimist (MCA v2.0.0, API v2.0.0, Component v1.10.2)<br>
<br>
MfG/Sincerely<br>
Stefan Friedel<br>
--<br>
IWR * 4.317 * INF205 * 69120 Heidelberg<br>
T +49 6221 5414404 * F +49 6221 5414427<br>
<a>stefan.friedel@iwr.uni-heidelberg.de</a><br>
</blockquote></div>

