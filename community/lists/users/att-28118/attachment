<div dir="ltr">Try and do a variable amount of work for every process, I see non-blocking as a way to speed-up communication if they arrive individually to the call. Please always have this at the back of your mind when doing this.<div><br></div><div>Surely non-blocking has overhead, and if the communication time is low, so will the overhead be much higher.</div><div>You haven&#39;t specified what nx*ny*nz is, and hence your &quot;slower&quot; and &quot;faster&quot; makes &quot;no sense&quot;...  And hence your questions are difficult to answer, basically &quot;it depends&quot;.</div></div><div class="gmail_extra"><br><div class="gmail_quote">2015-11-27 17:57 GMT+01:00 Felipe . <span dir="ltr">&lt;<a href="mailto:philip.fm@gmail.com" target="_blank">philip.fm@gmail.com</a>&gt;</span>:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><span style="font-family:monospace,monospace">Hello!<br><br></span></div><div><span style="font-family:monospace,monospace">I have a program that basically is (first implementation):<br><span style="color:rgb(0,0,255)">for i in N:<br></span></span></div><div><span style="color:rgb(0,0,255)"><span style="font-family:monospace,monospace">  local_computation(i)<br></span></span></div><div><span style="font-family:monospace,monospace"><span style="color:rgb(0,0,255)">  mpi_allreduce(in_place, i)<br></span><br></span></div><div><span style="font-family:monospace,monospace">In order to try to mitigate the implicit barrier of the mpi_allreduce, I tried to start an mpi_Iallreduce. Like this(second implementation):<br><span style="color:rgb(0,0,255)"><font size="2">for i in N:<br></font></span></span><div><span style="color:rgb(0,0,255)"><span style="font-family:monospace,monospace"><font size="2">  local_computation(i)<br></font></span></span></div><div><span style="color:rgb(0,0,255)"><span style="font-family:monospace,monospace"><font size="2">  j = i<br></font></span></span></div><div><span style="color:rgb(0,0,255)"><span style="font-family:monospace,monospace"><font size="2">  if i is not first:<br></font></span></span></div><div><span style="color:rgb(0,0,255)"><span style="font-family:monospace,monospace"><font size="2">    mpi_wait(request)<br></font></span></span></div><span style="font-family:monospace,monospace"><span style="color:rgb(0,0,255)"><font size="2">  mpi_Iallreduce(in_place, j, request)</font></span><br><br>The result was that the second was a lot worse. The processes spent 3 times more time on the mpi_wait than on the mpi_allreduce from the first implementation. I know it could be worst, but not that much.<br><br></span></div><div><span style="font-family:monospace,monospace">So, I made a microbenchmark to stress this, in Fortran. Here is the implementation:<br></span></div><div><span style="font-family:monospace,monospace">Blocking:<br><span style="color:rgb(0,0,255)">do i = 1, total_iter ! [<br>    t_0 = mpi_wtime()<br><br>    call mpi_allreduce(MPI_IN_PLACE, val, nx*ny*nz, MPI_REAL, MPI_SUM, MPI_COMM_WORLD, ierror)<br>    if (ierror .ne. 0) then ! [<br>        write(*,*) &quot;Error in line &quot;, __LINE__, &quot; rank = &quot;, rank<br>        call mpi_abort(MPI_COMM_WORLD, ierror, ierror2)<br>    end if ! ]<br>    t_reduce = t_reduce + (mpi_wtime() - t_0)<br>end do ! ]</span><br><br></span></div><div><span style="font-family:monospace,monospace">Non-Blocking:<br><span style="color:rgb(0,0,255)">do i = 1, total_iter ! [<br>    t_0 = mpi_wtime()<br>    call mpi_iallreduce(MPI_IN_PLACE, val, nx*ny*nz, MPI_REAL, MPI_SUM, MPI_COMM_WORLD, request, ierror)<br>    if (ierror .ne. 0) then ! [<br>        write(*,*) &quot;Error in line &quot;, __LINE__, &quot; rank = &quot;, rank<br>        call mpi_abort(MPI_COMM_WORLD, ierror, ierror2)<br>    end if ! ]<br>    t_reduce = t_reduce + (mpi_wtime() - t_0)<br><br>    t_0 = mpi_wtime()<br>    call mpi_wait(request, status, ierror)<br>    if (ierror .ne. 0) then ! [<br>        write(*,*) &quot;Error in line &quot;, __LINE__, &quot; rank = &quot;, rank<br>        call mpi_abort(MPI_COMM_WORLD, ierror, ierror2)<br>    end if ! ]<br>    t_reduce = t_reduce + (mpi_wtime() - t_0)<br><br>end do ! ]</span><br><br></span></div><div><span style="font-family:monospace,monospace">The non-blocking was about five times slower. I tried Intel&#39;s MPI and it was of 3 times, instead of 5.<br><br></span></div><div><span style="font-family:monospace,monospace"><span style="font-family:monospace,monospace">Question 1: Do you think that all this overhead makes sense?</span><br><br>Question 2: Why is there so much overhead for non-blocking collective calls?<br></span><br><span style="font-family:monospace,monospace"><span style="font-family:monospace,monospace">Question 3: Can I change the algorithm for the non-blocking allReduce to improve this?</span><br></span></div><div><span style="font-family:monospace,monospace"><br><br></span></div><div><span style="font-family:monospace,monospace">Best regards,<br>--<br></span></div><span style="font-family:monospace,monospace">Felipe<br></span></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/11/28117.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/11/28117.php</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br><div class="gmail_signature"><div dir="ltr"><div>Kind regards Nick</div></div></div>
</div>

