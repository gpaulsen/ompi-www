<div dir="ltr">Thank you for the very detailed reply Ralph. I will try what you say. I will need to ask the developers to let me know about threading of the main solver process.</div><div class="gmail_extra"><br><br><div class="gmail_quote">
On 30 January 2014 12:30, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div style="word-wrap:break-word"><br><div><div class="im"><div>On Jan 29, 2014, at 7:56 PM, Victor &lt;<a href="mailto:victor.major@gmail.com" target="_blank">victor.major@gmail.com</a>&gt; wrote:</div><br><blockquote type="cite">
<div dir="ltr">Thanks for the insights Tim. I was aware that the CPUs will choke beyond a certain point. From memory on my machine this happens with 5 concurrent MPI jobs with that benchmark that I am using.<div><br></div>

<div>My primary question was about scaling between the nodes. I was not getting close to double the performance when running MPI jobs acros two 4 core nodes. It may be better now since I have Open-MX in place, but I have not repeated the benchmarks yet since I need to get one simulation job done asap.</div>
</div></blockquote><div><br></div></div>Some of that may be due to expected loss of performance when you switch from shared memory to inter-node transports. While it is true about saturation of the memory path, what you reported could be more consistent with that transition - i.e., it isn&#39;t unusual to see applications perform better when run on a single node, depending upon how they are written, up to a certain size of problem (which your code may not be hitting).</div>
<div><div class="im"><br><blockquote type="cite"><div dir="ltr">
<div><br></div><div>Regarding your mention of setting affinities and MPI ranks do you have a specific (as in syntactically specific since I am a novice and easily confused...) examples how I may want to set affinities to get the Westmere node performing better?</div>
</div></blockquote><div><br></div></div>mpirun --bind-to-core -cpus-per-rank 2 ...</div><div><br></div><div>will bind each MPI rank to 2 cores. Note that this will definitely *not* be a good idea if you are running more than two threads in your process - if you are, then set --cpus-per-rank to the number of threads, keeping in mind that you want things to break evenly across the sockets. In other words, if you have two 6 core/socket Westmere&#39;s on the node, then you either want to run 6 process at cpus-per-rank=2 if each process runs 2 threads, or 4 processes with cpus-per-rank=3 if each process runs 3 threads, or 2 processes with no cpus-per-rank but --bind-to-socket instead of --bind-to-core for any other thread number &gt; 3.</div>
<div><br></div><div>You would not want to run any other number of processes on the node or else the binding pattern will cause a single process to split its threads across the sockets - which will definitely hurt performance.</div>
<div><div class="h5"><div><br></div><div><br><blockquote type="cite"><div dir="ltr">
<div><br></div><div>ompi_info returns this: MCA paffinity: hwloc (MCA v2.0, API v2.0, Component v1.6.5)</div><div><br></div><div>And finally to hybridisation... in a week or so I will get 4 AMD A10-6800 machines with 8Gb each on loan and will attempt to make them work along the existing Intel nodes. </div>

<div><br></div><div>Victor</div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On 29 January 2014 22:03, Tim Prince <span dir="ltr">&lt;<a href="mailto:n8tm@aol.com" target="_blank">n8tm@aol.com</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><br>
On 1/29/2014 8:02 AM, Reuti wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Quoting Victor &lt;<a href="mailto:victor.major@gmail.com" target="_blank">victor.major@gmail.com</a>&gt;:<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Thanks for the reply Reuti,<br>
<br>
There are two machines: Node1 with 12 physical cores (dual 6 core Xeon) and<br>
</blockquote>
<br>
Do you have this CPU?<br>
<br>
<a href="http://ark.intel.com/de/products/37109/Intel-Xeon-Processor-X5560-8M-Cache-2_80-GHz-6_40-GTs-Intel-QPI" target="_blank">http://ark.intel.com/de/<u></u>products/37109/Intel-Xeon-<u></u>Processor-X5560-8M-Cache-2_80-<u></u>GHz-6_40-GTs-Intel-QPI</a> <br>


<br>
-- Reuti<br>
<br>
</blockquote></div>
It&#39;s expected on the Xeon Westmere 6-core CPUs to see MPI performance saturating when all 4 of the internal buss paths are in use.  For this reason, hybrid MPI/OpenMP with 2 cores per MPI rank, with affinity set so that each MPI rank has its own internal CPU buss, could out-perform plain MPI on those CPUs.<br>


That scheme of pairing cores on selected internal buss paths hasn&#39;t been repeated.  Some influential customers learned to prefer the 4-core version of that CPU, given a reluctance to adopt MPI/OpenMP hybrid with affinity.<br>


If you want to talk about &quot;downright strange,&quot; start thinking about the schemes to optimize performance of 8 threads with 2 threads assigned to each internal CPU buss on that CPU model.  Or your scheme of trying to balance MPI performance between very different CPU models.<br>


Tim<div><div><br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Node2 with 4 physical cores (i5-2400).<br>
<br>
Regarding scaling on the single 12 core node, not it is also not linear. In<br>
fact it is downright strange. I do not remember the numbers right now but<br>
10 jobs are faster than 11 and 12 are the fastest with peak performance of<br>
approximately 66 Msu/s which is also far from triple the 4 core<br>
performance. This odd non-linear behaviour also happens at the lower job<br>
counts on that 12 core node. I understand the decrease in scaling with<br>
increase in core count on the single node as the memory bandwidth is an<br>
issue.<br>
<br>
On the 4 core machine the scaling is progressive, ie. every additional job<br>
brings an increase in performance. Single core delivers 8.1 Msu/s while 4<br>
cores deliver 30.8 Msu/s. This is almost linear.<br>
<br>
Since my original email I have also installed Open-MX and recompiled<br>
OpenMPI to use it. This has resulted in approximately 10% better<br>
performance using the existing GbE hardware.<br>
<br>
<br>
On 29 January 2014 19:40, Reuti &lt;<a href="mailto:reuti@staff.uni-marburg.de" target="_blank">reuti@staff.uni-marburg.de</a>&gt; wrote:<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Am 29.01.2014 um 03:00 schrieb Victor:<br>
<br>
&gt; I am running a CFD simulation benchmark cavity3d available within<br>
<a href="http://www.palabos.org/images/palabos_releases/palabos-v1.4r1.tgz" target="_blank">http://www.palabos.org/images/<u></u>palabos_releases/palabos-v1.<u></u>4r1.tgz</a><br>
&gt;<br>
&gt; It is a parallel friendly Lattice Botlzmann solver library.<br>
&gt;<br>
&gt; Palabos provides benchmark results for the cavity3d on several different<br>
platforms and variables here:<br>
<a href="http://wiki.palabos.org/plb_wiki:benchmark:cavity_n400" target="_blank">http://wiki.palabos.org/plb_<u></u>wiki:benchmark:cavity_n400</a><br>
&gt;<br>
&gt; The problem that I have is that the benchmark performance on my cluster<br>
does not scale even close to a linear scale.<br>
&gt;<br>
&gt; My cluster configuration:<br>
&gt;<br>
&gt; Node1: Dual Xeon 5560 48 Gb RAM<br>
&gt; Node2: i5-2400 24 Gb RAM<br>
&gt;<br>
&gt; Gigabit ethernet connection on eth0<br>
&gt;<br>
&gt; OpenMPI 1.6.5 on Ubuntu 12.04.3<br>
&gt;<br>
&gt;<br>
&gt; Hostfile:<br>
&gt;<br>
&gt; Node1 -slots=4 -max-slots=4<br>
&gt; Node2 -slots=4 -max-slots=4<br>
&gt;<br>
&gt; MPI command: mpirun --mca btl_tcp_if_include eth0 --hostfile<br>
/home/mpiuser/.mpi_hostfile -np 8 ./cavity3d 400<br>
&gt;<br>
&gt; Problem:<br>
&gt;<br>
&gt; cavity3d 400<br>
&gt;<br>
&gt; When I run mpirun -np 4 on Node1 I get 35.7615 Mega site updates per<br>
second<br>
&gt; When I run mpirun -np 4 on Node2 I get 30.7972 Mega site updates per<br>
second<br>
&gt; When I run mpirun --mca btl_tcp_if_include eth0 --hostfile<br>
/home/mpiuser/.mpi_hostfile -np 8 ./cavity3d 400 I get 47.3538 Mega site<br>
updates per second<br>
&gt;<br>
&gt; I understand that there are latencies with GbE and that there is MPI<br>
overhead, but this performance scaling still seems very poor. Are my<br>
expectations of scaling naive, or is there actually something wrong and<br>
fixable that will improve the scaling? Optimistically I would like each<br>
node to add to the cluster performance, not slow it down.<br>
&gt;<br>
&gt; Things get even worse if I run asymmetric number of mpi jobs in each<br>
node. For instance running -np 12 on Node1<br>
<br>
Isn&#39;t this overloading the machine with only 8 real cores in total?<br>
<br>
<br>
&gt; is significantly faster than running -np 16 across Node1 and Node2, thus<br>
adding Node2 actually slows down the performance.<br>
<br>
The i5-2400 has only 4 cores and no threads.<br>
<br>
It depends on the algorithm how much data has to be exchanged between the<br>
processes, and this can indeed be worse when used across a network.<br>
<br>
Also: is the algorithm scaling linear when used on node1 only with 8<br>
cores? When it&#39;s &quot;35.7615 &quot; with 4 cores, what result do you get with 8<br>
cores on this machine.<br>
<br>
-- Reuti<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
<br>
</blockquote></blockquote>
<br>
<br>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
</blockquote>
<br></div></div><span><font color="#888888">
-- <br>
Tim Prince</font></span><div><div><br>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>

