Also how can i find out where are my mpi libraries and include directories? <br><br><div class="gmail_quote">On Sat, Apr 18, 2009 at 2:29 PM, Ankush Kaul <span dir="ltr">&lt;<a href="mailto:ankush.rkaul@gmail.com">ankush.rkaul@gmail.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">Let me explain in detail,<br><br>when we had only 2 nodes, 1 master (192.168.67.18) + 1 compute node (192.168.45.65)<br>
my openmpi-default-hostfile looked like<i><br>192.168.67.18 slots=2<br>192.168.45.65 slots=2</i><br><br>
after this on running the command <b>miprun /work/Pi</b> on master node we got<br><i><br># <a href="mailto:root@192.168.45.65" target="_blank">root@192.168.45.65</a> password :</i><br><br>after entering the password the program ran on both de nodes.<br>

<br>Now after connecting a second compute node, and editing the hostfile:<br><br><i>192.168.67.18 slots=2<br>
192.168.45.65 slots=2</i><br><i>192.168.67.241 slots=2<br><br></i>and then running the command <b>miprun /work/Pi</b> on master node we got<br><br># <span style="font-style: italic;"><span style="color: rgb(102, 102, 102);"><a href="mailto:root@192.168.45.65" target="_blank">root@192.168.45.65</a>&#39;s password: <a href="mailto:root@192.168.67.241" target="_blank">root@192.168.67.241</a>&#39;s
password: </span></span><br><br>which does not accept the password.<br><br>Although we are trying to implement the passwordless cluster. i wud like to know what this problem is occuring? <br><div><div></div><div class="h5">
<br><br><div class="gmail_quote">
On Sat, Apr 18, 2009 at 3:40 AM, Gus Correa <span dir="ltr">&lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">

Ankush<br>
<br>
You need to setup passwordless connections with ssh to the node you just<br>
added.  You (or somebody else) probably did this already on the first compute node, otherwise the MPI programs wouldn&#39;t run<br>
across the network.<br>
<br>
See the very last sentence on this FAQ:<br>
<br>
<a href="http://www.open-mpi.org/faq/?category=running#run-prereqs" target="_blank">http://www.open-mpi.org/faq/?category=running#run-prereqs</a><br>
<br>
And try this recipe (if you use RSA keys instead of DSA, replace all &quot;dsa&quot; by &quot;rsa&quot;):<br>
<br>
<a href="http://www.sshkeychain.org/mirrors/SSH-with-Keys-HOWTO/SSH-with-Keys-HOWTO-4.html#ss4.3" target="_blank">http://www.sshkeychain.org/mirrors/SSH-with-Keys-HOWTO/SSH-with-Keys-HOWTO-4.html#ss4.3</a><div>
<br>
<br>
I hope this helps.<br>
<br>
Gus Correa<br>
---------------------------------------------------------------------<br>
Gustavo Correa<br>
Lamont-Doherty Earth Observatory - Columbia University<br>
Palisades, NY, 10964-8000 - USA<br>
---------------------------------------------------------------------<br>
<br>
<br></div>
Ankush Kaul wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div>
Thank you, i m reading up on de tools u suggested.<br>
<br>
I am facing another problem, my cluster is working fine with 2 hosts (1 master + 1 compute node) but when i tried 2 add another node (1 master + 2 compute node) its not working. it works fine when i give de command mpirun -host &lt;hostname&gt; /work/Pi<br>


<br>
but when i try to run<br>
mpirun  /work/Pi it gives following error:<br>
<br>
</div><a href="mailto:root@192.168.45.65" target="_blank">root@192.168.45.65</a> &lt;mailto:<a href="mailto:root@192.168.45.65" target="_blank">root@192.168.45.65</a>&gt;&#39;s password: <a href="mailto:root@192.168.67.241" target="_blank">root@192.168.67.241</a> &lt;mailto:<a href="mailto:root@192.168.67.241" target="_blank">root@192.168.67.241</a>&gt;&#39;s password:<div>

<br>
<br>
Permission denied, please try again. &lt;The password i provide is correct&gt;<br>
<br>
</div><a href="mailto:root@192.168.45.65" target="_blank">root@192.168.45.65</a> &lt;mailto:<a href="mailto:root@192.168.45.65" target="_blank">root@192.168.45.65</a>&gt;&#39;s password:<div><br>
<br>
Permission denied, please try again.<br>
<br>
</div><a href="mailto:root@192.168.45.65" target="_blank">root@192.168.45.65</a> &lt;mailto:<a href="mailto:root@192.168.45.65" target="_blank">root@192.168.45.65</a>&gt;&#39;s password:<div><br>
<br>
Permission denied (publickey,gssapi-with-mic,password).<br>
<br>
 <br>
Permission denied, please try again.<br>
<br>
</div><a href="mailto:root@192.168.67.241" target="_blank">root@192.168.67.241</a> &lt;mailto:<a href="mailto:root@192.168.67.241" target="_blank">root@192.168.67.241</a>&gt;&#39;s password: [ccomp1.cluster:03503] [0,0,0] ORTE_ERROR_LOG: Timeout in file base/pls_base_orted_cmds.c at line 275<div>

<br>
<br>
[ccomp1.cluster:03503] [0,0,0] ORTE_ERROR_LOG: Timeout in file pls_rsh_module.c at line 1166<br>
<br>
[ccomp1.cluster:03503] [0,0,0] ORTE_ERROR_LOG: Timeout in file errmgr_hnp.c at line 90<br>
<br>
[ccomp1.cluster:03503] ERROR: A daemon on node 192.168.45.65 failed to start as expected.<br>
<br>
[ccomp1.cluster:03503] ERROR: There may be more information available from<br>
<br>
[ccomp1.cluster:03503] ERROR: the remote shell (see above).<br>
<br>
[ccomp1.cluster:03503] ERROR: The daemon exited unexpectedly with status 255.<br>
<br>
[ccomp1.cluster:03503] [0,0,0] ORTE_ERROR_LOG: Timeout in file base/pls_base_orted_cmds.c at line 188<br>
<br>
[ccomp1.cluster:03503] [0,0,0] ORTE_ERROR_LOG: Timeout in file pls_rsh_module.c at line 1198<br>
<br>
<br>
What is the problem here?<br>
<br>
--------------------------------------------------------------------------<br>
<br>
mpirun was unable to cleanly terminate the daemons for this job. Returned value Timeout instead of ORTE_SUCCESS<br>
<br>
<br></div><div><div></div><div>
On Tue, Apr 14, 2009 at 7:15 PM, Eugene Loh &lt;<a href="mailto:Eugene.Loh@sun.com" target="_blank">Eugene.Loh@sun.com</a> &lt;mailto:<a href="mailto:Eugene.Loh@sun.com" target="_blank">Eugene.Loh@sun.com</a>&gt;&gt; wrote:<br>


<br>
    Ankush Kaul wrote:<br>
<br>
        Finally, after mentioning the hostfiles the cluster is working<br>
        fine. We downloaded few benchmarking softwares but i would like<br>
        to know if there is any GUI based benchmarking software so that<br>
        its easier to demonstrate the working of our cluster while<br>
        displaying our cluster.<br>
<br>
<br>
    I&#39;m confused what you&#39;re looking for here, but thought I&#39;d venture a<br>
    suggestion.<br>
<br>
    There are GUI-based performance analysis and tracing tools.  E.g.,<br>
    run a program, [[semi-]automatically] collect performance data, run<br>
    a GUI-based analysis tool on the data, visualize what happened on<br>
    your cluster.  Would this suit your purposes?<br>
<br>
    If so, there are a variety of tools out there you could try.  Some<br>
    are platform-specific or cost money.  Some are widely/freely<br>
    available.  Examples of these tools include Intel Trace Analyzer,<br>
    Jumpshot, Vampir, TAU, etc.  I do know that Sun Studio (Performance<br>
    Analyzer) is available via free download on x86 and SPARC and Linux<br>
    and Solaris and works with OMPI.  Possibly the same with Jumpshot.<br>
     VampirTrace instrumentation is already in OMPI, but then you need<br>
    to figure out the analysis-tool part.  (I think the Vampir GUI tool<br>
    requires a license, but I&#39;m not sure.  Maybe you can convert to TAU,<br>
    which is probably available for free download.)<br>
<br>
    Anyhow, I don&#39;t even know if that sort of thing fits your<br>
    requirements.  Just an idea.<br>
<br>
    _______________________________________________<br>
    users mailing list<br></div></div>
    <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<div><br>
    <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br></div>
------------------------------------------------------------------------<div><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></blockquote><div><div></div><div>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>
</div></div></blockquote></div><br>

