Peter,<div><br></div><div>mpirun does fork&amp;exec the orted daemon on the remote nodes,</div><div>an then orted firm&amp;exec the MPI app (your shell script here)</div><div>the MPI app connects back to orted (1.6.5 might use tcp instead of unix sockets,</div><div>but I do not recall if 127.0.0.1 is hardcoded, and if it is, what the impact of having the MPI app in a container is)</div><div>the environment used by the MPI app starts with OMPI_, so you might want to update your she&#39;ll script first.</div><div><br></div><div>I wrote a high level overview of these mechanisms a few days ago at <a href="http://www.open-mpi.org/community/lists/devel/2015/10/18189.php">http://www.open-mpi.org/community/lists/devel/2015/10/18189.php</a>, feel free to read it.</div><div><br></div>at first glance, the way you are using containers look overly complicated.<div><br><div>are you sure you want one MPI task per container ?</div><div>I mean, you could have one container per VM and several MPI tasks per container.</div><div><br></div><div>then, a simpler alternative could be to wrap mpirun, starts the containers and then mpirun &quot;into&quot; these containers.</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<br><div><div><br>On Sunday, October 18, 2015, Peter Davis &lt;<a href="mailto:peter.davis8@gmail.com">peter.davis8@gmail.com</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">







<p>Hello,</p>
<p><span>I am currently trying to get MPI working using docker containers across networked virtual machines. The problem I have is that I don&#39;t seem to get any output from a dockerized MPI program (a tutorial matrix multiply program), but inspecting the nodes (docker containers) each process sits at 98% cpu usage whilst mpirun does not seem to get any output, and nothing complains or finishes. Running the same application on my laptop (with a local mpirun call) the running time is less than a second. </span></p>
<p><span>Running a “hello world” program which just printed its “rank&quot; seems to work fine with this docker setup, with mpirun getting the stdout printf calls. However, that example never had any inter-node communications. </span></p>
<p>Much of the system setup we have is based on the idea that environment variables are what passes information from mpirun to the node processes (under ssh), but I have not had any exposure to MPI prior to this, so perhaps this is wrong.<br><span></span></p>
<p>A (relatively) quick description of the system;<br><span></span></p>
<p><span> - We have 3 virtual machines that are interconnected on an infiniband network (however, in the hope of it being simpler, we currently use the tcp/ip layer not infiniband)</span></p>
<p><span> - There is no resource manager (e.g. Slurm) running, everything is over ssh.</span></p>
<p><span> - Each vm is running Centos 7, has openmpi 1.6.4 installed, and loads the mpi environment module up via .bashrc.</span></p>
<p><span> - We have docker installed on each vm.</span></p>
<p><span> - We have a container that is based on ubuntu 14:04, has openmpi version 1.6.5 installed, and runs an mpi-based matrix_multiply program on startup. Ompi_info output is:</span></p>
<p><span>   &gt; ompi_info -v ompi full --parsable</span></p>
<p><span>     package:Open MPI buildd@allspice Distribution</span></p>
<p><span>     ompi:version:full:1.6.5</span></p>
<p><span>     ompi:version:svn:r28673</span></p>
<p><span>     ompi:version:release_date:Jun 26, 2013</span></p>
<p><span>     orte:version:full:1.6.5</span></p>
<p><span>     orte:version:svn:r28673</span></p>
<p><span>     orte:version:release_date:Jun 26, 2013</span></p>
<p><span>     opal:version:full:1.6.5</span></p>
<p><span>     opal:version:svn:r28673</span></p>
<p><span>     opal:version:release_date:Jun 26, 2013</span></p>
<p><span>     mpi-api:version:full:2.1</span></p>
<p><span>     ident:1.6.5</span></p>
<p><span> </span></p>
<p><span> - To start the mpi program(s), on one of the vms we call mpirun with:</span></p>
<p>     mpirun --mca btl_tcp_port_min_v4 7950 --mca btl_tcp_port_range_v4 50 --mca btl tcp,self --mca mpi_base_verbose 30 -H 10.3.2.41,10.3.2.42,10.3.2.43 /root/docker-machine-script.sh<br><span></span></p>
<p> - This then runs our shell script on the vm, which collects environment variables, filters out the vm mpi variables, and starts the docker container with those env variables. Our shell script is:<br><span></span></p>
<blockquote style="margin:0 0 0 40px;border:none;padding:0px"><p>#!/bin/bash</p></blockquote><p><span></span></p>
<blockquote style="margin:0 0 0 40px;border:none;padding:0px"><p><span>env | grep MPI | grep -v &#39;MPI_INCLUDE\|MPI_PYTHON\|MPI_LIB\|MPI_BIN\|MPI_COMPILER\|MPI_SYSCONFIG\|MPI_SUFFIX\|MPI_MAN\|MPI_HOME\|MPI_FORTRAN_MOD_DIR&#39; &gt; /root/env.txt</span></p></blockquote>
<blockquote style="margin:0 0 0 40px;border:none;padding:0px"><p><span>docker run --privileged --env-file /root/env.txt -p 7950-8000:7950-8000 mrmagooey/mpi-mm </span></p></blockquote>
<p><span></span><br></p>
<p><span> - Finally, the container runs the mpi application (the matrix multiply) using the environment variables.</span></p>
<p><span> - Each container is capable of ssh’ing into the other containers without password. </span></p>
<p>It is a rather complicated setup, but the real application that we will eventually run is a pain to compile from scratch and so docker is a very appealing solution. <br><span></span></p>
<p>Attached is the &quot;ompi_info —all” and “ifconfig” called on one of the host vm’s (as per the forum support instructions). Also attached is the result of /usr/bin/printenv on one of the containers.</p>
<p><span>Thank you,</span></p>
<p><span>Peter</span></p><p><span></span></p><p><span></span></p><p><span></span></p><p><span></span></p><p><span></span></p>
<p><span>    </span></p></div>
</blockquote></div></div></div></div>

