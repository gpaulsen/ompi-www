<table cellspacing="0" cellpadding="0" border="0" ><tr><td valign="top" style="font: inherit;">Just to make sure, because I have to use open mpi for this program:<br><br>I'm using the default mpirun command.<br><br>When I type "man mpirun", these are the first few lines:<br><br>MPIRUN(1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; OPEN MPI COMMANDS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MPIRUN(1)<br><br>NAME<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; orterun,&nbsp; mpirun,&nbsp; mpiexec&nbsp; - Execute serial and parallel jobs in Open<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MPI.<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Note: mpirun, mpiexec, and orterun are all&nbsp; exact&nbsp; synonyms&nbsp; for&nbsp; each<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; other.&nbsp; Using any of the names will result in
 exactly identical behav-<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ior.<br><br>Ted<br>--- On <b>Fri, 2/6/09, Ralph Castain <i>&lt;rhc@lanl.gov&gt;</i></b> wrote:<br><blockquote style="border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;">From: Ralph Castain &lt;rhc@lanl.gov&gt;<br>Subject: Re: [OMPI users] Global Communicator<br>To: tedhyu@wag.caltech.edu, "Open MPI Users" &lt;users@open-mpi.org&gt;<br>Date: Friday, February 6, 2009, 7:55 AM<br><br><div id="yiv434531328">Hi Ted<div><br></div><div>From what I can tell, you are not using Open MPI, but mpich's mpirun. You might want to ask for help on their mailing list.</div><div><br></div><div>Ralph</div><div><br><div><div>On Feb 6, 2009, at 8:49 AM, Ted Yu wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td style="font-family: inherit; font-style: inherit; font-variant: inherit; font-weight:
 inherit; font-size: inherit; line-height: inherit; font-size-adjust: inherit; font-stretch: inherit;" valign="top">Thanx for the reply.&nbsp; <br><br>I guess I should go back a step:&nbsp; I had used the openmpi version on my system which is simply:<br>"mpirun -machinefile $PBS_NODEFILE -np $NPROCS ${CODE} &gt;/ul/tedhyu/fuelcell/HOH/test/HH.out"<br><br>This did not work because I was just getting a blank output.<br><br>I tried this older version because at least i was getting an output.<br>"/opt/mpich-1.2.5.10-ch_p4-gcc/bin/mpirun -machinefile $PBS_NODEFILE -np<br><pre>$NPROCS ${CODE} &gt;/ul/tedhyu/fuelcell/HOH/test/HH.out"</pre> I think this older version is failing me for whatever reason.&nbsp; Do you have any clue?&nbsp; I read somewhere that new versions of mpirun adds extra commandline arguments to the end of the line.&nbsp; Therefore the newer version of mpirun may be not be giving an output because it sees all extra commandline arguments after
 my output file &gt;/ul/tedhyu/fuelcell/HOH/test/HH.out<br><br>This is where I'm reading that there are extra commandline arguments for a version of mpirun:<br><a rel="nofollow" target="_blank" href="https://lists.sdsc.edu/pipermail/npaci-rocks-discussion/2008-February/029333.html">https://lists.sdsc.edu/pipermail/npaci-rocks-discussion/2008-February/029333.html</a><br><br>Again, I'm new at this, and I'm just guessing.&nbsp; Any ideas of where to turn would be helpful!<br><br>Ted<br><br>--- On <b>Thu, 2/5/09, doriankrause <i>&lt;doriankrause@web.de&gt;</i></b> wrote:<br><blockquote style="border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;">From: doriankrause &lt;doriankrause@web.de&gt;<br>Subject: Re: [OMPI users] Global Communicator<br>To: tedhyu@wag.caltech.edu, "Open MPI Users" &lt;users@open-mpi.org&gt;<br>Date: Thursday, February 5, 2009, 11:14 PM<br><br><pre>Ted Yu wrote:<br>&gt; I'm trying to run a job based on openmpi. 
 For some reason, the<br>program and the global communicator are not in sync and it reads that there is<br>only one processors, whereas, there should be 2 or more.  Any advice on where to<br>look?  Here is my PBS script.  Thanx!<br>&gt;<br>&gt; PBS SCRIPT:<br>&gt; #!/bin/sh<br>&gt; ### Set the job name<br>&gt; #PBS -N HH<br>&gt; ### Declare myprogram non-rerunable<br>&gt; #PBS -r n<br>&gt; ### Combine standard error and standard out to one file.<br>&gt; #PBS -j oe<br>&gt; ### Have PBS mail you results<br>&gt; #PBS -m ae<br>&gt; #PBS -M tedhyu@wag.caltech.edu<br>&gt; ### Set the queue name, given to you when you get a reservation.<br>&gt; #PBS -q workq<br>&gt; ### Specify the number of cpus for your job.  This example will run on 32<br>cpus<br>&gt;<br> ### using 8 nodes with 4 processes per node.<br>&gt; #PBS -l nodes=1:ppn=2,walltime=70:00:00<br>&gt; # Switch to the working directory; by default PBS launches processes from<br>your home directory.<br>&gt;
 # Jobs should only be run from /home, /project, or /work; PBS returns<br>results via NFS.<br>&gt; PBS_O_WORKDIR=/temp1/tedhyu/HH<br>&gt; export<br>CODE=/project/source/seqquest/seqquest_source_v261j/hive_CentOS4.5_parallel/build_261j/quest_ompi.x<br>&gt;<br>&gt; echo Working directory is $PBS_O_WORKDIR<br>&gt; mkdir -p $PBS_O_WORKDIR<br>&gt; cd $PBS_O_WORKDIR<br>&gt; rm -rf *<br>&gt; cp /ul/tedhyu/fuelcell/HOH/test/HH.in ./lcao.in<br>&gt; cp /ul/tedhyu/atom_pbe/* .<br>&gt; echo Running on host `hostname`<br>&gt; echo Time is `date`<br>&gt; echo Directory is `pwd`<br>&gt; echo This jobs runs on the following processors:<br>&gt; echo `cat $PBS_NODEFILE`<br>&gt; Number=`wc -l $PBS_NODEFILE | awk '{print $1}'`<br>&gt;<br>&gt; export Number<br>&gt; echo<br> ${Number}<br>&gt; # Define number of processors<br>&gt; NPROCS=`wc -l &lt; $PBS_NODEFILE`<br>&gt; # And the number or hosts<br>&gt; NHOSTS=`cat $PBS_NODEFILE|uniq|wc -l`<br>&gt; echo This job has
 allocated $NPROCS cpus<br>&gt; echo NHOSTS<br>&gt; #mpirun  -machinefile $PBS_NODEFILE  ${CODE}<br>&gt;/ul/tedhyu/fuelcell/HOH/test/HH.out<br>&gt; #mpiexec -np 2  ${CODE} &gt;/ul/tedhyu/fuelcell/HOH/test/HH.out<br>&gt; /opt/mpich-1.2.5.10-ch_p4-gcc/bin/mpirun -machinefile $PBS_NODEFILE -np<br>$NPROCS ${CODE} &gt;/ul/tedhyu/fuelcell/HOH/test/HH.out<br>&gt; cd ..<br>&gt; rm -rf HH<br>&gt;<br>&gt;<br>&gt;   <br><br>Please note, that you are mixing Open MPI (API/Library) with MPICH <br>(mpirun). This is a mistake I like to make, too. If you use<br>the ompi mpiexec program, it probably works.<br><br>Dorian<br><br>&gt;<br>&gt;       <br>&gt;   <br>&gt; ------------------------------------------------------------------------<br>&gt;<br>&gt;<br> _______________________________________________<br>&gt; users mailing list<br>&gt; users@open-mpi.org<br>&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br><br></pre></blockquote></td></tr></tbody></table><br>  
     _______________________________________________<br>users mailing list<br><a rel="nofollow" target="_blank" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></div></blockquote></td></tr></table><br>

      
