<html dir="ltr">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<style id="owaParaStyle" type="text/css">P {margin-top:0;margin-bottom:0;}</style>
</head>
<body ocsi="0" fpstyle="1">
<div style="direction: ltr;font-family: Tahoma;color: #000000;font-size: 10pt;">Hi Gilles,<br>
<br>
Thanks for the support. :)<br>
<br>
This is a test which I am running on a single node, but I am intending to run calculations on multiple nodes. You mean it wouldn't work on multiple nodes? If I run on multiple nodes, how can I avoid these errors then? I would just test it for multiple nodes.
<br>
<br>
Regards,<br>
Rizwan<br>
<div style="font-family: Times New Roman; color: #000000; font-size: 16px">
<hr tabindex="-1">
<div style="direction: ltr;" id="divRpF925525"><font face="Tahoma" size="2" color="#000000"><b>From:</b> users [users-bounces@open-mpi.org] on behalf of Gilles Gouaillardet [gilles.gouaillardet@gmail.com]<br>
<b>Sent:</b> Monday, June 20, 2016 3:10 PM<br>
<b>To:</b> Open MPI Users<br>
<b>Subject:</b> [OMPI users] memory cg '(null)'<br>
</font><br>
</div>
<div></div>
<div>There are two points here
<div>1. slurm(stepd) is unable to put the processes in the (null) cgroup.</div>
<div>&nbsp; &nbsp;at first glance, this looks more of a slurm jus configuration</div>
<div>2. the MPI process forking. though this has a much better support than in the past, that might not always work, especially with fast interconnects. since you are running on a single node, you should be fine. simply</div>
<div>export OMPI_MCA_mpi_warn_on_fork=0</div>
<div>before invoking srun, in order to silence this message.</div>
<div><br>
</div>
<div>Cheers,</div>
<div><br>
</div>
<div>Gilles</div>
<div><br>
On Monday, June 20, 2016, Ahmed Rizwan &lt;<a href="UrlBlockedError.aspx" target="_blank">rizwan.ahmed@aalto.fi</a>&gt; wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex; border-left:1px #ccc solid; padding-left:1ex">
<div>
<div style="direction:ltr; font-family:Tahoma; color:#000000; font-size:10pt">Dear MPI users,<br>
<br>
I am getting the errors below while submitting/executing following script, <br>
<br>
#!/bin/sh<br>
#SBATCH -p short<br>
#SBATCH -J layers<br>
#SBATCH -n 12<br>
#SBATCH -N 1 <br>
#SBATCH -t 01:30:00<br>
#SBATCH --mem-per-cpu=2500<br>
#SBATCH --exclusive<br>
#SBATCH --mail-type=END<br>
#SBATCH --mail-user=<a>rizwan.ahmed@aalto.fi</a><br>
#SBATCH -o output_%j.out<br>
#SBATCH -e errors_%j.err<br>
<br>
srun --mpi=pmi2 gpaw-python layers.py<br>
<br>
--------------------------------------------------------------------------<br>
slurmstepd: error: task/cgroup: unable to add task[pid=126453] to memory cg '(null)'<br>
slurmstepd: error: task/cgroup: unable to add task[pid=80379] to memory cg '(null)'<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124258] to memory cg '(null)'<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124259] to memory cg '(null)'<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124261] to memory cg '(null)'<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124266] to memory cg '(null)'<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124264] to memory cg '(null)'<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124262] to memory cg '(null)'<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124260] to memory cg '(null)'<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124265] to memory cg '(null)'<br>
slurmstepd: error: task/cgroup: unable to add task[pid=124263] to memory cg '(null)'<br>
--------------------------------------------------------------------------<br>
An MPI process has executed an operation involving a call to the<br>
&quot;fork()&quot; system call to create a child process.&nbsp; Open MPI is currently<br>
operating in a condition that could result in memory corruption or<br>
other system errors; your MPI job may hang, crash, or produce silent<br>
data corruption.&nbsp; The use of fork() (or system() or other calls that<br>
create child processes) is strongly discouraged.&nbsp; <br>
<br>
The process that invoked fork was:<br>
<br>
&nbsp; Local host:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pe38 (PID 80379)<br>
&nbsp; MPI_COMM_WORLD rank: 1<br>
<br>
If you are *absolutely sure* that your application will successfully<br>
and correctly survive a call to fork(), you may disable this warning<br>
by setting the mpi_warn_on_fork MCA parameter to 0.<br>
--------------------------------------------------------------------------<br>
<br>
Is this error fatal or should it be ignored? Thanks<br>
Regards,<br>
Rizwan<br>
</div>
</div>
</blockquote>
</div>
</div>
</div>
</div>
</body>
</html>

