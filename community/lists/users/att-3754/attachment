Jeff, I did what you suggested<br><br>However no noticeable changes seem to happen. Same peaks and same latency times.<br><br>Are you sure that for disabling the Nagle&#39;s algorithm is needed just changing optval to 0?<br>
I saw that, in btl_tcp_endpoint.c, the optval assignement is inside a <br>#if defined(TCP_NODELAY) block.<br><br>Where does this macro can be defined? <br>Any other idea for manage latency peaks? <br><br>Biagio<br><br><br>
<div><span class="gmail_quote">On 7/24/07, <b class="gmail_sendername">Jeff Squyres</b> &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:</span><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
On Jul 23, 2007, at 6:43 AM, Biagio Cosenza wrote:<br><br>&gt; I&#39;m working on a parallel real time renderer: an embarassing<br>&gt; parallel problem where latency is the threshold to high perfomance.<br>&gt;<br>&gt; Two observations:
<br>&gt;<br>&gt; 1) I did a simple &quot;ping-pong&quot; test (the master does a Bcast + an<br>&gt; IRecv for each node + a Waitall) similar to effective renderer<br>&gt; workload. Using a cluster of 37 nodes on Gigabit Ethernet, seems
<br>&gt; that the latency is usually low (about 1-5 ms), but sometimes there<br>&gt; are some peaks of about 200 ms. I thought that the cause is a<br>&gt; packet retransmission in one of the 37 connections, that blow the<br>
&gt; overall performance of the test (of course, the final WaitAll is a<br>&gt; synch).<br>&gt;<br>&gt; 2) A research team argues in a paper&nbsp;&nbsp;that MPI suffers on<br>&gt; dynamically manage latency. They also arguing an interesting
<br>&gt; problem about enable/disable Nagle algorithm. (I paste the<br>&gt; interesting paragraph below)<br>&gt;<br>&gt;<br>&gt; So I have two questions:<br>&gt;<br>&gt; 1) Why my test have these peaks? How can I afford them (I think to
<br>&gt; btl tcp params)?<br><br>They are probably beyond Open MPI&#39;s control -- OMPI mainly does read<br>() and write() down TCP sockets and relies on the kernel to do all<br>the low-level TCP protocol / wire transmission stuff.
<br><br>You might want to try increasing your TCP buffer sizes, but I think<br>that the Linux kernel has some built in limits.&nbsp;&nbsp;Other experts might<br>want to chime in here...<br><br>&gt; 2) When does OpenMPI disable Nagle algorithm? Suppose I DON&#39;T need
<br>&gt; that Nagle has to be ON (focusing only on latency), how can I<br>&gt; increase performance?<br><br>It looks like we enable Nagle right when TCP BTL connections are<br>made.&nbsp;&nbsp;Surprisingly, it looks like we don&#39;t have a run-time option to
<br>turn it off for power-users like you who want to really tweak around.<br><br>If you want to play with it, please edit ompi/mca/btl/tcp/<br>btl_tcp_endpoint.c.&nbsp;&nbsp;You&#39;ll see the references to TCP_NODELAY in<br>conjunction with setsockopt().&nbsp;&nbsp;Set the optval to 0 instead of 1.&nbsp;&nbsp;A
<br>simple &quot;make install&quot; in that directory will recompile the TCP<br>component and re-install it (assuming you have done a default build<br>with OMPI components built as standalone plugins).&nbsp;&nbsp;Let us know what<br>
you find.<br><br>--<br>Jeff Squyres<br>Cisco Systems<br><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

