<div dir="ltr"><div><div><div><div><div><div><div><div><div><div><div>Hi all<br><br></div>I have reported this issue before, but then had brushed it off as something that was caused by my modifications to the source tree. It looks like that is not the case.<br><br></div>Just now, I did the following:<br><br></div>1. Cloned a fresh copy from master.<br></div>2. Configured with the following flags, built and installed it in my two-node &quot;cluster&quot;.<br></div>--enable-debug --enable-debug-symbols --disable-dlopen<br></div>3. Compiled the following program, mpitest.c with these flags: -g3 -Wall -Wextra<br></div>4. Ran it like this:<br>[durga@smallMPI ~]$ mpirun -np 2 -hostfile ~/hostfile -mca btl self,tcp -mca pml ob1 ./mpitest<br><br></div>With this, the code hangs at MPI_Barrier() on both nodes, after generating the following output:<br><br>Hello world from processor smallMPI, rank 0 out of 2 processors<br>Hello world from processor bigMPI, rank 1 out of 2 processors<br>smallMPI sent haha!<br>bigMPI received haha!<br></div>&lt;Hangs until killed by ^C&gt;<br></div>Attaching to the hung process at one node gives the following backtrace:<br><br>(gdb) bt<br>#0  0x00007f55b0f41c3d in poll () from /lib64/libc.so.6<br>#1  0x00007f55b03ccde6 in poll_dispatch (base=0x70e7b0, tv=0x7ffd1bb551c0) at poll.c:165<br>#2  0x00007f55b03c4a90 in opal_libevent2022_event_base_loop (base=0x70e7b0, flags=2) at event.c:1630<br>#3  0x00007f55b02f0144 in opal_progress () at runtime/opal_progress.c:171<br>#4  0x00007f55b14b4d8b in opal_condition_wait (c=0x7f55b19fec40 &lt;ompi_request_cond&gt;, m=0x7f55b19febc0 &lt;ompi_request_lock&gt;) at ../opal/threads/condition.h:76<br>#5  0x00007f55b14b531b in ompi_request_default_wait_all (count=2, requests=0x7ffd1bb55370, statuses=0x7ffd1bb55340) at request/req_wait.c:287<br>#6  0x00007f55b157a225 in ompi_coll_base_sendrecv_zero (dest=1, stag=-16, source=1, rtag=-16, comm=0x601280 &lt;ompi_mpi_comm_world&gt;)<br>    at base/coll_base_barrier.c:63<br>#7  0x00007f55b157a92a in ompi_coll_base_barrier_intra_two_procs (comm=0x601280 &lt;ompi_mpi_comm_world&gt;, module=0x7c2630) at base/coll_base_barrier.c:308<br>#8  0x00007f55b15aafec in ompi_coll_tuned_barrier_intra_dec_fixed (comm=0x601280 &lt;ompi_mpi_comm_world&gt;, module=0x7c2630) at coll_tuned_decision_fixed.c:196<br>#9  0x00007f55b14d36fd in PMPI_Barrier (comm=0x601280 &lt;ompi_mpi_comm_world&gt;) at pbarrier.c:63<br>#10 0x0000000000400b0b in main (argc=1, argv=0x7ffd1bb55658) at mpitest.c:26<br>(gdb) <br><br></div>Thinking that this might be a bug in tuned collectives, since that is what the stack shows, I ran the program like this (basically adding the ^tuned part)<br><br>[durga@smallMPI ~]$ mpirun -np 2 -hostfile ~/hostfile -mca btl self,tcp -mca pml ob1 -mca coll ^tuned ./mpitest<br><br><div><div><div><div>It still hangs, but now with a different stack trace:<br>(gdb) bt<br>#0  0x00007f910d38ac3d in poll () from /lib64/libc.so.6<br>#1  0x00007f910c815de6 in poll_dispatch (base=0x1a317b0, tv=0x7fff43ee3610) at poll.c:165<br>#2  0x00007f910c80da90 in opal_libevent2022_event_base_loop (base=0x1a317b0, flags=2) at event.c:1630<br>#3  0x00007f910c739144 in opal_progress () at runtime/opal_progress.c:171<br>#4  0x00007f910db130f7 in opal_condition_wait (c=0x7f910de47c40 &lt;ompi_request_cond&gt;, m=0x7f910de47bc0 &lt;ompi_request_lock&gt;)<br>    at ../../../../opal/threads/condition.h:76<br>#5  0x00007f910db132d8 in ompi_request_wait_completion (req=0x1b07680) at ../../../../ompi/request/request.h:383<br>#6  0x00007f910db1533b in mca_pml_ob1_send (buf=0x0, count=0, datatype=0x7f910de1e340 &lt;ompi_mpi_byte&gt;, dst=1, tag=-16, sendmode=MCA_PML_BASE_SEND_STANDARD, <br>    comm=0x601280 &lt;ompi_mpi_comm_world&gt;) at pml_ob1_isend.c:259<br>#7  0x00007f910d9c3b38 in ompi_coll_base_barrier_intra_basic_linear (comm=0x601280 &lt;ompi_mpi_comm_world&gt;, module=0x1b092c0) at base/coll_base_barrier.c:368<br>#8  0x00007f910d91c6fd in PMPI_Barrier (comm=0x601280 &lt;ompi_mpi_comm_world&gt;) at pbarrier.c:63<br>#9  0x0000000000400b0b in main (argc=1, argv=0x7fff43ee3a58) at mpitest.c:26<br>(gdb) <br><br></div><div><div>The mpitest.c program is as follows:<br>#include &lt;mpi.h&gt;<br>#include &lt;stdio.h&gt;<br>#include &lt;string.h&gt;<br><br>int main(int argc, char** argv)<br>{<br>    int world_size, world_rank, name_len;<br>    char hostname[MPI_MAX_PROCESSOR_NAME], buf[8];<br><br>    MPI_Init(&amp;argc, &amp;argv);<br>    MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);<br>    MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);<br>    MPI_Get_processor_name(hostname, &amp;name_len);<br>    printf(&quot;Hello world from processor %s, rank %d out of %d processors\n&quot;, hostname, world_rank, world_size);<br>    if (world_rank == 1)<br>    {<br>    MPI_Recv(buf, 6, MPI_CHAR, 0, 99, MPI_COMM_WORLD, MPI_STATUS_IGNORE);<br>    printf(&quot;%s received %s\n&quot;, hostname, buf);<br>    }<br>    else<br>    {<br>    strcpy(buf, &quot;haha!&quot;);<br>    MPI_Send(buf, 6, MPI_CHAR, 1, 99, MPI_COMM_WORLD);<br>    printf(&quot;%s sent %s\n&quot;, hostname, buf);<br>    }<br>    MPI_Barrier(MPI_COMM_WORLD);<br>    MPI_Finalize();<br>    return 0;<br>}<br><br></div><div>The hostfile is as follows:<br>10.10.10.10 slots=1<br>10.10.10.11 slots=1<br><br></div><div>The two nodes are connected by three physical and 3 logical networks:<br></div><div>Physical: Gigabit Ethernet, 10G iWARP, 20G Infiniband<br></div><div>Logical: IP (all 3), PSM (Qlogic Infiniband), Verbs (iWARP and Infiniband)<br><br></div><div>Please note again that this is a fresh, brand new clone.<br><br></div><div>Is this a bug (perhaps a side effect of --disable-dlopen) or something I am doing wrong?<br><br></div><div>Thanks<br></div><div>Durga<br></div><div><br clear="all"><div><div><div><div><div><div><div><div><div><div class="gmail_signature"><div dir="ltr"><div>We learn from history that we never learn from history.<br></div></div></div></div>
</div></div></div></div></div></div></div></div></div></div></div></div></div></div>

