<div dir="ltr">&gt;Try and do a variable amount of work for every process, I see non-blocking
<br>
&gt;as a way to speed-up communication if they arrive individually to the call.
<br>
&gt;Please always have this at the back of your mind when doing this.
<div class="gmail_extra"><br></div><div class="gmail_extra">I tried to simplify the problem at the explanation. The &quot;local_computation&quot; is variable among different processes, so there is load imbalance in the real problem.<br></div><div class="gmail_extra">The microbenchmark was just a way to test the overhead, which was really much greater than expectations.<br><br>&gt;Surely non-blocking has overhead, and if the communication time is low, so
<br>&gt;will the overhead be much higher.
<br><br></div><div class="gmail_extra">Off course there is. But for my case, which is a real HPC application for seismic data processing, it was prohibitive and strangely high.<br></div><div class="gmail_extra"><br>&gt;You haven&#39;t specified what nx*ny*nz is, and hence your &quot;slower&quot; and
<br>&gt;&quot;faster&quot; makes &quot;no sense&quot;...  And hence your questions are difficult to
<br>&gt;answer, basically &quot;it depends&quot;.
<br><br></div><div class="gmail_extra">On my tests, I used nx = 700, ny = 200,  nz = 60, total_iter = 1000. val is a real(4) array. This is basically the same sizeas the real application.<br></div><div class="gmail_extra">Since I used the same values for all tests, it is reasonable to analyze the results. <br>What I meant with question 1 was: overheads so high are expected?<br><br></div><div class="gmail_extra">The microbenchmark is attached to this e-mail.<br></div><div class="gmail_extra"><br></div><div class="gmail_extra">The detailed result was (using 11 nodes):<br><font face="Default Sans Serif,Verdana,Arial,Helvetica,sans-serif" size="2"><span><br>openmpi blocking:<br></span><font face="Default Monospace,Courier New,Courier,monospace"> ==================================<br> [RESULT] Reduce time =  21.790411<br> [RESULT] Total  time =  24.977373<br> ==================================</font><br><br><span><span>openmpi non-blocking:<br><font face="Default Monospace,Courier New,Courier,monospace"> ==================================<br> [RESULT] Reduce time =  97.332792<br> [RESULT] Total  time = 100.470874<br> ==================================</font><br><br>Intel MPI + blocking:<br><font face="Default Monospace,Courier New,Courier,monospace"> ==================================<br> [RESULT] Reduce time =  17.587828<br> [RESULT] Total  time =  20.655875<br> ==================================</font><br><br><br>Intel MPI + non-blocking:<br> <font face="Default Monospace,Courier New,Courier,monospace">==================================<br> [RESULT] Reduce time =  49.483195<br> [RESULT] Total  time =  52.642514<br> ==================================</font></span></span></font><br></div><div class="gmail_extra"><br></div><div class="gmail_extra">Thanks in advance.<br></div><div class="gmail_extra"><br><div class="gmail_quote">2015-11-27 14:57 GMT-02:00 Felipe . <span dir="ltr">&lt;<a href="mailto:philip.fm@gmail.com" target="_blank">philip.fm@gmail.com</a>&gt;</span>:<br><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"><div dir="ltr"><div><span style="font-family:monospace,monospace">Hello!<br><br></span></div><div><span style="font-family:monospace,monospace">I have a program that basically is (first implementation):<br><span style="color:rgb(0,0,255)">for i in N:<br></span></span></div><div><span style="color:rgb(0,0,255)"><span style="font-family:monospace,monospace">  local_computation(i)<br></span></span></div><div><span style="font-family:monospace,monospace"><span style="color:rgb(0,0,255)">  mpi_allreduce(in_place, i)<br></span><br></span></div><div><span style="font-family:monospace,monospace">In order to try to mitigate the implicit barrier of the mpi_allreduce, I tried to start an mpi_Iallreduce. Like this(second implementation):<br><span style="color:rgb(0,0,255)"><font size="2">for i in N:<br></font></span></span><div><span style="color:rgb(0,0,255)"><span style="font-family:monospace,monospace"><font size="2">  local_computation(i)<br></font></span></span></div><div><span style="color:rgb(0,0,255)"><span style="font-family:monospace,monospace"><font size="2">  j = i<br></font></span></span></div><div><span style="color:rgb(0,0,255)"><span style="font-family:monospace,monospace"><font size="2">  if i is not first:<br></font></span></span></div><div><span style="color:rgb(0,0,255)"><span style="font-family:monospace,monospace"><font size="2">    mpi_wait(request)<br></font></span></span></div><span style="font-family:monospace,monospace"><span style="color:rgb(0,0,255)"><font size="2">  mpi_Iallreduce(in_place, j, request)</font></span><br><br>The result was that the second was a lot worse. The processes spent 3 times more time on the mpi_wait than on the mpi_allreduce from the first implementation. I know it could be worst, but not that much.<br><br></span></div><div><span style="font-family:monospace,monospace">So, I made a microbenchmark to stress this, in Fortran. Here is the implementation:<br></span></div><div><span style="font-family:monospace,monospace">Blocking:<br><span style="color:rgb(0,0,255)">do i = 1, total_iter ! [<br>    t_0 = mpi_wtime()<br><br>    call mpi_allreduce(MPI_IN_PLACE, val, nx*ny*nz, MPI_REAL, MPI_SUM, MPI_COMM_WORLD, ierror)<br>    if (ierror .ne. 0) then ! [<br>        write(*,*) &quot;Error in line &quot;, __LINE__, &quot; rank = &quot;, rank<br>        call mpi_abort(MPI_COMM_WORLD, ierror, ierror2)<br>    end if ! ]<br>    t_reduce = t_reduce + (mpi_wtime() - t_0)<br>end do ! ]</span><br><br></span></div><div><span style="font-family:monospace,monospace">Non-Blocking:<br><span style="color:rgb(0,0,255)">do i = 1, total_iter ! [<br>    t_0 = mpi_wtime()<br>    call mpi_iallreduce(MPI_IN_PLACE, val, nx*ny*nz, MPI_REAL, MPI_SUM, MPI_COMM_WORLD, request, ierror)<br>    if (ierror .ne. 0) then ! [<br>        write(*,*) &quot;Error in line &quot;, __LINE__, &quot; rank = &quot;, rank<br>        call mpi_abort(MPI_COMM_WORLD, ierror, ierror2)<br>    end if ! ]<br>    t_reduce = t_reduce + (mpi_wtime() - t_0)<br><br>    t_0 = mpi_wtime()<br>    call mpi_wait(request, status, ierror)<br>    if (ierror .ne. 0) then ! [<br>        write(*,*) &quot;Error in line &quot;, __LINE__, &quot; rank = &quot;, rank<br>        call mpi_abort(MPI_COMM_WORLD, ierror, ierror2)<br>    end if ! ]<br>    t_reduce = t_reduce + (mpi_wtime() - t_0)<br><br>end do ! ]</span><br><br></span></div><div><span style="font-family:monospace,monospace">The non-blocking was about five times slower. I tried Intel&#39;s MPI and it was of 3 times, instead of 5.<br><br></span></div><div><span style="font-family:monospace,monospace"><span style="font-family:monospace,monospace">Question 1: Do you think that all this overhead makes sense?</span><br><br>Question 2: Why is there so much overhead for non-blocking collective calls?<br></span><br><span style="font-family:monospace,monospace"><span style="font-family:monospace,monospace">Question 3: Can I change the algorithm for the non-blocking allReduce to improve this?</span><br></span></div><div><span style="font-family:monospace,monospace"><br><br></span></div><div><span style="font-family:monospace,monospace">Best regards,<br>--<br></span></div><span style="font-family:monospace,monospace">Felipe<br></span></div>
</blockquote></div><br></div></div>

