<div dir="ltr">Open MPI Users,<div><br></div><div>I&#39;m hoping someone here can help. I built Open MPI 1.10.0 with PGI 15.7 using this configure string:</div><div><br></div><div><div><font face="monospace, monospace"> ./configure --disable-vt --with-tm=/PBS --with-verbs --disable-wrapper-rpath \</font></div><div><font face="monospace, monospace">    CC=pgcc CXX=pgCC FC=pgf90 F77=pgf77 CFLAGS=&#39;-fpic -m64&#39; \</font></div><div><font face="monospace, monospace">    CXXFLAGS=&#39;-fpic -m64&#39; FCFLAGS=&#39;-fpic -m64&#39; FFLAGS=&#39;-fpic -m64&#39; \</font></div><div><font face="monospace, monospace">    --prefix=/nobackup/gmao_SIteam/MPI/pgi_15.7-openmpi_1.10.0 |&amp; tee configure.pgi15.7.log</font></div><div><br></div><div>It seemed to pass &#39;make check&#39;. </div><div><br></div><div>I&#39;m working at pleiades at NAS, and there they have both Sandy Bridge nodes with GPUs (maia) and regular Sandy Bridge compute nodes (here after called Sandy) without. To be extra careful (since PGI compiles to the architecture you build on) I took a Westmere node and built Open MPI there just in case.</div><div><br></div><div>So, as I said, all seems to work with a test. I now grab a maia node, maia1, of an allocation of 4 I had:</div><div><br></div><div><div><font face="monospace, monospace">(102) $ mpicc -tp=px-64 -o helloWorld.x helloWorld.c</font></div><div><font face="monospace, monospace">(103) $ mpirun -np 2 ./helloWorld.x</font></div><div><font face="monospace, monospace">Process 0 of 2 is on maia1 </font></div><div><font face="monospace, monospace">Process 1 of 2 is on maia1 </font></div></div><div><br></div><div>Good. Now, let&#39;s go to a Sandy Bridge (non-GPU) node, r321i7n16, of an allocation of 8 I had:</div><div><br></div><div><div><font face="monospace, monospace">(49) $ mpicc -tp=px-64 -o helloWorld.x helloWorld.c</font></div><div><font face="monospace, monospace">(50) $ mpirun -np 2 ./helloWorld.x</font></div><div><font face="monospace, monospace">[r323i5n11:13063] [[62995,0],7] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n6:57417] [[62995,0],2] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n7:67287] [[62995,0],3] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n8:57429] [[62995,0],4] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n10:35329] [[62995,0],6] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n9:13456] [[62995,0],5] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div></div><div><br></div><div>Hmm. Let&#39;s try turning off tcp (often my first thought when on an Infiniband system):</div><div><br></div><div><div><font face="monospace, monospace">(51) $ mpirun --mca btl sm,openib,self -np 2 ./helloWorld.x</font></div><div><font face="monospace, monospace">[r323i5n6:57420] [[62996,0],2] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n9:13459] [[62996,0],5] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n8:57432] [[62996,0],4] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n7:67290] [[62996,0],3] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n11:13066] [[62996,0],7] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n10:35332] [[62996,0],6] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div></div><div><br></div><div>Now, the nodes reporting the issue seem to be the &quot;other&quot; nodes on the allocation that are in a different rack:</div><div><br></div><div><div><font face="monospace, monospace">(52) $ cat $PBS_NODEFILE | uniq</font></div><div><font face="monospace, monospace">r321i7n16</font></div><div><font face="monospace, monospace">r321i7n17</font></div><div><font face="monospace, monospace">r323i5n6</font></div><div><font face="monospace, monospace">r323i5n7</font></div><div><font face="monospace, monospace">r323i5n8</font></div><div><font face="monospace, monospace">r323i5n9</font></div><div><font face="monospace, monospace">r323i5n10</font></div><div><font face="monospace, monospace">r323i5n11</font></div></div><div><br></div><div>Maybe that&#39;s a clue? I didn&#39;t think this would matter if I only ran two processes...and it works on the multi-node maia allocation.</div><div><br></div><div>I&#39;ve tried searching the web, but the only place I&#39;ve seen tcp_peer_send_blocking is in a PDF where they say it&#39;s an error that can be seen:</div><div><br></div><div><a href="http://www.hpc.mcgill.ca/downloads/checkpointing_workshop/20150326%20-%20McGill%20-%20Checkpointing%20Techniques.pdf">http://www.hpc.mcgill.ca/downloads/checkpointing_workshop/20150326%20-%20McGill%20-%20Checkpointing%20Techniques.pdf</a><br></div><div><br></div><div>Any ideas for what this error can mean?</div><div><br></div>-- <br><div class="gmail_signature"><div dir="ltr"><div><div dir="ltr"><div>Matt Thompson</div></div></div><blockquote style="margin:0px 0px 0px 40px;border:none;padding:0px"><div><div><div>Man Among Men</div></div></div><div><div><div>Fulcrum of History</div></div></div></blockquote></div></div>
</div></div>

