Ronald,<div><br></div><div>the fix I mentioned landed into the v1.10 branch</div><div><a href="https://github.com/open-mpi/ompi-release/commit/c376994b81030cfa380c29d5b8f60c3e53d3df62">https://github.com/open-mpi/ompi-release/commit/c376994b81030cfa380c29d5b8f60c3e53d3df62</a></div><div><br></div><div>can you please post your configure command line ?</div><div><br></div><div>you can also try to</div><div>mpirun --mca btl self,vader,openib ...</div><div>to make sure your run will abort instead of falling back to tcp<br></div><div><br></div><div>then you can</div><div>mpirun ... grep Cpus_allowed_list /proc/self/status</div><div>to confirm your tasks do not end up bound to the same cores when running on two nodes.</div><div><br></div><div>is your application known to scale on infiniband network ?</div><div>or did you naively hope it would scale ?</div><div></div><div><br></div><div>at first, I recommend you run standard benchmark to make sure you get the performance you expect from your infiniband network</div><div>(for example IMB or OSU benchmark)</div><div>and run this test in the same environment than your app (e.g. via a batch manager if applicable)</div><div><br></div><div>if you do not get the performance you expect, then I suggest you try the stock gcc compiler shipped with your distro and see if it helps.</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<br><br>On Wednesday, March 23, 2016, Ronald Cohen &lt;<a href="mailto:recohen3@gmail.com">recohen3@gmail.com</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Thank  you! Here are the answers:<br>
<br>
I did not try a previous release of gcc.<br>
I built from a tarball.<br>
What should I do about the iirc issue--how should I check?<br>
Are there any flags I should be using for infiniband? Is this a<br>
problem with latency?<br>
<br>
Ron<br>
<br>
<br>
---<br>
Ron Cohen<br>
<a href="javascript:;" onclick="_e(event, &#39;cvml&#39;, &#39;recohen3@gmail.com&#39;)">recohen3@gmail.com</a><br>
skypename: ronaldcohen<br>
twitter: @recohen3<br>
<br>
<br>
On Wed, Mar 23, 2016 at 8:13 AM, Gilles Gouaillardet<br>
&lt;<a href="javascript:;" onclick="_e(event, &#39;cvml&#39;, &#39;gilles.gouaillardet@gmail.com&#39;)">gilles.gouaillardet@gmail.com</a>&gt; wrote:<br>
&gt; Ronald,<br>
&gt;<br>
&gt; did you try to build openmpi with a previous gcc release ?<br>
&gt; if yes, what about the performance ?<br>
&gt;<br>
&gt; did you build openmpi from a tarball or from git ?<br>
&gt; if from git and without VPATH, then you need to<br>
&gt; configure with --disable-debug<br>
&gt;<br>
&gt; iirc, one issue was identified previously<br>
&gt; (gcc optimization that prevents the memory wrapper from behaving as<br>
&gt; expected) and I am not sure the fix landed in v1.10 branch nor master ...<br>
&gt;<br>
&gt; thanks for the info about gcc 6.0.0<br>
&gt; now this is supported on a free compiler<br>
&gt; (cray and intel already support that, but they are commercial compilers),<br>
&gt; I will resume my work on supporting this<br>
&gt;<br>
&gt; Cheers,<br>
&gt;<br>
&gt; Gilles<br>
&gt;<br>
&gt; On Wednesday, March 23, 2016, Ronald Cohen &lt;<a href="javascript:;" onclick="_e(event, &#39;cvml&#39;, &#39;recohen3@gmail.com&#39;)">recohen3@gmail.com</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; I get 100 GFLOPS for 16 cores on one node, but 1 GFLOP running 8 cores<br>
&gt;&gt; on two nodes. It seems that quad-infiniband should do better than<br>
&gt;&gt; this. I built openmpi-1.10.2g with gcc version 6.0.0 20160317 . Any<br>
&gt;&gt; ideas of what to do to get usable performance? Thank you!<br>
&gt;&gt;<br>
&gt;&gt; bstatus<br>
&gt;&gt; Infiniband device &#39;mlx4_0&#39; port 1 status:<br>
&gt;&gt;         default gid:     fe80:0000:0000:0000:0002:c903:00ec:9301<br>
&gt;&gt;         base lid:        0x1<br>
&gt;&gt;         sm lid:          0x1<br>
&gt;&gt;         state:           4: ACTIVE<br>
&gt;&gt;         phys state:      5: LinkUp<br>
&gt;&gt;         rate:            56 Gb/sec (4X FDR)<br>
&gt;&gt;         link_layer:      InfiniBand<br>
&gt;&gt;<br>
&gt;&gt; Ron<br>
&gt;&gt; --<br>
&gt;&gt;<br>
&gt;&gt; Professor Dr. Ronald Cohen<br>
&gt;&gt; Ludwig Maximilians Universität<br>
&gt;&gt; Theresienstrasse 41 Room 207<br>
&gt;&gt; Department für Geo- und Umweltwissenschaften<br>
&gt;&gt; München<br>
&gt;&gt; 80333<br>
&gt;&gt; Deutschland<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; <a href="javascript:;" onclick="_e(event, &#39;cvml&#39;, &#39;ronald.cohen@min.uni-muenchen.de&#39;)">ronald.cohen@min.uni-muenchen.de</a><br>
&gt;&gt; skype: ronaldcohen<br>
&gt;&gt; +49 (0) 89 74567980<br>
&gt;&gt; ---<br>
&gt;&gt; Ronald Cohen<br>
&gt;&gt; Geophysical Laboratory<br>
&gt;&gt; Carnegie Institution<br>
&gt;&gt; 5251 Broad Branch Rd., N.W.<br>
&gt;&gt; Washington, D.C. 20015<br>
&gt;&gt; <a href="javascript:;" onclick="_e(event, &#39;cvml&#39;, &#39;rcohen@carnegiescience.edu&#39;)">rcohen@carnegiescience.edu</a><br>
&gt;&gt; office: 202-478-8937<br>
&gt;&gt; skype: ronaldcohen<br>
&gt;&gt; <a href="https://twitter.com/recohen3" target="_blank">https://twitter.com/recohen3</a><br>
&gt;&gt; <a href="https://www.linkedin.com/profile/view?id=163327727" target="_blank">https://www.linkedin.com/profile/view?id=163327727</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; ---<br>
&gt;&gt; Ron Cohen<br>
&gt;&gt; <a href="javascript:;" onclick="_e(event, &#39;cvml&#39;, &#39;recohen3@gmail.com&#39;)">recohen3@gmail.com</a><br>
&gt;&gt; skypename: ronaldcohen<br>
&gt;&gt; twitter: @recohen3<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="javascript:;" onclick="_e(event, &#39;cvml&#39;, &#39;users@open-mpi.org&#39;)">users@open-mpi.org</a><br>
&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; Link to this post:<br>
&gt;&gt; <a href="http://www.open-mpi.org/community/lists/users/2016/03/28791.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/03/28791.php</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="javascript:;" onclick="_e(event, &#39;cvml&#39;, &#39;users@open-mpi.org&#39;)">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post:<br>
&gt; <a href="http://www.open-mpi.org/community/lists/users/2016/03/28793.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/03/28793.php</a><br>
_______________________________________________<br>
users mailing list<br>
<a href="javascript:;" onclick="_e(event, &#39;cvml&#39;, &#39;users@open-mpi.org&#39;)">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/03/28794.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/03/28794.php</a></blockquote></div>

