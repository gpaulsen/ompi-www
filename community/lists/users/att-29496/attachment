<div dir="ltr">Do you know for sure that MKL is only using one thread or do you merely see that the performance is consistent with it using one thread?<div><br></div><div>If MPI does process pinning, it is possible for all OpenMP threads to run on one core, which means one will observe no speedup from threads (and potentially a slowdown due to oversubscription).  I do not know the option to disable this with Open-MPI, but I assume either you can find it in the docs or one of the Open-MPI experts will provide it.</div><div><br>I have observed this issue with MVAPICH2 and Pthread applications in the past (it has been fixed both in MVAPICH2 and the relevant applications), but not in Open-MPI with OpenMP, although I am not a heavy user of Open-MPI.</div><div><br></div><div>Best,</div><div><br></div><div>Jeff </div></div><div class="gmail_extra"><br><div class="gmail_quote">On Wed, Jun 22, 2016 at 9:17 AM, remi marchal <span dir="ltr">&lt;<a href="mailto:remi.marchal@univ-rennes1.fr" target="_blank">remi.marchal@univ-rennes1.fr</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">Dear openmpi users,<div><br></div><div>Today, I faced a strange problem.</div><div><br></div><div>I am compiling a quantum chemistry software (CASTEP-16) using intel16, mkl threaded libraries and openmpi-18.1.</div><div><br></div><div>The compilation works fine.</div><div><br></div><div>When I ask for MKL_NUM_THREAD=4 and call the program in serial mode (without mpirun), it works perfectly and use 4 threads.</div><div><br></div><div>However, when I start the program with mpirun, even with 1 mpi process, the program ran but only with 1 thread.</div><div><br></div><div>I never add such kind of trouble.</div><div><br></div><div>Does anyone have an explanation.</div><div><br></div><div>Regards,</div><div><br></div><div>Rémi</div><div><div><div style="color:rgb(0,0,0);letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;word-wrap:break-word"><div style="color:rgb(0,0,0);letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;word-wrap:break-word"><div style="color:rgb(0,0,0);letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;word-wrap:break-word"><br><br style="color:rgb(0,0,0);font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px"><br style="color:rgb(0,0,0);font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px"></div></div></div>
</div>
<br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/06/29495.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/06/29495.php</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br><div class="gmail_signature" data-smartmail="gmail_signature">Jeff Hammond<br><a href="mailto:jeff.science@gmail.com" target="_blank">jeff.science@gmail.com</a><br><a href="http://jeffhammond.github.io/" target="_blank">http://jeffhammond.github.io/</a></div>
</div>

