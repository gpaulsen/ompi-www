<html><head><base href="x-msg://186/"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">What di you have in the "hosts" file? We don't have a native integration with Condor, so you'll have to specify the hosts and number of slots on each, as Reuti explained. You'll also need to check that your sys admin allows you to ssh without password to each host.<div><br></div><div><div><br><div><div><div>On Mar 28, 2012, at 10:02 AM, Hameed Alzahrani wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><span class="Apple-style-span" style="border-collapse: separate; font-family: Helvetica; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; font-size: medium; "><div class="hmmessage" style="font-size: 10pt; font-family: Tahoma; "><div dir="ltr">Hi,<br><br>Thanks that works fine when I submit hello program but when I tried to benchmark the system it look like it does not do anything&nbsp;<span class="Apple-converted-space">&nbsp;</span><br>mpirun -np 8 --hostfile hosts xhpl<br><br>Regards,<br><br><br><div><div id="SkyDrivePlaceholder"></div>&gt; From:<span class="Apple-converted-space">&nbsp;</span><a href="mailto:reuti@staff.uni-marburg.de">reuti@staff.uni-marburg.de</a><br>&gt; Date: Wed, 28 Mar 2012 17:40:07 +0200<br>&gt; To:<span class="Apple-converted-space">&nbsp;</span><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; Subject: Re: [OMPI users] Can not run a parallel job on all the nodes in the	cluster<br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; Am 28.03.2012 um 17:35 schrieb Hameed Alzahrani:<br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; Hi,<br>&gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; Is there a specific name or location for the hostfile because I could not figure how to specify the number of processors for each machine in the command line.<br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; No, just specify the name (or path) to it with:<br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; --hostfile foobar<br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; -- Reuti<br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; Regards,<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; From: <a href="mailto:reuti@staff.uni-marburg.de">reuti@staff.uni-marburg.de</a><br>&gt; &gt; &gt; Date: Wed, 28 Mar 2012 17:21:39 +0200<br>&gt; &gt; &gt; To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt; &gt; Subject: Re: [OMPI users] Can not run a parallel job on all the nodes in the	cluster<br>&gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; Hi,<br>&gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; Am 28.03.2012 um 16:55 schrieb Hameed Alzahrani:<br>&gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; I ran hello program which return the host name when I run it using<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; mpirun -np 8 hello<br>&gt; &gt; &gt; &gt; all the 8 answer returned from the same machine<br>&gt; &gt; &gt; &gt; when I run it using<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; mpirun -np 8 --host host1,host2,host3 hello<br>&gt; &gt; &gt; &gt; I got answers from all the machines but it is not from all processors because I have 8 processors host1=4, host2=2, host3=2 the answer was 3 from host1, 3 from host2 and 2 from host3.<br>&gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; If you want to specify the number of slots you can put it in a hostfile (otherwise a round robin assignment is just used). I'm not aware that it can be specified on the command line with different values for each machine:<br>&gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; host1 slots=4<br>&gt; &gt; &gt; host2 slots=2<br>&gt; &gt; &gt; host3 slots=2<br>&gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; -- Reuti<br>&gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; Regards,<br>&gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; From: <a href="mailto:reuti@staff.uni-marburg.de">reuti@staff.uni-marburg.de</a><br>&gt; &gt; &gt; &gt; &gt; Date: Wed, 28 Mar 2012 16:42:21 +0200<br>&gt; &gt; &gt; &gt; &gt; To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt; &gt; &gt; &gt; Subject: Re: [OMPI users] Can not run a parallel job on all the nodes in the	cluster<br>&gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; Hi,<br>&gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; Am 28.03.2012 um 16:30 schrieb Hameed Alzahrani:<br>&gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; &gt; Hi,<br>&gt; &gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; &gt; I mean the node that I run mpirun command from. I use condor as a scheduler but I need to benchmark the cluster either from condor or directly from open MPI.<br>&gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; I can't say anything regarding the Condor integration of Open MPI, but starting it directly by mpirun and supplying a valid number of ranks and hostfile should start some processes on other machines as requested. Can you run a plain mpihello first and output rank and hostname? Do you have ssh access to all the machines in questions? You have a shared home directory with the applications?<br>&gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; -- Reuti<br>&gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; &gt; when I ran mpirun from a machine and checking the memory status for the three machines that I have it appear that the memory usage increased just in the same machine.<br>&gt; &gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; &gt; Regards,<br>&gt; &gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; From: <a href="mailto:reuti@staff.uni-marburg.de">reuti@staff.uni-marburg.de</a><br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; Date: Wed, 28 Mar 2012 15:12:17 +0200<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; Subject: Re: [OMPI users] Can not run a parallel job on all the nodes in the	cluster<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; Hi,<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; Am 27.03.2012 um 23:46 schrieb Hameed Alzahrani:<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; When I run any parallel job I get the answer just from the submitting node<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; what do you mean by submitting node: you use a queuing system - which one?<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; -- Reuti<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; even when I tried to benchmark the cluster using LINPACK but it look like the job just working on the submitting node is there a way to make openMPI send the job equally to all the nodes depending on the number of processor in the current mode even if I specify that the job should use 8 processor it look like openMPI use the submitting node 4 processors instead of using the other processors. I tried also --host but it does not work correctly in benchmarking the cluster so does any one use openMPI in benchmarking a cluster or does any one knows how to make openMPI divids the parallel job equally to every processor on the cluster.<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; Regards,<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; _______________________________________________<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; users mailing list<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; _______________________________________________<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; users mailing list<br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt; &gt; &gt; &gt; &gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; &gt; &gt; &gt; &gt; &gt; _______________________________________________<br>&gt; &gt; &gt; &gt; &gt; &gt; users mailing list<br>&gt; &gt; &gt; &gt; &gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt; &gt; &gt; &gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; &gt; &gt; _______________________________________________<br>&gt; &gt; &gt; &gt; &gt; users mailing list<br>&gt; &gt; &gt; &gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt; &gt; &gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; &gt; &gt; &gt; _______________________________________________<br>&gt; &gt; &gt; &gt; users mailing list<br>&gt; &gt; &gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt; &gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; &gt; &gt; _______________________________________________<br>&gt; &gt; &gt; users mailing list<br>&gt; &gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; &gt; _______________________________________________<br>&gt; &gt; users mailing list<br>&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt;<span class="Apple-converted-space">&nbsp;</span><br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></span></blockquote></div><br></div></div></div></body></html>
