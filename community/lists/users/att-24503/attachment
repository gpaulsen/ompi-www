<div dir="ltr"><div>Dear Jeff,<br><br></div>Thanks for the information and helping me out. I too delayed replying, I wanted to test this but the cluster here is down. I will check it and let you know in case it doesn&#39;t work.<br>

<div class="gmail_extra"><br></div><div class="gmail_extra">Thanks<br clear="all"></div><div class="gmail_extra"><div><div dir="ltr"><span style="color:rgb(0,0,0)">Bibrak Qamar</span><br style="color:rgb(0,0,0)"><span style="color:rgb(0,0,0)"><br>

</span><font color="#888888"></font></div></div>
<br><br><div class="gmail_quote">On Sat, May 24, 2014 at 5:23 AM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">

I am sorry for the delay in replying; this week got a bit crazy on me.<br>
<br>
I&#39;m guessing that Open MPI is striping across both your eth0 and ib0 interfaces.<br>
<br>
You can limit which interfaces it uses with the btl_tcp_if_include MCA param.  For example:<br>
<br>
    # Just use eth0<br>
    mpirun --mca btl tcp,sm,self --mca btl_tcp_if_include eth0 ...<br>
<br>
    # Just use ib0<br>
    mpirun --mca btl tcp,sm,self --mca btl_tcp_if_include ib0 ...<br>
<br>
Note that IPoIB is nowhere near as efficient as native verbs, so you won&#39;t get nearly as good performance as you do with OMPI&#39;s openib transport.<br>
<br>
Note, too, that I specifically included &quot;--mca btl tcp,sm,self&quot; in the above examples to force the use of the TCP MPI transport.  Otherwise, OMPI may well automatically choose the native IB (openib) transport.  I see you mentioned this in your first mail, too, but I am listing it here just to be specific/pedantic.<br>


<div class="im HOEnZb"><br>
<br>
<br>
On May 22, 2014, at 3:30 AM, Bibrak Qamar &lt;<a href="mailto:bibrakc@gmail.com">bibrakc@gmail.com</a>&gt; wrote:<br>
<br>
</div><div class="HOEnZb"><div class="h5">&gt; Hi,<br>
&gt;<br>
&gt; I am facing problem in running Open MPI using TCP (on 1G Ethernet). In practice the bandwidth must not exceed 1000 Mbps but for some data points (for point-to-point ping pong) it exceeds this limit. I checked with MPICH it works as desired.<br>


&gt;<br>
&gt; Following is the command I issue to run my program on TCP. Am I missing something?<br>
&gt;<br>
&gt; -bash-3.2$ mpirun -np 2  -machinefile machines -N 1 --mca btl tcp,self ./bandwidth.ompi<br>
&gt; --------------------------------------------------------------------------<br>
&gt; The following command line options and corresponding MCA parameter have<br>
&gt; been deprecated and replaced as follows:<br>
&gt;<br>
&gt;   Command line options:<br>
&gt;     Deprecated:  --npernode, -npernode<br>
&gt;     Replacement: --map-by ppr:N:node<br>
&gt;<br>
&gt;   Equivalent MCA parameter:<br>
&gt;     Deprecated:  rmaps_base_n_pernode, rmaps_ppr_n_pernode<br>
&gt;     Replacement: rmaps_base_mapping_policy=ppr:N:node<br>
&gt;<br>
&gt; The deprecated forms *will* disappear in a future version of Open MPI.<br>
&gt; Please update to the new syntax.<br>
&gt; --------------------------------------------------------------------------<br>
&gt; Hello, world.  I am 1 on compute-0-16.local<br>
&gt; Hello, world.  I am 0 on compute-0-15.local<br>
&gt; 1    25.66    0.30<br>
&gt; 2    25.54    0.60<br>
&gt; 4    25.34    1.20<br>
&gt; 8    25.27    2.42<br>
&gt; 16    25.24    4.84<br>
&gt; 32    25.49    9.58<br>
&gt; 64    26.44    18.47<br>
&gt; 128    26.85    36.37<br>
&gt; 256    29.43    66.37<br>
&gt; 512    36.02    108.44<br>
&gt; 1024    42.03    185.86<br>
&gt; 2048    194.30    80.42<br>
&gt; 4096    255.21    122.45<br>
&gt; 8192    258.85    241.45<br>
&gt; 16384    307.96    405.90<br>
&gt; 32768    422.78    591.32<br>
&gt; 65536    790.11    632.83<br>
&gt; 131072    1054.08    948.70<br>
&gt; 262144    1618.20    1235.94<br>
&gt; 524288    3126.65    1279.33<br>
&gt;<br>
&gt; -Bibrak<br>
</div></div><div class="HOEnZb"><div class="h5">&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div></div>

