<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">You don’t want to always use those options as your performance will take a hit - TCP vs Infiniband isn’t a good option. Sadly, this is something we need someone like Nathan to address as it is a bug in the code base, and in an area I’m not familiar with<div class=""><br class=""></div><div class="">For now, just use TCP so you can move forward</div><div class=""><br class=""></div><div class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Jun 14, 2016, at 2:14 PM, Jason Maldonis &lt;<a href="mailto:maldonis@wisc.edu" class="">maldonis@wisc.edu</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><div dir="ltr" class="">Ralph, The problem *does* go away if I add "<span style="font-size:12.8px" class="">-mca btl tcp,sm,self" to the mpiexec cmd line. (By the way, I am using mpiexec rather than mpirun; do you recommend one over the other?) Will you tell me what this means for me? For example, should I always append these arguments to mpiexec for my non-test jobs as well? &nbsp; I do not know what you mean by fabric unfortunately, but I can give you some system information (see end of email). Unfortunately I am not a system admin so I do not have sudo rights. Just let me know if I can tell you something more specific though and I will get it.</span><div class=""><span style="font-size:12.8px" class=""><br class=""></span></div><div class=""><span style="font-size:12.8px" class="">Nathan, &nbsp;Thank you for your response. Unfortunately I have no idea what that means :( &nbsp;I can forward that to our cluster managers, but I do not know if that is enough information for them to understand what they might need to do to help me with this issue.</span></div><div class=""><span style="font-size:12.8px" class=""><br class=""></span></div><font face="monospace, monospace" class="">$ lscpu<br class="">Architecture: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;x86_64<br class="">CPU op-mode(s): &nbsp; &nbsp; &nbsp; &nbsp;32-bit, 64-bit<br class="">Byte Order: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Little Endian<br class="">CPU(s): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;20<br class="">On-line CPU(s) list: &nbsp; 0-19<br class="">Thread(s) per core: &nbsp; &nbsp;1<br class="">Core(s) per socket: &nbsp; &nbsp;10<br class="">Socket(s): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2<br class="">NUMA node(s): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2<br class="">Vendor ID: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; GenuineIntel<br class="">CPU family: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;6<br class="">Model: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 63<br class="">Stepping: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2<br class="">CPU MHz: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2594.159<br class="">BogoMIPS: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5187.59<br class="">Virtualization: &nbsp; &nbsp; &nbsp; &nbsp;VT-x<br class="">L1d cache: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 32K<br class="">L1i cache: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 32K<br class="">L2 cache: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;256K<br class="">L3 cache: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;25600K<br class="">NUMA node0 CPU(s): &nbsp; &nbsp; 0,2,4,6,8,10,12,14,16,18<br class="">NUMA node1 CPU(s): &nbsp; &nbsp; 1,3,5,7,9,11,13,15,17,19</font><div class=""><span style="font-size:12.8px" class=""><br class=""></span></div><div class=""><span style="font-size:12.8px" class="">Thanks,</span></div><div class=""><span style="font-size:12.8px" class="">Jason</span></div></div><div class="gmail_extra"><br clear="all" class=""><div class=""><div class="gmail_signature" data-smartmail="gmail_signature"><div dir="ltr" class="">Jason Maldonis<div class="">Research Assistant of Professor Paul Voyles</div><div class="">Materials Science Grad Student<br class=""></div><div class="">University of Wisconsin, Madison<br class="">1509 University Ave, Rm M142<br class="">Madison, WI 53706<br class=""></div><div class=""><a href="mailto:maldonis@wisc.edu" target="_blank" class="">maldonis@wisc.edu</a></div><div class="">608-295-5532</div></div></div></div>
<br class=""><div class="gmail_quote">On Tue, Jun 14, 2016 at 1:27 PM, Nathan Hjelm <span dir="ltr" class="">&lt;<a href="mailto:hjelmn@me.com" target="_blank" class="">hjelmn@me.com</a>&gt;</span> wrote:<br class=""><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">


<div class=""><div class="">That message is coming from udcm in the openib btl. It indicates some sort of failure in the connection mechanism. It can happen if the listening thread no longer exists or is taking too long to process messages.</div><div class=""><br class=""></div><div class="">-Nathan</div><div class=""><div class="h5"><div class=""><br class=""></div><div class=""><br class="">On Jun 14, 2016, at 12:20 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank" class="">rhc@open-mpi.org</a>&gt; wrote:<br class=""><br class=""></div></div></div><div class=""><blockquote type="cite" class=""><div style="word-wrap:break-word" class=""><div class=""><div class="h5">Hmm…I’m unable to replicate a problem on my machines. What fabric are you using? Does the problem go away if you add “-mca btl tcp,sm,self” to the mpirun cmd line?<div class=""><br class=""><div class=""><blockquote type="cite" class=""><div class="">On Jun 14, 2016, at 11:15 AM, Jason Maldonis &lt;<a href="mailto:maldonis@wisc.edu" target="_blank" class="">maldonis@wisc.edu</a>&gt; wrote:</div><div class=""><div dir="ltr" class=""><div class="">Hi Ralph, et. al,</div><div class=""><br class=""></div>Great, thank you for the help. I downloaded the mpi loop spawn test directly from what I think is the master repo on github: &nbsp;<a href="https://github.com/open-mpi/ompi/blob/master/orte/test/mpi/loop_spawn.c" target="_blank" class="">https://github.com/open-mpi/ompi/blob/master/orte/test/mpi/loop_spawn.c</a><div class="">I am still using the mpi code from 1.10.2, however.<br class=""><div class=""><br class=""></div><div class="">Is that test updated with the correct code? If so, I am still getting the same "too many retries sending message to 0x0184:0x00001d27, giving up" errors. I also just downloaded the June 14 nightly tarball (7.79MB) from:&nbsp;<a href="https://www.open-mpi.org/nightly/v2.x/" target="_blank" class="">https://www.open-mpi.org/nightly/v2.x/</a> and I get the same error.</div><div class=""><br class=""></div><div class="">Could you please point me to the correct code?</div><div class=""><br class=""></div><div class="">If you need me to provide more information please let me know.<br class=""><br class="">Thank you,</div><div class="">Jason</div></div></div><div class="gmail_extra"><br clear="all" class=""><div class=""><div class=""><div dir="ltr" class="">Jason Maldonis<div class="">Research Assistant of Professor Paul Voyles</div><div class="">Materials Science Grad Student<br class=""></div><div class="">University of Wisconsin, Madison<br class="">1509 University Ave, Rm M142<br class="">Madison, WI 53706<br class=""></div><div class=""><a href="mailto:maldonis@wisc.edu" target="_blank" class="">maldonis@wisc.edu</a></div><div class=""><a href="tel:608-295-5532" value="+16082955532" target="_blank" class="">608-295-5532</a></div></div></div></div><br class=""><div class="gmail_quote">On Tue, Jun 14, 2016 at 10:59 AM, Ralph Castain <span dir="ltr" class="">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank" class="">rhc@open-mpi.org</a>&gt;</span> wrote:<br class=""><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word" class="">I dug into this a bit (with some help from others) and found that the spawn code appears to be working correctly - it is the test in orte/test that is wrong. The test has been correctly updated in the 2.x and master repos, but we failed to backport it to the 1.10 series. I have done so this morning, and it will be in the upcoming 1.10.3 release (out very soon).<div class=""><div class=""><div class=""><br class=""></div><div class=""><br class=""><div class=""><blockquote type="cite" class=""><div class="">On Jun 13, 2016, at 3:53 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank" class="">rhc@open-mpi.org</a>&gt; wrote:</div><br class=""><div class=""><div style="word-wrap:break-word" class="">No, that PR has nothing to do with loop_spawn. I’ll try to take a look at the problem.<div class=""><br class=""><div class=""><blockquote type="cite" class=""><div class="">On Jun 13, 2016, at 3:47 PM, Jason Maldonis &lt;<a href="mailto:maldonis@wisc.edu" target="_blank" class="">maldonis@wisc.edu</a>&gt; wrote:</div><br class=""><div class=""><div dir="ltr" class=""><div dir="ltr" class=""><p style="margin-bottom:16px;color:#333333;font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-size:14px;line-height:22.4px;margin-top:0px!important" class="">Hello,</p><p style="margin-bottom:16px;color:#333333;font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-size:14px;line-height:22.4px;margin-top:0px!important" class=""><span style="line-height:22.4px" class="">I am using OpenMPI 1.10.2 compiled with Intel. I am trying to get the&nbsp;</span><code style="line-height:22.4px;font-family:Consolas,'Liberation Mono',Menlo,Courier,monospace;font-size:11.9px;padding:0.2em 0px;margin:0px;border-radius:3px;background-color:rgba(0,0,0,0.0392157)" class="">spawn</code><span style="line-height:22.4px" class="">&nbsp;</span><span style="line-height:22.4px" class="">functionality to work inside a for loop, but continue to get the error</span><span style="line-height:22.4px" class="">&nbsp;"</span><code style="line-height:22.4px;font-family:Consolas,'Liberation Mono',Menlo,Courier,monospace;font-size:11.9px;padding:0.2em 0px;margin:0px;border-radius:3px;background-color:rgba(0,0,0,0.0392157)" class="">too many retries sending message to &lt;addr&gt;, giving up"</code><span style="line-height:22.4px" class="">&nbsp;</span><span style="line-height:22.4px" class="">somewhere down the line in the for loop, seemingly because the processors are not being fully freed when disconnecting/finishing. I found the</span><span style="line-height:22.4px" class="">&nbsp;</span><code style="line-height:22.4px;color:#4078c0;font-family:Consolas,'Liberation Mono',Menlo,Courier,monospace;font-size:11.9px;padding:0.2em 0px;margin:0px;border-radius:3px;background-color:rgba(0,0,0,0.0392157)" class=""><a href="https://github.com/open-mpi/ompi/blob/master/orte/test/mpi/loop_spawn.c" style="color:#4078c0;text-decoration:none;background-color:transparent" target="_blank" class="">orte/test/mpi/loop_spawn.c</a>&nbsp;</code><span style="line-height:22.4px" class="">example/test, and it has the exact same problem. I also found</span><span style="line-height:22.4px" class="">&nbsp;</span><a href="https://www.open-mpi.org/community/lists/devel/2016/04/18814.php" style="line-height:22.4px;color:#4078c0;text-decoration:none;background-color:transparent" target="_blank" class="">this</a><span style="line-height:22.4px" class="">&nbsp;</span><span style="line-height:22.4px" class="">mailing list post from ~ a month and a half ago.</span><br class=""></p><p style="margin-top:0px;margin-bottom:16px;color:#333333;font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-size:14px;line-height:22.4px" class="">Is this PR (<a href="https://github.com/open-mpi/ompi/pull/1473" target="_blank" class="">https://github.com/open-mpi/ompi/pull/1473</a>) about the same issue I am having (ie the&nbsp;<code style="font-family:Consolas,'Liberation Mono',Menlo,Courier,monospace;font-size:11.9px;padding:0.2em 0px;margin:0px;border-radius:3px;background-color:rgba(0,0,0,0.0392157)" class="">loop_spawn</code>&nbsp;example not working)? If so, do you know if we can downgrade to e.g. 1.10.1 or another version? Or is there another solution to fix this bug until you get a new release out (or is one coming shortly to fix this maybe?)?</p><p style="margin-top:0px;margin-bottom:16px;color:#333333;font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-size:14px;line-height:22.4px" class="">Below is the output of the&nbsp;<code style="font-family:Consolas,'Liberation Mono',Menlo,Courier,monospace;font-size:11.9px;padding:0.2em 0px;margin:0px;border-radius:3px;background-color:rgba(0,0,0,0.0392157)" class="">loop_spawn</code>&nbsp;test on our university's cluster, which I know very little about in terms of architecture but can get information if it's helpful. The large group of people who manage this cluster are very good.</p><p style="margin-top:0px;margin-bottom:16px;color:#333333;font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-size:14px;line-height:22.4px" class="">Thanks for your time.</p><p style="margin-top:0px;margin-bottom:16px;color:#333333;font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-size:14px;line-height:22.4px" class="">Jason</p><pre style="font-family:Consolas,'Liberation Mono',Menlo,Courier,monospace;font-size:11.9px;margin-top:0px;font-stretch:normal;line-height:1.45;word-wrap:normal;padding:16px;overflow:auto;border-radius:3px;color:#333333;margin-bottom:0px!important;background-color:#f7f7f7" class=""><code style="font-family:Consolas,'Liberation Mono',Menlo,Courier,monospace;font-size:11.9px;padding:0px;margin:0px;border-radius:3px;border:0px;display:inline;overflow:visible;line-height:inherit;word-wrap:normal;background:transparent" class="">mpiexec -np 5 loop_spawn
parent*******************************
parent: Launching MPI*
parent*******************************
parent: Launching MPI*
parent*******************************
parent: Launching MPI*
parent*******************************
parent: Launching MPI*
parent*******************************
parent: Launching MPI*
parent: MPI_Comm_spawn #0 return : 0
parent: MPI_Comm_spawn #0 return : 0
parent: MPI_Comm_spawn #0 return : 0
parent: MPI_Comm_spawn #0 return : 0
parent: MPI_Comm_spawn #0 return : 0
Child: launch
Child merged rank = 5, size = 6
parent: MPI_Comm_spawn #0 rank 4, size 6
parent: MPI_Comm_spawn #0 rank 0, size 6
parent: MPI_Comm_spawn #0 rank 2, size 6
parent: MPI_Comm_spawn #0 rank 3, size 6
parent: MPI_Comm_spawn #0 rank 1, size 6
Child 329941: exiting
parent: MPI_Comm_spawn #1 return : 0
parent: MPI_Comm_spawn #1 return : 0
parent: MPI_Comm_spawn #1 return : 0
parent: MPI_Comm_spawn #1 return : 0
parent: MPI_Comm_spawn #1 return : 0
Child: launch
parent: MPI_Comm_spawn #1 rank 0, size 6
parent: MPI_Comm_spawn #1 rank 2, size 6
parent: MPI_Comm_spawn #1 rank 1, size 6
parent: MPI_Comm_spawn #1 rank 3, size 6
Child merged rank = 5, size = 6
parent: MPI_Comm_spawn #1 rank 4, size 6
Child 329945: exiting
parent: MPI_Comm_spawn #2 return : 0
parent: MPI_Comm_spawn #2 return : 0
parent: MPI_Comm_spawn #2 return : 0
parent: MPI_Comm_spawn #2 return : 0
parent: MPI_Comm_spawn #2 return : 0
Child: launch
parent: MPI_Comm_spawn #2 rank 3, size 6
parent: MPI_Comm_spawn #2 rank 0, size 6
parent: MPI_Comm_spawn #2 rank 2, size 6
Child merged rank = 5, size = 6
parent: MPI_Comm_spawn #2 rank 1, size 6
parent: MPI_Comm_spawn #2 rank 4, size 6
Child 329949: exiting
parent: MPI_Comm_spawn #3 return : 0
parent: MPI_Comm_spawn #3 return : 0
parent: MPI_Comm_spawn #3 return : 0
parent: MPI_Comm_spawn #3 return : 0
parent: MPI_Comm_spawn #3 return : 0
Child: launch
[node:port?] too many retries sending message to &lt;addr&gt;, giving up
-------------------------------------------------------
Child job 5 terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[...],0]
  Exit code:    255
--------------------------------------------------------------------------</code></pre></div></div>_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" class="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/06/29425.php" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2016/06/29425.php</a></div></blockquote></div><br class=""></div></div></div></blockquote></div><br class=""></div></div></div></div><br class="">_______________________________________________<br class=""> users mailing list<br class=""> <a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class=""> Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank" class="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class=""> Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/06/29435.php" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2016/06/29435.php</a><br class=""></blockquote></div><br class=""></div>_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" class="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/06/29438.php" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2016/06/29438.php</a></div></blockquote></div><br class=""></div></div></div><div class=""><span class=""><div class=""><div class="h5">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" target="_blank" class="">users@open-mpi.org</a><br class="">Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" class="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class=""></div></div>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/06/29439.php" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2016/06/29439.php</a></span></div></div></blockquote></div></div><br class="">_______________________________________________<br class="">
users mailing list<br class="">
<a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank" class="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/06/29440.php" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/community/lists/users/2016/06/29440.php</a><br class=""></blockquote></div><br class=""></div>
_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: https://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2016/06/29444.php</div></blockquote></div><br class=""></div></body></html>
