As Ralph says, the intent has been to maintain binary compatibility across minor releases.  As for testing, I have not done extensive testing for binary compatibility, but I have tested 1.3 &amp; 1.3.2-built versions of IMB and the NPB suite with 1.3.3 and did not have any problems.  I did this for sm, tcp &amp; openib transports.  So, in my environment, binary compatibility looks to have been maintained within the 1.3-series releases.<div>
<br></div><div>Please let us know if you run across any binary incompatibilities.<br><div><br></div><div>--Brad</div><div><br></div><div><br><br><div class="gmail_quote">On Wed, Jul 15, 2009 at 4:36 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">Hmmm...there should be messages on both the user and devel lists regarding binary compatibility at the MPI level being promised for 1.3.2 and beyond.<br>

<br>
Anyway, we did make that pledge. However, as I said, I am not sure people verified that - though I hope someone did! :-)<div><div></div><div class="h5"><br>
<br>
On Jul 15, 2009, at 3:29 PM, Jim Kress wrote:<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Did not see any other email on the list wrt this topic.<br>
<br>
Thanks for your response.<br>
<br>
Jim<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
-----Original Message-----<br>
From: <a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a><br>
[mailto:<a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a>] On Behalf Of Ralph Castain<br>
Sent: Wednesday, July 15, 2009 4:26 PM<br>
To: Open MPI Users<br>
Subject: Re: [OMPI users] [Open MPI Announce] Open MPI v1.3.3 released<br>
<br>
I believe that was the intent, per other emails on that subject.<br>
<br>
However, I am not personally aware of people who have tested<br>
it - though they may well exist.<br>
<br>
<br>
<br>
On Wed, Jul 15, 2009 at 2:18 PM, Jim Kress<br>
&lt;<a href="mailto:jimkress_58@kressworks.org" target="_blank">jimkress_58@kressworks.org</a>&gt; wrote:<br>
<br>
<br>
         &gt; Does use of 1.3.3 require recompilation of applications that<br>
        &gt; were compiled using 1.3.2?<br>
        <br>
        &gt; -----Original Message-----<br>
        &gt; From: <a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a><br>
        &gt; [mailto:<a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a>] On Behalf Of jimkress_58<br>
        &gt; Sent: Tuesday, July 14, 2009 3:05 PM<br>
        &gt; To: <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
        &gt; Subject: Re: [OMPI users] [Open MPI Announce] Open<br>
MPI v1.3.3 released<br>
        &gt;<br>
        &gt; Does use of 1.3.3 require recompilation of applications that<br>
        &gt; were compiled using 1.3.2?<br>
        &gt;<br>
        &gt; Jim<br>
        <br>
        &gt;<br>
        &gt; -----Original Message-----<br>
        &gt; From: <a href="mailto:announce-bounces@open-mpi.org" target="_blank">announce-bounces@open-mpi.org</a><br>
        &gt; [mailto:<a href="mailto:announce-bounces@open-mpi.org" target="_blank">announce-bounces@open-mpi.org</a>]<br>
        &gt; On Behalf Of Ralph Castain<br>
        &gt; Sent: Tuesday, July 14, 2009 2:11 PM<br>
        &gt; To: OpenMPI Announce<br>
        &gt; Subject: [Open MPI Announce] Open MPI v1.3.3 released<br>
        &gt;<br>
        &gt; The Open MPI Team, representing a consortium of research,<br>
        &gt; academic, and industry partners, is pleased to announce the<br>
        &gt; release of Open MPI version 1.3.3. This release is mainly a<br>
        &gt; bug fix release over the v1.3.3 release, but there are few<br>
        &gt; new features, including support for Microsoft Windows.  We<br>
        &gt; strongly recommend that all users upgrade to version 1.3.3 if<br>
        &gt; possible.<br>
        &gt;<br>
        &gt; Version 1.3.3 can be downloaded from the main Open MPI web<br>
        &gt; site or any of its mirrors (mirrors will be updating shortly).<br>
        &gt;<br>
        &gt; Here is a list of changes in v1.3.3 as compared to v1.3.2:<br>
        &gt;<br>
        &gt; - Fix a number of issues with the openib BTL<br>
(OpenFabrics) RDMA CM,<br>
        &gt;    including a memory corruption bug, a shutdown<br>
deadlock, and a route<br>
        &gt;    timeout.  Thanks to David McMillen and Hal<br>
Rosenstock for help in<br>
        &gt;    tracking down the issues.<br>
        &gt; - Change the behavior of the EXTRA_STATE parameter<br>
that is passed to<br>
        &gt;    Fortran attribute callback functions: this value<br>
is now stored<br>
        &gt;    internally in MPI -- it no longer references the<br>
original value<br>
        &gt;    passed by MPI_*_CREATE_KEYVAL.<br>
        &gt; - Allow the overriding RFC1918 and RFC3330 for the<br>
specification of<br>
        &gt;    &quot;private&quot; networks, thereby influencing Open MPI&#39;s TCP<br>
        &gt;    &quot;reachability&quot; computations.<br>
        &gt; - Improve flow control issues in the sm btl, by both<br>
tweaking the<br>
        &gt;    shared memory progression rules and by enabling the &quot;sync&quot;<br>
        &gt; collective<br>
        &gt;    to barrier every 1,000th collective.<br>
        &gt; - Various fixes for the IBM XL C/C++ v10.1 compiler.<br>
        &gt; - Allow explicit disabling of ptmalloc2 hooks at<br>
runtime (e.g., enable<br>
        &gt;    support for Debain&#39;s builtroot system).  Thanks to<br>
Manuel Prinz and<br>
        &gt;    the rest of the Debian crew for helping identify and fix<br>
        &gt; this issue.<br>
        &gt; - Various minor fixes for the I/O forwarding subsystem.<br>
        &gt; - Big endian iWARP fixes in the Open Fabrics RDMA CM support.<br>
        &gt; - Update support for various OpenFabrics devices in<br>
the openib BTL&#39;s<br>
        &gt;    .ini file.<br>
        &gt; - Fixed undefined symbol issue with Open MPI&#39;s<br>
parallel debugger<br>
        &gt;    message queue support so it can be compiled by Sun Studio<br>
        &gt; compilers.<br>
        &gt; - Update MPI_SUBVERSION to 1 in the Fortran bindings.<br>
        &gt; - Fix MPI_GRAPH_CREATE Fortran 90 binding.<br>
        &gt; - Fix MPI_GROUP_COMPARE behavior with regards to<br>
MPI_IDENT.  Thanks to<br>
        &gt;    Geoffrey Irving for identifying the problem and<br>
supplying the fix.<br>
        &gt; - Silence gcc 4.1 compiler warnings about type<br>
punning.  Thanks to<br>
        &gt;    Number Cruncher for the fix.<br>
        &gt; - Added more Valgrind and other memory-cleanup fixes.<br>
Thanks to<br>
        &gt;    various Open MPI users for help with these issues.<br>
        &gt; - Miscellaneous VampirTrace fixes.<br>
        &gt; - More fixes for openib credits in heavy-congestion scenarios.<br>
        &gt; - Slightly decrease the latency in the openib BTL in<br>
some conditions<br>
        &gt;    (add &quot;send immediate&quot; support to the openib BTL).<br>
        &gt; - Ensure to allow MPI_REQUEST_GET_STATUS to accept an<br>
        &gt;    MPI_STATUS_IGNORE parameter.  Thanks to Shaun<br>
Jackman for the bug<br>
        &gt;    report.<br>
        &gt; - Added Microsoft Windows support.  See<br>
README.WINDOWS file for<br>
        &gt;    details.<br>
        &gt;<br>
        <br>
        &gt; _______________________________________________<br>
        &gt; announce mailing list<br>
        &gt; <a href="mailto:announce@open-mpi.org" target="_blank">announce@open-mpi.org</a><br>
        &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/announce" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/announce</a><br>
        &gt;<br>
        &gt; _______________________________________________<br>
        &gt; users mailing list<br>
        &gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
        &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
        <br>
        _______________________________________________<br>
        users mailing list<br>
        <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
        <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
        <br>
<br>
<br>
<br>
</blockquote>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div></div>

