<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">MPI_init() is actually called when import MPI module from MPi package...<div><br></div><div><br><div><div>On Sep 25, 2012, at 5:17 PM, Ralph Castain wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">You forgot to call MPI_Init at the beginning of your program.<div><br><div><div>On Sep 25, 2012, at 2:08 PM, Mariana Vargas Magana &lt;<a href="mailto:mmarianav@yahoo.com.mx">mmarianav@yahoo.com.mx</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><div><div><div>Hi</div><div>I think I'am not understanding what you said , here is the hello.py and next the command mpirun…</div><div><br></div><div>Thanks!</div><div><br></div><div>#!/usr/bin/env python</div><div>"""</div></div><div>Parallel Hello World</div><div>"""</div><div><br></div><div>from mpi4py import MPI</div><div>import sys</div><div><br></div><div>size = MPI.COMM_WORLD.Get_size()</div><div>rank = MPI.COMM_WORLD.Get_rank()</div><div>name = MPI.Get_processor_name()</div><div><br></div><div>sys.stdout.write(</div><div>&nbsp; &nbsp; "Hello, World! I am process %d of %d on %s.\n"</div><div>&nbsp; &nbsp; % (rank, size, name))</div></div><div><br></div><div>&nbsp;~/bin/mpirun -np 70 python2.7 helloworld.py&nbsp;</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div>Hello, World! I am process 0 of 1 on ferrari.</div><div><br></div><div><div>On Sep 25, 2012, at 4:46 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">The usual reason for this is that you aren't launching these processes correctly. How are you starting your job? Are you using mpirun?<div><br></div><div><br><div><div>On Sep 25, 2012, at 1:43 PM, mariana Vargas &lt;<a href="mailto:mmarianav@yahoo.com.mx">mmarianav@yahoo.com.mx</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><div>Hi</div><div><br></div><div>I fact I found what is the origin of this problem and it is because all processes have rank 0, so I tested and in effect even when I send the clasical Hello.py give the same, how can I solved this?? Do I &nbsp;re installed every again???</div><div><br></div><div>Help please...</div><div><br></div><div>Mariana</div><div><br></div><div><br></div><br><div><div>On Sep 24, 2012, at 9:13 PM, Mariana Vargas Magana wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><div><br></div><div><br></div><div>Yes you are right this is what it says but if fact the weird thing is that not all times the error message appears….I send to 20 nodes and only one gives this message, is this normal…</div><div><br></div><div><br></div><div><br></div><br><div><div>On Sep 24, 2012, at 8:00 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">Well, as it says, your processes called MPI_Init, but at least one of them exited without calling MPI_Finalize. That violates the MPI rules and we therefore terminate the remaining processes.<div><br></div><div>Check your code and see how/why you are doing that - you probably have a code path whereby a process exits without calling finalize.</div><div><br></div><div><br><div><div>On Sep 24, 2012, at 4:37 PM, mariana Vargas &lt;<a href="mailto:mmarianav@yahoo.com.mx">mmarianav@yahoo.com.mx</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><br><div><br><blockquote type="cite"></blockquote><font class="Apple-style-span" color="#144FAE"><br></font><blockquote type="cite"></blockquote>Hi all<br><blockquote type="cite"></blockquote><font class="Apple-style-span" color="#144FAE"><br></font><blockquote type="cite"></blockquote>I get this error when I run a paralelized python code in a cluster, could anyone give me an idea of what is happening? I'am new in this Thanks...<br><blockquote type="cite"></blockquote><font class="Apple-style-span" color="#144FAE"><br></font><blockquote type="cite"></blockquote>mpirun has exited due to process rank 2 with PID 10259 on<br><blockquote type="cite"></blockquote>node f01 exiting improperly. There are two reasons this could occur:<br><blockquote type="cite"></blockquote><font class="Apple-style-span" color="#144FAE"><br></font><blockquote type="cite"></blockquote>1. this process did not call "init" before exiting, but others in<br><blockquote type="cite"></blockquote>the job did. This can cause a job to hang indefinitely while it waits<br><blockquote type="cite"></blockquote>for all processes to call "init". By rule, if one process calls "init",<br><blockquote type="cite"></blockquote>then ALL processes must call "init" prior to termination.<br><blockquote type="cite"></blockquote><font class="Apple-style-span" color="#144FAE"><br></font><blockquote type="cite"></blockquote>2. this process called "init", but exited without calling "finalize".<br><blockquote type="cite"></blockquote>By rule, all processes that call "init" MUST call "finalize" prior to<br><blockquote type="cite"></blockquote>exiting or it will be considered an "abnormal termination"<br><blockquote type="cite"></blockquote><font class="Apple-style-span" color="#144FAE"><br></font><blockquote type="cite"></blockquote>This may have caused other processes in the application to be<br><blockquote type="cite"></blockquote>terminated by signals sent by mpirun (as reported here).<br><blockquote type="cite"></blockquote><font class="Apple-style-span" color="#144FAE"><br></font><blockquote type="cite"></blockquote>Thanks!!<br><blockquote type="cite"></blockquote><font class="Apple-style-span" color="#144FAE"><br></font><blockquote type="cite"></blockquote><font class="Apple-style-span" color="#144FAE"><br></font><blockquote type="cite"></blockquote><font class="Apple-style-span" color="#144FAE"><br></font><blockquote type="cite"><br>Dr. Mariana Vargas Magana<br>Astroparticule et Cosmologie - Bureau 409B<br>PHD student- Université Denis Diderot-Paris 7<br>10, rue Alice Domon et Léonie Duquet<br>75205 Paris Cedex - France<br>Tel. +33 (0)1 57 27 70 32<br>Fax. +33 (0)1 57 27 60 71<br><a href="mailto:mariana@apc.univ-paris7.fr">mariana@apc.univ-paris7.fr</a><br></blockquote></div><br></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote></div><br></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote></div><br></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote></div><br></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote></div><br></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote></div><br></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote></div><br></div></div>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
