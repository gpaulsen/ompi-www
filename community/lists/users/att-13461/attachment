<html>
<head>
<style><!--
.hmmessage P
{
margin:0px;
padding:0px
}
body.hmmessage
{
font-size: 10pt;
font-family:Verdana
}
--></style>
</head>
<body class='hmmessage'>
Thanks for all your replies. <br><br>I want to do master-worker asynchronous communication. <br><br>The master needs to distribute tasks to workers and then collect results from them. <br><br>master : <br><br>world.irecv(resultSourceRank, upStreamTaskTag, myResultTaskPackage[iRank][taskCounterT3]);<br><br>I got this error "MPI_ERR_TRUNCATE" , because I declared " TaskPackage myResultTaskPackage. "<br><br>It seems that the 2-dimension array cannot be used to receive my defined <br>class package from worker, who sends a TaskPackage to master. <br><br>So, I changed it to an int 2-d array to get the result, it works well. <br><br>But, I still want to find out how to store the result in a data structure with the type TaskPackage because <br>int type data can only be used to carry integers. Too limited.<br><br>What I want to do is: <br><br>The master can store the results from each worker and then combine them together <br>to form the final result after collecting all results from workers. <br><br>But, if the master has number of tasks that cannot be divided evenly by worker numbers, <br>each worker may have different number of tasks. <br><br>If we have 11 tasks and 3 workers.<br><br>aveTaskNumPerNode = (11 - 11%3) /3 = 3<br>leftTaskNum = 11%3 =2 = Z<br><br>the master distributes each of left tasks from worker 1 to work Z (Z &lt; totalNumWorkers).<br><br>For example, worker 1: 4 tasks, worker 2: 4 task, worker 3: 3 tasks.<br><br>The master tries to distribute tasks evenly so that the difference between workloads of <br>each worker is minimized. <br><br>I am going to use vector's vector to do the dynamic data storage. <br><br>The 2-dimensional data-structure that can store results from workers. <br><br>Each row element of the data-structure has different columns. <br><br>It can be indexed by iterator so that I can find the a specified number worker task result <br>by searching the data strucutre. <br><br>For example, <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; column&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; column <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2<br>&nbsp;row 1&nbsp;&nbsp; (worker1.task1)&nbsp;&nbsp;&nbsp; (worker1.task4)&nbsp;&nbsp;&nbsp;&nbsp; <br>&nbsp;row 2&nbsp;&nbsp; (worker2.task2)&nbsp;&nbsp;&nbsp;&nbsp; (worker1.task5)&nbsp;&nbsp; <br>&nbsp;row 3&nbsp;&nbsp; (worker3.task3) <br><br>the data strucutre should remember the location of work ID and the task ID.<br>So that the master can know which task comes from which worker. <br><br>Any help or comment are appreciated. <br><br>thanks<br><br>Jack <br><br>June 30&nbsp;&nbsp; 2010<br><br><br><br>&gt; Date: Thu, 1 Jul 2010 11:44:19 -0400<br>&gt; From: gus@ldeo.columbia.edu<br>&gt; To: users@open-mpi.org<br>&gt; Subject: Re: [OMPI users] Open MPI, Segmentation fault<br>&gt; <br>&gt; Hello Jack, list<br>&gt; <br>&gt; As others mentioned, this may be a problem with dynamic<br>&gt; memory allocation.<br>&gt; It could also be a violation of statically allocated memory,<br>&gt; I guess.<br>&gt; <br>&gt; You say:<br>&gt; <br>&gt; &gt; My program can run well for 1,2,10 processors, but fail when the<br>&gt; &gt; number of tasks cannot<br>&gt; &gt; be divided evenly by number of processes.<br>&gt; <br>&gt; Often times, when the division of the number of "tasks"<br>&gt; (or the global problem size) by the number of "processors" is not even, <br>&gt; one processor gets a lighter/heavier workload then the others,<br>&gt; it also allocates  less/more memory than the others,<br>&gt; and it accesses smaller/larger arrays than the others.<br>&gt; <br>&gt; In general integer division and remainder/module calculations<br>&gt; are used to control memory allocation, the array sizes, etc,<br>&gt; on different processors.<br>&gt; These formulas tend to use the MPI communicator size<br>&gt; (i.e., effectively the number of processors if you are using <br>&gt; MPI_COMM_WORLD) to split the workload across the processors.<br>&gt; <br>&gt; I would search for the lines of code where those calculations are done, <br>&gt; and where the arrays are allocated and accessed,<br>&gt; to make sure the algorithm works both when<br>&gt; they are of the same size<br>&gt; (even workload across the processors),<br>&gt; as when they are of different sizes<br>&gt; (uneven workload across the processors).<br>&gt; You may be violating memory access by a few bytes only, due to a small<br>&gt; mistake in one of those integer division / remainder/module formulas,<br>&gt; perhaps where an array index upper or lower bound is calculated.<br>&gt; It happened to me before, probably to others too.<br>&gt; <br>&gt; This type of code inspection can be done without a debugger,<br>&gt; or before you get to the debugger phase.<br>&gt; <br>&gt; I hope this helps,<br>&gt; Gus Correa<br>&gt; ---------------------------------------------------------------------<br>&gt; Gustavo Correa<br>&gt; Lamont-Doherty Earth Observatory - Columbia University<br>&gt; Palisades, NY, 10964-8000 - USA<br>&gt; ---------------------------------------------------------------------<br>&gt; <br>&gt; &gt; Jeff Squyres wrote:<br>&gt; &gt; Also see http://www.open-mpi.org/faq/?category=debugging.<br>&gt; &gt; <br>&gt; &gt; On Jul 1, 2010, at 3:17 AM, Asad Ali wrote:<br>&gt; &gt; <br>&gt; &gt;&gt; Hi Jack,<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Debugging OpenMPI with traditional debuggers is a pain.<br>&gt; &gt;&gt; &gt;From your error message it sounds that you have some memory allocation problem. Do you use dynamic memory allocation (allocate and then free)?<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; I use display (printf()) command with MPIrank command. It tells me which thread is giving segmentation fault.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Cheers,<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Asad<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; On Thu, Jul 1, 2010 at 4:13 PM, Jack Bryan &lt;dtustudy68@hotmail.com&gt; wrote:<br>&gt; &gt;&gt; thanks<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; I am not familiar with OpenMPI. <br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Would you please help me with how to ask openMPI to show where the fault occurs ?<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; GNU debuger ?<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Any help is appreciated. <br>&gt; &gt;&gt;<br>&gt; &gt;&gt; thanks!!!<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Jack <br>&gt; &gt;&gt;<br>&gt; &gt;&gt; June 30  2010<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Date: Wed, 30 Jun 2010 16:13:09 -0400<br>&gt; &gt;&gt; From: amjad11@gmail.com<br>&gt; &gt;&gt; To: users@open-mpi.org<br>&gt; &gt;&gt; Subject: Re: [OMPI users] Open MPI, Segmentation fault<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Based on my experiences, I would FULLY endorse (100% agree with) David Zhang.<br>&gt; &gt;&gt; It is usually a coding or typo mistake.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; At first, Ensure that array sizes and dimension are correct.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; I experience that if openmpi is compiled with gnu compilers (not with Intel) then it also point outs the subroutine exactly in which the fault occur. have a try.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; best,<br>&gt; &gt;&gt; AA<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;   <br>&gt; &gt;&gt;<br>&gt; &gt;&gt; On Wed, Jun 30, 2010 at 12:43 PM, David Zhang &lt;solarbikedz@gmail.com&gt; wrote:<br>&gt; &gt;&gt; When I got segmentation faults, it has always been my coding mistakes.  Perhaps your code is not robust against number of processes not divisible by 2?<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; On Wed, Jun 30, 2010 at 8:47 AM, Jack Bryan &lt;dtustudy68@hotmail.com&gt; wrote:<br>&gt; &gt;&gt; Dear All,<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; I am using Open MPI, I got the error: <br>&gt; &gt;&gt;<br>&gt; &gt;&gt; n337:37664] *** Process received signal ***<br>&gt; &gt;&gt; [n337:37664] Signal: Segmentation fault (11)<br>&gt; &gt;&gt; [n337:37664] Signal code: Address not mapped (1)<br>&gt; &gt;&gt; [n337:37664] Failing at address: 0x7fffcfe90000<br>&gt; &gt;&gt; [n337:37664] [ 0] /lib64/libpthread.so.0 [0x3c50e0e4c0]<br>&gt; &gt;&gt; [n337:37664] [ 1] /lustre/home/rhascheduler/RhaScheduler-0.4.1.1/mytest/nmn2 [0x414ed7]<br>&gt; &gt;&gt; [n337:37664] [ 2] /lib64/libc.so.6(__libc_start_main+0xf4) [0x3c5021d974]<br>&gt; &gt;&gt; [n337:37664] [ 3] /lustre/home/rhascheduler/RhaScheduler-0.4.1.1/mytest/nmn2(__gxx_personality_v0+0x1f1) [0x412139]<br>&gt; &gt;&gt; [n337:37664] *** End of error message ***<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; After searching answers, it seems that some functions fail. <br>&gt; &gt;&gt;  <br>&gt; &gt;&gt; My program can run well for 1,2,10 processors, but fail when the number of tasks cannot<br>&gt; &gt;&gt; be divided evenly by number of processes. <br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Any help is appreciated. <br>&gt; &gt;&gt;<br>&gt; &gt;&gt; thanks<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Jack<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; June 30  2010<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; The New Busy think 9 to 5 is a cute idea. Combine multiple calendars with Hotmail. Get busy.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; _______________________________________________<br>&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; users@open-mpi.org<br>&gt; &gt;&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; -- <br>&gt; &gt;&gt; David Zhang<br>&gt; &gt;&gt; University of California, San Diego<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; _______________________________________________<br>&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; users@open-mpi.org<br>&gt; &gt;&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Hotmail has tools for the New Busy. Search, chat and e-mail from your inbox. Learn more.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; _______________________________________________<br>&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; users@open-mpi.org<br>&gt; &gt;&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; -- <br>&gt; &gt;&gt; "Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write." - H.G. Wells<br>&gt; &gt;&gt; _______________________________________________<br>&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; users@open-mpi.org<br>&gt; &gt;&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt; &gt; <br>&gt; &gt; <br>&gt; <br>&gt; <br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; users@open-mpi.org<br>&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br> 		 	   		  <br /><hr />The New Busy is not the too busy. Combine all your e-mail accounts with Hotmail. <a href='http://www.windowslive.com/campaign/thenewbusy?tile=multiaccount&ocid=PID28326::T:WLMTAGL:ON:WL:en-US:WM_HMP:042010_4' target='_new'>Get busy.</a></body>
</html>
