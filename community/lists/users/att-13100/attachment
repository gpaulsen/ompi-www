Hello Terry,<br><br>Thanks for your answer.<br><br><div class="gmail_quote">2010/5/20 Terry Dontje <span dir="ltr">&lt;<a href="mailto:terry.dontje@oracle.com">terry.dontje@oracle.com</a>&gt;</span><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">



  

<div bgcolor="#ffffff" text="#000000"><div class="im">
Olivier Riff wrote:
<blockquote type="cite">Hello,<br>
  <br>
I assume this question has been already discussed many times, but I can
not find on Internet a solution to my problem.<br>
It is about buffer size limit of MPI_Send and MPI_Recv with
heterogeneous system (32 bit laptop / 64 bit cluster). <br>
My configuration is :<br>
open mpi 1.4, configured with: --without-openib --enable-heterogeneous
--enable-mpi-threads<br>
Program is launched a laptop (32 bit Mandriva 2008) which distributes
tasks to do to a cluster of 70 processors  (64 bit RedHat Entreprise
distribution):<br>
I have to send various buffer size from few bytes till 30Mo.<br>
  <br>
</blockquote></div>
You really want to get your program running without the tcp_eager_limit
set if you want a better usage of memory.  I believe the crash has
something to do with the rendezvous protocol in OMPI.  Have you
narrowed this failure down to a simple MPI program?  Also I noticed
that you&#39;re configuring with --enable-mpi-threads, have you tried
configuring without that option?<div class="im"><br></div></div></blockquote><div><br>-&gt; No, unfortunately I did not narrowed this behaviour to a simple MPI program. I think I will have to do it if I do not find a solution in the next days.<br>
I will also make the test without the --enable-mpi-threads configuration.<br> </div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div bgcolor="#ffffff" text="#000000">
<div class="im">
<blockquote type="cite">I tested following commands:<br>
1) mpirun -v -machinefile machinefile.txt MyMPIProgram <br>
-&gt; crash on client side ( 64 bit RedHat Entreprise ) when sent
buffer size &gt; 65536.<br>
2) mpirun --mca btl_tcp_eager_limit 30000000 -v -machinefile
machinefile.txt MyMPIProgram <br>
-&gt; works but has the effect of generating gigantic memory
consumption on 32 bit machine side after MPI_Recv. Memory consumption
goes from 800Mo to 2,1Go after receiving about 20ko from each 70
clients ( a total of about 1.4 Mo ).  This makes my program crash later
because I have no more memory to allocate new structures. I read in a
openmpi forum thread that setting btl_tcp_eager_limit to a huge value
explains this huge memory consumption when a message sent does not have
a preposted ready recv. Also after all messages have been received and
there is no more traffic activity : the memory consumed remains at
2.1go... and I do not understand why.<br>
</blockquote></div>
Are the 70 clients all on different nodes?  I am curious if the 2.1GB
is due to the SM BTL or possibly a leak in the TCP BTL.<br></div></blockquote><div><br>No, 70 clients are only on 9 nodes. In fact it is 72 clients: they are nine 8-processor machines.<br>The 2.1Gb memory consumption appears when I sequentially try to read the result on each 72 clients (for loop from 1 to 72 calling MPI_Recv). I assume that many clients have already sent the result whereas the server has not called the MPI_Rec for the corresponding rank yet.<br>
 </div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div bgcolor="#ffffff" text="#000000">
<blockquote type="cite"><div class="im"><br>
What is the best way to do in order to have a working program which
also has a small memory consumption (the speed performance can be
lower) ?<br>
I tried to play with mca paramters btl_tcp_sndbuf and mca
btl_tcp_rcvbuf, but without success.<br>
  <br>
Thanks in advance for you help.<br>
  <br>
Best regards,<br>
  <br>
Olivier<br>
  </div><pre><hr size="4" width="90%">
_______________________________________________
users mailing list
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></pre>
</blockquote>
<br>
<br>
<div>-- <br>


<img src="" alt="Oracle"><br>
<div>
<div>
<div>
<div>Terry D. Dontje | Principal Software Engineer<br>
<div><font color="#666666" face="Verdana" size="2">Developer
Tools
Engineering | +1.650.633.7054<br>
</font>
<font color="#ff0000" face="Verdana" size="2">Oracle
</font><font color="#666666" face="Verdana" size="2"><b> - Performance
Technologies</b></font><br>
<font color="#666666" face="Verdana" size="2">
95 Network Drive, Burlington, MA 01803<br>
Email <a href="mailto:terry.dontje@oracle.com" target="_blank">terry.dontje@oracle.com</a><br>
</font><br>
</div>
</div>
</div>
</div>
</div>
</div>
</div>

<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

