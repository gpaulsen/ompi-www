<p>Thanks Dick, Eugene.  That&#39;s what I figured.  I was just hoping there might be some more obscure MPI functions that might do what I want.  I&#39;ll go ahead and write my own yielding wrapper on irecv.</p>
<p>Thanks again,<br>
  Brian</p>
<p>sent from mobile phone</p>
<p><blockquote type="cite">On Oct 20, 2010 5:24 AM, &quot;Richard Treumann&quot; &lt;<a href="mailto:treumann@us.ibm.com">treumann@us.ibm.com</a>&gt; wrote:<br><br>
<br><font size="2" face="sans-serif">Brian </font>
<br>
<br><font size="2" face="sans-serif">Most HPC applications are run with one
processor and one working thread per MPI process. In this case, the node
is not being used for other work so if the MPI process does release a processor,
there is nothing else important for it to do anyway.</font>
<br>
<br><font size="2" face="sans-serif">In these applications, the blocking
MPI call (like MPI_Recv) is issued only when there is no more computation
that can be done until the MPI_Recv returns with with the message. </font>
<br>
<br><font size="2" face="sans-serif">Unless your application has other threads
that can make valuable use of the processor freed up by making MPI_Recv
do yields, the polling &quot;overhead&quot; is probably not something to
worry about.</font>
<br>
<br><font size="2" face="sans-serif">If you do have other work available
for the freed processor to turn to, the &quot;problem&quot; may be worth
solving. MPI implementations, in general, default to a polling approach
because it makes the MPI_Recv faster and if there is nothing else important
for the processor to turn to, a fast MPI_Recv is what matters.</font>
<br>
<br>
<br><font size="2" face="sans-serif">Dick Treumann  -  MPI Team
          <br>
IBM Systems &amp; Technology Group<br>
Dept X2ZA / MS P963 -- 2455 South Road -- Poughkeepsie, NY 12601<br>
Tele (845) 433-7846         Fax (845) 433-8363<br>
</font>
<br>
<br>
<br>
<table width="100%">
<tr valign="top">
<td><font size="1" color="#5f5f5f" face="sans-serif">From:</font>
<td><font size="1" face="sans-serif">Brian Budge &lt;<a href="mailto:brian.budge@gmail.com" target="_blank">brian.budge@gmail.com</a>&gt;</font>
<tr valign="top">
<td><font size="1" color="#5f5f5f" face="sans-serif">To:</font>
<td><font size="1" face="sans-serif">Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;</font>
<tr valign="top">
<td><font size="1" color="#5f5f5f" face="sans-serif">Date:</font>
<td><font size="1" face="sans-serif">10/19/2010 09:47 PM</font>
<tr valign="top">
<td><font size="1" color="#5f5f5f" face="sans-serif">Subject:</font>
<td><font size="1" face="sans-serif">[OMPI users] busy wait in MPI_Recv</font>
<tr valign="top">
<td><font size="1" color="#5f5f5f" face="sans-serif">Sent by:</font>
<td><font size="1" face="sans-serif"><a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a></font></td></td></tr></td></td></tr></td></td></tr></td></td></tr></td></td></tr></table>
<br>
<hr noshade>
<br>
<br>
<br><tt><font size="2"><p><font color="#500050">Hi all -<br><br>I just ran a small test to find out the overhead of an MPI_Recv call<br>when no communication...</font></p><p><font color="#500050">_______________________________________________<br>
users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a></font></p></font></tt><p><font color="#500050"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br></font></p><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></p>

