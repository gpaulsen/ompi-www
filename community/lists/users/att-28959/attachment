<font size=2 face="sans-serif">Hi Ralph,</font><br><br><font size=2 face="sans-serif">Thank you for looking into this!</font><br><br><font size=2 face="sans-serif">The job</font><br><font size=2 face="Courier New">#BSUB -J &quot;task_geometry&quot;<br>#BSUB -n 9<br>#BSUB -R &quot;span[ptile=3]&quot;<br>#BSUB -m &quot;p10a30 p10a33 p10a35 p10a55 p10a58&quot;<br>#BSUB -R &quot;affinity[core]&quot;<br>#BSUB -e &quot;task_geometry.stderr.%J&quot;<br>#BSUB -o &quot;task_geometry.stdout.%J&quot;<br>#BSUB -q &quot;normal&quot;<br>#BSUB -M &quot;800&quot;<br>#BSUB -R &quot;rusage[mem=800]&quot;<br>#BSUB -x</font><font size=3><br></font><font size=2 face="Courier New"><br>export PATH=/usr/local/OpenMPI/1.10.2/bin:${PATH}<br>export LD_LIBRARY_PATH=/usr/local/OpenMPI/1.10.2/lib:${PATH}</font><font size=3><br></font><font size=2 face="Courier New"><br>export LSB_PJL_TASK_GEOMETRY=&quot;{(5)(4,3)(2,1,0)}&quot;</font><font size=3><br></font><font size=2 face="Courier New"><br>mpirun /gpfs/gpfs_stage1/parpia/OpenMPI_tests/reporter/bin/reporter_MPI</font><br><font size=2 face="sans-serif">fails with the message</font><br><font size=2 face="Courier New">--------------------------------------------------------------------------</font><br><font size=2 face="Courier New">A request was made to bind to that
would result in binding more</font><br><font size=2 face="Courier New">processes than cpus on a resource:</font><br><br><font size=2 face="Courier New">&nbsp; &nbsp;Bind to: &nbsp; &nbsp;
CORE</font><br><font size=2 face="Courier New">&nbsp; &nbsp;Node: &nbsp; &nbsp; &nbsp;
&nbsp;p10a55</font><br><font size=2 face="Courier New">&nbsp; &nbsp;#processes: &nbsp;2</font><br><font size=2 face="Courier New">&nbsp; &nbsp;#cpus: &nbsp; &nbsp; &nbsp;
1</font><br><br><font size=2 face="Courier New">You can override this protection by
adding the &quot;overload-allowed&quot;</font><br><font size=2 face="Courier New">option to your binding directive.</font><br><font size=2 face="Courier New">--------------------------------------------------------------------------</font><br><font size=2 face="sans-serif">(Please see the first set of LSF output
files in the original message.) &nbsp;I did not expect this failure: I
haven't asked for more than one MPI process per core. &nbsp;In an attempt
to work around this failure, I added the option </font><font size=2 face="Courier New">-bind-to
core:overload-allowed</font><font size=2 face="sans-serif">, and this led
to 20 MPI processes (there are 20 cores on each host in this cluster) being
started on just one of the hosts. &nbsp;That is, neither job did what I
expected.</font><br><br><font size=2 face="sans-serif">I will try to put you in touch with
someone in LSF development immediately.</font><br><br><font size=2 face="sans-serif">Regards,</font><br><br><font size=2 face="sans-serif">Farid Parpia &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;IBM Corporation: 710-2-RF28, 2455 South Road, Poughkeepsie, NY 12601,
USA; Telephone: (845) 433-8420 = Tie Line 293-8420</font><br><font size=1 color=#800080 face="sans-serif">----- Forwarded by Farid
Parpia/Poughkeepsie/IBM on 04/18/2016 06:56 PM -----</font><br><br><font size=1 color=#5f5f5f face="sans-serif">From: &nbsp; &nbsp; &nbsp;
&nbsp;</font><font size=1 face="sans-serif">Ralph Castain &lt;rhc@open-mpi.org&gt;</font><br><font size=1 color=#5f5f5f face="sans-serif">To: &nbsp; &nbsp; &nbsp;
&nbsp;</font><font size=1 face="sans-serif">Open MPI Users &lt;users@open-mpi.org&gt;</font><br><font size=1 color=#5f5f5f face="sans-serif">Date: &nbsp; &nbsp; &nbsp;
&nbsp;</font><font size=1 face="sans-serif">04/18/2016 06:53 PM</font><br><font size=1 color=#5f5f5f face="sans-serif">Subject: &nbsp; &nbsp;
&nbsp; &nbsp;</font><font size=1 face="sans-serif">Re: [OMPI users]
LSF's LSB_PJL_TASK_GEOMETRY + OpenMPI 1.10.2</font><br><font size=1 color=#5f5f5f face="sans-serif">Sent by: &nbsp; &nbsp;
&nbsp; &nbsp;</font><font size=1 face="sans-serif">&quot;users&quot;
&lt;users-bounces@open-mpi.org&gt;</font><br><hr noshade><br><br><br><font size=3>Hi Farid</font><br><br><font size=3>I’m not sure I understand what you are asking here. If
your point is that OMPI isn’t placing and binding procs per the LSF directives,
then you are quite correct. The LSF folks never provided that level of
integration, nor the info by which we might have derived it (e.g., how
the pattern is communicated).</font><br><br><font size=3>If someone from IBM would like to provide that code, we’d
be happy to help answer questions as to how to perform the integration.</font><br><br><br><font size=3>On Apr 18, 2016, at 10:13 AM, Farid Parpia &lt;</font><a href=mailto:parpia@us.ibm.com><font size=3 color=blue><u>parpia@us.ibm.com</u></font></a><font size=3>&gt;
wrote:</font><br><br><font size=2 face="sans-serif">Greetings!</font><font size=3><br></font><font size=2 face="sans-serif"><br>The following batch script will successfully demo the use of LSF's task
geometry feature using IBM Parallel Environment:</font><font size=2 face="Courier New"><br>#BUB -J &quot;task_geometry&quot;<br>#BSUB -n 9<br>#BSUB -R &quot;span[ptile=3]&quot;<br>#BSUB -network &quot;type=sn_single:mode=us&quot;<br>#BSUB -R &quot;affinity[core]&quot;<br>#BSUB -e &quot;task_geometry.stderr.%J&quot;<br>#BSUB -o &quot;task_geometry.stdout.%J&quot;<br>#BSUB -q &quot;normal&quot;<br>#BSUB -M &quot;800&quot;<br>#BSUB -R &quot;rusage[mem=800]&quot;<br>#BSUB -x</font><font size=3><br></font><font size=2 face="Courier New"><br>export LSB_PJL_TASK_GEOMETRY=&quot;{(5)(4,3)(2,1,0)}&quot;</font><font size=3><br></font><font size=2 face="Courier New"><br>ldd /gpfs/gpfs_stage1/parpia/PE_tests/reporter/bin/reporter_MPI</font><font size=3><br></font><font size=2 face="Courier New"><br>/gpfs/gpfs_stage1/parpia/PE_tests/reporter/bin/reporter_MPI</font><font size=2 face="sans-serif"><br>The </font><font size=2 face="Courier New">reporter_MPI</font><font size=2 face="sans-serif">utility
simply reports the hostname and affinitization for each MPI process, and
is what I use to verify that the job is distributed to allocated nodes
and on them with the affinitization expected. &nbsp;Typical output is<br> &nbsp; &nbsp; &nbsp; &nbsp;, </font><font size=3><br></font><font size=2 face="sans-serif"><br>To adapt the above batch script to use OpenMPI, I modify it to</font><font size=2 face="Courier New"><br>#BSUB -J &quot;task_geometry&quot;<br>#BSUB -n 9<br>#BSUB -R &quot;span[ptile=3]&quot;<br>#BSUB -m &quot;p10a30 p10a33 p10a35 p10a55 p10a58&quot;<br>#BSUB -R &quot;affinity[core]&quot;<br>#BSUB -e &quot;task_geometry.stderr.%J&quot;<br>#BSUB -o &quot;task_geometry.stdout.%J&quot;<br>#BSUB -q &quot;normal&quot;<br>#BSUB -M &quot;800&quot;<br>#BSUB -R &quot;rusage[mem=800]&quot;<br>#BSUB -x</font><font size=3><br></font><font size=2 face="Courier New"><br>export PATH=/usr/local/OpenMPI/1.10.2/bin:${PATH}<br>export LD_LIBRARY_PATH=/usr/local/OpenMPI/1.10.2/lib:${PATH}</font><font size=3><br></font><font size=2 face="Courier New"><br>export LSB_PJL_TASK_GEOMETRY=&quot;{(5)(4,3)(2,1,0)}&quot;</font><font size=3><br></font><font size=2 face="Courier New"><br>echo &quot;=== LSB_DJOB_HOSTFILE ===&quot;<br>cat ${LSB_DJOB_HOSTFILE}<br>echo &quot;=== LSB_AFFINITY_HOSTFILE ===&quot;<br>cat ${LSB_AFFINITY_HOSTFILE}<br>echo &quot;=== LSB_DJOB_RANKFILE ===&quot;<br>cat ${LSB_DJOB_RANKFILE}<br>echo &quot;=========================&quot;</font><font size=3><br></font><font size=2 face="Courier New"><br>ldd /gpfs/gpfs_stage1/parpia/OpenMPI_tests/reporter/bin/reporter_MPI</font><font size=3><br></font><font size=2 face="Courier New"><br>mpirun /gpfs/gpfs_stage1/parpia/OpenMPI_tests/reporter/bin/reporter_MPI</font><font size=2 face="sans-serif"><br>There are additional lines of scripting that I have inserted to help with
debugging this failing job. &nbsp;Here are the output files from the job:<br> &nbsp; &nbsp; &nbsp; &nbsp;, <br>If I change the last line of the immediately above job script to<br> &nbsp; &nbsp; &nbsp; &nbsp;</font><font size=2 face="Courier New">mpirun
-bind-to core:overload-allowed /gpfs/gpfs_stage1/parpia/OpenMPI_tests/reporter/bin/reporter_MPI</font><font size=2 face="sans-serif"><br>the job runs through, but the host selection and affinization is completely
wrong (you can extract the relevant information with </font><font size=2 face="Courier New">grep
&quot;can be sched&quot; *.stdout.* | sort -n -k 9</font><font size=2 face="sans-serif">):<br> &nbsp; &nbsp; &nbsp; &nbsp;, <br>OpenMPI 1.10.2 was built using this script:<br> &nbsp; &nbsp; &nbsp; &nbsp;<br>It was installed with</font><font size=2 face="Courier New"><br>make install</font><font size=2 face="sans-serif"><br>executed from the top if the build tree. &nbsp;Here<br> &nbsp; &nbsp; &nbsp; &nbsp;<br>is the output of</font><font size=2 face="Courier New"><br>ompi_info --all</font><font size=3><br></font><font size=2 face="sans-serif"><br>Regards,</font><font size=3><br></font><font size=2 face="sans-serif"><br>Farid Parpia &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;IBM Corporation: 710-2-RF28,
2455 South Road, Poughkeepsie, NY 12601, USA; Telephone: (845) 433-8420
= Tie Line 293-8420</font><font size=3><br>&lt;task_geometry.stdout.43915.gz&gt;&lt;task_geometry.stderr.43915.gz&gt;&lt;task_geometry.stderr.43918.gz&gt;&lt;task_geometry.stdout.43918.gz&gt;&lt;task_geometry.stderr.43953.gz&gt;&lt;task_geometry.stdout.43953.gz&gt;&lt;build_OpenMPI.sh&gt;&lt;ompi_info--all.gz&gt;_______________________________________________<br>users mailing list</font><font size=3 color=blue><u><br></u></font><a href="mailto:users@open-mpi.org"><font size=3 color=blue><u>users@open-mpi.org</u></font></a><font size=3><br>Subscription: </font><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><font size=3>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></a><font size=3><br>Link to this post: </font><a href="http://www.open-mpi.org/community/lists/users/2016/04/28955.php"><font size=3>http://www.open-mpi.org/community/lists/users/2016/04/28955.php</font></a><br><tt><font size=2>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>Subscription: </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>Link to this post: </font></tt><a href="http://www.open-mpi.org/community/lists/users/2016/04/28958.php"><tt><font size=2>http://www.open-mpi.org/community/lists/users/2016/04/28958.php</font></tt></a><br><BR>
