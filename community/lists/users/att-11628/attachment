Thank you Ralph,<br><br>I did as you said. Programs are running fine, But still killing one process leads to terminate all processes. Am I missing something? Any thing else to be called with MPI::Comm::Disconnect()?<br><br>

Thanks &amp; Regards,<br><br><div class="gmail_quote">On Mon, Dec 21, 2009 at 8:00 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">

<div style="">Disconnect is a -collective- operation. Both parent and child have to call it. Your child process is &quot;hanging&quot; while it waits for the parent.<div><div></div><div class="h5"><div><br><div><div>On Dec 21, 2009, at 1:37 AM, vipin kumar wrote:</div>

<br><blockquote type="cite">Hello folks,<br><br>As I explained my problem earlier, I am looking for Fault <span>Tolerance</span> in MPI Programs. I read in Open MPI 2.1 standard document that two DISCONNECTED processes does not affect each other, i.e. they can die or can be killed without whithout affecting other processes.<br>



<br>So, I was trying this to achieve fault tolerance using MPI::Comm::Disconnect() to disconnect the CHILD process with  PARENT process, which was spawned by calling MPI::Comm::spawn(). I am calling MPI::Comm::Disconnect() from CHILD process immediatly after calling MPI::Init(). It seems that CHILD process is not returning from this call. <br>



<br>I tried MPI::Comm::Free() too, but this is also not working. Process is not progressing from this point of call. If I comment these statements, everything works fine. Note that I have tried this in Solaris as well as in Linux (fedora core).<br>



<br>My question is, whether Open-mpi suports to disconnect two processes( like child from parent). And if it is, then how?<br><br><br>Thanks &amp; Regards,<br><br><div class="gmail_quote">On Wed, Sep 23, 2009 at 6:41 PM, Josh Hursey <span dir="ltr">&lt;<a href="mailto:jjhursey@open-mpi.org" target="_blank">jjhursey@open-mpi.org</a>&gt;</span> wrote:<br>



<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">Unfortunately I cannot provide a precise time frame for availability at this point, but we are targeting the v1.5 release series. There is a handful of core developers working on this issue at the moment. Pieces of this  work have already made it into the Open MPI development trunk. If you want to play around with what is available try turning on the resilient mapper:<br>




  -mca rmaps resilient<br>
<br>
We will be sure to email the list once this work becomes more stable and available.<br><font color="#888888">
<br>
-- Josh</font><div><div></div><div><br>
<br>
On Sep 18, 2009, at 2:56 AM, vipin kumar wrote:<br>
<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Hi Josh,<br>
<br>
It is good to hear from you that work is in progress towards resiliency of Open-MPI. I was and I am waiting for this capability in Open-MPI. I have almost finished my development work and waiting for this to happen so that I can test my programs. It will be good if you can tell how long it will take to make Open-MPI a resilient impementation. Here by resiliency I mean abnormal termination or intentionally killing a process should not cause any(parent or sibling) process to be terminated, given that processes are connected.<br>




<br>
thanks.<br>
<br>
Regards,<br>
<br>
On Mon, Aug 3, 2009 at 8:37 PM, Josh Hursey &lt;<a href="mailto:jjhursey@open-mpi.org" target="_blank">jjhursey@open-mpi.org</a>&gt; wrote:<br>
Task-farm or manager/worker recovery models typically depend on intercommunicators (i.e., from MPI_Comm_spawn) and a resilient MPI implementation. William Gropp and Ewing Lusk have a paper entitled &quot;Fault Tolerance in MPI Programs&quot; that outlines how an application might take advantage of these features in order to recover from process failure.<br>




<br>
However, these techniques strongly depend upon resilient MPI implementations, and behaviors that, some may argue, are non-standard. Unfortunately there are not many MPI implementations that are sufficiently resilient in the face of process failure to support failure in task-farm scenarios. Though Open MPI supports the current MPI 2.1 standard, it is not as resilient to process failure as it could be.<br>




<br>
There are a number of people working on improving the resiliency of Open MPI in the face of network and process failure (including myself). We have started to move some of the resiliency work into the Open MPI trunk. Resiliency in Open MPI has been improving over the past few months, but I would not assess it as ready quite yet. Most of the work has focused on the runtime level (ORTE), and there are still some MPI level (OMPI) issues that need to be worked out.<br>




<br>
With all of that being said, I would try some of the techniques presented in the Gropp/Lusk paper in your application. Then test it with Open MPI and let us know how it goes.<br>
<br>
Best,<br>
Josh<br>
<br>
<br>
On Aug 3, 2009, at 10:30 AM, Durga Choudhury wrote:<br>
<br>
Is that kind of approach possible within an MPI framework? Perhaps a<br>
grid approach would be better. More experienced people, speak up,<br>
please?<br>
(The reason I say that is that I too am interested in the solution of<br>
that kind of problem, where an individual blade of a blade server<br>
fails and correcting for that failure on the fly is better than taking<br>
checkpoints and restarting the whole process excluding the failed<br>
blade.<br>
<br>
Durga<br>
<br>
On Mon, Aug 3, 2009 at 9:21 AM, jody&lt;<a href="mailto:jody.xha@gmail.com" target="_blank">jody.xha@gmail.com</a>&gt; wrote:<br>
Hi<br>
<br>
I guess &quot;task-farming&quot; could give you a certain amount of the kind of<br>
fault-tolerance you want.<br>
(i.e. a master process distributes tasks to idle slave processors -<br>
however, this will only work<br>
if the slave processes don&#39;t need to communicate with each other)<br>
<br>
Jody<br>
<br>
<br>
On Mon, Aug 3, 2009 at 1:24 PM, vipin kumar&lt;<a href="mailto:vipinkumar41@gmail.com" target="_blank">vipinkumar41@gmail.com</a>&gt; wrote:<br>
Hi all,<br>
<br>
Thanks Durga for your reply.<br>
<br>
Jeff, once you wrote code for Mandelbrot set to demonstrate fault tolerance<br>
in LAM-MPI. i. e. killing any slave process doesn&#39;t<br>
affect others. Exact behaviour I am looking for in Open MPI. I attempted,<br>
but no luck. Can you please tell how to write such programs in Open MPI.<br>
<br>
Thanks in advance.<br>
<br>
Regards,<br>
On Thu, Jul 9, 2009 at 8:30 PM, Durga Choudhury &lt;<a href="mailto:dpchoudh@gmail.com" target="_blank">dpchoudh@gmail.com</a>&gt; wrote:<br>
<br>
Although I have perhaps the least experience on the topic in this<br>
list, I will take a shot; more experienced people, please correct me:<br>
<br>
MPI standards specify communication mechanism, not fault tolerance at<br>
any level. You may achieve network tolerance at the IP level by<br>
implementing &#39;equal cost multipath&#39; routes (which means two equally<br>
capable NIC cards connecting to the same destination and modifying the<br>
kernel routing table to use both cards; the kernel will dynamically<br>
load balance.). At the MAC level, you can achieve the same effect by<br>
trunking multiple network cards.<br>
<br>
You can achieve process level fault tolerance by a checkpointing<br>
scheme such as BLCR, which has been tested to work with OpenMPI (and<br>
other processes as well)<br>
<br>
Durga<br>
<br>
On Thu, Jul 9, 2009 at 4:57 AM, vipin kumar&lt;<a href="mailto:vipinkumar41@gmail.com" target="_blank">vipinkumar41@gmail.com</a>&gt; wrote:<br>
<br>
Hi all,<br>
<br>
I want to know whether open mpi supports Network and process fault<br>
tolerance<br>
or not? If there is any example demonstrating these features that will<br>
be<br>
best.<br>
<br>
Regards,<br>
--<br>
Vipin K.<br>
Research Engineer,<br>
C-DOTB, India<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
--<br>
Vipin K.<br>
Research Engineer,<br>
C-DOTB, India<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
-- <br>
Vipin K.<br>
Research Engineer,<br>
C-DOTB, India<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br><br clear="all"><br>-- <br>Vipin K.<br>Research Engineer,<br>C-DOTB, India<br>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>

</div><br></div></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br><br clear="all"><br>-- <br>Vipin K.<br>Research Engineer,<br>C-DOTB, India<br>



