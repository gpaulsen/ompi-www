Like I said, I haven&#39;t tried any of that, so I have no idea if/how it would work. I don&#39;t have access to any hetero system and we don&#39;t see it very often at all, so it is quite possible the hetero support really isn&#39;t there.<br>
<br>I&#39;ll look at some of the Java-specific issues later.<br><br><br><div class="gmail_quote">On Thu, Oct 11, 2012 at 12:51 AM, Siegmar Gross <span dir="ltr">&lt;<a href="mailto:Siegmar.Gross@informatik.hs-fulda.de" target="_blank">Siegmar.Gross@informatik.hs-fulda.de</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi,<br>
<div class="im"><br>
&gt; I haven&#39;t tried heterogeneous apps on the Java code yet - could well not<br>
&gt; work. At the least, I would expect you need to compile your Java app<br>
&gt; against the corresponding OMPI install on each architecture, and ensure the<br>
&gt; right one gets run on each node. Even though it&#39;s a Java app, the classes<br>
&gt; need to get linked against the proper OMPI code for that node.<br>
&gt;<br>
&gt; As for Linux-only operation: it works fine for me. Did you remember to (a)<br>
&gt; build mpiexec on those linux machines (as opposed to using the Solaris<br>
&gt; version), and (b) recompile your app against that OMPI installation?<br>
<br>
</div>I didn&#39;t know that the classfiles are different, but it doesn&#39;t change<br>
anything, if I create different classfiles. I use a small shell script<br>
to create all neccessary files on all machines.<br>
<br>
<br>
tyr java 118 make_classfiles<br>
=========== rs0 ===========<br>
...<br>
mpijavac -d /home/fd1026/SunOS/sparc/mpi_classfiles MsgSendRecvMain.java<br>
mpijavac -d /home/fd1026/SunOS/sparc/mpi_classfiles ColumnSendRecvMain.java<br>
mpijavac -d /home/fd1026/SunOS/sparc/mpi_classfiles ColumnScatterMain.java<br>
mpijavac -d /home/fd1026/SunOS/sparc/mpi_classfiles EnvironVarMain.java<br>
=========== sunpc1 ===========<br>
...<br>
mpijavac -d /home/fd1026/SunOS/x86_64/mpi_classfiles MsgSendRecvMain.java<br>
mpijavac -d /home/fd1026/SunOS/x86_64/mpi_classfiles ColumnSendRecvMain.java<br>
mpijavac -d /home/fd1026/SunOS/x86_64/mpi_classfiles ColumnScatterMain.java<br>
mpijavac -d /home/fd1026/SunOS/x86_64/mpi_classfiles EnvironVarMain.java<br>
=========== linpc1 ===========<br>
...<br>
mpijavac -d /home/fd1026/Linux/x86_64/mpi_classfiles MsgSendRecvMain.java<br>
mpijavac -d /home/fd1026/Linux/x86_64/mpi_classfiles ColumnSendRecvMain.java<br>
mpijavac -d /home/fd1026/Linux/x86_64/mpi_classfiles ColumnScatterMain.java<br>
mpijavac -d /home/fd1026/Linux/x86_64/mpi_classfiles EnvironVarMain.java<br>
<br>
<br>
Every machine should now find its classfiles.<br>
<br>
tyr java 119 mpiexec -host sunpc0,linpc0,rs0 java EnvironVarMain<br>
<br>
Operating system: SunOS    Processor architecture: x86_64<br>
  CLASSPATH: ...:.:/home/fd1026/SunOS/x86_64/mpi_classfiles<br>
<br>
Operating system: Linux    Processor architecture: x86_64<br>
  CLASSPATH: ...:.:/home/fd1026/Linux/x86_64/mpi_classfiles<br>
<br>
Operating system: SunOS    Processor architecture: sparc<br>
  CLASSPATH: ...:.:/home/fd1026/SunOS/sparc/mpi_classfiles<br>
<br>
<br>
<br>
tyr java 120 mpiexec -host sunpc0,linpc0,rs0 java MsgSendRecvMain<br>
<div class="im">--------------------------------------------------------------------------<br>
It looks like opal_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during opal_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
  mca_base_open failed<br>
  --&gt; Returned value -2 instead of OPAL_SUCCESS<br>
--------------------------------------------------------------------------<br>
...<br>
<br>
<br>
<br>
</div>tyr java 121 mpiexec -host sunpc0,rs0 java MsgSendRecvMain<br>
[<a href="http://rs0.informatik.hs-fulda.de:13671" target="_blank">rs0.informatik.hs-fulda.de:13671</a>] *** An error occurred in MPI_Comm_dup<br>
[<a href="http://rs0.informatik.hs-fulda.de:13671" target="_blank">rs0.informatik.hs-fulda.de:13671</a>] *** reported by process [1077346305,1]<br>
[<a href="http://rs0.informatik.hs-fulda.de:13671" target="_blank">rs0.informatik.hs-fulda.de:13671</a>] *** on communicator MPI_COMM_WORLD<br>
[<a href="http://rs0.informatik.hs-fulda.de:13671" target="_blank">rs0.informatik.hs-fulda.de:13671</a>] *** MPI_ERR_INTERN: internal error<br>
[<a href="http://rs0.informatik.hs-fulda.de:13671" target="_blank">rs0.informatik.hs-fulda.de:13671</a>] *** MPI_ERRORS_ARE_FATAL (processes in this<br>
communicator will now abort,<br>
[<a href="http://rs0.informatik.hs-fulda.de:13671" target="_blank">rs0.informatik.hs-fulda.de:13671</a>] ***    and potentially your MPI job)<br>
<br>
<br>
<br>
I get an error even then, when I login on a Linux machine, before I<br>
run the command.<br>
<br>
linpc0 fd1026 99 mpiexec -host linpc0,linpc1 java MsgSendRecvMain<br>
<div class="im">--------------------------------------------------------------------------<br>
It looks like opal_init failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during opal_init; some of which are due to configuration or<br>
environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>
Open MPI developer):<br>
<br>
  mca_base_open failed<br>
  --&gt; Returned value -2 instead of OPAL_SUCCESS<br>
--------------------------------------------------------------------------<br>
...<br>
*** An error occurred in MPI_Init<br>
*** on a NULL communicator<br>
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>
***    and potentially your MPI job)<br>
</div>[linpc1:3004] Local abort before MPI_INIT completed successfully; not able to<br>
<div class="im">aggregate error messages, and not able to guarantee that all other processes<br>
were killed!<br>
...<br>
<br>
<br>
</div>linpc0 fd1026 99 mpijavac -showme<br>
/usr/local/jdk1.7.0_07-64/bin/javac -cp ...<br>
:.:/home/fd1026/Linux/x86_64/mpi_classfiles:/usr/local/openmpi-1.9_64_cc/lib64/<br>
mpi.jar<br>
<br>
<br>
By the way I have the same classfiles for all architectures. Are you<br>
sure that they should be different? I don&#39;t find any absolute path names<br>
in the files, when I use &quot;strings&quot;.<br>
<br>
tyr java 133 diff ~/SunOS/sparc/mpi_classfiles/MsgSendRecvMain.class \<br>
  ~/SunOS/x86_64/mpi_classfiles/MsgSendRecvMain.class<br>
tyr java 134 diff ~/SunOS/sparc/mpi_classfiles/MsgSendRecvMain.class \<br>
 ~/Linux/x86_64/mpi_classfiles/MsgSendRecvMain.class<br>
<br>
<br>
<br>
Can I provide more information to track the problem on my Linux systems?<br>
I think that I have to wait until you support a heterogeneous system, but<br>
it would be nice, if Java applications would run on my different<br>
homogeneous systems. The strange thing is that it works on my different<br>
Solaris systems and not on Linux this time.<br>
<br>
Do you know if my problem with Datatype.Vector is a problem of Open<br>
MPI as well (one of my other emails)? Do you use the extent of the base<br>
type and not the extent of the derived data type, if I use a derived<br>
data type in a scatter/gather operation or an operation with &quot;count&quot;<br>
greater than one?<br>
<br>
<br>
Kind regards<br>
<span class="HOEnZb"><font color="#888888"><br>
Siegmar<br>
</font></span><div class="HOEnZb"><div class="h5"><br>
<br>
<br>
&gt; On Wed, Oct 10, 2012 at 5:42 AM, Siegmar Gross &lt;<br>
&gt; <a href="mailto:Siegmar.Gross@informatik.hs-fulda.de">Siegmar.Gross@informatik.hs-fulda.de</a>&gt; wrote:<br>
&gt;<br>
&gt; &gt; Hi,<br>
&gt; &gt;<br>
&gt; &gt; I have built openmpi-1.9a1r27380 with Java support and implemented<br>
&gt; &gt; a small program that sends some kind of &quot;hello&quot; with Send/Recv.<br>
&gt; &gt;<br>
&gt; &gt; tyr java 164 make<br>
&gt; &gt; mpijavac -d /home/fd1026/mpi_classfiles MsgSendRecvMain.java<br>
&gt; &gt; ...<br>
&gt; &gt;<br>
&gt; &gt; Everything works fine, if I use Solaris 10 x86_84.<br>
&gt; &gt;<br>
&gt; &gt; tyr java 165 mpiexec -np 3 -host sunpc0,sunpc1 \<br>
&gt; &gt;   java -cp $HOME/mpi_classfiles MsgSendRecvMain<br>
&gt; &gt;<br>
&gt; &gt; Now 2 processes are sending greetings.<br>
&gt; &gt;<br>
&gt; &gt; Greetings from process 2:<br>
&gt; &gt;   message tag:    3<br>
&gt; &gt;   message length: 6<br>
&gt; &gt;   message:        sunpc1<br>
&gt; &gt;<br>
&gt; &gt; Greetings from process 1:<br>
&gt; &gt;   message tag:    3<br>
&gt; &gt;   message length: 6<br>
&gt; &gt;   message:        sunpc0<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; Everything works fine, if I use Solaris 10 Sparc.<br>
&gt; &gt;<br>
&gt; &gt; tyr java 166 mpiexec -np 3 -host rs0,rs1 \<br>
&gt; &gt;   java -cp $HOME/mpi_classfiles MsgSendRecvMain<br>
&gt; &gt;<br>
&gt; &gt; Now 2 processes are sending greetings.<br>
&gt; &gt;<br>
&gt; &gt; Greetings from process 2:<br>
&gt; &gt;   message tag:    3<br>
&gt; &gt;   message length: 26<br>
&gt; &gt;   message:        <a href="http://rs1.informatik.hs-fulda.de" target="_blank">rs1.informatik.hs-fulda.de</a><br>
&gt; &gt;<br>
&gt; &gt; Greetings from process 1:<br>
&gt; &gt;   message tag:    3<br>
&gt; &gt;   message length: 26<br>
&gt; &gt;   message:        <a href="http://rs0.informatik.hs-fulda.de" target="_blank">rs0.informatik.hs-fulda.de</a><br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; The program breaks, if I use both systems.<br>
&gt; &gt;<br>
&gt; &gt; tyr java 167 mpiexec -np 3 -host rs0,sunpc0 \<br>
&gt; &gt;   java -cp $HOME/mpi_classfiles MsgSendRecvMain<br>
&gt; &gt; [<a href="http://rs0.informatik.hs-fulda.de:9621" target="_blank">rs0.informatik.hs-fulda.de:9621</a>] *** An error occurred in MPI_Comm_dup<br>
&gt; &gt; [<a href="http://rs0.informatik.hs-fulda.de:9621" target="_blank">rs0.informatik.hs-fulda.de:9621</a>] *** reported by process [1976500225,0]<br>
&gt; &gt; [<a href="http://rs0.informatik.hs-fulda.de:9621" target="_blank">rs0.informatik.hs-fulda.de:9621</a>] *** on communicator MPI_COMM_WORLD<br>
&gt; &gt; [<a href="http://rs0.informatik.hs-fulda.de:9621" target="_blank">rs0.informatik.hs-fulda.de:9621</a>] *** MPI_ERR_INTERN: internal error<br>
&gt; &gt; [<a href="http://rs0.informatik.hs-fulda.de:9621" target="_blank">rs0.informatik.hs-fulda.de:9621</a>] *** MPI_ERRORS_ARE_FATAL (processes<br>
&gt; &gt;    in this communicator will now abort,<br>
&gt; &gt; [<a href="http://rs0.informatik.hs-fulda.de:9621" target="_blank">rs0.informatik.hs-fulda.de:9621</a>] ***    and potentially your MPI job)<br>
&gt; &gt; [<a href="http://tyr.informatik.hs-fulda.de:22491" target="_blank">tyr.informatik.hs-fulda.de:22491</a>] 1 more process has sent help message<br>
&gt; &gt;    help-mpi-errors.txt / mpi_errors_are_fatal<br>
&gt; &gt; [<a href="http://tyr.informatik.hs-fulda.de:22491" target="_blank">tyr.informatik.hs-fulda.de:22491</a>] Set MCA parameter<br>
&gt; &gt;   &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; The program breaks, if I use Linux x86_64.<br>
&gt; &gt;<br>
&gt; &gt; tyr java 168 mpiexec -np 3 -host linpc0,linpc1 \<br>
&gt; &gt;   java -cp $HOME/mpi_classfiles MsgSendRecvMain<br>
&gt; &gt; --------------------------------------------------------------------------<br>
&gt; &gt; It looks like opal_init failed for some reason; your parallel process is<br>
&gt; &gt; likely to abort.  There are many reasons that a parallel process can<br>
&gt; &gt; fail during opal_init; some of which are due to configuration or<br>
&gt; &gt; environment problems.  This failure appears to be an internal failure;<br>
&gt; &gt; here&#39;s some additional information (which may only be relevant to an<br>
&gt; &gt; Open MPI developer):<br>
&gt; &gt;<br>
&gt; &gt;   mca_base_open failed<br>
&gt; &gt;   --&gt; Returned value -2 instead of OPAL_SUCCESS<br>
&gt; &gt; --------------------------------------------------------------------------<br>
&gt; &gt; ...<br>
&gt; &gt;<br>
&gt; &gt; *** An error occurred in MPI_Init<br>
&gt; &gt; *** on a NULL communicator<br>
&gt; &gt; *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>
&gt; &gt; ***    and potentially your MPI job)<br>
&gt; &gt; [linpc0:20277] Local abort before MPI_INIT completed successfully;<br>
&gt; &gt;   not able to aggregate error messages, and not able to guarantee that<br>
&gt; &gt;   all other processes were killed!<br>
&gt; &gt; ...<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; Please let me know if you need more information to track the problem.<br>
&gt; &gt; Thank you very much for any help in advance.<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; Kind regards<br>
&gt; &gt;<br>
&gt; &gt; Siegmar<br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; &gt;<br>
<br>
</div></div></blockquote></div><br>

