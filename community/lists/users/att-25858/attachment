<div dir="ltr">I would argue this is a typical user level bug.<div><br></div><div>The major difference between the dist_create and dist_create_adjacent is that in the later each process provides its neighbors in an order that is expected (and that match the info provided to the MPI_Neighbor_alltoallw call. When the topology is created with dist_create, every process will end-up having the correct partial topology, but in an order that doesn&#39;t match what the user expected (not in the rank-order of the neighbors). However, I can&#39;t find anything in the standard that would require from the MPI library to sort the neighbors. I would assume is the user responsibility, to make sure that they are using the topology in the right order, where the right order is what the communicator really contains and not what the user expect based on prior knowledge.<div><br></div><div>  George.</div><div><br></div></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Fri, Nov 21, 2014 at 3:48 AM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles.gouaillardet@iferc.org" target="_blank">gilles.gouaillardet@iferc.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
  
    
  
  <div text="#000000" bgcolor="#FFFFFF">
    <div>Ghislain,<br>
      <br>
      i can confirm there is a bug in
      mca_topo_base_dist_graph_distribute<br>
      <br>
      FYI a proof of concept is available at
      <a href="https://github.com/open-mpi/ompi/pull/283" target="_blank">https://github.com/open-mpi/ompi/pull/283</a><br>
      and i recommend you use MPI_Dist_graph_create_adjacent if this
      meets your needs.<br>
      <br>
      as a side note, the right way to set the info is<br>
              MPI_Info info = MPI_INFO_NULL;<br>
      <br>
      /* mpich is more picky and crashes with info = NULL */<br>
      <br>
      Cheers,<br>
      <br>
      Gilles<div><div class="h5"><br>
      <br>
      On 2014/11/21 18:21, Ghislain Viguier wrote:<br>
    </div></div></div>
    <blockquote type="cite"><div><div class="h5">
      <pre>Hi Gilles and Howard,

The use of MPI_Dist_graph_create_adjacent solves the issue :)

Thanks for your help!

Best reagrds,
Ghislain

2014-11-21 7:23 GMT+01:00 Gilles Gouaillardet &lt;<a href="mailto:gilles.gouaillardet@iferc.org" target="_blank">gilles.gouaillardet@iferc.org</a>
</pre>
      <blockquote type="cite">
        <pre>:
</pre>
      </blockquote>
      <pre></pre>
      </div></div><blockquote type="cite">
        <pre><div><div class="h5"> Hi Ghislain,

that sound like a but in MPI_Dist_graph_create :-(

you can use MPI_Dist_graph_create_adjacent instead :

MPI_Dist_graph_create_</div></div>adjacent(MPI_COMM_WORLD, degrees, &amp;targets[0],
&amp;weights[0],
                        degrees, &amp;targets[0], &amp;weights[0], info,
rankReordering, &amp;commGraph);

it does not crash and as far as i understand, it produces correct results,

according the the mpi standard (example 7.3) that should do the same
thing, that&#39;s why
i think there is a bug in MPI_Dist_graph_create

Cheers,

Gilles




On 2014/11/21 2:21, Howard Pritchard wrote:

Hi Ghislain,

I tried to run your test with mvapich 1.9 and get a &quot;message truncated&quot;
failure at three ranks.

Howard


2014-11-20 8:51 GMT-07:00 Ghislain Viguier <a href="mailto:ghislain.viguier@gmail.com" target="_blank">&lt;ghislain.viguier@gmail.com&gt;</a> <a href="mailto:ghislain.viguier@gmail.com" target="_blank">&lt;ghislain.viguier@gmail.com&gt;</a>:


 Dear support,

I&#39;m encountering an issue with the MPI_Neighbor_alltoallw request of
mpi-1.8.3.
I have enclosed a test case with information of my workstation.

In this test, I define a weighted topology for 5 processes, where the
weight represent the number of buffers to send/receive :
    rank
      0 : | x |
      1 : | 2 | x |
      2 : | 1 | 1 | x |
      3 : | 3 | 2 | 3 | x |
      4 : | 5 | 2 | 2 | 2 | x |

In this topology, the rank 1 will send/receive :
   2 buffers to/from the rank 0,
   1 buffer to/from the rank 2,
   2 buffers to/from the rank 3,
   2 buffers to/from the rank 4,

The send buffer are defined with the MPI_Type_create_hindexed_block. This
allows to use a same buffer for several communications without duplicating
it (read only).
Here the rank 1 will have 2 send buffers (the max of 2, 1, 2, 2).
The receiver buffer is a contiguous buffer defined with
MPI_Type_contiguous request.
Here, the receiver buffer of the rank 1 is of size : 7 (2+1+2+2)

This test case succesful for 2 or 3 processes. For 4 processes, the test
fails 1 times for 3 successes. For 5 processes, the test fails all the time.

The error code is : *** MPI_ERR_IN_STATUS: error code in status

I don&#39;t understand what I am doing wrong.

Could you please have a look on it?

Thank you very much.

Best regards,
Ghislain Viguier

--
Ghislain Viguier
Tél. 06 31 95 03 17

_______________________________________________
users mailing <a href="mailto:listusers@open-mpi.org" target="_blank">listusers@open-mpi.org</a><div><div class="h5">
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post:<a href="http://www.open-mpi.org/community/lists/users/2014/11/25850.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/11/25850.php</a>



_______________________________________________
users mailing <a href="mailto:listusers@open-mpi.org" target="_blank">listusers@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>

Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/11/25852.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/11/25852.php</a>



_______________________________________________
users mailing list
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post:
<a href="http://www.open-mpi.org/community/lists/users/2014/11/25853.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/11/25853.php</a>

</div></div></pre>
      </blockquote>
      <pre>

</pre>
      <br>
      <fieldset></fieldset>
      <br>
      <pre><div><div class="h5">_______________________________________________
users mailing list
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/11/25855.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/11/25855.php</a></pre>
    </blockquote>
    <br>
  </div>

<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/11/25856.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/11/25856.php</a><br></blockquote></div><br></div>

