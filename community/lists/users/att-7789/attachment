<div>what is your message size and the number of cores per node?</div>
<div>is there any difference using different algorithms?<br><br></div>
<div class="gmail_quote">2009/1/23 Gabriele Fatigati <span dir="ltr">&lt;<a href="mailto:g.fatigati@cineca.it">g.fatigati@cineca.it</a>&gt;</span><br>
<blockquote class="gmail_quote" style="PADDING-LEFT: 1ex; MARGIN: 0px 0px 0px 0.8ex; BORDER-LEFT: #ccc 1px solid">Hi Jeff,<br>i would like to understand why, if i run over 512 procs or more, my<br>code stops over mpi collective, also with little send buffer. All<br>
processors are locked into call, doing nothing. But, if i add<br>MPI_Barrier &nbsp;after MPI collective, it works! I run over Infiniband<br>net.<br><br>I know many people with this strange problem, i think there is a<br>strange interaction between Infiniband and OpenMPI that causes it.<br>
<br><br><br>2009/1/23 Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;:<br>
<div>
<div></div>
<div class="Wj3C7c">&gt; On Jan 23, 2009, at 6:32 AM, Gabriele Fatigati wrote:<br>&gt;<br>&gt;&gt; I&#39;ve noted that OpenMPI has an asynchronous behaviour in the collective<br>&gt;&gt; calls.<br>&gt;&gt; The processors, doesn&#39;t wait that other procs arrives in the call.<br>
&gt;<br>&gt; That is correct.<br>&gt;<br>&gt;&gt; This behaviour sometimes can cause some problems with a lot of<br>&gt;&gt; processors in the jobs.<br>&gt;<br>&gt; Can you describe what exactly you mean? &nbsp;The MPI spec specifically allows<br>
&gt; this behavior; OMPI made specific design choices and optimizations to<br>&gt; support this behavior. &nbsp;FWIW, I&#39;d be pretty surprised if any optimized MPI<br>&gt; implementation defaults to fully synchronous collective operations.<br>
&gt;<br>&gt;&gt; Is there an OpenMPI parameter to lock all process in the collective<br>&gt;&gt; call until is finished? Otherwise &nbsp;i have to insert many MPI_Barrier<br>&gt;&gt; in my code and it is very tedious and strange..<br>
&gt;<br>&gt; As you have notes, MPI_Barrier is the *only* collective operation that MPI<br>&gt; guarantees to have any synchronization properties (and it&#39;s a fairly weak<br>&gt; guarantee at that; no process will exit the barrier until every process has<br>
&gt; entered the barrier -- but there&#39;s no guarantee that all processes leave the<br>&gt; barrier at the same time).<br>&gt;<br>&gt; Why do you need your processes to exit collective operations at the same<br>&gt; time?<br>
&gt;<br>&gt; --<br>&gt; Jeff Squyres<br>&gt; Cisco Systems<br>&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>&gt;<br><br><br><br></div></div>
<div class="Ih2E3d">--<br>Ing. Gabriele Fatigati<br><br>Parallel programmer<br><br>CINECA Systems &amp; Tecnologies Department<br><br>Supercomputing Group<br><br>Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br><br>
<a href="http://www.cineca.it/" target="_blank">www.cineca.it</a> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Tel: &nbsp; +39 051 6171722<br><br>g.fatigati [AT] <a href="http://cineca.it/" target="_blank">cineca.it</a><br>_______________________________________________<br>
</div>
<div>
<div></div>
<div class="Wj3C7c">users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

