<div dir="ltr"><div>Hello,</div><div>I&#39;m trying to use the neighborhood collective communication capabilities (MPI_Ineighbor_x) of MPI coupled with the distributed graph constructor (MPI_Dist_graph_create_adjacent) but I&#39;m encountering a segmentation fault on a test case.</div><div><br></div><div>I have attached a &#39;working&#39; example where I create a MPI communicator with a simple distributed graph topology where Rank 0 contains Node 0 that communicates bi-directionally (receiving from and sending to) with Node 1 located on Rank 1.  I then attempt to send integer messages using the neighborhood collective MPI_Ineighbor_alltoall.  The program run with the command &#39;mpirun -n 2 ./simpleneighborhood&#39; compiled with the latest OpenMPI  (1.10.2) encounters a segmentation fault during the non-blocking call.  The same program compiled with MPICH (3.2) runs without any problems and with the expected results.  To muddy the waters a little more, the same program compiled with OpenMPI but using the blocking neighborhood collective, MPI_Neighbor_alltoall, seems to run just fine as well.</div><div><br></div><div>I&#39;m not really sure at this point if I&#39;m making a simple mistake in the construction of my test or if something is more fundamentally wrong.  I would appreciate any insight into my problem!  </div><div><br></div><div>Thanks ahead of the time for help and let me know if I can provide anymore information.</div><div><br></div><div>Sincerely,</div><div>Jun</div></div>

