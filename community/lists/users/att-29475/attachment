<div dir="ltr">Dear Dr. Correa, <div><br></div><div>This is indeed the structure, it is a CFD program. Most of what you are suggesting is my current workflow, including saving, sending emails upon a crash and restarting. </div><div><br></div><div>The problem is that the code does not crash but hangs. If it is deadlocked then it sits there spinning cycles until I happen to check. Monitoring the code like this has become inefficient -- sometimes an overnight run works for half an hour and I don&#39;t notice until the morning. Also, to restart from this requires sitting in queue again. I will try to better understand job system&#39;s automatic resubmit, but for now I do not see how to use this to fix the deadlock problem. </div><div><br></div><div>After thinking about your email perhaps I can phrase my question more precisely -- How can I return control to the shell if the MPI process has deadlocked? </div><div><br></div><div>Thank you, </div><div>Alex </div><div><br></div><div><br></div><div><br></div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Thu, Jun 16, 2016 at 8:44 PM, Gus Correa <span dir="ltr">&lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi Alex<br>
<br>
You know all this, but just in case ...<br>
<br>
Restartable code goes like this:<br>
<br>
*****************************<br>
.start<br>
<br>
read the initial/previous configuration from a file<br>
...<br>
final_step = first_step + nsteps<br>
time_step = first_step<br>
while ( time_step .le. final_step )<br>
  ... march in time ...<br>
  time_step = time_step +1<br>
end<br>
<br>
save the final_step configuration (or phase space) to a file<br>
[depending on the algorithm you may need to save the<br>
previous config also, or perhaps a few more]<br>
<br>
.end<br>
************************************************<br>
<br>
Then restart the job time and again, until the desired<br>
number of time steps is completed.<br>
<br>
Job queue systems/resource managers allow a job to resubmit itself,<br>
so unless a job fails it feels like a single time integration.<br>
<br>
If a job fails in the middle, you don&#39;t lose all work,<br>
just restart from the previous saved configuration.<br>
That is the only situation where you need to &quot;monitor&quot; the code.<br>
Resource managers/ queue systems can also email you in<br>
case the job fails, warning you to do manual intervention.<br>
<br>
The time granularity per job (nsteps) is up to you.<br>
Normally it is adjusted to the max walltime of job queues<br>
(in a shared computer/cluster),<br>
but in your case it can be adjusted to how often the program fails.<br>
<br>
All atmosphere/ocean/climate/weather_forecast models work<br>
this way (that&#39;s what we mostly run here).<br>
I guess most CFD, computational Chemistry, etc, programs also do.<br>
<br>
I hope this helps,<br>
Gus Correa<div><div class="h5"><br>
<br>
<br>
On 06/16/2016 05:25 PM, Alex Kaiser wrote:<br>
</div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div class="h5">
Hello,<br>
<br>
I have an MPI code which sometimes hangs, simply stops running. It is<br>
not clear why and it uses many large third party libraries so I do not<br>
want to try to fix it. The code is easy to restart, but then it needs to<br>
be monitored closely by me, and I&#39;d prefer to do it automatically.<br>
<br>
Is there a way, within an MPI process, to detect the hang and abort if so?<br>
<br>
In psuedocode, I would like to do something like<br>
<br>
    for (all time steps)<br>
         step<br>
         if (nothing has happened for x minutes)<br>
<br>
             call mpi abort to return control to the shell<br>
<br>
         endif<br>
<br>
    endfor<br>
<br>
This structure does not work however, as it can no longer do anything,<br>
including check itself, when it is stuck.<br>
<br>
<br>
Thank you,<br>
Alex<br>
<br>
<br>
<br></div></div><span class="">
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/06/29471.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/06/29471.php</a><br>
<br>
</span></blockquote><span class="">
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></span>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/06/29473.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/06/29473.php</a><br>
</blockquote></div><br></div>

