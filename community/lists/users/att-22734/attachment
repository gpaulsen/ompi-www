<div dir="ltr"><div><div><div><div>Hi Gus, first of all thank you very much for you help. I really appreciate!<br><br></div>Then you are right, I have OpenFOAM so &#39;which mpif90&#39; addresses to another installation that probably wasn&#39;t meant to have f90 bindings. However, when I compile my f90 code I use absolute path.<br>
<br></div>Even when I am in the /bin directory  of the ompi, ./ompi_info says that f90 bindings are ok, but ./mpif90 complains about f90 supports.<br><br></div>I suspect there must be another issue.<br><br></div>Thanks again,<br>
Damiano<br></div><div class="gmail_extra"><br><br><div class="gmail_quote">2013/9/30 Gus Correa <span dir="ltr">&lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;</span><br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Hi Damiano<br>
<br>
Did you setup your PATH and LD_LIBRARY_PATH<br>
to point to your OMPI installation?<br>
I.e. to:<br>
 /home/damiano/fortran/openmpi-<u></u>1.6.5/installation/bin<br>
and<br>
 /home/damiano/fortran/openmpi-<u></u>1.6.5/installation/lib<br>
<br>
Some OS distributions, commercial compilers, and other software,<br>
come with &quot;extra&quot; OMPI installations, which can be ahead of<br>
your path.<br>
&quot;which mpif90&quot; will tell you what you are actually using.<br>
<br>
For what it is worth, disabling shared libraries<br>
at configure time may be challenging.<br>
<br>
I hope this helps,<br>
Gus Correa<div class="im"><br>
<br>
On 09/30/2013 11:58 AM, Damiano Natali wrote:<br>
</div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div class="im">
Dear list,<br>
<br>
I&#39;m trying to install openMPI on a Linux 64-bit OpenSuse machine with<br>
the following lines<br>
<br>
./configure FC=gfortran<br>
--prefix=/home/damiano/<u></u>fortran/openmpi-1.6.5/<u></u>installation/<br>
--disable-shared --enable-static --with-mpi-f90-size=medium<br>
--enable-mpi-f90 cflags=-m64 cxxflags=-m64 fflags=-m64 fcflags=-m64<br>
make -j4 all<br>
make install<br>
<br>
everything goes on nicely and I end up with an installation folder with<br>
a bin subfolder. However, when I try to launch the mpif90 compiler the<br>
error<br>
<br>
------------------------------<u></u>------------------------------<u></u>--------------<br>
Unfortunately, this installation of Open MPI was not compiled with<br>
Fortran 90 support.  As such, the mpif90 compiler is non-functional.<br>
------------------------------<u></u>------------------------------<u></u>--------------<br>
<br>
is still there. The output of the ompi_info is<br>
<br>
  Configured architecture: x86_64-unknown-linux-gnu<br>
           Configure host: <a href="http://caillou.dicat.unige.it" target="_blank">caillou.dicat.unige.it</a><br></div>
&lt;<a href="http://caillou.dicat.unige.it" target="_blank">http://caillou.dicat.unige.it</a><u></u>&gt;<div class="im"><br>
            Configured by: damiano<br>
            Configured on: Mon Sep 30 17:17:39 CEST 2013<br>
           Configure host: <a href="http://caillou.dicat.unige.it" target="_blank">caillou.dicat.unige.it</a><br></div>
&lt;<a href="http://caillou.dicat.unige.it" target="_blank">http://caillou.dicat.unige.it</a><u></u>&gt;<div class="im"><br>
                 Built by: damiano<br>
                 Built on: Mon Sep 30 17:26:12 CEST 2013<br>
               Built host: <a href="http://caillou.dicat.unige.it" target="_blank">caillou.dicat.unige.it</a><br></div>
&lt;<a href="http://caillou.dicat.unige.it" target="_blank">http://caillou.dicat.unige.it</a><u></u>&gt;<div><div class="h5"><br>
               C bindings: yes<br>
             C++ bindings: yes<br>
       Fortran77 bindings: yes (all)<br>
       Fortran90 bindings: yes<br>
  Fortran90 bindings size: medium<br>
               C compiler: gcc<br>
      C compiler absolute: /usr/bin/gcc<br>
   C compiler family name: GNU<br>
       C compiler version: 4.7.1<br>
             C++ compiler: g++<br>
    C++ compiler absolute: /usr/bin/g++<br>
       Fortran77 compiler: gfortran<br>
   Fortran77 compiler abs: /usr/bin/gfortran<br>
       Fortran90 compiler: gfortran<br>
   Fortran90 compiler abs: /usr/bin/gfortran<br>
              C profiling: yes<br>
            C++ profiling: yes<br>
      Fortran77 profiling: yes<br>
      Fortran90 profiling: yes<br>
           C++ exceptions: no<br>
           Thread support: posix (MPI_THREAD_MULTIPLE: no, progress: no)<br>
            Sparse Groups: no<br>
   Internal debug support: no<br>
   MPI interface warnings: no<br>
      MPI parameter check: runtime<br>
Memory profiling support: no<br>
Memory debugging support: no<br>
          libltdl support: yes<br>
    Heterogeneous support: no<br>
  mpirun default --prefix: no<br>
          MPI I/O support: yes<br>
        MPI_WTIME support: gettimeofday<br>
      Symbol vis. support: yes<br>
    Host topology support: yes<br>
           MPI extensions: affinity example<br>
    FT Checkpoint support: no (checkpoint thread: no)<br>
      VampirTrace support: yes<br>
   MPI_MAX_PROCESSOR_NAME: 256<br>
     MPI_MAX_ERROR_STRING: 256<br>
      MPI_MAX_OBJECT_NAME: 64<br>
         MPI_MAX_INFO_KEY: 36<br>
         MPI_MAX_INFO_VAL: 256<br>
        MPI_MAX_PORT_NAME: 1024<br>
   MPI_MAX_DATAREP_STRING: 128<br>
            MCA backtrace: execinfo (MCA v2.0, API v2.0, Component v1.6.4)<br>
               MCA memory: linux (MCA v2.0, API v2.0, Component v1.6.4)<br>
            MCA paffinity: hwloc (MCA v2.0, API v2.0, Component v1.6.4)<br>
                MCA carto: auto_detect (MCA v2.0, API v2.0, Component<br>
v1.6.4)<br>
                MCA carto: file (MCA v2.0, API v2.0, Component v1.6.4)<br>
                MCA shmem: mmap (MCA v2.0, API v2.0, Component v1.6.4)<br>
                MCA shmem: posix (MCA v2.0, API v2.0, Component v1.6.4)<br>
                MCA shmem: sysv (MCA v2.0, API v2.0, Component v1.6.4)<br>
            MCA maffinity: first_use (MCA v2.0, API v2.0, Component v1.6.4)<br>
            MCA maffinity: hwloc (MCA v2.0, API v2.0, Component v1.6.4)<br>
                MCA timer: linux (MCA v2.0, API v2.0, Component v1.6.4)<br>
          MCA installdirs: env (MCA v2.0, API v2.0, Component v1.6.4)<br>
          MCA installdirs: config (MCA v2.0, API v2.0, Component v1.6.4)<br>
              MCA sysinfo: linux (MCA v2.0, API v2.0, Component v1.6.4)<br>
                MCA hwloc: hwloc132 (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA dpm: orte (MCA v2.0, API v2.0, Component v1.6.4)<br>
               MCA pubsub: orte (MCA v2.0, API v2.0, Component v1.6.4)<br>
            MCA allocator: basic (MCA v2.0, API v2.0, Component v1.6.4)<br>
            MCA allocator: bucket (MCA v2.0, API v2.0, Component v1.6.4)<br>
                 MCA coll: basic (MCA v2.0, API v2.0, Component v1.6.4)<br>
                 MCA coll: hierarch (MCA v2.0, API v2.0, Component v1.6.4)<br>
                 MCA coll: inter (MCA v2.0, API v2.0, Component v1.6.4)<br>
                 MCA coll: self (MCA v2.0, API v2.0, Component v1.6.4)<br>
                 MCA coll: sm (MCA v2.0, API v2.0, Component v1.6.4)<br>
                 MCA coll: sync (MCA v2.0, API v2.0, Component v1.6.4)<br>
                 MCA coll: tuned (MCA v2.0, API v2.0, Component v1.6.4)<br>
                   MCA io: romio (MCA v2.0, API v2.0, Component v1.6.4)<br>
                MCA mpool: fake (MCA v2.0, API v2.0, Component v1.6.4)<br>
                MCA mpool: rdma (MCA v2.0, API v2.0, Component v1.6.4)<br>
                MCA mpool: sm (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA pml: bfo (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA pml: csum (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA pml: ob1 (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA pml: v (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA bml: r2 (MCA v2.0, API v2.0, Component v1.6.4)<br>
               MCA rcache: vma (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA btl: self (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA btl: sm (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA btl: tcp (MCA v2.0, API v2.0, Component v1.6.4)<br>
                 MCA topo: unity (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA osc: pt2pt (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA osc: rdma (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA iof: hnp (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA iof: orted (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA iof: tool (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA oob: tcp (MCA v2.0, API v2.0, Component v1.6.4)<br>
                 MCA odls: default (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA ras: cm (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA ras: loadleveler (MCA v2.0, API v2.0, Component<br>
v1.6.4)<br>
                  MCA ras: slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA ras: gridengine (MCA v2.0, API v2.0, Component v1.6.3)<br>
                MCA rmaps: load_balance (MCA v2.0, API v2.0, Component<br>
v1.6.4)<br>
                MCA rmaps: rank_file (MCA v2.0, API v2.0, Component v1.6.4)<br>
                MCA rmaps: resilient (MCA v2.0, API v2.0, Component v1.6.4)<br>
                MCA rmaps: round_robin (MCA v2.0, API v2.0, Component<br>
v1.6.4)<br>
                MCA rmaps: seq (MCA v2.0, API v2.0, Component v1.6.4)<br>
                MCA rmaps: topo (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA rml: oob (MCA v2.0, API v2.0, Component v1.6.4)<br>
               MCA routed: binomial (MCA v2.0, API v2.0, Component v1.6.4)<br>
               MCA routed: cm (MCA v2.0, API v2.0, Component v1.6.4)<br>
               MCA routed: direct (MCA v2.0, API v2.0, Component v1.6.4)<br>
               MCA routed: linear (MCA v2.0, API v2.0, Component v1.6.4)<br>
               MCA routed: radix (MCA v2.0, API v2.0, Component v1.6.4)<br>
               MCA routed: slave (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA plm: rsh (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA plm: slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>
                MCA filem: rsh (MCA v2.0, API v2.0, Component v1.6.4)<br>
               MCA errmgr: default (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA ess: env (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA ess: hnp (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA ess: singleton (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA ess: slave (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA ess: slurm (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA ess: slurmd (MCA v2.0, API v2.0, Component v1.6.4)<br>
                  MCA ess: tool (MCA v2.0, API v2.0, Component v1.6.4)<br>
              MCA grpcomm: bad (MCA v2.0, API v2.0, Component v1.6.4)<br>
              MCA grpcomm: basic (MCA v2.0, API v2.0, Component v1.6.4)<br>
              MCA grpcomm: hier (MCA v2.0, API v2.0, Component v1.6.4)<br>
             MCA notifier: command (MCA v2.0, API v1.0, Component v1.6.4)<br>
             MCA notifier: syslog (MCA v2.0, API v1.0, Component v1.6.4)<br>
<br>
as far as I know, the f90 bindings seem to be configured properly. What<br>
can be wrong?<br>
<br>
Thanks you for your attention,<br>
Damiano<br>
<br>
--<br>
Damiano Natali<br></div></div>
mail <a href="mailto:damiano.natali@gmail.com" target="_blank">damiano.natali@gmail.com</a> &lt;mailto:<a href="mailto:damiano.natali@gmail.com" target="_blank">damiano.natali@gmail.<u></u>com</a>&gt;<br>
skype damiano.natali<br>
<br>
<br>
<br>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
</blockquote>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
</blockquote></div><br><br clear="all"><br>-- <br><div>Damiano Natali</div><div>mail <a href="mailto:damiano.natali@gmail.com" target="_blank">damiano.natali@gmail.com</a></div><div>skype damiano.natali</div><div><br></div>
<div><br></div>
</div>

