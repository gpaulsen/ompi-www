<p>Hi Gus,<br>
thank you for your replay.</p>
<p>The strange path I have chosen is because this was only a test. However my home dir is shared on all nodes and the lib dir is not a simple simlink. I think that Thomas is right, I have to remove intel64 from Intel/lib path. Monday I will try.</p>

<p>Thank you again.</p>
<div class="gmail_quote">Il giorno 21/giu/2013 17:55, &quot;Gus Correa&quot; &lt;<a href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt; ha scritto:<br type="attribution"><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Hi Stefano<br>
<br>
Make sure your Intel compiler&#39;s shared libraries<br>
are accessible on all nodes.<br>
<br>
Is your /home directory shared across all nodes?<br>
How about /opt (if Intel is installed there)?<br>
<br>
By default Intel installs the compilers on /opt, which in typical<br>
clusters (and Linux distributions) is a local directory (to each node),<br>
not shared via NFS.<br>
Although you seem to have installed it somewhere else,<br>
/home/stefano/opt maybe, if /home/stefano/opt<br>
is just a soft link to /opt, not a real directory,<br>
that may not to do the trick across the cluster network.<br>
<br>
This error:<br>
<br>
&gt;&gt;         /home/stefano/opt/mpi/openmpi/<u></u>1.6.4/intel/bin/orted: error<br>
&gt;&gt;         while loading shared libraries: libimf.so: cannot open shared<br>
&gt;&gt;         object file: No such file or directory<br>
<br>
suggests something like that is going on (libimf.so is an<br>
*Intel shared library*, it is *not an Open MPI libary*).<br>
<br>
<br>
To have all needed tools (OpenMPI and Intel)<br>
available on all nodes, there are two typical solutions<br>
(by the way, see this FAQ: <a href="http://www.open-mpi.org/faq/?category=building#where-to-install" target="_blank">http://www.open-mpi.org/faq/?<u></u>category=building#where-to-<u></u>install</a>):<br>
<br>
1) Install them on all nodes, via RPM, or configure/make/install, or other mechanism.<br>
This is time consuming and costly to maintain, but scales well<br>
in big or small clusters.<br>
<br>
2)  Install them on your master/head/adminsitration/<u></u>storage node,<br>
and and share them via network (typicaly via NFS export/mount).<br>
This is easy to maintain, and scales well in small/medium clusters,<br>
but not so much on big ones.<br>
<br>
Make sure the Intel and MPI directories are either shared by<br>
or present/installed on all nodes.<br>
<br>
I also wonder if you really need these many environment variables:<br>
<br>
&gt;&gt; LD_LIBRARY_PATH=${MPI}/lib/<u></u>openmpi:${MPI}/lib:$LD_<u></u>LIBRARY_PATH<br>
&gt;&gt; export LD_RUN_PATH=${MPI}/lib/<u></u>openmpi:${MPI}/lib:$LD_RUN_<u></u>PATH<br>
<br>
or if that may be actually replaced by the simpler form:<br>
<br>
&gt;&gt; LD_LIBRARY_PATH=${MPI}/lib:$<u></u>LD_LIBRARY_PATH<br>
<br>
I hope it helps,<br>
Gus Correa<br>
<br>
<br>
<br>
On 06/21/2013 04:35 AM, Stefano Zaghi wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Wow... I think you are right... I will am check after the job I have<br>
just started will finish.<br>
<br>
Thank you again.<br>
<br>
See you soon<br>
<br>
Stefano Zaghi<br>
Ph.D. Aerospace Engineer,<br>
Research Scientist, Dept. of Computational Hydrodynamics at *CNR-INSEAN*<br>
&lt;<a href="http://www.insean.cnr.it/en/content/cnr-insean" target="_blank">http://www.insean.cnr.it/en/<u></u>content/cnr-insean</a>&gt;<br>
The Italian Ship Model Basin<br>
<a href="tel:%28%2B39%29%2006.50299297" value="+390650299297" target="_blank">(+39) 06.50299297</a> (Office)<br>
My codes:<br>
*OFF* &lt;<a href="https://github.com/szaghi/OFF" target="_blank">https://github.com/szaghi/OFF</a><u></u>&gt;, Open source Finite volumes Fluid<br>
dynamics code<br>
*Lib_VTK_IO* &lt;<a href="https://github.com/szaghi/Lib_VTK_IO" target="_blank">https://github.com/szaghi/<u></u>Lib_VTK_IO</a>&gt;, a Fortran library<br>
to write and read data conforming the VTK standard<br>
*IR_Precision* &lt;<a href="https://github.com/szaghi/IR_Precision" target="_blank">https://github.com/szaghi/IR_<u></u>Precision</a>&gt;, a Fortran<br>
(standard 2003) module to develop portable codes<br>
<br>
<br>
2013/6/21 &lt;<a href="mailto:thomas.forde@ulstein.com" target="_blank">thomas.forde@ulstein.com</a> &lt;mailto:<a href="mailto:thomas.forde@ulstein.com" target="_blank">thomas.forde@ulstein.<u></u>com</a>&gt;&gt;<br>
<br>
    hi Stefano<br>
<br>
    /home/stefano/opt/intel/2013.<u></u>4.183/lib/intel64/ is also the wrong<br>
    path, as the file is in ..183/lib/ and not ...183/lib/intel64/<br>
<br>
    is that why?<br>
    ./Thomas<br>
<br>
<br>
    Den 21. juni 2013 kl. 10:26 skrev &quot;Stefano Zaghi&quot;<br>
    &lt;<a href="mailto:stefano.zaghi@gmail.com" target="_blank">stefano.zaghi@gmail.com</a> &lt;mailto:<a href="mailto:stefano.zaghi@gmail.com" target="_blank">stefano.zaghi@gmail.<u></u>com</a>&gt;&gt;:<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
    Dear Thomas,<br>
    thank you again.<br>
<br>
    Symlink in /usr/lib64 is not enough, I have symlinked also<br>
    in /home/stefano/opt/mpi/openmpi/<u></u>1.6.4/intel/lib and, as expected,<br>
    not only libimf.so but also ibirng.so and libintlc.so.5 are necessary.<br>
<br>
    Now also remote runs works, but this is only a workaround, I still<br>
    not understand why mpirun do not find intel library even if<br>
    LD_LIBRARY_PATH contains also<br>
     /home/stefano/opt/intel/2013.<u></u>4.183/lib/intel64. Can you try<br>
    explain again?<br>
<br>
    Thank you very much.<br>
<br>
    Stefano Zaghi<br>
    Ph.D. Aerospace Engineer,<br>
    Research Scientist, Dept. of Computational Hydrodynamics at<br>
    *CNR-INSEAN* &lt;<a href="http://www.insean.cnr.it/en/content/cnr-insean" target="_blank">http://www.insean.cnr.it/en/<u></u>content/cnr-insean</a>&gt;<br>
    The Italian Ship Model Basin<br>
    <a href="tel:%28%2B39%29%2006.50299297" value="+390650299297" target="_blank">(+39) 06.50299297</a> (Office)<br>
    My codes:<br>
    *OFF* &lt;<a href="https://github.com/szaghi/OFF" target="_blank">https://github.com/szaghi/OFF</a><u></u>&gt;, Open source Finite volumes<br>
    Fluid dynamics code<br>
    *Lib_VTK_IO* &lt;<a href="https://github.com/szaghi/Lib_VTK_IO" target="_blank">https://github.com/szaghi/<u></u>Lib_VTK_IO</a>&gt;, a  would<br>
    Fortran library to write and read data conforming the VTK standard<br>
    *IR_Precision* &lt;<a href="https://github.com/szaghi/IR_Precision" target="_blank">https://github.com/szaghi/IR_<u></u>Precision</a>&gt;, a Fortran<br>
    (standard 2003) module to develop portable codes<br>
<br>
<br>
    2013/6/21 &lt;<a href="mailto:thomas.forde@ulstein.com" target="_blank">thomas.forde@ulstein.com</a> &lt;mailto:<a href="mailto:thomas.forde@ulstein.com" target="_blank">thomas.forde@ulstein.<u></u>com</a>&gt;&gt;<br>

<br>
        your settings are as following:<br>
        export MPI=/home/stefano/opt/mpi/<u></u>openmpi/1.6.4/intel<br>
        export PATH=${MPI}/bin:$PATH<br>
        export<br>
        LD_LIBRARY_PATH=${MPI}/lib/<u></u>openmpi:${MPI}/lib:$LD_<u></u>LIBRARY_PATH<br>
        export LD_RUN_PATH=${MPI}/lib/<u></u>openmpi:${MPI}/lib:$LD_RUN_<u></u>PATH<br>
<br>
        and your path to libimf.so file is<br>
        /home/stefano/opt/intel/2013.<u></u>4.183/lib/libimf.so<br>
<br>
        your export LD_LIbrary_PATH if i can decude it right would be<br>
        because you use the $MPI first.<br>
<br>
        /home/stefano/opt/mpi/openmpi/<u></u>1.64./intel/lib/openmpi and<br>
        /home/stefano/opt/mpi/openmpi/<u></u>1.64./intel/lib<br>
<br>
        as you can see it doesnt look for the files int he right place.<br>
<br>
        the simplest thing i would try is to symlink the libimf.so<br>
        file to /usr/lib64 and should give you a workaround.<br>
<br>
<br>
<br>
<br>
<br>
<br>
        From: Stefano Zaghi &lt;<a href="mailto:stefano.zaghi@gmail.com" target="_blank">stefano.zaghi@gmail.com</a><br>
        &lt;mailto:<a href="mailto:stefano.zaghi@gmail.com" target="_blank">stefano.zaghi@gmail.<u></u>com</a>&gt;&gt;<br>
        To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
        &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;&gt;,<br>
        Date: <a href="tel:21.06.2013%2009" value="+12106201309" target="_blank">21.06.2013 09</a>:45<br>
        Subject: Re: [OMPI users] OpenMPI 1.6.4 and Intel<br>
        Composer_xe_2013.4.183: problem with remote runs, orted: error<br>
        while loading shared libraries: libimf.so<br>
        Sent by: <a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a><br>
        &lt;mailto:<a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-<u></u>mpi.org</a>&gt;<br>
        ------------------------------<u></u>------------------------------<u></u>------------<br>
<br>
<br>
<br>
        Dear Thomas,<br>
<br>
        thank you very much for your very fast replay.<br>
<br>
        Yes I have that library in the correct place:<br>
<br>
        -rwxr-xr-x 1 stefano users 3.0M May 20 14:22<br>
        opt/intel/2013.4.183/lib/<u></u>intel64/libimf.so<br>
<br>
        Stefano Zaghi<br>
        Ph.D. Aerospace Engineer,<br>
        Research Scientist, Dept. of Computational Hydrodynamics at<br>
        *_CNR-INSEAN_* &lt;<a href="http://www.insean.cnr.it/en/content/cnr-insean" target="_blank">http://www.insean.cnr.it/en/<u></u>content/cnr-insean</a>&gt;<br>
        The Italian Ship Model Basin<br>
        <a href="tel:%28%2B39%29%2006.50299297" value="+390650299297" target="_blank">(+39) 06.50299297</a> (Office)<br>
        My codes:<br>
        *_OFF_* &lt;<a href="https://github.com/szaghi/OFF" target="_blank">https://github.com/szaghi/OFF</a><u></u>&gt;, Open source Finite<br>
        volumes Fluid dynamics code<br>
        *_Lib_VTK_IO_* &lt;<a href="https://github.com/szaghi/Lib_VTK_IO" target="_blank">https://github.com/szaghi/<u></u>Lib_VTK_IO</a>&gt;, a<br>
        Fortran library to write and read data conforming the VTK standard<br>
        *_IR_Precision_* &lt;<a href="https://github.com/szaghi/IR_Precision" target="_blank">https://github.com/szaghi/IR_<u></u>Precision</a>&gt;, a<br>
        Fortran (standard 2003) module to develop portable codes<br>
<br>
<br>
        2013/6/21 &lt;_thomas.forde@ulstein.com_<br>
        &lt;mailto:<a href="mailto:thomas.forde@ulstein.com" target="_blank">thomas.forde@ulstein.<u></u>com</a>&gt;&gt;<br>
        hi Stefano<br>
<br>
        your error message show that you are missing a shared library,<br>
        not necessary that library path is wrong.<br>
<br>
        do you actually have libimf.so, can you find the file on your<br>
        system.<br>
<br>
        ./Thomas<br>
<br>
<br>
<br>
<br>
        From: Stefano Zaghi &lt;_stefano.zaghi@gmail.com_<br>
        &lt;mailto:<a href="mailto:stefano.zaghi@gmail.com" target="_blank">stefano.zaghi@gmail.<u></u>com</a>&gt;&gt;<br>
        To: _users@open-mpi.org_ &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;,<br>
        Date: <a href="tel:21.06.2013%2009" value="+12106201309" target="_blank">21.06.2013 09</a>:27<br>
        Subject: [OMPI users] OpenMPI 1.6.4 and Intel<br>
        Composer_xe_2013.4.183: problem with remote runs, orted: error<br>
        while loading shared libraries: libimf.so<br>
        Sent by: _users-bounces@open-mpi.org_<br>
        &lt;mailto:<a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-<u></u>mpi.org</a>&gt;<br>
        ------------------------------<u></u>------------------------------<u></u>------------<br>
<br>
<br>
<br>
<br>
        Dear All,<br>
        I have compiled OpenMPI 1.6.4 with Intel Composer_xe_2013.4.183.<br>
<br>
        My configure is:<br>
<br>
        ./configure --prefix=/home/stefano/opt/<u></u>mpi/openmpi/1.6.4/intel<br>
        CC=icc CXX=icpc F77=ifort FC=ifort<br>
<br>
        Intel Composer has been installed in:<br>
<br>
        /home/stefano/opt/intel/2013.<u></u>4.183/composer_xe_2013.4.183<br>
<br>
        Into the .bashrc and .profile in all nodes there is:<br>
<br>
        source /home/stefano/opt/intel/2013.<u></u>4.183/bin/compilervars.sh<br>
        intel64<br>
        export MPI=/home/stefano/opt/mpi/<u></u>openmpi/1.6.4/intel<br>
        export PATH=${MPI}/bin:$PATH<br>
        export<br>
        LD_LIBRARY_PATH=${MPI}/lib/<u></u>openmpi:${MPI}/lib:$LD_<u></u>LIBRARY_PATH<br>
        export LD_RUN_PATH=${MPI}/lib/<u></u>openmpi:${MPI}/lib:$LD_RUN_<u></u>PATH<br>
<br>
        If I run parallel job into each single node (e.g. mpirun -np 8<br>
        myprog) all works well. However, when I tried to run parallel<br>
        job in more nodes of the cluster (remote runs) like the<br>
        following:<br>
<br>
        mpirun -np 16 --bynode --machinefile nodi.txt -x<br>
        LD_LIBRARY_PATH -x LD_RUN_PATH myprog<br>
<br>
        I got the following error:<br>
<br>
        /home/stefano/opt/mpi/openmpi/<u></u>1.6.4/intel/bin/orted: error<br>
        while loading shared libraries: libimf.so: cannot open shared<br>
        object file: No such file or directory<br>
<br>
        I have read many FAQs and online resources, all indicating<br>
        LD_LIBRARY_PATH as the possible problem (wrong setting).<br>
        However I am not able to figure out what is going wrong, the<br>
        LD_LIBRARY_PATH seems to set right in all nodes.<br>
<br>
        It is worth noting that in the same cluster I have successful<br>
        installed OpenMPI 1.4.3 with Intel Composer_xe_2011_sp1.6.233<br>
        following exactly the same procedure.<br>
<br>
        Thank you in advance for all suggestion,<br>
        sincerely<br>
<br>
        Stefano Zaghi<br>
        Ph.D. Aerospace Engineer,<br>
        Research Scientist, Dept. of Computational Hydrodynamics at<br>
        *_CNR-INSEAN_* &lt;<a href="http://www.insean.cnr.it/en/content/cnr-insean" target="_blank">http://www.insean.cnr.it/en/<u></u>content/cnr-insean</a>&gt;<br>
        The Italian Ship Model Basin<br>
        <a href="tel:%28%2B39%29%2006.50299297" value="+390650299297" target="_blank">(+39) 06.50299297</a> (Office)<br>
        My codes: _<br>
        _*_OFF_* &lt;<a href="https://github.com/szaghi/OFF" target="_blank">https://github.com/szaghi/OFF</a><u></u>&gt;, Open source Finite<br>
        volumes Fluid dynamics code _<br>
        _*_Lib_VTK_IO_* &lt;<a href="https://github.com/szaghi/Lib_VTK_IO" target="_blank">https://github.com/szaghi/<u></u>Lib_VTK_IO</a>&gt;, a<br>
        Fortran library to write and read data conforming the VTK<br>
        standard<br>
        *_IR_Precision_* &lt;<a href="https://github.com/szaghi/IR_Precision" target="_blank">https://github.com/szaghi/IR_<u></u>Precision</a>&gt;, a<br>
        Fortran (standard 2003) module to develop portable<br>
        codes_________________________<u></u>______________________<br>
        users mailing list_<br>
        __users@open-mpi.org_ &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;_<br>
        __<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users_" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users_</a><br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
        Denne e-posten kan innehalde informasjon som er konfidensiell<br>
        og/eller underlagt lovbestemt teieplikt. Kun den tiltenkte<br>
        adressat har adgang<br>
        til å lese eller vidareformidle denne e-posten eller<br>
        tilhøyrande vedlegg. Dersom De ikkje er den tiltenkte<br>
        mottakar, vennligst kontakt avsendar pr e-post, slett denne<br>
        e-posten med vedlegg og makuler samtlige utskrifter og kopiar<br>
        av den.<br>
<br>
        This e-mail may contain confidential information, or otherwise<br>
        be protected against unauthorised use. Any disclosure,<br>
        distribution or other use of the information by anyone but the<br>
        intended recipient is strictly prohibited.<br>
        If you have received this e-mail in error, please advise the<br>
        sender by immediate reply and destroy the received documents<br>
        and any copies hereof.<br>
<br>
<br>
        PBefore<br>
        printing, think about the environment<br>
<br>
<br>
<br>
        ______________________________<u></u>_________________<br>
        users mailing list_<br>
        __users@open-mpi.org_ &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;_<br>
        __<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users_" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users_</a><br>
        ______________________________<u></u>_________________<br>
        users mailing list<br>
        <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
        <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
        Denne e-posten kan innehalde informasjon som er konfidensiell<br>
        og/eller underlagt lovbestemt teieplikt. Kun den tiltenkte<br>
        adressat har adgang<br>
        til å lese eller vidareformidle denne e-posten eller<br>
        tilhøyrande vedlegg. Dersom De ikkje er den tiltenkte<br>
        mottakar, vennligst kontakt avsendar pr e-post, slett denne<br>
        e-posten med vedlegg og makuler samtlige utskrifter og kopiar<br>
        av den.<br>
<br>
        This e-mail may contain confidential information, or otherwise<br>
        be protected against unauthorised use. Any disclosure,<br>
        distribution or other use of the information by anyone but the<br>
        intended recipient is strictly prohibited.<br>
        If you have received this e-mail in error, please advise the<br>
        sender by immediate reply and destroy the received documents<br>
        and any copies hereof.<br>
<br>
<br>
        PBefore<br>
        printing, think about the environment<br>
<br>
<br>
<br>
<br>
        ______________________________<u></u>_________________<br>
        users mailing list<br>
        <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
        <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
<br>
<br>
    ______________________________<u></u>_________________<br>
    users mailing list<br>
    <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>  &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">htt<u></u>p://www.open-mpi.org/mailman/<u></u>listinfo.cgi/users</a><br>

</blockquote>
<br>
<br>
    Denne e-posten kan innehalde informasjon som er konfidensiell<br>
    og/eller underlagt lovbestemt teieplikt. Kun den tiltenkte adressat<br>
    har adgang til å lese eller vidareformidle denne e-posten eller<br>
    tilhøyrande vedlegg. Dersom De ikkje er den tiltenkte mottakar,<br>
    vennligst kontakt avsendar pr e-post, slett denne e-posten med<br>
    vedlegg og makuler samtlige utskrifter og kopiar av den.<br>
<br>
    This e-mail may contain confidential information, or otherwise be<br>
    protected against unauthorised use. Any disclosure, distribution or<br>
    other use of the information by anyone but the intended recipient is<br>
    strictly prohibited. If you have received this e-mail in error,<br>
    please advise the sender by immediate reply and destroy the received<br>
    documents and any copies hereof.<br>
<br>
    PBefore printing, think about the environment<br>
<br>
<br>
    ______________________________<u></u>_________________<br>
    users mailing list<br>
    <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
    <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
</blockquote>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
</blockquote></div>

