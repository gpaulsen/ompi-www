<div dir="ltr">Hello Ed,<div><br></div><div>Could you post the output of ompi_info?  It would also help to know which variant of the collective ops</div><div>your doing.  If you could post the output when you run with</div><div><br></div><div>mpirun --mca coll_base_verbose 10 &quot;other mpirun args you&#39;ve been using&quot; </div><div><br></div><div>that would be great</div><div><br></div><div>Also, if you know the sizes (number of elements) involved in the reduce and allreduce operations it</div><div>would be helpful to know this as well.</div><div><br></div><div>Thanks,</div><div><br></div><div>Howard</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">2014-09-25 3:34 GMT-06:00 Blosch, Edwin L <span dir="ltr">&lt;<a href="mailto:edwin.l.blosch@lmco.com" target="_blank">edwin.l.blosch@lmco.com</a>&gt;</span>:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">





<div lang="EN-US" link="blue" vlink="purple">
<div>
<p class="MsoNormal">I had an application suddenly stop making progress.  By killing the last process out of 208 processes, then looking at the stack trace, I found 3 of 208 processes were in an MPI_REDUCE call.  The other 205 had progressed in their execution
 to another routine, where they were waiting in an unrelated MPI_ALLREDUCE call.<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">The code structure is such that each processes calls MPI_REDUCE 5 times for different variables, then some work is done, then the MPI_ALLREDUCE call happens early in the next iteration of the solution procedure.  I thought it was also noteworthy
 that the 3 processes stuck at MPI_REDUCE, were actually stuck on the 4<sup>th</sup> of 5 MPI_REDUCE calls, not the 5<sup>th</sup> call.
<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">No issues with MVAPICH.  Problem easily solved by adding MPI_BARRIER after the section of MPI_REDUCE calls.<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">It seems like MPI_REDUCE has some kind of non-blocking implementation, and it was not safe to enter the MPI_ALLREDUCE while those MPI_REDUCE calls had not yet completed for other processes.<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">This was in OpenMPI 1.8.1.  Same problem seen on 3 slightly different systems, all QDR Infiniband, Mellanox HCAs, using a Mellanox OFED stack (slightly different versions on each cluster).  Intel compilers, again slightly different versions
 on each of the 3 systems.  <u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">Has anyone encountered anything similar?  While I have a workaround, I want to make sure the root cause of the deadlock gets fixed.  Please let me know what I can do to help.<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">Thanks,<u></u><u></u></p>
<p class="MsoNormal"><u></u> <u></u></p>
<p class="MsoNormal">Ed<u></u><u></u></p>
</div>
</div>

<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/09/25389.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/09/25389.php</a><br></blockquote></div><br></div>

