<div>So if I understand correctly, if a message size is smaller than it will use the MPI way (non-RDMA, 2 way communication), if its larger, then it would use the Open Fabrics, by using the ibverbs (and ofed stack) instead of using the MPI&#39;s stack?</div>

<div> </div><div>If so, could that be the reason why the MPI_Put &quot;hangs&quot; when sending a message more than 512KB (or may be 1MB)?<br></div><div>Also is there a way to know if for a particular MPI call, OF uses send/recv or RDMA exchange?<br>

</div><div class="gmail_quote">On Wed, Feb 29, 2012 at 11:36 AM, Jeffrey Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote style="margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid" class="gmail_quote">

<div class="im">On Feb 29, 2012, at 2:30 PM, Jingcha Joba wrote:<br>
<br>
&gt; Squyres,<br>
&gt; I thought RDMA read and write are implemented as one side communication using get and put respectively..<br>
&gt; Is it not so?<br>
<br>
</div>Yes and no.<br>
<br>
Keep in mind the difference between two things here:<br>
<br>
- An an underlying transport&#39;s one-sided capabilities (e.g., using InfiniBand RDMA reads/writes)<br>
- MPI one-sided and/or two-sided message passing<br>
<br>
Most OpenFabrics-capable MPI&#39;s use OF RDMA reads and writes for sending large messages (both one and two sided).  But it&#39;s not always the case.  For example, it may not be worth it to use RDMA for short messages because of the cost of registering memory, negotiating the target address for the RDMA read/write (which may require a round-tip ACK), etc.<br>


<br>
So OF-capable MPI&#39;s basically divorce the two issues.  The underlying transport will choose the &quot;best&quot; method (whether it&#39;s a send/recv style exchange, an RDMA-stle exchange, or a mixture of the two).<br>


<br>
Make sense?<br>
<div class="HOEnZb"><div class="h5"><br>
<br>
&gt; On Wed, Feb 29, 2012 at 10:49 AM, Jeffrey Squyres &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br>
&gt; FWIW, if Brian says that our one-sided stuff is a bit buggy, I believe him (because he wrote it).  :-)<br>
&gt;<br>
&gt; The fact is that the MPI-2 one-sided stuff is extremely complicated and somewhat open to interpretation.  In practice, I haven&#39;t seen the MPI-2 one-sided stuff used much in the wild.  The MPI-3 working group just revamped the one-sided support and generally made it much mo&#39;betta.  Brian is re-implementing that stuff, and I believe it&#39;ll also be much mo&#39;betta.<br>


&gt;<br>
&gt; My point: I wouldn&#39;t worry if not all one-sided benchmarks run with OMPI.  No one uses them (yet) anyway.<br>
&gt;<br>
&gt;<br>
&gt; On Feb 29, 2012, at 1:42 PM, Jingcha Joba wrote:<br>
&gt;<br>
&gt; &gt; When I ran my osu tests , I was able to get the numbers out of all the tests except latency_mt (which was obvious, as I didnt compile open-mpi with multi threaded support).<br>
&gt; &gt; A good way to know if the problem is with openmpi or with your custom OFED stack would be to use some other device like tcp instead of ib and rerun these one sided comm tests.<br>
&gt; &gt; On Wed, Feb 29, 2012 at 10:04 AM, Barrett, Brian W &lt;<a href="mailto:bwbarre@sandia.gov">bwbarre@sandia.gov</a>&gt; wrote:<br>
&gt; &gt; I&#39;m pretty sure that they are correct.  Our one-sided implementation is<br>
&gt; &gt; buggier than I&#39;d like (indeed, I&#39;m in the process of rewriting most of it<br>
&gt; &gt; as part of Open MPI&#39;s support for MPI-3&#39;s revised RDMA), so it&#39;s likely<br>
&gt; &gt; that the bugs are in Open MPI&#39;s onesided support.  Can you try a more<br>
&gt; &gt; recent release (something from the 1.5 tree) and see if the problem<br>
&gt; &gt; persists?<br>
&gt; &gt;<br>
&gt; &gt; Thanks,<br>
&gt; &gt;<br>
&gt; &gt; Brian<br>
&gt; &gt;<br>
&gt; &gt; On 2/29/12 10:56 AM, &quot;Jeffrey Squyres&quot; &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br>
&gt; &gt;<br>
&gt; &gt; &gt;FWIW, I&#39;m immediately suspicious of *any* MPI application that uses the<br>
&gt; &gt; &gt;MPI one-sided operations (i.e., MPI_PUT and MPI_GET).  It looks like<br>
&gt; &gt; &gt;these two OSU benchmarks are using those operations.<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;Is it known that these two benchmarks are correct?<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;On Feb 29, 2012, at 11:33 AM, Venkateswara Rao Dokku wrote:<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;&gt; Sorry, i forgot to introduce the system.. Ours is the customized OFED<br>
&gt; &gt; &gt;&gt;stack implemented to work on the specific hardware.. We tested the stack<br>
&gt; &gt; &gt;&gt;with the q-perf and Intel Benchmarks(IMB-3.2.2).. they went fine.. We<br>
&gt; &gt; &gt;&gt;want to execute the osu_benchamark3.1.1 suite on our OFED..<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt; On Wed, Feb 29, 2012 at 9:57 PM, Venkateswara Rao Dokku<br>
&gt; &gt; &gt;&gt;&lt;<a href="mailto:dvrao.584@gmail.com">dvrao.584@gmail.com</a>&gt; wrote:<br>
&gt; &gt; &gt;&gt; Hiii,<br>
&gt; &gt; &gt;&gt; I tried executing osu_benchamarks-3.1.1 suite with the openmpi-1.4.3...<br>
&gt; &gt; &gt;&gt;I could run 10 bench-mark tests (except osu_put_bibw,osu_put_bw,osu_<br>
&gt; &gt; &gt;&gt; get_bw,osu_latency_mt) out of 14 tests in the bench-mark suite... and<br>
&gt; &gt; &gt;&gt;the remaining tests are hanging at some message size.. the output is<br>
&gt; &gt; &gt;&gt;shown below<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt; [root@test2 ~]# mpirun --prefix /usr/local/ -np 2 --mca btl<br>
&gt; &gt; &gt;&gt;openib,self,sm -H 192.168.0.175,192.168.0.174 --mca<br>
&gt; &gt; &gt;&gt;orte_base_help_aggregate 0<br>
&gt; &gt; &gt;&gt;/root/ramu/ofed_pkgs/osu_benchmarks-3.1.1/osu_put_bibw<br>
&gt; &gt; &gt;&gt; failed to create doorbell file /dev/plx2_char_dev<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;-------------------------------------------------------------------------<br>
&gt; &gt; &gt;&gt;-<br>
&gt; &gt; &gt;&gt; WARNING: No preset parameters were found for the device that Open MPI<br>
&gt; &gt; &gt;&gt; detected:<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;   Local host:            test1<br>
&gt; &gt; &gt;&gt;   Device name:           plx2_0<br>
&gt; &gt; &gt;&gt;   Device vendor ID:      0x10b5<br>
&gt; &gt; &gt;&gt;   Device vendor part ID: 4277<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt; Default device parameters will be used, which may result in lower<br>
&gt; &gt; &gt;&gt; performance.  You can edit any of the files specified by the<br>
&gt; &gt; &gt;&gt; btl_openib_device_param_files MCA parameter to set values for your<br>
&gt; &gt; &gt;&gt; device.<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt; NOTE: You can turn off this warning by setting the MCA parameter<br>
&gt; &gt; &gt;&gt;       btl_openib_warn_no_device_params_found to 0.<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;-------------------------------------------------------------------------<br>
&gt; &gt; &gt;&gt;-<br>
&gt; &gt; &gt;&gt; failed to create doorbell file /dev/plx2_char_dev<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;-------------------------------------------------------------------------<br>
&gt; &gt; &gt;&gt;-<br>
&gt; &gt; &gt;&gt; WARNING: No preset parameters were found for the device that Open MPI<br>
&gt; &gt; &gt;&gt; detected:<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;   Local host:            test2<br>
&gt; &gt; &gt;&gt;   Device name:           plx2_0<br>
&gt; &gt; &gt;&gt;   Device vendor ID:      0x10b5<br>
&gt; &gt; &gt;&gt;   Device vendor part ID: 4277<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt; Default device parameters will be used, which may result in lower<br>
&gt; &gt; &gt;&gt; performance.  You can edit any of the files specified by the<br>
&gt; &gt; &gt;&gt; btl_openib_device_param_files MCA parameter to set values for your<br>
&gt; &gt; &gt;&gt; device.<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt; NOTE: You can turn off this warning by setting the MCA parameter<br>
&gt; &gt; &gt;&gt;       btl_openib_warn_no_device_params_found to 0.<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;-------------------------------------------------------------------------<br>
&gt; &gt; &gt;&gt;-<br>
&gt; &gt; &gt;&gt; alloc_srq max: 512 wqe_shift: 5<br>
&gt; &gt; &gt;&gt; alloc_srq max: 512 wqe_shift: 5<br>
&gt; &gt; &gt;&gt; alloc_srq max: 512 wqe_shift: 5<br>
&gt; &gt; &gt;&gt; alloc_srq max: 512 wqe_shift: 5<br>
&gt; &gt; &gt;&gt; alloc_srq max: 512 wqe_shift: 5<br>
&gt; &gt; &gt;&gt; alloc_srq max: 512 wqe_shift: 5<br>
&gt; &gt; &gt;&gt; # OSU One Sided MPI_Put Bi-directional Bandwidth Test v3.1.1<br>
&gt; &gt; &gt;&gt; # Size     Bi-Bandwidth (MB/s)<br>
&gt; &gt; &gt;&gt; plx2_create_qp line: 415<br>
&gt; &gt; &gt;&gt; plx2_create_qp line: 415<br>
&gt; &gt; &gt;&gt; plx2_create_qp line: 415<br>
&gt; &gt; &gt;&gt; plx2_create_qp line: 415<br>
&gt; &gt; &gt;&gt; 1                         0.00<br>
&gt; &gt; &gt;&gt; 2                         0.00<br>
&gt; &gt; &gt;&gt; 4                         0.01<br>
&gt; &gt; &gt;&gt; 8                         0.03<br>
&gt; &gt; &gt;&gt; 16                        0.07<br>
&gt; &gt; &gt;&gt; 32                        0.15<br>
&gt; &gt; &gt;&gt; 64                        0.11<br>
&gt; &gt; &gt;&gt; 128                       0.21<br>
&gt; &gt; &gt;&gt; 256                       0.43<br>
&gt; &gt; &gt;&gt; 512                       0.88<br>
&gt; &gt; &gt;&gt; 1024                      2.10<br>
&gt; &gt; &gt;&gt; 2048                      4.21<br>
&gt; &gt; &gt;&gt; 4096                      8.10<br>
&gt; &gt; &gt;&gt; 8192                     16.19<br>
&gt; &gt; &gt;&gt; 16384                     8.46<br>
&gt; &gt; &gt;&gt; 32768                    20.34<br>
&gt; &gt; &gt;&gt; 65536                    39.85<br>
&gt; &gt; &gt;&gt; 131072                   84.22<br>
&gt; &gt; &gt;&gt; 262144                  142.23<br>
&gt; &gt; &gt;&gt; 524288                  234.83<br>
&gt; &gt; &gt;&gt; mpirun: killing job...<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;-------------------------------------------------------------------------<br>
&gt; &gt; &gt;&gt;-<br>
&gt; &gt; &gt;&gt; mpirun noticed that process rank 0 with PID 7305 on node test2 exited<br>
&gt; &gt; &gt;&gt;on signal 0 (Unknown signal 0).<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;-------------------------------------------------------------------------<br>
&gt; &gt; &gt;&gt;-<br>
&gt; &gt; &gt;&gt; 2 total processes killed (some possibly by mpirun during cleanup)<br>
&gt; &gt; &gt;&gt; mpirun: clean termination accomplished<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt; [root@test2 ~]# mpirun --prefix /usr/local/ -np 2 --mca btl<br>
&gt; &gt; &gt;&gt;openib,self,sm -H 192.168.0.175,192.168.0.174 --mca<br>
&gt; &gt; &gt;&gt;orte_base_help_aggregate 0<br>
&gt; &gt; &gt;&gt;/root/ramu/ofed_pkgs/osu_benchmarks-3.1.1/osu_put_bw<br>
&gt; &gt; &gt;&gt; failed to create doorbell file /dev/plx2_char_dev<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;-------------------------------------------------------------------------<br>
&gt; &gt; &gt;&gt;-<br>
&gt; &gt; &gt;&gt; WARNING: No preset parameters were found for the device that Open MPI<br>
&gt; &gt; &gt;&gt; detected:<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;   Local host:            test1<br>
&gt; &gt; &gt;&gt;   Device name:           plx2_0<br>
&gt; &gt; &gt;&gt;   Device vendor ID:      0x10b5<br>
&gt; &gt; &gt;&gt;   Device vendor part ID: 4277<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt; Default device parameters will be used, which may result in lower<br>
&gt; &gt; &gt;&gt; performance.  You can edit any of the files specified by the<br>
&gt; &gt; &gt;&gt; btl_openib_device_param_files MCA parameter to set values for your<br>
&gt; &gt; &gt;&gt; device.<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt; NOTE: You can turn off this warning by setting the MCA parameter<br>
&gt; &gt; &gt;&gt;       btl_openib_warn_no_device_params_found to 0.<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;-------------------------------------------------------------------------<br>
&gt; &gt; &gt;&gt;-<br>
&gt; &gt; &gt;&gt; failed to create doorbell file /dev/plx2_char_dev<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;-------------------------------------------------------------------------<br>
&gt; &gt; &gt;&gt;-<br>
&gt; &gt; &gt;&gt; WARNING: No preset parameters were found for the device that Open MPI<br>
&gt; &gt; &gt;&gt; detected:<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;   Local host:            test2<br>
&gt; &gt; &gt;&gt;   Device name:           plx2_0<br>
&gt; &gt; &gt;&gt;   Device vendor ID:      0x10b5<br>
&gt; &gt; &gt;&gt;   Device vendor part ID: 4277<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt; Default device parameters will be used, which may result in lower<br>
&gt; &gt; &gt;&gt; performance.  You can edit any of the files specified by the<br>
&gt; &gt; &gt;&gt; btl_openib_device_param_files MCA parameter to set values for your<br>
&gt; &gt; &gt;&gt; device.<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt; NOTE: You can turn off this warning by setting the MCA parameter<br>
&gt; &gt; &gt;&gt;       btl_openib_warn_no_device_params_found to 0.<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;-------------------------------------------------------------------------<br>
&gt; &gt; &gt;&gt;-<br>
&gt; &gt; &gt;&gt; alloc_srq max: 512 wqe_shift: 5<br>
&gt; &gt; &gt;&gt; alloc_srq max: 512 wqe_shift: 5<br>
&gt; &gt; &gt;&gt; alloc_srq max: 512 wqe_shift: 5<br>
&gt; &gt; &gt;&gt; alloc_srq max: 512 wqe_shift: 5<br>
&gt; &gt; &gt;&gt; alloc_srq max: 512 wqe_shift: 5<br>
&gt; &gt; &gt;&gt; alloc_srq max: 512 wqe_shift: 5<br>
&gt; &gt; &gt;&gt; # OSU One Sided MPI_Put Bandwidth Test v3.1.1<br>
&gt; &gt; &gt;&gt; # Size        Bandwidth (MB/s)<br>
&gt; &gt; &gt;&gt; plx2_create_qp line: 415<br>
&gt; &gt; &gt;&gt; plx2_create_qp line: 415<br>
&gt; &gt; &gt;&gt; plx2_create_qp line: 415<br>
&gt; &gt; &gt;&gt; plx2_create_qp line: 415<br>
&gt; &gt; &gt;&gt; 1                         0.02<br>
&gt; &gt; &gt;&gt; 2                         0.05<br>
&gt; &gt; &gt;&gt; 4                         0.10<br>
&gt; &gt; &gt;&gt; 8                         0.19<br>
&gt; &gt; &gt;&gt; 16                        0.39<br>
&gt; &gt; &gt;&gt; 32                        0.77<br>
&gt; &gt; &gt;&gt; 64                        1.53<br>
&gt; &gt; &gt;&gt; 128                       2.57<br>
&gt; &gt; &gt;&gt; 256                       4.16<br>
&gt; &gt; &gt;&gt; 512                       8.30<br>
&gt; &gt; &gt;&gt; 1024                     16.62<br>
&gt; &gt; &gt;&gt; 2048                     33.22<br>
&gt; &gt; &gt;&gt; 4096                     66.51<br>
&gt; &gt; &gt;&gt; 8192                     42.45<br>
&gt; &gt; &gt;&gt; 16384                    11.99<br>
&gt; &gt; &gt;&gt; 32768                    18.20<br>
&gt; &gt; &gt;&gt; 65536                    76.04<br>
&gt; &gt; &gt;&gt; 131072                   98.64<br>
&gt; &gt; &gt;&gt; 262144                  407.66<br>
&gt; &gt; &gt;&gt; 524288                  489.84<br>
&gt; &gt; &gt;&gt; mpirun: killing job...<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;-------------------------------------------------------------------------<br>
&gt; &gt; &gt;&gt;-<br>
&gt; &gt; &gt;&gt; mpirun noticed that process rank 0 with PID 7314 on node test2 exited<br>
&gt; &gt; &gt;&gt;on signal 0 (Unknown signal 0).<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;-------------------------------------------------------------------------<br>
&gt; &gt; &gt;&gt;-<br>
&gt; &gt; &gt;&gt; 2 total processes killed (some possibly by mpirun during cleanup)<br>
&gt; &gt; &gt;&gt; mpirun: clean termination accomplished<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt; I even checked the logs but i couldn&#39;t see any errors...<br>
&gt; &gt; &gt;&gt; Could you suggest a way to overcome/debug this issue..<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt; Thanks for the kind reply..<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt; --<br>
&gt; &gt; &gt;&gt; Thanks &amp; Regards,<br>
&gt; &gt; &gt;&gt; D.Venkateswara Rao,<br>
&gt; &gt; &gt;&gt; Software Engineer,One Convergence Devices Pvt Ltd.,<br>
&gt; &gt; &gt;&gt; Jubille Hills,Hyderabad.<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt; --<br>
&gt; &gt; &gt;&gt; Thanks &amp; Regards,<br>
&gt; &gt; &gt;&gt; D.Venkateswara Rao,<br>
&gt; &gt; &gt;&gt; Software Engineer,One Convergence Devices Pvt Ltd.,<br>
&gt; &gt; &gt;&gt; Jubille Hills,Hyderabad.<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt; _______________________________________________<br>
&gt; &gt; &gt;&gt; users mailing list<br>
&gt; &gt; &gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; &gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;--<br>
&gt; &gt; &gt;Jeff Squyres<br>
&gt; &gt; &gt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; &gt; &gt;For corporate legal information go to:<br>
&gt; &gt; &gt;<a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;_______________________________________________<br>
&gt; &gt; &gt;users mailing list<br>
&gt; &gt; &gt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; &gt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; --<br>
&gt; &gt;  Brian W. Barrett<br>
&gt; &gt;  Dept. 1423: Scalable System Software<br>
&gt; &gt;  Sandia National Laboratories<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Jeff Squyres<br>
&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

