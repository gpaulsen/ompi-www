<div dir="ltr">I confess I am sorely puzzled. I replace the Info key with MPI_INFO_NULL, but still had to pass a bogus argument to master since you still have the Info_set code in there - otherwise, info_set segfaults due to a NULL argv[1]. Doing that (and replacing &quot;hostname&quot; with an MPI example code) makes everything work just fine.<div><br></div><div>I&#39;ve attached one of our example comm_spawn codes that we test against - it also works fine with the current head of the 1.8 code base. I confess that some changes have been made since 1.8.4 was released, and it is entirely possible that this was a problem in 1.8.4 and has since been fixed.</div><div><br></div><div>So I&#39;d suggest trying with the nightly 1.8 tarball and seeing if it works for you. You can download it from here:</div><div><br></div><div><a href="http://www.open-mpi.org/nightly/v1.8/">http://www.open-mpi.org/nightly/v1.8/</a><br></div><div><br></div><div>HTH</div><div>Ralph</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Tue, Feb 3, 2015 at 6:20 PM, Evan Samanas <span dir="ltr">&lt;<a href="mailto:evan.samanas@gmail.com" target="_blank">evan.samanas@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Yes, I did.  I replaced the info argument of MPI_Comm_spawn with MPI_INFO_NULL.<br></div><div class="gmail_extra"><br><div class="gmail_quote"><div><div class="h5">On Tue, Feb 3, 2015 at 5:54 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br></div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div class="h5"><div dir="ltr">When running your comm_spawn code, did you remove the Info key code? You wouldn&#39;t need to provide a hostfile or hosts any more, which is why it should resolve that problem.<div><br></div><div>I agree that providing either hostfile or host as an Info key will cause the program to segfault - I&#39;m woking on that issue.</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote"><div><div>On Tue, Feb 3, 2015 at 3:46 PM, Evan Samanas <span dir="ltr">&lt;<a href="mailto:evan.samanas@gmail.com" target="_blank">evan.samanas@gmail.com</a>&gt;</span> wrote:<br></div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div><div dir="ltr"><div>Setting these environment variables did indeed change the way mpirun maps things, and I didn&#39;t have to specify a hostfile.  However, setting these for my MPI_Comm_spawn code still resulted in the same segmentation fault.<br><br></div>Evan<br></div><div class="gmail_extra"><br><div class="gmail_quote"><div><div>On Tue, Feb 3, 2015 at 10:09 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br></div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div><div dir="ltr">If you add the following to your environment, you should run on multiple nodes:<div><br></div><div>OMPI_MCA_<span style="font-family:Menlo;font-size:11px">rmaps_base_mapping_policy=node</span></div><div><span style="font-family:Menlo;font-size:11px">OMPI_MCA_</span><span style="font-family:Menlo;font-size:11px">orte_default_hostfile=&lt;your hostfile&gt;</span></div><div><span style="font-family:Menlo;font-size:11px"><br></span></div><div><span style="font-family:Menlo;font-size:11px">The first tells OMPI to map-by node. The second passes in your default hostfile so you don&#39;t need to specify it as an Info key.</span></div><div><span style="font-family:Menlo;font-size:11px"><br></span></div><div><span style="font-family:Menlo;font-size:11px">HTH</span></div><div><span style="font-family:Menlo;font-size:11px">Ralph</span></div><div><span style="font-family:Menlo;font-size:11px"><br></span></div></div><div class="gmail_extra"><br><div class="gmail_quote"><div><div>On Tue, Feb 3, 2015 at 9:23 AM, Evan Samanas <span dir="ltr">&lt;<a href="mailto:evan.samanas@gmail.com" target="_blank">evan.samanas@gmail.com</a>&gt;</span> wrote:<br></div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div><div dir="ltr"><div><div><div>Hi Ralph,<br><br></div>Good to know you&#39;ve reproduced it.  I was experiencing this using both the hostfile and host key.  A simple comm_spawn was working for me as well, but it was only launching locally, and I&#39;m pretty sure each node only has 4 slots given past behavior (the mpirun -np 8 example I gave in my first email launches on both hosts).  Is there a way to specify the hosts I want to launch on without the hostfile or host key so I can test remote launch?<br><br></div>And to the &quot;hostname&quot; response...no wonder it was hanging!  I just constructed that as a basic example.  In my real use I&#39;m launching something that calls MPI_Init.<span><font color="#888888"><br><br></font></span></div><span><font color="#888888">Evan<br><div><div><div><div><br><br></div></div></div></div></font></span></div>
<br></div></div>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/02/26271.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/02/26271.php</a><br></blockquote></div><br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/02/26272.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/02/26272.php</a><br></blockquote></div><br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/02/26281.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/02/26281.php</a><br></blockquote></div><br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/02/26285.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/02/26285.php</a><br></blockquote></div><br></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/02/26286.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/02/26286.php</a><br></blockquote></div><br></div>

