You can try running using this script:<div><div>#!/bin/bash</div><div><br></div><div>s=$(($OMPI_COMM_WORLD_NODE_RANK))</div><div><br></div><div>numactl --physcpubind=$((s)) --localalloc ./YOUR_PROG</div><div><br></div><div>
instead of &#39;mpirun ... ./YOUR_PROG&#39; run &#39;mpirun ... ./SCRIPT</div><div><br></div><div>I tried this with openmpi-1.5.4 and it helped.</div><div><br></div><div>Best regards, Pavel Mezentsev</div><div><br></div><div>
P.S openmpi-1.5.5 bind processes correctly, so you can try it as well.</div><br><div class="gmail_quote">2012/3/30 Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;</span><br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
I think you&#39;d have much better luck using the developer&#39;s trunk as the binding there is much better - e.g., you can bind to NUMA instead of just cores. The 1.4 binding is pretty limited.<br>
<br>
<a href="http://www.open-mpi.org/nightly/trunk/" target="_blank">http://www.open-mpi.org/nightly/trunk/</a><br>
<div class="HOEnZb"><div class="h5"><br>
On Mar 30, 2012, at 5:02 AM, Ricardo Fonseca wrote:<br>
<br>
&gt; Hi guys<br>
&gt;<br>
&gt; I&#39;m benchmarking our (well tested) parallel code on and AMD based system, featuring 2x AMD Opteron(TM) Processor 6276, with 16 cores each for a total of 32 cores. The system is running Scientific Linux 6.1 and OpenMPI 1.4.5.<br>

&gt;<br>
&gt; When I run a single core job the performance is as expected. However, when I run with 32 processes the performance drops to about 60% (when compared with other systems running the exact same problem, so this is not a code scaling issue). I think this may have to do with core binding / NUMA, but I haven&#39;t been able to get any improvement out of the bind-* mpirun options.<br>

&gt;<br>
&gt; Any suggestions?<br>
&gt;<br>
&gt; Thanks in advance,<br>
&gt; Ricardo<br>
&gt;<br>
&gt; P.S: Here&#39;s the output of lscpu<br>
&gt;<br>
&gt; Architecture:          x86_64<br>
&gt; CPU op-mode(s):        32-bit, 64-bit<br>
&gt; Byte Order:            Little Endian<br>
&gt; CPU(s):                32<br>
&gt; On-line CPU(s) list:   0-31<br>
&gt; Thread(s) per core:    2<br>
&gt; Core(s) per socket:    8<br>
&gt; CPU socket(s):         2<br>
&gt; NUMA node(s):          4<br>
&gt; Vendor ID:             AuthenticAMD<br>
&gt; CPU family:            21<br>
&gt; Model:                 1<br>
&gt; Stepping:              2<br>
&gt; CPU MHz:               2300.045<br>
&gt; BogoMIPS:              4599.38<br>
&gt; Virtualization:        AMD-V<br>
&gt; L1d cache:             16K<br>
&gt; L1i cache:             64K<br>
&gt; L2 cache:              2048K<br>
&gt; L3 cache:              6144K<br>
&gt; NUMA node0 CPU(s):     0,2,4,6,8,10,12,14<br>
&gt; NUMA node1 CPU(s):     16,18,20,22,24,26,28,30<br>
&gt; NUMA node2 CPU(s):     1,3,5,7,9,11,13,15<br>
&gt; NUMA node3 CPU(s):     17,19,21,23,25,27,29,31<br>
&gt;<br>
&gt; ---<br>
&gt; Ricardo Fonseca<br>
&gt;<br>
&gt; Associate Professor<br>
&gt; GoLP - Grupo de Lasers e Plasmas<br>
&gt; Instituto de Plasmas e Fusão Nuclear<br>
&gt; Instituto Superior Técnico<br>
&gt; Av. Rovisco Pais<br>
&gt; 1049-001 Lisboa<br>
&gt; Portugal<br>
&gt;<br>
&gt; tel: +351 21 8419202<br>
&gt; fax: +351 21 8464455<br>
&gt; web: <a href="http://golp.ist.utl.pt/" target="_blank">http://golp.ist.utl.pt/</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div>

