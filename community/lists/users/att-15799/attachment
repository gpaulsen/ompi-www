Dear all,<br><br>I am doing some tests using MPI_Allgatherv function recently. After these tests, I found a wield problem of if.<br><br>When I wanted to use the MPI_Allgatherv function to gather a large number of data from some processes(for example, 2GB data per process). If the number of processes was even number, the function worked well and my network card can receive <b>and </b>send data at the same time at his max speed; But if the number of processes was odd number, the problem came, I found my network card can only receive <b>or </b>send data at the same time at the max speed.<br>
<br><b>My sample enviroments</b>: Openmpi 1.4.3, Linux 2.6.32<br><br><b>My source codes</b>:<br>
<br>
int main(int argc, char **argv)<br>
{<br>
    char *psend_buf, *precv_buf;<br>
    int rank;<br>
    int process_cnt;<br>
    int repeat_time;<br>
<br>
    int *pele_cnt;<br>
    int *pdipal;<br>
<br>
    long buf_size;      // Assume the long keyword is 64-bits width<br>
<br>
    buf_size = 2000;    // 2000MB<br>
    repeat_time = 1; <br>
<br>
    MPI_Init(&amp;argc, &amp;argv);<br>
    MPI_Datatype mpi_meta;<br>
<br>
    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);<br>
    MPI_Comm_size(MPI_COMM_WORLD, &amp;process_cnt);<br>
<br>
    precv_buf = (char*)malloc(buf_size * process_cnt * 1024 * 1024);<br>
    psend_buf = (char*)malloc(buf_size * 1024 * 1024);<br>
    memset(precv_buf, 0, buf_size * process_cnt * 1024 * 1024);<br>
    memset(psend_buf, 1, buf_size * 1024 * 1024);<br>
<br>
    // new data type: 1MB per unit<br>
    MPI_Type_contiguous(1024 * 1024, MPI_CHAR, &amp;mpi_meta);<br>
    MPI_Type_commit(&amp;mpi_meta);<br>
<br>
    pele_cnt = (int*)malloc(sizeof(int) * process_cnt);<br>
    pdipal = (int*)malloc(sizeof(int) * process_cnt);<br>
<br>
    for (int i = 0; i &lt; process_cnt; i++)<br>
    {  <br>
        pele_cnt[i] = buf_size;<br>
        pdipal[i] = i * buf_size;<br>
    }  <br><br>    for (int i = 0; i &lt; repeat_time; i++)<br>    {<br>        MPI_Allgatherv(psend_buf, buf_size, mpi_meta, precv_buf, pele_cnt, pdipal, mpi_meta, \<br>                MPI_COMM_WORLD);<br>    }<br><br>    printf(&quot;rank %d, used time = %.3f\n&quot;, rank, totle_time);<br>
<br>    free(psend_buf);<br>    free(precv_buf);<br><br>    free(pele_cnt);<br>    free(pdipal);<br><br>    MPI_Type_free(&amp;mpi_meta);<br>    MPI_Finalize();<br>}<br><br><br>I guess the problem is because of the implementation of the algorithm of MPI_Allgather. Did anybody meet the same problem and have any suggestions for me? Thanks<br>
<br>Best Regards<br>Xianjun<br><br><br><br>

