<html><head><meta http-equiv="Content-Type" content="text/html charset=windows-1252"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">Perhaps it would help if you could give us some idea of the interest here? The prior Mesos integration was done as an academic project, which is why it died once the student graduated.<div class=""><br class=""></div><div class="">Is there some long-term interest here? Or is this part of an academic effort?</div><div class=""><br class=""></div><div class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Jun 5, 2016, at 7:22 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" class="">rhc@open-mpi.org</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><div style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><blockquote type="cite" class=""><div class=""><br class="Apple-interchange-newline">On Jun 5, 2016, at 4:30 PM, Du, Fan &lt;<a href="mailto:fan.du@intel.com" class="">fan.du@intel.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><div class="">Thanks for your reply!<br class=""><br class="">On 2016/6/5 3:01, Ralph Castain wrote:<br class=""><blockquote type="cite" class="">The closest thing we have to what you describe is the “orte-dvm” - this<br class="">allows one to launch a persistent collection of daemons. You can then<br class="">run your applications against it using “mpiexec -hnp &lt;url&gt;” where the<br class="">url is that of the orte-dvm “head” daemon.<br class=""></blockquote><br class="">I tried this, maybe I miss something.<br class=""><br class="">On host1:<br class="">orte-dvm &nbsp;--allow-run-as-root<br class="">VMURI: 2783903744.0;<a href="tcp://192.168.10.55:47325" class="">tcp://192.168.10.55:47325</a><br class="">DVM ready<br class=""><br class="">On host2:<br class="">mpiexec -hnp 2783903744.0;<a href="tcp://192.168.10.55:47325" class="">tcp://192.168.10.55:47325</a><br class=""></div></div></blockquote><div class=""><br class=""></div>Your shell will take the semi-colon to mean the end of the line - you have to enclose it all in quotes</div><div style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><br class=""><blockquote type="cite" class=""><div class=""><div class="">OMPI_MCA_orte_hnp_uri=2783903744.0<br class="">OMPI_MCA_ess=tool<br class="">[grantleyIPDC01:03305] [[21695,0],0] ORTE_ERROR_LOG: Bad parameter in file base/rml_base_contact.c at line 161<br class="">-bash:<span class="Apple-converted-space">&nbsp;</span><a href="tcp://192.168.10.55:47325:" class="">tcp://192.168.10.55:47325:</a><span class="Apple-converted-space">&nbsp;</span>No such file or directory<br class=""><br class="">digging the code a bit deeper, the uri expects to have job id and rank id.<br class="">and how to make the subsequent orte-dvm know where the head orte-dvm is?<br class="">I checked orte-dvm help, seems no such option there.<br class=""><br class=""><blockquote type="cite" class="">If I understand you correctly, however, then you would want the orte-dvm<br class="">to assemble itself based on the asynchronous start of the individual<br class="">daemons. In other words, Mesos would start a daemon on each node as that<br class="">node became available. Then, once all the daemons have been started,<br class="">Mesos would execute “mpiexec” to start the application.<br class=""><br class="">Is that correct?<br class=""></blockquote><br class="">Yes<br class=""><br class=""><blockquote type="cite" class="">If so, then we don’t support that mode today, but it could fairly easily<br class="">be added.<br class="">However, I don’t see why you couldn’t just write a small<br class="">standalone tool that collects all the Mesos resources in a file until<br class="">all have been assembled, and then executes “mpiexec -hostfile &lt;myfile&gt;”.<br class=""></blockquote><br class="">Because, mpiexec will eventually rely ssh to run mpi proxy on hosts,</div></div></blockquote><div class=""><br class=""></div>What’s the problem with that? It’s how many HPC clusters work. Is ssh not enabled?</div><div style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><br class=""><blockquote type="cite" class=""><div class=""><div class="">while<br class="">in Mesos, it works like: framework passes information about on which host<br class="">to run what commands, and pass such information to Mesos master, then Mesos<br class="">master will instruct hosts to run commands.<br class=""><br class="">This is where Mesos work module doesn't fit into Open MPI.<br class=""></div></div></blockquote><div class=""><br class=""></div>Easiest thing would be to add a Mesos PLM plugin to OMPI - IIRC, someone once did that, but nobody was interested and so it died</div><div style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><br class=""></div><div style="font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=""><br class=""><blockquote type="cite" class=""><div class=""><div class=""><br class=""><blockquote type="cite" class="">Is there some reason this won’t work? It would be much simpler and would<br class="">work with any MPI.<br class=""><br class="">Ralph<br class=""><br class=""><br class=""><blockquote type="cite" class="">On Jun 3, 2016, at 5:10 PM, Du, Fan &lt;<a href="mailto:fan.du@intel.com" class="">fan.du@intel.com</a><br class="">&lt;<a href="mailto:fan.du@intel.com" class="">mailto:fan.du@intel.com</a>&gt;&gt; wrote:<br class=""><br class=""><br class=""><br class="">On 2016/6/2 19:14, Gilles Gouaillardet wrote:<br class=""><blockquote type="cite" class="">Hi,<br class=""><br class="">may I ask why you need/want to launch orted manually ?<br class=""></blockquote><br class="">Good question.<br class=""><br class="">The intention is to get orted commands, and run orted with Mesos.<br class="">This all comes from who Mesos works, in essence it offers<br class="">resources(cpu/memory/ports)<br class="">in a per host basis to framework, framework then builds information of<br class="">how to run<br class="">specific tasks, and pass those information to Mesos master, at last<br class="">Mesos will<br class="">instructs hosts to execute the framework tasks.<br class=""><br class="">Take MPICH2 as example, the framework to support MPICH2 works as above.<br class="">1. framework gets offers from Mesos master, and tells the Mesos master<br class="">to run a wrapper<br class="">of MPICH2 proxy(hydra_pmi_proxy), at this time, the wrapper waits for<br class="">commands to<br class="">execute the proxy.<br class=""><br class="">2. After launch enough MPICH2 proxy wrapper on hosts as user expect,<br class="">then run the<br class="">real mpiexec program with '-launcher manual' to grab commands for the<br class="">proxy, then<br class="">pass those commands to the proxy wrapper, so finally the real MPICH2<br class="">proxy got launched,<br class="">and mpiexec will proceed on normally.<br class=""><br class="">That's why I'm looking for similar functionality as '-launcher manual<br class="">MPICH2.<br class="">Non native speaker, I hope I tell the story clear :)<br class=""><br class=""><br class=""><br class=""><blockquote type="cite" class="">unless you are running under a batch manager, Open MPI uses the rsh pml<br class="">to remotely start orted.<br class="">basically, it does<br class="">ssh host orted &lt;orted params&gt;<br class=""><br class="">the best I can suggest is you do<br class=""><br class="">mpirun --mca orte_rsh_agent myrshagent.sh --mca orte_launch_agent<br class="">mylaunchagent.sh &nbsp;...<br class="">under the hood, mpirun will do<br class="">myrshagent.sh host mylaunchagent.sh &lt;orted params&gt;<br class=""><br class="">Cheers,<br class=""><br class="">Gilles<br class=""><br class="">On Thursday, June 2, 2016, Du, Fan &lt;<a href="mailto:fan.du@intel.com" class="">fan.du@intel.com</a><br class="">&lt;<a href="mailto:fan.du@intel.com" class="">mailto:fan.du@intel.com</a>&gt;<br class="">&lt;<a href="mailto:fan.du@intel.com" class="">mailto:fan.du@intel.com</a>&gt;&gt; wrote:<br class=""><br class="">&nbsp;&nbsp;Hi folks<br class=""><br class="">&nbsp;&nbsp;Starting from Open MPI, I can launch mpi application a.out as<br class="">&nbsp;&nbsp;following on host1<br class="">&nbsp;&nbsp;mpirun --allow-run-as-root --host host1,host2 -np 4 /tmp/a.out<br class=""><br class="">&nbsp;&nbsp;On host2, I saw an proxy, say orted here is spawned:<br class="">&nbsp;&nbsp;orted --hnp-topo-sig 4N:2S:4L3:20L2:20L1:20C:40H:x86_64 -mca ess env<br class="">&nbsp;&nbsp;-mca orte_ess_jobid 1275133952 -mca orte_ess_vpid 1 -mca<br class="">&nbsp;&nbsp;orte_ess_num_procs 2 -mca orte_hnp_uri<br class="">&nbsp;&nbsp;1275133952.0;<a href="tcp://host1_ip:40024" class="">tcp://host1_ip:40024</a><span class="Apple-converted-space">&nbsp;</span>--tree-spawn -mca plm rsh<br class="">--tree-spawn<br class=""><br class="">&nbsp;&nbsp;It seems mpirun use ssh as launcher on my system.<br class="">&nbsp;&nbsp;What if I want to run orted things manually, not by mpirun<br class="">&nbsp;&nbsp;automatically,<br class="">&nbsp;&nbsp;I mean, does mpirun has any option to produce commands for orted?<br class=""><br class="">&nbsp;&nbsp;As for MPICH2 implementation, there is "-launcher manual" option to<br class="">&nbsp;&nbsp;make this works,<br class="">&nbsp;&nbsp;for example:<br class="">&nbsp;&nbsp;# mpiexec.hydra -launcher manual -np 4 htop<br class="">&nbsp;&nbsp;HYDRA_LAUNCH: /usr/local/bin/hydra_pmi_proxy --control-port<br class="">&nbsp;&nbsp;grantleyIPDC04:34652 --rmk user --launcher manual --demux poll<br class="">&nbsp;&nbsp;--pgid 0 --retries 10 --usize -2 --proxy-id 0<br class="">&nbsp;&nbsp;HYDRA_LAUNCH_END<br class=""><br class="">&nbsp;&nbsp;Then I can manually run hydra_pmi_proxy with commands, and<br class="">&nbsp;&nbsp;mpiexec.hydra will proceed on.<br class=""><br class="">&nbsp;&nbsp;Thanks!<br class="">&nbsp;&nbsp;_______________________________________________<br class="">&nbsp;&nbsp;users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><span class="Apple-converted-space">&nbsp;</span>&lt;<a href="mailto:users@open-mpi.org" class="">mailto:users@open-mpi.org</a>&gt;<br class="">&nbsp;&nbsp;Subscription:<span class="Apple-converted-space">&nbsp;</span><a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" class="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">&nbsp;&nbsp;Link to this post:<br class=""><a href="http://www.open-mpi.org/community/lists/users/2016/06/29346.php" class="">http://www.open-mpi.org/community/lists/users/2016/06/29346.php</a><br class=""><br class=""><br class=""><br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a> &lt;<a href="mailto:users@open-mpi.org" class="">mailto:users@open-mpi.org</a>&gt;<br class="">Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" class="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this post:<br class=""><a href="http://www.open-mpi.org/community/lists/users/2016/06/29347.php" class="">http://www.open-mpi.org/community/lists/users/2016/06/29347.php</a><br class=""><br class=""></blockquote>_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><span class="Apple-converted-space">&nbsp;</span>&lt;<a href="mailto:users@open-mpi.org" class="">mailto:users@open-mpi.org</a>&gt;<br class="">Subscription:https://<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" class="">www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this<br class="">post:http://<a href="http://www.open-mpi.org/community/lists/users/2016/06/29364.php" class="">www.open-mpi.org/community/lists/users/2016/06/29364.php</a><br class=""></blockquote><br class=""><br class=""><br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" class="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/06/29367.php" class="">http://www.open-mpi.org/community/lists/users/2016/06/29367.php</a><br class=""><br class=""></blockquote>_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" class="">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/06/29375.php" class="">http://www.open-mpi.org/community/lists/users/2016/06/29375.php</a></div></div></blockquote></div></div></blockquote></div><br class=""></div></body></html>
