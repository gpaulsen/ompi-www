Dear Bart,<div><br></div><div>I think OpenMPI don&#39;t need to be installed on all machines because they are NFS shared with the master node. I don&#39;t know how to check output of which orted, it is running just on the master node. I have another application which is running similarly but I am having problem with WRF.<br>
<br><div class="gmail_quote">On Tue, May 3, 2011 at 9:06 PM, Bart Brashers <span dir="ltr">&lt;<a href="mailto:bbrashers@environcorp.com">bbrashers@environcorp.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
<div lang="EN-US" link="blue" vlink="purple"><div><p class="MsoNormal"><span style="font-size:11.0pt;color:#1F497D">It looks like OpenMPI is not installed on all your execution machines.  You need to install at least the libs on all machines, or on an NFS-shared location.  Check the output of &quot;which orted&quot; on the machine that works.</span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;color:#1F497D"> </span></p><p class="MsoNormal"><span style="font-size:11.0pt;color:#1F497D">Bart</span></p><p class="MsoNormal"><span style="font-size:11.0pt;color:#1F497D"> </span></p>
<p class="MsoNormal"><b><span style="font-size:10.0pt">From:</span></b><span style="font-size:10.0pt"> <a href="mailto:wrf-users-bounces@ucar.edu" target="_blank">wrf-users-bounces@ucar.edu</a> [mailto:<a href="mailto:wrf-users-bounces@ucar.edu" target="_blank">wrf-users-bounces@ucar.edu</a>] <b>On Behalf Of </b>Ahsan Ali<br>
<b>Sent:</b> Tuesday, May 03, 2011 1:04 AM<br><b>To:</b> <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><b>Subject:</b> [Wrf-users] WRF Problem running in Parallel on multiple nodes(cluster)</span></p>
<div><div></div><div class="h5"><p class="MsoNormal"> </p><p class="MsoNormal">Hello,</p><div><p class="MsoNormal"> </p></div><div><p class="MsoNormal">I am able to run WRFV3.2.1 using mpirun on multiple cores of single machine, but when I want to run it across multiple nodes in cluster using hostlist then I get error, The compute nodes are mounted with the master node during boot using NFS. I get following error. Please help.</p>
</div><div><p class="MsoNormal"> </p></div><div><div><p class="MsoNormal">[root@pmd02 em_real]# mpirun -np 10 -hostfile /home/pmdtest/hostlist ./real.exe</p></div><div><p class="MsoNormal">bash: orted: command not found</p>
</div><div><p class="MsoNormal">bash: orted: command not found</p></div><div><p class="MsoNormal">--------------------------------------------------------------------------</p></div><div><p class="MsoNormal">A daemon (pid 22006) died unexpectedly with status 127 while attempting</p>
</div><div><p class="MsoNormal">to launch so we are aborting.</p></div><div><p class="MsoNormal"> </p></div><div><p class="MsoNormal">There may be more information reported by the environment (see above).</p></div><div><p class="MsoNormal">
 </p></div><div><p class="MsoNormal">This may be because the daemon was unable to find all the needed shared</p></div><div><p class="MsoNormal">libraries on the remote node. You may set your LD_LIBRARY_PATH to have the</p>
</div><div><p class="MsoNormal">location of the shared libraries on the remote nodes and this will</p></div><div><p class="MsoNormal">automatically be forwarded to the remote nodes.</p></div><div><p class="MsoNormal">--------------------------------------------------------------------------</p>
</div><div><p class="MsoNormal">--------------------------------------------------------------------------</p></div><div><p class="MsoNormal">mpirun noticed that the job aborted, but has no info as to the process</p></div>
<div><p class="MsoNormal">that caused that situation.</p></div><div><p class="MsoNormal">--------------------------------------------------------------------------</p></div><div><p class="MsoNormal">mpirun: clean termination accomplished</p>
</div><div><p class="MsoNormal"> </p></div><p class="MsoNormal"><br>-- <br>Syed Ahsan Ali Bokhari <br>Electronic Engineer (EE)</p><div><p class="MsoNormal"><br>Research &amp; Development Division<br>Pakistan Meteorological Department H-8/4, Islamabad.<br>
Phone # off  +92518358714</p></div><div><p class="MsoNormal">Cell # +923155145014</p></div><p class="MsoNormal"> </p></div></div></div></div>
<div><p></p><hr>
This message contains information that may be confidential, privileged or otherwise protected by law from disclosure. It is intended for the exclusive use of the Addressee(s). Unless you are the addressee or authorized agent of the addressee, you may not review, copy, distribute or disclose to anyone the message or any information contained within. If you have received this message in error, please contact the sender by electronic reply to <a href="mailto:email@environcorp.com" target="_blank">email@environcorp.com</a> and immediately delete all copies of the message.
<p></p></div>
</div></blockquote></div><br><br clear="all"><br>-- <br>Syed Ahsan Ali Bokhari <br>Electronic Engineer (EE)<div><br>Research &amp; Development Division<br>Pakistan Meteorological Department H-8/4, Islamabad.<br>Phone # off  +92518358714</div>
<div>Cell # +923155145014<br></div><br>
</div>

