<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">It is very hard to debug the problem with so little information. We regularly run OMPI jobs on Torque without issue.<div><br></div><div>Are you getting an allocation from somewhere for the nodes? If so, are you using Moab to get it? Do you have a $PBS_NODEFILE in your environment?</div><div><br></div><div>I have no idea why your processes are crashing when run via Torque - are you sure that the processes themselves crash? Are they segfaulting - if so, can you use gdb to find out where?</div><div><br></div><div>Information would be most helpful - the information we really need is specified here:&nbsp;<a href="http://www.open-mpi.org/community/help/">http://www.open-mpi.org/community/help/</a></div><div><br></div><div>Thanks</div><div>Ralph</div><div><br></div><div><br><div><div>On Mar 31, 2009, at 5:50 PM, Rahul Nabar wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div>I've a strange OpenMPI/Torque problem while trying to run a job on our<br>Opteron-SC-1435 based cluster:<br><br>Each node has 8 cpus.<br><br>If I got to a node and run like so then the job works:<br><br>mpirun -np 6 ${EXE_PATH}/${DACAPOEXE_PAR} ${ARGS}<br><br>Same job if I submit through PBS/Torque then it starts running but the<br>individual processes keep crashing:<br><br>mpirun -np 6 ${EXE_PATH}/${DACAPOEXE_PAR} ${ARGS}<br><br>I know that the --hostfile directive is not needed in the latest<br>torque-OpenMPI jobs.<br><br>I also tried including:<br><br>mpirun -np 6 --hosts node17,node17,node17,node17,node17,node17<br>${EXE_PATH}/${DACAPOEXE_PAR} ${ARGS}<br><br>Still does not work.<br><br>What could be going wrong? Are there other things I need to worry<br>about when PBS steps in? Any tips?<br><br>The ${DACAPOEXE_PAR} refers to a fortran executable for the<br>computational chemistry code DACAPO.<br><br>What;s the differences between submitting a job on a node via mpirun<br>directly vs via Torque. Shouldn't these both be transparent to the<br>fortran calls. I am assuming don't have to dig into the fortran code.<br>Any debug tips?<br><br>Thanks!<br><br>--<br>Rahul<br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></div></blockquote></div><br></div></body></html>
