<div dir="ltr"><br><div class="gmail_quote">On Thu, Oct 9, 2008 at 10:13 AM, Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<div class="Ih2E3d">On Oct 9, 2008, at 8:06 AM, Sangamesh B wrote:<br></div><div class="Ih2E3d"><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
OpenMPI : 120m 6s<br>
MPICH2 : &nbsp;67m 44s<br>
</blockquote>
<br></div>
That seems to indicate that something else is going on -- with -np 1, there should be no MPI communication, right? &nbsp;I wonder if the memory allocator performance is coming into play here.</blockquote><div><br>&nbsp; I&#39;d be more inclined to double-check how the Gromacs app is being compiled in the first place - I wouldn&#39;t think the OpenMPI memory allocator would make anywhere near that much difference.&nbsp; Sangamesh, do you know what command line was used to compile both of these?&nbsp; Someone correct me if I&#39;m wrong, but <i>if</i> MPICH2 embeds optimization flags in the &#39;mpicc&#39; command and OpenMPI does not, then if he&#39;s not specifying any optimization flags in the compilation of Gromacs, MPICH2 will pass its embedded ones on to the Gromacs compile and be faster.&nbsp; I&#39;m rusty on my GCC, too, though - does it default to an O2 level, or does it default to no optimizations?<br>
<br>&nbsp; Since the benchmark is readily available, I&#39;ll try running it later today.. didn&#39;t get a chance last night.<br><br>&nbsp; Cheers,<br>&nbsp; - Brian<br><br></div></div><br></div>

