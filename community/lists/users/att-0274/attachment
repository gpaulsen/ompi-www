<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta content="text/html;charset=ISO-8859-1" http-equiv="Content-Type">
  <title></title>
</head>
<body bgcolor="#ffffff" text="#000000">
<pre wrap="">Message: 2
Date: Wed, 2 Nov 2005 17:28:33 -0500
From: Jeff Squyres <a class="moz-txt-link-rfc2396E"
 href="mailto:jsquyres@open-mpi.org">&lt;jsquyres@open-mpi.org&gt;</a>
Subject: Re: [O-MPI users] HPL and OpenMpi RC3
To: Open MPI Users <a class="moz-txt-link-rfc2396E"
 href="mailto:users@open-mpi.org">&lt;users@open-mpi.org&gt;</a>
Message-ID: <a class="moz-txt-link-rfc2396E"
 href="mailto:10BFC5D6-F68C-46F0-A984-5C8710675811@open-mpi.org">&lt;10BFC5D6-F68C-46F0-A984-5C8710675811@open-mpi.org&gt;</a>
Content-Type: text/plain; charset=US-ASCII; delsp=yes; format=flowed

Allan --

We have been unable to reproduce this bad TCP performance behavior.   
Indeed, in our runs, TEG TCP is performing slower than OB1 TCP.

Sidenote: is there any reason you're supplying the pls_rsh_orted MCA  
parameter on the command line?  It shouldn't really be necessary if  
OMPI is in your PATH (although you may need to add it to your PATH in  
your shell startup files, or use the --prefix option -- see <a
 class="moz-txt-link-freetext" href="http://">http://</a> 
<a class="moz-txt-link-abbreviated"
 href="http://www.open-mpi.org/faq/?category=running#adding-ompi-to-path">www.open-mpi.org/faq/?category=running#adding-ompi-to-path</a> and <a
 class="moz-txt-link-freetext" href="http://">http://</a> 
<a class="moz-txt-link-abbreviated"
 href="http://www.open-mpi.org/faq/?category=running#mpirun-prefix">www.open-mpi.org/faq/?category=running#mpirun-prefix</a>).

Some followup questions:

1. Do you only have one TCP NIC on each node?
2. Are you running HPL in a size that is not going to thrash your  
memory? (I'm guessing not, since teg runs were ok, but just to be  
sure...)
3. Is anyone else running on these nodes at the same time?  (Again,  
I'm assuming no, but just want to be sure)
4. Can you try this again with the latest v1.0 snapshot?  (<a
 class="moz-txt-link-freetext" href="http://">http://</a> 
<a class="moz-txt-link-abbreviated"
 href="http://www.open-mpi.org/nightly/v1.0/">www.open-mpi.org/nightly/v1.0/</a>)

Thanks!

Hi Jeff,
Answers to the above Questions:
1. No, I have 4 NICs on the head node and two on each of the 15 other compute nodes. I use the realtek 8169 gigabit ethernet cards on the compute nodes as eth1 or eth0(one only) connected to a gigabit ethernet switch with bisection bandwidth of 32Gbps and a sk98lin driver 3Com built in gigabit ethernet NIC card on the head node(eth3). The other ethernet cards 10/100M on the head node handle a network laser printer(eth0) and eth2 (10/100M) internet access. Eth1 is a spare 10/100M which I can remove. The compute nodes each have two ethernet cards one 10/100Mbps ethernet not connected to anything(built in to M/B) and a PCI realtek 8169 gigabit ethernet connected to the TCP network LAN(Gigabit). When I tried it without the switches -mca pml teg the maximum performace I would get with it was 9GFlops for P=4 Q=4 N=approx 12- 16 thousand and NB ridiculously low at 10 block size. If I tried bigger block sizes it would run for along time for large N ~ 16,000 unless I killed xhpl. I u
se atlas BLAS 3.7.11 libs compiled for each node and linked to HPL when creating xhpl. I also use open mpi mpicc in the hpl make file for compile and link both.
   Maybe I should according to the new faq use the TCP switch to use eth3 on the head node?
2. I have 512MB of memory per node which is 8 GB total, so I can safely go upto N=22,000 24,000. I used sizes of 22000 for TCP teg and did not run into problems. But if I do not specify the switches suggested by Tim I get bad performance for N = 12000.
3. No , just me.
4. My cluster is an experimental Basement Cluster [BSquared = Brampton Beowulf] built out of x86 Durons(6), 2 athlons, 2 semprons, two P4s, 2 64 bit x86_64 AMD64 ATHLONS and two AMD x86_64 SEmprons(754 pin) for a total of 16 machines running FC3 and Oscar beta cluster software. I have not tried it with the latest open mpi snapshot yet but I will to night. I think I should reinstall FC3 on the head node P4 2.8GHz and reinstall all the compute nodes with Oscar beta Nov 3, 2005 and open mpi of todays Nov 3, 2005 1.0 snapshot and try again. I could have made an errror somewhere before. It should not take me long. But I doubt it as MPICH2 and open mpi with the switches pml teg give good comparable performance. I was not using jumo MTU frames either just 1500bytes. It is not homogenous (BSquared) but a good test set up. 
If you have any advice, Please tell me and I could try it out.
Thank you and good luck!
Allan 





On Oct 27, 2005, at 10:19 AM, Jeff Squyres wrote:

</pre>
<blockquote type="cite">
  <pre wrap=""><span class="moz-txt-citetags">&gt; </span>On Oct 19, 2005, at 12:04 AM, Allan Menezes wrote:
<span class="moz-txt-citetags">&gt;</span>
<span class="moz-txt-citetags">&gt;</span>
  </pre>
  <blockquote type="cite">
    <pre wrap=""><span class="moz-txt-citetags">&gt;&gt; </span>We've done linpack runs recently w/ Infiniband, which result in
<span class="moz-txt-citetags">&gt;&gt; </span>performance
<span class="moz-txt-citetags">&gt;&gt; </span>comparable to mvapich, but not w/ the tcp port. Can you try  
<span class="moz-txt-citetags">&gt;&gt; </span>running w/
<span class="moz-txt-citetags">&gt;&gt; </span>an
<span class="moz-txt-citetags">&gt;&gt; </span>earlier version, specify on the command line:
<span class="moz-txt-citetags">&gt;&gt;</span>
<span class="moz-txt-citetags">&gt;&gt; </span>-mca pml teg
<span class="moz-txt-citetags">&gt;&gt; </span>Hi Tim,
<span class="moz-txt-citetags">&gt;&gt; </span>  I tried the same cluster (16 node x86) with the switches -mca pml
<span class="moz-txt-citetags">&gt;&gt; </span>teg and I get good performance of 24.52Gflops at N=22500
<span class="moz-txt-citetags">&gt;&gt; </span>and Block size NB=120.
<span class="moz-txt-citetags">&gt;&gt; </span>My command line now looks like :
<span class="moz-txt-citetags">&gt;&gt; </span>a1&gt; mpirun -mca pls_rsh_orted /home/allan/openmpi/bin/orted -mca pml
<span class="moz-txt-citetags">&gt;&gt; </span>teg -hostile aa -np 16 ./xhpl
<span class="moz-txt-citetags">&gt;&gt; </span>hostfile = aa, containing the addresses of the 16 machines.
<span class="moz-txt-citetags">&gt;&gt; </span>I am using a GS116 16 port netgear Gigabit ethernet switch with Gnet
<span class="moz-txt-citetags">&gt;&gt; </span>realtek gig ethernet cards
<span class="moz-txt-citetags">&gt;&gt; </span>Why, PLEASE, do these switches pml teg make such a difference? It's
<span class="moz-txt-citetags">&gt;&gt; </span>2.6 times more performance in GFlops than what I was getting without
<span class="moz-txt-citetags">&gt;&gt; </span>them.
<span class="moz-txt-citetags">&gt;&gt; </span>I tried version rc3 and not an earlier version.
<span class="moz-txt-citetags">&gt;&gt; </span>Thank you very much for your assistance!
<span class="moz-txt-citetags">&gt;&gt;</span>
    </pre>
  </blockquote>
  <pre wrap=""><span class="moz-txt-citetags">&gt;</span>
<span class="moz-txt-citetags">&gt; </span>Sorry for the delay in replying to this...
<span class="moz-txt-citetags">&gt;</span>
<span class="moz-txt-citetags">&gt; </span>The "pml teg" switch tells Open MPI to use the 2nd generation TCP
<span class="moz-txt-citetags">&gt; </span>implementation rather than the 3rd generation TCP.  More specifically,
<span class="moz-txt-citetags">&gt; </span>the "PML" is the point-to-point management layer.  There are 2
<span class="moz-txt-citetags">&gt; </span>different components for this -- teg (2nd generation) and ob1 (3rd
<span class="moz-txt-citetags">&gt; </span>generation).  "ob1" is the default; specifying "--mca pml teg" tells
<span class="moz-txt-citetags">&gt; </span>Open MPI to use the "teg" component instead of ob1.
<span class="moz-txt-citetags">&gt;</span>
<span class="moz-txt-citetags">&gt; </span>Note, however, that teg and ob1 know nothing about TCP -- it's the 2nd
<span class="moz-txt-citetags">&gt; </span>order implications that make the difference here.  teg and ob1 use
<span class="moz-txt-citetags">&gt; </span>different back-end components to talk across networks:
<span class="moz-txt-citetags">&gt;</span>
<span class="moz-txt-citetags">&gt; </span>- teg uses PTL components (point-to-point transport layer -- 2nd gen)
<span class="moz-txt-citetags">&gt; </span>- ob1 uses BTL components (byte transfer layer -- 3rd gen)
<span class="moz-txt-citetags">&gt;</span>
<span class="moz-txt-citetags">&gt; </span>We obviously have TCP implementations for both the PTL and BTL.
<span class="moz-txt-citetags">&gt; </span>Considerable time was spent optimizing the TCP PTL (i.e., 2nd gen).
<span class="moz-txt-citetags">&gt; </span>Unfortunately, as yet, little time has been spent optimizing the TCP
<span class="moz-txt-citetags">&gt; </span>BTL (i.e., 3rd gen) -- it was a simple port, nothing more.
<span class="moz-txt-citetags">&gt;</span>
<span class="moz-txt-citetags">&gt; </span>We have spent the majority of our time, so far, optimizing the Myrinet
<span class="moz-txt-citetags">&gt; </span>and Infiniband BTLs (therefore showing that excellent performance is
<span class="moz-txt-citetags">&gt; </span>achievable in the BTLs).  However, I'm quite disappointed by the TCP
<span class="moz-txt-citetags">&gt; </span>BTL performance -- it sounds like we have a protocol mismatch that is
<span class="moz-txt-citetags">&gt; </span>arbitrarily slowing everything down, and something that needs to be
<span class="moz-txt-citetags">&gt; </span>fixed before 1.0 (it's not a problem with the BTL design, since IB and
<span class="moz-txt-citetags">&gt; </span>Myrinet performance is quite good -- just a problem/bug in the TCP
<span class="moz-txt-citetags">&gt; </span>implementation of the BTL).  That much performance degradation is
<span class="moz-txt-citetags">&gt; </span>clearly unacceptable.
<span class="moz-txt-citetags">&gt;</span>
<span class="moz-txt-citetags">&gt; </span>-- 
<span class="moz-txt-citetags">&gt; </span>{+} Jeff Squyres
<span class="moz-txt-citetags">&gt; </span>{+} The Open MPI Project
<span class="moz-txt-citetags">&gt; </span>{+} <a
 class="moz-txt-link-freetext" href="http://www.open-mpi.org/">http://www.open-mpi.org/</a>
<span class="moz-txt-citetags">&gt;</span>
<span class="moz-txt-citetags">&gt; </span>_______________________________________________
<span class="moz-txt-citetags">&gt; </span>users mailing list
<span class="moz-txt-citetags">&gt; </span><a
 class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
<span class="moz-txt-citetags">&gt; </span><a
 class="moz-txt-link-freetext"
 href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
<span class="moz-txt-citetags">&gt;</span>
  </pre>
</blockquote>
<pre wrap=""><!---->


<div class="moz-txt-sig">-- </div></pre>
</body>
</html>

