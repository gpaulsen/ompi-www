<table cellspacing="0" cellpadding="0" border="0" ><tr><td valign="top" style="font: inherit;"><font class="Apple-style-span" face="arial" size="2">I have discovered&nbsp;</font><font class="Apple-style-span" face="arial"><span class="Apple-style-span" style="font-size: 14px;">slightly</span></font><font class="Apple-style-span" face="arial" size="2">&nbsp;more information:</font><div style="font-family: arial; font-size: 10pt; ">When I replace node 'B' from the new cluster with node 'C' from the old cluster</div><div><font class="Apple-style-span" face="arial" size="2">I get the similar&nbsp;</font><font class="Apple-style-span" face="arial"><span class="Apple-style-span" style="font-size: 14px;">behavior</span></font><font class="Apple-style-span" face="arial" size="2">&nbsp;but with an error message:</font></div><div><font class="Apple-style-span" face="arial" size="2"><meta charset="utf-8"><span class="Apple-style-span" style="font-size: medium;
 ">mpirun -H A,A,A,A,A,A,A &nbsp;ring &nbsp; &nbsp; (works from either node)</span><br></font></div><div><font class="Apple-style-span" face="arial"><meta charset="utf-8">mpirun -H C,C,C &nbsp;ring &nbsp; &nbsp; (works from either node)<br></font></div><div><font class="Apple-style-span" face="arial"><meta charset="utf-8">mpirun -H A,C &nbsp;ring &nbsp; &nbsp; (Fails from either node:)</font></div><div><font class="Apple-style-span" face="arial">Process 0 sending 10 to 1, tag 201 (3 processes in ring)<br></font><font class="Apple-style-span" face="arial">[C:23465] *** &nbsp;An error occurred in MPI_Recv<br><meta charset="utf-8">[C:23465] *** &nbsp;on communicator MPI_COMM_WORLD</font></div><div><font class="Apple-style-span" face="arial"><meta charset="utf-8">[C:23465] *** &nbsp;MPI_ERRORS_ARE FATAL (your job will now abort)</font></div><div><font class="Apple-style-span" face="arial">Process 0 sent to 1</font></div><div><font class="Apple-style-span"
 face="arial">----------------------------------</font></div><div><font class="Apple-style-span" face="arial">Running this on either node A or C produces the same result</font></div><div><font class="Apple-style-span" face="arial">Node C runs openMPI 1.4.1 and is an ordinary dual core on FC10 , not an i5 2400 like the others.</font></div><div><font class="Apple-style-span" face="arial">all the binaries are compiled on FC10 with&nbsp;</font><span class="Apple-style-span" style="font-family: arial; ">gcc 4.3.2</span></div><div></div><div></div><div><font class="Apple-style-span" face="arial"><br></font></div><div><font class="Apple-style-span" face="arial">--- On </font><b style="font-family: arial; font-size: 10pt; ">Tue, 12/7/11, Randolph Pullen <i>&lt;randolph_pullen@yahoo.com.au&gt;</i></b><font class="Apple-style-span" face="arial" size="2"> wrote:</font><br><blockquote style="font-family: arial; font-size: 10pt; border-left-width: 2px;
 border-left-style: solid; border-left-color: rgb(16, 16, 255); margin-left: 5px; padding-left: 5px; "><br>From: Randolph Pullen &lt;randolph_pullen@yahoo.com.au&gt;<br>Subject: Re: [OMPI users] Mpirun only works when n&lt; 3<br>To: "Open MPI Users" &lt;users@open-mpi.org&gt;, "Jeff Squyres" &lt;jsquyres@cisco.com&gt;<br>Received: Tuesday, 12 July, 2011, 1:31 AM<br><br><div id="yiv722358223"><table cellspacing="0" cellpadding="0" border="0"><tbody><tr><td valign="top" style="font:inherit;">There are no firewalls by default. &nbsp;I can ssh between both nodes without a password so I assumed that all is good with the comms.<div>I can also get both nodes to participate in the ring program at the same time.</div><div>Its just that I am limited to inly 2 processes if they are split between the nodes<br><div>ie:</div><div>mpirun -H A,B ring &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (works)</div><div><div>mpirun -H
 A,A,A,A,A,A,A &nbsp;ring &nbsp; &nbsp; (works)</div></div><div><div><div>mpirun -H B,B,B,B ring &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (works)</div></div><div>mpirun -H A,B,A &nbsp;ring &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(hangs)</div></div><div><br></div><div><br>--- On <b>Tue, 12/7/11,
 Jeff Squyres <i>&lt;jsquyres@cisco.com&gt;</i></b> wrote:<br><blockquote style="border-left:2px solid rgb(16, 16, 255);margin-left:5px;padding-left:5px;"><br>From: Jeff Squyres &lt;jsquyres@cisco.com&gt;<br>Subject: Re: [OMPI users] Mpirun only works when n&lt; 3<br>To: randolph_pullen@yahoo.com.au, "Open MPI Users" &lt;users@open-mpi.org&gt;<br>Received: Tuesday, 12 July, 2011, 12:21 AM<br><br><div class="yiv722358223plainMail">Have you disabled firewalls between your compute nodes?<br><br><br>On Jul 11, 2011, at 9:34 AM, Randolph Pullen wrote:<br><br>&gt; This appears to be similar to the problem described in:<br>&gt; <br>&gt; <a rel="nofollow" target="_blank" href="https://svn.open-mpi.org/trac/ompi/ticket/2043">https://svn.open-mpi.org/trac/ompi/ticket/2043</a><br>&gt; <br>&gt; However, those fixes do not work for me.<br>&gt; <br>&gt; I am running on an <br>&gt; <br>&gt; - i5 sandy bridge under Ubuntu 10.10&nbsp; 8 G RAM<br>&gt; <br>&gt; - Kernel
 2.6.32.14 with OpenVZ
 tweaks<br>&gt; <br>&gt; - OpenMPI V 1.4.1<br>&gt; <br>&gt; I am trying to migrate existing software to a new cluster (A,B)<br>&gt; <br>&gt; Symptoms:<br>&gt; <br>&gt; I can run the ring demo on a single machine, either A or B with any number of processes.<br>&gt; <br>&gt; But when I combine the 2 machines I am limited to 2 processes, any more and MPI hangs.&nbsp;&nbsp;&nbsp;It gets as far as:<br>&gt; <br>&gt;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;Process 0 sending 10 to 1, tag 201 (3 processes in ring)<br>&gt; <br>&gt;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;Process 0 sent to 1<br>&gt; <br>&gt; and there it stays...<br>&gt; <br>&gt; Any help greatly appreciated.<br>&gt; <br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a rel="nofollow">users@open-mpi.org</a><br>&gt; <a rel="nofollow" target="_blank" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><br>--
 <br>Jeff Squyres<br><a rel="nofollow">jsquyres@cisco.com</a><br>For corporate legal information go to:<br><a rel="nofollow" target="_blank" href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br><br></div></blockquote></div></div></td></tr></tbody></table></div><br>-----Inline Attachment Follows-----<br><br><div class="plainMail">_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="/mc/compose?to=users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></blockquote></div></td></tr></table>
