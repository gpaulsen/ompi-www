<div dir="ltr"><div>I agree with you and still struglling with subnet ID settings because I couldn&#39;t find <font face="Courier New">/var/cache/opensm/opensm.opts file.</font></div><div><font face="Courier New"></font> </div>
<div><font face="Courier New">Secondly, if OMPI is going for TCP then it should be able to find as compute nodes are available via ping and ssh</font></div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Sun, Jan 19, 2014 at 9:38 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">If OMPI finds infiniband support on the node, it will attempt to use it. In this case, it would appear you have an incorrectly configured IB adaptor on the node, so you get the additional warning about that fact.<div>
<br></div><div>OMPI then falls back to look for another transport, in this case TCP. However, the TCP transport is unable to create a socket to the remote host. The most likely cause is a firewall, so you might want to check that and turn it off.</div>
<div><br></div><div><br><div><div><div class="h5"><div>On Jan 19, 2014, at 4:19 AM, Syed Ahsan Ali &lt;<a href="mailto:ahsanshah01@gmail.com" target="_blank">ahsanshah01@gmail.com</a>&gt; wrote:</div><br></div></div><blockquote type="cite">
<div><div class="h5"><div dir="ltr"><div>Dear All</div><div> </div><div>I am getting infiniband errors while running mpirun applications on cluster. I get these errors even when I don&#39;t include infiniband usage flags in mpirun command. Please guide</div>

<div> </div><div>mpirun -np 72 -hostfile hostlist ../bin/regcmMPI <a href="http://regcm.in/" target="_blank">regcm.in</a></div><div> </div><div>--------------------------------------------------------------------------<br>
[[59183,1],24]: A high-performance Open MPI point-to-point messaging module<br>
was unable to find any relevant network interfaces:</div><p>Module: OpenFabrics (openib)<br>  Host: compute-01-10.private.dns.zone</p><p>Another transport will be used instead, although this may result in<br>lower performance.<br>

--------------------------------------------------------------------------<br>--------------------------------------------------------------------------<br>WARNING: There are more than one active ports on host &#39;compute-01-15.private.dns.zone&#39;, but the<br>

default subnet GID prefix was detected on more than one of these<br>ports.  If these ports are connected to different physical IB<br>networks, this configuration will fail in Open MPI.  This version of<br>Open MPI requires that every physically separate IB subnet that is<br>

used between connected MPI processes must have different subnet ID<br>values.</p><p>Please see this FAQ entry for more details:</p><p>  <a href="http://www.open-mpi.org/faq/?category=openfabrics#ofa-default-subnet-gid" target="_blank">http://www.open-mpi.org/faq/?category=openfabrics#ofa-default-subnet-gid</a></p>
<p>NOTE: You can turn off this warning by setting the MCA parameter<br>      btl_openib_warn_default_gid_prefix to 0.<br>--------------------------------------------------------------------------</p><p>  This is RegCM trunk<br>

   SVN Revision: tag 4.3.5.6 compiled at: data : Sep  3 2013  time: 05:10:53</p><p>[<a href="http://pmd.pakmet.com:03309/" target="_blank">pmd.pakmet.com:03309</a>] 15 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics<br>

[<a href="http://pmd.pakmet.com:03309/" target="_blank">pmd.pakmet.com:03309</a>] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>[<a href="http://pmd.pakmet.com:03309/" target="_blank">pmd.pakmet.com:03309</a>] 47 more processes have sent help message help-mpi-btl-openib.txt / default subnet prefix<br>

[compute-01-03.private.dns.zone][[59183,1],1][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.108.10 failed: No route to host (113)<br>[compute-01-03.private.dns.zone][[59183,1],2][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.108.10 failed: No route to host (113)<br>

[compute-01-03.private.dns.zone][[59183,1],5][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.108.10 failed: No route to host (113)<br>[compute-01-03.private.dns.zone][[59183,1],3][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] [compute-01-03.private.dns.zone][[59183,1],0][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.108.10 failed: No route to host (113)<br>

[compute-01-03.private.dns.zone][[59183,1],7][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.108.10 failed: No route to host (113)<br>connect() to 192.168.108.10 failed: No route to host (113)<br>

[compute-01-03.private.dns.zone][[59183,1],6][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.108.10 failed: No route to host (113)<br>[compute-01-03.private.dns.zone][[59183,1],4][btl_tcp_endpoint.c:638:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.108.10 failed: No route to host (113)<br clear="all">

<br>Ahsan<br></p>
</div></div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br><br clear="all"><br>-- <br>Syed Ahsan Ali Bokhari <br>Electronic Engineer (EE)<div>
<br>Research &amp; Development Division<br>Pakistan Meteorological Department H-8/4, Islamabad.<br>Phone # off  +92518358714</div><div>Cell # +923155145014<br></div>
</div>

