<html><head><meta http-equiv="Content-Type" content="text/html charset=iso-8859-1"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">We did update ROMIO at some point in there, so it is possible this is a ROMIO bug that we have picked up. I've asked someone to check upstream about it.<div><br></div><div><br><div><div>On Jan 17, 2014, at 12:02 PM, Ronald Cohen &lt;<a href="mailto:rhcohen@lbl.gov">rhcohen@lbl.gov</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr"><div>Sorry, too many entries in this thread, I guess.&nbsp; My general goal is to get a working parallel hdf5 with openmpi on Mac OS X Mavericks.&nbsp; At one point in the saga I had romio disabled, which naturally doesn't work for hdf5 (which is trying to read/write files in parallel).&nbsp; So the hdf5 tests would of course fail.&nbsp;&nbsp;&nbsp; I subsequently had link errors with hdf5 because I was building openmpi with --disable-static, whereas the default (and recommended) option for hdf5 is to disable shared and build static.&nbsp; My most recent attempts were with openmpi with enable-static, enable-nodlopen.&nbsp; In that case, with openmpi 1.7.4rc1, hdf5 1.8.12 configured and built successfully but make chek-p produced many errors in its t-mpi tests, with messages like "proc 4: found data error at [2140143616+0], expect -7, got 6".&nbsp;&nbsp; The errors were reproduced by the HDF5 testing team with openmpir 1.7.4rc1, but not with 1.7.3 (which I am now building).<br>
<br></div>Hopefully that is an adequate summary.<br><br>Ron<br><br></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Fri, Jan 17, 2014 at 11:44 AM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Can you specify exactly which issue you're referring to?<br>
<br>
- test failing when you had ROMIO disabled<br>
- test (sometimes) failing when you had ROMIO disabled<br>
- compiling / linking issues<br>
<br>
?<br>
<br>
<br>
On Jan 17, 2014, at 1:50 PM, Ronald Cohen &lt;<a href="mailto:rhcohen@lbl.gov">rhcohen@lbl.gov</a>&gt; wrote:<br>
<br>
&gt; Hello Ralph and others, I just got the following back from the HDF-5 support group, suggesting an ompi bug. &nbsp; So I should either try 1.7.3 or a recent nightly 1.7.4. &nbsp; &nbsp;Will likely opt for 1.7.3, but hopefully someone at openmpi can look at the problem for 1.7.4. &nbsp; In short, the challenge is to get a parallel hdf5 that passes make check-p with 1.7.4.<br>

&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; ------------------<br>
&gt; Hi Ron,<br>
&gt;<br>
&gt; I had sent your message to the developer and he can reproduce the issue.<br>
&gt; Here is what he says:<br>
&gt;<br>
&gt; &nbsp;---<br>
&gt; &nbsp;I replicated this on Jam with ompi 1.7.4rc1. I saw the same error he is seeing.<br>
&gt; &nbsp;Note that this is an un-stable release for ompi.<br>
&gt; &nbsp;I tried ompi 1.7.3 (feature - little more stable release). I didn't see the<br>
&gt; &nbsp;problems there.<br>
&gt;<br>
&gt; &nbsp;So this is an ompi bug. He can report it to the ompi list. He can just point<br>
&gt; &nbsp;them to the t_mpi.c tests in our test suite in testpar/ and say it occurs with<br>
&gt; &nbsp;their 1.7.4 rc1.<br>
&gt; &nbsp;---<br>
&gt;<br>
&gt; -Barbara<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; On Fri, Jan 17, 2014 at 9:39 AM, Ronald Cohen &lt;<a href="mailto:rhcohen@lbl.gov">rhcohen@lbl.gov</a>&gt; wrote:<br>
&gt; Thanks, I've just gotten an email with some suggestions (and promise of more help) from the HDF5 support team. &nbsp; I will report back here, as it may be of interest to others trying to build hdf5 on mavericks.<br>

&gt;<br>
&gt;<br>
&gt; On Fri, Jan 17, 2014 at 9:08 AM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt; Afraid I have no idea, but hopefully someone else here with experience with HDF5 can chime in?<br>
&gt;<br>
&gt;<br>
&gt; On Jan 17, 2014, at 9:03 AM, Ronald Cohen &lt;<a href="mailto:rhcohen@lbl.gov">rhcohen@lbl.gov</a>&gt; wrote:<br>
&gt;<br>
&gt;&gt; Still a timely response, thank you. &nbsp; &nbsp;The particular problem I noted hasn't recurred; for reasons I will explain shortly I had to rebuild openmpi again, and this time Sample_mpio.c compiled and ran successfully from the start.<br>

&gt;&gt;<br>
&gt;&gt; But now my problem is trying to get parallel HDF5 to run. &nbsp;In my first attempt to build HDF5 it failed in the load stage because of unsatisifed externals from openmpi, and I deduced the problem was having built openmpi with --disable-static. &nbsp; So I rebuilt with --enable-static and --disable-dlopen (emulating a successful openmpi + hdf5 combination I had built on Snow Leopard). &nbsp; Once again openmpi passed its make check's, and as noted above the Sample_mpio.c test compiled and ran fine. &nbsp; And the parallel hdf5 configure and make steps ran successfully. &nbsp; But when I ran make check for hdf5, the serial tests passed but none of the parallel tests did. &nbsp;Over a million test failures! &nbsp;Error messages like:<br>

&gt;&gt;<br>
&gt;&gt; Proc 0: *** MPIO File size range test...<br>
&gt;&gt; --------------------------------<br>
&gt;&gt; MPI_Offset is signed 8 bytes integeral type<br>
&gt;&gt; MPIO GB file write test MPItest.h5<br>
&gt;&gt; MPIO GB file write test MPItest.h5<br>
&gt;&gt; MPIO GB file write test MPItest.h5<br>
&gt;&gt; MPIO GB file write test MPItest.h5<br>
&gt;&gt; MPIO GB file write test MPItest.h5<br>
&gt;&gt; MPIO GB file write test MPItest.h5<br>
&gt;&gt; MPIO GB file read test MPItest.h5<br>
&gt;&gt; MPIO GB file read test MPItest.h5<br>
&gt;&gt; MPIO GB file read test MPItest.h5<br>
&gt;&gt; MPIO GB file read test MPItest.h5<br>
&gt;&gt; proc 3: found data error at [2141192192+0], expect -6, got 5<br>
&gt;&gt; proc 3: found data error at [2141192192+1], expect -6, got 5<br>
&gt;&gt;<br>
&gt;&gt; And -- the specific errors reported, which processor, which location, and the total number of errors changes if I rerun make check.<br>
&gt;&gt;<br>
&gt;&gt; I've sent configure, make and make check logs to the HDF5 help desk but haven't gotten a response.<br>
&gt;&gt;<br>
&gt;&gt; I am now configuring openmpi (still 1.7.4rc1) with:<br>
&gt;&gt;<br>
&gt;&gt; ./configure --prefix=/usr/local/openmpi CC=gcc CXX=g++ FC=gfortran F77=gfortran --enable-static --with-pic --disable-dlopen --enable-mpirun-prefix-by-default<br>
&gt;&gt;<br>
&gt;&gt; and configuring HDF5 (version 1.8.12) with:<br>
&gt;&gt;<br>
&gt;&gt; configure --prefix=/usr/local/hdf5/par CC=mpicc CFLAGS=-fPIC FC=mpif90 FCFLAGS=-fPIC CXX=mpicxx CXXFLAGS=-fPIC --enable-parallel --enable-fortran<br>
&gt;&gt;<br>
&gt;&gt; This is the combination that worked for me with Snow Leopard (though it was then earlier versions of both openmpi and hdf5.)<br>
&gt;&gt;<br>
&gt;&gt; If it matters, the gcc is the stock one with Mavericks' XCode, and gfortran is 4.9.0.<br>
&gt;&gt;<br>
&gt;&gt; (I just noticed that the mpi fortran wrapper is now mpifort, but I also see that mpif90 is still there and is a just link to mpifort.)<br>
&gt;&gt;<br>
&gt;&gt; Any suggestions?<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; On Fri, Jan 17, 2014 at 8:14 AM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt;&gt; sorry for delayed response - just getting back from travel. I don't know why you would get that behavior other than a race condition. Afraid that code path is foreign to me, but perhaps one of the folks in the MPI-IO area can respond<br>

&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; On Jan 15, 2014, at 4:26 PM, Ronald Cohen &lt;<a href="mailto:rhcohen@lbl.gov">rhcohen@lbl.gov</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt;&gt; Update: I reconfigured with enable_io_romio=yes, and this time -- mostly -- the test using Sample_mpio.c &nbsp;passes. &nbsp; Oddly the very first time I tried I got errors:<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; % mpirun -np 2 sampleio<br>
&gt;&gt;&gt; Proc 1: hostname=Ron-Cohen-MBP.local<br>
&gt;&gt;&gt; Testing simple C MPIO program with 2 processes accessing file ./mpitest.data<br>
&gt;&gt;&gt; &nbsp; &nbsp; (Filename can be specified via program argument)<br>
&gt;&gt;&gt; Proc 0: hostname=Ron-Cohen-MBP.local<br>
&gt;&gt;&gt; Proc 1: read data[0:1] got 0, expect 1<br>
&gt;&gt;&gt; Proc 1: read data[0:2] got 0, expect 2<br>
&gt;&gt;&gt; Proc 1: read data[0:3] got 0, expect 3<br>
&gt;&gt;&gt; Proc 1: read data[0:4] got 0, expect 4<br>
&gt;&gt;&gt; Proc 1: read data[0:5] got 0, expect 5<br>
&gt;&gt;&gt; Proc 1: read data[0:6] got 0, expect 6<br>
&gt;&gt;&gt; Proc 1: read data[0:7] got 0, expect 7<br>
&gt;&gt;&gt; Proc 1: read data[0:8] got 0, expect 8<br>
&gt;&gt;&gt; Proc 1: read data[0:9] got 0, expect 9<br>
&gt;&gt;&gt; Proc 1: read data[1:0] got 0, expect 10<br>
&gt;&gt;&gt; Proc 1: read data[1:1] got 0, expect 11<br>
&gt;&gt;&gt; Proc 1: read data[1:2] got 0, expect 12<br>
&gt;&gt;&gt; Proc 1: read data[1:3] got 0, expect 13<br>
&gt;&gt;&gt; Proc 1: read data[1:4] got 0, expect 14<br>
&gt;&gt;&gt; Proc 1: read data[1:5] got 0, expect 15<br>
&gt;&gt;&gt; Proc 1: read data[1:6] got 0, expect 16<br>
&gt;&gt;&gt; Proc 1: read data[1:7] got 0, expect 17<br>
&gt;&gt;&gt; Proc 1: read data[1:8] got 0, expect 18<br>
&gt;&gt;&gt; Proc 1: read data[1:9] got 0, expect 19<br>
&gt;&gt;&gt; --------------------------------------------------------------------------<br>
&gt;&gt;&gt; MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD<br>
&gt;&gt;&gt; with errorcode 1.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; But when I reran the same mpirun command, the test was successful. &nbsp; And deleting the executable and recompiling and then again running the same mpirun command, the test was successful. &nbsp; Can someone explain that?<br>

&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Wed, Jan 15, 2014 at 1:16 PM, Ronald Cohen &lt;<a href="mailto:rhcohen@lbl.gov">rhcohen@lbl.gov</a>&gt; wrote:<br>
&gt;&gt;&gt; Aha. &nbsp; I guess I didn't know what the io-romio option does. &nbsp; If you look at my config.log you will see my configure line included --disable-io-romio. &nbsp; &nbsp;Guess I should change --disable to --enable.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; You seem to imply that the nightly build is stable enough that I should probably switch to that rather than 1.7.4rc1. &nbsp; Am I reading between the lines correctly?<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Wed, Jan 15, 2014 at 10:56 AM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt;&gt;&gt; Oh, a word of caution on those config params - you might need to check to ensure I don't disable romio in them. I don't normally build it as I don't use it. Since that is what you are trying to use, just change the "no" to "yes" (or delete that line altogether) and it will build.<br>

&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Wed, Jan 15, 2014 at 10:53 AM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt;&gt;&gt; You can find my configure options in the OMPI distribution at contrib/platform/intel/bend/mac. You are welcome to use them - just configure --with-platform=intel/bend/mac<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; I work on the developer's trunk, of course, but also run the head of the 1.7.4 branch (essentially the nightly tarball) on a fairly regular basis.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; As for the opal_bitmap test: it wouldn't surprise me if that one was stale. I can check on it later tonight, but I'd suspect that the test is bad as we use that class in the code base and haven't seen an issue.<br>

&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Wed, Jan 15, 2014 at 10:49 AM, Ronald Cohen &lt;<a href="mailto:rhcohen@lbl.gov">rhcohen@lbl.gov</a>&gt; wrote:<br>
&gt;&gt;&gt; Ralph,<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; I just sent out another post with the c file attached.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; If you can get that to work, and even if you can't can you tell me what configure options you use, and what version of open-mpi? &nbsp; Thanks.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Ron<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Wed, Jan 15, 2014 at 10:36 AM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt;&gt;&gt; BTW: could you send me your sample test code?<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Wed, Jan 15, 2014 at 10:34 AM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt;&gt;&gt; I regularly build on Mavericks and run without problem, though I haven't tried a parallel IO app. I'll give yours a try later, when I get back to my Mac.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Wed, Jan 15, 2014 at 10:04 AM, Ronald Cohen &lt;<a href="mailto:rhcohen@lbl.gov">rhcohen@lbl.gov</a>&gt; wrote:<br>
&gt;&gt;&gt; I have been struggling trying to get a usable build of openmpi on Mac OSX Mavericks (10.9.1). &nbsp;I can get openmpi to configure and build without error, but have problems after that which depend on the openmpi version.<br>

&gt;&gt;&gt;<br>
&gt;&gt;&gt; With 1.6.5, make check fails the opal_datatype_test, ddt_test, and ddt_raw tests. &nbsp;The various atomic_* tests pass. &nbsp; &nbsp;See checklogs_1.6.5, attached as a .gz file.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Following suggestions from openmpi discussions I tried openmpi version 1.7.4rc1. &nbsp;In this case make check indicates all tests passed. &nbsp;But when I proceeded to try to build a parallel code (parallel HDF5) it failed. &nbsp;Following an email exchange with the HDF5 support people, they suggested I try to compile and run the attached bit of simple code Sample_mpio.c (which they supplied) which does not use any HDF5, but just attempts a parallel write to a file and parallel read. &nbsp; That test failed when requesting more than 1 processor -- which they say indicates a failure of the openmpi installation. &nbsp; The error message was:<br>

&gt;&gt;&gt;<br>
&gt;&gt;&gt; MPI_INIT: argc 1<br>
&gt;&gt;&gt; MPI_INIT: argc 1<br>
&gt;&gt;&gt; Testing simple C MPIO program with 2 processes accessing file ./mpitest.data<br>
&gt;&gt;&gt; &nbsp; &nbsp; (Filename can be specified via program argument)<br>
&gt;&gt;&gt; Proc 0: hostname=Ron-Cohen-MBP.local<br>
&gt;&gt;&gt; Proc 1: hostname=Ron-Cohen-MBP.local<br>
&gt;&gt;&gt; MPI_BARRIER[0]: comm MPI_COMM_WORLD<br>
&gt;&gt;&gt; MPI_BARRIER[1]: comm MPI_COMM_WORLD<br>
&gt;&gt;&gt; Proc 0: MPI_File_open with MPI_MODE_EXCL failed (MPI_ERR_FILE: invalid file)<br>
&gt;&gt;&gt; MPI_ABORT[0]: comm MPI_COMM_WORLD errorcode 1<br>
&gt;&gt;&gt; MPI_BCAST[1]: buffer 7fff5a483048 count 1 datatype MPI_INT root 0 comm MPI_COMM_WORLD<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; I then went back to my openmpi directories and tried running some of the individual tests in the test and examples directories. &nbsp;In particular in test/class I found one test that seem to not be run as part of make check which failed, even with one processor; this is opal_bitmap. &nbsp;Not sure if this is because 1.7.4rc1 is incomplete, or there is something wrong with the installation, or maybe a 32 vs 64 bit thing? &nbsp; The error message is<br>

&gt;&gt;&gt;<br>
&gt;&gt;&gt; mpirun detected that one or more processes exited with non-zero status, thus causing the job to be terminated. The first process to do so was:<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; &nbsp; Process name: [[48805,1],0]<br>
&gt;&gt;&gt; &nbsp; Exit code: &nbsp; &nbsp;255<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Any suggestions?<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; More generally has anyone out there gotten an openmpi build on Mavericks to work with sufficient success that they can get the attached Sample_mpio.c (or better yet, parallel HDF5) to build?<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Details: Running Mac OS X 10.9.1 on a mid-2009 Macbook pro with 4 GB memory; tried openmpi 1.6.5 and 1.7.4rc1. &nbsp;Built openmpi against the stock gcc that comes with XCode 5.0.2, and gfortran 4.9.0.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Files attached: config.log.gz, openmpialllog.gz (output of running ompi_info --all), checklog2.gz (output of make.check in top openmpi &nbsp;directory).<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; I am not attaching logs of make and install since those seem to have been successful, but can generate those if that would be helpful.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<span class="HOEnZb"><font color="#888888"><br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</font></span></blockquote></div><br></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
