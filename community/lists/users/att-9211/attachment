<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><br><div><div>On May 5, 2009, at 3:37 AM, Geoffroy Pignot wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite">Hi<br><br>&nbsp;The result is : everything works fine with MPI executables : logical !!!</blockquote><blockquote type="cite"><br><br>What I was trying to do , was to run non MPI exes thanks to mpirun. There , openmpi is not able to bind these processes to a particular CPU.<br> My conclusion is that the process affinity is set in MPI_Init, right ?</blockquote><div><br></div>Yes - sorry, I should have caught that in your cmd line. Not enough sleep lately... :-)</div><div><br><blockquote type="cite"><br><br>Could it be possible to have the paffinity features working without any MPI_Init call, using taskset for example. I agree , it's not your job to support the execution of any kind of exes but it would be nice !!</blockquote><div><br></div>Actually, it is worth the question. As things stand, processes don't bind until they call MPI_Init. This has caused some problems for people that rely on the procs to be restricted to specific processor sets, but who don't (for various reasons) call MPI_Init at the beginning of their program.</div><div><br></div><div>I'll raise the question inside the devel community and see what people think.</div><div><br><blockquote type="cite"><br> Thanks again for all your efforts, I really appreciate</blockquote><div><br></div>No problem! Thanks for your patience while debugging this...</div><div><br></div><div><br><blockquote type="cite"><br><br>I am looking forward to downloading, trying and deploying the next official release<br><br>Regards<br><br>Geoffroy <br><br><br><br><div class="gmail_quote">2009/5/4 Geoffroy Pignot <span dir="ltr">&lt;<a href="mailto:geopignot@gmail.com">geopignot@gmail.com</a>></span><br> <blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">Hi Ralph<br><br>Thanks for your extra tests.&nbsp; Before leaving , I just pointed out a problem coming from running plpa across different rh distribs (&lt;=> different Linux kernels). Indeed, I configure and compile openmpi on rhel4 , then I run on rhel5. I think my problem comes from this approximation. I'll do few more tests tomorrow morning (France) and keep you inform.<br> <br>Regards<br><br>Geoffroy<br><br><div class="gmail_quote"><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> <br> <br> Message: 2<br> Date: Mon, 4 May 2009 13:34:40 -0600<div class="im"><br> From: Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>><br> Subject: Re: [OMPI users] 1.3.1 -rf rankfile behaviour ??<br> To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>><br> Message-ID:<br></div> &nbsp; &nbsp; &nbsp; &nbsp;&lt;<a href="mailto:71d2d8cc0905041234m76eb5a9dx57a773997779db73@mail.gmail.com" target="_blank">71d2d8cc0905041234m76eb5a9dx57a773997779db73@mail.gmail.com</a>><div class="im"><br> Content-Type: text/plain; charset="iso-8859-1"<br> <br></div> Hmmm...I'm afraid I can't replicate the problem. All seems to be working<br> just fine on the RHEL systems available to me. The procs indeed bind to the<br> specified processors in every case.<br> <br> rhc@odin ~/trunk]$ cat rankfile<br> rank 0=odin001 slot=0<br> rank 1=odin002 slot=1<br> <br> [rhc@odin mpi]$ mpirun -rf ../../../rankfile -n 2 --leave-session-attached<br> -mca paffinity_base_verbose 5 ./mpi_spin<br> [<a href="http://odin001.cs.indiana.edu:09297" target="_blank">odin001.cs.indiana.edu:09297</a> &lt;<a href="http://odin001.cs.indiana.edu:9297/" target="_blank">http://odin001.cs.indiana.edu:9297/</a>>]<br> paffinity slot assignment: slot_list == 0<br> [<a href="http://odin001.cs.indiana.edu:09297" target="_blank">odin001.cs.indiana.edu:09297</a> &lt;<a href="http://odin001.cs.indiana.edu:9297/" target="_blank">http://odin001.cs.indiana.edu:9297/</a>>]<br> paffinity slot assignment: rank 0 runs on cpu #0 (#0)<br> [<a href="http://odin002.cs.indiana.edu:13566" target="_blank">odin002.cs.indiana.edu:13566</a>] paffinity slot assignment: slot_list == 1<br> [<a href="http://odin002.cs.indiana.edu:13566" target="_blank">odin002.cs.indiana.edu:13566</a>] paffinity slot assignment: rank 1 runs on cpu<br> #1 (#1)<br> <br> Suspended<br> [rhc@odin mpi]$ ssh odin001<br> [rhc@odin001 ~]$ ps axo stat,user,psr,pid,pcpu,comm | grep rhc<br> S &nbsp; &nbsp;rhc &nbsp; &nbsp; &nbsp; &nbsp;0 &nbsp;9296 &nbsp;0.0 orted<br> RLl &nbsp;rhc &nbsp; &nbsp; &nbsp; &nbsp;0 &nbsp;9297 &nbsp;100 mpi_spin<br> <br> [rhc@odin mpi]$ ssh odin002<br> [rhc@odin002 ~]$ ps axo stat,user,psr,pid,pcpu,comm | grep rhc<br> S &nbsp; &nbsp;rhc &nbsp; &nbsp; &nbsp; &nbsp;0 13562 &nbsp;0.0 orted<br> RLl &nbsp;rhc &nbsp; &nbsp; &nbsp; &nbsp;1 13566 &nbsp;102 mpi_spin<br> <br> <br> Not sure where to go from here...perhaps someone else can spot the problem?<br> Ralph<br> <br> <br> On Mon, May 4, 2009 at 8:28 AM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>> wrote:<br> <br> > Unfortunately, I didn't write any of that code - I was just fixing the<br> > mapper so it would properly map the procs. From what I can tell, the proper<br> > things are happening there.<br> ><br> > I'll have to dig into the code that specifically deals with parsing the<br> > results to bind the processes. Afraid that will take awhile longer - pretty<br> > dark in that hole.<br> ><br> ><br> ><br> > On Mon, May 4, 2009 at 8:04 AM, Geoffroy Pignot &lt;<a href="mailto:geopignot@gmail.com" target="_blank">geopignot@gmail.com</a>>wrote:<div><div></div><div class="h5"><br> ><br> >> Hi,<br> >><br> >> So, there are no more crashes with my "crazy" mpirun command. But the<br> >> paffinity feature seems to be broken. Indeed I am not able to pin my<br> >> processes.<br> >><br> >> Simple test with a program using your plpa library :<br> >><br> >> r011n006% cat hostf<br> >> r011n006 slots=4<br> >><br> >> r011n006% cat rankf<br> >> rank 0=r011n006 slot=0 &nbsp; ----> bind to CPU 0 , exact ?<br> >><br> >> r011n006% /tmp/HALMPI/openmpi-1.4a/bin/mpirun --hostfile hostf --rankfile<br> >> rankf --wdir /tmp -n 1 a.out<br> >> &nbsp;>>> PLPA Number of processors online: 4<br> >> &nbsp;>>> PLPA Number of processor sockets: 2<br> >> &nbsp;>>> PLPA Socket 0 (ID 0): 2 cores<br> >> &nbsp;>>> PLPA Socket 1 (ID 3): 2 cores<br> >><br> >> Ctrl+Z<br> >> r011n006%bg<br> >><br> >> r011n006% ps axo stat,user,psr,pid,pcpu,comm | grep gpignot<br> >> R+ &nbsp; gpignot &nbsp; &nbsp;3 &nbsp;9271 97.8 a.out<br> >><br> >> In fact whatever the slot number I put in my rankfile , a.out always runs<br> >> on the CPU 3. I was looking for it on CPU 0 accordind to my cpuinfo file<br> >> (see below)<br> >> The result is the same if I try another syntax (rank 0=r011n006 slot=0:0<br> >> bind to socket 0 - core 0 &nbsp;, exact ? )<br> >><br> >> Thanks in advance<br> >><br> >> Geoffroy<br> >><br> >> PS: I run on rhel5<br> >><br> >> r011n006% uname -a<br> >> Linux r011n006 2.6.18-92.1.1NOMAP32.el5 #1 SMP Sat Mar 15 01:46:39 CDT<br> >> 2008 x86_64 x86_64 x86_64 GNU/Linux<br> >><br> >> My configure is :<br> >> &nbsp;./configure --prefix=/tmp/openmpi-1.4a --libdir='${exec_prefix}/lib64'<br> >> --disable-dlopen --disable-mpi-cxx --enable-heterogeneous<br> >><br> >><br> >> r011n006% cat /proc/cpuinfo<br> >> processor &nbsp; &nbsp; &nbsp; : 0<br> >> vendor_id &nbsp; &nbsp; &nbsp; : GenuineIntel<br> >> cpu family &nbsp; &nbsp; &nbsp;: 6<br> >> model &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : 15<br> >> model name &nbsp; &nbsp; &nbsp;: Intel(R) Xeon(R) CPU &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5150 &nbsp;@ 2.66GHz<br> >> stepping &nbsp; &nbsp; &nbsp; &nbsp;: 6<br> >> cpu MHz &nbsp; &nbsp; &nbsp; &nbsp; : 2660.007<br> >> cache size &nbsp; &nbsp; &nbsp;: 4096 KB<br> >> physical id &nbsp; &nbsp; : 0<br> >> siblings &nbsp; &nbsp; &nbsp; &nbsp;: 2<br> >> core id &nbsp; &nbsp; &nbsp; &nbsp; : 0<br> >> cpu cores &nbsp; &nbsp; &nbsp; : 2<br> >> fpu &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : yes<br> >> fpu_exception &nbsp; : yes<br> >> cpuid level &nbsp; &nbsp; : 10<br> >> wp &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;: yes<br> >> flags &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca<br> >> cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm syscall nx lm<br> >> constant_tsc pni monitor ds_cpl vmx est tm2 cx16 xtpr lahf_lm<br> >> bogomips &nbsp; &nbsp; &nbsp; &nbsp;: 5323.68<br> >> clflush size &nbsp; &nbsp;: 64<br> >> cache_alignment : 64<br> >> address sizes &nbsp; : 36 bits physical, 48 bits virtual<br> >> power management:<br> >><br> >> processor &nbsp; &nbsp; &nbsp; : 1<br> >> vendor_id &nbsp; &nbsp; &nbsp; : GenuineIntel<br> >> cpu family &nbsp; &nbsp; &nbsp;: 6<br> >> model &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : 15<br> >> model name &nbsp; &nbsp; &nbsp;: Intel(R) Xeon(R) CPU &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5150 &nbsp;@ 2.66GHz<br> >> stepping &nbsp; &nbsp; &nbsp; &nbsp;: 6<br> >> cpu MHz &nbsp; &nbsp; &nbsp; &nbsp; : 2660.007<br> >> cache size &nbsp; &nbsp; &nbsp;: 4096 KB<br> >> physical id &nbsp; &nbsp; : 3<br> >> siblings &nbsp; &nbsp; &nbsp; &nbsp;: 2<br> >> core id &nbsp; &nbsp; &nbsp; &nbsp; : 0<br> >> cpu cores &nbsp; &nbsp; &nbsp; : 2<br> >> fpu &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : yes<br> >> fpu_exception &nbsp; : yes<br> >> cpuid level &nbsp; &nbsp; : 10<br> >> wp &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;: yes<br> >> flags &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca<br> >> cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm syscall nx lm<br> >> constant_tsc pni monitor ds_cpl vmx est tm2 cx16 xtpr lahf_lm<br> >> bogomips &nbsp; &nbsp; &nbsp; &nbsp;: 5320.03<br> >> clflush size &nbsp; &nbsp;: 64<br> >> cache_alignment : 64<br> >> address sizes &nbsp; : 36 bits physical, 48 bits virtual<br> >> power management:<br> >><br> >> processor &nbsp; &nbsp; &nbsp; : 2<br> >> vendor_id &nbsp; &nbsp; &nbsp; : GenuineIntel<br> >> cpu family &nbsp; &nbsp; &nbsp;: 6<br> >> model &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : 15<br> >> model name &nbsp; &nbsp; &nbsp;: Intel(R) Xeon(R) CPU &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5150 &nbsp;@ 2.66GHz<br> >> stepping &nbsp; &nbsp; &nbsp; &nbsp;: 6<br> >> cpu MHz &nbsp; &nbsp; &nbsp; &nbsp; : 2660.007<br> >> cache size &nbsp; &nbsp; &nbsp;: 4096 KB<br> >> physical id &nbsp; &nbsp; : 0<br> >> siblings &nbsp; &nbsp; &nbsp; &nbsp;: 2<br> >> core id &nbsp; &nbsp; &nbsp; &nbsp; : 1<br> >> cpu cores &nbsp; &nbsp; &nbsp; : 2<br> >> fpu &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : yes<br> >> fpu_exception &nbsp; : yes<br> >> cpuid level &nbsp; &nbsp; : 10<br> >> wp &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;: yes<br> >> flags &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca<br> >> cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm syscall nx lm<br> >> constant_tsc pni monitor ds_cpl vmx est tm2 cx16 xtpr lahf_lm<br> >> bogomips &nbsp; &nbsp; &nbsp; &nbsp;: 5319.39<br> >> clflush size &nbsp; &nbsp;: 64<br> >> cache_alignment : 64<br> >> address sizes &nbsp; : 36 bits physical, 48 bits virtual<br> >> power management:<br> >><br> >> processor &nbsp; &nbsp; &nbsp; : 3<br> >> vendor_id &nbsp; &nbsp; &nbsp; : GenuineIntel<br> >> cpu family &nbsp; &nbsp; &nbsp;: 6<br> >> model &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : 15<br> >> model name &nbsp; &nbsp; &nbsp;: Intel(R) Xeon(R) CPU &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5150 &nbsp;@ 2.66GHz<br> >> stepping &nbsp; &nbsp; &nbsp; &nbsp;: 6<br> >> cpu MHz &nbsp; &nbsp; &nbsp; &nbsp; : 2660.007<br> >> cache size &nbsp; &nbsp; &nbsp;: 4096 KB<br> >> physical id &nbsp; &nbsp; : 3<br> >> siblings &nbsp; &nbsp; &nbsp; &nbsp;: 2<br> >> core id &nbsp; &nbsp; &nbsp; &nbsp; : 1<br> >> cpu cores &nbsp; &nbsp; &nbsp; : 2<br> >> fpu &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : yes<br> >> fpu_exception &nbsp; : yes<br> >> cpuid level &nbsp; &nbsp; : 10<br> >> wp &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;: yes<br> >> flags &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca<br> >> cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm syscall nx lm<br> >> constant_tsc pni monitor ds_cpl vmx est tm2 cx16 xtpr lahf_lm<br> >> bogomips &nbsp; &nbsp; &nbsp; &nbsp;: 5320.03<br> >> clflush size &nbsp; &nbsp;: 64<br> >> cache_alignment : 64<br> >> address sizes &nbsp; : 36 bits physical, 48 bits virtual<br> >> power management:<br> >><br> >><br> >>> ------------------------------<br> >>><br> >>> Message: 2<br> >>> Date: Mon, 4 May 2009 04:45:57 -0600<br> >>> From: Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>><br> >>> Subject: Re: [OMPI users] 1.3.1 -rf rankfile behaviour ??<br> >>> To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>><br> >>> Message-ID: &lt;<a href="mailto:D01D7B16-4B47-46F3-AD41-D1A90B2E4927@open-mpi.org" target="_blank">D01D7B16-4B47-46F3-AD41-D1A90B2E4927@open-mpi.org</a>><br> >>><br> >>> Content-Type: text/plain; charset="us-ascii"; Format="flowed";<br> >>> &nbsp; &nbsp; &nbsp; &nbsp;DelSp="yes"<br> >>><br> >>> My apologies - I wasn't clear enough. You need a tarball from r21111<br> >>> or greater...such as:<br> >>><br> >>> <a href="http://www.open-mpi.org/nightly/trunk/openmpi-1.4a1r21142.tar.gz" target="_blank">http://www.open-mpi.org/nightly/trunk/openmpi-1.4a1r21142.tar.gz</a><br> >>><br> >>> HTH<br> >>> Ralph<br> >>><br> >>><br> >>> On May 4, 2009, at 2:14 AM, Geoffroy Pignot wrote:<br> >>><br> >>> > Hi ,<br> >>> ><br> >>> > I got the openmpi-1.4a1r21095.tar.gz tarball, but unfortunately my<br> >>> > command doesn't work<br> >>> ><br> >>> > cat rankf:<br> >>> > rank 0=node1 slot=*<br> >>> > rank 1=node2 slot=*<br> >>> ><br> >>> > cat hostf:<br> >>> > node1 slots=2<br> >>> > node2 slots=2<br> >>> ><br> >>> > mpirun &nbsp;--rankfile rankf --hostfile hostf &nbsp;--host node1 -n 1<br> >>> > hostname : --host node2 -n 1 hostname<br> >>> ><br> >>> > Error, invalid rank (1) in the rankfile (rankf)<br> >>> ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > [r011n006:28986] [[45541,0],0] ORTE_ERROR_LOG: Bad parameter in file<br> >>> > rmaps_rank_file.c at line 403<br> >>> > [r011n006:28986] [[45541,0],0] ORTE_ERROR_LOG: Bad parameter in file<br> >>> > base/rmaps_base_map_job.c at line 86<br> >>> > [r011n006:28986] [[45541,0],0] ORTE_ERROR_LOG: Bad parameter in file<br> >>> > base/plm_base_launch_support.c at line 86<br> >>> > [r011n006:28986] [[45541,0],0] ORTE_ERROR_LOG: Bad parameter in file<br> >>> > plm_rsh_module.c at line 1016<br> >>> ><br> >>> ><br> >>> > Ralph, could you tell me if my command syntax is correct or not ? if<br> >>> > not, give me the expected one ?<br> >>> ><br> >>> > Regards<br> >>> ><br> >>> > Geoffroy<br> >>> ><br> >>> ><br> >>> ><br> >>> ><br> >>> > 2009/4/30 Geoffroy Pignot &lt;<a href="mailto:geopignot@gmail.com" target="_blank">geopignot@gmail.com</a>><br> >>> > Immediately Sir !!! :)<br> >>> ><br> >>> > Thanks again Ralph<br> >>> ><br> >>> > Geoffroy<br> >>> ><br> >>> ><br> >>> ><br> >>> ><br> >>> ><br> >>> > ------------------------------<br> >>> ><br> >>> > Message: 2<br> >>> > Date: Thu, 30 Apr 2009 06:45:39 -0600<br> >>> > From: Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>><br> >>> > Subject: Re: [OMPI users] 1.3.1 -rf rankfile behaviour ??<br> >>> > To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>><br> >>> > Message-ID:<br> >>> > &nbsp; &nbsp; &nbsp; &nbsp;&lt;<a href="mailto:71d2d8cc0904300545v61a42fe1k50086d2704d0f7e6@mail.gmail.com" target="_blank">71d2d8cc0904300545v61a42fe1k50086d2704d0f7e6@mail.gmail.com</a>><br> >>> > Content-Type: text/plain; charset="iso-8859-1"<br> >>> ><br> >>> > I believe this is fixed now in our development trunk - you can<br> >>> > download any<br> >>> > tarball starting from last night and give it a try, if you like. Any<br> >>> > feedback would be appreciated.<br> >>> ><br> >>> > Ralph<br> >>> ><br> >>> ><br> >>> > On Apr 14, 2009, at 7:57 AM, Ralph Castain wrote:<br> >>> ><br> >>> > Ah now, I didn't say it -worked-, did I? :-)<br> >>> ><br> >>> > Clearly a bug exists in the program. I'll try to take a look at it<br> >>> > (if Lenny<br> >>> > doesn't get to it first), but it won't be until later in the week.<br> >>> ><br> >>> > On Apr 14, 2009, at 7:18 AM, Geoffroy Pignot wrote:<br> >>> ><br> >>> > I agree with you Ralph , and that 's what I expect from openmpi but my<br> >>> > second example shows that it's not working<br> >>> ><br> >>> > cat hostfile.0<br> >>> > &nbsp; r011n002 slots=4<br> >>> > &nbsp; r011n003 slots=4<br> >>> ><br> >>> > &nbsp;cat rankfile.0<br> >>> > &nbsp; &nbsp;rank 0=r011n002 slot=0<br> >>> > &nbsp; &nbsp;rank 1=r011n003 slot=1<br> >>> ><br> >>> > mpirun --hostfile hostfile.0 -rf rankfile.0 -n 1 hostname : -n 1<br> >>> > hostname<br> >>> > ### CRASHED<br> >>> ><br> >>> > > > Error, invalid rank (1) in the rankfile (rankfile.0)<br> >>> > > ><br> >>> > ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > > > [r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in<br> >>> > file<br> >>> > > > rmaps_rank_file.c at line 404<br> >>> > > > [r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in<br> >>> > file<br> >>> > > > base/rmaps_base_map_job.c at line 87<br> >>> > > > [r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in<br> >>> > file<br> >>> > > > base/plm_base_launch_support.c at line 77<br> >>> > > > [r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in<br> >>> > file<br> >>> > > > plm_rsh_module.c at line 985<br> >>> > > ><br> >>> > ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > > > A daemon (pid unknown) died unexpectedly on signal 1 &nbsp;while<br> >>> > > attempting to<br> >>> > > > launch so we are aborting.<br> >>> > > ><br> >>> > > > There may be more information reported by the environment (see<br> >>> > > above).<br> >>> > > ><br> >>> > > > This may be because the daemon was unable to find all the needed<br> >>> > > shared<br> >>> > > > libraries on the remote node. You may set your LD_LIBRARY_PATH to<br> >>> > > have the<br> >>> > > > location of the shared libraries on the remote nodes and this will<br> >>> > > > automatically be forwarded to the remote nodes.<br> >>> > > ><br> >>> > ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > > ><br> >>> > ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > > > orterun noticed that the job aborted, but has no info as to the<br> >>> > > process<br> >>> > > > that caused that situation.<br> >>> > > ><br> >>> > ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > > > orterun: clean termination accomplished<br> >>> ><br> >>> ><br> >>> ><br> >>> > Message: 4<br> >>> > Date: Tue, 14 Apr 2009 06:55:58 -0600<br> >>> > From: Ralph Castain &lt;<a href="mailto:rhc@lanl.gov" target="_blank">rhc@lanl.gov</a>><br> >>> > Subject: Re: [OMPI users] 1.3.1 -rf rankfile behaviour ??<br> >>> > To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>><br> >>> > Message-ID: &lt;<a href="mailto:F6290ADA-A196-43F0-A853-CBCB802D8D9C@lanl.gov" target="_blank">F6290ADA-A196-43F0-A853-CBCB802D8D9C@lanl.gov</a>><br> >>> > Content-Type: text/plain; charset="us-ascii"; Format="flowed";<br> >>> > &nbsp; &nbsp; &nbsp; DelSp="yes"<br> >>> ><br> >>> > The rankfile cuts across the entire job - it isn't applied on an<br> >>> > app_context basis. So the ranks in your rankfile must correspond to<br> >>> > the eventual rank of each process in the cmd line.<br> >>> ><br> >>> > Unfortunately, that means you have to count ranks. In your case, you<br> >>> > only have four, so that makes life easier. Your rankfile would look<br> >>> > something like this:<br> >>> ><br> >>> > rank 0=r001n001 slot=0<br> >>> > rank 1=r001n002 slot=1<br> >>> > rank 2=r001n001 slot=1<br> >>> > rank 3=r001n002 slot=2<br> >>> ><br> >>> > HTH<br> >>> > Ralph<br> >>> ><br> >>> > On Apr 14, 2009, at 12:19 AM, Geoffroy Pignot wrote:<br> >>> ><br> >>> > > Hi,<br> >>> > ><br> >>> > > I agree that my examples are not very clear. What I want to do is to<br> >>> > > launch a multiexes application (masters-slaves) and benefit from the<br> >>> > > processor affinity.<br> >>> > > Could you show me how to convert this command , using -rf option<br> >>> > > (whatever the affinity is)<br> >>> > ><br> >>> > > mpirun -n 1 -host r001n001 master.x options1 &nbsp;: -n 1 -host r001n002<br> >>> > > master.x options2 : -n 1 -host r001n001 slave.x options3 : -n 1 -<br> >>> > > host r001n002 slave.x options4<br> >>> > ><br> >>> > > Thanks for your help<br> >>> > ><br> >>> > > Geoffroy<br> >>> > ><br> >>> > ><br> >>> > ><br> >>> > ><br> >>> > ><br> >>> > > Message: 2<br> >>> > > Date: Sun, 12 Apr 2009 18:26:35 +0300<br> >>> > > From: Lenny Verkhovsky &lt;<a href="mailto:lenny.verkhovsky@gmail.com" target="_blank">lenny.verkhovsky@gmail.com</a>><br> >>> > > Subject: Re: [OMPI users] 1.3.1 -rf rankfile behaviour ??<br> >>> > > To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>><br> >>> > > Message-ID:<br> >>> > > &nbsp; &nbsp; &nbsp; &nbsp;&lt;<a href="mailto:453d39990904120826t2e1d1d33l7bb1fe3de65b5361@mail.gmail.com" target="_blank">453d39990904120826t2e1d1d33l7bb1fe3de65b5361@mail.gmail.com</a>><br> >>> > > Content-Type: text/plain; charset="iso-8859-1"<br> >>> > ><br> >>> > > Hi,<br> >>> > ><br> >>> > > The first "crash" is OK, since your rankfile has ranks 0 and 1<br> >>> > > defined,<br> >>> > > while n=1, which means only rank 0 is present and can be allocated.<br> >>> > ><br> >>> > > NP must be >= the largest rank in rankfile.<br> >>> > ><br> >>> > > What exactly are you trying to do ?<br> >>> > ><br> >>> > > I tried to recreate your seqv but all I got was<br> >>> > ><br> >>> > > ~/work/svn/ompi/trunk/build_x86-64/install/bin/mpirun --hostfile<br> >>> > > hostfile.0<br> >>> > > -rf rankfile.0 -n 1 hostname : -rf rankfile.1 -n 1 hostname<br> >>> > > [witch19:30798] mca: base: component_find: paffinity<br> >>> > > "mca_paffinity_linux"<br> >>> > > uses an MCA interface that is not recognized (component MCA<br> >>> > v1.0.0 !=<br> >>> > > supported MCA v2.0.0) -- ignored<br> >>> > ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > > It looks like opal_init failed for some reason; your parallel<br> >>> > > process is<br> >>> > > likely to abort. There are many reasons that a parallel process can<br> >>> > > fail during opal_init; some of which are due to configuration or<br> >>> > > environment problems. This failure appears to be an internal<br> >>> > failure;<br> >>> > > here's some additional information (which may only be relevant to an<br> >>> > > Open MPI developer):<br> >>> > ><br> >>> > > &nbsp;opal_carto_base_select failed<br> >>> > > &nbsp;--> Returned value -13 instead of OPAL_SUCCESS<br> >>> > ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > > [witch19:30798] [[INVALID],INVALID] ORTE_ERROR_LOG: Not found in<br> >>> > file<br> >>> > > ../../orte/runtime/orte_init.c at line 78<br> >>> > > [witch19:30798] [[INVALID],INVALID] ORTE_ERROR_LOG: Not found in<br> >>> > file<br> >>> > > ../../orte/orted/orted_main.c at line 344<br> >>> > ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > > A daemon (pid 11629) died unexpectedly with status 243 while<br> >>> > > attempting<br> >>> > > to launch so we are aborting.<br> >>> > ><br> >>> > > There may be more information reported by the environment (see<br> >>> > above).<br> >>> > ><br> >>> > > This may be because the daemon was unable to find all the needed<br> >>> > > shared<br> >>> > > libraries on the remote node. You may set your LD_LIBRARY_PATH to<br> >>> > > have the<br> >>> > > location of the shared libraries on the remote nodes and this will<br> >>> > > automatically be forwarded to the remote nodes.<br> >>> > ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > > mpirun noticed that the job aborted, but has no info as to the<br> >>> > process<br> >>> > > that caused that situation.<br> >>> > ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > > mpirun: clean termination accomplished<br> >>> > ><br> >>> > ><br> >>> > > Lenny.<br> >>> > ><br> >>> > ><br> >>> > > On 4/10/09, Geoffroy Pignot &lt;<a href="mailto:geopignot@gmail.com" target="_blank">geopignot@gmail.com</a>> wrote:<br> >>> > > ><br> >>> > > > Hi ,<br> >>> > > ><br> >>> > > > I am currently testing the process affinity capabilities of<br> >>> > > openmpi and I<br> >>> > > > would like to know if the rankfile behaviour I will describe below<br> >>> > > is normal<br> >>> > > > or not ?<br> >>> > > ><br> >>> > > > cat hostfile.0<br> >>> > > > r011n002 slots=4<br> >>> > > > r011n003 slots=4<br> >>> > > ><br> >>> > > > cat rankfile.0<br> >>> > > > rank 0=r011n002 slot=0<br> >>> > > > rank 1=r011n003 slot=1<br> >>> > > ><br> >>> > > ><br> >>> > > ><br> >>> > ><br> >>> ><br> >>> ##################################################################################<br> >>> > > ><br> >>> > > > mpirun --hostfile hostfile.0 -rf rankfile.0 -n 2 &nbsp;hostname ### OK<br> >>> > > > r011n002<br> >>> > > > r011n003<br> >>> > > ><br> >>> > > ><br> >>> > > ><br> >>> > ><br> >>> ><br> >>> ##################################################################################<br> >>> > > > but<br> >>> > > > mpirun --hostfile hostfile.0 -rf rankfile.0 -n 1 hostname : -n 1<br> >>> > > hostname<br> >>> > > > ### CRASHED<br> >>> > > > *<br> >>> > > ><br> >>> > ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > > > Error, invalid rank (1) in the rankfile (rankfile.0)<br> >>> > > ><br> >>> > ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > > > [r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in<br> >>> > file<br> >>> > > > rmaps_rank_file.c at line 404<br> >>> > > > [r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in<br> >>> > file<br> >>> > > > base/rmaps_base_map_job.c at line 87<br> >>> > > > [r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in<br> >>> > file<br> >>> > > > base/plm_base_launch_support.c at line 77<br> >>> > > > [r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in<br> >>> > file<br> >>> > > > plm_rsh_module.c at line 985<br> >>> > > ><br> >>> > ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > > > A daemon (pid unknown) died unexpectedly on signal 1 &nbsp;while<br> >>> > > attempting to<br> >>> > > > launch so we are aborting.<br> >>> > > ><br> >>> > > > There may be more information reported by the environment (see<br> >>> > > above).<br> >>> > > ><br> >>> > > > This may be because the daemon was unable to find all the needed<br> >>> > > shared<br> >>> > > > libraries on the remote node. You may set your LD_LIBRARY_PATH to<br> >>> > > have the<br> >>> > > > location of the shared libraries on the remote nodes and this will<br> >>> > > > automatically be forwarded to the remote nodes.<br> >>> > > ><br> >>> > ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > > ><br> >>> > ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > > > orterun noticed that the job aborted, but has no info as to the<br> >>> > > process<br> >>> > > > that caused that situation.<br> >>> > > ><br> >>> > ><br> >>> ><br> >>> --------------------------------------------------------------------------<br> >>> > > > orterun: clean termination accomplished<br> >>> > > > *<br> >>> > > > It seems that the rankfile option is not propagted to the second<br> >>> > > command<br> >>> > > > line ; there is no global understanding of the ranking inside a<br> >>> > > mpirun<br> >>> > > > command.<br> >>> > > ><br> >>> > > ><br> >>> > > ><br> >>> > ><br> >>> ><br> >>> ##################################################################################<br> >>> > > ><br> >>> > > > Assuming that , I tried to provide a rankfile to each command<br> >>> > line:<br> >>> > > ><br> >>> > > > cat rankfile.0<br> >>> > > > rank 0=r011n002 slot=0<br> >>> > > ><br> >>> > > > cat rankfile.1<br> >>> > > > rank 0=r011n003 slot=1<br> >>> > > ><br> >>> > > > mpirun --hostfile hostfile.0 -rf rankfile.0 -n 1 hostname : -rf<br> >>> > > rankfile.1<br> >>> > > > -n 1 hostname ### CRASHED<br> >>> > > > *[r011n002:28778] *** Process received signal ***<br> >>> > > > [r011n002:28778] Signal: Segmentation fault (11)<br> >>> > > > [r011n002:28778] Signal code: Address not mapped (1)<br> >>> > > > [r011n002:28778] Failing at address: 0x34<br> >>> > > > [r011n002:28778] [ 0] [0xffffe600]<br> >>> > > > [r011n002:28778] [ 1]<br> >>> > > > /tmp/HALMPI/openmpi-1.3.1/lib/libopen-rte.so.<br> >>> > > 0(orte_odls_base_default_get_add_procs_data+0x55d)<br> >>> > > > [0x5557decd]<br> >>> > > > [r011n002:28778] [ 2]<br> >>> > > > /tmp/HALMPI/openmpi-1.3.1/lib/libopen-rte.so.<br> >>> > > 0(orte_plm_base_launch_apps+0x117)<br> >>> > > > [0x555842a7]<br> >>> > > > [r011n002:28778] [ 3] /tmp/HALMPI/openmpi-1.3.1/lib/openmpi/<br> >>> > > mca_plm_rsh.so<br> >>> > > > [0x556098c0]<br> >>> > > > [r011n002:28778] [ 4] /tmp/HALMPI/openmpi-1.3.1/bin/orterun<br> >>> > > [0x804aa27]<br> >>> > > > [r011n002:28778] [ 5] /tmp/HALMPI/openmpi-1.3.1/bin/orterun<br> >>> > > [0x804a022]<br> >>> > > > [r011n002:28778] [ 6] /lib/libc.so.6(__libc_start_main+0xdc)<br> >>> > > [0x9f1dec]<br> >>> > > > [r011n002:28778] [ 7] /tmp/HALMPI/openmpi-1.3.1/bin/orterun<br> >>> > > [0x8049f71]<br> >>> > > > [r011n002:28778] *** End of error message ***<br> >>> > > > Segmentation fault (core dumped)*<br> >>> > > ><br> >>> > > ><br> >>> > > ><br> >>> > > > I hope that I've found a bug because it would be very important<br> >>> > > for me to<br> >>> > > > have this kind of capabiliy .<br> >>> > > > Launch a multiexe mpirun command line and be able to bind my exes<br> >>> > > and<br> >>> > > > sockets together.<br> >>> > > ><br> >>> > > > Thanks in advance for your help<br> >>> > > ><br> >>> > > > Geoffroy<br> >>> > > _______________________________________________<br> >>> > > users mailing list<br> >>> > > <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> >>> > > <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> >>> ><br> >>> > -------------- next part --------------<br> >>> > HTML attachment scrubbed and removed<br> >>> ><br> >>> > ------------------------------<br> >>> ><br> >>> > _______________________________________________<br> >>> > users mailing list<br> >>> > <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> >>> > <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> >>> ><br> >>> > End of users Digest, Vol 1202, Issue 2<br> >>> > **************************************<br> >>> ><br> >>> > _______________________________________________<br> >>> > users mailing list<br> >>> > <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> >>> > <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> >>> ><br> >>> > _______________________________________________<br> >>> > users mailing list<br> >>> > <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> >>> > -------------- next part --------------<br> >>> > HTML attachment scrubbed and removed<br> >>> ><br> >>> > ------------------------------<br> >>> ><br> >>> > _______________________________________________<br> >>> > users mailing list<br> >>> > <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> >>> > <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> >>> ><br> >>> > End of users Digest, Vol 1218, Issue 2<br> >>> > **************************************<br> >>> ><br> >>> ><br> >>> > _______________________________________________<br> >>> > users mailing list<br> >>> > <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> >>> > <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> >>><br> >>> -------------- next part --------------<br> >>> HTML attachment scrubbed and removed<br> >>><br> >>> ------------------------------<br> >>><br> >>> _______________________________________________<br> >>> users mailing list<br> >>> <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> >>> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> >>><br> >>> End of users Digest, Vol 1221, Issue 3<br> >>> **************************************<br> >>><br> >><br> >><br> >> _______________________________________________<br> >> users mailing list<br> >> <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> >> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> >><br> ><br> ><br> -------------- next part --------------<br> HTML attachment scrubbed and removed<br> <br> ------------------------------<br> <br> _______________________________________________<br> users mailing list<br> <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> <br></div></div> End of users Digest, Vol 1221, Issue 17<br> ***************************************<br> </blockquote></div><br> </blockquote></div><br> _______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></body></html>
