<html><body bgcolor="#FFFFFF"><div>Ah! &nbsp;Are you using MPI-thread-multiple?<br><br>Sent from my phone.<span class="Apple-style-span" style="-webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469); ">&nbsp;No type good.&nbsp;</span></div><div><br>On Sep 14, 2011, at 5:28 AM, "devendra rai" &lt;<a href="mailto:rai.devendra@yahoo.co.uk">rai.devendra@yahoo.co.uk</a>&gt; wrote:<br><br></div><div></div><blockquote type="cite"><div><div style="color:#000; background-color:#fff; font-family:Courier New, courier, monaco, monospace, sans-serif;font-size:10pt"><div><span>Hello MPI Forum, Jeff</span></div><div><br><span></span></div><div><span>Well, the problem is not with incorrect arguments with MPI_Ssend(...) and/or MPI_Recv(...).&nbsp; I confirmed this. Also, if there were any obvious programming and logical errors, I would never have any successful run. In fact, I am having some&nbsp; runs which go through successfully.</span></div><div><br><span></span></div><div><span>The problem seems to be random, and therefore, I am finding it hard to debug. I am using Open MPI v. 1.4.3, on Linux kernel 2.6.37.6-0.7. <br></span></div><div><br><span></span></div><div><span>I am also using tags, in addition to normal sender node and receiver nodes. I am using integers for tags, and none of the tags are more than 1000. I am also running MPI_Ssends and MPI_Recv within pthreads, and I
 believe that it should be okay.</span></div><div><br><span></span></div><div><span>So, the question is, am I stumbling across a bug, or an incorrect MPI installation?</span></div><div><br><span></span></div><div><span>Thanks a lot.</span></div><div><br><span></span></div><div><span>Best</span></div><div><span><br>Devendra<br></span></div><div><br></div><div style="font-family: Courier New, courier, monaco, monospace, sans-serif; font-size: 10pt;"><div style="font-family: times new roman, new york, times, serif; font-size: 12pt;"><font face="Arial" size="2"><hr size="1"><b><span style="font-weight:bold;">From:</span></b> Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br><b><span style="font-weight: bold;">To:</span></b> devendra rai &lt;<a href="mailto:rai.devendra@yahoo.co.uk">rai.devendra@yahoo.co.uk</a>&gt;; Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br><b><span style="font-weight: bold;">Sent:</span></b> Tuesday, 13 September 2011, 16:13<br><b><span style="font-weight: bold;">Subject:</span></b> Re:
 [OMPI users] Question on MPI_Ssend<br></font><br>On Sep 13, 2011, at 8:41 AM, devendra rai wrote:<br><br>&gt; Also, I read the definition of MPI_Ssend(...) that you sent, but then it does not explain why both MPI_Ssend(...) and MPI_Recv(...) are blocked seemingly forever. <br><br>Oh, they're blocked *forever*!&nbsp; Sorry; I didn't get that from your prior description -- I thought you just wanted non-blocking instead of blocking.<br><br>&gt; I notice that such a block happens when MPI_Recv(...) is posted before MPI_Ssend(...).<br><br>It doesn't matter if the receive is posted before the send or the other way around.<br><br>When blocking sends/receives block forever, it usually means that there's a mismatch in the communicator, tag, or src/dest arguments between the two.<br><br>-- <br>Jeff Squyres<br><a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com"><a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a></a><br>For corporate legal information go to:<br><a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank"><a href="http://www.cisco.com/web/about/doing_business/legal/cri/">http://www.cisco.com/web/about/doing_business/legal/cri/</a></a><br><br><br><br></div></div></div></div></blockquote></body></html>