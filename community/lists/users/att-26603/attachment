<div dir="ltr"><img src="https://t.yesware.com/t/d1fcbaa1b12e0f6b1beef0b50d5ebbd873d1b8f9/0aa9db0e0ae8ef2e1a98ea7fcac355c9/spacer.gif" style="border: 0px; width: 0px; height: 0px; overflow: hidden;" width="0" height="0"><img src="http://t.yesware.com/t/d1fcbaa1b12e0f6b1beef0b50d5ebbd873d1b8f9/0aa9db0e0ae8ef2e1a98ea7fcac355c9/spacer.gif" style="border: 0px; width: 0px; height: 0px; overflow: hidden;" width="0" height="0">Hi Jeff,<font face="yw-d1fcbaa1b12e0f6b1beef0b50d5ebbd873d1b8f9-0aa9db0e0ae8ef2e1a98ea7fcac355c9--to" style></font><div><br></div><div>Yes, the memory issue caused by Isend/Irecv without calling MPI_Wait probably is the reason. Attached is my test results showing that calling MPI_Isend without using MPI_Wait at all leads to a wired wtime for my program. The Wtime should be linear, but some jumps show up after several iterations.</div><div><br></div><div>Yes, I&#39;m using MPI_Waitall or Testall in my case for the common approach. For one iteration, my common approach is </div><div>-----------------------------------------------------------------------------------------------</div><div>// </div><div>// packing data</div><div>//</div><div> ....</div><div><br></div><div> //</div><div> // init send/rev </div><div> //</div><div> for(int z=0;z&lt;_n_proc;++z){</div><div>      int nif=n_if_to_proc[z];</div><div>      </div><div>      //send data</div><div>      if(nif&gt;0){</div><div>        MPI_Isend(&amp;snd_buf_[z][0],n_buf_[z],MPI_DOUBLE,z,tag, MPI_COMM_WORLD, &amp;s_sol_req_[n_proc_exchange]);<br></div><div>        MPI_Irecv(&amp;rev_buf_[z][0],n_buf_[z],MPI_DOUBLE,z,tag, MPI_COMM_WORLD, &amp;r_sol_req_[n_proc_exchange]);</div><div>        n_proc_exchange++;</div><div>      }</div><div>  }</div><div><br></div><div>  //</div><div>  // unpacking and do some local jobs here</div><div>  //</div><div>  ....</div><div><br></div><div>  //</div><div>  // wait for send/rev finish</div><div>  //</div><div>  MPI_Waitall(n_proc_exchange_,s_sol_req_,MPI_STATUS_IGNORE);</div><div>  MPI_Waitall(n_proc_exchange_,r_sol_req_,MPI_STATUS_IGNORE);<br></div><div>  </div><div>  //do some jobs which depend on the exchanged data (rev_buf_)</div><div>  ..... </div><div>-----------------------------------------------------------------------------------------------<br></div><div><br></div><div><br></div><div>But I want to avoid calling MPI_Waitall,  since for my case, I dont care the data is the latest correct one or some previous initial data. When I comment out the MPI_Waitall, some wired thing happens. I think as you said some memory is leaked. The system memory may run out after some iterations. </div><div><br></div><div>I try to use MPI_Ibsend. So I provide my own system buffer to the MPI library. But still slow down the program (as shown in the test figure). </div><div><br></div><div>Can I use MPI_Irecv with MPI_Ibsend or I need to pair MPI_Ibsend with the blocking MPI_Recv function?</div><div><br></div><div>P.S.  <span style="font-size:12.8000001907349px">Pavan suggests me to use </span><span style="font-size:12.8000001907349px">MPI_Request_free. I will give it a try.</span></div><div><span style="font-size:12.8000001907349px"><br></span></div><div><span style="font-size:12.8000001907349px">Thanks for your reply and suggestions.</span></div><div><br></div><div>Best,</div><div><br></div><div>Lei Shi</div><div><br></div></div><div class="gmail_extra"><br clear="all"><div><div class="gmail_signature">Sincerely Yours,<br><br>Lei Shi <br>---------</div></div>
<br><div class="gmail_quote">On Fri, Apr 3, 2015 at 5:34 AM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">In the general case, MPI defines that you *have* to call one of the MPI_Test or MPI_Wait functions to finish the communication request.  If you don&#39;t do so, you&#39;re basically leaking resources (e.g., memory).<br>
<br>
In a single-threaded MPI implementation, the call to MPI_Test/MPI_Wait/etc. may be where the actual message passing progress occurs, too.<br>
<br>
If you don&#39;t want to block waiting for the communication, you can keep an array of outstanding MPI_Requests and call MPI_Testall() on them to allow MPI to make progress on them, but not block your application until all (or any) of them complete.<br>
<span class=""><br>
<br>
<br>
&gt; On Apr 3, 2015, at 3:43 AM, Matthieu Brucher &lt;<a href="mailto:matthieu.brucher@gmail.com">matthieu.brucher@gmail.com</a>&gt; wrote:<br>
&gt;<br>
&gt; Hi,<br>
&gt;<br>
&gt; I think you have to call either Wait or Test to make the communications move forward in the general case. Some hardware may have a hardware thread that makes the communication, but usually you have to make it &quot;advance&quot; yourself by either calling Wait ot Test.<br>
&gt;<br>
&gt; Cheers,<br>
&gt;<br>
&gt; Matthieu<br>
&gt;<br>
&gt; 2015-04-03 5:48 GMT+01:00 Lei Shi &lt;<a href="mailto:leishi@ku.edu">leishi@ku.edu</a>&gt;:<br>
&gt; I want to use non-blocking send/rev MPI_Isend/MPI_Irev to do communication. But in my case, I don&#39;t really care what kind of data I get or it is ready to use or not. So I don&#39;t want to waste my time to do any synchronization  by calling MPI_Wait or etc API.<br>
&gt;<br>
&gt; But when I avoid calling MPI_Wait, my program is freezed several secs after running some iterations (after multiple MPI_Isend/Irev callings), then continues. It takes even more time than the case with MPI_Wait.  So my question is how to do a &quot;true&quot; non-blocking communication without waiting for the data ready or not. Thanks.<br>
&gt;<br>
&gt; Sincerely Yours,<br>
&gt;<br>
&gt; Lei Shi<br>
&gt; ---------<br>
&gt;<br>
&gt;<br>
</span><span class="">&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/04/26596.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/04/26596.php</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Information System Engineer, Ph.D.<br>
&gt; Blog: <a href="http://matt.eifelle.com" target="_blank">http://matt.eifelle.com</a><br>
&gt; LinkedIn: <a href="http://www.linkedin.com/in/matthieubrucher" target="_blank">http://www.linkedin.com/in/matthieubrucher</a><br>
&gt; Music band: <a href="http://liliejay.com/" target="_blank">http://liliejay.com/</a><br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</span>&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/04/26598.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/04/26598.php</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<span class=""><br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</span>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/04/26600.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/04/26600.php</a><br>
</blockquote></div><br></div>

