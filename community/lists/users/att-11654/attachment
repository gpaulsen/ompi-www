<div dir="ltr"><div>This is the a knowing issue,</div><div>        <a href="https://svn.open-mpi.org/trac/ompi/ticket/2087">https://svn.open-mpi.org/trac/ompi/ticket/2087</a><br></div><div></div><div>Maybe it&#39;s priority should be raised up.</div>
<div></div><div>Lenny.</div><div></div><br><div class="gmail_quote">On Wed, Dec 30, 2009 at 12:13 PM, Daniel Spångberg <span dir="ltr">&lt;<a href="mailto:daniels@mkem.uu.se">daniels@mkem.uu.se</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
Dear OpenMPI list,<br>
<br>
I have used the dynamic rules for collectives to be able to select one specific algorithm. With the latest versions of openmpi this seems to be broken. Just enabling coll_tuned_use_dynamic_rules causes the code to segfault. However, I do not provide a file with rules, since I just want to modify the behavior of one routine.<br>

<br>
I have tried the below example code on openmpi 1.3.2, 1.3.3, 1.3.4, and 1.4. It *works* on 1.3.2, 1.3.3, but segfaults on 1.3.4 and 1.4. I have confirmed this on Scientific Linux 5.2, and 5.4. I have also successfully reproduced the crash using version 1.4 running on debian etch. All running on amd64, compiled from source without other options to configure than --prefix. The crash occurs whether I use the intel 11.1 compiler (via env CC) or gcc. It also occurs no matter the btl is set to openib,self tcp,self sm,self or combinations of those. See below for ompi_info and other info. I have tried MPI_Alltoall, MPI_Alltoallv, and MPI_Allreduce which behave the same.<br>

<br>
#include &lt;stdlib.h&gt;<br>
#include &lt;mpi.h&gt;<br></blockquote><div> </div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
<br>
int main(int argc, char **argv)<br>
{<br>
  int rank,size;<br>
  char *buffer, *buffer2;<br>
<br>
  MPI_Init(&amp;argc,&amp;argv);<br>
<br>
  MPI_Comm_size(MPI_COMM_WORLD,&amp;size);<br>
  MPI_Comm_rank(MPI_COMM_WORLD,&amp;rank);<br>
<br>
  buffer=calloc(100*size,1);<br>
  buffer2=calloc(100*size,1);<br>
<br>
  MPI_Alltoall(buffer,100,MPI_BYTE,buffer2,100,MPI_BYTE,MPI_COMM_WORLD);<br>
<br>
  MPI_Finalize();<br>
  return 0;<br>
}<br>
<br>
Demonstrated behaviour:<br>
<br>
$ ompi_info<br>
                 Package: Open MPI daniels@arthur Distribution<br>
                Open MPI: 1.4<br>
   Open MPI SVN revision: r22285<br>
   Open MPI release date: Dec 08, 2009<br>
                Open RTE: 1.4<br>
   Open RTE SVN revision: r22285<br>
   Open RTE release date: Dec 08, 2009<br>
                    OPAL: 1.4<br>
       OPAL SVN revision: r22285<br>
       OPAL release date: Dec 08, 2009<br>
            Ident string: 1.4<br>
                  Prefix: /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install<br>
 Configured architecture: x86_64-unknown-linux-gnu<br>
          Configure host: arthur<br>
           Configured by: daniels<br>
           Configured on: Tue Dec 29 16:54:37 CET 2009<br>
          Configure host: arthur<br>
                Built by: daniels<br>
                Built on: Tue Dec 29 17:04:36 CET 2009<br>
              Built host: arthur<br>
              C bindings: yes<br>
            C++ bindings: yes<br>
      Fortran77 bindings: yes (all)<br>
      Fortran90 bindings: yes<br>
 Fortran90 bindings size: small<br>
              C compiler: gcc<br>
     C compiler absolute: /usr/bin/gcc<br>
            C++ compiler: g++<br>
   C++ compiler absolute: /usr/bin/g++<br>
      Fortran77 compiler: gfortran<br>
  Fortran77 compiler abs: /usr/bin/gfortran<br>
      Fortran90 compiler: gfortran<br>
  Fortran90 compiler abs: /usr/bin/gfortran<br>
             C profiling: yes<br>
           C++ profiling: yes<br>
     Fortran77 profiling: yes<br>
     Fortran90 profiling: yes<br>
          C++ exceptions: no<br>
          Thread support: posix (mpi: no, progress: no)<br>
           Sparse Groups: no<br>
  Internal debug support: no<br>
     MPI parameter check: runtime<br>
Memory profiling support: no<br>
Memory debugging support: no<br>
         libltdl support: yes<br>
   Heterogeneous support: no<br>
 mpirun default --prefix: no<br>
         MPI I/O support: yes<br>
       MPI_WTIME support: gettimeofday<br>
Symbol visibility support: yes<br>
   FT Checkpoint support: no  (checkpoint thread: no)<br>
           MCA backtrace: execinfo (MCA v2.0, API v2.0, Component v1.4)<br>
              MCA memory: ptmalloc2 (MCA v2.0, API v2.0, Component v1.4)<br>
           MCA paffinity: linux (MCA v2.0, API v2.0, Component v1.4)<br>
<br>
               MCA carto: auto_detect (MCA v2.0, API v2.0, Component v1.4)<br>
               MCA carto: file (MCA v2.0, API v2.0, Component v1.4)<br>
           MCA maffinity: first_use (MCA v2.0, API v2.0, Component v1.4)<br>
               MCA timer: linux (MCA v2.0, API v2.0, Component v1.4)<br>
         MCA installdirs: env (MCA v2.0, API v2.0, Component v1.4)<br>
         MCA installdirs: config (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA dpm: orte (MCA v2.0, API v2.0, Component v1.4)<br>
              MCA pubsub: orte (MCA v2.0, API v2.0, Component v1.4)<br>
           MCA allocator: basic (MCA v2.0, API v2.0, Component v1.4)<br>
           MCA allocator: bucket (MCA v2.0, API v2.0, Component v1.4)<br>
                MCA coll: basic (MCA v2.0, API v2.0, Component v1.4)<br>
                MCA coll: hierarch (MCA v2.0, API v2.0, Component v1.4)<br>
                MCA coll: inter (MCA v2.0, API v2.0, Component v1.4)<br>
                MCA coll: self (MCA v2.0, API v2.0, Component v1.4)<br>
                MCA coll: sm (MCA v2.0, API v2.0, Component v1.4)<br>
                MCA coll: sync (MCA v2.0, API v2.0, Component v1.4)<br>
                MCA coll: tuned (MCA v2.0, API v2.0, Component v1.4)<br>
                  MCA io: romio (MCA v2.0, API v2.0, Component v1.4)<br>
               MCA mpool: fake (MCA v2.0, API v2.0, Component v1.4)<br>
               MCA mpool: rdma (MCA v2.0, API v2.0, Component v1.4)<br>
               MCA mpool: sm (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA pml: cm (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA pml: csum (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA pml: ob1 (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA pml: v (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA bml: r2 (MCA v2.0, API v2.0, Component v1.4)<br>
              MCA rcache: vma (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA btl: self (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA btl: sm (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA btl: tcp (MCA v2.0, API v2.0, Component v1.4)<br>
                MCA topo: unity (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA osc: pt2pt (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA osc: rdma (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA iof: hnp (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA iof: orted (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA iof: tool (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA oob: tcp (MCA v2.0, API v2.0, Component v1.4)<br>
                MCA odls: default (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA ras: slurm (MCA v2.0, API v2.0, Component v1.4)<br>
               MCA rmaps: load_balance (MCA v2.0, API v2.0, Component v1.4)<br>
               MCA rmaps: rank_file (MCA v2.0, API v2.0, Component v1.4)<br>
               MCA rmaps: round_robin (MCA v2.0, API v2.0, Component v1.4)<br>
               MCA rmaps: seq (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA rml: oob (MCA v2.0, API v2.0, Component v1.4)<br>
              MCA routed: binomial (MCA v2.0, API v2.0, Component v1.4)<br>
              MCA routed: direct (MCA v2.0, API v2.0, Component v1.4)<br>
              MCA routed: linear (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA plm: rsh (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA plm: slurm (MCA v2.0, API v2.0, Component v1.4)<br>
               MCA filem: rsh (MCA v2.0, API v2.0, Component v1.4)<br>
              MCA errmgr: default (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA ess: env (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA ess: hnp (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA ess: singleton (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA ess: slurm (MCA v2.0, API v2.0, Component v1.4)<br>
                 MCA ess: tool (MCA v2.0, API v2.0, Component v1.4)<br>
             MCA grpcomm: bad (MCA v2.0, API v2.0, Component v1.4)<br>
             MCA grpcomm: basic (MCA v2.0, API v2.0, Component v1.4)<br>
<br>
$ mpicc -O2 -o bug_openmpi_1.4_test bug_openmpi_1.4_test.c<br>
$ ldd ./bug_openmpi_1.4_test<br>
        libmpi.so.0 =&gt; /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/libmpi.so.0 (0x00002b33fa57e000)<br>
        libopen-rte.so.0 =&gt; /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/libopen-rte.so.0 (0x00002b33fa821000)<br>
        libopen-pal.so.0 =&gt; /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/libopen-pal.so.0 (0x00002b33faa6b000)<br>
        libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00000032c7400000)<br>
        libnsl.so.1 =&gt; /lib64/libnsl.so.1 (0x00000032cfe00000)<br>
        libutil.so.1 =&gt; /lib64/libutil.so.1 (0x00000032d4a00000)<br>
        libm.so.6 =&gt; /lib64/libm.so.6 (0x00000032c7000000)<br>
        libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00000032c7800000)<br>
        libc.so.6 =&gt; /lib64/libc.so.6 (0x00000032c6c00000)<br>
        /lib64/ld-linux-x86-64.so.2 (0x00000032c5c00000)<br>
$ mpirun -mca btl tcp,self -mca coll_tuned_use_dynamic_rules 0 -np 8 ./bug_openmpi_1.4_test<br>
$ mpirun -mca btl tcp,self -mca coll_tuned_use_dynamic_rules 1 -np 8 ./bug_openmpi_1.4_test<br>
[girasole:27510] *** Process received signal ***<br>
[girasole:27510] Signal: Segmentation fault (11)<br>
[girasole:27510] Signal code:  (128)<br>
[girasole:27510] Failing at address: (nil)<br>
[girasole:27503] *** Process received signal ***<br>
[girasole:27503] Signal: Segmentation fault (11)<br>
[girasole:27503] Signal code:  (128)<br>
[girasole:27503] Failing at address: (nil)<br>
[girasole:27510] [ 0] /lib64/libpthread.so.0 [0x32c780de80]<br>
[girasole:27510] [ 1] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/openmpi/mca_coll_tuned.so [0x2ae2b29fbeb5]<br>
[girasole:27510] [ 2] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/openmpi/mca_coll_tuned.so [0x2ae2b29fa8ca]<br>
[girasole:27510] [ 3] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/libmpi.so.0(MPI_Alltoall+0x15f) [0x2ae2ae76bbff]<br>
[girasole:27510] [ 4] ./bug_openmpi_1.4_test(main+0x97) [0x4009b7]<br>
[girasole:27510] [ 5] /lib64/libc.so.6(__libc_start_main+0xf4) [0x32c6c1d8b4]<br>
[girasole:27510] [ 6] ./bug_openmpi_1.4_test [0x400869]<br>
[girasole:27510] *** End of error message ***<br>
[girasole:27503] [ 0] /lib64/libpthread.so.0 [0x32c780de80]<br>
[girasole:27503] [ 1] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/openmpi/mca_coll_tuned.so [0x2b534b1b6eb5]<br>
[girasole:27503] [ 2] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/openmpi/mca_coll_tuned.so [0x2b534b1b58ca]<br>
[girasole:27503] [ 3] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/libmpi.so.0(MPI_Alltoall+0x15f) [0x2b5346f26bff]<br>
[girasole:27503] [ 4] ./bug_openmpi_1.4_test(main+0x97) [0x4009b7]<br>
[girasole:27503] [ 5] /lib64/libc.so.6(__libc_start_main+0xf4) [0x32c6c1d8b4]<br>
[girasole:27503] [ 6] ./bug_openmpi_1.4_test [0x400869]<br>
[girasole:27503] *** End of error message ***<br>
[girasole:27505] *** Process received signal ***<br>
[girasole:27505] Signal: Segmentation fault (11)<br>
[girasole:27505] Signal code:  (128)<br>
[girasole:27505] Failing at address: (nil)<br>
[girasole:27509] *** Process received signal ***<br>
[girasole:27509] Signal: Segmentation fault (11)<br>
[girasole:27509] Signal code:  (128)<br>
[girasole:27509] Failing at address: (nil)<br>
[girasole:27505] [ 0] /lib64/libpthread.so.0 [0x32c780de80]<br>
[girasole:27505] [ 1] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/openmpi/mca_coll_tuned.so [0x2ab662aa0eb5]<br>
[girasole:27505] [ 2] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/openmpi/mca_coll_tuned.so [0x2ab662a9f8ca]<br>
[girasole:27505] [ 3] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/libmpi.so.0(MPI_Alltoall+0x15f) [0x2ab65e810bff]<br>
[girasole:27505] [ 4] ./bug_openmpi_1.4_test(main+0x97) [0x4009b7]<br>
[girasole:27505] [ 5] /lib64/libc.so.6(__libc_start_main+0xf4) [0x32c6c1d8b4]<br>
[girasole:27505] [ 6] ./bug_openmpi_1.4_test [0x400869]<br>
[girasole:27505] *** End of error message ***<br>
[girasole:27507] *** Process received signal ***<br>
[girasole:27507] Signal: Segmentation fault (11)<br>
[girasole:27507] Signal code:  (128)<br>
[girasole:27507] Failing at address: (nil)<br>
[girasole:27509] [ 0] /lib64/libpthread.so.0 [0x32c780de80]<br>
[girasole:27509] [ 1] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/openmpi/mca_coll_tuned.so [0x2b7dc1863eb5]<br>
[girasole:27509] [ 2] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/openmpi/mca_coll_tuned.so [0x2b7dc18628ca]<br>
[girasole:27509] [ 3] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/libmpi.so.0(MPI_Alltoall+0x15f) [0x2b7dbd5d3bff]<br>
[girasole:27509] [ 4] ./bug_openmpi_1.4_test(main+0x97) [0x4009b7]<br>
[girasole:27509] [ 5] /lib64/libc.so.6(__libc_start_main+0xf4) [0x32c6c1d8b4]<br>
[girasole:27509] [ 6] ./bug_openmpi_1.4_test [0x400869]<br>
[girasole:27509] *** End of error message ***<br>
[girasole:27507] [ 0] /lib64/libpthread.so.0 [0x32c780de80]<br>
[girasole:27507] [ 1] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/openmpi/mca_coll_tuned.so [0x2b09eb873eb5]<br>
[girasole:27507] [ 2] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/openmpi/mca_coll_tuned.so [0x2b09eb8728ca]<br>
[girasole:27507] [ 3] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/libmpi.so.0(MPI_Alltoall+0x15f) [0x2b09e75e3bff]<br>
[girasole:27507] [ 4] ./bug_openmpi_1.4_test(main+0x97) [0x4009b7]<br>
[girasole:27507] [ 5] /lib64/libc.so.6(__libc_start_main+0xf4) [0x32c6c1d8b4]<br>
[girasole:27507] [ 6] ./bug_openmpi_1.4_test [0x400869]<br>
[girasole:27507] *** End of error message ***<br>
[girasole:27504] *** Process received signal ***<br>
[girasole:27504] Signal: Segmentation fault (11)<br>
[girasole:27504] Signal code:  (128)<br>
[girasole:27504] Failing at address: (nil)<br>
[girasole:27506] *** Process received signal ***<br>
[girasole:27506] Signal: Segmentation fault (11)<br>
[girasole:27506] Signal code:  (128)<br>
[girasole:27506] Failing at address: (nil)<br>
[girasole:27504] [ 0] /lib64/libpthread.so.0 [0x32c780de80]<br>
[girasole:27504] [ 1] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/openmpi/mca_coll_tuned.so [0x2b6fde1afeb5]<br>
[girasole:27504] [ 2] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/openmpi/mca_coll_tuned.so [0x2b6fde1ae8ca]<br>
[girasole:27504] [ 3] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/libmpi.so.0(MPI_Alltoall+0x15f) [0x2b6fd9f1fbff]<br>
[girasole:27504] [ 4] ./bug_openmpi_1.4_test(main+0x97) [0x4009b7]<br>
[girasole:27504] [ 5] /lib64/libc.so.6(__libc_start_main+0xf4) [0x32c6c1d8b4]<br>
[girasole:27504] [ 6] ./bug_openmpi_1.4_test [0x400869]<br>
[girasole:27504] *** End of error message ***<br>
--------------------------------------------------------------------------<br>
mpirun noticed that process rank 7 with PID 27510 on node girasole exited on signal 11 (Segmentation fault).<br>
--------------------------------------------------------------------------<br>
[girasole:27506] [ 0] /lib64/libpthread.so.0 [0x32c780de80]<br>
[girasole:27506] [ 1] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/openmpi/mca_coll_tuned.so [0x2b66f2908eb5]<br>
[girasole:27506] [ 2] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/openmpi/mca_coll_tuned.so [0x2b66f29078ca]<br>
[girasole:27506] [ 3] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/libmpi.so.0(MPI_Alltoall+0x15f) [0x2b66ee678bff]<br>
[girasole:27506] [ 4] ./bug_openmpi_1.4_test(main+0x97) [0x4009b7]<br>
[girasole:27506] [ 5] /lib64/libc.so.6(__libc_start_main+0xf4) [0x32c6c1d8b4]<br>
[girasole:27506] [ 6] ./bug_openmpi_1.4_test [0x400869]<br>
[girasole:27506] *** End of error message ***<br>
[girasole:27508] *** Process received signal ***<br>
[girasole:27508] Signal: Segmentation fault (11)<br>
[girasole:27508] Signal code:  (128)<br>
[girasole:27508] Failing at address: (nil)<br>
[girasole:27508] [ 0] /lib64/libpthread.so.0 [0x32c780de80]<br>
[girasole:27508] [ 1] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/openmpi/mca_coll_tuned.so [0x2b89b09a1eb5]<br>
[girasole:27508] [ 2] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/openmpi/mca_coll_tuned.so [0x2b89b09a08ca]<br>
[girasole:27508] [ 3] /home/daniels/src/MISC/openmpi-1.4/openmpi-1.4_install/lib/libmpi.so.0(MPI_Alltoall+0x15f) [0x2b89ac711bff]<br>
[girasole:27508] [ 4] ./bug_openmpi_1.4_test(main+0x97) [0x4009b7]<br>
[girasole:27508] [ 5] /lib64/libc.so.6(__libc_start_main+0xf4) [0x32c6c1d8b4]<br>
[girasole:27508] [ 6] ./bug_openmpi_1.4_test [0x400869]<br>
[girasole:27508] *** End of error message ***<br>
<br>
<br>
Best regards,<br><font color="#888888">
<br>
-- <br>
Daniel Spångberg<br>
Materialkemi<br>
Uppsala Universitet<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</font></blockquote></div><br></div>

