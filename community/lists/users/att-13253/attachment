<html><head><style type="text/css"><!-- DIV {margin:0px;} --></style></head><body><div style="font-family:times new roman,new york,times,serif;font-size:12pt"><div>I saw <br></div><div style="font-family: times new roman,new york,times,serif; font-size: 12pt;"> Host: &lt;somename&gt; pid: &lt;somepid&gt; nodeid: 0 nnodes: 1<br><br>really it`s running in 1 node<br>and All of you really undestood my problem, thanks<br><br>But how can I fix it.<br>How can I run 1 job in 4 nodes...?<br>I really need help, <br>I took a look in my files and erase all the errors and the implementations seem corect.<br>From the beginning, please.<br>`case all tutorials only explain the same thing that look right.<br>And thanks very much for this help!<br><br><br>&nbsp;<br><div style="font-family: arial,helvetica,sans-serif; font-size: 13px;"><font face="Tahoma" size="2"><hr size="1"><b><span style="font-weight: bold;">De:</span></b> Jeff Squyres
 &lt;jsquyres@cisco.com&gt;<br><b><span style="font-weight: bold;">Para:</span></b> Open MPI Users &lt;users@open-mpi.org&gt;<br><b><span style="font-weight: bold;">Enviadas:</span></b> Terça-feira, 8 de Junho de 2010 10:30:03<br><b><span style="font-weight: bold;">Assunto:</span></b> Re: [OMPI users] Res:  Gromacs run in parallel<br></font><br>No, I'm sorry -- I wasn't clear.&nbsp; What I meant was, that if you run:<br><br>&nbsp;  mpirun -np 4 my_mpi_application<br><br>1. If you see a single, 4-process MPI job (regardless of how many nodes/servers it's spread across), then all is good.&nbsp; This is what you want.<br><br>2. But if you're seeing 4 independent 1-process MPI jobs (again, regardless of how many nodes/servers they are spread across), it's possible that you compiled your application with MPI implementation X and then used the "mpirun" from MPI implementation Y.&nbsp; <br><br>You will need X==Y to make it work properly -- i.e., to see case
 #1, above.&nbsp; I mention this because your first post mentioned that you're seeing the same job run 4 times.&nbsp; This implied to me that you are running into case #2.&nbsp; If I misunderstood your problem, then ignore me and forgive the noise.<br><br><br><br>On Jun 8, 2010, at 9:20 AM, Carsten Kutzner wrote:<br><br>&gt; On Jun 8, 2010, at 3:06 PM, Jeff Squyres wrote:<br>&gt; <br>&gt; &gt; I know nothing about Gromacs, but you might want to ensure that your Gromacs was compiled with Open MPI.&nbsp; A common symptom of "mpirun -np 4 my_mpi_application" running 4 1-process MPI jobs (instead of 1 4-process MPI job) is that you compiled my_mpi_application with one MPI implementation, but then used the mpirun from a different MPI implementation.<br>&gt; &gt;<br>&gt; Hi,<br>&gt; <br>&gt; this can be checked by looking at the Gromacs output file md.log. The second line should<br>&gt; read something like<br>&gt; <br>&gt; Host: &lt;somename&gt; pid:
 &lt;somepid&gt; nodeid: 0 nnodes: 4<br>&gt; <br>&gt; Lauren, you will want to ensure that nnodes is 4 in your case, and not 1.<br>&gt; <br>&gt; You can also easily test that without any input file by typing<br>&gt; <br>&gt; mpirun -np 4 mdrun -h<br>&gt; <br>&gt; and then should see<br>&gt; <br>&gt; NNODES=4, MYRANK=1, HOSTNAME=&lt;...&gt;<br>&gt; NNODES=4, MYRANK=2, HOSTNAME=&lt;...&gt;<br>&gt; NNODES=4, MYRANK=3, HOSTNAME=&lt;...&gt;<br>&gt; NNODES=4, MYRANK=4, HOSTNAME=&lt;...&gt;<br>&gt; ...<br>&gt; <br>&gt; <br>&gt; Carsten<br>&gt; <br>&gt; <br>&gt; &gt;<br>&gt; &gt; On Jun 8, 2010, at 8:59 AM, lauren wrote:<br>&gt; &gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; The version of Gromacs is 4.0.7.<br>&gt; &gt;&gt; This is the first time that I using Gromacs, then excuse me if I'm nonsense.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Wich part of md.log output&nbsp; should I post?<br>&gt; &gt;&gt; after or before the input description?<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;
 thanks for all,<br>&gt; &gt;&gt; and sorry<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; De: Carsten Kutzner &lt;<a ymailto="mailto:ckutzne@gwdg.de" href="mailto:ckutzne@gwdg.de">ckutzne@gwdg.de</a>&gt;<br>&gt; &gt;&gt; Para: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>&gt; &gt;&gt; Enviadas: Domingo, 6 de Junho de 2010 9:51:26<br>&gt; &gt;&gt; Assunto: Re: [OMPI users] Gromacs run in parallel<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Hi,<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; which version of Gromacs is this? Could you post the first lines of<br>&gt; &gt;&gt; the md.log output file?<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Carsten<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; On Jun 5, 2010, at 10:23 PM, lauren wrote:<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;&gt; sorry my english..<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; I want to know how can I run&nbsp; Gromancs in parallel!<br>&gt; &gt;&gt;&gt; Because
 when I used <br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; mdrun &amp;<br>&gt; &gt;&gt;&gt; mpiexec -np 4 mdrun_mpi -v -deffnm em<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; to run the minimization in 4 cores &gt; all cores make the same job, again!<br>&gt; &gt;&gt;&gt; They don't run together. <br>&gt; &gt;&gt;&gt; I want all in parallel make the job faster.<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; what could be wrong?<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; thank's a lot!<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; _______________________________________________<br>&gt; &gt;&gt;&gt; users mailing list<br>&gt; &gt;&gt;&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; &gt;&gt;<br>&gt;
 &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; _______________________________________________<br>&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt; --<br>&gt; &gt; Jeff Squyres<br>&gt; &gt; <a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>&gt; &gt; For corporate legal information go to:<br>&gt; &gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt; _______________________________________________<br>&gt; &gt; users mailing list<br>&gt; &gt; <a ymailto="mailto:users@open-mpi.org"
 href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; <br>&gt; <br>&gt; --<br>&gt; Dr. Carsten Kutzner<br>&gt; Max Planck Institute for Biophysical Chemistry<br>&gt; Theoretical and Computational Biophysics<br>&gt; Am Fassberg 11, 37077 Goettingen, Germany<br>&gt; Tel. +49-551-2012313, Fax: +49-551-2012302<br>&gt; <a href="http://www.mpibpc.mpg.de/home/grubmueller/ihp/ckutzne" target="_blank">http://www.mpibpc.mpg.de/home/grubmueller/ihp/ckutzne</a><br>&gt; <br>&gt; <br>&gt; <br>&gt; <br>&gt; <br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
 target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; <br><br><br>-- <br>Jeff Squyres<br><a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>For corporate legal information go to:<br><a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br><br><br>_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div>
</div><br>



      &nbsp;</body></html>
