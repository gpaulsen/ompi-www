<div dir="ltr"><div><div>Unfortunately, each execution of mpirun has no knowledge of where the procs have been placed and bound by another execution of mpirun. So what is happening is that the procs of the two jobs are being bound to the same cores, thus causing contention.<br>
<br></div>If you truly want to run two jobs at the same time on the same nodes, then you should add &quot;--bind-to none&quot; on the cmd line. Each job will see a performance impact relative to running bound on their own, but the jobs will run much better if they are sharing nodes.<br>
<br></div>Ralph<br><br></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Thu, Apr 17, 2014 at 10:37 AM, Alfonso Sanchez <span dir="ltr">&lt;<a href="mailto:alfonso.sanchez@tyndall.ie" target="_blank">alfonso.sanchez@tyndall.ie</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi all,<br>
<br>
I&#39;ve compiled OMPI 1.8 on a x64 linux cluster using the PGI compilers v14.1 (I&#39;ve tried it with PGI v11.10 and get the same result). I&#39;m able to compile with the resulting mpicc/mpifort/etc. When running the codes, everything seems to be working fine when there&#39;s only one job running on a given computing node. However, whenever a second job gets assigned the same computing node, the CPU load of every process gets divided by 2. I&#39;m using pbs torque. As an example:<br>

<br>
-Submit jobA using torque to node1 using mpirun -n 4<br>
<br>
-All 4 rocesses of jobA show 100% CPU load.<br>
<br>
-Submit jobB using torque to node1 using mpirun -n 4<br>
<br>
-All 8 processes ( 4 from jobA &amp; 4 from jobB ) show 50% CPU load.<br>
<br>
Moreover, whilst jobA/jobB would run in 30 mins by itself; when both jobs are on the same node they&#39;ve gone 14 hrs without completing.<br>
<br>
I&#39;m attaching config.log &amp; the output of ompi_info --all (bzipped)<br>
<br>
Some more info:<br>
<br>
$&gt; ompi_info | grep tm<br>
<br>
MCA ess: tm (MCA v2.0, API v3.0, Component v1.8)<br>
MCA plm: tm (MCA v2.0, API v2.0, Component v1.8)<br>
MCA ras: tm (MCA v2.0, API v2.0, Component v1.8)<br>
<br>
Sorry if this is a common problem but I&#39;ve tried searching for posts discussing similar problems but haven&#39;t been able to find any.<br>
<br>
Thanks for your help,<br>
Alfonso<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br></div>

