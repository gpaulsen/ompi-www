<div dir="ltr">&gt;There is a whole chapter in the MPI standard about file I/O operations.
I&#39;m quite confident you will find whatever you&#39;re looking for there :)<br><br>Hi George, i know this chapter :) But i&#39;m using MPI-1, not MPI-2. I would like to know methods for I/O with MPI-1.<br><br><div class="gmail_quote">
2008/7/23 George Bosilca &lt;<a href="mailto:bosilca@eecs.utk.edu">bosilca@eecs.utk.edu</a>&gt;:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
There is a whole chapter in the MPI standard about file I/O operations. I&#39;m quite confident you will find whatever you&#39;re looking for there :)<br>
<br>
Open MPI use ROMIO for file operations, and normally this is compiled in by default. You should not have any troubles using MPI I/O with Open MPI.<br>
<br>
 &nbsp;Have fun,<br>
 &nbsp; &nbsp;george.<div><div></div><div class="Wj3C7c"><br>
<br>
On Jul 23, 2008, at 11:51 AM, Gabriele Fatigati wrote:<br>
<br>
</div></div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div><div></div><div class="Wj3C7c">
<br>
Hi,<br>
i have a question about parallel i/o. In my application, actually i have implemented a file lock with C system calls, like flock. But, is this the right way to do concurrent write?<br>
<br>
In this cluster, every node has our operating system, so, the file lock functions only on the processors of that node, not for all. I can have two or more process of different nodes that writes concurrent in the file. Is this dangerous or not? It&#39;s depends by file system? I&#39;m using MPI-1 under OpenMPI.<br>

<br>
Thanks.<br>
-- <br>
Gabriele Fatigati<br>
<br>
CINECA Systems &amp; Tecnologies Department<br>
<br>
Supercomputing Group<br>
<br>
Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>
<br>
<a href="http://www.cineca.it" target="_blank">www.cineca.it</a> Tel: +39 051 6171722<br>
<br>
<a href="mailto:g.fatigati@cineca.it" target="_blank">g.fatigati@cineca.it</a><br></div></div>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote>
<br>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br><br clear="all"><br>-- <br>Gabriele Fatigati<br><br>CINECA Systems &amp; Tecnologies Department<br>
<br>Supercomputing Group<br><br>Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br><br><a href="http://www.cineca.it">www.cineca.it</a> Tel: +39 051 6171722<br><br><a href="mailto:g.fatigati@cineca.it">g.fatigati@cineca.it</a> 
</div>

