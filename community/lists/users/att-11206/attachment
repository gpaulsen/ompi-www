<HTML><HEAD>
<META content="text/html; charset=utf-8" http-equiv=Content-Type>
<META name=GENERATOR content="MSHTML 8.00.6001.18812"></HEAD>
<BODY style="MARGIN: 4px 4px 1px; FONT: 10pt Tahoma">
<DIV>Hi,</DIV>
<DIV>&nbsp;</DIV>
<DIV>i want to build a cluster with openmpi.</DIV>
<DIV>&nbsp;</DIV>
<DIV>2 nodes:</DIV>
<DIV>node 1: 4 x Amd Quad Core, ubuntu 9.04, openmpi 1.3.2</DIV>
<DIV>node 2: Sony PS3, ubuntu 9.04, openmpi 1.3</DIV>
<DIV>&nbsp;</DIV>
<DIV>both can connect with ssh to each other and to itself without passwd.</DIV>
<DIV>&nbsp;</DIV>
<DIV>I can run the sample proramm pi.c on both nodes seperatly (see below). But if i try to start it on node1 with --hostfile option to use node 2 "remote" i got this error:</DIV>
<DIV>&nbsp;</DIV>
<DIV><A href="mailto:cluster@bioclust:~$">cluster@bioclust:~$</A> mpirun --hostfile /etc/openmpi/openmpi-default-hostfile -np 17 /mnt/projects/PS3Cluster/Benchmark/pi<BR>--------------------------------------------------------------------------<BR>mpirun noticed that the job aborted, but has no info as to the process<BR>that caused that situation.<BR>--------------------------------------------------------------------------<BR></DIV>
<DIV>my hostfile:</DIV>
<DIV><A href="mailto:cluster@bioclust:~$">cluster@bioclust:~$</A> cat /etc/openmpi/openmpi-default-hostfile</DIV>
<DIV>10.4.23.107 slots=16<BR>10.4.1.23 slots=2<BR></DIV>
<DIV>i can see with top that the processors of node2 begin to work shortly, then it apports on node1.</DIV>
<DIV>&nbsp;</DIV>
<DIV>I use this sample/test program:</DIV>
<DIV>#include &lt;stdio.h&gt;<BR>#include &lt;stdlib.h&gt;</DIV>
<DIV>#include "mpi.h"</DIV>
<DIV>int main(int argc, char *argv[])<BR>{<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int&nbsp;&nbsp;&nbsp; i, n;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; double h, pi, x;</DIV>
<DIV>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int&nbsp;&nbsp;&nbsp; me, nprocs;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; double piece;</DIV>
<DIV>/* --------------------------------------------------- */</DIV>
<DIV>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MPI_Init (&amp;argc, &amp;argv);</DIV>
<DIV>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MPI_Comm_size (MPI_COMM_WORLD, &amp;nprocs);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MPI_Comm_rank (MPI_COMM_WORLD, &amp;me);</DIV>
<DIV>/* --------------------------------------------------- */</DIV>
<DIV>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (me == 0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; printf("%s", "Input number of intervals:\n");<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; scanf ("%d", &amp;n);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</DIV>
<DIV>/* --------------------------------------------------- */</DIV>
<DIV>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MPI_Bcast (&amp;n, 1, MPI_INT, 0, MPI_COMM_WORLD);</DIV>
<DIV>/* --------------------------------------------------- */</DIV>
<DIV>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h&nbsp;&nbsp;&nbsp;&nbsp; = 1. / (double) n;</DIV>
<DIV>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; piece = 0.;</DIV>
<DIV>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for (i=me+1; i &lt;= n; i+=nprocs)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x&nbsp;&nbsp;&nbsp;&nbsp; = (i-1)*h;</DIV>
<DIV>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; piece = piece + ( 4/(1+(x)*(x)) + 4/(1+(x+h)*(x+h))) / 2 * h;<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</DIV>
<DIV>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; printf("%d: pi = %25.15f\n", me, piece);</DIV>
<DIV>/* --------------------------------------------------- */</DIV>
<DIV>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MPI_Reduce (&amp;piece, &amp;pi, 1, MPI_DOUBLE,<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MPI_SUM, 0, MPI_COMM_WORLD);</DIV>
<DIV>/* --------------------------------------------------- */</DIV>
<DIV>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (me == 0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; printf("pi = %25.15f\n", pi);<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</DIV>
<DIV>/* --------------------------------------------------- */</DIV>
<DIV>&nbsp;&nbsp;&nbsp;&nbsp; MPI_Finalize();</DIV>
<DIV>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return 0;<BR>}<BR></DIV>
<DIV>it works on each node.</DIV>
<DIV>node1:</DIV>
<DIV><A href="mailto:cluster@bioclust:~$">cluster@bioclust:~$</A> mpirun -np 4 /mnt/projects/PS3Cluster/Benchmark/piInput number of intervals:<BR>20<BR>0: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.822248040052981<BR>2: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.773339953424083<BR>3: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.747089984650041<BR>1: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.798498008827023<BR>pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.141175986954128</DIV>
<DIV>&nbsp;</DIV>
<DIV>node2:</DIV>
<DIV><A href="mailto:cluster@kasimir:~$">cluster@kasimir:~$</A> mpirun -np 2 /mnt/projects/PS3Cluster/Benchmark/pi<BR>Input number of intervals:<BR>5<BR>1: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.267463056905495<BR>0: pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.867463056905495<BR>pi =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.134926113810990<BR><A href="mailto:cluster@kasimir:~$">cluster@kasimir:~$</A> </DIV>
<DIV>&nbsp;</DIV>
<DIV>Thx in advance,</DIV>
<DIV>Laurin<BR></DIV>
<DIV><BR>&nbsp;</DIV>
<DIV>&nbsp;</DIV></BODY></HTML>

