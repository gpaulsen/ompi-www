<div dir="ltr">Dear Gilles, Dear all,<div>I have found the error. Some CPU has no element to share. It was a my error.</div><div><br></div><div>Now I have another one:</div><div><br></div><div><div><i>Fatal error in MPI_Isend: Invalid communicator, error stack:</i></div><div><i>MPI_Isend(158): MPI_Isend(buf=0x137b7b4, count=1, INVALID DATATYPE, dest=0, tag=0, comm=0x0, request=0x7fffe8726fc0) failed</i></div></div><div><i><br></i></div><div>In this case with MPI does not work, with openMPI it works.</div><div><br></div><div>Could you see some particular information from the error message?</div><div><br></div><div>Diego</div><div><br></div></div><div class="gmail_extra"><br clear="all"><div><div class="gmail_signature">Diego<br><br></div></div>
<br><div class="gmail_quote">On 2 September 2015 at 14:52, Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles.gouaillardet@gmail.com" target="_blank">gilles.gouaillardet@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Diego,<div><br></div><div>about MPI_Allreduce, you should use MPI_IN_PLACE if you want the same buffer in send and recv</div><div><br></div><div>about the stack, I notice comm is NULL which is a bit surprising...</div><div>at first glance, type creation looks good.</div><div>that being said, you do not check MPIdata%iErr is MPI_SUCCESS after each MPI call.</div><div>I recommend you first do this, so you can catch the error as soon it happens, and hopefully understand why it occurs．</div><div><br></div>Cheers,<div><br></div><div>Gilles<div><div class="h5"><br><div><br>On Wednesday, September 2, 2015, Diego Avesani &lt;<a href="mailto:diego.avesani@gmail.com" target="_blank">diego.avesani@gmail.com</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Dear all,<div><br></div><div>I have notice small difference between OPEN-MPI and intel MPI. </div><div>For example in MPI_ALLREDUCE in intel MPI is not allowed to use the same variable in send and receiving Buff.</div><div><br></div><div>I have written my code in OPEN-MPI, but unfortunately I have to run in on a intel-MPI cluster. </div><div>Now I have the following error:</div><div><br></div><div><div><i>atal error in MPI_Isend: Invalid communicator, error stack:</i></div><div><i>MPI_Isend(158): MPI_Isend(buf=0x1dd27b0, count=1, INVALID DATATYPE, dest=0, tag=0, comm=0x0, request=0x7fff9d7dd9f0) failed</i></div></div><div><br></div><div><br></div><div>This is ho I create my type:</div><div><br></div><div><div><i>  CALL  MPI_TYPE_VECTOR(1, Ncoeff_MLS, Ncoeff_MLS, MPI_DOUBLE_PRECISION, coltype, MPIdata%iErr) </i></div><div><i>  CALL  MPI_TYPE_COMMIT(coltype, MPIdata%iErr)</i></div><div><i>  !</i></div><div><i>  CALL  MPI_TYPE_VECTOR(1, nVar, nVar, coltype, MPI_WENO_TYPE, MPIdata%iErr) </i></div><div><i>  CALL  MPI_TYPE_COMMIT(MPI_WENO_TYPE, MPIdata%iErr)</i></div></div><div><br></div><div><br></div><div>do you believe that is here the problem?</div><div>Is also this the way how intel MPI create a datatype?</div><div><br></div><div>maybe I could also ask to intel MPI users</div><div>What do you think?</div><div><br clear="all"><div><div>Diego<br><br></div></div>
</div></div>
</blockquote></div></div></div></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/09/27523.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/09/27523.php</a><br></blockquote></div><br></div>

