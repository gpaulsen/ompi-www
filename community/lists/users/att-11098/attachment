Thanks Dear,<br><br>Jonathan seems almost perfect; I percieve the same.<br><br><div class="gmail_quote">On Fri, Nov 6, 2009 at 6:17 PM, Tom Rosmond <span dir="ltr">&lt;<a href="mailto:rosmond@reachone.com">rosmond@reachone.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">AMJAD<br>
<br>
On your first question, the answer is probably, if everything else is<br>
done correctly.  The first test is to not try to do the overlapping<br>
communication and computation, but do them sequentially and make sure<br>
the answers are correct. Have you done this test?  Debugging your<br>
original approach will be challenging, and having a control solution<br>
will be a big help.<br></blockquote><div><br>I followed the path of sequentional---then--parallel blocking----and then parallel non-blocking.<br>
My serial solution is the control solution.<br> 
<br></div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<br>
On your second question, if I  understand it correctly, is that it is<br>
always better to minimize the number of messages.  In problems like this<br>
communication costs are dominated by latency, so bundling the data into<br>
the fewest possible messages will ALWAYS be better.<br></blockquote><div><br>Thats good. <br>But what pointed out by Jonathan: <br><br> If you really do hide most of the communications cost with your non-blocking communications, then it may not matter too much.<br>
<br>is the point I want to be sure about.<br><br></div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<br>
T. Rosmond<br>
<div><div></div><div class="h5"><br>
<br>
<br>
On Fri, 2009-11-06 at 17:44 -0500, amjad ali wrote:<br>
&gt; Hi all,<br>
&gt;<br>
&gt; I need/request some help from those who have some experience in<br>
&gt; debugging/profiling/tuning parallel scientific codes, specially for<br>
&gt; PDEs/CFD.<br>
&gt;<br>
&gt; I have parallelized a Fortran CFD code to run on<br>
&gt; Ethernet-based-Linux-Cluster. Regarding MPI communication what I do is<br>
&gt; that:<br>
&gt;<br>
&gt; Suppose that the grid/mesh is decomposed for n number of processors,<br>
&gt; such that each processors has a number of elements that share their<br>
&gt; side/face with different processors. What I do is that I start non<br>
&gt; blocking MPI communication at the partition boundary faces (faces<br>
&gt; shared between any two processors) , and then start computing values<br>
&gt; on the internal/non-shared faces. When I complete this computation, I<br>
&gt; put WAITALL to ensure MPI communication completion. Then I do<br>
&gt; computation on the partition boundary faces (shared-ones). This way I<br>
&gt; try to hide the communication behind computation. Is it correct?<br>
&gt;<br>
&gt; IMPORTANT: Secondly, if processor A shares 50 faces (on 50 or less<br>
&gt; elements) with an another processor B then it sends/recvs 50 different<br>
&gt; messages. So in general if a processors has X number of faces sharing<br>
&gt; with any number of other processors it sends/recvs that much messages.<br>
&gt; Is this way has &quot;very much reduced&quot; performance in comparison to the<br>
&gt; possibility that processor A will send/recv a single-bundle message<br>
&gt; (containg all 50-faces-data) to process B. Means that in general a<br>
&gt; processor will only send/recv that much messages as the number of<br>
&gt; processors neighbour to it.  It will send a single bundle/pack of<br>
&gt; messages to each neighbouring processor.<br>
&gt; Is their &quot;quite a much difference&quot; between these two approaches?<br>
&gt;<br>
&gt; THANK YOU VERY MUCH.<br>
&gt; AMJAD.<br>
&gt;<br>
&gt;<br>
</div></div><div><div></div><div class="h5">&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

