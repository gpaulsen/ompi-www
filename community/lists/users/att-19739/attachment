<html xmlns:v="urn:schemas-microsoft-com:vml" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:w="urn:schemas-microsoft-com:office:word" xmlns:m="http://schemas.microsoft.com/office/2004/12/omml" xmlns="http://www.w3.org/TR/REC-html40">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<meta name="Generator" content="Microsoft Word 14 (filtered medium)">
<style><!--
/* Font Definitions */
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:Tahoma;
	panose-1:2 11 6 4 3 5 4 4 2 4;}
/* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin:0cm;
	margin-bottom:.0001pt;
	font-size:12.0pt;
	font-family:"Times New Roman","serif";}
a:link, span.MsoHyperlink
	{mso-style-priority:99;
	color:blue;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{mso-style-priority:99;
	color:purple;
	text-decoration:underline;}
span.EmailStyle17
	{mso-style-type:personal-reply;
	font-family:"Calibri","sans-serif";
	color:#1F497D;}
.MsoChpDefault
	{mso-style-type:export-only;
	font-family:"Calibri","sans-serif";
	mso-fareast-language:EN-US;}
@page WordSection1
	{size:612.0pt 792.0pt;
	margin:72.0pt 72.0pt 72.0pt 72.0pt;}
div.WordSection1
	{page:WordSection1;}
--></style><!--[if gte mso 9]><xml>
<o:shapedefaults v:ext="edit" spidmax="1026" />
</xml><![endif]--><!--[if gte mso 9]><xml>
<o:shapelayout v:ext="edit">
<o:idmap v:ext="edit" data="1" />
</o:shapelayout></xml><![endif]-->
</head>
<body lang="EN-GB" link="blue" vlink="purple">
<div class="WordSection1">
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D">Hi<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D"><o:p>&nbsp;</o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D">Do you have any details about the performance of mxm, e.g. for real applications?<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D"><o:p>&nbsp;</o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D">Thanks<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D"><o:p>&nbsp;</o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D">Henk<o:p></o:p></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1F497D"><o:p>&nbsp;</o:p></span></p>
<div style="border:none;border-left:solid blue 1.5pt;padding:0cm 0cm 0cm 4.0pt">
<div>
<div style="border:none;border-top:solid #B5C4DF 1.0pt;padding:3.0pt 0cm 0cm 0cm">
<p class="MsoNormal"><b><span lang="EN-US" style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;">From:</span></b><span lang="EN-US" style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;"> users-bounces@open-mpi.org [mailto:users-bounces@open-mpi.org]
<b>On Behalf Of </b>Mike Dubman<br>
<b>Sent:</b> 11 May 2012 19:23<br>
<b>To:</b> Open MPI Users<br>
<b>Subject:</b> Re: [OMPI users] ompi mca mxm version<o:p></o:p></span></p>
</div>
</div>
<p class="MsoNormal"><o:p>&nbsp;</o:p></p>
<div>
<div>
<p class="MsoNormal">ob1/openib is RC based which have scalability issues, mxm 1.1 is ud based and kicks in at scale.<o:p></o:p></p>
</div>
<div>
<p class="MsoNormal">We observe mxm outperforms ob1 on 8&#43; nodes.<o:p></o:p></p>
</div>
<div>
<p class="MsoNormal">&nbsp;<o:p></o:p></p>
</div>
<div>
<p class="MsoNormal">We will update docs as you mentioned, thanks<o:p></o:p></p>
</div>
<div>
<p class="MsoNormal">&nbsp;<o:p></o:p></p>
</div>
<div>
<p class="MsoNormal">Regards<o:p></o:p></p>
</div>
<div>
<p class="MsoNormal">&nbsp;<o:p></o:p></p>
</div>
<div>
<p class="MsoNormal"><br>
<br>
&nbsp;<o:p></o:p></p>
</div>
<div>
<p class="MsoNormal">On Thu, May 10, 2012 at 4:30 PM, Derek Gerstmann &lt;<a href="mailto:derek.gerstmann@uwa.edu.au" target="_blank">derek.gerstmann@uwa.edu.au</a>&gt; wrote:<o:p></o:p></p>
<div>
<p class="MsoNormal" style="margin-bottom:12.0pt">On May 9, 2012, at 7:41 PM, Mike Dubman wrote:<br>
<br>
&gt; you need latest OMPI 1.6.x and latest MXM (<a href="ftp://bgate.mellanox.com/hpc/mxm/v1.1/mxm_1.1.1067.tar" target="_blank">ftp://bgate.mellanox.com/hpc/mxm/v1.1/mxm_1.1.1067.tar</a>)<o:p></o:p></p>
</div>
<p class="MsoNormal">Excellent! &nbsp;Thanks for the quick response! &nbsp;Using the MXM v1.1.1067 against OMPI v1.6.x did the trick. &nbsp;Please (!!!) add a note to the docs for OMPI 1.6.x to help out other users -- there's zero mention of this anywhere that I could find
 from scouring the archives and source code.<br>
<br>
Sadly, performance isn't what we'd expect. &nbsp;OB1 is outperforming CM MXM (consistently).<br>
<br>
Are there any suggested configuration settings? &nbsp;We tried all the obvious ones listed in the OMPI Wiki and mailing list archives, but few have had much of an effect.<br>
<br>
We seem to do better with the OB1 openib btl, than the lower level CM MXM. &nbsp;Any suggestions?<br>
<br>
Here's numbers from the OSU MicroBenchmarks (for the MBW_MR test) running on 2x pairs, aka 4 separate hosts, each using Mellanox ConnectX, one card per host, single port, single switch):<br>
<br>
-- OB1<br>
&gt; /opt/openmpi/1.6.0/bin/mpiexec -np 4 --mca pml ob1 --mca btl ^tcp --mca mpi_use_pinned 1 -hostfile all_hosts ./osu-micro-benchmarks/osu_mbw_mr<br>
# OSU MPI Multiple Bandwidth / Message Rate Test v3.6<br>
# [ pairs: 2 ] [ window size: 64 ]<br>
# Size &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MB/s &nbsp; &nbsp; &nbsp; &nbsp;Messages/s<br>
1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2.91 &nbsp; &nbsp; &nbsp; &nbsp;2909711.73<br>
2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5.97 &nbsp; &nbsp; &nbsp; &nbsp;2984274.11<br>
4 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;11.70 &nbsp; &nbsp; &nbsp; &nbsp;2924292.78<br>
8 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;23.00 &nbsp; &nbsp; &nbsp; &nbsp;2874502.93<br>
16 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 44.75 &nbsp; &nbsp; &nbsp; &nbsp;2796639.64<br>
32 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 89.49 &nbsp; &nbsp; &nbsp; &nbsp;2796639.64<br>
64 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;175.98 &nbsp; &nbsp; &nbsp; &nbsp;2749658.96<br>
128 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 292.41 &nbsp; &nbsp; &nbsp; &nbsp;2284459.86<br>
256 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 527.84 &nbsp; &nbsp; &nbsp; &nbsp;2061874.61<br>
512 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 961.65 &nbsp; &nbsp; &nbsp; &nbsp;1878221.77<br>
1024 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1669.06 &nbsp; &nbsp; &nbsp; &nbsp;1629943.87<br>
2048 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2220.43 &nbsp; &nbsp; &nbsp; &nbsp;1084193.45<br>
4096 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2906.57 &nbsp; &nbsp; &nbsp; &nbsp; 709611.68<br>
8192 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 3017.65 &nbsp; &nbsp; &nbsp; &nbsp; 368365.70<br>
16384 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5225.97 &nbsp; &nbsp; &nbsp; &nbsp; 318967.95<br>
32768 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5418.98 &nbsp; &nbsp; &nbsp; &nbsp; 165374.23<br>
65536 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5998.07 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;91523.27<br>
131072 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6031.69 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;46018.16<br>
262144 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6063.38 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;23129.97<br>
524288 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5971.77 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;11390.24<br>
1048576 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5788.75 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5520.59<br>
2097152 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5791.39 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2761.55<br>
4194304 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5820.60 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1387.74<br>
<br>
-- MXM<br>
&gt; /opt/openmpi/1.6.0/bin/mpiexec -np 4 --mca pml cm --mca mtl mxm --mca btl ^tcp --mca mpi_use_pinned 1 -hostfile all_hosts ./osu-micro-benchmarks/osu_mbw_mr<br>
# OSU MPI Multiple Bandwidth / Message Rate Test v3.6<br>
# [ pairs: 2 ] [ window size: 64 ]<br>
# Size &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MB/s &nbsp; &nbsp; &nbsp; &nbsp;Messages/s<br>
1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2.07 &nbsp; &nbsp; &nbsp; &nbsp;2074863.43<br>
2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 4.14 &nbsp; &nbsp; &nbsp; &nbsp;2067830.81<br>
4 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;10.57 &nbsp; &nbsp; &nbsp; &nbsp;2642471.39<br>
8 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;23.16 &nbsp; &nbsp; &nbsp; &nbsp;2895275.37<br>
16 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 38.73 &nbsp; &nbsp; &nbsp; &nbsp;2420627.22<br>
32 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 66.77 &nbsp; &nbsp; &nbsp; &nbsp;2086718.41<br>
64 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;147.87 &nbsp; &nbsp; &nbsp; &nbsp;2310414.05<br>
128 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 284.94 &nbsp; &nbsp; &nbsp; &nbsp;2226109.85<br>
256 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 537.27 &nbsp; &nbsp; &nbsp; &nbsp;2098709.64<br>
512 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1041.91 &nbsp; &nbsp; &nbsp; &nbsp;2034989.43<br>
1024 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1930.93 &nbsp; &nbsp; &nbsp; &nbsp;1885676.34<br>
2048 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1998.68 &nbsp; &nbsp; &nbsp; &nbsp; 975916.00<br>
4096 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2880.72 &nbsp; &nbsp; &nbsp; &nbsp; 703299.77<br>
8192 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 3608.45 &nbsp; &nbsp; &nbsp; &nbsp; 440484.17<br>
16384 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4027.15 &nbsp; &nbsp; &nbsp; &nbsp; 245797.51<br>
32768 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4464.85 &nbsp; &nbsp; &nbsp; &nbsp; 136256.47<br>
65536 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4594.22 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;70102.23<br>
131072 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 4655.62 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;35519.55<br>
262144 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 4671.56 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;17820.58<br>
524288 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 4604.16 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 8781.74<br>
1048576 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4635.51 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 4420.77<br>
2097152 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3575.17 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1704.78<br>
4194304 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2828.19 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;674.29<o:p></o:p></p>
<div>
<p class="MsoNormal" style="margin-bottom:12.0pt"><br>
Thanks!<br>
<br>
-[dg]<br>
<br>
Derek Gerstmann, PhD Student<br>
The University of Western Australia (UWA)<br>
<br>
w: <a href="http://local.ivec.uwa.edu.au/~derek" target="_blank">http://local.ivec.uwa.edu.au/~derek</a><br>
e: derek.gerstmann [at] <a href="http://icrar.org" target="_blank">icrar.org</a><o:p></o:p></p>
</div>
<div>
<div>
<p class="MsoNormal">On May 9, 2012, at 7:41 PM, Mike Dubman wrote:<br>
<br>
&gt; you need latest OMPI 1.6.x and latest MXM (<a href="ftp://bgate.mellanox.com/hpc/mxm/v1.1/mxm_1.1.1067.tar" target="_blank">ftp://bgate.mellanox.com/hpc/mxm/v1.1/mxm_1.1.1067.tar</a>)<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; On Wed, May 9, 2012 at 6:02 AM, Derek Gerstmann &lt;<a href="mailto:derek.gerstmann@uwa.edu.au">derek.gerstmann@uwa.edu.au</a>&gt; wrote:<br>
&gt; What versions of OpenMPI and the Mellanox MXM libraries have been tested and verified to work?<br>
&gt;<br>
&gt; We are currently trying to build OpenMPI v1.5.5 against the MXM 1.0.601 (included in the MLNX_OFED_LINUX-1.5.3-3.0.0 distribution) and are getting build errors.<br>
&gt;<br>
&gt; Specifically, there's a single undefined type (mxm_wait_t) being used in the OpenMPI tree:<br>
&gt;<br>
&gt; &nbsp; &nbsp; &nbsp; openmpi-1.5.5/ompi/mca/mtl/mxm/mtl_mxm_send.c:44 &nbsp; &nbsp; &nbsp; &nbsp;mxm_wait_t wait;<br>
&gt;<br>
&gt; There is no mxm_wait_t defined anywhere in the current MXM API (/opt/mellanox/mxm/include/mxm/api), which suggests a version mismatch.<br>
&gt;<br>
&gt; The OpenMPI v1.6 branch has a note in the readme saying &quot;Minor Fixes for Mellanox MXM&quot; were added, but the same undefined mxm_wait_t is still being used.<br>
&gt;<br>
&gt; What versions of OpenMPI and MXM are verified to work?<br>
&gt;<br>
&gt; Thanks!<br>
&gt;<br>
&gt; -[dg]<br>
&gt;<br>
&gt; Derek Gerstmann, PhD Student<br>
&gt; The University of Western Australia (UWA)<br>
&gt;<br>
&gt; w: <a href="http://local.ivec.uwa.edu.au/~derek" target="_blank">http://local.ivec.uwa.edu.au/~derek</a><br>
&gt; e: derek.gerstmann [at] <a href="http://icrar.org" target="_blank">icrar.org</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
-[dg]<br>
<br>
Derek Gerstmann, PhD Student<br>
The University of Western Australia (UWA)<br>
<br>
w: <a href="http://local.ivec.uwa.edu.au/~derek" target="_blank">http://local.ivec.uwa.edu.au/~derek</a><br>
e: derek.gerstmann [at] <a href="http://icrar.org" target="_blank">icrar.org</a><br>
<br>
<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><o:p></o:p></p>
</div>
</div>
</div>
<p class="MsoNormal"><o:p>&nbsp;</o:p></p>
</div>
</div>
</div>
</body>
</html>

