<div dir="ltr"><div><div> I am configuring with all defaults.   Just doing a ./configure and then<br></div>make and make install.   I have used open mpi on several kinds of <br>unix  systems this way and have had no trouble before.   I believe I<br>
</div>last had success on a redhat version of linux.<br></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Sat, May 3, 2014 at 11:00 AM,  <span dir="ltr">&lt;<a href="mailto:users-request@open-mpi.org" target="_blank">users-request@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Send users mailing list submissions to<br>
        <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<br>
To subscribe or unsubscribe via the World Wide Web, visit<br>
        <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
or, via email, send a message with subject or body &#39;help&#39; to<br>
        <a href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a><br>
<br>
You can reach the person managing the list at<br>
        <a href="mailto:users-owner@open-mpi.org">users-owner@open-mpi.org</a><br>
<br>
When replying, please edit your Subject line so it is more specific<br>
than &quot;Re: Contents of users digest...&quot;<br>
<br>
<br>
Today&#39;s Topics:<br>
<br>
   1. MPI_Barrier hangs on second attempt but only when multiple<br>
      hosts used. (Clay Kirkland)<br>
   2. Re: MPI_Barrier hangs on second attempt but only when<br>
      multiple hosts used. (Ralph Castain)<br>
<br>
<br>
----------------------------------------------------------------------<br>
<br>
Message: 1<br>
Date: Fri, 2 May 2014 16:24:04 -0500<br>
From: Clay Kirkland &lt;<a href="mailto:clay.kirkland@versityinc.com">clay.kirkland@versityinc.com</a>&gt;<br>
To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subject: [OMPI users] MPI_Barrier hangs on second attempt but only<br>
        when    multiple hosts used.<br>
Message-ID:<br>
        &lt;CAJDnjA8Wi=FEjz6Vz+Bc34b+nFE=<a href="mailto:TF4B7g0BQgMbeKg7H-pV%2BA@mail.gmail.com">TF4B7g0BQgMbeKg7H-pV+A@mail.gmail.com</a>&gt;<br>
Content-Type: text/plain; charset=&quot;utf-8&quot;<br>
<br>
I have been using MPI for many many years so I have very well debugged mpi<br>
tests.   I am<br>
having trouble on either openmpi-1.4.5  or  openmpi-1.6.5 versions though<br>
with getting the<br>
MPI_Barrier calls to work.   It works fine when I run all processes on one<br>
machine but when<br>
I run with two or more hosts the second call to MPI_Barrier always hangs.<br>
Not the first one,<br>
but always the second one.   I looked at FAQ&#39;s and such but found nothing<br>
except for a comment<br>
that MPI_Barrier problems were often problems with fire walls.  Also<br>
mentioned as a problem<br>
was not having the same version of mpi on both machines.  I turned<br>
firewalls off and removed<br>
and reinstalled the same version on both hosts but I still see the same<br>
thing.   I then installed<br>
lam mpi on two of my machines and that works fine.   I can call the<br>
MPI_Barrier function when run on<br>
one of two machines by itself  many times with no hangs.  Only hangs if two<br>
or more hosts are involved.<br>
These runs are all being done on CentOS release 6.4.   Here is test program<br>
I used.<br>
<br>
main (argc, argv)<br>
int argc;<br>
char **argv;<br>
{<br>
    char message[20];<br>
    char hoster[256];<br>
    char nameis[256];<br>
    int fd, i, j, jnp, iret, myrank, np, ranker, recker;<br>
    MPI_Comm comm;<br>
    MPI_Status status;<br>
<br>
    MPI_Init( &amp;argc, &amp;argv );<br>
    MPI_Comm_rank( MPI_COMM_WORLD, &amp;myrank);<br>
    MPI_Comm_size( MPI_COMM_WORLD, &amp;np);<br>
<br>
        gethostname(hoster,256);<br>
<br>
        printf(&quot; In rank %d and host= %s  Do Barrier call<br>
1.\n&quot;,myrank,hoster);<br>
    MPI_Barrier(MPI_COMM_WORLD);<br>
        printf(&quot; In rank %d and host= %s  Do Barrier call<br>
2.\n&quot;,myrank,hoster);<br>
    MPI_Barrier(MPI_COMM_WORLD);<br>
        printf(&quot; In rank %d and host= %s  Do Barrier call<br>
3.\n&quot;,myrank,hoster);<br>
    MPI_Barrier(MPI_COMM_WORLD);<br>
    MPI_Finalize();<br>
    exit(0);<br>
}<br>
<br>
  Here are three runs of test program.  First with two processes on one<br>
host, then with<br>
two processes on another host, and finally with one process on each of two<br>
hosts.  The<br>
first two runs are fine but the last run hangs on the second MPI_Barrier.<br>
<br>
[root@centos MPI]# /usr/local/bin/mpirun -np 2 --host centos a.out<br>
 In rank 0 and host= centos  Do Barrier call 1.<br>
 In rank 1 and host= centos  Do Barrier call 1.<br>
 In rank 1 and host= centos  Do Barrier call 2.<br>
 In rank 1 and host= centos  Do Barrier call 3.<br>
 In rank 0 and host= centos  Do Barrier call 2.<br>
 In rank 0 and host= centos  Do Barrier call 3.<br>
[root@centos MPI]# /usr/local/bin/mpirun -np 2 --host RAID a.out<br>
/root/.bashrc: line 14: unalias: ls: not found<br>
 In rank 0 and host= RAID  Do Barrier call 1.<br>
 In rank 0 and host= RAID  Do Barrier call 2.<br>
 In rank 0 and host= RAID  Do Barrier call 3.<br>
 In rank 1 and host= RAID  Do Barrier call 1.<br>
 In rank 1 and host= RAID  Do Barrier call 2.<br>
 In rank 1 and host= RAID  Do Barrier call 3.<br>
[root@centos MPI]# /usr/local/bin/mpirun -np 2 --host centos,RAID a.out<br>
/root/.bashrc: line 14: unalias: ls: not found<br>
 In rank 0 and host= centos  Do Barrier call 1.<br>
 In rank 0 and host= centos  Do Barrier call 2.<br>
In rank 1 and host= RAID  Do Barrier call 1.<br>
 In rank 1 and host= RAID  Do Barrier call 2.<br>
<br>
  Since it is such a simple test and problem and such a widely used MPI<br>
function, it must obviously<br>
be an installation or configuration problem.   A pstack for each of the<br>
hung MPI_Barrier processes<br>
on the two machines shows this:<br>
<br>
[root@centos ~]# pstack 31666<br>
#0  0x0000003baf0e8ee3 in __epoll_wait_nocancel () from /lib64/libc.so.6<br>
#1  0x00007f5de06125eb in epoll_dispatch () from /usr/local/lib/libmpi.so.1<br>
#2  0x00007f5de061475a in opal_event_base_loop () from<br>
/usr/local/lib/libmpi.so.1<br>
#3  0x00007f5de0639229 in opal_progress () from /usr/local/lib/libmpi.so.1<br>
#4  0x00007f5de0586f75 in ompi_request_default_wait_all () from<br>
/usr/local/lib/libmpi.so.1<br>
#5  0x00007f5ddc59565e in ompi_coll_tuned_sendrecv_actual () from<br>
/usr/local/lib/openmpi/mca_coll_tuned.so<br>
#6  0x00007f5ddc59d8ff in ompi_coll_tuned_barrier_intra_two_procs () from<br>
/usr/local/lib/openmpi/mca_coll_tuned.so<br>
#7  0x00007f5de05941c2 in PMPI_Barrier () from /usr/local/lib/libmpi.so.1<br>
#8  0x0000000000400a43 in main ()<br>
<br>
[root@RAID openmpi-1.6.5]# pstack 22167<br>
#0  0x00000030302e8ee3 in __epoll_wait_nocancel () from /lib64/libc.so.6<br>
#1  0x00007f7ee46885eb in epoll_dispatch () from /usr/local/lib/libmpi.so.1<br>
#2  0x00007f7ee468a75a in opal_event_base_loop () from<br>
/usr/local/lib/libmpi.so.1<br>
#3  0x00007f7ee46af229 in opal_progress () from /usr/local/lib/libmpi.so.1<br>
#4  0x00007f7ee45fcf75 in ompi_request_default_wait_all () from<br>
/usr/local/lib/libmpi.so.1<br>
#5  0x00007f7ee060b65e in ompi_coll_tuned_sendrecv_actual () from<br>
/usr/local/lib/openmpi/mca_coll_tuned.so<br>
#6  0x00007f7ee06138ff in ompi_coll_tuned_barrier_intra_two_procs () from<br>
/usr/local/lib/openmpi/mca_coll_tuned.so<br>
#7  0x00007f7ee460a1c2 in PMPI_Barrier () from /usr/local/lib/libmpi.so.1<br>
#8  0x0000000000400a43 in main ()<br>
<br>
 Which looks exactly the same on each machine.  Any thoughts or ideas would<br>
be greatly appreciated as<br>
I am stuck.<br>
<br>
 Clay Kirkland<br>
-------------- next part --------------<br>
HTML attachment scrubbed and removed<br>
<br>
------------------------------<br>
<br>
Message: 2<br>
Date: Sat, 3 May 2014 06:39:20 -0700<br>
From: Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>
Subject: Re: [OMPI users] MPI_Barrier hangs on second attempt but only<br>
        when    multiple hosts used.<br>
Message-ID: &lt;<a href="mailto:3CF53D73-15D9-40BB-A2DE-50BA3561A0F4@open-mpi.org">3CF53D73-15D9-40BB-A2DE-50BA3561A0F4@open-mpi.org</a>&gt;<br>
Content-Type: text/plain; charset=&quot;us-ascii&quot;<br>
<br>
Hmmm...just testing on my little cluster here on two nodes, it works just fine with 1.8.2:<br>
<br>
[rhc@bend001 v1.8]$ mpirun -n 2 --map-by node ./a.out<br>
 In rank 0 and host= bend001  Do Barrier call 1.<br>
 In rank 0 and host= bend001  Do Barrier call 2.<br>
 In rank 0 and host= bend001  Do Barrier call 3.<br>
 In rank 1 and host= bend002  Do Barrier call 1.<br>
 In rank 1 and host= bend002  Do Barrier call 2.<br>
 In rank 1 and host= bend002  Do Barrier call 3.<br>
[rhc@bend001 v1.8]$<br>
<br>
<br>
How are you configuring OMPI?<br>
<br>
<br>
On May 2, 2014, at 2:24 PM, Clay Kirkland &lt;<a href="mailto:clay.kirkland@versityinc.com">clay.kirkland@versityinc.com</a>&gt; wrote:<br>
<br>
&gt; I have been using MPI for many many years so I have very well debugged mpi tests.   I am<br>
&gt; having trouble on either openmpi-1.4.5  or  openmpi-1.6.5 versions though with getting the<br>
&gt; MPI_Barrier calls to work.   It works fine when I run all processes on one machine but when<br>
&gt; I run with two or more hosts the second call to MPI_Barrier always hangs.   Not the first one,<br>
&gt; but always the second one.   I looked at FAQ&#39;s and such but found nothing except for a comment<br>
&gt; that MPI_Barrier problems were often problems with fire walls.  Also mentioned as a problem<br>
&gt; was not having the same version of mpi on both machines.  I turned firewalls off and removed<br>
&gt; and reinstalled the same version on both hosts but I still see the same thing.   I then installed<br>
&gt; lam mpi on two of my machines and that works fine.   I can call the MPI_Barrier function when run on<br>
&gt; one of two machines by itself  many times with no hangs.  Only hangs if two or more hosts are involved.<br>
&gt; These runs are all being done on CentOS release 6.4.   Here is test program I used.<br>
&gt;<br>
&gt; main (argc, argv)<br>
&gt; int argc;<br>
&gt; char **argv;<br>
&gt; {<br>
&gt;     char message[20];<br>
&gt;     char hoster[256];<br>
&gt;     char nameis[256];<br>
&gt;     int fd, i, j, jnp, iret, myrank, np, ranker, recker;<br>
&gt;     MPI_Comm comm;<br>
&gt;     MPI_Status status;<br>
&gt;<br>
&gt;     MPI_Init( &amp;argc, &amp;argv );<br>
&gt;     MPI_Comm_rank( MPI_COMM_WORLD, &amp;myrank);<br>
&gt;     MPI_Comm_size( MPI_COMM_WORLD, &amp;np);<br>
&gt;<br>
&gt;         gethostname(hoster,256);<br>
&gt;<br>
&gt;         printf(&quot; In rank %d and host= %s  Do Barrier call 1.\n&quot;,myrank,hoster);<br>
&gt;     MPI_Barrier(MPI_COMM_WORLD);<br>
&gt;         printf(&quot; In rank %d and host= %s  Do Barrier call 2.\n&quot;,myrank,hoster);<br>
&gt;     MPI_Barrier(MPI_COMM_WORLD);<br>
&gt;         printf(&quot; In rank %d and host= %s  Do Barrier call 3.\n&quot;,myrank,hoster);<br>
&gt;     MPI_Barrier(MPI_COMM_WORLD);<br>
&gt;     MPI_Finalize();<br>
&gt;     exit(0);<br>
&gt; }<br>
&gt;<br>
&gt;   Here are three runs of test program.  First with two processes on one host, then with<br>
&gt; two processes on another host, and finally with one process on each of two hosts.  The<br>
&gt; first two runs are fine but the last run hangs on the second MPI_Barrier.<br>
&gt;<br>
&gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host centos a.out<br>
&gt;  In rank 0 and host= centos  Do Barrier call 1.<br>
&gt;  In rank 1 and host= centos  Do Barrier call 1.<br>
&gt;  In rank 1 and host= centos  Do Barrier call 2.<br>
&gt;  In rank 1 and host= centos  Do Barrier call 3.<br>
&gt;  In rank 0 and host= centos  Do Barrier call 2.<br>
&gt;  In rank 0 and host= centos  Do Barrier call 3.<br>
&gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host RAID a.out<br>
&gt; /root/.bashrc: line 14: unalias: ls: not found<br>
&gt;  In rank 0 and host= RAID  Do Barrier call 1.<br>
&gt;  In rank 0 and host= RAID  Do Barrier call 2.<br>
&gt;  In rank 0 and host= RAID  Do Barrier call 3.<br>
&gt;  In rank 1 and host= RAID  Do Barrier call 1.<br>
&gt;  In rank 1 and host= RAID  Do Barrier call 2.<br>
&gt;  In rank 1 and host= RAID  Do Barrier call 3.<br>
&gt; [root@centos MPI]# /usr/local/bin/mpirun -np 2 --host centos,RAID a.out<br>
&gt; /root/.bashrc: line 14: unalias: ls: not found<br>
&gt;  In rank 0 and host= centos  Do Barrier call 1.<br>
&gt;  In rank 0 and host= centos  Do Barrier call 2.<br>
&gt; In rank 1 and host= RAID  Do Barrier call 1.<br>
&gt;  In rank 1 and host= RAID  Do Barrier call 2.<br>
&gt;<br>
&gt;   Since it is such a simple test and problem and such a widely used MPI function, it must obviously<br>
&gt; be an installation or configuration problem.   A pstack for each of the hung MPI_Barrier processes<br>
&gt; on the two machines shows this:<br>
&gt;<br>
&gt; [root@centos ~]# pstack 31666<br>
&gt; #0  0x0000003baf0e8ee3 in __epoll_wait_nocancel () from /lib64/libc.so.6<br>
&gt; #1  0x00007f5de06125eb in epoll_dispatch () from /usr/local/lib/libmpi.so.1<br>
&gt; #2  0x00007f5de061475a in opal_event_base_loop () from /usr/local/lib/libmpi.so.1<br>
&gt; #3  0x00007f5de0639229 in opal_progress () from /usr/local/lib/libmpi.so.1<br>
&gt; #4  0x00007f5de0586f75 in ompi_request_default_wait_all () from /usr/local/lib/libmpi.so.1<br>
&gt; #5  0x00007f5ddc59565e in ompi_coll_tuned_sendrecv_actual () from /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt; #6  0x00007f5ddc59d8ff in ompi_coll_tuned_barrier_intra_two_procs () from /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt; #7  0x00007f5de05941c2 in PMPI_Barrier () from /usr/local/lib/libmpi.so.1<br>
&gt; #8  0x0000000000400a43 in main ()<br>
&gt;<br>
&gt; [root@RAID openmpi-1.6.5]# pstack 22167<br>
&gt; #0  0x00000030302e8ee3 in __epoll_wait_nocancel () from /lib64/libc.so.6<br>
&gt; #1  0x00007f7ee46885eb in epoll_dispatch () from /usr/local/lib/libmpi.so.1<br>
&gt; #2  0x00007f7ee468a75a in opal_event_base_loop () from /usr/local/lib/libmpi.so.1<br>
&gt; #3  0x00007f7ee46af229 in opal_progress () from /usr/local/lib/libmpi.so.1<br>
&gt; #4  0x00007f7ee45fcf75 in ompi_request_default_wait_all () from /usr/local/lib/libmpi.so.1<br>
&gt; #5  0x00007f7ee060b65e in ompi_coll_tuned_sendrecv_actual () from /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt; #6  0x00007f7ee06138ff in ompi_coll_tuned_barrier_intra_two_procs () from /usr/local/lib/openmpi/mca_coll_tuned.so<br>
&gt; #7  0x00007f7ee460a1c2 in PMPI_Barrier () from /usr/local/lib/libmpi.so.1<br>
&gt; #8  0x0000000000400a43 in main ()<br>
&gt;<br>
&gt;  Which looks exactly the same on each machine.  Any thoughts or ideas would be greatly appreciated as<br>
&gt; I am stuck.<br>
&gt;<br>
&gt;  Clay Kirkland<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
-------------- next part --------------<br>
HTML attachment scrubbed and removed<br>
<br>
------------------------------<br>
<br>
Subject: Digest Footer<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
------------------------------<br>
<br>
End of users Digest, Vol 2879, Issue 1<br>
**************************************<br>
</blockquote></div><br></div>

