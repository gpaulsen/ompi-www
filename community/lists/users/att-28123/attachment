<div dir="ltr"><div>Thanks for the reply, Ralph. <br><br>Now I think it is clearer to me why it could be so much slower. The reason would be that the blocking algorithm for reduction has a implementation very different than the non-blocking. <br></div><div><br></div><div>Since there are lots of ways to implement it, are there options to tune the non-blocking reduction algorithm and its parameters?<br><br></div><div>Something like the ones we have for the blocking versions, for instance: &quot;coll_tuned_allreduce_algorithm&quot;, &quot;coll_tuned_reduce_algorithm&quot;, etc.<br></div><div></div><div><br>--<br></div><div>Felipe<br></div></div><div class="gmail_extra"><br><div class="gmail_quote">2015-11-27 18:20 GMT-02:00 Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span>:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">One thing you might want to keep in mind is that “non-blocking” doesn’t mean “asynchronous progress”. The API may not block, but the communications only progress whenever you actually call down into the library.<div><br></div><div>So if you are calling a non-blocking collective, and then make additional calls into MPI only rarely, you should expect to see slower performance.</div><div><br></div><div>We are working on providing async progress on all operations, but I don’t believe much (if any) of it is in the release branches so far.</div><div><br></div><div><br><div><blockquote type="cite"><div><div class="h5"><div>On Nov 27, 2015, at 11:37 AM, Felipe . &lt;<a href="mailto:philip.fm@gmail.com" target="_blank">philip.fm@gmail.com</a>&gt; wrote:</div><br></div></div><div><div><div class="h5"><div dir="ltr" style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px">&gt;Try and do a variable amount of work for every process, I see non-blocking<span> </span><br>&gt;as a way to speed-up communication if they arrive individually to the call.<span> </span><br>&gt;Please always have this at the back of your mind when doing this.<div class="gmail_extra"><br></div><div class="gmail_extra">I tried to simplify the problem at the explanation. The &quot;local_computation&quot; is variable among different processes, so there is load imbalance in the real problem.<br></div><div class="gmail_extra">The microbenchmark was just a way to test the overhead, which was really much greater than expectations.<br><br>&gt;Surely non-blocking has overhead, and if the communication time is low, so<span> </span><br>&gt;will the overhead be much higher.<span> </span><br><br></div><div class="gmail_extra">Off course there is. But for my case, which is a real HPC application for seismic data processing, it was prohibitive and strangely high.<br></div><div class="gmail_extra"><br>&gt;You haven&#39;t specified what nx*ny*nz is, and hence your &quot;slower&quot; and<span> </span><br>&gt;&quot;faster&quot; makes &quot;no sense&quot;... And hence your questions are difficult to<span> </span><br>&gt;answer, basically &quot;it depends&quot;.<span> </span><br><br></div><div class="gmail_extra">On my tests, I used nx = 700, ny = 200,  nz = 60, total_iter = 1000. val is a real(4) array. This is basically the same sizeas the real application.<br></div><div class="gmail_extra">Since I used the same values for all tests, it is reasonable to analyze the results.<span> </span><br>What I meant with question 1 was: overheads so high are expected?<br><br></div><div class="gmail_extra">The microbenchmark is attached to this e-mail.<br></div><div class="gmail_extra"><br></div><div class="gmail_extra">The detailed result was (using 11 nodes):<br><font face="Default Sans Serif,Verdana,Arial,Helvetica,sans-serif" size="2"><span><br>openmpi blocking:<br></span><font face="Default Monospace,Courier New,Courier,monospace"> ==================================<br> [RESULT] Reduce time =  21.790411<br> [RESULT] Total  time =  24.977373<br> ==================================</font><br><br><span><span>openmpi non-blocking:<br><font face="Default Monospace,Courier New,Courier,monospace"> ==================================<br> [RESULT] Reduce time =  97.332792<br> [RESULT] Total  time = 100.470874<br> ==================================</font><br><br>Intel MPI + blocking:<br><font face="Default Monospace,Courier New,Courier,monospace"> ==================================<br> [RESULT] Reduce time =  17.587828<br> [RESULT] Total  time =  20.655875<br> ==================================</font><br><br><br>Intel MPI + non-blocking:<br> <font face="Default Monospace,Courier New,Courier,monospace">==================================<br> [RESULT] Reduce time =  49.483195<br> [RESULT] Total  time =  52.642514<br> ==================================</font></span></span></font><br></div><div class="gmail_extra"><br></div><div class="gmail_extra">Thanks in advance.<br></div><div class="gmail_extra"><br><div class="gmail_quote">2015-11-27 14:57 GMT-02:00 Felipe .<span> </span><span dir="ltr">&lt;<a href="mailto:philip.fm@gmail.com" target="_blank">philip.fm@gmail.com</a>&gt;</span>:<br><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid;border-left-color:rgb(204,204,204);padding-left:1ex"><div dir="ltr"><div><span style="font-family:monospace,monospace">Hello!<br><br></span></div><div><span style="font-family:monospace,monospace">I have a program that basically is (first implementation):<br><span style="color:rgb(0,0,255)">for i in N:<br></span></span></div><div><span style="color:rgb(0,0,255)"><span style="font-family:monospace,monospace"> <span> </span>local_computation(i)<br></span></span></div><div><span style="font-family:monospace,monospace"><span style="color:rgb(0,0,255)"> <span> </span>mpi_allreduce(in_place, i)<br></span><br></span></div><div><span style="font-family:monospace,monospace">In order to try to mitigate the implicit barrier of the mpi_allreduce, I tried to start an mpi_Iallreduce. Like this(second implementation):<br><span style="color:rgb(0,0,255)"><font size="2">for i in N:<br></font></span></span><div><span style="color:rgb(0,0,255)"><span style="font-family:monospace,monospace"><font size="2"> <span> </span>local_computation(i)<br></font></span></span></div><div><span style="color:rgb(0,0,255)"><span style="font-family:monospace,monospace"><font size="2"> <span> </span>j = i<br></font></span></span></div><div><span style="color:rgb(0,0,255)"><span style="font-family:monospace,monospace"><font size="2"> <span> </span>if i is not first:<br></font></span></span></div><div><span style="color:rgb(0,0,255)"><span style="font-family:monospace,monospace"><font size="2">   <span> </span>mpi_wait(request)<br></font></span></span></div><span style="font-family:monospace,monospace"><span style="color:rgb(0,0,255)"><font size="2"> <span> </span>mpi_Iallreduce(in_place, j, request)</font></span><br><br>The result was that the second was a lot worse. The processes spent 3 times more time on the mpi_wait than on the mpi_allreduce from the first implementation. I know it could be worst, but not that much.<br><br></span></div><div><span style="font-family:monospace,monospace">So, I made a microbenchmark to stress this, in Fortran. Here is the implementation:<br></span></div><div><span style="font-family:monospace,monospace">Blocking:<br><span style="color:rgb(0,0,255)">do i = 1, total_iter ! [<br>   <span> </span>t_0 = mpi_wtime()<br><br>   <span> </span>call mpi_allreduce(MPI_IN_PLACE, val, nx*ny*nz, MPI_REAL, MPI_SUM, MPI_COMM_WORLD, ierror)<br>   <span> </span>if (ierror .ne. 0) then ! [<br>       <span> </span>write(*,*) &quot;Error in line &quot;, __LINE__, &quot; rank = &quot;, rank<br>       <span> </span>call mpi_abort(MPI_COMM_WORLD, ierror, ierror2)<br>   <span> </span>end if ! ]<br>   <span> </span>t_reduce = t_reduce + (mpi_wtime() - t_0)<br>end do ! ]</span><br><br></span></div><div><span style="font-family:monospace,monospace">Non-Blocking:<br><span style="color:rgb(0,0,255)">do i = 1, total_iter ! [<br>   <span> </span>t_0 = mpi_wtime()<br>   <span> </span>call mpi_iallreduce(MPI_IN_PLACE, val, nx*ny*nz, MPI_REAL, MPI_SUM, MPI_COMM_WORLD, request, ierror)<br>   <span> </span>if (ierror .ne. 0) then ! [<br>       <span> </span>write(*,*) &quot;Error in line &quot;, __LINE__, &quot; rank = &quot;, rank<br>       <span> </span>call mpi_abort(MPI_COMM_WORLD, ierror, ierror2)<br>   <span> </span>end if ! ]<br>   <span> </span>t_reduce = t_reduce + (mpi_wtime() - t_0)<br><br>   <span> </span>t_0 = mpi_wtime()<br>   <span> </span>call mpi_wait(request, status, ierror)<br>   <span> </span>if (ierror .ne. 0) then ! [<br>       <span> </span>write(*,*) &quot;Error in line &quot;, __LINE__, &quot; rank = &quot;, rank<br>       <span> </span>call mpi_abort(MPI_COMM_WORLD, ierror, ierror2)<br>   <span> </span>end if ! ]<br>   <span> </span>t_reduce = t_reduce + (mpi_wtime() - t_0)<br><br>end do ! ]</span><br><br></span></div><div><span style="font-family:monospace,monospace">The non-blocking was about five times slower. I tried Intel&#39;s MPI and it was of 3 times, instead of 5.<br><br></span></div><div><span style="font-family:monospace,monospace"><span style="font-family:monospace,monospace">Question 1: Do you think that all this overhead makes sense?</span><br><br>Question 2: Why is there so much overhead for non-blocking collective calls?<br></span><br><span style="font-family:monospace,monospace"><span style="font-family:monospace,monospace">Question 3: Can I change the algorithm for the non-blocking allReduce to improve this?</span><br></span></div><div><span style="font-family:monospace,monospace"><br><br></span></div><div><span style="font-family:monospace,monospace">Best regards,<br>--<br></span></div><span style="font-family:monospace,monospace">Felipe<br></span></div></blockquote></div><br></div></div></div></div><span>&lt;Teste_AllReduce.F90&gt;</span><span style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;float:none;display:inline!important">_______________________________________________</span><br style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px"><span style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;float:none;display:inline!important">users mailing list</span><br style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px"><a href="mailto:users@open-mpi.org" style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" target="_blank">users@open-mpi.org</a><br style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px"><span style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;float:none;display:inline!important">Subscription:<span> </span></span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px"><span style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;float:none;display:inline!important">Link to this post:<span> </span></span><a href="http://www.open-mpi.org/community/lists/users/2015/11/28119.php" style="font-family:Helvetica;font-size:12px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px" target="_blank">http://www.open-mpi.org/community/lists/users/2015/11/28119.php</a></div></blockquote></div><br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/11/28120.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/11/28120.php</a><br></blockquote></div><br></div>

