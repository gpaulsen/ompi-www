
<HTML><BODY><p></p><p>I enclosure 2 files with output of two foloowing commands (OMPI&nbsp;1.9a1r32570)</p><p></p><p><span style="font-size: 12px;">$time mpirun --leave-session-attached -mca oob_base_verbose 100 -np 1 ./hello_c &gt;&amp; out1.txt <br>(Hello, world, I am ....)<br></span><span style="font-size: 12px;">real 1m3.952s<br></span><span style="font-size: 12px;">user 0m0.035s<br></span><span style="font-size: 12px;">sys 0m0.107s</span></p><p>$time mpirun --leave-session-attached -mca oob_base_verbose 100 --mca oob_tcp_if_include ib0 -np 1 ./hello_c &gt;&amp; out2.txt&nbsp;<br>(no Hello, word, I am ....)<br><span style="font-size: 12px;">real 0m9.337s<br></span><span style="font-size: 12px;">user 0m0.059s<br></span><span style="font-size: 12px;">sys 0m0.098s</span></p><p><span style="font-size: 12px;">Wed, 27 Aug 2014 06:31:02 -0700 от Ralph Castain &lt;rhc@open-mpi.org&gt;:</span></p><blockquote style="border-left:1px solid #0857A6; margin:10px; padding:0 0 0 10px;">
	<div id="">
	



    









	
	


	
	
	
	
	

	
	

	
	



<div class="js-helper js-readmsg-msg">
	<style type="text/css"></style>
 	<div>
		<base target="_self" href="https://e.mail.ru/">
		
			<div id="style_14091463080000000167_BODY">How bizarre. Please add "--leave-session-attached -mca oob_base_verbose 100" to your cmd line<div><br><div><div>On Aug 27, 2014, at 4:31 AM, Timur Ismagilov &lt;<a href="//e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt; wrote:</div><br><blockquote type="cite"><div style="font-family: Helvetica;font-size: 12px;font-style: normal;font-variant: normal;font-weight: normal;letter-spacing: normal;line-height: normal;orphans: auto;text-align: start;text-indent: 0px;text-transform: none;white-space: normal;widows: auto;word-spacing: 0px;-webkit-text-stroke-width: 0px;"><p>When i try to specify oob with&nbsp;--mca oob_tcp_if_include &lt;one of interface from ifconfig&gt;, i alwase get error:</p><p>$ mpirun &nbsp;--mca oob_tcp_if_include ib0 -np 1 ./hello_c<br>--------------------------------------------------------------------------<br>An ORTE daemon has unexpectedly failed after launch and before<br>communicating back to mpirun. This could be caused by a number<br>of factors, including an inability to create a connection back<br>to mpirun due to a lack of common network interfaces and/or no<br>route found between them. Please check network connectivity<br>(including firewalls and network routing requirements).<br>-------------------------------------------------------------------------<br><br>Earlier, in ompi 1.8.1, I can not run mpi jobs without "&nbsp;--mca oob_tcp_if_include ib0 "... but now(ompi 1.9.a1) with this flag i get above error.<br><br>Here is an output of ifconfig</p><p>$ ifconfig<br>eth1 Link encap:Ethernet HWaddr 00:15:17:EE:89:E1<span>&nbsp;</span><br>inet addr:10.0.251.53 Bcast:10.0.251.255 Mask:255.255.255.0<br>UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1<br>RX packets:215087433 errors:0 dropped:0 overruns:0 frame:0<br>TX packets:2648 errors:0 dropped:0 overruns:0 carrier:0<br>collisions:0 txqueuelen:1000<span>&nbsp;</span><br>RX bytes:26925754883 (25.0 GiB) TX bytes:137971 (134.7 KiB)<br>Memory:b2c00000-b2c20000</p><p>eth2 Link encap:Ethernet HWaddr 00:02:C9:04:73:F8<span>&nbsp;</span><br>inet addr:10.0.0.4 Bcast:10.0.0.255 Mask:255.255.255.0<br>UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1<br>RX packets:4892833125 errors:0 dropped:0 overruns:0 frame:0<br>TX packets:8708606918 errors:0 dropped:0 overruns:0 carrier:0<br>collisions:0 txqueuelen:1000<span>&nbsp;</span><br>RX bytes:1823986502132 (1.6 TiB) TX bytes:11957754120037 (10.8 TiB)</p><p>eth2.911 Link encap:Ethernet HWaddr 00:02:C9:04:73:F8<span>&nbsp;</span><br>inet addr:93.180.7.38 Bcast:93.180.7.63 Mask:255.255.255.224<br>UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1<br>RX packets:3746454225 errors:0 dropped:0 overruns:0 frame:0<br>TX packets:1131917608 errors:0 dropped:3 overruns:0 carrier:0<br>collisions:0 txqueuelen:0<span>&nbsp;</span><br>RX bytes:285174723322 (265.5 GiB) TX bytes:11523163526058 (10.4 TiB)</p><p>eth3 Link encap:Ethernet HWaddr 00:02:C9:04:73:F9<span>&nbsp;</span><br>inet addr:10.2.251.14 Bcast:10.2.251.255 Mask:255.255.255.0<br>UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1<br>RX packets:591156692 errors:0 dropped:56 overruns:56 frame:56<br>TX packets:679729229 errors:0 dropped:0 overruns:0 carrier:0<br>collisions:0 txqueuelen:1000<span>&nbsp;</span><br>RX bytes:324195989293 (301.9 GiB) TX bytes:770299202886 (717.3 GiB)</p><p>Ifconfig uses the ioctl access method to get the full address information, which limits hardware addresses to 8 bytes.<br>Because Infiniband address has 20 bytes, only the first 8 bytes are displayed correctly.<br>Ifconfig is obsolete! For replacement check ip.<br>ib0 Link encap:InfiniBand HWaddr 80:00:00:48:FE:80:00:00:00:00:00:00:00:00:00:00:00:00:00:00<span>&nbsp;</span><br>inet addr:10.128.0.4 Bcast:10.128.255.255 Mask:255.255.0.0<br>UP BROADCAST RUNNING MULTICAST MTU:2044 Metric:1<br>RX packets:10843859 errors:0 dropped:0 overruns:0 frame:0<br>TX packets:8089839 errors:0 dropped:15 overruns:0 carrier:0<br>collisions:0 txqueuelen:1024<span>&nbsp;</span><br>RX bytes:939249464 (895.7 MiB) TX bytes:886054008 (845.0 MiB)</p><p>lo Link encap:Local Loopback<span>&nbsp;</span><br>inet addr:127.0.0.1 Mask:255.0.0.0<br>UP LOOPBACK RUNNING MTU:16436 Metric:1<br>RX packets:31235107 errors:0 dropped:0 overruns:0 frame:0<br>TX packets:31235107 errors:0 dropped:0 overruns:0 carrier:0<br>collisions:0 txqueuelen:0<span>&nbsp;</span><br>RX bytes:132750916041 (123.6 GiB) TX bytes:132750916041 (123.6 GiB)</p><p><br><br><br>Tue, 26 Aug 2014 09:48:35 -0700 от Ralph Castain &lt;<a href="//e.mail.ru/compose/?mailto=mailto%3arhc@open%2dmpi.org" target="_blank">rhc@open-mpi.org</a>&gt;:<br></p><blockquote style="border-left-width: 1px;border-left-style: solid;border-left-color: rgb(8, 87, 166);margin: 10px;padding: 0px 0px 0px 10px;"><div><div><div>I think something may be messed up with your installation. I went ahead and tested this on a Slurm 2.5.4 cluster, and got the following:<div><br></div><div><div style="margin: 0px;font-size: 11px;font-family: Menlo;background-color: rgb(254, 244, 156);">$ time mpirun -np 1 --host bend001 ./hello</div><div style="margin: 0px;font-size: 11px;font-family: Menlo;background-color: rgb(254, 244, 156);">Hello, World, I am 0 of 1 [0 local peers]: get_cpubind: 0 bitmap 0,12</div><div style="margin: 0px;font-size: 11px;font-family: Menlo;background-color: rgb(254, 244, 156);min-height: 13px;"><br></div><div style="margin: 0px;font-size: 11px;font-family: Menlo;background-color: rgb(254, 244, 156);">real<span style="white-space: pre;">	</span>0m0.086s</div><div style="margin: 0px;font-size: 11px;font-family: Menlo;background-color: rgb(254, 244, 156);">user<span style="white-space: pre;">	</span>0m0.039s</div><div style="margin: 0px;font-size: 11px;font-family: Menlo;background-color: rgb(254, 244, 156);">sys<span style="white-space: pre;">	</span>0m0.046s</div><div><br></div><div><div style="margin: 0px;font-size: 11px;font-family: Menlo;background-color: rgb(254, 244, 156);">$ time mpirun -np 1 --host bend002 ./hello</div><div style="margin: 0px;font-size: 11px;font-family: Menlo;background-color: rgb(254, 244, 156);">Hello, World, I am 0 of 1 [0 local peers]: get_cpubind: 0 bitmap 0,12</div><div style="margin: 0px;font-size: 11px;font-family: Menlo;background-color: rgb(254, 244, 156);min-height: 13px;"><br></div><div style="margin: 0px;font-size: 11px;font-family: Menlo;background-color: rgb(254, 244, 156);">real<span style="white-space: pre;">	</span>0m0.528s</div><div style="margin: 0px;font-size: 11px;font-family: Menlo;background-color: rgb(254, 244, 156);">user<span style="white-space: pre;">	</span>0m0.021s</div><div style="margin: 0px;font-size: 11px;font-family: Menlo;background-color: rgb(254, 244, 156);">sys<span style="white-space: pre;">	</span>0m0.023s</div></div><div><br></div><div>Which is what I would have expected. With --host set to the local host, no daemons are being launched and so the time is quite short (just spent mapping and fork/exec). With --host set to a single remote host, you have the time it takes Slurm to launch our daemon on the remote host, so you get about half of a second.</div><div><br></div><div>IIRC, you were having some problems with the OOB setup. If you specify the TCP interface to use, does your time come down?</div><div><br></div><div><br></div><div><div>On Aug 26, 2014, at 8:32 AM, Timur Ismagilov &lt;<a target="_blank">tismagilov@mail.ru</a>&gt; wrote:</div><br><blockquote type="cite"><div style="font-family: Helvetica;font-size: 12px;font-style: normal;font-variant: normal;font-weight: normal;letter-spacing: normal;line-height: normal;orphans: auto;text-align: start;text-indent: 0px;text-transform: none;white-space: normal;widows: auto;word-spacing: 0px;-webkit-text-stroke-width: 0px;"><div><p>I'm using slurm 2.5.6<br><br>$salloc -N8 --exclusive -J ompi -p test<br></p><p>$ srun hostname<br>node1-128-21<br>node1-128-24<br>node1-128-22<br>node1-128-26<br>node1-128-27<br>node1-128-20<br>node1-128-25<br>node1-128-23</p><p>$ time mpirun -np 1 --host node1-128-21 ./hello_c<br>Hello, world, I am 0 of 1, (Open MPI v1.9a1, package: Open MPI semenov@compiler-2 Distribution, ident: 1.9a1r32570, repo rev: r32570, Aug 21, 2014 (nightly snapshot tarball), 146)</p><p>real 1m3.932s<br>user 0m0.035s<br>sys 0m0.072s</p><br><br><br>Tue, 26 Aug 2014 07:03:58 -0700 от Ralph Castain &lt;<a target="_blank">rhc@open-mpi.org</a>&gt;:<br><blockquote style="border-left-width: 1px;border-left-style: solid;border-left-color: rgb(8, 87, 166);margin: 10px;padding: 0px 0px 0px 10px;">hmmm....what is your allocation like? do you have a large hostfile, for example?<div><br></div><div>if you add a --host argument that contains just the local host, what is the time for that scenario?</div><div><br><div><div>On Aug 26, 2014, at 6:27 AM, Timur Ismagilov &lt;<a target="_blank">tismagilov@mail.ru</a>&gt; wrote:</div><br><blockquote type="cite"><div style="font-family: Helvetica;font-size: 12px;font-style: normal;font-variant: normal;font-weight: normal;letter-spacing: normal;line-height: normal;orphans: auto;text-align: start;text-indent: 0px;text-transform: none;white-space: normal;widows: auto;word-spacing: 0px;-webkit-text-stroke-width: 0px;"><p>Hello!<br>Here is my time results:</p><p>$time mpirun -n 1 ./hello_c<br>Hello, world, I am 0 of 1, (Open MPI v1.9a1, package: Open MPI semenov@compiler-2 Distribution, ident: 1.9a1r32570, repo rev: r32570, Aug 21, 2014 (nightly snapshot tarball), 146)</p><p>real 1m3.985s<br>user 0m0.031s<br>sys 0m0.083s</p><br><br><br>Fri, 22 Aug 2014 07:43:03 -0700 от Ralph Castain &lt;<a target="_blank">rhc@open-mpi.org</a>&gt;:<br><blockquote style="border-left-width: 1px;border-left-style: solid;border-left-color: rgb(8, 87, 166);margin: 10px;padding: 0px 0px 0px 10px;"><div>I'm also puzzled by your timing statement - I can't replicate it:<div><br></div><div><div style="margin: 0px;font-size: 11px;font-family: Menlo;color: rgb(206, 51, 204);background-color: rgb(254, 244, 156);"><span style="color: rgb(124, 124, 124);">07:41:43&nbsp;</span><span>&nbsp;</span><span>$ time mpirun -n 1 ./hello_c</span></div><div style="margin: 0px;font-size: 11px;font-family: Menlo;background-color: rgb(254, 244, 156);">Hello, world, I am 0 of 1, (Open MPI v1.9a1, package: Open MPI rhc@bend001 Distribution, ident: 1.9a1r32577, repo rev: r32577, Unreleased developer copy, 125)</div><div style="margin: 0px;font-size: 11px;font-family: Menlo;background-color: rgb(254, 244, 156);min-height: 13px;"><br></div><div style="margin: 0px;font-size: 11px;font-family: Menlo;background-color: rgb(254, 244, 156);">real<span style="white-space: pre;">	</span>0m0.547s</div><div style="margin: 0px;font-size: 11px;font-family: Menlo;background-color: rgb(254, 244, 156);">user<span style="white-space: pre;">	</span>0m0.043s</div><div style="margin: 0px;font-size: 11px;font-family: Menlo;background-color: rgb(254, 244, 156);">sys<span style="white-space: pre;">	</span>0m0.046s</div><div><br></div><div>The entire thing ran in 0.5 seconds</div><div><br></div><div><br></div><div><div>On Aug 22, 2014, at 6:33 AM, Mike Dubman &lt;<a target="_blank">miked@dev.mellanox.co.il</a>&gt; wrote:</div><br><blockquote type="cite"><div dir="ltr">Hi,<div>The default delimiter is ";" . You can change delimiter with mca_base_env_list_delimiter.</div><div><br></div></div><div><br><br><div>On Fri, Aug 22, 2014 at 2:59 PM, Timur Ismagilov<span>&nbsp;</span><span dir="ltr">&lt;<a target="_blank">tismagilov@mail.ru</a>&gt;</span><span>&nbsp;</span>wrote:<br><blockquote style="margin: 0px 0px 0px 0.8ex;border-left-width: 1px;border-left-color: rgb(204, 204, 204);border-left-style: solid;padding-left: 1ex;"><div>Hello!<br>If i use latest night snapshot:<br><p>$ ompi_info -V<br>Open MPI v1.9a1r32570<br></p><ol><li>In programm hello_c initialization&nbsp;takes ~1 min<br><span style="font-size: 12px;">In ompi 1.8.2rc4 and ealier it takes ~1 sec(or less)</span></li><li><span style="font-size: 12px;">if i use&nbsp;<br>$mpirun &nbsp;--mca mca_base_env_list 'MXM_SHM_KCOPY_MODE=off,OMP_NUM_THREADS=8' --map-by slot:pe=8 -np 1 ./hello_c<br>i got error&nbsp;<br>config_parser.c:657 &nbsp;MXM &nbsp;ERROR Invalid value for SHM_KCOPY_MODE: 'off,OMP_NUM_THREADS=8'. Expected: [off|knem|cma|autodetect]<br>but with -x all works fine (but with warn)<br></span><span style="font-size: 12px;">$mpirun &nbsp;-x MXM_SHM_KCOPY_MODE=off -x OMP_NUM_THREADS=8 -np 1 ./hello_c<br></span><p>WARNING: The mechanism by which environment variables are explicitly<br>..............<br>..............<br>..............<br><span style="font-size: 12px;">Hello, world, I am 0 of 1, (Open MPI v1.9a1, package: Open MPI semenov@compiler-2 Distribution, ident: 1.9a1r32570, repo rev: r32570, Aug 21, 2014 (nightly snapshot tarball), 146)</span></p><span style="font-size: 12px;"><br></span></li></ol>Thu, 21 Aug 2014 06:26:13 -0700 от Ralph Castain &lt;<a target="_blank">rhc@open-mpi.org</a>&gt;:<br><blockquote style="border-left-width: 1px;border-left-style: solid;border-left-color: rgb(8, 87, 166);margin: 10px;padding: 0px 0px 0px 10px;">Not sure I understand. The problem has been fixed in both the trunk and the 1.8 branch now, so you should be able to work with either of those nightly builds.<div><br><div><div>On Aug 21, 2014, at 12:02 AM, Timur Ismagilov &lt;<a target="_blank">tismagilov@mail.ru</a>&gt; wrote:</div><br><blockquote type="cite"><div style="font-family: Helvetica;font-size: 12px;font-style: normal;font-variant: normal;font-weight: normal;letter-spacing: normal;line-height: normal;text-align: start;text-indent: 0px;text-transform: none;white-space: normal;word-spacing: 0px;">Have i&nbsp;I any opportunity to run mpi jobs?<br><br><br>Wed, 20 Aug 2014 10:48:38 -0700 от Ralph Castain &lt;<a target="_blank">rhc@open-mpi.org</a>&gt;:<br><blockquote style="border-left-width: 1px;border-left-style: solid;border-left-color: rgb(8, 87, 166);margin: 10px;padding: 0px 0px 0px 10px;"><div>yes, i know - it is cmr'd<div><br><div><div>On Aug 20, 2014, at 10:26 AM, Mike Dubman &lt;<a>miked@dev.mellanox.co.il</a>&gt; wrote:</div><br><blockquote type="cite"><div dir="ltr">btw, we get same error in v1.8 branch as well.</div><div><br><br><div>On Wed, Aug 20, 2014 at 8:06 PM, Ralph Castain<span>&nbsp;</span><span dir="ltr">&lt;<a>rhc@open-mpi.org</a>&gt;</span><span>&nbsp;</span>wrote:<br><blockquote style="margin: 0px 0px 0px 0.8ex;border-left-width: 1px;border-left-color: rgb(204, 204, 204);border-left-style: solid;padding-left: 1ex;"><div style="word-wrap: break-word;">It was not yet fixed - but should be now.<div><br><div><div><div>On Aug 20, 2014, at 6:39 AM, Timur Ismagilov &lt;<a>tismagilov@mail.ru</a>&gt; wrote:</div><br></div><blockquote type="cite"><div style="font-family: Helvetica;font-size: 12px;font-style: normal;font-variant: normal;font-weight: normal;letter-spacing: normal;line-height: normal;text-align: start;text-indent: 0px;text-transform: none;white-space: normal;word-spacing: 0px;"><div><p>Hello!<br><br>As i can see, the bug is fixed, but in&nbsp;Open MPI v1.9a1r32516&nbsp; i still have the problem<br><br>a)<br><span style="font-size: 12px;">$ mpirun &nbsp;-np 1 ./hello_c</span></p><p>--------------------------------------------------------------------------<br>An ORTE daemon has unexpectedly failed after launch and before<br>communicating back to mpirun. This could be caused by a number<br>of factors, including an inability to create a connection back<br>to mpirun due to a lack of common network interfaces and/or no<br>route found between them. Please check network connectivity<br>(including firewalls and network routing requirements).<br>--------------------------------------------------------------------------<br></p><p>b)<br>$ mpirun --mca oob_tcp_if_include ib0 -np 1 ./hello_c<br>--------------------------------------------------------------------------<br>An ORTE daemon has unexpectedly failed after launch and before<br>communicating back to mpirun. This could be caused by a number<br>of factors, including an inability to create a connection back<br>to mpirun due to a lack of common network interfaces and/or no<br>route found between them. Please check network connectivity<br>(including firewalls and network routing requirements).<br>--------------------------------------------------------------------------<br><br>c)<br><br><span style="font-size: 12px;">$ mpirun --mca oob_tcp_if_include ib0 -debug-daemons --mca plm_base_verbose 5 -mca oob_base_verbose 10 -mca rml_base_verbose 10 -np 1 ./hello_c</span></p><p>[compiler-2:14673] mca:base:select:( plm) Querying component [isolated]<br>[compiler-2:14673] mca:base:select:( plm) Query of component [isolated] set priority to 0<br>[compiler-2:14673] mca:base:select:( plm) Querying component [rsh]<br>[compiler-2:14673] mca:base:select:( plm) Query of component [rsh] set priority to 10<br>[compiler-2:14673] mca:base:select:( plm) Querying component [slurm]<br>[compiler-2:14673] mca:base:select:( plm) Query of component [slurm] set priority to 75<br>[compiler-2:14673] mca:base:select:( plm) Selected component [slurm]<br>[compiler-2:14673] mca: base: components_register: registering oob components<br>[compiler-2:14673] mca: base: components_register: found loaded component tcp<br>[compiler-2:14673] mca: base: components_register: component tcp register function successful<br>[compiler-2:14673] mca: base: components_open: opening oob components<br>[compiler-2:14673] mca: base: components_open: found loaded component tcp<br>[compiler-2:14673] mca: base: components_open: component tcp open function successful<br>[compiler-2:14673] mca:oob:select: checking available component tcp<br>[compiler-2:14673] mca:oob:select: Querying component [tcp]<br>[compiler-2:14673] oob:tcp: component_available called<br>[compiler-2:14673] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>[compiler-2:14673] WORKING INTERFACE 2 KERNEL INDEX 3 FAMILY: V4<br>[compiler-2:14673] WORKING INTERFACE 3 KERNEL INDEX 4 FAMILY: V4<br>[compiler-2:14673] WORKING INTERFACE 4 KERNEL INDEX 5 FAMILY: V4<br>[compiler-2:14673] WORKING INTERFACE 5 KERNEL INDEX 6 FAMILY: V4<br>[compiler-2:14673] [[49095,0],0] oob:tcp:init adding 10.128.0.4 to our list of V4 connections<br>[compiler-2:14673] WORKING INTERFACE 6 KERNEL INDEX 7 FAMILY: V4<br>[compiler-2:14673] [[49095,0],0] TCP STARTUP<br>[compiler-2:14673] [[49095,0],0] attempting to bind to IPv4 port 0<br>[compiler-2:14673] [[49095,0],0] assigned IPv4 port 59460<br>[compiler-2:14673] mca:oob:select: Adding component to end<br>[compiler-2:14673] mca:oob:select: Found 1 active transports<br>[compiler-2:14673] mca: base: components_register: registering rml components<br>[compiler-2:14673] mca: base: components_register: found loaded component oob<br>[compiler-2:14673] mca: base: components_register: component oob has no register or open function<br>[compiler-2:14673] mca: base: components_open: opening rml components<br>[compiler-2:14673] mca: base: components_open: found loaded component oob<br>[compiler-2:14673] mca: base: components_open: component oob open function successful<br>[compiler-2:14673] orte_rml_base_select: initializing rml component oob<br>[compiler-2:14673] [[49095,0],0] posting recv<br>[compiler-2:14673] [[49095,0],0] posting persistent recv on tag 30 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:14673] [[49095,0],0] posting recv<br>[compiler-2:14673] [[49095,0],0] posting persistent recv on tag 15 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:14673] [[49095,0],0] posting recv<br>[compiler-2:14673] [[49095,0],0] posting persistent recv on tag 32 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:14673] [[49095,0],0] posting recv<br>[compiler-2:14673] [[49095,0],0] posting persistent recv on tag 33 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:14673] [[49095,0],0] posting recv<br>[compiler-2:14673] [[49095,0],0] posting persistent recv on tag 5 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:14673] [[49095,0],0] posting recv<br>[compiler-2:14673] [[49095,0],0] posting persistent recv on tag 10 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:14673] [[49095,0],0] posting recv<br>[compiler-2:14673] [[49095,0],0] posting persistent recv on tag 12 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:14673] [[49095,0],0] posting recv<br>[compiler-2:14673] [[49095,0],0] posting persistent recv on tag 9 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:14673] [[49095,0],0] posting recv<br>[compiler-2:14673] [[49095,0],0] posting persistent recv on tag 34 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:14673] [[49095,0],0] posting recv<br>[compiler-2:14673] [[49095,0],0] posting persistent recv on tag 2 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:14673] [[49095,0],0] posting recv<br>[compiler-2:14673] [[49095,0],0] posting persistent recv on tag 21 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:14673] [[49095,0],0] posting recv<br>[compiler-2:14673] [[49095,0],0] posting persistent recv on tag 22 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:14673] [[49095,0],0] posting recv<br>[compiler-2:14673] [[49095,0],0] posting persistent recv on tag 45 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:14673] [[49095,0],0] posting recv<br>[compiler-2:14673] [[49095,0],0] posting persistent recv on tag 46 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:14673] [[49095,0],0] posting recv<br>[compiler-2:14673] [[49095,0],0] posting persistent recv on tag 1 for peer [[WILDCARD],WILDCARD]<br>[compiler-2:14673] [[49095,0],0] posting recv<br>[compiler-2:14673] [[49095,0],0] posting persistent recv on tag 27 for peer [[WILDCARD],WILDCARD]<br>Daemon was launched on node1-128-01 - beginning to initialize<br>--------------------------------------------------------------------------<br>WARNING: An invalid value was given for oob_tcp_if_include. This<br>value will be ignored.</p><p>Local host: node1-128-01<br>Value: "ib0"<br>Message: Invalid specification (missing "/")<br>--------------------------------------------------------------------------<br>--------------------------------------------------------------------------<br>None of the TCP networks specified to be included for out-of-band communications<br>could be found:</p><p>Value given:</p><p>Please revise the specification and try again.<br>--------------------------------------------------------------------------<br>--------------------------------------------------------------------------<br>No network interfaces were found for out-of-band communications. We require<br>at least one available network for out-of-band messaging.<br>--------------------------------------------------------------------------<br>--------------------------------------------------------------------------<br>It looks like orte_init failed for some reason; your parallel process is<br>likely to abort. There are many reasons that a parallel process can<br>fail during orte_init; some of which are due to configuration or<br>environment problems. This failure appears to be an internal failure;<br>here's some additional information (which may only be relevant to an<br>Open MPI developer):</p><p>orte_oob_base_select failed<br>--&gt; Returned value (null) (-43) instead of ORTE_SUCCESS<br>--------------------------------------------------------------------------<br>srun: error: node1-128-01: task 0: Exited with exit code 213<br>srun: Terminating job step 661215.0<br>--------------------------------------------------------------------------<br>An ORTE daemon has unexpectedly failed after launch and before<br>communicating back to mpirun. This could be caused by a number<br>of factors, including an inability to create a connection back<br>to mpirun due to a lack of common network interfaces and/or no<br>route found between them. Please check network connectivity<br>(including firewalls and network routing requirements).<br>--------------------------------------------------------------------------<br>[compiler-2:14673] [[49095,0],0] orted_cmd: received halt_vm cmd<br>[compiler-2:14673] mca: base: close: component oob closed<br>[compiler-2:14673] mca: base: close: unloading component oob<br>[compiler-2:14673] [[49095,0],0] TCP SHUTDOWN<br>[compiler-2:14673] mca: base: close: component tcp closed<br>[compiler-2:14673] mca: base: close: unloading component tcp</p><br><br><br>Tue, 12 Aug 2014 18:33:24 +0000 от "Jeff Squyres (jsquyres)" &lt;<a>jsquyres@cisco.com</a>&gt;:<br><blockquote style="border-left-width: 1px;border-left-style: solid;border-left-color: rgb(8, 87, 166);margin: 10px;padding: 0px 0px 0px 10px;">I filed the following ticket:<br><br>&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://svn.open-mpi.org/trac/ompi/ticket/4857" target="_blank">https://svn.open-mpi.org/trac/ompi/ticket/4857</a><br><br><br>On Aug 12, 2014, at 12:39 PM, Jeff Squyres (jsquyres) &lt;<a>jsquyres@cisco.com</a>&gt; wrote:<br><br>&gt; (please keep the users list CC'ed)<br>&gt;<span>&nbsp;</span><br>&gt; We talked about this on the weekly engineering call today. Ralph has an idea what is happening -- I need to do a little investigation today and file a bug. I'll make sure you're CC'ed on the bug ticket.<br>&gt;<span>&nbsp;</span><br>&gt;<span>&nbsp;</span><br>&gt;<span>&nbsp;</span><br>&gt; On Aug 12, 2014, at 12:27 PM, Timur Ismagilov &lt;<a>tismagilov@mail.ru</a>&gt; wrote:<br>&gt;<span>&nbsp;</span><br>&gt;&gt; I don't have this error in OMPI 1.9a1r32252 and OMPI 1.8.1 (with --mca oob_tcp_if_include ib0), but in all latest night snapshots i got this error.<br>&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;<span>&nbsp;</span><br>&gt;&gt; Tue, 12 Aug 2014 13:08:12 +0000 от "Jeff Squyres (jsquyres)" &lt;<a>jsquyres@cisco.com</a>&gt;:<br>&gt;&gt; Are you running any kind of firewall on the node where mpirun is invoked? Open MPI needs to be able to use arbitrary TCP ports between the servers on which it runs.<br>&gt;&gt;<span>&nbsp;</span><br>&gt;&gt; This second mail seems to imply a bug in OMPI's oob_tcp_if_include param handling, however -- it's supposed to be able to handle an interface name (not just a network specification).<br>&gt;&gt;<span>&nbsp;</span><br>&gt;&gt; Ralph -- can you have a look?<br>&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;<span>&nbsp;</span><br>&gt;&gt; On Aug 12, 2014, at 8:41 AM, Timur Ismagilov &lt;<a>tismagilov@mail.ru</a>&gt; wrote:<br>&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; When i add --mca oob_tcp_if_include ib0 (infiniband interface) to mpirun (as it was here:<span>&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/07/24857.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/07/24857.php</a><span>&nbsp;</span>) i got this output:<br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; [compiler-2:08792] mca:base:select:( plm) Querying component [isolated]<br>&gt;&gt;&gt; [compiler-2:08792] mca:base:select:( plm) Query of component [isolated] set priority to 0<br>&gt;&gt;&gt; [compiler-2:08792] mca:base:select:( plm) Querying component [rsh]<br>&gt;&gt;&gt; [compiler-2:08792] mca:base:select:( plm) Query of component [rsh] set priority to 10<br>&gt;&gt;&gt; [compiler-2:08792] mca:base:select:( plm) Querying component [slurm]<br>&gt;&gt;&gt; [compiler-2:08792] mca:base:select:( plm) Query of component [slurm] set priority to 75<br>&gt;&gt;&gt; [compiler-2:08792] mca:base:select:( plm) Selected component [slurm]<br>&gt;&gt;&gt; [compiler-2:08792] mca: base: components_register: registering oob components<br>&gt;&gt;&gt; [compiler-2:08792] mca: base: components_register: found loaded component tcp<br>&gt;&gt;&gt; [compiler-2:08792] mca: base: components_register: component tcp register function successful<br>&gt;&gt;&gt; [compiler-2:08792] mca: base: components_open: opening oob components<br>&gt;&gt;&gt; [compiler-2:08792] mca: base: components_open: found loaded component tcp<br>&gt;&gt;&gt; [compiler-2:08792] mca: base: components_open: component tcp open function successful<br>&gt;&gt;&gt; [compiler-2:08792] mca:oob:select: checking available component tcp<br>&gt;&gt;&gt; [compiler-2:08792] mca:oob:select: Querying component [tcp]<br>&gt;&gt;&gt; [compiler-2:08792] oob:tcp: component_available called<br>&gt;&gt;&gt; [compiler-2:08792] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>&gt;&gt;&gt; [compiler-2:08792] WORKING INTERFACE 2 KERNEL INDEX 3 FAMILY: V4<br>&gt;&gt;&gt; [compiler-2:08792] WORKING INTERFACE 3 KERNEL INDEX 4 FAMILY: V4<br>&gt;&gt;&gt; [compiler-2:08792] WORKING INTERFACE 4 KERNEL INDEX 5 FAMILY: V4<br>&gt;&gt;&gt; [compiler-2:08792] WORKING INTERFACE 5 KERNEL INDEX 6 FAMILY: V4<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] oob:tcp:init adding 10.128.0.4 to our list of V4 connections<br>&gt;&gt;&gt; [compiler-2:08792] WORKING INTERFACE 6 KERNEL INDEX 7 FAMILY: V4<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] TCP STARTUP<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] attempting to bind to IPv4 port 0<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] assigned IPv4 port 53883<br>&gt;&gt;&gt; [compiler-2:08792] mca:oob:select: Adding component to end<br>&gt;&gt;&gt; [compiler-2:08792] mca:oob:select: Found 1 active transports<br>&gt;&gt;&gt; [compiler-2:08792] mca: base: components_register: registering rml components<br>&gt;&gt;&gt; [compiler-2:08792] mca: base: components_register: found loaded component oob<br>&gt;&gt;&gt; [compiler-2:08792] mca: base: components_register: component oob has no register or open function<br>&gt;&gt;&gt; [compiler-2:08792] mca: base: components_open: opening rml components<br>&gt;&gt;&gt; [compiler-2:08792] mca: base: components_open: found loaded component oob<br>&gt;&gt;&gt; [compiler-2:08792] mca: base: components_open: component oob open function successful<br>&gt;&gt;&gt; [compiler-2:08792] orte_rml_base_select: initializing rml component oob<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting persistent recv on tag 30 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting persistent recv on tag 15 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting persistent recv on tag 32 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting persistent recv on tag 33 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting persistent recv on tag 5 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting persistent recv on tag 10 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting persistent recv on tag 12 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting persistent recv on tag 9 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting persistent recv on tag 34 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting persistent recv on tag 2 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting persistent recv on tag 21 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting persistent recv on tag 22 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting persistent recv on tag 45 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting persistent recv on tag 46 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting persistent recv on tag 1 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] posting persistent recv on tag 27 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; Daemon was launched on node1-128-01 - beginning to initialize<br>&gt;&gt;&gt; Daemon was launched on node1-128-02 - beginning to initialize<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; WARNING: An invalid value was given for oob_tcp_if_include. This<br>&gt;&gt;&gt; value will be ignored.<br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; Local host: node1-128-01<br>&gt;&gt;&gt; Value: "ib0"<br>&gt;&gt;&gt; Message: Invalid specification (missing "/")<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; WARNING: An invalid value was given for oob_tcp_if_include. This<br>&gt;&gt;&gt; value will be ignored.<br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; Local host: node1-128-02<br>&gt;&gt;&gt; Value: "ib0"<br>&gt;&gt;&gt; Message: Invalid specification (missing "/")<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; None of the TCP networks specified to be included for out-of-band communications<br>&gt;&gt;&gt; could be found:<br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; Value given:<br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; Please revise the specification and try again.<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; None of the TCP networks specified to be included for out-of-band communications<br>&gt;&gt;&gt; could be found:<br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; Value given:<br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; Please revise the specification and try again.<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; No network interfaces were found for out-of-band communications. We require<br>&gt;&gt;&gt; at least one available network for out-of-band messaging.<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; No network interfaces were found for out-of-band communications. We require<br>&gt;&gt;&gt; at least one available network for out-of-band messaging.<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; It looks like orte_init failed for some reason; your parallel process is<br>&gt;&gt;&gt; likely to abort. There are many reasons that a parallel process can<br>&gt;&gt;&gt; fail during orte_init; some of which are due to configuration or<br>&gt;&gt;&gt; environment problems. This failure appears to be an internal failure;<br>&gt;&gt;&gt; here's some additional information (which may only be relevant to an<br>&gt;&gt;&gt; Open MPI developer):<br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; orte_oob_base_select failed<br>&gt;&gt;&gt; --&gt; Returned value (null) (-43) instead of ORTE_SUCCESS<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; It looks like orte_init failed for some reason; your parallel process is<br>&gt;&gt;&gt; likely to abort. There are many reasons that a parallel process can<br>&gt;&gt;&gt; fail during orte_init; some of which are due to configuration or<br>&gt;&gt;&gt; environment problems. This failure appears to be an internal failure;<br>&gt;&gt;&gt; here's some additional information (which may only be relevant to an<br>&gt;&gt;&gt; Open MPI developer):<br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; orte_oob_base_select failed<br>&gt;&gt;&gt; --&gt; Returned value (null) (-43) instead of ORTE_SUCCESS<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; srun: error: node1-128-02: task 1: Exited with exit code 213<br>&gt;&gt;&gt; srun: Terminating job step 657300.0<br>&gt;&gt;&gt; srun: error: node1-128-01: task 0: Exited with exit code 213<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; An ORTE daemon has unexpectedly failed after launch and before<br>&gt;&gt;&gt; communicating back to mpirun. This could be caused by a number<br>&gt;&gt;&gt; of factors, including an inability to create a connection back<br>&gt;&gt;&gt; to mpirun due to a lack of common network interfaces and/or no<br>&gt;&gt;&gt; route found between them. Please check network connectivity<br>&gt;&gt;&gt; (including firewalls and network routing requirements).<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] orted_cmd: received halt_vm cmd<br>&gt;&gt;&gt; [compiler-2:08792] mca: base: close: component oob closed<br>&gt;&gt;&gt; [compiler-2:08792] mca: base: close: unloading component oob<br>&gt;&gt;&gt; [compiler-2:08792] [[42190,0],0] TCP SHUTDOWN<br>&gt;&gt;&gt; [compiler-2:08792] mca: base: close: component tcp closed<br>&gt;&gt;&gt; [compiler-2:08792] mca: base: close: unloading component tcp<br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; Tue, 12 Aug 2014 16:14:58 +0400 от Timur Ismagilov &lt;<a>tismagilov@mail.ru</a>&gt;:<br>&gt;&gt;&gt; Hello!<br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; I have Open MPI v1.8.2rc4r32485<br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; When i run hello_c, I got this error message<br>&gt;&gt;&gt; $mpirun -np 2 hello_c<br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; An ORTE daemon has unexpectedly failed after launch and before<br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; communicating back to mpirun. This could be caused by a number<br>&gt;&gt;&gt; of factors, including an inability to create a connection back<br>&gt;&gt;&gt; to mpirun due to a lack of common network interfaces and/or no<br>&gt;&gt;&gt; route found between them. Please check network connectivity<br>&gt;&gt;&gt; (including firewalls and network routing requirements).<br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; When i run with --debug-daemons --mca plm_base_verbose 5 -mca oob_base_verbose 10 -mca rml_base_verbose 10 i got this output:<br>&gt;&gt;&gt; $mpirun --debug-daemons --mca plm_base_verbose 5 -mca oob_base_verbose 10 -mca rml_base_verbose 10 -np 2 hello_c<br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; [compiler-2:08780] mca:base:select:( plm) Querying component [isolated]<br>&gt;&gt;&gt; [compiler-2:08780] mca:base:select:( plm) Query of component [isolated] set priority to 0<br>&gt;&gt;&gt; [compiler-2:08780] mca:base:select:( plm) Querying component [rsh]<br>&gt;&gt;&gt; [compiler-2:08780] mca:base:select:( plm) Query of component [rsh] set priority to 10<br>&gt;&gt;&gt; [compiler-2:08780] mca:base:select:( plm) Querying component [slurm]<br>&gt;&gt;&gt; [compiler-2:08780] mca:base:select:( plm) Query of component [slurm] set priority to 75<br>&gt;&gt;&gt; [compiler-2:08780] mca:base:select:( plm) Selected component [slurm]<br>&gt;&gt;&gt; [compiler-2:08780] mca: base: components_register: registering oob components<br>&gt;&gt;&gt; [compiler-2:08780] mca: base: components_register: found loaded component tcp<br>&gt;&gt;&gt; [compiler-2:08780] mca: base: components_register: component tcp register function successful<br>&gt;&gt;&gt; [compiler-2:08780] mca: base: components_open: opening oob components<br>&gt;&gt;&gt; [compiler-2:08780] mca: base: components_open: found loaded component tcp<br>&gt;&gt;&gt; [compiler-2:08780] mca: base: components_open: component tcp open function successful<br>&gt;&gt;&gt; [compiler-2:08780] mca:oob:select: checking available component tcp<br>&gt;&gt;&gt; [compiler-2:08780] mca:oob:select: Querying component [tcp]<br>&gt;&gt;&gt; [compiler-2:08780] oob:tcp: component_available called<br>&gt;&gt;&gt; [compiler-2:08780] WORKING INTERFACE 1 KERNEL INDEX 1 FAMILY: V4<br>&gt;&gt;&gt; [compiler-2:08780] WORKING INTERFACE 2 KERNEL INDEX 3 FAMILY: V4<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] oob:tcp:init adding 10.0.251.53 to our list of V4 connections<br>&gt;&gt;&gt; [compiler-2:08780] WORKING INTERFACE 3 KERNEL INDEX 4 FAMILY: V4<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] oob:tcp:init adding 10.0.0.4 to our list of V4 connections<br>&gt;&gt;&gt; [compiler-2:08780] WORKING INTERFACE 4 KERNEL INDEX 5 FAMILY: V4<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] oob:tcp:init adding 10.2.251.14 to our list of V4 connections<br>&gt;&gt;&gt; [compiler-2:08780] WORKING INTERFACE 5 KERNEL INDEX 6 FAMILY: V4<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] oob:tcp:init adding 10.128.0.4 to our list of V4 connections<br>&gt;&gt;&gt; [compiler-2:08780] WORKING INTERFACE 6 KERNEL INDEX 7 FAMILY: V4<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] oob:tcp:init adding 93.180.7.38 to our list of V4 connections<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] TCP STARTUP<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] attempting to bind to IPv4 port 0<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] assigned IPv4 port 38420<br>&gt;&gt;&gt; [compiler-2:08780] mca:oob:select: Adding component to end<br>&gt;&gt;&gt; [compiler-2:08780] mca:oob:select: Found 1 active transports<br>&gt;&gt;&gt; [compiler-2:08780] mca: base: components_register: registering rml components<br>&gt;&gt;&gt; [compiler-2:08780] mca: base: components_register: found loaded component oob<br>&gt;&gt;&gt; [compiler-2:08780] mca: base: components_register: component oob has no register or open function<br>&gt;&gt;&gt; [compiler-2:08780] mca: base: components_open: opening rml components<br>&gt;&gt;&gt; [compiler-2:08780] mca: base: components_open: found loaded component oob<br>&gt;&gt;&gt; [compiler-2:08780] mca: base: components_open: component oob open function successful<br>&gt;&gt;&gt; [compiler-2:08780] orte_rml_base_select: initializing rml component oob<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting persistent recv on tag 30 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting persistent recv on tag 15 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting persistent recv on tag 32 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting persistent recv on tag 33 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting persistent recv on tag 5 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting persistent recv on tag 10 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting persistent recv on tag 12 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting persistent recv on tag 9 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting persistent recv on tag 34 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting persistent recv on tag 2 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting persistent recv on tag 21 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting persistent recv on tag 22 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting persistent recv on tag 45 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting persistent recv on tag 46 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting persistent recv on tag 1 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting recv<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] posting persistent recv on tag 27 for peer [[WILDCARD],WILDCARD]<br>&gt;&gt;&gt; Daemon was launched on node1-130-08 - beginning to initialize<br>&gt;&gt;&gt; Daemon was launched on node1-130-03 - beginning to initialize<br>&gt;&gt;&gt; Daemon was launched on node1-130-05 - beginning to initialize<br>&gt;&gt;&gt; Daemon was launched on node1-130-02 - beginning to initialize<br>&gt;&gt;&gt; Daemon was launched on node1-130-01 - beginning to initialize<br>&gt;&gt;&gt; Daemon was launched on node1-130-04 - beginning to initialize<br>&gt;&gt;&gt; Daemon was launched on node1-130-07 - beginning to initialize<br>&gt;&gt;&gt; Daemon was launched on node1-130-06 - beginning to initialize<br>&gt;&gt;&gt; Daemon [[42202,0],3] checking in as pid 7178 on host node1-130-03<br>&gt;&gt;&gt; [node1-130-03:07178] [[42202,0],3] orted: up and running - waiting for commands!<br>&gt;&gt;&gt; Daemon [[42202,0],2] checking in as pid 13581 on host node1-130-02<br>&gt;&gt;&gt; [node1-130-02:13581] [[42202,0],2] orted: up and running - waiting for commands!<br>&gt;&gt;&gt; Daemon [[42202,0],1] checking in as pid 17220 on host node1-130-01<br>&gt;&gt;&gt; [node1-130-01:17220] [[42202,0],1] orted: up and running - waiting for commands!<br>&gt;&gt;&gt; Daemon [[42202,0],5] checking in as pid 6663 on host node1-130-05<br>&gt;&gt;&gt; [node1-130-05:06663] [[42202,0],5] orted: up and running - waiting for commands!<br>&gt;&gt;&gt; Daemon [[42202,0],8] checking in as pid 6683 on host node1-130-08<br>&gt;&gt;&gt; [node1-130-08:06683] [[42202,0],8] orted: up and running - waiting for commands!<br>&gt;&gt;&gt; Daemon [[42202,0],7] checking in as pid 7877 on host node1-130-07<br>&gt;&gt;&gt; [node1-130-07:07877] [[42202,0],7] orted: up and running - waiting for commands!<br>&gt;&gt;&gt; Daemon [[42202,0],4] checking in as pid 7735 on host node1-130-04<br>&gt;&gt;&gt; [node1-130-04:07735] [[42202,0],4] orted: up and running - waiting for commands!<br>&gt;&gt;&gt; Daemon [[42202,0],6] checking in as pid 8451 on host node1-130-06<br>&gt;&gt;&gt; [node1-130-06:08451] [[42202,0],6] orted: up and running - waiting for commands!<br>&gt;&gt;&gt; srun: error: node1-130-03: task 2: Exited with exit code 1<br>&gt;&gt;&gt; srun: Terminating job step 657040.1<br>&gt;&gt;&gt; srun: error: node1-130-02: task 1: Exited with exit code 1<br>&gt;&gt;&gt; slurmd[node1-130-04]: *** STEP 657040.1 KILLED AT 2014-08-12T12:59:07 WITH SIGNAL 9 ***<br>&gt;&gt;&gt; slurmd[node1-130-07]: *** STEP 657040.1 KILLED AT 2014-08-12T12:59:07 WITH SIGNAL 9 ***<br>&gt;&gt;&gt; slurmd[node1-130-06]: *** STEP 657040.1 KILLED AT 2014-08-12T12:59:07 WITH SIGNAL 9 ***<br>&gt;&gt;&gt; srun: Job step aborted: Waiting up to 2 seconds for job step to finish.<br>&gt;&gt;&gt; srun: error: node1-130-01: task 0: Exited with exit code 1<br>&gt;&gt;&gt; srun: error: node1-130-05: task 4: Exited with exit code 1<br>&gt;&gt;&gt; srun: error: node1-130-08: task 7: Exited with exit code 1<br>&gt;&gt;&gt; srun: error: node1-130-07: task 6: Exited with exit code 1<br>&gt;&gt;&gt; srun: error: node1-130-04: task 3: Killed<br>&gt;&gt;&gt; srun: error: node1-130-06: task 5: Killed<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; An ORTE daemon has unexpectedly failed after launch and before<br>&gt;&gt;&gt; communicating back to mpirun. This could be caused by a number<br>&gt;&gt;&gt; of factors, including an inability to create a connection back<br>&gt;&gt;&gt; to mpirun due to a lack of common network interfaces and/or no<br>&gt;&gt;&gt; route found between them. Please check network connectivity<br>&gt;&gt;&gt; (including firewalls and network routing requirements).<br>&gt;&gt;&gt; --------------------------------------------------------------------------<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] orted_cmd: received halt_vm cmd<br>&gt;&gt;&gt; [compiler-2:08780] mca: base: close: component oob closed<br>&gt;&gt;&gt; [compiler-2:08780] mca: base: close: unloading component oob<br>&gt;&gt;&gt; [compiler-2:08780] [[42202,0],0] TCP SHUTDOWN<br>&gt;&gt;&gt; [compiler-2:08780] mca: base: close: component tcp closed<br>&gt;&gt;&gt; [compiler-2:08780] mca: base: close: unloading component tcp<br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; _______________________________________________<br>&gt;&gt;&gt; users mailing list<br>&gt;&gt;&gt;<span>&nbsp;</span><a>users@open-mpi.org</a><br>&gt;&gt;&gt; Subscription:<span>&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;&gt;&gt; Link to this post:<span>&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/08/24987.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/24987.php</a><br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;&gt; _______________________________________________<br>&gt;&gt;&gt; users mailing list<br>&gt;&gt;&gt;<span>&nbsp;</span><a>users@open-mpi.org</a><br>&gt;&gt;&gt; Subscription:<span>&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;&gt;&gt; Link to this post:<span>&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/08/24988.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/24988.php</a><br>&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;<span>&nbsp;</span><br>&gt;&gt; --<span>&nbsp;</span><br>&gt;&gt; Jeff Squyres<br>&gt;&gt;<span>&nbsp;</span><a>jsquyres@cisco.com</a><br>&gt;&gt; For corporate legal information go to:<span>&nbsp;</span><a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;<span>&nbsp;</span><br>&gt;&gt;<span>&nbsp;</span><br>&gt;<span>&nbsp;</span><br>&gt;<span>&nbsp;</span><br>&gt; --<span>&nbsp;</span><br>&gt; Jeff Squyres<br>&gt;<span>&nbsp;</span><a>jsquyres@cisco.com</a><br>&gt; For corporate legal information go to:<span>&nbsp;</span><a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>&gt;<span>&nbsp;</span><br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt;<span>&nbsp;</span><a>users@open-mpi.org</a><br>&gt; Subscription:<span>&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; Link to this post:<span>&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/08/25001.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25001.php</a><br><br><br>--<span>&nbsp;</span><br>Jeff Squyres<br><a>jsquyres@cisco.com</a><br>For corporate legal information go to:<span>&nbsp;</span><a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br><br><br></blockquote><br><br><br>_______________________________________________<br>users mailing list<br><a>users@open-mpi.org</a><br>Subscription:<span>&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div>Link to this post:<span>&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/08/25086.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25086.php</a></div></blockquote></div><br></div></div><br>_______________________________________________<br>users mailing list<br><a>users@open-mpi.org</a><br>Subscription:<span>&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post:<span>&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/08/25093.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25093.php</a><br></blockquote></div><br></div>_______________________________________________<br>users mailing list<br><a>users@open-mpi.org</a><br>Subscription:<span>&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post:<span>&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/08/25094.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25094.php</a></blockquote></div><br></div></div><div>_______________________________________________<br>users mailing list<br><a>users@open-mpi.org</a><br>Subscription:<span>&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post:<span>&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/08/25095.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25095.php</a><br></div></blockquote><br><br><br>_______________________________________________<br>users mailing list<br><a target="_blank">users@open-mpi.org</a><br>Subscription:<span>&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post:<span>&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/08/25105.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25105.php</a></div></blockquote></div><br></div></blockquote><br><br><br></div><br>_______________________________________________<br>users mailing list<br><a target="_blank">users@open-mpi.org</a><br>Subscription:<span>&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post:<span>&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/08/25127.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25127.php</a><br></blockquote></div><br><br clear="all"><div><br></div>--<span>&nbsp;</span><br><div dir="ltr"><br><div>Kind Regards,</div><div><br></div><div>M.</div></div></div>_______________________________________________<br>users mailing list<br><a target="_blank">users@open-mpi.org</a><br>Subscription:<span>&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post:<span>&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/08/25128.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25128.php</a></blockquote></div><br></div></div><div>_______________________________________________<br>users mailing list<br><a target="_blank">users@open-mpi.org</a><br>Subscription:<span>&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post:<span>&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/08/25129.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25129.php</a><br></div></blockquote><br><br></div></blockquote></div><br></div></blockquote><br><br><br></div><br><hr><br><br>_______________________________________________<br>users mailing list<br><a target="_blank">users@open-mpi.org</a><br>Subscription:<span>&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post:<span>&nbsp;</span><a href="http://www.open-mpi.org/community/lists/users/2014/08/25154.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/25154.php</a></div></blockquote></div><br></div></div></div></div></blockquote><br><br></div></blockquote></div><br></div>
</div>
			
		
		<base target="_self" href="https://e.mail.ru/">
	</div>

	
</div>


</div>
</blockquote>
<br>
<br><br></BODY></HTML>
