Hi George,<br>  That was an informative mail. Thanks.<br>  <br>  &gt;Open MPI is MPI 2 compliant, therefore it support dynamic processes.  <br>  &gt;The is a FAQ on the web site on how to do it.<br>  <br>  Does it also mean that openMPI is tolearnt to process or node failures?<br>  Then in such a case what is the solution?? Restart the job ??<br>  <br>  Thanks,<br>  Imran <br><br><b><i>George Bosilca &lt;bosilca@cs.utk.edu&gt;</i></b> wrote:<blockquote class="replbq" style="border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;">  <br>On Sep 15, 2006, at 10:36 AM, imran shaik wrote:<br><br>&gt; Can you elaborate on this.?<br>&gt; I have few doubts as well:<br>&gt; 1) OpenMPI runtime supports SGE?? Does it uses SGE instead of MPI  <br>&gt; runtime when it finds SGE running??<br><br>It's a difficult question if you expect an answer describing the deep  <br>internals of the Open MPI implementation. Let's say from a high level  <br>point of view that the
 MPI runtime detect SGE and use it in order to  <br>start the MPI job.<br><br>&gt; 2) Is it possible to check point and run MPI jobs?<br><br>Not with the released version. It's still work in progress.  <br>Eventually it will be one of the features of Open MPI but not before  <br>SC2006.<br><br>&gt; 3) Is it possible to add and remove processes dynamically from the  <br>&gt; MPI communicator?<br><br>Open MPI is MPI 2 compliant, therefore it support dynamic processes.  <br>The is a FAQ on the web site on how to do it.<br><br>&gt; 5) When do we actually need many different communicators?<br><br>It depend on what you plan to do. Usually, from the programmer point  <br>of view using multiple communicators make the code more readable as  <br>they allow you to have a logic view of the messages in transit. But  <br>it is not a requirement. One can write a million lines of code MPI  <br>application and only use the MPI_COMM_WORLD.<br><br>&gt; 4) Is MPI only suitable for low latency
 communication in  a cluster  <br>&gt; environment?<br><br>MPI was designed as a programming paradigm. It allow expressing  <br>parallel algorithms based on communications between peers. These  <br>communications can be point-to-point or collectives. The goal is  <br>wider than just low latency communications, as the standard allow you  <br>[as an example] to describe the memory layout of the data that get  <br>involved in the communication. The MPI forum have the full  <br>documentation about all the features of the MPI 2 standard.<br><br>   george.<br><br>&gt;<br>&gt;<br>&gt;<br>&gt;<br>&gt;<br>&gt; Ralph H Castain <rhc @lanl.gov=""> wrote: I can't speak to the Perl  <br>&gt; bindings, but Open MPI's runtime already supports<br>&gt; SGE, so all you have to do is "mpirun" like usual and we take care  <br>&gt; of the<br>&gt; rest. You may have to check your version of Open MPI as this  <br>&gt; capability was<br>&gt; added in the more recent releases.<br>&gt;<br>&gt;
 Ralph<br>&gt;<br>&gt;<br>&gt; On 9/13/06 8:52 AM, "Renato Golin" wrote:<br>&gt;<br>&gt; &gt; On 9/13/06, imran shaik wrote:<br>&gt; &gt;&gt; I need to run parallel jobs on a cluster typically of size 600  <br>&gt; nodes and<br>&gt; &gt;&gt; running SGE, but the programmers are good at perl but not C or C+ <br>&gt; +. So i<br>&gt; &gt;&gt; thought of MPI, but i dont know whether it has perl support?<br>&gt; &gt;<br>&gt; &gt; Hi Imran,<br>&gt; &gt;<br>&gt; &gt; SGE will dispatch process among the nodes of your cluster but it  <br>&gt; does<br>&gt; &gt; not support interprocess communication, which MPI does. If your<br>&gt; &gt; problem is easily splittable (like parse a large apache log, read a<br>&gt; &gt; large xml list of things) you might be able to split the data and<br>&gt; &gt; spawn as many process as you can.<br>&gt; &gt;<br>&gt; &gt; I do it using LSF (another dispatcher) and a Makefile that controls<br>&gt; &gt; the dependencies and spawn the processes (using
 make's -j flag)  <br>&gt; and it<br>&gt; &gt; works quite well. But if your job need the communication (like<br>&gt; &gt; processing big matrices, collecting and distributing data among<br>&gt; &gt; processes etc) you'll need an interprocess communication and that's<br>&gt; &gt; what MPI is best at.<br>&gt; &gt;<br>&gt; &gt; In a nutshell, you'll need the runtime environment to run MPI  <br>&gt; programs<br>&gt; &gt; as well as you need SGE's runtime environments on every node to<br>&gt; &gt; dispatch jobs and collect information.<br>&gt; &gt;<br>&gt; &gt; About MPI bindings for Perl, there's this module:<br>&gt; &gt; http://search.cpan.org/~josh/Parallel-MPI-0.03/MPI.pm<br>&gt; &gt;<br>&gt; &gt; but it's far too young to be trustworthy, IMHO, and you'll probably<br>&gt; &gt; need the MPI runtime on all nodes as well...<br>&gt; &gt;<br>&gt; &gt; cheers,<br>&gt; &gt; --renato<br>&gt; &gt; _______________________________________________<br>&gt; &gt; users mailing
 list<br>&gt; &gt; users@open-mpi.org<br>&gt; &gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt;<br>&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; users@open-mpi.org<br>&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br>&gt;<br>&gt;<br>&gt; Talk is cheap. Use Yahoo! Messenger to make PC-to-Phone calls.  <br>&gt; Great rates starting at 1¢/min.<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; users@open-mpi.org<br>&gt; http://www.open-mpi.org/mailman/listinfo.cgi/users<br><br><br>_______________________________________________<br>users mailing list<br>users@open-mpi.org<br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></rhc></blockquote><br><p>&#32;
	
		<hr size=1>Get your own <a href=" http://us.rd.yahoo.com/evt=43290/*http://smallbusiness.yahoo.com/domains"
>web address for just $1.99/1st yr</a>. We'll help. <a href="http://us.rd.yahoo.com/evt=41244/*http://smallbusiness.yahoo.com/"
>Yahoo! Small Business</a>.

