<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">I can't replicate that behavior - it all seems to be working just fine. I can launch apps of different name, we correctly detect and respond to missing executables, etc.<div><br></div><div>Can you provide more info as to how this was built? Also, be sure to check that the remote hosts are using the same version of OMPI - hangs are typically a good indicator that the remote node is picking up a different OMPI version.</div><div><br></div><div>Ralph</div><div><br></div><div><br><div><div>On Jan 22, 2009, at 5:42 AM, Geoffroy Pignot wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite">Hello , still a bug ???<br><br>compil03% /tmp/openmpi-1.3/bin/mpirun -n 1 --wdir /tmp --host compil03 a.out : -n 1 --host compil02 a.out<br>Hello world from process 0 of 2<br> Hello world from process 1 of 2<br><br>compil03% mv a.out a.out_32<br>compil03% /tmp/openmpi-1.3/bin/mpirun -n 1 --wdir /tmp --host compil03 a.out_32 : -n 1 --host compil02 a.out<br>HANGS<br><br>Thanks in advance for your expertise<br> <br>Geoffroy<br><br><br><br><br><br><div class="gmail_quote">2009/1/22  <span dir="ltr">&lt;<a href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a>></span><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"> Send users mailing list submissions to<br> &nbsp; &nbsp; &nbsp; &nbsp;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br> <br> To subscribe or unsubscribe via the World Wide Web, visit<br> &nbsp; &nbsp; &nbsp; &nbsp;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> or, via email, send a message with subject or body 'help' to<br> &nbsp; &nbsp; &nbsp; &nbsp;<a href="mailto:users-request@open-mpi.org">users-request@open-mpi.org</a><br> <br> You can reach the person managing the list at<br> &nbsp; &nbsp; &nbsp; &nbsp;<a href="mailto:users-owner@open-mpi.org">users-owner@open-mpi.org</a><br> <br> When replying, please edit your Subject line so it is more specific<br> than "Re: Contents of users digest..."<br> <br> <br> Today's Topics:<br> <br> &nbsp; 1. One additional (unwanted) process when using dynamical<br> &nbsp; &nbsp; &nbsp;process management (Evgeniy Gromov)<br> &nbsp; 2. Re: One additional (unwanted) process when using &nbsp;dynamical<br> &nbsp; &nbsp; &nbsp;process management (Ralph Castain)<br> &nbsp; 3. Re: One additional (unwanted) process when using &nbsp;dynamical<br> &nbsp; &nbsp; &nbsp;process management (Evgeniy Gromov)<br> &nbsp; 4. Re: One additional (unwanted) process when &nbsp; &nbsp; &nbsp; &nbsp;using &nbsp; dynamical<br> &nbsp; &nbsp; &nbsp;process management (Ralph Castain)<br> &nbsp; 5. Re: openmpi 1.3 and --wdir problem (Ralph Castain)<br> &nbsp; 6. Re: Problem compiling open mpi 1.3 with sunstudio12 &nbsp; &nbsp; &nbsp; express<br> &nbsp; &nbsp; &nbsp;(Jeff Squyres)<br> &nbsp; 7. Handling output of processes (jody)<br> <br> <br> ----------------------------------------------------------------------<br> <br> Message: 1<br> Date: Wed, 21 Jan 2009 19:02:48 +0100<br> From: Evgeniy Gromov &lt;<a href="mailto:Evgeniy.Gromov@pci.uni-heidelberg.de">Evgeniy.Gromov@pci.uni-heidelberg.de</a>><br> Subject: [OMPI users] One additional (unwanted) process when using<br> &nbsp; &nbsp; &nbsp; &nbsp;dynamical &nbsp; &nbsp; &nbsp; process management<br> To: <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br> Message-ID: &lt;<a href="mailto:49776348.8000900@pci.uni-heidelberg.de">49776348.8000900@pci.uni-heidelberg.de</a>><br> Content-Type: text/plain; charset=ISO-8859-1; format=flowed<br> <br> Dear OpenMPI users,<br> <br> I have the following (problem) related to OpenMPI:<br> I have recently compiled with OPenMPI the new (4-1)<br> Global Arrays package using ARMCI_NETWORK=MPI-SPAWN,<br> which implies the use of dynamic process management<br> realised in MPI2. It got compiled and tested successfully.<br> However when it is spawning on different nodes (machine) one<br> additional process on each node appears, i.e. if nodes=2:ppn=2<br> then on each node there are 3 running processes. In the case<br> when it runs just on one pc with a few cores (let say nodes=1:ppn=4),<br> the number of processes exactly equals the number of cpus (ppn)<br> requested and there is no additional process.<br> I am wondering whether it is normal behavior. Thanks!<br> <br> Best regards,<br> Evgeniy<br> <br> <br> <br> <br> <br> ------------------------------<br> <br> Message: 2<br> Date: Wed, 21 Jan 2009 11:15:00 -0700<br> From: Ralph Castain &lt;<a href="mailto:rhc@lanl.gov">rhc@lanl.gov</a>><br> Subject: Re: [OMPI users] One additional (unwanted) process when using<br> &nbsp; &nbsp; &nbsp; &nbsp;dynamical process management<br> To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>><br> Message-ID: &lt;<a href="mailto:4CCBD3F8-937F-4F8B-B953-F9CF9DD45EF5@lanl.gov">4CCBD3F8-937F-4F8B-B953-F9CF9DD45EF5@lanl.gov</a>><br> Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes<br> <br> Not that I've seen. What version of OMPI are you using, and on what<br> type of machine/environment?<br> <br> <br> On Jan 21, 2009, at 11:02 AM, Evgeniy Gromov wrote:<br> <br> > Dear OpenMPI users,<br> ><br> > I have the following (problem) related to OpenMPI:<br> > I have recently compiled with OPenMPI the new (4-1)<br> > Global Arrays package using ARMCI_NETWORK=MPI-SPAWN,<br> > which implies the use of dynamic process management<br> > realised in MPI2. It got compiled and tested successfully.<br> > However when it is spawning on different nodes (machine) one<br> > additional process on each node appears, i.e. if nodes=2:ppn=2<br> > then on each node there are 3 running processes. In the case<br> > when it runs just on one pc with a few cores (let say nodes=1:ppn=4),<br> > the number of processes exactly equals the number of cpus (ppn)<br> > requested and there is no additional process.<br> > I am wondering whether it is normal behavior. Thanks!<br> ><br> > Best regards,<br> > Evgeniy<br> ><br> ><br> ><br> > _______________________________________________<br> > users mailing list<br> > <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br> > <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> <br> <br> <br> ------------------------------<br> <br> Message: 3<br> Date: Wed, 21 Jan 2009 19:30:27 +0100<br> From: Evgeniy Gromov &lt;<a href="mailto:Evgeniy.Gromov@pci.uni-heidelberg.de">Evgeniy.Gromov@pci.uni-heidelberg.de</a>><br> Subject: Re: [OMPI users] One additional (unwanted) process when using<br> &nbsp; &nbsp; &nbsp; &nbsp;dynamical process management<br> To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>><br> Message-ID: &lt;<a href="mailto:497769C3.8070201@pci.uni-heidelberg.de">497769C3.8070201@pci.uni-heidelberg.de</a>><br> Content-Type: text/plain; charset=ISO-8859-1; format=flowed<br> <br> Dear Ralph,<br> <br> Thanks for your reply.<br> I encountered this problem using openmpi-1.2.5,<br> on a Opteron cluster with Myrinet-mx. I tried for<br> compilation of Global Arrays different compilers<br> (gfortran, intel, pathscale), the result is the same.<br> <br> As I mentioned in the previous message GA itself works<br> fine, but the application which uses it doesn't work<br> correctly if it runs on several nodes. If it runs on<br> one node with several cores everything is fine. So I<br> thought that the problem might be in this additional<br> process.<br> <br> Should I try to use the latest 1.3 version of openmpi?<br> <br> Best,<br> Evgeniy<br> <br> Ralph Castain wrote:<br> > Not that I've seen. What version of OMPI are you using, and on what type<br> > of machine/environment?<br> ><br> ><br> > On Jan 21, 2009, at 11:02 AM, Evgeniy Gromov wrote:<br> ><br> >> Dear OpenMPI users,<br> >><br> >> I have the following (problem) related to OpenMPI:<br> >> I have recently compiled with OPenMPI the new (4-1)<br> >> Global Arrays package using ARMCI_NETWORK=MPI-SPAWN,<br> >> which implies the use of dynamic process management<br> >> realised in MPI2. It got compiled and tested successfully.<br> >> However when it is spawning on different nodes (machine) one<br> >> additional process on each node appears, i.e. if nodes=2:ppn=2<br> >> then on each node there are 3 running processes. In the case<br> >> when it runs just on one pc with a few cores (let say nodes=1:ppn=4),<br> >> the number of processes exactly equals the number of cpus (ppn)<br> >> requested and there is no additional process.<br> >> I am wondering whether it is normal behavior. Thanks!<br> >><br> >> Best regards,<br> >> Evgeniy<br> >><br> >><br> >><br> >> _______________________________________________<br> >> users mailing list<br> >> <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br> >> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> ><br> > _______________________________________________<br> > users mailing list<br> > <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br> > <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> ><br> <br> <br> --<br> _______________________________________<br> Dr. Evgeniy Gromov<br> Theoretische Chemie<br> Physikalisch-Chemisches Institut<br> Im Neuenheimer Feld 229<br> D-69120 Heidelberg<br> Germany<br> <br> Telefon: +49/(0)6221/545263<br> Fax: +49/(0)6221/545221<br> E-mail: <a href="mailto:evgeniy@pci.uni-heidelberg.de">evgeniy@pci.uni-heidelberg.de</a><br> _______________________________________<br> <br> <br> <br> <br> <br> ------------------------------<br> <br> Message: 4<br> Date: Wed, 21 Jan 2009 11:38:48 -0700<br> From: Ralph Castain &lt;<a href="mailto:rhc@lanl.gov">rhc@lanl.gov</a>><br> Subject: Re: [OMPI users] One additional (unwanted) process when &nbsp; &nbsp; &nbsp; &nbsp;using<br> &nbsp; &nbsp; &nbsp; &nbsp;dynamical process management<br> To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>><br> Message-ID: &lt;<a href="mailto:75C59577-D1EA-422B-A0B9-7F1C28E8D4CF@lanl.gov">75C59577-D1EA-422B-A0B9-7F1C28E8D4CF@lanl.gov</a>><br> Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes<br> <br> If you can, 1.3 would certainly be a good step to take. I'm not sure<br> why 1.2.5 would be behaving this way, though, so it may indeed be<br> something in the application (perhaps in the info key being passed to<br> us?) that is the root cause.<br> <br> Still, if it isn't too much trouble, moving to 1.3 will provide you<br> with a better platform for dynamic process management regardless.<br> <br> Ralph<br> <br> <br> On Jan 21, 2009, at 11:30 AM, Evgeniy Gromov wrote:<br> <br> > Dear Ralph,<br> ><br> > Thanks for your reply.<br> > I encountered this problem using openmpi-1.2.5,<br> > on a Opteron cluster with Myrinet-mx. I tried for<br> > compilation of Global Arrays different compilers<br> > (gfortran, intel, pathscale), the result is the same.<br> ><br> > As I mentioned in the previous message GA itself works<br> > fine, but the application which uses it doesn't work<br> > correctly if it runs on several nodes. If it runs on<br> > one node with several cores everything is fine. So I<br> > thought that the problem might be in this additional<br> > process.<br> ><br> > Should I try to use the latest 1.3 version of openmpi?<br> ><br> > Best,<br> > Evgeniy<br> ><br> > Ralph Castain wrote:<br> >> Not that I've seen. What version of OMPI are you using, and on what<br> >> type of machine/environment?<br> >> On Jan 21, 2009, at 11:02 AM, Evgeniy Gromov wrote:<br> >>> Dear OpenMPI users,<br> >>><br> >>> I have the following (problem) related to OpenMPI:<br> >>> I have recently compiled with OPenMPI the new (4-1)<br> >>> Global Arrays package using ARMCI_NETWORK=MPI-SPAWN,<br> >>> which implies the use of dynamic process management<br> >>> realised in MPI2. It got compiled and tested successfully.<br> >>> However when it is spawning on different nodes (machine) one<br> >>> additional process on each node appears, i.e. if nodes=2:ppn=2<br> >>> then on each node there are 3 running processes. In the case<br> >>> when it runs just on one pc with a few cores (let say<br> >>> nodes=1:ppn=4),<br> >>> the number of processes exactly equals the number of cpus (ppn)<br> >>> requested and there is no additional process.<br> >>> I am wondering whether it is normal behavior. Thanks!<br> >>><br> >>> Best regards,<br> >>> Evgeniy<br> >>><br> >>><br> >>><br> >>> _______________________________________________<br> >>> users mailing list<br> >>> <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br> >>> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> >> _______________________________________________<br> >> users mailing list<br> >> <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br> >> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> ><br> ><br> > --<br> > _______________________________________<br> > Dr. Evgeniy Gromov<br> > Theoretische Chemie<br> > Physikalisch-Chemisches Institut<br> > Im Neuenheimer Feld 229<br> > D-69120 Heidelberg<br> > Germany<br> ><br> > Telefon: +49/(0)6221/545263<br> > Fax: +49/(0)6221/545221<br> > E-mail: <a href="mailto:evgeniy@pci.uni-heidelberg.de">evgeniy@pci.uni-heidelberg.de</a><br> > _______________________________________<br> ><br> ><br> ><br> > _______________________________________________<br> > users mailing list<br> > <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br> > <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> <br> <br> <br> ------------------------------<br> <br> Message: 5<br> Date: Wed, 21 Jan 2009 11:40:28 -0700<br> From: Ralph Castain &lt;<a href="mailto:rhc@lanl.gov">rhc@lanl.gov</a>><br> Subject: Re: [OMPI users] openmpi 1.3 and --wdir problem<br> To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>><br> Message-ID: &lt;<a href="mailto:B57EA438-1C8A-467C-B791-96EABE6031F4@lanl.gov">B57EA438-1C8A-467C-B791-96EABE6031F4@lanl.gov</a>><br> Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes<br> <br> This is now fixed in the trunk and will be in the 1.3.1 release.<br> <br> Thanks again for the heads-up!<br> Ralph<br> <br> On Jan 21, 2009, at 8:45 AM, Ralph Castain wrote:<br> <br> > You are correct - that is a bug in 1.3.0. I'm working on a fix for<br> > it now and will report back.<br> ><br> > Thanks for catching it!<br> > Ralph<br> ><br> ><br> > On Jan 21, 2009, at 3:22 AM, Geoffroy Pignot wrote:<br> ><br> >> Hello<br> >><br> >> &nbsp; I'm currently trying the new release but I cant reproduce the<br> >> 1.2.8 behaviour<br> >> &nbsp; concerning --wdir option<br> >><br> >> &nbsp; Then<br> >> &nbsp; %% /tmp/openmpi-1.2.8/bin/mpirun -n 1 --wdir /tmp --host r003n030<br> >> pwd : &nbsp; --wdir /scr1 -n 1 --host r003n031 pwd<br> >> &nbsp; /scr1<br> >> &nbsp; /tmp<br> >><br> >> &nbsp; but<br> >> &nbsp; %% /tmp/openmpi-1.3/bin/mpirun -n 1 --wdir /tmp --host r003n030<br> >> pwd : --wdir &nbsp;/scr1 -n 1 --host r003n031 pwd<br> >> &nbsp; /scr1<br> >> &nbsp; /scr1<br> >> &nbsp; Thanks in advance<br> >> &nbsp; Regards<br> >> &nbsp; Geoffroy<br> >><br> >> _______________________________________________<br> >> users mailing list<br> >> <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br> >> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> ><br> > _______________________________________________<br> > users mailing list<br> > <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br> > <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> <br> <br> <br> ------------------------------<br> <br> Message: 6<br> Date: Wed, 21 Jan 2009 14:06:42 -0500<br> From: Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>><br> Subject: Re: [OMPI users] Problem compiling open mpi 1.3 with<br> &nbsp; &nbsp; &nbsp; &nbsp;sunstudio12 &nbsp; &nbsp; express<br> To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>><br> Message-ID: &lt;<a href="mailto:36FCDF58-9138-46A9-A432-CDF2A99A1CD7@cisco.com">36FCDF58-9138-46A9-A432-CDF2A99A1CD7@cisco.com</a>><br> Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes<br> <br> FWIW, I have run with my LD_LIBRARY_PATH set to a combination of<br> multiple OMPI installations; it ended up using the leftmost entry in<br> the LD_LIBRARY_PATH (as I intended). &nbsp;I'm not quite sure why it<br> wouldn't do that for you. &nbsp;:-(<br> <br> <br> On Jan 21, 2009, at 4:53 AM, Olivier Marsden wrote:<br> <br> ><br> >><br> >> - Check that /opt/mpi_sun and /opt/mpi_gfortran* are actually<br> >> distinct subdirectories; there's no hidden sym/hard links in there<br> >> somewhere (where directories and/or individual files might<br> >> accidentally be pointing to the other tree)<br> >><br> ><br> > no hidden links in the directories<br> ><br> >> - does "env | grep mpi_" show anything interesting / revealing?<br> >> What is your LD_LIBRARY_PATH set to?<br> >><br> > Nothing in env | grep mpi, &nbsp;and for the purposes of building,<br> > LD_LIBRARY_PATH is set to<br> > /opt/sun/express/sunstudioceres/lib/:/opt/mpi_sun/lib:xxx<br> > where xxx is, among other things, the other mpi installations.<br> > This lead me to find a problem, but which seems to be more related<br> > to my linux configuration than openmpi:<br> > I tried redefining ld_library_path to point just to sun, and<br> > everything works correctly.<br> > Putting my previous paths back into the variable leads to erroneous<br> > behaviour, with ldd indicating that mpif90<br> > is linked to libraries in the gfortran tree.<br> > I thought that ld looked for libraries in folders in the order that<br> > the folders are given in ld_library_path, and so<br> > having mpi_sun as the first folder would suffice for its libraries<br> > to be used; is that where I was wrong?<br> > Sorry for the trouble, in any case redefining the ld_library_path to<br> > remove all references to other installations works.<br> > Looks like I'll have to swot up on my linker configuration knowledge!<br> > Thanks alot for your time,<br> ><br> > Olivier Marsden<br> ><br> ><br> > _______________________________________________<br> > users mailing list<br> > <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br> > <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> <br> <br> --<br> Jeff Squyres<br> Cisco Systems<br> <br> <br> <br> ------------------------------<br> <br> Message: 7<br> Date: Thu, 22 Jan 2009 09:58:22 +0100<br> From: jody &lt;<a href="mailto:jody.xha@gmail.com">jody.xha@gmail.com</a>><br> Subject: [OMPI users] Handling output of processes<br> To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>><br> Message-ID:<br> &nbsp; &nbsp; &nbsp; &nbsp;&lt;<a href="mailto:9b0da5ce0901220058n6e534224i78a6daf6b0afc209@mail.gmail.com">9b0da5ce0901220058n6e534224i78a6daf6b0afc209@mail.gmail.com</a>><br> Content-Type: text/plain; charset=ISO-8859-1<br> <br> Hi<br> I have a small cluster consisting of 9 computers (8x2 CPUs, 1x4 CPUs).<br> I would like to be able to observe the output of the processes<br> separately during an mpirun.<br> <br> What i currently do is to apply the mpirun to a shell script which<br> opens a xterm for each process,<br> which then starts the actual application.<br> <br> This works, but is a bit complicated, e.g. finding the window you're<br> interested in among 19 others.<br> <br> So i was wondering is there a possibility to capture the processes'<br> outputs separately, so<br> i can make an application in which i can switch between the different<br> processor outputs?<br> I could imagine that could be done by wrapper applications which<br> redirect the output over a TCP<br> socket to a server application.<br> <br> But perhaps there is an easier way, or something like this alread does exist?<br> <br> Thank You<br> &nbsp;Jody<br> <br> <br> ------------------------------<br> <br> _______________________________________________<br> users mailing list<br> <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> <br> End of users Digest, Vol 1126, Issue 1<br> **************************************<br> </blockquote></div><br> _______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
