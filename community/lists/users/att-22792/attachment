<div dir="ltr"><div>Hi,</div><div><br></div><div>     As per your instruction, I did the profiling of the application with mpiP. Following is the difference between the two runs:</div><div><br></div><div>Run 1: 16 mpi processes on single node</div>
<div><br></div><div><div>@--- MPI Time (seconds) ---------------------------------------------------</div><div>---------------------------------------------------------------------------</div><div>Task    AppTime    MPITime     MPI%</div>
<div>   0   3.61e+03        661    18.32</div><div>   1   3.61e+03        627    17.37</div><div>   2   3.61e+03        700    19.39</div><div>   3   3.61e+03        665    18.41</div><div>   4   3.61e+03        702    19.45</div>
<div>   5   3.61e+03        703    19.48</div><div>   6   3.61e+03        740    20.50</div><div>   7   3.61e+03        763    21.14</div></div><div>...</div><div>...</div><div><br></div><div>Run 2: 16 mpi processes on two nodes - 8 mpi processes per node</div>
<div><br></div><div><div>@--- MPI Time (seconds) ---------------------------------------------------</div><div>---------------------------------------------------------------------------</div><div>Task    AppTime    MPITime     MPI%</div>
<div>   0   1.27e+04   1.06e+04    84.14</div><div>   1   1.27e+04   1.07e+04    84.34</div><div>   2   1.27e+04   1.07e+04    84.20</div><div>   3   1.27e+04   1.07e+04    84.20</div><div>   4   1.27e+04   1.07e+04    84.22</div>
<div>   5   1.27e+04   1.07e+04    84.25</div><div>   6   1.27e+04   1.06e+04    84.02</div><div>   7   1.27e+04   1.07e+04    84.35</div><div>   8   1.27e+04   1.07e+04    84.29</div></div><div><br></div><div><br></div><div>
          The time spent for MPI functions in run 1 is less than 20%, where as it is more than 80% in the run 2. For more details, I&#39;ve attached both output files. Please go thru these files and suggest what optimization we can do with OpenMPI or Intel MKL.</div>
<div><br></div><div>Thanks</div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Mon, Oct 7, 2013 at 12:15 PM, San B <span dir="ltr">&lt;<a href="mailto:forum.san@gmail.com" target="_blank">forum.san@gmail.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><div><p>Hi,<br></p><p>I&#39;m facing a  performance issue with a scientific 
application(Fortran). The issue is, it runs faster on single node but 
runs very slow on multiple nodes. For example, a 16 core job on single node
 finishes in 1hr 2mins, but the same job on two nodes (i.e. 8 cores per 
node &amp; remaining 8 cores kept free) takes 3hr 20mins. The code is 
compiled with ifort-13.1.1, openmpi-1.4.5 and intel MKL libraries - 
lapack, blas, scalapack, blacs &amp; fftw. What could be the problem 
here with?</p>Is it possible to do any tuning in OpenMPI? FY More info: The cluster has Intel Sandybridge processor (E5-2670), 
infiniband and Hyperthreading is Enabled. Jobs are submitted thru LSF 
scheduler.<br><br></div>Does HyperThreading causing any problem here?<br><br><br></div><div>Thanks</div></div>
</blockquote></div><br></div>

