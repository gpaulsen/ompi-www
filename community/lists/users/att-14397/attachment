
<br><font size=2 face="sans-serif">When you use MPI message passing in
your application, the MPI library decides how to deliver the message. The
&quot;magic&quot; is simply that when sender process and receiver process
are on the same node (shared memory domain) the library uses shared memory
to deliver the message from process to process. &nbsp;When the sender process
and receiver process are on different nodes, some interconnect method is
used.</font>
<br>
<br><font size=2 face="sans-serif">The MPI API does not have any explicit
recognition of shared memory. If you are thinking of the MPI 1sided when
you mention &quot;MPI-2 shared memory&quot;, we should be clear that MPI
1-sided communication is only vaguely similar to shared memory and only
provide access through MPI calls (MPI_Put, MPI_Get and MPI_Aaccumulate)
and does not magically created shared memory that you can load/store.</font>
<br>
<br>
<br><font size=2 face="sans-serif">Dick Treumann &nbsp;- &nbsp;MPI Team
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br>
IBM Systems &amp; Technology Group<br>
Dept X2ZA / MS P963 -- 2455 South Road -- Poughkeepsie, NY 12601<br>
Tele (845) 433-7846 &nbsp; &nbsp; &nbsp; &nbsp; Fax (845) 433-8363<br>
</font>
<br>
<br>
<br>
<table width=100%>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">From:</font>
<td><font size=1 face="sans-serif">Andrei Fokau &lt;andrei.fokau@neutron.kth.se&gt;</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">To:</font>
<td><font size=1 face="sans-serif">Open MPI Users &lt;users@open-mpi.org&gt;</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Date:</font>
<td><font size=1 face="sans-serif">10/06/2010 10:12 AM</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Subject:</font>
<td><font size=1 face="sans-serif">Re: [OMPI users] Shared memory</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Sent by:</font>
<td><font size=1 face="sans-serif">users-bounces@open-mpi.org</font></table>
<br>
<hr noshade>
<br>
<br>
<br><font size=3 face="Arial">Currently we run a code on a cluster with
distributed memory, and this code needs a lot of memory. Part of the data
stored in memory is the same for each process, but it is organized as one
array - we can split it if necessary. So far no magic&nbsp;occurred&nbsp;for
us. What do we need to do to make the magic working?</font><font size=3><br>
<br>
</font>
<br><font size=3>On Wed, Oct 6, 2010 at 12:43, Jeff Squyres (jsquyres)
&lt;</font><a href=mailto:jsquyres@cisco.com><font size=3 color=blue><u>jsquyres@cisco.com</u></font></a><font size=3>&gt;
wrote:</font>
<br><font size=3>Open MPI will use shared memory to communicate between
peers on the sane node - but that's hidden beneath the covers; it's not
exposed via the MPI API. You just MPI-send and magic occurs and the receiver
gets the message.&nbsp;<br>
</font>
<br><font size=3>On Oct 4, 2010, at 11:13 AM, &quot;Andrei Fokau&quot;
&lt;</font><a href=mailto:andrei.fokau@neutron.kth.se target=_blank><font size=3 color=blue><u>andrei.fokau@neutron.kth.se</u></font></a><font size=3>&gt;
wrote:</font>
<br><font size=3 face="Arial">Does OMPI have shared memory capabilities
(as it is mentioned in MPI-2)?</font>
<br><font size=3 face="Arial">How can I use them?</font><font size=3><br>
</font>
<br><font size=3>On Sat, Sep 25, 2010 at 23:19, Andrei Fokau &lt;</font><a href=mailto:andrei.fokau@neutron.kth.se target=_blank></a><a href=mailto:andrei.fokau@neutron.kth.se target=_blank><font size=3 color=blue><u>andrei.fokau@neutron.kth.se</u></font></a><font size=3>&gt;
wrote:</font>
<br><font size=3 face="Arial">Here are some more details about our problem.
We use a dozen of 4-processor nodes with 8 GB memory on each node. The
code we run needs about 3 GB per processor, so we can load only 2 processors
out of 4. The vast majority of those 3 GB is the same for each processor
and is accessed&nbsp;continuously during calculation. In my original question
I wasn't very clear asking about a possibility to use shared memory with
Open MPI - in our case we do not need to have a remote access to the data,
and&nbsp;it would be sufficient to share memory within each node only.</font>
<br>
<br><font size=3 face="Arial">Of course, the possibility to access the
data remotely (via mmap) is attractive because it would allow to store
much larger arrays (up to 10 GB) at one remote place, meaning higher accuracy
for our calculations. However, I believe that the access time would be
too long for the data read so frequently, and therefore the performance
would be lost.</font>
<br>
<br><font size=3 face="Arial">I still hope that some of the subscribers
to this mailing list have an experience of using Global Arrays. This library
seems to be fine for our case, however I feel that there should be a simpler
solution. Open MPI conforms with MPI-2 standard, and the later has a description
of shared memory application. Do you see any other way for us to use&nbsp;shared
memory (within node) apart of using Global Arrays?</font>
<br>
<br><font size=3>On Fri, Sep 24, 2010 at 19:03, Durga Choudhury &lt;</font><a href=mailto:dpchoudh@gmail.com target=_blank></a><a href=mailto:dpchoudh@gmail.com target=_blank><font size=3 color=blue><u>dpchoudh@gmail.com</u></font></a><font size=3>&gt;
wrote:</font>
<br><font size=3>I think the 'middle ground' approach can be simplified
even further if<br>
the data file is in a shared device (e.g. NFS/Samba mount) that can be<br>
mounted at the same location of the file system tree on all nodes. I<br>
have never tried it, though and mmap()'ing a non-POSIX compliant file<br>
system such as Samba might have issues I am unaware of.<br>
<br>
However, I do not see why you should not be able to do this even if<br>
the file is being written to as long as you call msync() before using<br>
the mapped pages.</font><font size=3 color=#8f8f8f><br>
<br>
Durga</font>
<br><font size=3><br>
<br>
On Fri, Sep 24, 2010 at 12:31 PM, Eugene Loh &lt;</font><a href=mailto:eugene.loh@oracle.com target=_blank></a><a href=mailto:eugene.loh@oracle.com target=_blank><font size=3 color=blue><u>eugene.loh@oracle.com</u></font></a><font size=3>&gt;
wrote:<br>
&gt; It seems to me there are two extremes.<br>
&gt;<br>
&gt; One is that you replicate the data for each process.&nbsp; This has
the<br>
&gt; disadvantage of consuming lots of memory &quot;unnecessarily.&quot;<br>
&gt;<br>
&gt; Another extreme is that shared data is distributed over all processes.&nbsp;
This<br>
&gt; has the disadvantage of making at least some of the data less accessible,<br>
&gt; whether in programming complexity and/or run-time performance.<br>
&gt;<br>
&gt; I'm not familiar with Global Arrays.&nbsp; I was somewhat familiar
with HPF.&nbsp; I<br>
&gt; think the natural thing to do with those programming models is to
distribute<br>
&gt; data over all processes, which may relieve the excessive memory consumption<br>
&gt; you're trying to address but which may also just put you at a different<br>
&gt; &quot;extreme&quot; of this spectrum.<br>
&gt;<br>
&gt; The middle ground I think might make most sense would be to share
data only<br>
&gt; within a node, but to replicate the data for each node.&nbsp; There
are probably<br>
&gt; multiple ways of doing this -- possibly even GA, I don't know.&nbsp;
One way<br>
&gt; might be to use one MPI process per node, with OMP multithreading
within<br>
&gt; each process|node.&nbsp; Or (and I thought this was the solution you
were looking<br>
&gt; for), have some idea which processes are collocal.&nbsp; Have one
process per<br>
&gt; node create and initialize some shared memory -- mmap, perhaps, or
SysV<br>
&gt; shared memory.&nbsp; Then, have its peers map the same shared memory
into their<br>
&gt; address spaces.<br>
&gt;<br>
&gt; You asked what source code changes would be required.&nbsp; It depends.&nbsp;
If<br>
&gt; you're going to mmap shared memory in on each node, you need to know
which<br>
&gt; processes are collocal.&nbsp; If you're willing to constrain how processes
are<br>
&gt; mapped to nodes, this could be easy.&nbsp; (E.g., &quot;every 4 processes
are<br>
&gt; collocal&quot;.)&nbsp; If you want to discover dynamically at run
time which are<br>
&gt; collocal, it would be harder.&nbsp; The mmap stuff could be in a stand-alone<br>
&gt; function of about a dozen lines.&nbsp; If the shared area is allocated
as one<br>
&gt; piece, substituting the single malloc() call with a call to your mmap<br>
&gt; function should be simple.&nbsp; If you have many malloc()s you're
trying to<br>
&gt; replace, it's harder.<br>
&gt;<br>
&gt; Andrei Fokau wrote:<br>
&gt;<br>
&gt; The data are read from a file and processed before calculations begin,
so I<br>
&gt; think that mapping will not work in our case.<br>
&gt; Global Arrays look promising indeed. As I said, we need to put just
a part<br>
&gt; of data to the shared section. John, do you (or may be other users)
have an<br>
&gt; experience of working with GA?<br>
&gt; </font><a href=http://www.emsl.pnl.gov/docs/global/um/build.html target=_blank></a><a href=http://www.emsl.pnl.gov/docs/global/um/build.html target=_blank><font size=3 color=blue><u>http://www.emsl.pnl.gov/docs/global/um/build.html</u></font></a><font size=3><br>
&gt; When GA runs with MPI:<br>
&gt; MPI_Init(..) &nbsp; &nbsp; &nbsp;! start MPI<br>
&gt; GA_Initialize() &nbsp; ! start global arrays<br>
&gt; MA_Init(..) &nbsp; &nbsp; &nbsp; ! start memory allocator<br>
&gt; &nbsp;&nbsp; .... do work<br>
&gt; GA_Terminate() &nbsp; &nbsp;! tidy up global arrays<br>
&gt; MPI_Finalize() &nbsp; &nbsp;! tidy up MPI<br>
&gt; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;!
exit program<br>
&gt; On Fri, Sep 24, 2010 at 13:44, Reuti &lt;</font><a href="mailto:reuti@staff.uni-marburg.de" target=_blank></a><a href="mailto:reuti@staff.uni-marburg.de" target=_blank><font size=3 color=blue><u>reuti@staff.uni-marburg.de</u></font></a><font size=3>&gt;
wrote:<br>
&gt;&gt;<br>
&gt;&gt; Am 24.09.2010 um 13:26 schrieb John Hearns:<br>
&gt;&gt;<br>
&gt;&gt; &gt; On 24 September 2010 08:46, Andrei Fokau &lt;</font><a href=mailto:andrei.fokau@neutron.kth.se target=_blank></a><a href=mailto:andrei.fokau@neutron.kth.se target=_blank><font size=3 color=blue><u>andrei.fokau@neutron.kth.se</u></font></a><font size=3>&gt;<br>
&gt;&gt; &gt; wrote:<br>
&gt;&gt; &gt;&gt; We use a C-program which consumes a lot of memory per
process (up to<br>
&gt;&gt; &gt;&gt; few<br>
&gt;&gt; &gt;&gt; GB), 99% of the data being the same for each process.
So for us it<br>
&gt;&gt; &gt;&gt; would be<br>
&gt;&gt; &gt;&gt; quite reasonable to put that part of data in a shared
memory.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; </font><a href=http://www.emsl.pnl.gov/docs/global/ target=_blank></a><a href=http://www.emsl.pnl.gov/docs/global/ target=_blank><font size=3 color=blue><u>http://www.emsl.pnl.gov/docs/global/</u></font></a><font size=3><br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Is this eny help? Apologies if I'm talking through my hat.<br>
&gt;&gt;<br>
&gt;&gt; I was also thinking of this when I read &quot;data in a shared
memory&quot; (besides<br>
&gt;&gt; approaches like </font><a href=http://www.kerrighed.org/wiki/index.php/Main_Page target=_blank></a><a href=http://www.kerrighed.org/wiki/index.php/Main_Page target=_blank><font size=3 color=blue><u>http://www.kerrighed.org/wiki/index.php/Main_Page</u></font></a><font size=3>).
Wasn't<br>
&gt;&gt; this also one idea behind &quot;High Performance Fortran&quot;
- running in parallel<br>
&gt;&gt; across nodes even without knowing that it's across nodes at all
while<br>
&gt;&gt; programming and access all data like it's being local.<br>
&gt;</font>
<br>
<br><tt><font size=2>_______________________________________________<br>
users mailing list<br>
users@open-mpi.org<br>
</font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a>
<br>
<br>
