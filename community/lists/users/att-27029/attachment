
<HTML><BODY><p>Hi, Mike!<br></p><p>I have impi v 4.1.2 (- impi)<br>I build ompi 1.8.5 with MXM and hcoll (- ompi_yalla)<br>I build ompi 1.8.5 without MXM and hcoll (- ompi_clear)</p><p>I start osu p2p: osu_mbr_mr test with this MPIs.<br></p>You can find the result of benchmark in attached file(mvs10p_mpi.xls: list osu_mbr_mr)<br><p><br>On 64 nodes (and 1024 mpi processes) ompi_yalla get 2 time worse perf than ompi_clear.<br>Is mxm with yalla <span id="result_box" class="short_text" lang="en"><span class="hps">reduces</span> <span class="hps">performance</span> in p2p <span class="hps">compared</span></span> with ompi_clear(and impi)?<br><span id="result_box" class="short_text" lang="en">Am <span class="hps">I&nbsp;</span><span class="hps">doing something</span> <span class="hps">wrong?</span></span><br></p><p>P.S. My colleague Alexander Semenov is in CC</p><p>Best regards,<br>Timur<br><br>Четверг, 28 мая 2015, 20:02 +03:00 от Mike Dubman &lt;miked@dev.mellanox.co.il&gt;:<br>
</p><blockquote style="border-left:1px solid #0857A6; margin:10px; padding:0 0 0 10px;">
	<div id="">
	



    











	
	


	
	
	

	

	
	

	

	
	



<div class="js-helper js-readmsg-msg">
	<style type="text/css"></style>
 	<div>
		<base target="_self" href="https://e.mail.ru/">
		
            <div id="style_14328327170000000882_BODY"><div dir="ltr">it is not apples-to-apples comparison.<div><br></div><div>yalla/mxm is point-to-point library, it is not collective library.</div><div>collective algorithm happens on top of yalla.</div><div><br></div><div>Intel collective algorithm for a2a is better than OMPI built-in collective algorithm.</div><div><br></div><div>To see benefit of yalla - you should run p2p benchmarks (osu_lat/bw/bibw/mr)</div><div><br></div></div><div><br><div>On Thu, May 28, 2015 at 7:35 PM, Timur Ismagilov <span dir="ltr">&lt;<a href="//e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span> wrote:<br><blockquote style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div>I compare ompi-1.8.5 (hpcx-1.3.3-icc) with impi v 4.1.4.<br><br>I build ompi with MXM but without HCOLL and without&nbsp; knem (I work on it). Configure options are:<br>&nbsp;./configure&nbsp; --prefix=my_prefix&nbsp;&nbsp; --with-mxm=path/to/hpcx/hpcx-v1.3.330-icc-OFED-1.5.4.1-redhat6.2-x86_64/mxm&nbsp;&nbsp; --with-platform=contrib/platform/mellanox/optimized<br><br>As a result of the IMB-MPI1 Alltoall test, I have got disappointing&nbsp; results: for the most message sizes on 64 nodes and 16 processes per&nbsp; node impi is much (~40%) better.<br><br>You can look at the results in the file "mvs10p_mpi.xlsx", I attach it. System configuration is also there.<br><br>What do you think about? Is there any way to improve ompi yalla performance results?<br><br>I attach the output of&nbsp; "IMB-MPI1 Alltoall" for yalla and impi.<br><br>P.S. My colleague Alexander Semenov is in CC<br><br>Best regards,<br>Timur</div>
</blockquote></div><br><br clear="all"><div><br></div>-- <br><div><div dir="ltr"><br><div>Kind Regards,</div><div><br></div><div>M.</div></div></div>
</div>
</div>
            
        
		<base target="_self" href="https://e.mail.ru/">
	</div>

	
</div>


</div>
</blockquote>
<br>
<br><br></BODY></HTML>
