<html><head><meta http-equiv="Content-Type" content="text/html charset=us-ascii"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">Have you tried a typical benchmark (e.g., NetPipe or OMB) to ensure the problem isn't in your program? Outside of that, you might want to explicitly tell it to --bind-to core just to be sure it does so - it's supposed to do that by default, but might as well be sure. You can check by adding --report-binding to the cmd line.<div><br></div><div><br><div><div>On Apr 14, 2014, at 11:10 PM, Muhammad Ansar Javed &lt;<a href="mailto:muhammad.ansar@seecs.edu.pk">muhammad.ansar@seecs.edu.pk</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr"><div>Hi,<br>I am trying to benchmark Open MPI performance on 10G Ethernet network between two hosts. The performance numbers of benchmarks are less than expected. The maximum bandwidth achieved by OMPI-C is 5678 Mbps and I was expecting around 9000+ Mbps. Moreover latency is also quite higher than expected, ranging from 37 to 59 us. Here is complete set of numbers.<br>
<br><b>Latency<br>Open MPI C&nbsp;&nbsp;&nbsp; <br>Size&nbsp;&nbsp;&nbsp; Time (us)</b><br>1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 37.76<br>2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 37.75<br>4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 37.78<br>8&nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp; 55.17<br>16&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 37.89<br>32&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 39.08<br>64&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 37.78<br>128&nbsp;&nbsp;&nbsp;&nbsp; 59.46<br>256&nbsp;&nbsp;&nbsp;&nbsp; 39.37<br>
512&nbsp;&nbsp;&nbsp;&nbsp; 40.39<br>1024&nbsp;&nbsp; 47.18<br>2048&nbsp;&nbsp; 47.84<br>&nbsp;&nbsp;&nbsp; <br><br><b>Bandwidth<br>Open MPI C&nbsp;&nbsp;&nbsp; <br>Size (Bytes)&nbsp;&nbsp;&nbsp; Bandwidth (Mbps)</b><br>2048&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp; 412.22<br>4096&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; 539.59<br>8192&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; 827.73<br>
16384&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 1655.35<br>32768&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp; 3274.3<br>65536&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1995.22<br>131072&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3270.84<br>262144&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4316.22<br>524288&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5019.46<br>1048576&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; 5236.17<br>2097152&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; 5362.61<br>
4194304&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; 5495.2<br>8388608&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; 5565.32<br>16777216&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; 5678.32<br><br><br>My environments consists of two hosts having point-to-point (switch-less) 10Gbps Ethernet connection.&nbsp; Environment (OS, user, directory structure etc) on both hosts is exactly same. There is no NAS or shared file system between both hosts. Following are configuration and job launching commands that I am using. Moreover, I have attached output of script<span style="font-family:courier new,monospace"> ompi_info --all</span>.<br>
<br>Configuration commmand: <span style="font-family:courier new,monospace">./configure --enable-mpi-java --prefix=/home/mpj/installed/openmpi_installed CC=/usr/bin/gcc --disable-mpi-fortran </span><br><br>Job launching command: <span style="font-family:courier new,monospace">mpirun -np 2 -hostfile machines -npernode 1 ./latency.out<br>
</span><br>Are these numbers okay? If not then please suggest performance tuning steps...<br><br></div>Thanks<br clear="all"><div><br>--<br><div dir="ltr">Ansar Javed<br>HPC Lab<br>SEECS NUST <br>Contact: +92 334 438 9394<br>
Email: <a href="mailto:muhammad.ansar@seecs.edu.pk" target="_blank">muhammad.ansar@seecs.edu.pk</a><br></div>
</div></div>
<span>&lt;ompi_info.tar.bz2&gt;</span>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
