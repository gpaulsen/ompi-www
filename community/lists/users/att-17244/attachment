<p style="padding:0 0 0 0; margin:0 0 0 0;">Thanks for info. I was thinking it could be some wrong interpretation of per cpu core count.</p>

<p style="padding:0 0 0 0; margin:0 0 0 0;">I will try newer library.</p>

<p style="padding:0 0 0 0; margin:0 0 0 0;">&nbsp;</p>

<p style="padding:0 0 0 0; margin:0 0 0 0;">&nbsp;</p>

<p style="padding:0 0 0 0; margin:0 0 0 0;">&nbsp;</p>

<p style="padding:0 0 0 0; margin:0 0 0 0;">______________________________________________________________<br />
&gt; Od: "Brice Goglin" &lt;Brice.Goglin@inria.fr&gt;<br />
&gt; Komu: Open MPI Users &lt;users@open-mpi.org&gt;<br />
&gt; D&aacute;tum: 13.09.2011 13:28<br />
&gt; Predmet: Re: [OMPI users] #cpus/socket<br />
&gt;<br />
Le 13/09/2011 18:59, Peter Kjellstr&ouml;m a &eacute;crit :<br />
&gt; On Tuesday, September 13, 2011 09:07:32 AM nn3003 wrote:<br />
&gt;&gt; Hello !<br />
&gt;&gt; &nbsp;<br />
&gt;&gt; I am running wrf model on 4x AMD 6172 which is 12 core CPU. I use OpenMPI<br />
&gt;&gt; 1.4.3 and libgomp 4.3.4. I have binaries compiled for shared-memory and<br />
&gt;&gt; distributed-memory (OpenMP and OpenMPI) I use following command<br />
&gt;&gt; mpirun -np 4 --cpus-per-proc 6 --report-bindings --bysocket wrf.exe<br />
&gt;&gt; It works ok and in top i see there are 4 wrf.exe and each has 6 threads on<br />
&gt;&gt; cpu0-5 12-17 24-29 36-41 However, if I want to run 8 or more e.g.<br />
&gt;&gt; mpirun -np 4 --cpus-per-proc 12 --report-bindings --bysocket wrf.exe<br />
&gt;&gt; I get error<br />
&gt;&gt; Your job has requested more cpus per process(rank) than there<br />
&gt;&gt; are cpus in a socket:<br />
&gt;&gt; &nbsp; Cpus/rank: 8<br />
&gt;&gt; &nbsp; #cpus/socket: 6<br />
&gt;&gt; &nbsp;<br />
&gt;&gt; Why is that ? There are 12 cores per socket in AMD 6172.<br />
&gt; In reality a 12 core Magnycours is two 6 core dies on a socket. I'm guessing <br />
&gt; that the topology code sees your 4x 12 core as a 8x 6 core.<br />
<br />
plpa-info reports 4*6cores:<br />
 &nbsp;Number of processor sockets: 4<br />
 &nbsp;Number of processors online: 48<br />
 &nbsp;Number of processors offline: 0 (no topology information available)<br />
 &nbsp;Socket 0 (ID 0): 6 cores (max core ID: 5)<br />
 &nbsp;Socket 1 (ID 1): 6 cores (max core ID: 5)<br />
 &nbsp;Socket 2 (ID 2): 6 cores (max core ID: 5)<br />
 &nbsp;Socket 3 (ID 3): 6 cores (max core ID: 5)<br />
<br />
This should be fixed with Open MPI 1.5.2+ with hwloc.<br />
<br />
Brice<br />
<br />
_______________________________________________<br />
users mailing list<br />
users@open-mpi.org<br />
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"></a></p>


