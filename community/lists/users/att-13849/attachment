<div>Gus</div><div>my kernel for all nodes is this one:</div><div><div>Linux 2.6.32-22-server #36-Ubuntu SMP Thu Jun 3 20:38:33 UTC 2010 x86_64 GNU/Linux</div></div><div><br></div><div>at least for the moment i will use this configuration, at least for deveplopment/testing  of the parallel programs.</div>

<div>lag is minimum :)</div><div><br></div><div>whenever i get another kernel update, i will test again to check if sm works, would be good to know that suddenly another distribution supports nehalem sm.</div><div><br></div>

<div>best regards and thanks again</div><div>Cristobal</div><div>ps: guess what are the names of the other 2 nodes lol</div><div><br></div><div><br></div><div><br></div><div><div class="gmail_quote">On Wed, Jul 28, 2010 at 5:50 PM, Gus Correa <span dir="ltr">&lt;<a href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">Hi Cristobal<br>
<br>
Please, read my answer (way down the message) below.<br>
<br>
Cristobal Navarro wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div class="im">
<br>
<br>
On Wed, Jul 28, 2010 at 3:28 PM, Gus Correa &lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a> &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;&gt; wrote:<br>


<br>
    Hi Cristobal<br>
<br>
    Cristobal Navarro wrote:<br>
<br>
<br>
<br>
        On Wed, Jul 28, 2010 at 11:09 AM, Gus Correa<br>
        &lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a> &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;<br></div><div class="im">
        &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a> &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;&gt;&gt;<br>
        wrote:<br>
<br>
           Hi Cristobal<br>
<br></div><div><div></div><div class="h5">
           In case you are not using full path name for mpiexec/mpirun,<br>
           what does &quot;which mpirun&quot; say?<br>
<br>
<br>
        --&gt; $which mpirun<br>
             /opt/openmpi-1.4.2<br>
<br>
<br>
           Often times this is a source of confusion, old versions may<br>
           be first on the PATH.<br>
<br>
           Gus<br>
<br>
<br>
        openMPI version problem is now gone, i can confirm that the<br>
        version is consistent now :), thanks.<br>
<br>
<br>
    This is good news.<br>
<br>
<br>
        however, i keep getting this kernel crash randomnly when i<br>
        execute with -np higher than 5<br>
        these are Xeons, with Hyperthreading On, is that a problem??<br>
<br>
<br>
    The problem may be with Hyperthreading, maybe not.<br>
    Which Xeons?<br>
<br>
<br>
--&gt; they are not so old, not so new either<br>
fcluster@agua:~$ cat /proc/cpuinfo | more<br>
processor : 0<br>
vendor_id : GenuineIntel<br>
cpu family : 6<br>
model : 26<br>
model name : Intel(R) Xeon(R) CPU           E5520  @ 2.27GHz<br>
stepping : 5<br>
cpu MHz : 1596.000<br>
cache size : 8192 KB<br>
physical id : 0<br>
siblings : 8<br>
core id : 0<br>
cpu cores : 4<br>
apicid : 0<br>
initial apicid : 0<br>
fpu : yes<br>
fpu_exception : yes<br>
cpuid level : 11<br>
wp : yes<br>
flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss h<br>
t tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_<br>
cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 popcnt lahf_lm ida tpr_shadow vnmi flexpriority ept vpid<br>
bogomips : 4522.21<br>
clflush size : 64<br>
cache_alignment : 64<br>
address sizes : 40 bits physical, 48 bits virtual<br>
power management:<br>
...same for cpu1, 2, 3, ..., 15.<br>
<br>
</div></div></blockquote>
<br>
AHA! Nehalems!<br>
<br>
Here they are E5540, just a different clock speed, I suppose.<div><div></div><div class="h5"><br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
information on how the cpu is distributed<br>
fcluster@agua:~$ lstopo<br>
System(7992MB)<br>
  Socket#0 + L3(8192KB)<br>
    L2(256KB) + L1(32KB) + Core#0<br>
      P#0<br>
      P#8<br>
    L2(256KB) + L1(32KB) + Core#1<br>
      P#2<br>
      P#10<br>
    L2(256KB) + L1(32KB) + Core#2<br>
      P#4<br>
      P#12<br>
    L2(256KB) + L1(32KB) + Core#3<br>
      P#6<br>
      P#14<br>
  Socket#1 + L3(8192KB)<br>
    L2(256KB) + L1(32KB) + Core#0<br>
      P#1<br>
      P#9<br>
    L2(256KB) + L1(32KB) + Core#1<br>
      P#3<br>
      P#11<br>
    L2(256KB) + L1(32KB) + Core#2<br>
      P#5<br>
      P#13<br>
    L2(256KB) + L1(32KB) + Core#3<br>
      P#7<br>
      P#15<br>
<br>
<br>
<br>
 <br>
    If I remember right, the old hyperthreading on old Xeons was<br>
    problematic.<br>
<br>
    OTOH, about 1-2 months ago I had trouble with OpenMPI on a<br>
    relatively new Xeon Nehalem machine with (the new) Hyperthreading<br>
    turned on,<br>
    and Fedora Core 13.<br>
    The machine would hang with the OpenMPI connectivity example.<br>
    I reported this to the list, you may find in the archives.<br>
<br>
<br>
--i foudn the archives recently about an hour ago, was not sure if it was the same problem but i removed HT for testing with setting the online flag to 0 on the extra cpus showed with lstopo, unfortenately i also crashes, so HT may not be the problem. <br>


</blockquote>
<br></div></div>
It didn&#39;t fix the problem in our Nehalem machine here either,<br>
although it was FC13, and I don&#39;t know what OS and kernel you&#39;re using.<div class="im"><br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
    Apparently other people got everything (OpenMPI with HT on Nehalem)<br>
    working in more stable distributions (CentOS, RHEL, etc).<br>
<br>
    That problem was likely to be in the FC13 kernel,<br>
    because even turning off HT I still had the machine hanging.<br>
    Nothing worked with shared memory turned on,<br>
    so I had to switch OpenMPI to use tcp instead,<br>
    which is kind of ridiculous in a standalone machine.<br>
<br>
<br>
--&gt; very interesting, sm can be the problem<br>
 <br>
<br>
<br>
<br>
        im trying to locate the kernel error on logs, but after<br>
        rebooting a crash, the error is not in the kern.log (neither<br>
        kern.log.1).<br>
        all i remember is that it starts with &quot;Kernel BUG...&quot;<br>
        and somepart it mentions a certain CPU X, where that cpu can be<br>
        any from 0 to 15 (im testing only in main node).  Someone knows<br>
        where the log of kernel error could be?<br>
<br>
<br>
    Have you tried to turn off hyperthreading?<br>
<br>
<br>
--&gt; yes, tried, same crashes.<br>
 <br>
    In any case, depending on the application, it may not help much<br>
    performance to have HT on.<br>
<br>
    A more radical alternative is to try<br>
    -mca btl tcp,self<br>
    in the mpirun command line.<br>
    That is what worked in the case I mentioned above.<br>
<br>
<br>
wow!, this worked really :),  you pointed out the problem, it was shared memory.<br>
</blockquote>
<br></div>
Great news!<br>
That&#39;s exactly the problem we had here.<br>
Glad that the same solution worked for you.<br>
<br>
Over a year ago another fellow reported the same problem on Nehalem,<br>
on the very early days of Nehalem.<br>
The thread should be in the archives.<br>
Somebody back then (Ralph, or Jeff, or other?)<br>
suggested that turning off &quot;sm&quot; might work.<br>
So, I take no credit for this.<div class="im"><br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
i have 4 nodes, so anyways there will be node comunication, do you think i can rely on working with -mca btl tcp,self?? i dont mind small lag.<br>
<br>
</blockquote>
<br></div>
Well, this may be it, short from reinstalling the OS.<br>
<br>
Some people reported everything works with OpenMPI+HT+sm in CentOS and RHEL, see the thread I mentioned in the archives from 1-2 months ago.<br>
I don&#39;t administer that machine, and didn&#39;t have the time to do OS reinstall either.<br>
So I left it with -mca btl tcp,self, and the user/machine owner<br>
is happy that he can run his programs right,<br>
and with a performance that he considers good.<div class="im"><br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
i just have one more question, is this a problem of the ubuntu server kernel?? from the Nehalem Cpus?? from openMPI (i dont think) ?? <br>
</blockquote>
<br></div>
I don&#39;t have any idea.<br>
It may be a problem with some kernels, not sure.<br>
Which kernel do you have?<br>
<br>
Ours was FC-13, maybe FC-12, I don&#39;t remember exactly.<br>
Currently that machine has kernel 2.6.33.6-147.fc13.x86_64 #1 SMP.<br>
However, it may have been a slightly older kernel when I installed<br>
OpenMPI there.<br>
It may have been 2.6.33.5-124.fc13.x86_64 or 2.6.32.14-127.fc12.x86_64.<br>
My colleague here updates the machines with yum,<br>
so it may have gotten a new kernel since then.<br>
<br>
Our workhorse machines in the clusters that I take care<br>
of are AMD Opteron, never had this problem there.<br>
Maybe the kernels have yet to catch up with Nehalem,<br>
now Westmere, soon another one.<div class="im"><br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
and on what depends that in the future, sm could be possible on the same configuration i have?? kernel update?.<br>
<br>
</blockquote>
<br></div>
You may want to try CentOS or RHEL, but I can&#39;t guarantee the results.<br>
Somebody else in the list may have had the direct experience,<br>
and may speak out.<br>
<br>
It may be worth the effort anyway.<br>
After all, intra-node communication should be<br>
running on shared memory.<br>
Having to turn it off is outrageous.<br>
<br>
If you try another OS distribution,<br>
and if it works, please report the results back to the list:<br>
OS/distro, kernel, OpenMPI version, HT on or off,<br>
mca btl sm/tcp/self/etc choices, compilers, etc.<br>
This type of information is a real time saving for everybody.<div class="im"><br>
<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Thanks very much Gus, really!<br>
Cristobal<br>
<br>
<br>
</blockquote>
<br></div>
My pleasure.<br>
Glad that there was a solution, even though not the best.<br>
Enjoy your cluster with vocano-named nodes!<br>
Have fun with OpenMPI and PETSc!<br>
<br>
Gus Correa<br>
---------------------------------------------------------------------<br>
Gustavo Correa<br>
Lamont-Doherty Earth Observatory - Columbia University<br>
Palisades, NY, 10964-8000 - USA<br>
---------------------------------------------------------------------<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div></div><div class="h5">
 <br>
<br>
    My $0.02<br>
    Gus Correa<br>
<br>
<br>
           Cristobal Navarro wrote:<br>
<br>
<br>
               On Tue, Jul 27, 2010 at 7:29 PM, Gus Correa<br>
               &lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a> &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;<br>
        &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a> &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;&gt;<br>
               &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a><br>
        &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt; &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a><br>
        &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;&gt;&gt;&gt;<br>
<br>
               wrote:<br>
<br>
                  Hi Cristobal<br>
<br>
                  Does it run only on the head node alone?<br>
                  (Fuego? Agua? Acatenango?)<br>
                  Try to put only the head node on the hostfile and execute<br>
               with mpiexec.<br>
<br>
               --&gt; i will try only with the head node, and post results back<br>
                  This may help sort out what is going on.<br>
                  Hopefully it will run on the head node.<br>
<br>
                  Also, do you have Infinband connecting the nodes?<br>
                  The error messages refer to the openib btl (i.e.<br>
        Infiniband),<br>
                  and complains of<br>
<br>
<br>
               no we are just using normal network 100MBit/s , since i<br>
        am just<br>
               testing yet.<br>
<br>
<br>
                  &quot;perhaps a missing symbol, or compiled for a different<br>
                  version of Open MPI?&quot;.<br>
                  It sounds as a mixup of versions/builds.<br>
<br>
<br>
               --&gt; i agree, somewhere there must be the remains of the older<br>
               version<br>
<br>
                  Did you configure/build OpenMPI from source, or did<br>
        you install<br>
                  it with apt-get?<br>
                  It may be easier/less confusing to install from source.<br>
                  If you did, what configure options did you use?<br>
<br>
<br>
               --&gt;i installed from source, ./configure<br>
               --prefix=/opt/openmpi-1.4.2 --with-sge --without-xgid<br>
               --disable--static<br>
<br>
                  Also, as for the OpenMPI runtime environment,<br>
                  it is not enough to set it on<br>
                  the command line, because it will be effective only on the<br>
               head node.<br>
                  You need to either add them to the PATH and<br>
        LD_LIBRARY_PATH<br>
                  on your .bashrc/.cshrc files (assuming these files and<br>
        your home<br>
                  directory are *also* shared with the nodes via NFS),<br>
                  or use the --prefix option of mpiexec to point to the<br>
        OpenMPI<br>
               main<br>
                  directory.<br>
<br>
<br>
               yes, all nodes have their PATH and LD_LIBRARY_PATH set up<br>
               properly inside the login scripts ( .bashrc in my case  )<br>
<br>
                  Needless to say, you need to check and ensure that the<br>
        OpenMPI<br>
                  directory (and maybe your home directory, and your work<br>
               directory)<br>
                  is (are)<br>
                  really mounted on the nodes.<br>
<br>
<br>
               --&gt; yes, doublechecked that they are<br>
<br>
                  I hope this helps,<br>
<br>
<br>
               --&gt; thanks really!<br>
<br>
                  Gus Correa<br>
<br>
                  Update: i just reinstalled openMPI, with the same<br>
        parameters,<br>
               and it<br>
                  seems that the problem has gone, i couldnt test<br>
        entirely but<br>
               when i<br>
                  get back to lab ill confirm.<br>
<br>
               best regards! Cristobal<br>
<br>
<br>
                      ------------------------------------------------------------------------<br>
<br>
               _______________________________________________<br>
               users mailing list<br>
               <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br></div></div>
        &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;&gt;<div class="im"><br>
<br>
               <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
           _______________________________________________<br>
           users mailing list<br>
           <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br></div>
        &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;&gt;<div class="im"><br>
<br>
           <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
        ------------------------------------------------------------------------<br>
<br>
        _______________________________________________<br>
        users mailing list<br>
        <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
        <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
    _______________________________________________<br>
    users mailing list<br>
    <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
    <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
------------------------------------------------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></blockquote><div><div></div><div class="h5">
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div>

