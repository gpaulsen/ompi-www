<div dir="ltr"><div>Tried this but mpirun exits with this error</div><div> </div><div>mpirun -np 40 /home/MET/hrm/bin/hrm<br>librdmacm: couldn&#39;t read ABI version.<br>librdmacm: assuming: 4<br>librdmacm: couldn&#39;t read ABI version.<br>
librdmacm: assuming: 4<br>librdmacm: couldn&#39;t read ABI version.<br>librdmacm: assuming: 4<br>librdmacm: couldn&#39;t read ABI version.<br>librdmacm: assuming: 4<br>librdmacm: couldn&#39;t read ABI version.<br>CMA: unable to get RDMA device list<br>
CMA: unable to get RDMA device list<br>CMA: unable to get RDMA device list<br>CMA: unable to get RDMA device list<br>librdmacm: assuming: 4<br>librdmacm: couldn&#39;t read ABI version.<br>librdmacm: assuming: 4<br>CMA: unable to get RDMA device list<br>
CMA: unable to get RDMA device list<br>librdmacm: couldn&#39;t read ABI version.<br>librdmacm: couldn&#39;t read ABI version.<br>librdmacm: assuming: 4<br>CMA: unable to get RDMA device list<br>librdmacm: assuming: 4<br>CMA: unable to get RDMA device list<br>
--------------------------------------------------------------------------<br>[[33095,1],8]: A high-performance Open MPI point-to-point messaging module<br>was unable to find any relevant network interfaces:</div><div>Module: OpenFabrics (openib)<br>
  Host: <a href="http://pmd04.pakmet.com">pmd04.pakmet.com</a></div><div>Another transport will be used instead, although this may result in<br>lower performance.<br>--------------------------------------------------------------------------<br>
--------------------------------------------------------------------------<br>At least one pair of MPI processes are unable to reach each other for<br>MPI communications.  This means that no Open MPI device has indicated<br>
that it can be used to communicate between these processes.  This is<br>an error; Open MPI requires that all MPI processes be able to reach<br>each other.  This error can sometimes be the result of forgetting to<br>specify the &quot;self&quot; BTL.</div>
<div>  Process 1 ([[33095,1],28]) is on host: <a href="http://compute-02-00.private02.pakmet.com">compute-02-00.private02.pakmet.com</a><br>  Process 2 ([[33095,1],0]) is on host: pmd02<br>  BTLs attempted: openib self sm</div>
<div>Your MPI job is now going to abort; sorry.<br>--------------------------------------------------------------------------<br></div><div class="gmail_extra"> </div><div class="gmail_extra"> </div><div class="gmail_extra">
Ahsan<br><br><div class="gmail_quote">On Fri, Mar 22, 2013 at 7:09 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote style="margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid" class="gmail_quote">
<div style="word-wrap:break-word"><br><div><div class="im"><div>On Mar 22, 2013, at 3:42 AM, Syed Ahsan Ali &lt;<a href="mailto:ahsanshah01@gmail.com" target="_blank">ahsanshah01@gmail.com</a>&gt; wrote:</div><br><blockquote type="cite">
<div dir="ltr"><div>Actually due to some data base corruption I am not able to add any new node to cluster from the installer node. So I want to run parallel job on more nodes without adding them to existing cluster.</div>

<div>You are right the binaries must be present on the remote node as well. </div><div>Is this possible throught nfs? just as the compute nodes are nfs mounted with the installer node.</div></div></blockquote><div><br></div>
</div>Sure - OMPI doesn&#39;t care how the binaries got there. Just so long as they are present on the compute node.</div><div><div class="h5"><div><br><blockquote type="cite"><div dir="ltr"><div> </div><div>Ahsan</div></div>

<div class="gmail_extra"><br><br><div class="gmail_quote">On Fri, Mar 22, 2013 at 3:33 PM, Reuti <span dir="ltr">&lt;<a href="mailto:reuti@staff.uni-marburg.de" target="_blank">reuti@staff.uni-marburg.de</a>&gt;</span> wrote:<br>

<blockquote style="margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid" class="gmail_quote">Am 22.03.2013 um 10:14 schrieb Syed Ahsan Ali:<br>
<div><br>
&gt; I have a very basic question. If we want to run mpirun job on two systems which are not part of cluster, then how we can make it possible. Can the host be specifiend on mpirun which is not compute node, rather a stand alone system.<br>


<br>
</div>Sure, the machines can be specified as argument to `mpiexec`. But do you want to run applications just between these two machines, or should they participate on a larger parallel job with machines of the cluster: then a direct network connection between outside and inside of the cluster is necessary by some kind of forwarding in case these are separated networks.<br>


<br>
Also the paths to the started binaries may be different, in case the two machines are not sharing the same /home with the cluster and this needs to be honored.<br>
<br>
In case you are using a queuing system and want to route jobs to outside machines of the set up cluster: it&#39;s necessary to negotiate with the admin to allow jobs being scheduled thereto.<br>
<br>
-- Reuti<br>
<br>
<br>
&gt; Thanks<br>
&gt; Ahsan<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br><br clear="all"><br>-- <br>Syed Ahsan Ali Bokhari <br>Electronic Engineer (EE)<div><br>Research &amp; Development Division<br>Pakistan Meteorological Department H-8/4, Islamabad.<br>Phone # off  <a href="tel:%2B92518358714" target="_blank" value="+92518358714">+92518358714</a></div>

<div>Cell # <a href="tel:%2B923155145014" target="_blank" value="+923155145014">+923155145014</a><br></div>
</div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote>
</div><br></div></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><div><br> </div>
</div></div>

