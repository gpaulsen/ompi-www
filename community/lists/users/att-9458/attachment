<html><head><style type="text/css"><!-- DIV {margin:0px;} --></style></head><body><div style="font-family:'times new roman', 'new york', times, serif;font-size:12pt"><div>Hi Mr. Jeff Squyres,</div><div>Is it true to use bidirectianal communication with MPI in ethernet Cluster?</div><div>I have tried once (I thought, it is possible because of fully duplex swithes).</div><div> However, I could not get bandwidth improvement as I was expecting.</div><div><br></div><div>If you answer is YES, would you please tell me about <span class="Apple-style-span" style="border-collapse: collapse; font-family: Arial; font-size: 13px; white-space: pre; -webkit-border-horizontal-spacing: 2px; -webkit-border-vertical-spacing: 2px; ">pseudocode for </span></div><div><font class="Apple-style-span" face="Arial" size="3"><span class="Apple-style-span" style="border-collapse: collapse; font-size: 13px; white-space: pre; -webkit-border-horizontal-spacing: 2px;
 -webkit-border-vertical-spacing: 2px;">bidirectional communication ? </span></font></div><div><font class="Apple-style-span" face="Arial" size="3"><span class="Apple-style-span" style="border-collapse: collapse; font-size: 13px; white-space: pre; -webkit-border-horizontal-spacing: 2px; -webkit-border-vertical-spacing: 2px;"><br></span></font></div><div style="font-family:times new roman, new york, times, serif;font-size:12pt">Thank you.</div><div style="font-family:times new roman, new york, times, serif;font-size:12pt">Axida </div><div style="font-family:times new roman, new york, times, serif;font-size:12pt"><br></div><div style="font-family:times new roman, new york, times, serif;font-size:12pt"><br></div><div style="font-family:times new roman, new york, times, serif;font-size:12pt"><br><div style="font-family:arial, helvetica, sans-serif;font-size:13px"><font size="2" face="Tahoma"><hr size="1"><b><span style="font-weight: bold;">From:</span></b>
 Jeff Squyres &lt;jsquyres@cisco.com><br><b><span style="font-weight: bold;">To:</span></b> Open MPI Users &lt;users@open-mpi.org><br><b><span style="font-weight: bold;">Sent:</span></b> Wednesday, May 27, 2009 11:28:42 PM<br><b><span style="font-weight: bold;">Subject:</span></b> Re: [OMPI users] How to use Multiple links with OpenMPI??????????????????<br></font><br>
Open MPI considers hosts differently than network links.<br><br>So you should only list the actual hostname in the hostfile, with slots equal to the number of processors (4 in your case, I think?).<br><br>Once the MPI processes are launched, they each look around on the host that they're running and find network paths to each of their peers.  If they are multiple paths between pairs of peers, Open MPI will round-robin stripe messages across each of the links.  We don't really have an easy setting for each peer pair only using 1 link.  Indeed, since connectivity is bidirectional, the traffic patterns become less obvious if you want MPI_COMM_WORLD rank X to only use link Y -- what does that mean to the other 4 MPI processes on the other host (with whom you have assumedly assigned their own individual links as well)?<br><br><br>On May 26, 2009, at 12:24 AM, shan axida wrote:<br><br>> Hi everyone,<br>> I want to ask how to use multiple links (multiple
 NICs) with OpenMPI.<br>> For example, how can I assign a link to each process, if there are 4 links<br>> and 4 processors on each node in our cluster?<br>> Is this a correct way?<br>> hostfile:<br>> ----------------------<br>> host1-eth0 slots=1<br>> host1-eth1 slots=1<br>> host1-eth2 slots=1<br>> host1-eth3 slots=1<br>> host2-eth0 slots=1<br>> host2-eth1 slots=1<br>> host2-eth2 slots=1<br>> host2-eth3 slots=1<br>> ...             ...<br>> ...          ...<br>> host16-eth0 slots=1<br>> host16-eth1 slots=1<br>> host16-eth2 slots=1<br>> host16-eth3 slots=1<br>> ------------------------<br>> <br>> <br>> <br>> <br>> <br>> <br>> <br>> <br>> <br>> <br>> _______________________________________________<br>> users mailing list<br>> <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><span>> <a target="_blank"
 href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></span><br><br><br>--Jeff Squyres<br>Cisco Systems<br><br>_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div><div style="position:fixed"></div></div><br>



      </body></html>
