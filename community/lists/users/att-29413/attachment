<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    <p>Christopher,</p>
    <p><br>
    </p>
    <p>the sm btl does not work with inter communicators and hence
      disqualifies itself.</p>
    <p>i guess this is what you interpreted as 'partially working'</p>
    <p><br>
    </p>
    <p>i am surprised you are using a privileged port (260 &lt; 1024),
      are you running as an admin ?</p>
    <p><br>
    </p>
    <p>Open MPI is no more supported on windows, and the 1.6 series is
      pretty antique these days...</p>
    <p><br>
    </p>
    <p>regardless this, the source code points to</p>
    <p><br>
    </p>
    static __inline int opal_get_socket_errno(void) {<br>
    <p>    int ret = WSAGetLastError();<br>
          switch (ret) {<br>
            case WSAEINTR: return EINTR;<br>
      ...</p>
    <p>      default: printf("Feature not implemented: %d %s\n",
      __LINE__, __FILE__); return OPAL_ERROR;<br>
          };<br>
      }<br>
    </p>
    <p><br>
    </p>
    <p>at first, it is worth printing (ret) if the feature is not
      implemented.</p>
    <p>then you can hack this part and add the missing case</p>
    <p>recent windows (7) might use a newer one that was not available
      on older ones (xp)</p>
    <p><br>
    </p>
    <p>Cheers,</p>
    <p><br>
    </p>
    <p>Gilles<br>
    </p>
    <p><br>
    </p>
    <p><br>
    </p>
    <br>
    <div class="moz-cite-prefix">On 6/9/2016 1:51 AM, Roth, Christopher
      wrote:<br>
    </div>
    <blockquote
cite="mid:95110DC63CB062409084890E698980AD1D83D4C9@VAW1PWINFMBXP1.veriskdom.com"
      type="cite">
      <meta http-equiv="Content-Type" content="text/html;
        charset=windows-1252">
      <style id="owaParaStyle" type="text/css">P {margin-top:0;margin-bottom:0;}</style>
      <div style="direction: ltr;font-family: Tahoma;color:
        #000000;font-size: 12pt;">Well, that obvious error message
        states the basic problem - I was hoping you had noticed a detail
        in the ompi_info output that would point to a reason for it.<br>
        <br>
        Further test runs with the option '-mca btl tcp,self' (excluding
        'sm' from the mix) and '-mca btl_base_verbose 100', supply some
        more information:<br>
        ------<br>
        [sweet1:04556] btl: tcp: attempting to connect() to address
        10.3.2.109 on port 260<br>
        [sweet1:04556] btl: tcp: attempting to connect() to address
        10.3.2.109 on port 260<br>
        ------<br>
        The ip address is the host machine's.  The process ID
        corresponds to the first of the executor programs.  The programs
        now hang at that point (within the scheduler's MPI_Comm_spawn
        call and the executors' MPI_Init calls), and and have to be
        manually killed.<br>
        <br>
        Yet another test, adding the '-mca mpi_preconnect_mpi 1' (along
        with the other two added arguments), gives more info:<br>
        ------<br>
        [sweet1:04976] btl: tcp: attempting to connect() to address
        10.3.2.109 on port 260<br>
        [sweet1:04516] btl: tcp: attempting to connect() to address
        10.3.2.109 on port 260<br>
        [sweet1:03824] btl: tcp: attempting to connect() to address
        10.3.2.109 on port 260<br>
[sweet1][[17613,2],1][..\..\..\openmpi-1.6.2\ompi\mca\btl\tcp\btl_tcp_endpoint.c:486:..\..\..\openmpi-1.6.2\ompi\mca\btl<br>
        \tcp\btl_tcp_endpoint.c] received unexpected process identifier
        [[17613,2],0]<br>
[sweet1][[17613,2],0][..\..\..\openmpi-1.6.2\ompi\mca\btl\tcp\btl_tcp_frag.c:215:..\..\..\openmpi-1.6.2\ompi\mca\btl\tcp<br>
        \btl_tcp_frag.c] Feature not implemented: 130
        D:/temp/OpenMPI/openmpi-1.6.2/opal/include\opal/opal_socket_errno.h<br>
        Feature not implemented: 130
        D:/temp/OpenMPI/openmpi-1.6.2/opal/include\opal/opal_socket_errno.h<br>
        mca_btl_tcp_frag_recv: readv failed: Unknown error (-1)<br>
        ------<br>
        With the 'preconnect' option, it sets up the TCP link for all of
        the executor processes, but then runs into an actual error,
        regarding some function not implemented.  This option is not
        required, but I had to give it a whirl.<br>
        <br>
        All of these test runs have the same behavior when performed
        with and without the firewall active.<br>
        <br>
        The fact that the executor programs don't get past the MPI_Init
        call when the 'sm' is excluded from btl set , implies that the
        'sm' is at least partially working.<br>
        <br>
        <div style="font-family: Times New Roman; color: #000000;
          font-size: 16px">
          <hr tabindex="-1">
          <div style="direction: ltr;" id="divRpF221342"><font
              face="Tahoma" color="#000000" size="2"><b>From:</b> users
              [<a class="moz-txt-link-abbreviated" href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>] on behalf of Ralph Castain
              [<a class="moz-txt-link-abbreviated" href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>]<br>
              <b>Sent:</b> Wednesday, June 08, 2016 10:47 AM<br>
              <b>To:</b> Open MPI Users<br>
              <b>Subject:</b> Re: [OMPI users] Processes unable to
              communicate when using MPI_Comm_spawn on Windows<br>
            </font><br>
          </div>
          <div><br class="">
            <div>
              <blockquote type="cite" class="">
                <div class="">On Jun 8, 2016, at 4:30 AM, Roth,
                  Christopher &lt;<a moz-do-not-send="true"
                    href="mailto:CRoth@aer.com" class="" target="_blank">CRoth@aer.com</a>&gt;
                  wrote:</div>
                <br class="Apple-interchange-newline">
                <div class="">
                  <div class="" style="font-style:normal;
                    font-weight:normal; letter-spacing:normal;
                    orphans:auto; text-align:start; text-indent:0px;
                    text-transform:none; white-space:normal;
                    widows:auto; word-spacing:0px; direction:ltr;
                    font-family:Tahoma; font-size:12pt">
                    What part of this output indicates this
                    non-communicative configuration?<br class="">
                  </div>
                </div>
              </blockquote>
              <div><br class="">
              </div>
              <span class="" style="font-family:Tahoma; font-size:16px">--------------------------------------------------------------------------</span><br
                class="" style="font-family:Tahoma; font-size:16px">
              <span class="" style="font-family:Tahoma; font-size:16px">At
                least one pair of MPI processes are unable to reach each
                other for</span><br class="" style="font-family:Tahoma;
                font-size:16px">
              <span class="" style="font-family:Tahoma; font-size:16px">MPI
                communications.  This means that no Open MPI device has
                indicated</span><br class="" style="font-family:Tahoma;
                font-size:16px">
              <span class="" style="font-family:Tahoma; font-size:16px">that
                it can be used to communicate between these processes. 
                This is</span><br class="" style="font-family:Tahoma;
                font-size:16px">
              <span class="" style="font-family:Tahoma; font-size:16px">an
                error; Open MPI requires that all MPI processes be able
                to reach</span><br class="" style="font-family:Tahoma;
                font-size:16px">
              <span class="" style="font-family:Tahoma; font-size:16px">each
                other.  This error can sometimes be the result of
                forgetting to</span><br class=""
                style="font-family:Tahoma; font-size:16px">
              <span class="" style="font-family:Tahoma; font-size:16px">specify
                the "self" BTL.</span><br class=""
                style="font-family:Tahoma; font-size:16px">
              <br class="" style="font-family:Tahoma; font-size:16px">
              <span class="" style="font-family:Tahoma; font-size:16px"> 
                Process 1 ([[20141,1],0]) is on host: sweet1</span><br
                class="" style="font-family:Tahoma; font-size:16px">
              <span class="" style="font-family:Tahoma; font-size:16px"> 
                Process 2 ([[20141,2],0]) is on host: sweet1</span><br
                class="" style="font-family:Tahoma; font-size:16px">
              <span class="" style="font-family:Tahoma; font-size:16px"> 
                BTLs attempted: tcp sm self</span><br class=""
                style="font-family:Tahoma; font-size:16px">
              <br class="" style="font-family:Tahoma; font-size:16px">
              <span class="" style="font-family:Tahoma; font-size:16px">Your
                MPI job is now going to abort; sorry.</span><br class=""
                style="font-family:Tahoma; font-size:16px">
              <font class="" face="Tahoma" size="3">—————————————————————————————————————</font><br
                class="" style="font-family:Tahoma; font-size:16px">
              <br class="">
              Both procs are on the same host. Since they cannot
              communicate, it means that (a) the shared memory component
              (sm) was unable to be used, and (b) the TCP subsystem did
              not provide a usable address for the two procs to reach
              each other. The former could mean that there wasn’t enough
              room in the tmp directory, and the latter indicates that
              the TCP subsystem isn’t configured to allow connections
              from its own local IP address.</div>
            <div><br class="">
            </div>
            <div>I don’t know anything about Windows configuration I’m
              afraid.</div>
            <div><br class="">
              <br class="">
              <blockquote type="cite" class="">
                <div class="">
                  <div class="" style="font-style:normal;
                    font-weight:normal; letter-spacing:normal;
                    orphans:auto; text-align:start; text-indent:0px;
                    text-transform:none; white-space:normal;
                    widows:auto; word-spacing:0px; direction:ltr;
                    font-family:Tahoma; font-size:12pt">
                    Please recall, this is using the precompiled OpenMpi
                    Windows installation<br class="">
                    <br class="">
                    When the 'verbose' option is added, I see this
                    sequence of output for the scheduler and each of the
                    executor processes:<br class="">
                    ------<br class="">
                    [sweet1:06412] mca: base: components_open: Looking
                    for btl components<br class="">
                    [sweet1:06412] mca: base: components_open: opening
                    btl components<br class="">
                    [sweet1:06412] mca: base: components_open: found
                    loaded component tcp<br class="">
                    [sweet1:06412] mca: base: components_open: component
                    tcp register function successful<br class="">
                    [sweet1:06412] mca: base: components_open: component
                    tcp open function successful<br class="">
                    [sweet1:06412] mca: base: components_open: found
                    loaded component sm<br class="">
                    [sweet1:06412] mca: base: components_open: component
                    sm has no register function<br class="">
                    [sweet1:06412] mca: base: components_open: component
                    sm open function successful<br class="">
                    [sweet1:06412] mca: base: components_open: found
                    loaded component self<br class="">
                    [sweet1:06412] mca: base: components_open: component
                    self has no register function<br class="">
                    [sweet1:06412] mca: base: components_open: component
                    self open function successful<br class="">
                    [sweet1:06412] select: initializing btl component
                    tcp<br class="">
                    [sweet1:06412] select: init of component tcp
                    returned success<br class="">
                    [sweet1:06412] select: initializing btl component sm<br
                      class="">
                    [sweet1:06412] select: init of component sm returned
                    success<br class="">
                    [sweet1:06412] select: initializing btl component
                    self<br class="">
                    [sweet1:06412] select: init of component self
                    returned success<br class="">
                    -------<br class="">
                    <br class="">
                    This output appears to show the btl components for
                    TCP, SM and Self are all available, but this is
                    contradicted the error message shown in the initial
                    post  ("At least one pair of MPI processes are
                    unable to reach each other for MPI
                    communications....")<br class="">
                    <br class="">
                    If there is some sort of misconfiguration present,
                    do you have a suggestion for correcting the
                    situation?<br class="">
                    <br class="">
                    <div class="" style="font-family:'Times New Roman';
                      font-size:16px">
                      <hr tabindex="-1" class="">
                      <div id="divRpF549668" class=""
                        style="direction:ltr"><font class=""
                          face="Tahoma" size="2"><b class="">From:</b><span
                            class="Apple-converted-space"> </span>users
                          [<a moz-do-not-send="true"
                            href="mailto:users-bounces@open-mpi.org"
                            class="" target="_blank">users-bounces@open-mpi.org</a>]
                          on behalf of Ralph Castain [<a
                            moz-do-not-send="true"
                            href="mailto:rhc@open-mpi.org" class=""
                            target="_blank"><a class="moz-txt-link-abbreviated" href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a></a>]<br
                            class="">
                          <b class="">Sent:</b><span
                            class="Apple-converted-space"> </span>Tuesday,
                          June 07, 2016 3:39 PM<br class="">
                          <b class="">To:</b><span
                            class="Apple-converted-space"> </span>Open
                          MPI Users<br class="">
                          <b class="">Subject:</b><span
                            class="Apple-converted-space"> </span>Re:
                          [OMPI users] Processes unable to communicate
                          when using MPI_Comm_spawn on Windows<br
                            class="">
                        </font><br class="">
                      </div>
                      <div class="">Just looking at this output, it
                        would appear that Windows is configured in a way
                        that prevents the procs from connecting to each
                        other via TCP while on the same node, and shared
                        memory is disqualifying itself - which leaves no
                        way for two procs on the same node to
                        communicate.
                        <div class=""><br class="">
                        </div>
                        <div class=""><br class="">
                          <div class="">
                            <blockquote type="cite" class="">
                              <div class="">On Jun 7, 2016, at 12:16 PM,
                                Roth, Christopher &lt;<a
                                  moz-do-not-send="true"
                                  href="mailto:CRoth@aer.com" class=""
                                  target="_blank"><a class="moz-txt-link-abbreviated" href="mailto:CRoth@aer.com">CRoth@aer.com</a></a>&gt;
                                wrote:</div>
                              <br class="Apple-interchange-newline">
                              <div class="">
                                <div class="" style="font-style:normal;
                                  font-weight:normal;
                                  letter-spacing:normal; orphans:auto;
                                  text-align:start; text-indent:0px;
                                  text-transform:none;
                                  white-space:normal; widows:auto;
                                  word-spacing:0px; direction:ltr;
                                  font-family:Tahoma; font-size:12pt">
                                  <div class="" style="direction:ltr;
                                    font-family:Tahoma; font-size:12pt">I
                                    have developed a set of C++ MPI
                                    programs for performing a series of
                                    scientific calculations.  The master
                                    'scheduler' program spawns off sets
                                    of parallelized 'executor' programs
                                    using the MPI_Comm_spawn routine;
                                    these executors communicate back and
                                    forth with the scheduler (only small
                                    amounts of information) via
                                    MPI_Bcast, MPI_Recv and MPI_Send
                                    routines (the 'C' library versions).<br
                                      class="">
                                    <br class="">
                                    This software was originally
                                    developed on a multi-core Linux
                                    machine using OpenMpi v1.5.2, and
                                    works extremely well; now I'm
                                    attempting to port it to multi-core
                                    Windows 7 machine, using Visual
                                    Studios 2012 and the precompiled
                                    OpenMpi v1.6.2 Windows release.  It
                                    all compiles and links properly
                                    under VS2012.<br class="">
                                    When attempting to run this software
                                    on the Windows machine, the
                                    scheduler program is able to spawn
                                    off the executor programs as
                                    intended, but everything chokes when
                                    scheduler sends its initial
                                    broadcast.  There is slightly
                                    different behavior when launching
                                    the scheduler via 'mpirun', or just
                                    by itself, as shown in the logs
                                    below:<br class="">
                                    (the warning about the 127.0.0.1
                                    address is benign - there is no
                                    loopback on Windows)<br class="">
                                    <br class="">
C:\Users\cjr\Desktop\mpi_demo&gt;mpirun -np 1 mpi_scheduler.exe<br
                                      class="">
                                     scheduler: MPI_Init<br class="">
--------------------------------------------------------------------------<br
                                      class="">
                                    WARNING: An invalid value was given
                                    for btl_tcp_if_exclude.  This<br
                                      class="">
                                    value will be ignored.<br class="">
                                    <br class="">
                                      Local host: sweet1<br class="">
                                      Value:      127.0.0.1/8<br
                                      class="">
                                      Message:    Did not find interface
                                    matching this subnet<br class="">
--------------------------------------------------------------------------<br
                                      class="">
                                    --&gt;MPI_COMM_WORLD size = 1<br
                                      class="">
                                    parent: MPI_UNIVERSE_SIZE = 1<br
                                      class="">
                                    scheduler: calling MPI_Comm_spawn
                                    for 4 instances of
                                    'mpi_executor.exe'<br class="">
                                     executor: MPI_Init<br class="">
                                     executor: MPI_Init<br class="">
                                     executor: MPI_Init<br class="">
                                     executor: MPI_Init<br class="">
[sweet1][[20141,1],0][..\..\..\openmpi-1.6.2\ompi\mca\btl\tcp\btl_tcp_proc.c:128:..\..\..\openmpi-1.6.2\ompi\mca\btl\tcp<br
                                      class="">
                                    \btl_tcp_proc.c]
                                    mca_base_modex_recv: failed with
                                    return value=-13<br class="">
--------------------------------------------------------------------------<br
                                      class="">
                                    At least one pair of MPI processes
                                    are unable to reach each other for<br
                                      class="">
                                    MPI communications.  This means that
                                    no Open MPI device has indicated<br
                                      class="">
                                    that it can be used to communicate
                                    between these processes.  This is<br
                                      class="">
                                    an error; Open MPI requires that all
                                    MPI processes be able to reach<br
                                      class="">
                                    each other.  This error can
                                    sometimes be the result of
                                    forgetting to<br class="">
                                    specify the "self" BTL.<br class="">
                                    <br class="">
                                      Process 1 ([[20141,1],0]) is on
                                    host: sweet1<br class="">
                                      Process 2 ([[20141,2],0]) is on
                                    host: sweet1<br class="">
                                      BTLs attempted: tcp sm self<br
                                      class="">
                                    <br class="">
                                    Your MPI job is now going to abort;
                                    sorry.<br class="">
--------------------------------------------------------------------------<br
                                      class="">
                                     subtask rank = 1 out of 4<br
                                      class="">
                                     subtask rank = 2 out of 4<br
                                      class="">
                                     subtask rank = 0 out of 4<br
                                      class="">
                                     subtask rank = 3 out of 4<br
                                      class="">
                                    <br class="">
                                    scheduler: MPI_Comm_spawn completed<br
                                      class="">
                                     scheduler broadcasting function
                                    string length = 4<br class="">
                                    child: MPI_UNIVERSE_SIZE = 4<br
                                      class="">
                                    child: MPI_UNIVERSE_SIZE = 4<br
                                      class="">
                                    child: MPI_UNIVERSE_SIZE = 4<br
                                      class="">
                                    child: MPI_UNIVERSE_SIZE = 4<br
                                      class="">
                                    Proc0 wait for first broadcast<br
                                      class="">
                                    Proc1 wait for first broadcast<br
                                      class="">
                                    Proc2 wait for first broadcast<br
                                      class="">
                                    Proc3 wait for first broadcast<br
                                      class="">
                                    [sweet1:6800] *** An error occurred
                                    in MPI_Bcast<br class="">
                                    [sweet1:6800] *** on communicator<br
                                      class="">
                                    [sweet1:6800] *** MPI_ERR_INTERN:
                                    internal error<br class="">
                                    [sweet1:6800] ***
                                    MPI_ERRORS_ARE_FATAL: your MPI job
                                    will now abort<br class="">
                                    [sweet1:02412]
                                    [[20141,0],0]-[[20141,1],0]
                                    mca_oob_tcp_msg_recv: readv failed:
                                    Unknown error (108)<br class="">
                                    [sweet1:02412] 4 more processes have
                                    sent help message
                                    help-mpi-btl-tcp.txt / invalid
                                    if_inexclude<br class="">
                                    [sweet1:02412] Set MCA parameter
                                    "orte_base_help_aggregate" to 0 to
                                    see all help / error messages<br
                                      class="">
--------------------------------------------------------------------------<br
                                      class="">
                                    WARNING: A process refused to die!<br
                                      class="">
                                    <br class="">
                                    Host: sweet1<br class="">
                                    PID:  524<br class="">
                                    <br class="">
                                    This process may still be running
                                    and/or consuming resources.<br
                                      class="">
                                    <br class="">
--------------------------------------------------------------------------<br
                                      class="">
                                    [sweet1:02412]
                                    [[20141,0],0]-[[20141,2],1]
                                    mca_oob_tcp_msg_recv: readv failed:
                                    Unknown error (108)<br class="">
                                    [sweet1:02412]
                                    [[20141,0],0]-[[20141,2],0]
                                    mca_oob_tcp_msg_recv: readv failed:
                                    Unknown error (108)<br class="">
                                    [sweet1:02412]
                                    [[20141,0],0]-[[20141,2],2]
                                    mca_oob_tcp_msg_recv: readv failed:
                                    Unknown error (108)<br class="">
--------------------------------------------------------------------------<br
                                      class="">
                                    mpirun has exited due to process
                                    rank 0 with PID 488 on<br class="">
                                    node sweet1 exiting improperly.
                                    There are two reasons this could
                                    occur:<br class="">
                                    <br class="">
                                    1. this process did not call "init"
                                    before exiting, but others in<br
                                      class="">
                                    the job did. This can cause a job to
                                    hang indefinitely while it waits<br
                                      class="">
                                    for all processes to call "init". By
                                    rule, if one process calls "init",<br
                                      class="">
                                    then ALL processes must call "init"
                                    prior to termination.<br class="">
                                    <br class="">
                                    2. this process called "init", but
                                    exited without calling "finalize".<br
                                      class="">
                                    By rule, all processes that call
                                    "init" MUST call "finalize" prior to<br
                                      class="">
                                    exiting or it will be considered an
                                    "abnormal termination"<br class="">
                                    <br class="">
                                    This may have caused other processes
                                    in the application to be<br class="">
                                    terminated by signals sent by mpirun
                                    (as reported here).<br class="">
--------------------------------------------------------------------------<br
                                      class="">
                                    [sweet1:02412] 3 more processes have
                                    sent help message
                                    help-odls-default.txt /
                                    odls-default:could-not-kill<br
                                      class="">
                                    <br class="">
                                    C:\Users\cjr\Desktop\mpi_demo&gt;<br
                                      class="">
                                    <br class="">
====================================================<br class="">
                                    <br class="">
C:\Users\cjr\Desktop\mpi_demo&gt;mpi_scheduler.exe<br class="">
                                     scheduler: MPI_Init<br class="">
--------------------------------------------------------------------------<br
                                      class="">
                                    WARNING: An invalid value was given
                                    for btl_tcp_if_exclude.  This<br
                                      class="">
                                    value will be ignored.<br class="">
                                    <br class="">
                                      Local host: sweet1<br class="">
                                      Value:      127.0.0.1/8<br
                                      class="">
                                      Message:    Did not find interface
                                    matching this subnet<br class="">
--------------------------------------------------------------------------<br
                                      class="">
                                    --&gt;MPI_COMM_WORLD size = 1<br
                                      class="">
                                    parent: MPI_UNIVERSE_SIZE = 1<br
                                      class="">
                                    scheduler: calling MPI_Comm_spawn
                                    for 4 instances of
                                    'mpi_executor.exe'<br class="">
                                     executor: MPI_Init<br class="">
                                     executor: MPI_Init<br class="">
                                     executor: MPI_Init<br class="">
                                     executor: MPI_Init<br class="">
                                    [sweet1:04400] 1 more process has
                                    sent help message
                                    help-mpi-btl-tcp.txt / invalid
                                    if_inexclude<br class="">
                                    [sweet1:04400] Set MCA parameter
                                    "orte_base_help_aggregate" to 0 to
                                    see all help / error messages<br
                                      class="">
                                     subtask rank = 2 out of 4<br
                                      class="">
                                     subtask rank = 1 out of 4<br
                                      class="">
                                     subtask rank = 0 out of 4<br
                                      class="">
                                     subtask rank = 3 out of 4<br
                                      class="">
                                    <br class="">
                                    scheduler: MPI_Comm_spawn completed<br
                                      class="">
                                     scheduler broadcasting function
                                    string length = 4<br class="">
                                    <br class="">
                                    child: MPI_UNIVERSE_SIZE = 4<br
                                      class="">
                                    child: MPI_UNIVERSE_SIZE = 4<br
                                      class="">
                                    child: MPI_UNIVERSE_SIZE = 4<br
                                      class="">
                                    child: MPI_UNIVERSE_SIZE = 4<br
                                      class="">
                                    Proc0 wait for first broadcast<br
                                      class="">
                                    Proc1 wait for first broadcast<br
                                      class="">
                                    Proc2 wait for first broadcast<br
                                      class="">
                                    Proc3 wait for first broadcast<br
                                      class="">
                                    <br class="">
                                    [sweet1:04400] 3 more processes have
                                    sent help message
                                    help-mpi-btl-tcp.txt / invalid
                                    if_inexclude<br class="">
                                    <br class="">
                                    &lt;&lt;&lt;&lt;***mpi_executor.exe
                                    processes are running, but 'hung'
                                    while wating for first
                                    broadcast***&gt;&gt;&gt;&gt;<br
                                      class="">
                                    &lt;&lt;&lt;&lt;***manually killed
                                    one of the 'mpi_executor.exe'
                                    processes; others subsequently
                                    exited***&gt;&gt;&gt;&gt;<br
                                      class="">
                                    <br class="">
                                    [sweet1:04400]
                                    [[22257,0],0]-[[22257,2],3]
                                    mca_oob_tcp_msg_recv: readv failed:
                                    Unknown error (108)<br class="">
--------------------------------------------------------------------------<br
                                      class="">
                                    WARNING: A process refused to die!<br
                                      class="">
                                    <br class="">
                                    Host: sweet1<br class="">
                                    PID:  568<br class="">
                                    <br class="">
                                    This process may still be running
                                    and/or consuming resources.<br
                                      class="">
                                    <br class="">
--------------------------------------------------------------------------<br
                                      class="">
                                    [sweet1:04400]
                                    [[22257,0],0]-[[22257,2],0]
                                    mca_oob_tcp_msg_recv: readv failed:
                                    Unknown error (108)<br class="">
                                    [sweet1:04400]
                                    [[22257,0],0]-[[22257,2],1]
                                    mca_oob_tcp_msg_recv: readv failed:
                                    Unknown error (108)<br class="">
                                    [sweet1:04400] 2 more processes have
                                    sent help message
                                    help-odls-default.txt /
                                    odls-default:could-not-kill<br
                                      class="">
                                    <br class="">
                                    C:\Users\cjr\Desktop\mpi_demo&gt;<br
                                      class="">
                                    <br class="">
================================================<br class="">
                                    <br class="">
                                    The addition of the mpirun option
                                    "-mca btl_tcp_if_exclude none"
                                    eliminates the benign 127.0.0.1
                                    warning; the option "-mca
                                    btl_base_verbose 100" adds output
                                    that verifies that the tcp, sm and
                                    self btl modules are successfully
                                    initialized - nothing else seems to
                                    be amiss!<br class="">
                                    I have also tested this with the
                                    firewall completely disabled on the
                                    Windows machine, with no change in
                                    behavior.<br class="">
                                    <br class="">
                                    I have been unable to determine what
                                    the error codes indicate, and am
                                    puzzled why the behavior is
                                    different when using the 'mpirun'
                                    launcher.<br class="">
                                    I have attached the prototype
                                    scheduler and executor program
                                    source code files, as well as files
                                    containing the Windows installation
                                    ompi information.<br class="">
                                    <br class="">
                                    Please help me figure out what is
                                    needed to enable this MPI
                                    communication.<br class="">
                                    <br class="">
                                    Thanks,<br class="">
                                    CJ Roth<br class="">
                                  </div>
                                </div>
                                <br class=""
                                  style="font-family:Helvetica;
                                  font-size:12px; font-style:normal;
                                  font-weight:normal;
                                  letter-spacing:normal; orphans:auto;
                                  text-align:start; text-indent:0px;
                                  text-transform:none;
                                  white-space:normal; widows:auto;
                                  word-spacing:0px">
                                <hr class=""
                                  style="font-family:Helvetica;
                                  font-size:12px; font-style:normal;
                                  font-weight:normal;
                                  letter-spacing:normal; orphans:auto;
                                  text-align:start; text-indent:0px;
                                  text-transform:none;
                                  white-space:normal; widows:auto;
                                  word-spacing:0px">
                                <font class="" style="font-style:normal;
                                  font-weight:normal;
                                  letter-spacing:normal; orphans:auto;
                                  text-align:start; text-indent:0px;
                                  text-transform:none;
                                  white-space:normal; widows:auto;
                                  word-spacing:0px" face="Arial"
                                  color="Gray" size="1"><br class="">
                                  This email is intended solely for the
                                  recipient. It may contain privileged,
                                  proprietary or confidential
                                  information or material. If you are
                                  not the intended recipient, please
                                  delete this email and any attachments
                                  and notify the sender of the error.<br
                                    class="">
                                </font><span class=""
                                  style="font-family:Helvetica;
                                  font-size:12px; font-style:normal;
                                  font-weight:normal;
                                  letter-spacing:normal; orphans:auto;
                                  text-align:start; text-indent:0px;
                                  text-transform:none;
                                  white-space:normal; widows:auto;
                                  word-spacing:0px; float:none;
                                  display:inline!important"></span><span
id="cid:E59D0D70-4035-4F3B-8716-3D8C811BA301" class="">&lt;mpi_scheduler.cpp&gt;</span><span
id="cid:01F750A5-28D4-4D9E-84BF-91A8C9EC47CC" class="">&lt;mpi_executor.cpp&gt;</span><span
id="cid:3396E384-451C-4648-92FD-8B6D1F16F19E" class="">&lt;ompi_info-all.txt&gt;</span><span
id="cid:170EA87A-7BFD-4AEE-8AAF-544A82C15EEC" class="">&lt;ompi_btl_info.txt&gt;</span><span
                                  class="" style="font-family:Helvetica;
                                  font-size:12px; font-style:normal;
                                  font-weight:normal;
                                  letter-spacing:normal; orphans:auto;
                                  text-align:start; text-indent:0px;
                                  text-transform:none;
                                  white-space:normal; widows:auto;
                                  word-spacing:0px; float:none;
                                  display:inline!important">_______________________________________________</span><br
                                  class="" style="font-family:Helvetica;
                                  font-size:12px; font-style:normal;
                                  font-weight:normal;
                                  letter-spacing:normal; orphans:auto;
                                  text-align:start; text-indent:0px;
                                  text-transform:none;
                                  white-space:normal; widows:auto;
                                  word-spacing:0px">
                                <span class=""
                                  style="font-family:Helvetica;
                                  font-size:12px; font-style:normal;
                                  font-weight:normal;
                                  letter-spacing:normal; orphans:auto;
                                  text-align:start; text-indent:0px;
                                  text-transform:none;
                                  white-space:normal; widows:auto;
                                  word-spacing:0px; float:none;
                                  display:inline!important">users
                                  mailing list</span><br class=""
                                  style="font-family:Helvetica;
                                  font-size:12px; font-style:normal;
                                  font-weight:normal;
                                  letter-spacing:normal; orphans:auto;
                                  text-align:start; text-indent:0px;
                                  text-transform:none;
                                  white-space:normal; widows:auto;
                                  word-spacing:0px">
                                <a moz-do-not-send="true"
                                  href="mailto:users@open-mpi.org"
                                  class="" style="font-family:Helvetica;
                                  font-size:12px; font-style:normal;
                                  font-weight:normal;
                                  letter-spacing:normal; orphans:auto;
                                  text-align:start; text-indent:0px;
                                  text-transform:none;
                                  white-space:normal; widows:auto;
                                  word-spacing:0px" target="_blank">users@open-mpi.org</a><br
                                  class="" style="font-family:Helvetica;
                                  font-size:12px; font-style:normal;
                                  font-weight:normal;
                                  letter-spacing:normal; orphans:auto;
                                  text-align:start; text-indent:0px;
                                  text-transform:none;
                                  white-space:normal; widows:auto;
                                  word-spacing:0px">
                                <span class=""
                                  style="font-family:Helvetica;
                                  font-size:12px; font-style:normal;
                                  font-weight:normal;
                                  letter-spacing:normal; orphans:auto;
                                  text-align:start; text-indent:0px;
                                  text-transform:none;
                                  white-space:normal; widows:auto;
                                  word-spacing:0px; float:none;
                                  display:inline!important">Subscription:<span
                                    class="Apple-converted-space"> </span></span><a
                                  moz-do-not-send="true"
                                  href="https://www.open-mpi.org/mailman/listinfo.cgi/users"
                                  class="" style="font-family:Helvetica;
                                  font-size:12px; font-style:normal;
                                  font-weight:normal;
                                  letter-spacing:normal; orphans:auto;
                                  text-align:start; text-indent:0px;
                                  text-transform:none;
                                  white-space:normal; widows:auto;
                                  word-spacing:0px" target="_blank"><a class="moz-txt-link-freetext" href="https://www.open-mpi.org/mailman/listinfo.cgi/users">https://www.open-mpi.org/mailman/listinfo.cgi/users</a></a><br
                                  class="" style="font-family:Helvetica;
                                  font-size:12px; font-style:normal;
                                  font-weight:normal;
                                  letter-spacing:normal; orphans:auto;
                                  text-align:start; text-indent:0px;
                                  text-transform:none;
                                  white-space:normal; widows:auto;
                                  word-spacing:0px">
                                <span class=""
                                  style="font-family:Helvetica;
                                  font-size:12px; font-style:normal;
                                  font-weight:normal;
                                  letter-spacing:normal; orphans:auto;
                                  text-align:start; text-indent:0px;
                                  text-transform:none;
                                  white-space:normal; widows:auto;
                                  word-spacing:0px; float:none;
                                  display:inline!important">Link to this
                                  post:<span
                                    class="Apple-converted-space"> </span></span><a
                                  moz-do-not-send="true"
                                  href="http://www.open-mpi.org/community/lists/users/2016/06/29395.php"
                                  class="" style="font-family:Helvetica;
                                  font-size:12px; font-style:normal;
                                  font-weight:normal;
                                  letter-spacing:normal; orphans:auto;
                                  text-align:start; text-indent:0px;
                                  text-transform:none;
                                  white-space:normal; widows:auto;
                                  word-spacing:0px" target="_blank"><a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/06/29395.php">http://www.open-mpi.org/community/lists/users/2016/06/29395.php</a></a></div>
                            </blockquote>
                          </div>
                          <br class="">
                        </div>
                      </div>
                    </div>
                  </div>
                  <span class="" style="font-family:Helvetica;
                    font-size:12px; font-style:normal;
                    font-weight:normal; letter-spacing:normal;
                    orphans:auto; text-align:start; text-indent:0px;
                    text-transform:none; white-space:normal;
                    widows:auto; word-spacing:0px; float:none;
                    display:inline!important">_______________________________________________</span><br
                    class="" style="font-family:Helvetica;
                    font-size:12px; font-style:normal;
                    font-weight:normal; letter-spacing:normal;
                    orphans:auto; text-align:start; text-indent:0px;
                    text-transform:none; white-space:normal;
                    widows:auto; word-spacing:0px">
                  <span class="" style="font-family:Helvetica;
                    font-size:12px; font-style:normal;
                    font-weight:normal; letter-spacing:normal;
                    orphans:auto; text-align:start; text-indent:0px;
                    text-transform:none; white-space:normal;
                    widows:auto; word-spacing:0px; float:none;
                    display:inline!important">users mailing list</span><br
                    class="" style="font-family:Helvetica;
                    font-size:12px; font-style:normal;
                    font-weight:normal; letter-spacing:normal;
                    orphans:auto; text-align:start; text-indent:0px;
                    text-transform:none; white-space:normal;
                    widows:auto; word-spacing:0px">
                  <a moz-do-not-send="true"
                    href="mailto:users@open-mpi.org" class=""
                    style="font-family:Helvetica; font-size:12px;
                    font-style:normal; font-weight:normal;
                    letter-spacing:normal; orphans:auto;
                    text-align:start; text-indent:0px;
                    text-transform:none; white-space:normal;
                    widows:auto; word-spacing:0px" target="_blank">users@open-mpi.org</a><br
                    class="" style="font-family:Helvetica;
                    font-size:12px; font-style:normal;
                    font-weight:normal; letter-spacing:normal;
                    orphans:auto; text-align:start; text-indent:0px;
                    text-transform:none; white-space:normal;
                    widows:auto; word-spacing:0px">
                  <span class="" style="font-family:Helvetica;
                    font-size:12px; font-style:normal;
                    font-weight:normal; letter-spacing:normal;
                    orphans:auto; text-align:start; text-indent:0px;
                    text-transform:none; white-space:normal;
                    widows:auto; word-spacing:0px; float:none;
                    display:inline!important">Subscription:<span
                      class="Apple-converted-space"> </span></span><a
                    moz-do-not-send="true"
                    href="https://www.open-mpi.org/mailman/listinfo.cgi/users"
                    class="" style="font-family:Helvetica;
                    font-size:12px; font-style:normal;
                    font-weight:normal; letter-spacing:normal;
                    orphans:auto; text-align:start; text-indent:0px;
                    text-transform:none; white-space:normal;
                    widows:auto; word-spacing:0px" target="_blank"><a class="moz-txt-link-freetext" href="https://www.open-mpi.org/mailman/listinfo.cgi/users">https://www.open-mpi.org/mailman/listinfo.cgi/users</a></a><br
                    class="" style="font-family:Helvetica;
                    font-size:12px; font-style:normal;
                    font-weight:normal; letter-spacing:normal;
                    orphans:auto; text-align:start; text-indent:0px;
                    text-transform:none; white-space:normal;
                    widows:auto; word-spacing:0px">
                  <span class="" style="font-family:Helvetica;
                    font-size:12px; font-style:normal;
                    font-weight:normal; letter-spacing:normal;
                    orphans:auto; text-align:start; text-indent:0px;
                    text-transform:none; white-space:normal;
                    widows:auto; word-spacing:0px; float:none;
                    display:inline!important">Link to this post:<span
                      class="Apple-converted-space"> </span></span><a
                    moz-do-not-send="true"
                    href="http://www.open-mpi.org/community/lists/users/2016/06/29408.php"
                    class="" style="font-family:Helvetica;
                    font-size:12px; font-style:normal;
                    font-weight:normal; letter-spacing:normal;
                    orphans:auto; text-align:start; text-indent:0px;
                    text-transform:none; white-space:normal;
                    widows:auto; word-spacing:0px" target="_blank"><a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/06/29408.php">http://www.open-mpi.org/community/lists/users/2016/06/29408.php</a></a></div>
              </blockquote>
            </div>
            <br class="">
          </div>
        </div>
      </div>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="https://www.open-mpi.org/mailman/listinfo.cgi/users">https://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/06/29412.php">http://www.open-mpi.org/community/lists/users/2016/06/29412.php</a></pre>
    </blockquote>
    <br>
  </body>
</html>

