<div dir="ltr"><div class="gmail_extra"><div class="gmail_quote">On Thu, Sep 24, 2015 at 12:10 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div style="word-wrap:break-word">Ah, sorry - wrong param. It’s the out-of-band that is having the problem. Try adding —mca oob_tcp_if_include &lt;foo&gt;</div></blockquote><div><br></div><div>Ooh. Okay. Look at this:</div><div><br></div><div><div><font face="monospace, monospace">(13) $ mpirun --mca oob_tcp_if_include ib0 -np 2 ./helloWorld.x</font></div><div><font face="monospace, monospace">Process 1 of 2 is on r509i2n17 </font></div><div><font face="monospace, monospace">Process 0 of 2 is on r509i2n17 </font></div></div><div><br></div><div>So that is nice. Now the spin up if I have 8 or so nodes is rather...slow. But at this point I&#39;ll take working over efficient. Quick startup can come later.</div><div><br></div><div>Matt</div><div><br></div><div> </div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div style="word-wrap:break-word"><div><br></div><div><br><div><blockquote type="cite"><div><div class="h5"><div>On Sep 24, 2015, at 8:56 AM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com" target="_blank">fortran@gmail.com</a>&gt; wrote:</div><br></div></div><div><div><div class="h5"><div dir="ltr">Ralph,<div><br></div><div>I believe these nodes might have both an Ethernet and Infiniband port where the Ethernet port is not the one to use. Is there a way to tell Open MPI to ignore any ethernet devices it sees? I&#39;ve tried:</div><pre>--mca btl sm,openib,self</pre><div>and (based on the advice of the much more intelligent support at NAS):</div><div><pre>--mca btl openib,self --mca btl_openib_if_include mlx4_0,mlx4_1
</pre></div><div>But neither worked.</div><div><br></div><div>Matt</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Thu, Sep 24, 2015 at 11:41 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div style="word-wrap:break-word">Starting in the 1.7 series, OMPI by default launches daemons on all nodes in the allocation during startup. This is done so we can “probe” the topology of the nodes and use that info during the process mapping procedure - e.g., if you want to map-by NUMA regions.<div><br></div><div>What is happening here is that some of the nodes in your allocation aren’t allowing those daemons to callback to mpirun. Either a firewall is in the way, or something is preventing it.</div><div><br></div><div>If you don’t want to launch on those other nodes, you could just add —novm to your cmd line, or use the —host option to restrict us to your local node. However, I imagine you got the bigger allocation so you could use it :-)</div><div><br></div><div>In which case, you need to remove the obstacle. You might check for firewall, or check to see if multiple NICs are on the non-maia nodes (this can sometimes confuse things, especially if someone put the NICs on the same IP subnet)</div><div><br></div><div>HTH</div><div>Ralph</div><div><br></div><div><br></div><div><br><div><blockquote type="cite"><div><div><div>On Sep 24, 2015, at 8:18 AM, Matt Thompson &lt;<a href="mailto:fortran@gmail.com" target="_blank">fortran@gmail.com</a>&gt; wrote:</div><br></div></div><div><div><div><div dir="ltr">Open MPI Users,<div><br></div><div>I&#39;m hoping someone here can help. I built Open MPI 1.10.0 with PGI 15.7 using this configure string:</div><div><br></div><div><div><font face="monospace, monospace"> ./configure --disable-vt --with-tm=/PBS --with-verbs --disable-wrapper-rpath \</font></div><div><font face="monospace, monospace">    CC=pgcc CXX=pgCC FC=pgf90 F77=pgf77 CFLAGS=&#39;-fpic -m64&#39; \</font></div><div><font face="monospace, monospace">    CXXFLAGS=&#39;-fpic -m64&#39; FCFLAGS=&#39;-fpic -m64&#39; FFLAGS=&#39;-fpic -m64&#39; \</font></div><div><font face="monospace, monospace">    --prefix=/nobackup/gmao_SIteam/MPI/pgi_15.7-openmpi_1.10.0 |&amp; tee configure.pgi15.7.log</font></div><div><br></div><div>It seemed to pass &#39;make check&#39;. </div><div><br></div><div>I&#39;m working at pleiades at NAS, and there they have both Sandy Bridge nodes with GPUs (maia) and regular Sandy Bridge compute nodes (here after called Sandy) without. To be extra careful (since PGI compiles to the architecture you build on) I took a Westmere node and built Open MPI there just in case.</div><div><br></div><div>So, as I said, all seems to work with a test. I now grab a maia node, maia1, of an allocation of 4 I had:</div><div><br></div><div><div><font face="monospace, monospace">(102) $ mpicc -tp=px-64 -o helloWorld.x helloWorld.c</font></div><div><font face="monospace, monospace">(103) $ mpirun -np 2 ./helloWorld.x</font></div><div><font face="monospace, monospace">Process 0 of 2 is on maia1 </font></div><div><font face="monospace, monospace">Process 1 of 2 is on maia1 </font></div></div><div><br></div><div>Good. Now, let&#39;s go to a Sandy Bridge (non-GPU) node, r321i7n16, of an allocation of 8 I had:</div><div><br></div><div><div><font face="monospace, monospace">(49) $ mpicc -tp=px-64 -o helloWorld.x helloWorld.c</font></div><div><font face="monospace, monospace">(50) $ mpirun -np 2 ./helloWorld.x</font></div><div><font face="monospace, monospace">[r323i5n11:13063] [[62995,0],7] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n6:57417] [[62995,0],2] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n7:67287] [[62995,0],3] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n8:57429] [[62995,0],4] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n10:35329] [[62995,0],6] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n9:13456] [[62995,0],5] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div></div><div><br></div><div>Hmm. Let&#39;s try turning off tcp (often my first thought when on an Infiniband system):</div><div><br></div><div><div><font face="monospace, monospace">(51) $ mpirun --mca btl sm,openib,self -np 2 ./helloWorld.x</font></div><div><font face="monospace, monospace">[r323i5n6:57420] [[62996,0],2] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n9:13459] [[62996,0],5] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n8:57432] [[62996,0],4] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n7:67290] [[62996,0],3] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n11:13066] [[62996,0],7] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div><div><font face="monospace, monospace">[r323i5n10:35332] [[62996,0],6] tcp_peer_send_blocking: send() to socket 9 failed: Broken pipe (32)</font></div></div><div><br></div><div>Now, the nodes reporting the issue seem to be the &quot;other&quot; nodes on the allocation that are in a different rack:</div><div><br></div><div><div><font face="monospace, monospace">(52) $ cat $PBS_NODEFILE | uniq</font></div><div><font face="monospace, monospace">r321i7n16</font></div><div><font face="monospace, monospace">r321i7n17</font></div><div><font face="monospace, monospace">r323i5n6</font></div><div><font face="monospace, monospace">r323i5n7</font></div><div><font face="monospace, monospace">r323i5n8</font></div><div><font face="monospace, monospace">r323i5n9</font></div><div><font face="monospace, monospace">r323i5n10</font></div><div><font face="monospace, monospace">r323i5n11</font></div></div><div><br></div><div>Maybe that&#39;s a clue? I didn&#39;t think this would matter if I only ran two processes...and it works on the multi-node maia allocation.</div><div><br></div><div>I&#39;ve tried searching the web, but the only place I&#39;ve seen tcp_peer_send_blocking is in a PDF where they say it&#39;s an error that can be seen:</div><div><br></div><div><a href="http://www.hpc.mcgill.ca/downloads/checkpointing_workshop/20150326%20-%20McGill%20-%20Checkpointing%20Techniques.pdf" target="_blank">http://www.hpc.mcgill.ca/downloads/checkpointing_workshop/20150326%20-%20McGill%20-%20Checkpointing%20Techniques.pdf</a><br></div><div><br></div><div>Any ideas for what this error can mean?</div><div><br></div>-- <br><div><div dir="ltr"><div><div dir="ltr"><div>Matt Thompson</div></div></div><blockquote style="margin:0px 0px 0px 40px;border:none;padding:0px"><div><div><div>Man Among Men</div></div></div><div><div><div>Fulcrum of History</div></div></div></blockquote></div></div>
</div></div></div></div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/09/27669.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/09/27669.php</a></div></blockquote></div><br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/09/27670.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/09/27670.php</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br><div><div dir="ltr"><div><div dir="ltr"><div>Matt Thompson</div></div></div><blockquote style="margin:0px 0px 0px 40px;border:none;padding:0px"><div><div><div>Man Among Men</div></div></div><div><div><div>Fulcrum of History</div></div></div></blockquote></div></div>
</div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/09/27671.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/09/27671.php</a></div></blockquote></div><br></div></div><br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/09/27672.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/09/27672.php</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br><div class="gmail_signature"><div dir="ltr"><div><div dir="ltr"><div>Matt Thompson</div></div></div><blockquote style="margin:0px 0px 0px 40px;border:none;padding:0px"><div><div><div>Man Among Men</div></div></div><div><div><div>Fulcrum of History</div></div></div></blockquote></div></div>
</div></div>

