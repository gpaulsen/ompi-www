Hi <div><br></div><div>I was about <span class="GRcorrect" id="GRmark_4440df55fff08f0745b5861e2fc6713c9b1c3598_tell:0">tell</span> that <span class="GRcorrect" id="GRmark_4440df55fff08f0745b5861e2fc6713c9b1c3598_i:1">i</span> have written an MPI code in which <span class="GRcorrect" id="GRmark_4440df55fff08f0745b5861e2fc6713c9b1c3598_i:2">i</span> have specified only for the communication between nodes and <span class="GRcorrect" id="GRmark_4440df55fff08f0745b5861e2fc6713c9b1c3598_i:3">i</span> <span class="GRcorrect" id="GRmark_4440df55fff08f0745b5861e2fc6713c9b1c3598_dont:4">dont</span> whether can <span class="GRcorrect" id="GRmark_4440df55fff08f0745b5861e2fc6713c9b1c3598_i:5">i</span> run <span class="GRcorrect" id="GRmark_4440df55fff08f0745b5861e2fc6713c9b1c3598_program:6">program</span> as per my requirement <span class="GRcorrect" id="GRmark_4440df55fff08f0745b5861e2fc6713c9b1c3598_i.e:7">i.e</span> to use the cores present in my <span class="GRcorrect" id="GRmark_4440df55fff08f0745b5861e2fc6713c9b1c3598_node:8">node</span> (all 4 cores). So my doubt is should <span class="GRcorrect" id="GRmark_f87f378409c0b9f56ab7f5841dd01f7df727c744_i:0">i</span> need to include <span class="GRcorrect" id="GRmark_f87f378409c0b9f56ab7f5841dd01f7df727c744_pthreads:1">pthreads</span> or the program which <span class="GRcorrect" id="GRmark_f87f378409c0b9f56ab7f5841dd01f7df727c744_i:2">i</span> have written is sufficient.</div>
<div><br></div><div>My program gives the output has MPI-size=4</div><div> <span class="GRcorrect" id="GRmark_3a8e5306d02101fe219cc1cbbc1a8104c3fbaa40_time:0">time</span> at node1:</div><div><span class="GRcorrect" id="GRmark_0c96c5edfa6de64d0adaad349ebbd3ce8703965f_time:0">time</span> at node 2:</div>
<div><span class="GRcorrect" id="GRmark_39dce0341971d772a2e8bd4d617d8830924e8d57_time:0">time</span> at node3:</div><div><span class="GRcorrect" id="GRmark_bb8df4bd5807003ac415c53d10b6fd7a8b1478c6_time:0">time</span> at node 4:</div>
<div><br></div><div><br></div><div>This is what <span class="GRcorrect" id="GRmark_de3bb5fde0d5255a05f7e796f728f2043f8a0b79_i:0">i</span> was explaining.</div><div><br></div><div><br><div class="gmail_quote">On Tue, Aug 14, 2012 at 2:17 PM, Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div class="im">On Aug 14, 2012, at 7:55 AM, <span class="GRcorrect" id="GRmark_97dc340ba6e7191e3519b4cd65b989372161cd37_seshendra:0">seshendra</span> <span class="GRcorrect" id="GRmark_97dc340ba6e7191e3519b4cd65b989372161cd37_seshu:1">seshu</span> wrote:<br>

<br>
&gt; I haven&#39;t still changed my code to run when threading is  needed (presently working).<br>
<br>
</div>I&#39;m afraid I can&#39;t parse that sentence; I don&#39;t know what you mean.<br>
<div class="im"><br>
&gt; I have <span class="GRcorrect" id="GRmark_1c5f1f73ce6aaddbe5593a1205b2f5d7379846c7_doubt:0">doubt</span> that when <span class="GRcorrect" id="GRmark_1c5f1f73ce6aaddbe5593a1205b2f5d7379846c7_i:1">i</span> calculate the MPI ranks using the MPI command  it gives only the nodes which have given in a host file.<br>

<br>
</div>I&#39;m not sure what this is saying, either.  Sorry!  :-(<br>
<br>
If you are running in a scheduled environment (e.g., under the SLURM or Torque/PBS schedulers), then Open MPI will find out from the schedule how many processors are on that machine and react accordingly.<br>
<br>
Otherwise, if you are running in an unscheduled environment (e.g., if you just specify a <span class="GRcorrect" id="GRmark_8ef53730515bb2e929930aedcf51b50472c4dcd2_hostfile:0">hostfile</span> to tell Open MPI where to run), then you need to tell Open MPI how many processes you want it to launch on each node.  Per Tom&#39;s advice, we usually recommend running -- at most -- one MPI process per core.<br>

<div class="im"><br>
&gt; But how can <span class="GRcorrect" id="GRmark_9f64ba27cd200e1ea9ab683ffa11cda2f0e1c457_i:0">i</span> calculate the MPI ranks as you have told i.e N=H<span class="GRcorrect" id="GRmark_9f64ba27cd200e1ea9ab683ffa11cda2f0e1c457_(:1">(</span>    number of CPUs showing on a node ) x M ( number of nodes in a cluster).<br>

<br>
</div>Use the Hwloc utility &quot;<span class="GRcorrect" id="GRmark_c0ea880b771377d7c9783749dd0f2b75e927a0ed_lstopo:0">lstopo</span>&quot; to give you a graphical depiction of the internal topology of your machine(s).  <a href="http://www.open-mpi.org/projects/hwloc/" target="_blank">http://www.open-mpi.org/projects/hwloc/</a><br>

<br>
Then you can count how many cores are on your machine and set your <span class="GRcorrect" id="GRmark_aaac8d04b6d58141baccab3a59c0975d56fabf8b_hostfile:0">hostfile</span> accordingly.<br>
<div class="im"><br>
&gt; And I would like test like this if have 2 nodes and where I can use up to 50 cores. I would like to scale like 2,4,8,16,32,45 cores and at the same time if use 8 cores then I would like to take the readings as node1- 3cores and node2-5 cores. So in order to do that should I need mention anything in the host file as I have mentioned the No.of nodes.<br>

<br>
</div>In general, just list how many slots you have on each machine in the <span class="GRcorrect" id="GRmark_30f373a3d423783c6126b18304c00bc5e80eedce_hostfile:0">hostfile</span> and then use <span class="GRcorrect" id="GRmark_30f373a3d423783c6126b18304c00bc5e80eedce_mpirun&#39;s:1">mpirun&#39;s</span> -<span class="GRcorrect" id="GRmark_30f373a3d423783c6126b18304c00bc5e80eedce_np:2">np</span> and -<span class="GRcorrect" id="GRmark_30f373a3d423783c6126b18304c00bc5e80eedce_npernode:3">npernode</span> options to specify how many total processes to run and how many to run per machine.  For example:<br>

<br>
-----<br>
shell$ cat <span class="GRcorrect" id="GRmark_06e9f9f8ed60122c112eb9a9648c0f51e4896cdf_hostfile:0">hostfile</span><br>
# This <span class="GRcorrect" id="GRmark_3c626aec04ce3cf137cee5b2ae68e603cc861c90_hostfile:0">hostfile</span> represents 2 Intel Romley-based servers; 16 cores each<br>
server1 slots=16<br>
server2 slots=16<br>
shell$ mpirun --hostfile hostfile -np 4 --npernode 2 my_mpi_executable<br>
# ^^ this runs a total of 4 MPI processes, 2 on each node<br>
shell$ mpirun --hostfile hostfile -np 16 --npernode 8 my_mpi_executable<br>
# ^^ this runs a total of 16 MPI processes, 8 on each node<br>
-----<br>
<br>
...and so on.<br>
<span class="HOEnZb"><font color="#888888"><br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
</font></span><div class="HOEnZb"><div class="h5"><br>
<br>
_______________________________________________<br>
<span class="GRcorrect" id="GRmark_b97129d8eeccbeeaaf55ebe03a86c7d0e432f2f2_users:0">users</span> mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br><br clear="all"><div><br></div>-- <br> WITH REGARDS<br>M<span class="GRcorrect" id="GRmark_6c40ef572784c245b17e59ef0746a3312d826496_.:0">.</span>L<span class="GRcorrect" id="GRmark_6c40ef572784c245b17e59ef0746a3312d826496_.:1">.</span>N<span class="GRcorrect" id="GRmark_6c40ef572784c245b17e59ef0746a3312d826496_.:2">.</span>Seshendra<br>

</div>

