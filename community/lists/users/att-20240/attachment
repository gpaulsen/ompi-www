<html><head><meta http-equiv="Content-Type" content="text/html charset=iso-8859-1"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">As the error states, your code is segfaulting - your best way to find out where might be to use a debugger (e.g., gdb) on the core dump, or use a parallel debugger if you have one.<div><br></div><div><br><div><div>On Sep 18, 2012, at 2:14 PM, Alidoust &lt;<a href="mailto:phymalidoust@gmail.com">phymalidoust@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><br><font color="#000099"><font size="4"><font face="garamond,serif">Dear Madam/Sir,<br><br></font></font></font><br><font color="#000099"><font size="4"><font face="garamond,serif">I have a serial Fortran code (f90), dealing with matrix 
diagonalizing subroutines, and recently got its parallel version to be 
faster in some unfeasible parts via the serial program. <br>I have been using the following commands for initializing MPI in the code<br>




---------------<br>&nbsp;&nbsp;&nbsp; call MPI_INIT(ierr)<br>&nbsp;&nbsp;&nbsp; call MPI_COMM_SIZE(MPI_COMM_WORLD, p, ierr)<br>&nbsp;&nbsp;&nbsp; call MPI_COMM_RANK(MPI_COMM_WORLD, my_rank, ierr)<br><br>CPU requirement &gt;&gt; <font color="#000099"><font size="4"><font face="garamond,serif">pmem=1500mb,nodes=5:ppn=8 &lt;&lt;</font></font></font></font></font></font><br>











<font color="#000099"><font size="4"><font face="garamond,serif">-------------------<br>Everything looks OK when matrix dimensions are less than 1000x1000. When I increase 
the matrix dimensions to some larger values the parallel code gets the 
following error<br>



------------------<br></font></font></font><font color="#000099"><font size="4"><font face="garamond,serif"><font color="#000099"><font size="4"><font face="garamond,serif">mpirun noticed that process rank 6 with PID 1566 on node node1082 exited on signal 11 (Segmentation fault)</font></font></font>
<br>------------------<br>There is no such error with the serial version
 even for larger matrix dimensions than 2400x2400. I then thought it 
might be raised by the number of nodes and memory space I'm requiring. 
Then changed it as follows<br>


</font></font></font><br><font color="#000099"><font size="4"><font face="garamond,serif">pmem=10gb,nodes=20:ppn=2<br><br>which is more or less similar to what I'm using for serial jobs (</font></font></font><font color="#000099"><font size="4"><font face="garamond,serif"><font color="#000099"><font size="4"><font face="garamond,serif">mem=10gb,nodes=1:ppn=1)</font></font></font>.
 But the problem persists still. Is there any limitation on MPI 
subroutines for transferring data size or the issue would be raised by 
some cause else? <br>


<br>Best of Regards,<br>Mohammad</font></font></font><br>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
