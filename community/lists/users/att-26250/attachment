<html>
  <head>
    <meta content="text/html; charset=ISO-8859-1"
      http-equiv="Content-Type">
  </head>
  <body text="#000000" bgcolor="#FFFFFF">
    <div class="moz-cite-prefix">Dave,<br>
      <br>
      the QDR Infiniband uses the openib btl (by default :
      btl_openib_exclusivity=1024)<br>
      i assume the RoCE 10Gbps card is using the tcp btl (by default :
      btl_tcp_exclusivity=100)<br>
      <br>
      that means that by default, when both openib and tcp btl could be
      used, the tcp btl is discarded.<br>
      <br>
      could you give a try by settings the same exclusivity value on
      both btl ?<br>
      e.g.<br>
      OMPI_MCA_btl_tcp_exclusivity=1024 mpirun ...<br>
      <br>
      assuming this is enough the get traffic on both interfaces, you
      migh want *not* to use the eth0 interface<br>
      (e.g. OMPI_MCA_btl_tcp_if_exlude=eth0 ...)<br>
      <br>
      you might also have to tweak the bandwidth parameters (i assume
      QDR interface should get 4 times more<br>
      traffic than the 10Gbe interface)<br>
      by default :<br>
      btl_openib_bandwidth=4<br>
      btl_tcp_bandwidth=100<br>
      /* value is in Mbps, so the openib value should be 40960 (!), and
      in your case, tcp bandwidth should be 10240 */<br>
      you might also want to try btl_*_bandwidth=0 (auto-detect value at
      run time)<br>
      <br>
      i hope this helps,<br>
      <br>
      Cheers,<br>
      <br>
      Gilles<br>
      On 2015/01/29 9:45, Dave Turner wrote:<br>
    </div>
    <blockquote
cite="mid:CAFGXdkxAJ6f1Jq_dWe5Y5s8i3Qk8NOxZPRNQ6AajKWE-2jcN0Q@mail.gmail.com"
      type="cite">
      <pre wrap="">     I ran some aggregate bandwidth tests between 2 hosts connected by
both QDR InfiniBand and RoCE enabled 10 Gbps Mellanox cards.  The tests
measured the aggregate performance for 16 cores on one host communicating
with 16 on the second host.  I saw the same performance as with the QDR
InfiniBand alone, so it appears that the addition of the 10 Gbps RoCE cards
is
not helping.

     Should OpenMPI be using both in this case by default, or is there
something
I need to configure to allow for this?  I suppose this is the same question
as
how to make use of 2 identical IB connections on each node, or is the system
simply ignoring the 10 Gbps cards because they are the slower option.

     Any clarification on this would be helpful.  The only posts I've found
are very
old and discuss mostly channel bonding of 1 Gbps cards.

                     Dave Turner

</pre>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2015/01/26243.php">http://www.open-mpi.org/community/lists/users/2015/01/26243.php</a></pre>
    </blockquote>
    <br>
  </body>
</html>

