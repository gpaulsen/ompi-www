<div dir="ltr">If you are using instance types that support SR-IOV (aka. &quot;enhanced networking&quot; in AWS), then turn it on. We saw huge differences when SR-IOV is enabled<div><div class="gmail_extra"><br></div><div class="gmail_extra"><a href="http://blogs.scalablelogic.com/2013/12/enhanced-networking-in-aws-cloud.html">http://blogs.scalablelogic.com/2013/12/enhanced-networking-in-aws-cloud.html</a><br></div><div class="gmail_extra"><a href="http://blogs.scalablelogic.com/2014/01/enhanced-networking-in-aws-cloud-part-2.html">http://blogs.scalablelogic.com/2014/01/enhanced-networking-in-aws-cloud-part-2.html</a><br></div><div class="gmail_extra"><br></div><div class="gmail_extra">Make sure you start your instances with a placement group -- otherwise, the instances can be data centers apart!<br></div><div class="gmail_extra"><br></div><div class="gmail_extra">And check that jumbo frames are enabled properly:</div><div class="gmail_extra"><br></div><div class="gmail_extra"><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/network_mtu.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/network_mtu.html</a></div><div class="gmail_extra"><br></div><div class="gmail_extra">But still, it is interesting that Intel MPI is getting a 2X speedup with the same setup! Can you post the raw numbers so that we can take a deeper look??</div><div class="gmail_extra"><br clear="all"><div><div class="gmail_signature">Rayson<br><br>==================================================<br>Open Grid Scheduler - The Official Open Source Grid Engine<br><a href="http://gridscheduler.sourceforge.net/" target="_blank">http://gridscheduler.sourceforge.net/</a><br><a href="http://gridscheduler.sourceforge.net/GridEngine/GridEngineCloud.html" target="_blank">http://gridscheduler.sourceforge.net/GridEngine/GridEngineCloud.html</a></div></div>
<div class="gmail_extra"><br></div><div class="gmail_extra"><br></div><div class="gmail_extra"><br></div><br><div class="gmail_quote">On Tue, Mar 8, 2016 at 9:08 AM, Jackson, Gary L. <span dir="ltr">&lt;<a href="mailto:Gary.Jackson@jhuapl.edu" target="_blank">Gary.Jackson@jhuapl.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">



<div style="word-wrap:break-word;color:rgb(0,0,0);font-size:14px;font-family:Calibri,sans-serif">
<div><br>
</div>
<div>I&#39;ve built OpenMPI 1.10.1 on Amazon EC2. Using NetPIPE, I&#39;m seeing about half the performance for MPI over TCP as I do with raw TCP. Before I start digging in to this more deeply, does anyone know what might cause that?</div>
<div><br>
</div>
<div>For what it&#39;s worth, I see the same issues with MPICH, but I do not see it with Intel MPI.</div><span class=""><font color="#888888">
<div><br>
</div>
<div>
<div>--Â </div>
<div>Gary Jackson</div>
<div><br>
</div>
</div>
</font></span></div>

<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/03/28659.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/03/28659.php</a><br></blockquote></div><br></div></div></div>

