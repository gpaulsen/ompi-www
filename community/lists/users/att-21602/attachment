<html><head><meta http-equiv="Content-Type" content="text/html charset=iso-8859-1"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><br><div><div>On Mar 22, 2013, at 3:42 AM, Syed Ahsan Ali &lt;<a href="mailto:ahsanshah01@gmail.com">ahsanshah01@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr"><div>Actually due to some data base corruption I am not able to add any new node to cluster from the installer node. So I want to run parallel job on more nodes without adding them to existing cluster.</div>
<div>You are right the binaries must be present on the remote node as well. </div><div>Is this possible throught nfs? just as the compute nodes are nfs mounted with the installer node.</div></div></blockquote><div><br></div>Sure - OMPI doesn't care how the binaries got there. Just so long as they are present on the compute node.</div><div><br><blockquote type="cite"><div dir="ltr"><div>&nbsp;</div><div>Ahsan</div></div>
<div class="gmail_extra"><br><br><div class="gmail_quote">On Fri, Mar 22, 2013 at 3:33 PM, Reuti <span dir="ltr">&lt;<a href="mailto:reuti@staff.uni-marburg.de" target="_blank">reuti@staff.uni-marburg.de</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Am 22.03.2013 um 10:14 schrieb Syed Ahsan Ali:<br>
<div class="im"><br>
&gt; I have a very basic question. If we want to run mpirun job on two systems which are not part of cluster, then how we can make it possible. Can the host be specifiend on mpirun which is not compute node, rather a stand alone system.<br>

<br>
</div>Sure, the machines can be specified as argument to `mpiexec`. But do you want to run applications just between these two machines, or should they participate on a larger parallel job with machines of the cluster: then a direct network connection between outside and inside of the cluster is necessary by some kind of forwarding in case these are separated networks.<br>

<br>
Also the paths to the started binaries may be different, in case the two machines are not sharing the same /home with the cluster and this needs to be honored.<br>
<br>
In case you are using a queuing system and want to route jobs to outside machines of the set up cluster: it's necessary to negotiate with the admin to allow jobs being scheduled thereto.<br>
<br>
-- Reuti<br>
<br>
<br>
&gt; Thanks<br>
&gt; Ahsan<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br><br clear="all"><br>-- <br>Syed Ahsan Ali Bokhari <br>Electronic Engineer (EE)<div><br>Research &amp; Development Division<br>Pakistan Meteorological Department H-8/4, Islamabad.<br>Phone&nbsp;#&nbsp;off &nbsp;+92518358714</div>
<div>Cell # +923155145014<br></div>
</div>
_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></body></html>
