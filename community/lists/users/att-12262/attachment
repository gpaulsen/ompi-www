<html><head><base href="x-msg://124/"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><br><div><div>On Mar 5, 2010, at 8:52 AM, abc def wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><span class="Apple-style-span" style="border-collapse: separate; font-family: Helvetica; font-size: medium; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; "><div class="hmmessage" style="font-size: 10pt; font-family: Verdana; ">Hello,<br>From within the MPI fortran program I run the following command:<br><br>CALL SYSTEM("cd " // TRIM(dir) // " ; mpirun -machinefile ./machinefile -np 1 /home01/group/Execute/DLPOLY.X &gt; job.out 2&gt; job.err ; cd - &gt; /dev/null")<br></div></span></blockquote><div><br></div>That is guaranteed not to work. The problem is that mpirun sets environmental variables for the original launch. Your system call carries over those envars, causing mpirun to become confused.</div><div><br><blockquote type="cite"><span class="Apple-style-span" style="border-collapse: separate; font-family: Helvetica; font-size: medium; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; "><div class="hmmessage" style="font-size: 10pt; font-family: Verdana; "><br>where "dir" is a process-number-dependent directory, to ensure the processes don't over-write each other, and machinefile is written earlier by using hostname to obtain the node of the current process, so this new program launches using the same node as the one that launches it.<br><br>In fortran, the program automatically waits until the system call is complete.<br><br>Since you mentioned MPI_COMM_SPAWN, I looked into this. I read that it's non-blocking, so somehow I'd have to prevent the program from moving forwards until it was complete, and secondly, I need to "cd" into the directory I mentioned above, before running the external command, and I don't know how one would achieve this with this command.<br><br>Do you think MPI_COMM_SPAWN can help?<br></div></span></blockquote><div><br></div>It's the only method supported by the MPI standard. If you need it to block until this new executable completes, you could use a barrier or other MPI method to determine it.</div><div><br></div><div><br><blockquote type="cite"><span class="Apple-style-span" style="border-collapse: separate; font-family: Helvetica; font-size: medium; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; "><div class="hmmessage" style="font-size: 10pt; font-family: Verdana; ">I appreciate your time.<br><br><hr id="stopSpelling">From:<span class="Apple-converted-space">&nbsp;</span><a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a><br>Date: Fri, 5 Mar 2010 07:55:59 -0700<br>To:<span class="Apple-converted-space">&nbsp;</span><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subject: Re: [OMPI users] running external program	on	same	processor	(Fortran)<br><br>How are you trying to start this external program? With an MPI_Comm_spawn? Or are you just fork/exec'ing it?<div><br></div><div>How are you waiting for this external program to finish?</div><div><br><div><div>On Mar 5, 2010, at 7:52 AM, abc def wrote:</div><br class="ecxApple-interchange-newline"><blockquote><span class="ecxApple-style-span" style="border-collapse: separate; font-family: Helvetica; font-size: medium; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; "><div class="ecxhmmessage" style="font-size: 10pt; font-family: Verdana; ">Hello,<br><br>Thanks for the comments. Indeed, until yesterday, I didn't realise the difference between MVAPICH, MVAPICH2 and Open-MPI.<br><br>This problem has moved from mvapich2 to open-mpi now however, because I now realise that the production environment uses Open-MPI, which means my solution for mvapich2 doesn't work now. So if I may ask your kind assistance:<br><br>Just to re-cap, I have an MPI fortran program, which runs on N nodes, and each node needs to run an external program. This is external program was written for MPI, although I want to run it in serial as part of the process on each node.<br><br>Is there any way at all to launch this external MPI program so it's treated simply as a (serial) extension of the current node's processes? As I say, the MPI originating program simply waits for the external program to finish before continuing, so it it's essentially a bit like a "subroutine", in that sense.<br><br>I'm having problems launching this external program from within my MPI program, under the open-mpi system, even without worrying about node assignment, and I think this might be because the system detects that I'm trying to launch another process from one of the nodes, and stops it. I'm guessing here, but it simply stops with an error saying the MPI process was stopped.<br><br>Any help is very much appreciated; I have been going at this for more than a day now and don't seem to be getting anywhere.<br><br>Thank you!<br><br><hr id="ecxstopSpelling">From:<span class="ecxApple-converted-space">&nbsp;</span><a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a><br>Date: Wed, 3 Mar 2010 07:24:32 -0700<br>To:<span class="ecxApple-converted-space">&nbsp;</span><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subject: Re: [OMPI users] running external program on	same	processor	(Fortran)<br><br>It also would have been really helpful to know that you were using MVAPICH and -not- Open MPI as this mailing list is for the latter. We could have directed you to the appropriate place if we had known.<div><br></div><div><br><div><div>On Mar 3, 2010, at 5:17 AM, abc def wrote:</div><br class="ecxecxApple-interchange-newline"><blockquote><span class="ecxecxApple-style-span" style="border-collapse: separate; font-family: Helvetica; font-size: medium; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; "><div class="ecxecxhmmessage" style="font-size: 10pt; font-family: Verdana; ">I don't know (I'm a little new to this area), but I figured out how to get around the problem:<br><br>Using SGE and MVAPICH2, the "-env MV2_CPU_MAPPING 0:1....." option in mpiexec seems to do the trick.<br><br>So when calling the external program with mpiexec, I map the called process to the current core rank, and it seems to stay distributed and separated as I want.<br><br>Hope someone else finds this useful in the future.<br><br>&gt; Date: Wed, 3 Mar 2010 13:13:01 +1100<br>&gt; Subject: Re: [OMPI users] running external program on same	processor	(Fortran)<br>&gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; Surely this is the problem of the scheduler that your system uses,<br>&gt; rather than MPI?<br>&gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; On Wed, 2010-03-03 at 00:48 +0000, abc def wrote:<br>&gt; &gt; Hello,<br>&gt; &gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; &gt; I wonder if someone can help.<br>&gt; &gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; &gt; The situation is that I have an MPI-parallel fortran program. I run it<br>&gt; &gt; and it's distributed on N cores, and each of these processes must call<br>&gt; &gt; an external program.<br>&gt; &gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; &gt; This external program is also an MPI program, however I want to run it<br>&gt; &gt; in serial, on the core that is calling it, as if it were part of the<br>&gt; &gt; fortran program. The fortran program waits until the external program<br>&gt; &gt; has completed, and then continues.<br>&gt; &gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; &gt; The problem is that this external program seems to run on any core,<br>&gt; &gt; and not necessarily the (now idle) core that called it. This slows<br>&gt; &gt; things down a lot as you get one core doing multiple tasks.<br>&gt; &gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; &gt; Can anyone tell me how I can call the program and ensure it runs only<br>&gt; &gt; on the core that's calling it? Note that there are several cores per<br>&gt; &gt; node. I can ID the node by running the hostname command (I don't know<br>&gt; &gt; a way to do this for individual cores).<br>&gt; &gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; &gt; Thanks!<br>&gt; &gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; &gt; ====<br>&gt; &gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; &gt; Extra information that might be helpful:<br>&gt; &gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; &gt; If I simply run the external program from the command line (ie, type<br>&gt; &gt; "/path/myprogram.ex &lt;enter&gt;"), it runs fine. If I run it within the<br>&gt; &gt; fortran program by calling it via<br>&gt; &gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; &gt; CALL SYSTEM("/path/myprogram.ex")<br>&gt; &gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; &gt; it doesn't run at all (doesn't even start) and everything crashes. I<br>&gt; &gt; don't know why this is.<br>&gt; &gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; &gt; If I call it using mpiexec:<br>&gt; &gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; &gt; CALL SYSTEM("mpiexec -n 1 /path/myprogram.ex")<br>&gt; &gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; &gt; then it does work, but I get the problem that it can go on any core.<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; &gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; &gt; ______________________________________________________________________<br>&gt; &gt; Do you want a Hotmail account? Sign-up now - Free<br>&gt; &gt; _______________________________________________<br>&gt; &gt; users mailing list<br>&gt; &gt;<span class="ecxecxApple-converted-space">&nbsp;</span><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; &gt;<span class="ecxecxApple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<span class="ecxecxApple-converted-space">&nbsp;</span><br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt;<span class="ecxecxApple-converted-space">&nbsp;</span><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;<span class="ecxecxApple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><hr>Not got a Hotmail account?<span class="ecxecxApple-converted-space">&nbsp;</span><a href="http://clk.atdmt.com/UKM/go/197222280/direct/01/">Sign-up now - Free</a><span class="ecxecxApple-converted-space">&nbsp;</span>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></span></blockquote></div><br></div><br><hr>Not got a Hotmail account?<span class="ecxApple-converted-space">&nbsp;</span><a href="http://clk.atdmt.com/UKM/go/197222280/direct/01/">Sign-up now - Free</a><span class="ecxApple-converted-space">&nbsp;</span>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></span></blockquote></div><br></div><br><hr>Got a cool Hotmail story?<span class="Apple-converted-space">&nbsp;</span><a href="http://clk.atdmt.com/UKM/go/195013117/direct/01/" target="_new">Tell us now</a><span class="Apple-converted-space">&nbsp;</span>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></span></blockquote></div><br></body></html>
