<div dir="ltr">Couldn&#39;t it be that the slot list should be 0,1,2,3?<div><div>It depends on the setup. </div><div>You can get some more information about _what it does_ by using --report-bindings (when/if it succeeds).</div><div><br></div></div></div><div class="gmail_extra"><br><div class="gmail_quote">2015-12-07 16:18 GMT+01:00 Carl Ponder <span dir="ltr">&lt;<a href="mailto:cponder@nvidia.com" target="_blank">cponder@nvidia.com</a>&gt;</span>:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
  
    
  
  <div text="#000000" bgcolor="#FFFFFF">
    <div><span class="">
      <div><small><b>On 12/06/2015 11:07 AM,
            Carl Ponder wrote:</b></small><br>
      </div>
      <blockquote type="cite">
        <blockquote><font color="#003333">I&#39;m trying to run a multi-node
            job but I want to map all of the processes to cores on
            socket #0 only.<br>
            I&#39;m having a hard time figuring out how to do this, the
            obvious combinations<br>
          </font>
          <blockquote><font color="#003333"><tt>mpirun -n  8 -npernode 4
                -report-bindings ...</tt><br>
            </font> <font color="#003333"><tt>mpirun -n  8 -npernode 4
                --map-by core -report-bindings ...</tt><br>
            </font> <font color="#003333"><tt>mpirun -n  8 -npernode 4
                -cpu-set S0 -report-bindings ...</tt><br>
            </font> <font color="#003333"><tt>mpirun -n  8 --map-by
                ppr:4:node,ppr:4:socket -report-bindings ...</tt><br>
            </font> <font color="#003333"><tt>mpirun -n  8 -npernode 4
                -bind-to slot=0:0,2,4,6 -report-bindings ...</tt><br>
            </font> <font color="#003333"><tt>mpirun -n  8 -npernode 4
                -bind-to slot=0:0,0:2,0:4,0:6 -report-bindings ...</tt><br>
            </font> <font color="#003333"><tt>mpirun -n  8 -npernode 4
                --map-by core:PE=2 -bind-to core -report-bindings ...</tt><br>
            </font> </blockquote>
        </blockquote>
        <blockquote><font color="#003333"> all are reported as having
            conflicting resource requirements.</font></blockquote>
      </blockquote>
      </span><span class=""><small><b>
          On 12/06/2015 11:28 AM, Ralph Castain wrote:</b></small><br>
    </span></div><span class="">
    <blockquote type="cite">
      <blockquote><font color="#003333">You want &quot;-bind-to socket
          -slot-list=0,2,4,6&quot;<br>
          Or if you want each process bound to a single core on the
          socket, then change “socket” to “core” in the above</font></blockquote>
    </blockquote></span>
    So far I can&#39;t get this to work. Using the above form<br>
    <blockquote><tt>mpirun -n 8 <b>-bind-to socket --slot-list 0,2,4,6</b>
        -report-bindings ...</tt><br>
    </blockquote>
    it says that it&#39;s a mis-specification:<br>
    <blockquote><tt>Conflicting directives for binding policy are
        causing the policy</tt><br>
      <tt>to be redefined:</tt><br>
      <br>
      <tt>  New policy:   socket</tt><br>
      <tt>  Prior policy:  SOCKET</tt><br>
      <br>
      <tt>Please check that only one policy is defined.</tt><br>
    </blockquote>
    If I treat the socket-binding as redundant and just use this<br>
    <blockquote><tt>mpirun -n 8 -<b>-slot-list 0,2,4,6</b>
        -report-bindings ...</tt><br>
    </blockquote>
    it looks like it&#39;s ignoring slots 0,2,4,6 available on the second
    node:<br>
    <blockquote><tt>A rank is missing its location specification:</tt><br>
      <br>
      <tt>  Rank:        0</tt><br>
      <tt>  Rank file:   (null)</tt><br>
      <br>
      <tt>All processes must have their location specified in the rank
        file. Either</tt><br>
      <tt>add an entry to the file, or provide a default slot_list to
        use for</tt><br>
      <tt>any unspecified ranks.</tt><br>
    </blockquote>
    (One question is whether it is interacting with Torque correctly).<br>
    Trying to force it to split the processes across nodes<br>
    <blockquote><tt>mpirun -n 8 <b>-npernode 4 --slot-list 0,2,4,6</b>
        -report-bindings ....</tt><br>
    </blockquote>
    gives<br>
    <blockquote><tt>Conflicting directives for mapping policy are
        causing the policy</tt><br>
      <tt>to be redefined:</tt><br>
      <br>
      <tt>  New policy:   RANK_FILE</tt><br>
      <tt>  Prior policy:  UNKNOWN</tt><br>
      <br>
      <tt>Please check that only one policy is defined.</tt><br>
    </blockquote>
    Do you know what to do here? I&#39;m using OpenMPI 1.10.1.<span class=""><br>
    Thanks,<br>
    <br>
                        Carl<br>
    <br>
  
<div>
<hr>
</div>
<div>This email message is for the sole use of the intended recipient(s) and may 
contain confidential information.  Any unauthorized review, use, disclosure 
or distribution is prohibited.  If you are not the intended recipient, 
please contact the sender by reply email and destroy all copies of the original 
message. </div>
<div>
<hr>
</div>
</span></div>

<br>_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/12/28139.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2015/12/28139.php</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br><div class="gmail_signature"><div dir="ltr"><div>Kind regards Nick</div></div></div>
</div>

