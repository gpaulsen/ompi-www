Hi, <br><br>I am trying to execute following code on cluster. <br><br>run_kernel is a cuda call with signature int run_kernel(int array[],int nelements, int taskid, char hostname[]);<br><br>My MPI code is <br><br><br>#include &quot;mpi.h&quot;
<br>#include &lt;stdio.h&gt;<br>#include &lt;stdlib.h&gt;<br>#include &lt;string.h&gt;<br><br>#define  MASTER        0<br> <br> <br>int *masterarray;<br>int *onearray;<br>int *twoarray;<br>int *threearray;<br>int *fourarray;<br>
int *fivearray;<br>int *sixarray;<br>int *sevenarray;<br>int *eightarray;<br>int *ninearray;<br><br> <br> <br> <br>int main(int argc, char* argv[])<br>{
<br>int   numtasks, taskid, rc, dest, offset, i, j, tag1, tag2, source, chunksize, namelen; 
<br><br>int mysum;
<br>int update(int myoffset, int chunk, int myid);<br>char myname[MPI_MAX_PROCESSOR_NAME];
<br>MPI_Status status;<br>int n;<br>int k=0;<br><br><br> <br>/***** Initializations *****/
<br>MPI_Init(&amp;argc, &amp;argv);
<br>MPI_Comm_size(MPI_COMM_WORLD, &amp;numtasks);
<br>MPI_Comm_rank(MPI_COMM_WORLD,&amp;taskid); <br>MPI_Get_processor_name(myname, &amp;namelen);
<br>printf (&quot;MPI task %d has started on host %s...\n&quot;, taskid, myname);
<br>chunksize = 20000000;
<br>tag2 = 1;
<br>tag1 = 2;<br><br>masterarray= malloc(chunksize * sizeof(int));<br>onearray= malloc(chunksize * sizeof(int));<br>twoarray= malloc(chunksize * sizeof(int));<br>threearray= malloc(chunksize * sizeof(int));<br>fourarray= malloc(chunksize * sizeof(int));<br>
fivearray= malloc(chunksize * sizeof(int));<br>sixarray= malloc(chunksize * sizeof(int));<br>sevenarray= malloc(chunksize * sizeof(int));<br>eightarray= malloc(chunksize * sizeof(int));<br>ninearray= malloc(chunksize * sizeof(int));<br>
<br>int a;<br><br>/*initialize all the arrays*/<br>for(a=0;a&lt;chunksize;a++){<br>   masterarray[a] = 1;<br>   onearray[a] = 1;<br>   twoarray[a] = 1;<br>   threearray[a] = 1;<br>   fourarray[a] = 1;<br>   fivearray[a] = 1;<br>
   sixarray[a] = 1;<br>   sevenarray[a] = 1;<br>   eightarray[a] = 1;<br>   ninearray[a] = 1;<br> <br> }<br> <br>/***** Master task only ******/
<br>if (taskid == MASTER){<br><br> <br>  mysum = run_kernel(&amp;masterarray[20000000], chunksize, taskid, myname);<br> <br> }  /* end of master section */
<br> <br> <br>if (taskid &gt; MASTER) {
<br> <br>  <br>    mysum = run_kernel(&amp;onearray[20000000], chunksize, taskid, myname);<br>   }<br><br>   if(taskid == 2){ <br>   <br>    mysum = run_kernel(&amp;twoarray[20000000], chunksize, taskid, myname);<br>   }<br>
<br>   if(taskid == 3){ <br>    <br>    mysum = run_kernel(&amp;threearray[20000000], chunksize, taskid, myname);<br>   }<br><br>   if(taskid == 4){ <br>   <br>    mysum = run_kernel(&amp;fourarray[20000000], chunksize, taskid, myname);<br>
   }<br><br>  if(taskid == 5){ <br>   <br>    mysum = run_kernel(&amp;fivearray[20000000], chunksize, taskid, myname);<br>   }<br><br>    if(taskid == 6){ <br>   <br>    mysum = run_kernel(&amp;sixarray[20000000], chunksize, taskid, myname);<br>
   }<br>  <br>  if(taskid == 7){ <br>  <br>    mysum = run_kernel(&amp;sevenarray[20000000], chunksize, taskid, myname);<br>   }<br><br>  if(taskid == 8){ <br> <br>    mysum = run_kernel(&amp;eightarray[20000000], chunksize, taskid, myname);<br>
   }<br><br>  if(taskid == 9){ <br>  <br>    mysum = run_kernel(&amp;ninearray[20000000], chunksize, taskid, myname);<br>   }<br>  <br> }
<br> <br> MPI_Finalize();
<br> <br>}   <br clear="all"><br><br>I am simply trying to calculate sum of array elements using kernel function. Each task has its own data and it calculates its own sum. <br><br>I am getting segmentation fault on master task but all other task calculate the sum successfully.<br>
<br>Here is the output<br><br><br>MPI task 0 has started on host node4<br>MPI task 1 has started on host node4<br>MPI task 2 has started on host node5<br>MPI task 6 has started on host node6<br>MPI task 5 has started on node5<br>
MPI task 9 has started on host node6<br>MPI task 8 has started on host node6<br>MPI task 3 has started on node5<br>MPI task 4 has started on hnode5<br>MPI task 7 has started on node6<br>[node4] *** Process received signal ***<br>
[node4] Signal: Segmentation fault (11)<br>[node4] Signal code: Address not mapped (1)<br>[node4] Failing at address: 0xb7866000<br>[node4] [ 0] [0xbc040c]<br>[node4] [ 1] /usr/lib/libcuda.so(+0x13a0f6) [0x10640f6]<br>[node4] [ 2] /usr/lib/libcuda.so(+0x146912) [0x1070912]<br>
[node4] [ 3] /usr/lib/libcuda.so(+0x147231) [0x1071231]<br>[node4] [ 4] /usr/lib/libcuda.so(+0x13cb64) [0x1066b64]<br>[node4] [ 5] /usr/lib/libcuda.so(+0x11863c) [0x104263c]<br>[node4] [ 6] /usr/lib/libcuda.so(+0x11d93b) [0x104793b]<br>
[node4] [ 7] /usr/lib/libcuda.so(cuMemcpyHtoD_v2+0x64) [0x1037264]<br>[node4] [ 8] /usr/local/cuda/lib/libcudart.so.4(+0x20336) [0x224336]<br>[node4] [ 9] /usr/local/cuda/lib/libcudart.so.4(cudaMemcpy+0x230) [0x257360]<br>
[node4] [10] mpi_array_distributed(run_kernel+0x9a) [0x804a482]<br>[node4] [11] mpi_array_distributed(main+0x325) [0x804a139]<br>[node4] [12] /lib/libc.so.6(__libc_start_main+0xe6) [0x4dece6]<br>[node4] [13] mpi_array_distributed() [0x8049d81]<br>
[node4] *** End of error message ***<br><br> MPI Task 1 is executing Kernel function........<br> Task 1 has sum (on GPU): 100002306 Time for the kernel: 39.462273 ms <br><br> MPI Task 7 is executing Kernel function........<br>
 Task 7 has sum (on GPU): 100002306 Time for the kernel: 64.105377 ms <br><br> MPI Task 9 is executing Kernel function.....<br> Task 9 has sum (on GPU): 100002306 Time for the kernel: 45.486912 ms <br><br> MPI Task 8 is executing Kernel function........Size of shared memory: 2048<br>
<br> MPI Task 4 is executing Kernel function.......<br> Task 8 has sum (on GPU): 100002306 Time for the kernel: 70.883362 ms <br><br> MPI Task 2 is executing Kernel function......<br> Task 4 has sum (on GPU): 100002306 Time for the kernel: 129.759079 ms <br>
 Task 2 has sum (on GPU): 100002306 Time for the kernel: 139.709473 ms <br><br> MPI Task 6 is executing Kernel function......<br><br> MPI Task 3 is executing Kernel function.......<br> Task 6 has sum (on GPU): 100002306 Time for the kernel: 47.691456 ms <br>
 Task 3 has sum (on GPU): 100002306 Time for the kernel: 110.210335 ms <br><br> MPI Task 5 is executing Kernel function......<br> Task 5 has sum (on GPU): 100002306 Time for the kernel: 110.706787 ms <br><br> --------------------------------------------------------------------------<br>
mpirun noticed that process rank 0 with PID 3054 on node <a href="http://ecm-c-l-207-004.uniwa.uwa.edu.au">ecm-c-l-207-004.uniwa.uwa.edu.au</a> exited on signal 11 (Segmentation fault).<br>--------------------------------------------------------------------------<br>
<br>Sadly i cant install memory checker such as valgrind on my machine due to some restrictions. I could not spot any error by looking in code.<br><br>Can anyone help me ?what is wrong in above code.<br><br>Thanks<br>-- <br>
<div><span style="font-size:13px;border-collapse:collapse"><font color="#000099" face="verdana, sans-serif"><font></font></font></span><span style="font-size:13px"><br><font color="#666666" face="georgia, serif"><br></font></span></div>
<br>

