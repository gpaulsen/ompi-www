<html><body><div>ibv_devinfo -v</div><div><br data-mce-bogus="1"></div><div>-Nathan</div><div><br>On Jun 15, 2016, at 12:43 PM, "Sasso, John (GE Power, Non-GE)" &lt;John1.Sasso@ge.com&gt; wrote:<br><br></div><div><blockquote type="cite"><div class="msg-quote"><div class="_stretch"><span class="body-text-content">QUESTION: Since the error said the system may have run out of queue pairs, how do I determine the # of queue pairs the IB HCA can support?<br><br><br>-----Original Message-----<br>From: users [mailto:users-bounces@open-mpi.org] On Behalf Of Sasso, John (GE Power, Non-GE)<br>Sent: Wednesday, June 15, 2016 2:35 PM<br>To: Open MPI Users<br>Subject: EXT: [OMPI users] "failed to create queue pair" problem, but settings appear OK<br><br>Chuck, <br><br>The per-process limits appear fine, including those for the resource mgr daemons:<br><br>Limit Soft Limit Hard Limit Units <br>Max address space unlimited unlimited bytes <br>Max core file size 0 0 bytes <br>Max cpu time unlimited unlimited seconds <br>Max data size unlimited unlimited bytes <br>Max file locks unlimited unlimited locks <br>Max file size unlimited unlimited bytes <br>Max locked memory unlimited unlimited bytes <br>Max msgqueue size 819200 819200 bytes <br>Max nice priority 0 0 <br>Max open files 16384 16384 files <br>Max pending signals 515625 515625 signals <br>Max processes 515625 515625 processes <br>Max realtime priority 0 0 <br>Max realtime timeout unlimited unlimited us <br>Max resident set unlimited unlimited bytes <br>Max stack size 307200000 unlimited bytes <br><br><br><br>As for the FAQ re registered memory, checking our OpenMPI settings with ompi_info, we have:<br><br>mpool_rdma_rcache_size_limit = 0 ==&gt; Open MPI will register as much user memory as necessary <br>btl_openib_free_list_max = -1 ==&gt; Open MPI will try to allocate as many registered buffers as it needs<br>btl_openib_eager_rdma_num = 16 <br>btl_openib_max_eager_rdma = 16 <br>btl_openib_eager_limit = 12288 <br><br><br>Other suggestions welcome. Hitting a brick wall here. Thanks!<br><br>--john<br><br><br><br>-----Original Message-----<br>From: users [mailto:users-bounces@open-mpi.org] On Behalf Of Gus Correa<br>Sent: Wednesday, June 15, 2016 1:39 PM<br>To: Open MPI Users<br>Subject: EXT: Re: [OMPI users] "failed to create queue pair" problem, but settings appear OK<br><br>Hi John<br><br>1) For diagnostic, you could check the actual "per process" limits on the nodes while that big job is running:<br><br>cat /proc/$PID/limits<br><br>2) If you're using a resource manager to launch the job, the resource manager daemon/deamons (local to the nodes) may have to to set the memlock and other limits, so that the Open MPI processes inherit them.<br>I use Torque, so I put these lines in the pbs_mom (Torque local daemon) initialization script:<br><br># pbs_mom system limits<br># max file descriptors<br>ulimit -n 32768<br># locked memory<br>ulimit -l unlimited<br># stacksize<br>ulimit -s unlimited<br><br>3) See also this FAQ related to registered memory.<br>I set these parameters in /etc/modprobe.d/mlx4_core.conf, but where they're set may depend on the Linux distro/release and the OFED you're using.<br><br><a href="https://urldefense.proofpoint.com/v2/url?u=https-3A__www.open-2Dmpi.org_faq_-3Fcategory-3Dopenfabrics-23ib-2Dlow-2Dreg-2Dmem&amp;d=CwIF-g&amp;c=IV_clAzoPDE253xZdHuilRgztyh_RiV3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimBPnb-JT-Js0Fmk&amp;m=fkBwjwn1Rvenp2NGwrQM3JtenpfbO_fxYUSK4lrHnzE&amp;s=UFQ0uSWQoNPCfwg9q02YzMJczt7g4jEcaCvYOd46RRA&amp;e=" data-mce-href="https://urldefense.proofpoint.com/v2/url?u=https-3A__www.open-2Dmpi.org_faq_-3Fcategory-3Dopenfabrics-23ib-2Dlow-2Dreg-2Dmem&amp;d=CwIF-g&amp;c=IV_clAzoPDE253xZdHuilRgztyh_RiV3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimBPnb-JT-Js0Fmk&amp;m=fkBwjwn1Rvenp2NGwrQM3JtenpfbO_fxYUSK4lrHnzE&amp;s=UFQ0uSWQoNPCfwg9q02YzMJczt7g4jEcaCvYOd46RRA&amp;e=">https://urldefense.proofpoint.com/v2/url?u=https-3A__www.open-2Dmpi.org_faq_-3Fcategory-3Dopenfabrics-23ib-2Dlow-2Dreg-2Dmem&amp;d=CwIF-g&amp;c=IV_clAzoPDE253xZdHuilRgztyh_RiV3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimBPnb-JT-Js0Fmk&amp;m=fkBwjwn1Rvenp2NGwrQM3JtenpfbO_fxYUSK4lrHnzE&amp;s=UFQ0uSWQoNPCfwg9q02YzMJczt7g4jEcaCvYOd46RRA&amp;e=</a> <br><br>I hope this helps,<br>Gus Correa<br><br>On 06/15/2016 11:05 AM, Sasso, John (GE Power, Non-GE) wrote:<br><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">In doing testing with IMB, I find that running a 4200+ core case with</blockquote><blockquote type="cite" class="quoted-plain-text">the IMB test Alltoall, and message lengths of 16..1024 bytes (as per</blockquote><blockquote type="cite" class="quoted-plain-text">-msglog 4:10 IMB option), it fails with:</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">----------------------------------------------------------------------</blockquote><blockquote type="cite" class="quoted-plain-text">----</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">A process failed to create a queue pair. This usually means either</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">the device has run out of queue pairs (too many connections) or</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">there are insufficient resources available to allocate a queue pair</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">(out of memory). The latter can happen if either 1) insufficient</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">memory is available, or 2) no more physical memory can be registered</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">with the device.</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">For more information on memory registration see the Open MPI FAQs at:</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text"><a href="https://urldefense.proofpoint.com/v2/url?u=http-3A__www.open-2Dmpi.org" data-mce-href="https://urldefense.proofpoint.com/v2/url?u=http-3A__www.open-2Dmpi.org">https://urldefense.proofpoint.com/v2/url?u=http-3A__www.open-2Dmpi.org</a></blockquote><blockquote type="cite" class="quoted-plain-text">_faq_-3Fcategory-3Dopenfabrics-23ib-2Dlocked-2Dpages&amp;d=CwIF-g&amp;c=IV_clA</blockquote><blockquote type="cite" class="quoted-plain-text">zoPDE253xZdHuilRgztyh_RiV3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimB</blockquote><blockquote type="cite" class="quoted-plain-text">Pnb-JT-Js0Fmk&amp;m=fkBwjwn1Rvenp2NGwrQM3JtenpfbO_fxYUSK4lrHnzE&amp;s=dKT5yJta</blockquote><blockquote type="cite" class="quoted-plain-text">2xW_ZUh06x95KTWjE1LgO8NU3OsjbwQsYLc&amp;e=</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">Local host: node7106</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">Local device: mlx4_0</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">Queue pair type: Reliable connected (RC)</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">----------------------------------------------------------------------</blockquote><blockquote type="cite" class="quoted-plain-text">----</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">[node7106][[51922,1],0][connect/btl_openib_connect_oob.c:867:rml_recv_</blockquote><blockquote type="cite" class="quoted-plain-text">cb]</blockquote><blockquote type="cite" class="quoted-plain-text">error in endpoint reply start connect</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">[node7106:06503] [[51922,0],0]-[[51922,1],0] mca_oob_tcp_msg_recv:</blockquote><blockquote type="cite" class="quoted-plain-text">readv failed: Connection reset by peer (104)</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">----------------------------------------------------------------------</blockquote><blockquote type="cite" class="quoted-plain-text">----</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">mpirun has exited due to process rank 0 with PID 6504 on</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">node node7106 exiting improperly. There are two reasons this could occur:</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">1. this process did not call "init" before exiting, but others in</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">the job did. This can cause a job to hang indefinitely while it waits</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">for all processes to call "init". By rule, if one process calls</blockquote><blockquote type="cite" class="quoted-plain-text">"init",</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">then ALL processes must call "init" prior to termination.</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">2. this process called "init", but exited without calling "finalize".</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">By rule, all processes that call "init" MUST call "finalize" prior to</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">exiting or it will be considered an "abnormal termination"</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">This may have caused other processes in the application to be</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">terminated by signals sent by mpirun (as reported here).</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">----------------------------------------------------------------------</blockquote><blockquote type="cite" class="quoted-plain-text">----</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">Yes, these are ALL of the error messages. I did not get a message</blockquote><blockquote type="cite" class="quoted-plain-text">about not being able to register enough memory. I verified that</blockquote><blockquote type="cite" class="quoted-plain-text">log_num_mtt = 24 and log_mtts_per_seg = 0 (via catting of their files</blockquote><blockquote type="cite" class="quoted-plain-text">in /sys/module/mlx4_core/parameters and what is set in</blockquote><blockquote type="cite" class="quoted-plain-text">/etc/modprobe.d/mlx4_core.conf). While such a large-scale job runs, I</blockquote><blockquote type="cite" class="quoted-plain-text">run 'vmstat 10' to examine memory usage, but there appears to be a</blockquote><blockquote type="cite" class="quoted-plain-text">good amount of memory still available and swap is never used. In</blockquote><blockquote type="cite" class="quoted-plain-text">terms of settings in /etc/security/limits.conf:</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">* soft memlock unlimited</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">* hard memlock unlimited</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">* soft stack 300000</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">* hard stack unlimited</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">I don't know if btl_openib_connect_oob.c or mca_oob_tcp_msg_recv are</blockquote><blockquote type="cite" class="quoted-plain-text">clues, but I am now at a loss as to where the problem lies.</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">This is for an application using OpenMPI 1.6.5, and the systems have</blockquote><blockquote type="cite" class="quoted-plain-text">Mellanox OFED 3.1.1 installed.</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">*--john*</blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text"><br></blockquote><blockquote type="cite" class="quoted-plain-text">_______________________________________________</blockquote><blockquote type="cite" class="quoted-plain-text">users mailing list</blockquote><blockquote type="cite" class="quoted-plain-text"><a href="mailto:users@open-mpi.org" data-mce-href="mailto:users@open-mpi.org">users@open-mpi.org</a></blockquote><blockquote type="cite" class="quoted-plain-text">Subscription:</blockquote><blockquote type="cite" class="quoted-plain-text"><a href="https://urldefense.proofpoint.com/v2/url?u=https-3A__www.open-2Dmpi.or" data-mce-href="https://urldefense.proofpoint.com/v2/url?u=https-3A__www.open-2Dmpi.or">https://urldefense.proofpoint.com/v2/url?u=https-3A__www.open-2Dmpi.or</a></blockquote><blockquote type="cite" class="quoted-plain-text">g_mailman_listinfo.cgi_users&amp;d=CwIF-g&amp;c=IV_clAzoPDE253xZdHuilRgztyh_Ri</blockquote><blockquote type="cite" class="quoted-plain-text">V3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimBPnb-JT-Js0Fmk&amp;m=fkBwjwn1</blockquote><blockquote type="cite" class="quoted-plain-text">Rvenp2NGwrQM3JtenpfbO_fxYUSK4lrHnzE&amp;s=jTwvPXqRGWpfeRFC_6XkYAx5DH99crNb</blockquote><blockquote type="cite" class="quoted-plain-text">mWhBN9r1hdg&amp;e= Link to this post:</blockquote><blockquote type="cite" class="quoted-plain-text"><a href="https://urldefense.proofpoint.com/v2/url?u=http-3A__www.open-2Dmpi.org" data-mce-href="https://urldefense.proofpoint.com/v2/url?u=http-3A__www.open-2Dmpi.org">https://urldefense.proofpoint.com/v2/url?u=http-3A__www.open-2Dmpi.org</a></blockquote><blockquote type="cite" class="quoted-plain-text">_community_lists_users_2016_06_29455.php&amp;d=CwIF-g&amp;c=IV_clAzoPDE253xZdH</blockquote><blockquote type="cite" class="quoted-plain-text">uilRgztyh_RiV3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimBPnb-JT-Js0Fm</blockquote><blockquote type="cite" class="quoted-plain-text">k&amp;m=fkBwjwn1Rvenp2NGwrQM3JtenpfbO_fxYUSK4lrHnzE&amp;s=8xTBNYgBKnKVf6SD7vEn</blockquote><blockquote type="cite" class="quoted-plain-text">3-wizYAxVVSS63L5bCdfidE&amp;e=</blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" data-mce-href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subscription: <a href="https://urldefense.proofpoint.com/v2/url?u=https-3A__www.open-2Dmpi.org_mailman_listinfo.cgi_users&amp;d=CwIF-g&amp;c=IV_clAzoPDE253xZdHuilRgztyh_RiV3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimBPnb-JT-Js0Fmk&amp;m=fkBwjwn1Rvenp2NGwrQM3JtenpfbO_fxYUSK4lrHnzE&amp;s=jTwvPXqRGWpfeRFC_6XkYAx5DH99crNbmWhBN9r1hdg&amp;e=" data-mce-href="https://urldefense.proofpoint.com/v2/url?u=https-3A__www.open-2Dmpi.org_mailman_listinfo.cgi_users&amp;d=CwIF-g&amp;c=IV_clAzoPDE253xZdHuilRgztyh_RiV3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimBPnb-JT-Js0Fmk&amp;m=fkBwjwn1Rvenp2NGwrQM3JtenpfbO_fxYUSK4lrHnzE&amp;s=jTwvPXqRGWpfeRFC_6XkYAx5DH99crNbmWhBN9r1hdg&amp;e=">https://urldefense.proofpoint.com/v2/url?u=https-3A__www.open-2Dmpi.org_mailman_listinfo.cgi_users&amp;d=CwIF-g&amp;c=IV_clAzoPDE253xZdHuilRgztyh_RiV3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimBPnb-JT-Js0Fmk&amp;m=fkBwjwn1Rvenp2NGwrQM3JtenpfbO_fxYUSK4lrHnzE&amp;s=jTwvPXqRGWpfeRFC_6XkYAx5DH99crNbmWhBN9r1hdg&amp;e=</a><br>Link to this post: <a href="https://urldefense.proofpoint.com/v2/url?u=http-3A__www.open-2Dmpi.org_community_lists_users_2016_06_29458.php&amp;d=CwIF-g&amp;c=IV_clAzoPDE253xZdHuilRgztyh_RiV3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimBPnb-JT-Js0Fmk&amp;m=fkBwjwn1Rvenp2NGwrQM3JtenpfbO_fxYUSK4lrHnzE&amp;s=uK1Ww0uehyaqSfXOtAt3Lqhers5lzDnBPhdDVCQx_hk&amp;e=" data-mce-href="https://urldefense.proofpoint.com/v2/url?u=http-3A__www.open-2Dmpi.org_community_lists_users_2016_06_29458.php&amp;d=CwIF-g&amp;c=IV_clAzoPDE253xZdHuilRgztyh_RiV3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimBPnb-JT-Js0Fmk&amp;m=fkBwjwn1Rvenp2NGwrQM3JtenpfbO_fxYUSK4lrHnzE&amp;s=uK1Ww0uehyaqSfXOtAt3Lqhers5lzDnBPhdDVCQx_hk&amp;e=">https://urldefense.proofpoint.com/v2/url?u=http-3A__www.open-2Dmpi.org_community_lists_users_2016_06_29458.php&amp;d=CwIF-g&amp;c=IV_clAzoPDE253xZdHuilRgztyh_RiV3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimBPnb-JT-Js0Fmk&amp;m=fkBwjwn1Rvenp2NGwrQM3JtenpfbO_fxYUSK4lrHnzE&amp;s=uK1Ww0uehyaqSfXOtAt3Lqhers5lzDnBPhdDVCQx_hk&amp;e=</a> <br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" data-mce-href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subscription: <a href="https://urldefense.proofpoint.com/v2/url?u=https-3A__www.open-2Dmpi.org_mailman_listinfo.cgi_users&amp;d=CwICAg&amp;c=IV_clAzoPDE253xZdHuilRgztyh_RiV3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimBPnb-JT-Js0Fmk&amp;m=wfoogAf9WJFU5BkEyYvxgk4izDUvAfO8MYCQXiucJoA&amp;s=2l_p5Kv-f-eUrtBHVrFzSSfD0J44dpVMI_9HXv8z6M0&amp;e=" data-mce-href="https://urldefense.proofpoint.com/v2/url?u=https-3A__www.open-2Dmpi.org_mailman_listinfo.cgi_users&amp;d=CwICAg&amp;c=IV_clAzoPDE253xZdHuilRgztyh_RiV3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimBPnb-JT-Js0Fmk&amp;m=wfoogAf9WJFU5BkEyYvxgk4izDUvAfO8MYCQXiucJoA&amp;s=2l_p5Kv-f-eUrtBHVrFzSSfD0J44dpVMI_9HXv8z6M0&amp;e=">https://urldefense.proofpoint.com/v2/url?u=https-3A__www.open-2Dmpi.org_mailman_listinfo.cgi_users&amp;d=CwICAg&amp;c=IV_clAzoPDE253xZdHuilRgztyh_RiV3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimBPnb-JT-Js0Fmk&amp;m=wfoogAf9WJFU5BkEyYvxgk4izDUvAfO8MYCQXiucJoA&amp;s=2l_p5Kv-f-eUrtBHVrFzSSfD0J44dpVMI_9HXv8z6M0&amp;e=</a> <br>Link to this post: <a href="https://urldefense.proofpoint.com/v2/url?u=http-3A__www.open-2Dmpi.org_community_lists_users_2016_06_29459.php&amp;d=CwICAg&amp;c=IV_clAzoPDE253xZdHuilRgztyh_RiV3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimBPnb-JT-Js0Fmk&amp;m=wfoogAf9WJFU5BkEyYvxgk4izDUvAfO8MYCQXiucJoA&amp;s=DtkJHiJd9cKWvqDjW4r2B_pCgVFjsOHxGTNXhTYvHbc&amp;e=" data-mce-href="https://urldefense.proofpoint.com/v2/url?u=http-3A__www.open-2Dmpi.org_community_lists_users_2016_06_29459.php&amp;d=CwICAg&amp;c=IV_clAzoPDE253xZdHuilRgztyh_RiV3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimBPnb-JT-Js0Fmk&amp;m=wfoogAf9WJFU5BkEyYvxgk4izDUvAfO8MYCQXiucJoA&amp;s=DtkJHiJd9cKWvqDjW4r2B_pCgVFjsOHxGTNXhTYvHbc&amp;e=">https://urldefense.proofpoint.com/v2/url?u=http-3A__www.open-2Dmpi.org_community_lists_users_2016_06_29459.php&amp;d=CwICAg&amp;c=IV_clAzoPDE253xZdHuilRgztyh_RiV3wUrLrDQYWSI&amp;r=tqKZ2vRCLufSSXPvzNxBrKr01YPimBPnb-JT-Js0Fmk&amp;m=wfoogAf9WJFU5BkEyYvxgk4izDUvAfO8MYCQXiucJoA&amp;s=DtkJHiJd9cKWvqDjW4r2B_pCgVFjsOHxGTNXhTYvHbc&amp;e=</a> <br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org" data-mce-href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/users" data-mce-href="https://www.open-mpi.org/mailman/listinfo.cgi/users">https://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/06/29460.php" data-mce-href="http://www.open-mpi.org/community/lists/users/2016/06/29460.php">http://www.open-mpi.org/community/lists/users/2016/06/29460.php</a><br></span></div></div></blockquote></div></body></html>
