Hi,<div>&nbsp;&nbsp; &nbsp;thanks for your reply. let&#39;s say i have 3 processors. I sent msg from 1st,2nd processors and want to gather in processor 0 processor. so i tried like following. it couldn&#39;t receive msg sent from processor 1 and 2.</div>
<div><br></div><div><span class="Apple-style-span" style="font-family: Tahoma; font-size: 12px; "><a href="http://www.nomorepasting.com/getpaste.php?pasteid=22985">http://www.nomorepasting.com/getpaste.php?pasteid=22985</a></span><br>
</div><div><span class="Apple-style-span" style="font-family: Tahoma; font-size: 12px;"><br></span></div><div><span class="Apple-style-span" style="font-family: Tahoma; font-size: 12px;">PS: is MPI_Recv is better to receive msg from multiple processors and gather in 1 processor? or MPI_Gather is better?</span></div>
<div><span class="Apple-style-span" style="font-family: Tahoma; font-size: 12px;">thanks</span></div><div><span class="Apple-style-span" style="font-family: Tahoma; font-size: 12px;">winthan</span></div><div><br></div><div>
<br><br><div class="gmail_quote">On Tue, Dec 23, 2008 at 1:23 PM, Eugene Loh <span dir="ltr">&lt;<a href="mailto:Eugene.Loh@sun.com">Eugene.Loh@sun.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
<div class="Ih2E3d">Win Than Aung wrote:<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
MPI_Recv(....) &lt;&lt; is it possible to receive the message sent from other sources? I tried MPI_ANY_SOURCE in place of source but it doesn&#39;t work out<br>
</blockquote>
<br></div>
Yes of course. &nbsp;Can you send a short example of what doesn&#39;t work? &nbsp;The example should presumably be less than about 20 lines. &nbsp;Here is an example that works:<br>
<br>
% cat a.c<br>
#include &lt;stdio.h&gt;<br>
#include &lt;mpi.h&gt;<br>
<br>
int main(int argc, char **argv) {<br>
&nbsp;int np, me, sbuf = -1, rbuf = -2;<br>
<br>
&nbsp;MPI_Init(&amp;argc,&amp;argv);<br>
&nbsp;MPI_Comm_size(MPI_COMM_WORLD,&amp;np);<br>
&nbsp;MPI_Comm_rank(MPI_COMM_WORLD,&amp;me);<br>
&nbsp;if ( np &lt; 2 ) MPI_Abort(MPI_COMM_WORLD,-1);<br>
<br>
&nbsp;if ( me == 1 ) MPI_Send(&amp;sbuf,1,MPI_INT,0,344,MPI_COMM_WORLD);<br>
&nbsp;if ( me == 0 ) {<br>
 &nbsp; MPI_Recv(&amp;rbuf,1,MPI_INT,MPI_ANY_SOURCE,344,MPI_COMM_WORLD,MPI_STATUS_IGNORE);<br>
 &nbsp; if ( rbuf == sbuf ) printf(&quot;Send/Recv self passed\n&quot;);<br>
 &nbsp; else &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;printf(&quot;Send/Recv self FAILED\n&quot;);<br>
&nbsp;}<br>
<br>
&nbsp;MPI_Finalize();<br>
<br>
&nbsp;return 0;<br>
}<br>
% mpicc a.c<br>
% mpirun -np 2 a.out<br>
Send/Recv self passed<br>
%<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br></div>

