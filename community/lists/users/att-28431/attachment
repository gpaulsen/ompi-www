Thanks Peter,<div><br></div><div>that is quite unexpected ...</div><div><br></div><div>let s try an other workaround, can you replace</div><div><br></div><div><pre style="word-wrap:break-word">integer            :: comm_group</pre><font face="monospace"><span style="white-space:pre-wrap">with</span></font></div><div><font face="monospace"><span style="white-space:pre-wrap"><br></span></font></div><div><pre style="word-wrap:break-word">integer            :: comm_group, comm_tmp</pre><pre style="word-wrap:break-word"><br></pre><pre style="word-wrap:break-word">and</pre></div><div><pre style="word-wrap:break-word">call MPI_COMM_SPLIT(comm, irank*2/num_procs, irank, comm_group, ierr)</pre><pre style="word-wrap:break-word"><br></pre><pre style="word-wrap:break-word">with</pre><pre style="word-wrap:break-word"><br></pre><pre style="word-wrap:break-word"><pre style="word-wrap:break-word">call MPI_COMM_SPLIT(comm, irank*2/num_procs, irank, comm_tmp, ierr)</pre><pre style="word-wrap:break-word">if (irank &lt; (num_procs/2)) then</pre><pre style="word-wrap:break-word">    comm_group = comm_tmp</pre><pre style="word-wrap:break-word">else</pre><pre style="word-wrap:break-word">    call MPI_Comm_dup(comm_tmp, comm_group, ierr)<span></span></pre><pre style="word-wrap:break-word">endif</pre><pre style="word-wrap:break-word"><br></pre><pre style="word-wrap:break-word"><br></pre></pre>if it works, I will make a fix tomorrow when I can access my workstation.</div><div>if not, can you please run</div>mpirun --mca osc_base_verbose 100 ...<div>and post the output ?</div><div><br></div><div>I will then try to reproduce the issue and investigate it</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles</div><div><br><div>On Tuesday, February 2, 2016, Peter Wind &lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;peter.wind@met.no&#39;);" target="_blank">peter.wind@met.no</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div style="font-family:arial,helvetica,sans-serif;font-size:12pt;color:#000000"><div>Thanks Gilles,<br></div><div><br></div><div>I get the following output (I guess it is not what you wanted?).<br></div><div><br></div><div>Peter<br></div><div><br></div><div><br></div><div>$ mpirun --mca osc pt2pt -np 4 a.out<br>--------------------------------------------------------------------------<br>A requested component was not found, or was unable to be opened.  This<br>means that this component is either not installed or is unable to be<br>used on your system (e.g., sometimes this means that shared libraries<br>that the component requires are unable to be found/loaded).  Note that<br>Open MPI stopped checking at the first component that it did not find.<br><br>Host:      stallo-2.local<br>Framework: osc<br>Component: pt2pt<br>--------------------------------------------------------------------------<br>--------------------------------------------------------------------------<br>It looks like MPI_INIT failed for some reason; your parallel process is<br>likely to abort.  There are many reasons that a parallel process can<br>fail during MPI_INIT; some of which are due to configuration or environment<br>problems.  This failure appears to be an internal failure; here&#39;s some<br>additional information (which may only be relevant to an Open MPI<br>developer):<br><br>  ompi_osc_base_open() failed<br>  --&gt; Returned &quot;Not found&quot; (-13) instead of &quot;Success&quot; (0)<br>--------------------------------------------------------------------------<br>*** An error occurred in MPI_Init<br>*** on a NULL communicator<br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>***    and potentially your MPI job)<br>[stallo-2.local:38415] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** on a NULL communicator<br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>***    and potentially your MPI job)<br>[stallo-2.local:38418] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** on a NULL communicator<br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>***    and potentially your MPI job)<br>[stallo-2.local:38416] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!<br>*** An error occurred in MPI_Init<br>*** on a NULL communicator<br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>***    and potentially your MPI job)<br>[stallo-2.local:38417] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!<br>-------------------------------------------------------<br>Primary job  terminated normally, but 1 process returned<br>a non-zero exit code.. Per user-direction, the job has been aborted.<br>-------------------------------------------------------<br>--------------------------------------------------------------------------<br>mpirun detected that one or more processes exited with non-zero status, thus causing<br>the job to be terminated. The first process to do so was:<br><br>  Process name: [[52507,1],0]<br>  Exit code:    1<br>--------------------------------------------------------------------------<br>[stallo-2.local:38410] 3 more processes have sent help message help-mca-base.txt / find-available:not-valid<br>[stallo-2.local:38410] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>[stallo-2.local:38410] 2 more processes have sent help message help-mpi-runtime / mpi_init:startup:internal-failure<br><br></div><div><br></div><hr><blockquote style="border-left:2px solid #1010ff;margin-left:5px;padding-left:5px;color:#000;font-weight:normal;font-style:normal;text-decoration:none;font-family:Helvetica,Arial,sans-serif;font-size:12pt">Peter,<div><br></div><div>at first glance, your test program looks correct.</div><div><br></div><div>can you please try to run</div><div>mpirun --mca osc pt2pt -np 4 ...</div><div><br></div><div>I  might have identified a bug with the sm osc component.</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<br><div><br></div>On Tuesday, February 2, 2016, Peter Wind &lt;<a>peter.wind@met.no</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Enclosed is a short (&lt; 100 lines) fortran code example that uses shared memory.<br> It seems to me it behaves wrongly if openmpi is used.<br> Compiled with SGI/mpt , it gives the right result.<br> <br> To fail, the code must be run on a single node.<br> It creates two groups of 2 processes each. Within each group memory is shared.<br> The error is that the two groups get the same memory allocated, but they should not.<br> <br> Tested with openmpi 1.8.4, 1.8.5, 1.10.2 and gfortran, intel 13.0, intel 14.0<br> all fail.<br> <br> The call:<br>    call MPI_Win_allocate_shared(win_size, disp_unit, MPI_INFO_NULL, comm_group, cp1, win, ierr)<br> <br> Should allocate memory only within the group. But when the other group allocates memory, the pointers from the two groups point to the same address in memory.<br> <br> Could you please confirm that this is the wrong behaviour?<br> <br> Best regards,<br> Peter Wind</blockquote></div><br>_______________________________________________<br>users mailing list<br><a>users@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/02/28429.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/02/28429.php</a></blockquote><div><br></div></div></div></blockquote></div>
</div>

