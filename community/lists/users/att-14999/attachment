<div>Unfortunately DRAGON is old FORTRAN77. Integers have been used instead of pointers. If I compile it in 64bits without <font class="Apple-style-span" face="&#39;courier new&#39;, monospace">-f-default-integer-8</font><font class="Apple-style-span" face="arial, helvetica, sans-serif">, </font>the so-called <meta http-equiv="content-type" content="text/html; charset=utf-8">pointers will remain in 32bits. Problems could also arise from its data structure handlers.</div>
<div><br></div><div><font class="Apple-style-span" face="arial, helvetica, sans-serif">Therefore </font><font class="Apple-style-span" face="&#39;courier new&#39;, monospace">-f-default-integer-8</font> is absolutely necessary.</div>
<div><br></div><div>Futhermore MPI_SEND and MPI_RECEIVE are called a dozen times in only one source file (used for passing a data structure from one node to another) and it has proved to be working in every situtation.</div>
<div><br></div><div>Not knowing which line is causing my segfault is annoying. <img src="cid:323@goomoji.gmail" style="margin-top: 0px; margin-right: 0,2ex; margin-bottom: 0px; margin-left: 0,2ex; vertical-align: middle; " goomoji="323"></div>
<meta http-equiv="content-type" content="text/html; charset=utf-8"><div><br></div><div>Regards,</div><div>Benjamin<br><br><div class="gmail_quote">2010/12/6 Gustavo Correa <span dir="ltr">&lt;<a href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt;</span><br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">Hi Benjamin<br>
<br>
I would just rebuild OpenMPI withOUT the compiler flags that change the standard<br>
sizes of  &quot;int&quot; and &quot;float&quot; (do a &quot;make cleandist&quot; first!), then recompile your program,<br>
and see how it goes.<br>
I don&#39;t think you are gaining anything by trying to change the standard &quot;int/integer&quot; and<br>
&quot;real/float&quot; sizdes, and most likely they are inviting trouble, making things more confusing.<br>
Worst scenario, you will at least be sure that the bug is somewhere else, not on the mismatch<br>
of basic type sizes.<br>
<br>
If you need to pass 8-byte real buffers, use MPI_DOUBLE_PRECISION, or MPI_REAL8<br>
in your (Fortran) MPI calls, and declare them in the Fortran code accordingly<br>
(double precision or real(kind=8)).<br>
<br>
If I remember right, there is no  8-byte integer support in the Fortran MPI bindings,<br>
only in the C bindings, but some OpenMPI expert could clarify this.<br>
Hence, if you are passing 8-byte integers in your MPI calls this may be also problematic.<br>
<br>
My two cents,<br>
<font color="#888888">Gus Correa<br>
</font><div><div></div><div class="h5"><br>
On Dec 5, 2010, at 3:04 PM, Benjamin Toueg wrote:<br>
<br>
&gt; Hi,<br>
&gt;<br>
&gt; First of all thanks for your insight !<br>
&gt;<br>
&gt; Do you get a corefile?<br>
&gt; I don&#39;t get a core file, but I get a file called _FIL001. It doesn&#39;t contain any debugging symbols. It&#39;s most likely a digested version of the input file given to the executable : ./myexec &lt; inputfile.<br>

&gt;<br>
&gt; there&#39;s no line numbers printed in the stack trace<br>
&gt; I would love to see those, but even if I compile openmpi with -debug -mem-debug -mem-profile, they don&#39;t show up. I recompiled my sources to be sure to properly link them to the newly debugged version of openmpi. I assumed I didn&#39;t need to compile my own sources with -g option since it crashes in openmpi itself ? I didn&#39;t try to run mpiexec via gdb either, I guess it wont help since I already get the trace.<br>

&gt;<br>
&gt; the -fdefault-integer-8 options ought to be highly dangerous<br>
&gt; Thanks for noting. Indeed I had some issues with this option. For instance I have to declare some arguments as INTEGER*4 like RANK,SIZE,IERR in :<br>
&gt; CALL MPI_COMM_RANK(MPI_COMM_WORLD,RANK,IERR)<br>
&gt; CALL MPI_COMM_SIZE(MPI_COMM_WORLD,SIZE,IERR)<br>
&gt; In your example &quot;call MPI_Send(buf, count, MPI_INTEGER, dest, tag, MPI_COMM_WORLD, mpierr)&quot; I checked that count is never bigger than 2000 (as you mentioned it could flip to the negative). However I haven&#39;t declared it as INTEGER*4 and I think I should.<br>

&gt; When I said &quot;I had to raise the number of data strucutures to be sent&quot;, I meant that I had to call MPI_SEND many more times, not that buffers were bigger than before.<br>
&gt;<br>
&gt; I&#39;ll get back to you with more info when I&#39;ll be able to fix my connexion problem to the cluster...<br>
&gt;<br>
&gt; Thanks,<br>
&gt; Benjamin<br>
&gt;<br>
&gt; 2010/12/3 Martin Siegert &lt;<a href="mailto:siegert@sfu.ca">siegert@sfu.ca</a>&gt;<br>
&gt; Hi All,<br>
&gt;<br>
&gt; just to expand on this guess ...<br>
&gt;<br>
&gt; On Thu, Dec 02, 2010 at 05:40:53PM -0500, Gus Correa wrote:<br>
&gt; &gt; Hi All<br>
&gt; &gt;<br>
&gt; &gt; I wonder if configuring OpenMPI while<br>
&gt; &gt; forcing the default types to non-default values<br>
&gt; &gt; (-fdefault-integer-8 -fdefault-real-8) might have<br>
&gt; &gt; something to do with the segmentation fault.<br>
&gt; &gt; Would this be effective, i.e., actually make the<br>
&gt; &gt; the sizes of MPI_INTEGER/MPI_INT and MPI_REAL/MPI_FLOAT bigger,<br>
&gt; &gt; or just elusive?<br>
&gt;<br>
&gt; I believe what happens is that this mostly affects the fortran<br>
&gt; wrapper routines and the way Fortran variables are mapped to C:<br>
&gt;<br>
&gt; MPI_INTEGER -&gt; MPI_LONG<br>
&gt; MPI_FLOAT   -&gt; MPI_DOUBLE<br>
&gt; MPI_DOUBLE_PRECISION -&gt; MPI_DOUBLE<br>
&gt;<br>
&gt; In that respect I believe that the -fdefault-real-8 option is harmless,<br>
&gt; i.e., it does the expected thing.<br>
&gt; But the -fdefault-integer-8 options ought to be highly dangerous:<br>
&gt; It works for integer variables that are used as &quot;buffer&quot; arguments<br>
&gt; in MPI statements, but I would assume that this does not work for<br>
&gt; &quot;count&quot; and similar arguments.<br>
&gt; Example:<br>
&gt;<br>
&gt; integer, allocatable :: buf(*,*)<br>
&gt; integer i, count, dest, tag, mpierr<br>
&gt;<br>
&gt; i = 32768<br>
&gt; i2 = 2*i<br>
&gt; allocate(buf(i,i2))<br>
&gt; count = i*i2<br>
&gt; buf = 1<br>
&gt; call MPI_Send(buf, count, MPI_INTEGER, dest, tag, MPI_COMM_WORLD, mpierr)<br>
&gt;<br>
&gt; Now count is 2^31 which overflows a 32bit integer.<br>
&gt; The MPI standard requires that count is a 32bit integer, correct?<br>
&gt; Thus while buf gets the type MPI_LONG, count remains an int.<br>
&gt; Is this interpretation correct? If it is, then you are calling<br>
&gt; MPI_Send with a count argument of -2147483648.<br>
&gt; Which could result in a segmentation fault.<br>
&gt;<br>
&gt; Cheers,<br>
&gt; Martin<br>
&gt;<br>
&gt; --<br>
&gt; Martin Siegert<br>
&gt; Head, Research Computing<br>
&gt; WestGrid/ComputeCanada Site Lead<br>
&gt; IT Services                                phone: 778 782-4691<br>
&gt; Simon Fraser University                    fax:   778 782-4242<br>
&gt; Burnaby, British Columbia                  email: <a href="mailto:siegert@sfu.ca">siegert@sfu.ca</a><br>
&gt; Canada  V5A 1S6<br>
&gt;<br>
&gt; &gt; There were some recent discussions here about MPI<br>
&gt; &gt; limiting counts to MPI_INTEGER.<br>
&gt; &gt; Since Benjamin said he &quot;had to raise the number of data structures&quot;,<br>
&gt; &gt; which eventually led to the the error,<br>
&gt; &gt; I wonder if he is inadvertently flipping to negative integer<br>
&gt; &gt; side of the 32-bit universe (i.e. &gt;= 2**31), as was reported here by<br>
&gt; &gt; other list subscribers a few times.<br>
&gt; &gt;<br>
&gt; &gt; Anyway, segmentation fault can come from many different places,<br>
&gt; &gt; this is just a guess.<br>
&gt; &gt;<br>
&gt; &gt; Gus Correa<br>
&gt; &gt;<br>
&gt; &gt; Jeff Squyres wrote:<br>
&gt; &gt; &gt;Do you get a corefile?<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;It looks like you&#39;re calling MPI_RECV in Fortran and then it segv&#39;s.  This is *likely* because you&#39;re either passing a bad parameter or your buffer isn&#39;t big enough.  Can you double check all your parameters?<br>

&gt; &gt; &gt;<br>
&gt; &gt; &gt;Unfortunately, there&#39;s no line numbers printed in the stack trace, so it&#39;s not possible to tell exactly where in the ob1 PML it&#39;s dying (i.e., so we can&#39;t see exactly what it&#39;s doing to cause the segv).<br>

&gt; &gt; &gt;<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;On Dec 2, 2010, at 9:36 AM, Benjamin Toueg wrote:<br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;&gt;Hi,<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;I am using DRAGON, a neutronic simulation code in FORTRAN77 that has its own datastructures. I added a module to send these data structures thanks to MPI_SEND / MPI_RECEIVE, and everything worked perfectly for a while.<br>

&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;Then I had to raise the number of data structures to be sent up to a point where my cluster has this bug :<br>
&gt; &gt; &gt;&gt;*** Process received signal ***<br>
&gt; &gt; &gt;&gt;Signal: Segmentation fault (11)<br>
&gt; &gt; &gt;&gt;Signal code: Address not mapped (1)<br>
&gt; &gt; &gt;&gt;Failing at address: 0x2c2579fc0<br>
&gt; &gt; &gt;&gt;[ 0] /lib/libpthread.so.0 [0x7f52d2930410]<br>
&gt; &gt; &gt;&gt;[ 1] /home/toueg/openmpi/lib/openmpi/mca_pml_ob1.so [0x7f52d153fe03]<br>
&gt; &gt; &gt;&gt;[ 2] /home/toueg/openmpi/lib/libmpi.so.0(PMPI_Recv+0x2d2) [0x7f52d3504a1e]<br>
&gt; &gt; &gt;&gt;[ 3] /home/toueg/openmpi/lib/libmpi_f77.so.0(pmpi_recv_+0x10e) [0x7f52d36cf9c6]<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;How can I make this error more explicit ?<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;I use the following configuration of openmpi-1.4.3 :<br>
&gt; &gt; &gt;&gt;./configure --enable-debug --prefix=/home/toueg/openmpi CXX=g++ CC=gcc F77=gfortran FC=gfortran FLAGS=&quot;-m64 -fdefault-integer-8 -fdefault-real-8 -fdefault-double-8&quot; FCFLAGS=&quot;-m64 -fdefault-integer-8 -fdefault-real-8 -fdefault-double-8&quot; --disable-mpi-f90<br>

&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;Here is the output of mpif77 -v :<br>
&gt; &gt; &gt;&gt;mpif77 for 1.2.7 (release) of : 2005/11/04 11:54:51<br>
&gt; &gt; &gt;&gt;Driving: f77 -L/usr/lib/mpich-mpd/lib -v -lmpich-p4mpd -lpthread -lrt -lfrtbegin -lg2c -lm -shared-libgcc<br>
&gt; &gt; &gt;&gt;Lecture des spécification à partir de /usr/lib/gcc/x86_64-linux-gnu/3.4.6/specs<br>
&gt; &gt; &gt;&gt;Configuré avec: ../src/configure -v --enable-languages=c,c++,f77,pascal --prefix=/usr --libexecdir=/usr/lib --with-gxx-include-dir=/usr/include/c++/3.4 --enable-shared --with-system-zlib --enable-nls --without-included-gettext --program-suffix=-3.4 --enable-__cxa_atexit --enable-clocale=gnu --enable-libstdcxx-debug x86_64-linux-gnu<br>

&gt; &gt; &gt;&gt;Modèle de thread: posix<br>
&gt; &gt; &gt;&gt;version gcc 3.4.6 (Debian 3.4.6-5)<br>
&gt; &gt; &gt;&gt; /usr/lib/gcc/x86_64-linux-gnu/3.4.6/collect2 --eh-frame-hdr -m elf_x86_64 -dynamic-linker /lib64/ld-linux-x86-64.so.2 /usr/lib/gcc/x86_64-linux-gnu/3.4.6/../../../../lib/crt1.o /usr/lib/gcc/x86_64-linux-gnu/3.4.6/../../../../lib/crti.o /usr/lib/gcc/x86_64-linux-gnu/3.4.6/crtbegin.o -L/usr/lib/mpich-mpd/lib -L/usr/lib/gcc/x86_64-linux-gnu/3.4.6 -L/usr/lib/gcc/x86_64-linux-gnu/3.4.6 -L/usr/lib/gcc/x86_64-linux-gnu/3.4.6/../../../../lib -L/usr/lib/gcc/x86_64-linux-gnu/3.4.6/../../.. -L/lib/../lib -L/usr/lib/../lib -lmpich-p4mpd -lpthread -lrt -lfrtbegin -lg2c -lm -lgcc_s -lgcc -lc -lgcc_s -lgcc /usr/lib/gcc/x86_64-linux-gnu/3.4.6/crtend.o /usr/lib/gcc/x86_64-linux-gnu/3.4.6/../../../../lib/crtn.o<br>

&gt; &gt; &gt;&gt;/usr/lib/gcc/x86_64-linux-gnu/3.4.6/../../../../lib/libfrtbegin.a(frtbegin.o): dans la fonction ▒ main ▒:<br>
&gt; &gt; &gt;&gt;(.text+0x1e): référence indéfinie vers ▒ MAIN__ ▒<br>
&gt; &gt; &gt;&gt;collect2: ld a retourné 1 code d&#39;état d&#39;exécution<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;Thanks,<br>
&gt; &gt; &gt;&gt;Benjamin<br>
&gt; &gt; &gt;&gt;<br>
&gt; &gt; &gt;&gt;_______________________________________________<br>
&gt; &gt; &gt;&gt;users mailing list<br>
&gt; &gt; &gt;&gt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; &gt;&gt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; &gt; &gt;<br>
&gt; &gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; users mailing list<br>
&gt; &gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></div></blockquote></div><br></div>

