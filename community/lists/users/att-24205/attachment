<div dir="ltr"><div><div><div>Hi,<br><br></div>It is working now. It shows: <br>-------------------------------------------- <br>starting wrf task            0  of            4<br> starting wrf task            1  of            4<br>

 starting wrf task            2  of            4<br> starting wrf task            3  of            4<br>---------------------------------------------<br></div>Thank you so much!!! You helped me a lot! Finally :) And plus I know the difference between OpenMP and Open MPI (well, to be honest not completely, but more than i knew before). :D<br>

<br>Thanks, <br><br></div><div>Djordje</div><div><div><br><br></div></div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Tue, Apr 15, 2014 at 11:57 AM, Gus Correa <span dir="ltr">&lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi Djordje<br>
<br>
&quot;locate mpirun&quot; shows items labled &quot;intel&quot;, &quot;mpich&quot;, and &quot;openmpi&quot;, maybe more.<br>
Is it Ubuntu or Debian?<br>
<br>
Anyway, if you got this mess from somebody else,<br>
instead of sorting it out,<br>
it may save you time and headaches installing Open MPI from<br>
source.<br>
Since it is a single machine, there are no worries about<br>
having an homogeneous installation for several computers (which<br>
could be done if needed, though).<br>
<br>
0. Make sure you have gcc, g++, and gfortran installed,<br>
including any &quot;devel&quot; packages that may exist.<br>
[apt-get or yum should tell you]<br>
If something is missing, install it.<br>
<br>
1. Download the Open MPI (a.k.a OMPI) tarball to a work directory<br>
of your choice,<br>
say /home/djordje/inst/openmpi/1.8 (create the directory if needed),<br>
and untar the tarball (tar -jxvf ...)<br>
<br>
<a href="http://www.open-mpi.org/software/ompi/v1.8/" target="_blank">http://www.open-mpi.org/<u></u>software/ompi/v1.8/</a><br>
<br>
2. Configure it to be installed in yet another directory under<br>
your home, say /home/djordje/sw/openmpi/1.8 (with --prefix).<br>
<br>
cd /home/djordje/inst/openmpi/1.8<br>
<br>
./configure --prefix=/home/djordje/sw/<u></u>openmpi/1.8 CC=gcc, CXX=g++,<br>
FC=gfortran<br>
<br>
[Not sure if with 1.8 there is a separate F77 interface, if there is<br>
add F77=gfortran to the configure command line above.<br>
Also, I am using OMPI 1.6.5,<br>
but my recollection is that Jeff would phase off mpif90 and mpif77 in<br>
favor of a single mpifortran of sorts.  Please check the OMPI README file.]<br>
<br>
Then do<br>
<br>
make<br>
make install<br>
<br>
3. Setup your environment variables PATH and LD_LIBRARY_PATH<br>
to point to *this* Open MPI installation ahead of anything else.<br>
This is easily done in your .bashrc or .tcshrc/.cshrc file,<br>
depending on which shell you use<br>
<br>
.bashrc :<br>
export PATH=/home/djordje/sw/openmpi/<u></u>1.8/bin:$PATH<br>
export LD_LIBRARY_PATH=/home/djordje/<u></u>sw/openmpi/1.8/lib:$LD_<u></u>LIBRARY_PATH<br>
<br>
.tcshrc/.cshrc:<br>
<br>
setenv PATH /home/djordje/sw/openmpi/1.8/<u></u>bin:$PATH<br>
setenv LD_LIBRARY_PATH /home/djordje/sw/openmpi/1.8/<u></u>lib:$LD_LIBRARY_PATH<br>
<br>
4. Logout, login again (or open a new terminal), and check if you<br>
get the right mpirun, etc:<br>
<br>
which mpicc<br>
which mpif90<br>
which mpirun<br>
<br>
They should point to items in /home/djordje/sw/openmpi/1.8/<u></u>bin<br>
<br>
5. Rebuild WRF from scratch.<br>
<br>
6. Check if WRF got the libraries right:<br>
<br>
ldd wrf.exe<br>
<br>
This should show mpi libraries in /home/djordje/sw/openmpi/1.8/<u></u>lib<br>
<br>
7. Run WRF<br>
mpirun -np 4 wrf.exe<div class=""><br>
<br>
I hope this helps,<br>
Gus Correa<br>
<br>
<br>
<br>
<br></div><div class="">
On 04/14/2014 08:21 PM, Djordje Romanic wrote:<br>
</div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div class="">
Hi,<br>
<br>
Thanks for this guys. I think I might have two MPI implementations<br>
installed because &#39;locate mpirun&#39; gives (see bold lines) :<br>
------------------------------<u></u>-----------<br>
/etc/alternatives/mpirun<br>
/etc/alternatives/mpirun.1.gz<br></div>
*/home/djordje/Build_WRF/<u></u>LIBRARIES/mpich/bin/mpirun*<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/intel/<a href="http://4.1.1.036/linux-x86_64/bin/mpirun" target="_blank">4.<u></u>1.1.036/linux-x86_64/bin/<u></u>mpirun</a><br>
&lt;<a href="http://4.1.1.036/linux-x86_64/bin/mpirun" target="_blank">http://4.1.1.036/linux-x86_<u></u>64/bin/mpirun</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/intel/<a href="http://4.1.1.036/linux-x86_64/bin64/mpirun" target="_blank">4.<u></u>1.1.036/linux-x86_64/bin64/<u></u>mpirun</a><br>
&lt;<a href="http://4.1.1.036/linux-x86_64/bin64/mpirun" target="_blank">http://4.1.1.036/linux-x86_<u></u>64/bin64/mpirun</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/intel/<a href="http://4.1.1.036/linux-x86_64/ia32/bin/mpirun" target="_blank">4.<u></u>1.1.036/linux-x86_64/ia32/bin/<u></u>mpirun</a><br>
&lt;<a href="http://4.1.1.036/linux-x86_64/ia32/bin/mpirun" target="_blank">http://4.1.1.036/linux-x86_<u></u>64/ia32/bin/mpirun</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/intel/<a href="http://4.1.1.036/linux-x86_64/intel64/bin/mpirun" target="_blank">4.<u></u>1.1.036/linux-x86_64/intel64/<u></u>bin/mpirun</a><br>
&lt;<a href="http://4.1.1.036/linux-x86_64/intel64/bin/mpirun" target="_blank">http://4.1.1.036/linux-x86_<u></u>64/intel64/bin/mpirun</a>&gt;<div class=""><br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/openmpi/<u></u>1.4.3/linux-x86_64-2.3.4/gnu4.<u></u>5/bin/mpirun<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/openmpi/<u></u>1.4.3/linux-x86_64-2.3.4/gnu4.<u></u>5/share/man/man1/mpirun.1<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/openmpi/<u></u>1.6.4/linux-x86_64-2.3.4/gnu4.<u></u>6/bin/mpirun<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/openmpi/<u></u>1.6.4/linux-x86_64-2.3.4/gnu4.<u></u>6/share/man/man1/mpirun.1<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/bin/mpirun" target="_blank">8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/bin/mpirun</a><br></div>


&lt;<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/bin/mpirun" target="_blank">http://8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/bin/mpirun</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/bin/mpirun.mpich" target="_blank">8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/bin/mpirun.<u></u>mpich</a><br>


&lt;<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/bin/mpirun.mpich" target="_blank">http://8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/bin/mpirun.<u></u>mpich</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/bin/mpirun.mpich2" target="_blank">8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/bin/mpirun.<u></u>mpich2</a><br>


&lt;<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/bin/mpirun.mpich2" target="_blank">http://8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/bin/mpirun.<u></u>mpich2</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun" target="_blank">8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/bin/<u></u>mpirun</a><br>


&lt;<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun" target="_blank">http://8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/bin/<u></u>mpirun</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun.mpich" target="_blank">8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/bin/<u></u>mpirun.mpich</a><br>


&lt;<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun.mpich" target="_blank">http://8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/bin/<u></u>mpirun.mpich</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun.mpich2" target="_blank">8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/bin/<u></u>mpirun.mpich2</a><br>


&lt;<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun.mpich2" target="_blank">http://8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/bin/<u></u>mpirun.mpich2</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/lib/linux_amd64/libmpirun.so" target="_blank">8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/lib/<u></u>linux_amd64/libmpirun.so</a><br>


&lt;<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/lib/linux_amd64/libmpirun.so" target="_blank">http://8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/lib/<u></u>linux_amd64/libmpirun.so</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/lib/linux_ia32/libmpirun.so" target="_blank">8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/lib/<u></u>linux_ia32/libmpirun.so</a><br>


&lt;<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/ia32/lib/linux_ia32/libmpirun.so" target="_blank">http://8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/lib/<u></u>linux_ia32/libmpirun.so</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/lib/linux_amd64/libmpirun.so" target="_blank">8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/lib/linux_<u></u>amd64/libmpirun.so</a><br>


&lt;<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/lib/linux_amd64/libmpirun.so" target="_blank">http://8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/lib/linux_<u></u>amd64/libmpirun.so</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/lib/linux_ia32/libmpirun.so" target="_blank">8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/lib/linux_<u></u>ia32/libmpirun.so</a><br>


&lt;<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/lib/linux_ia32/libmpirun.so" target="_blank">http://8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/lib/linux_<u></u>ia32/libmpirun.so</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/share/man/man1/mpirun.1.gz" target="_blank">8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/share/man/<u></u>man1/mpirun.1.gz</a><br>


&lt;<a href="http://8.2.0.0/linux64_2.6-x86-glibc_2.3.4/share/man/man1/mpirun.1.gz" target="_blank">http://8.2.0.0/linux64_2.6-<u></u>x86-glibc_2.3.4/share/man/<u></u>man1/mpirun.1.gz</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/bin/mpirun" target="_blank">8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/bin/mpirun</a><br>
&lt;<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/bin/mpirun" target="_blank">http://8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/bin/mpirun</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/bin/mpirun.mpich" target="_blank">8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/bin/mpirun.<u></u>mpich</a><br>


&lt;<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/bin/mpirun.mpich" target="_blank">http://8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/bin/mpirun.<u></u>mpich</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/bin/mpirun.mpich2" target="_blank">8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/bin/mpirun.<u></u>mpich2</a><br>


&lt;<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/bin/mpirun.mpich2" target="_blank">http://8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/bin/mpirun.<u></u>mpich2</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun" target="_blank">8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/bin/<u></u>mpirun</a><br>


&lt;<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun" target="_blank">http://8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/bin/<u></u>mpirun</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun.mpich" target="_blank">8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/bin/<u></u>mpirun.mpich</a><br>


&lt;<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun.mpich" target="_blank">http://8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/bin/<u></u>mpirun.mpich</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun.mpich2" target="_blank">8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/bin/<u></u>mpirun.mpich2</a><br>


&lt;<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/bin/mpirun.mpich2" target="_blank">http://8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/bin/<u></u>mpirun.mpich2</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/lib/linux_amd64/libmpirun.so" target="_blank">8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/lib/<u></u>linux_amd64/libmpirun.so</a><br>


&lt;<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/lib/linux_amd64/libmpirun.so" target="_blank">http://8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/lib/<u></u>linux_amd64/libmpirun.so</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/lib/linux_ia32/libmpirun.so" target="_blank">8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/lib/<u></u>linux_ia32/libmpirun.so</a><br>


&lt;<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/ia32/lib/linux_ia32/libmpirun.so" target="_blank">http://8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/ia32/lib/<u></u>linux_ia32/libmpirun.so</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/lib/linux_amd64/libmpirun.so" target="_blank">8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/lib/linux_<u></u>amd64/libmpirun.so</a><br>


&lt;<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/lib/linux_amd64/libmpirun.so" target="_blank">http://8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/lib/linux_<u></u>amd64/libmpirun.so</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/lib/linux_ia32/libmpirun.so" target="_blank">8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/lib/linux_<u></u>ia32/libmpirun.so</a><br>


&lt;<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/lib/linux_ia32/libmpirun.so" target="_blank">http://8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/lib/linux_<u></u>ia32/libmpirun.so</a>&gt;<br>
/home/djordje/StarCCM/Install/<u></u>STAR-CCM+8.06.007/mpi/<u></u>platform/<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/share/man/man1/mpirun.1.gz" target="_blank">8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/share/man/<u></u>man1/mpirun.1.gz</a><br>


&lt;<a href="http://8.3.0.2/linux64_2.6-x86-glibc_2.3.4/share/man/man1/mpirun.1.gz" target="_blank">http://8.3.0.2/linux64_2.6-<u></u>x86-glibc_2.3.4/share/man/<u></u>man1/mpirun.1.gz</a>&gt;<br>
*/usr/bin/mpirun*<div class=""><br>
/usr/bin/mpirun.openmpi<br>
/usr/lib/openmpi/include/<u></u>openmpi/ompi/runtime/<u></u>mpiruntime.h<br>
/usr/share/man/man1/mpirun.1.<u></u>gz<br>
/usr/share/man/man1/mpirun.<u></u>openmpi.1.gz<br>
/var/lib/dpkg/alternatives/<u></u>mpirun<br>
------------------------------<u></u>-----------<br>
This is a single machine. I actually just got it... another user used it<br>
for 1-2 years.<br>
<br>
Is this a possible cause of the problem?<br>
<br>
Regards,<br>
Djordje<br>
<br>
<br>
On Mon, Apr 14, 2014 at 7:06 PM, Gus Correa &lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a><br></div><div><div class="h5">
&lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;<u></u>&gt; wrote:<br>
<br>
    Apologies for stirring even more the confusion by mispelling<br>
    &quot;Open MPI&quot; as &quot;OpenMPI&quot;.<br>
    &quot;OMPI&quot; doesn&#39;t help either, because all OpenMP environment<br>
    variables and directives start with &quot;OMP&quot;.<br>
    Maybe associating the names to<br>
    &quot;message passing&quot; vs. &quot;threads&quot; would help?<br>
<br>
    Djordje:<br>
<br>
    &#39;which mpif90&#39; etc show everything in /usr/bin.<br>
    So, very likely they were installed from packages<br>
    (yum, apt-get, rpm ...),right?<br>
    Have you tried something like<br>
    &quot;yum list |grep mpi&quot;<br>
    to see what you have?<br>
<br>
    As Dave, Jeff and Tom said, this may be a mixup of different<br>
    MPI implementations at compilation (mpicc mpif90) and runtime (mpirun).<br>
    That is common, you may have different MPI implementations installed.<br>
<br>
    Other possibilities that may tell what MPI you have:<br>
<br>
    mpirun --version<br>
    mpif90 --show<br>
    mpicc --show<br>
<br>
    Yet another:<br>
<br>
    locate mpirun<br>
    locate mpif90<br>
    locate mpicc<br>
<br>
    The ldd didn&#39;t show any MPI libraries, maybe they are static libraries.<br>
<br>
    An alternative is to install Open MPI from source,<br>
    and put it in a non-system directory<br>
    (not /usr/bin, not /usr/local/bin, etc).<br>
<br>
    Is this a single machine or a cluster?<br>
    Or perhaps a set of PCs that you have access to?<br>
    If it is a cluster, do you have access to a filesystem that is<br>
    shared across the cluster?<br>
    On clusters typically /home is shared, often via NFS.<br>
<br>
    Gus Correa<br>
<br>
<br>
    On 04/14/2014 05:15 PM, Jeff Squyres (jsquyres) wrote:<br>
<br>
        Maybe we should rename OpenMP to be something less confusing --<br>
        perhaps something totally unrelated, perhaps even non-sensical.<br>
        That&#39;ll end lots of confusion!<br>
<br>
        My vote: OpenMP --&gt; SharkBook<br>
<br>
        It&#39;s got a ring to it, doesn&#39;t it?  And it sounds fearsome!<br>
<br>
<br>
<br>
        On Apr 14, 2014, at 5:04 PM, &quot;Elken, Tom&quot; &lt;<a href="mailto:tom.elken@intel.com" target="_blank">tom.elken@intel.com</a><br></div></div><div class="">
        &lt;mailto:<a href="mailto:tom.elken@intel.com" target="_blank">tom.elken@intel.com</a>&gt;&gt; wrote:<br>
<br>
            That’s OK.  Many of us make that mistake, though often as a<br>
            typo.<br>
            One thing that helps is that the correct spelling of Open<br>
            MPI has a space in it,<br>
<br>
    but OpenMP does not.<br>
<br>
            If not aware what OpenMP is, here is a link:<br>
            <a href="http://openmp.org/wp/" target="_blank">http://openmp.org/wp/</a><br>
<br>
            What makes it more confusing is that more and more apps.<br>
<br>
    offer the option of running in a hybrid mode, such as WRF,<br>
    with OpenMP threads running over MPI ranks with the same executable.<br>
    And sometimes that MPI is Open MPI.<br>
<br>
<br>
            Cheers,<br>
            -Tom<br>
<br></div>
            From: users [mailto:<a href="mailto:users-bounces@open-__mpi.org" target="_blank">users-bounces@open-__<u></u>mpi.org</a><div class=""><br>
            &lt;mailto:<a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-<u></u>mpi.org</a>&gt;] On Behalf Of Djordje<br>
            Romanic<br>
            Sent: Monday, April 14, 2014 1:28 PM<br>
            To: Open MPI Users<br>
            Subject: Re: [OMPI users] mpirun runs in serial even I set<br>
            np to several processors<br>
<br>
            OK guys... Thanks for all this info. Frankly, I didn&#39;t know<br>
            these diferences between OpenMP and OpenMPI. The commands:<br>
            which mpirun<br>
            which mpif90<br>
            which mpicc<br>
            give,<br>
            /usr/bin/mpirun<br>
            /usr/bin/mpif90<br>
            /usr/bin/mpicc<br>
            respectively.<br>
<br>
            A tutorial on how to compile WRF<br></div>
            (<a href="http://www.mmm.ucar.edu/wrf/__OnLineTutorial/compilation___tutorial.php" target="_blank">http://www.mmm.ucar.edu/wrf/_<u></u>_OnLineTutorial/compilation___<u></u>tutorial.php</a><br>
            &lt;<a href="http://www.mmm.ucar.edu/wrf/OnLineTutorial/compilation_tutorial.php" target="_blank">http://www.mmm.ucar.edu/wrf/<u></u>OnLineTutorial/compilation_<u></u>tutorial.php</a>&gt;)<div class=""><br>
            provides a test program to test MPI. I ran the program and<br>
            it gave me the output of successful run, which is:<br></div>
            ------------------------------<u></u>__---------------<div class=""><br>
            C function called by Fortran<br>
            Values are xx = 2.00 and ii = 1<br>
            status = 2<br>
            SUCCESS test 2 fortran + c + netcdf + mpi<br></div>
            ------------------------------<u></u>__---------------<div class=""><br>
            It uses mpif90 and mpicc for compiling. Below is the output<br>
            of &#39;ldd ./wrf.exe&#39;:<br>
<br>
<br>
                  linux-vdso.so.1 =&gt;  (0x00007fff584e7000)<br>
                  libpthread.so.0 =&gt;<br></div>
            /lib/x86_64-linux-gnu/__<u></u>libpthread.so.0 (0x00007f4d160ab000)<br>
                  libgfortran.so.3 =&gt;<br>
            /usr/lib/x86_64-linux-gnu/__<u></u>libgfortran.so.3<br>
            (0x00007f4d15d94000)<br>
                  libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.<u></u>__6<br>
            (0x00007f4d15a97000)<br>
                  libgcc_s.so.1 =&gt; /lib/x86_64-linux-gnu/libgcc__<u></u>_s.so.1<br>
            (0x00007f4d15881000)<br>
                  libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.<u></u>__6<div class=""><br>
            (0x00007f4d154c1000)<br>
                  /lib64/ld-linux-x86-64.so.2 (0x00007f4d162e8000)<br>
                  libquadmath.so.0 =&gt;<br></div>
            /usr/lib/x86_64-linux-gnu/__<u></u>libquadmath.so.0<div class=""><br>
            (0x00007f4d1528a000)<br>
<br>
<br>
<br>
            On Mon, Apr 14, 2014 at 4:09 PM, Gus Correa<br></div><div><div class="h5">
            &lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a> &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;<u></u>&gt; wrote:<br>
            Djordje<br>
<br>
            Your WRF configure file seems to use mpif90 and mpicc (line<br>
            115 &amp; following).<br>
            In addition, it also seems to have DISABLED OpenMP (NO<br>
            TRAILING &quot;I&quot;)<br>
            (lines 109-111, where OpenMP stuff is commented out).<br>
            So, it looks like to me your intent was to compile with MPI.<br>
<br>
            Whether it is THIS MPI (OpenMPI) or another MPI (say MPICH,<br>
            or MVAPICH,<br>
            or Intel MPI, or Cray, or ...) only your environment can tell.<br>
<br>
            What do you get from these commands:<br>
<br>
            which mpirun<br>
            which mpif90<br>
            which mpicc<br>
<br>
            I never built WRF here (but other people here use it).<br>
            Which input do you provide to the command that generates the<br>
            configure<br>
            script that you sent before?<br>
            Maybe the full command line will shed some light on the problem.<br>
<br>
<br>
            I hope this helps,<br>
            Gus Correa<br>
<br>
<br>
            On 04/14/2014 03:11 PM, Djordje Romanic wrote:<br>
            to get help :)<br>
<br>
<br>
<br>
            On Mon, Apr 14, 2014 at 3:11 PM, Djordje Romanic<br>
            &lt;<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a> &lt;mailto:<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a>&gt;<br></div></div><div class="">
            &lt;mailto:<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a> &lt;mailto:<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a>&gt;&gt;&gt; wrote:<br>
<br>
                  Yes, but I was hoping to get. :)<br>
<br>
<br>
                  On Mon, Apr 14, 2014 at 3:02 PM, Jeff Squyres (jsquyres)<br>
                  &lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a> &lt;mailto:<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;<br></div><div class="">
            &lt;mailto:<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a> &lt;mailto:<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;&gt;&gt; wrote:<br>
<br>
                      If you didn&#39;t use Open MPI, then this is the wrong<br>
            mailing list<br>
                      for you.  :-)<br>
<br>
                      (this is the Open MPI users&#39; support mailing list)<br>
<br>
<br>
                      On Apr 14, 2014, at 2:58 PM, Djordje Romanic<br>
            &lt;<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a> &lt;mailto:<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a>&gt;<br></div>
                      &lt;mailto:<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a><div class=""><br>
            &lt;mailto:<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a>&gt;&gt;&gt; wrote:<br>
<br>
                       &gt; I didn&#39;t use OpenMPI.<br>
                       &gt;<br>
                       &gt;<br>
                       &gt; On Mon, Apr 14, 2014 at 2:37 PM, Jeff Squyres<br>
            (jsquyres)<br>
                      &lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a> &lt;mailto:<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;<br></div><div class="">
            &lt;mailto:<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a> &lt;mailto:<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;&gt;&gt; wrote:<br>
                       &gt; This can also happen when you compile your<br>
            application with<br>
                      one MPI implementation (e.g., Open MPI), but then<br>
            mistakenly use<br>
                      the &quot;mpirun&quot; (or &quot;mpiexec&quot;) from a different MPI<br>
            implementation<br>
                      (e.g., MPICH).<br>
                       &gt;<br>
                       &gt;<br>
                       &gt; On Apr 14, 2014, at 2:32 PM, Djordje Romanic<br>
                      &lt;<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a> &lt;mailto:<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a>&gt;<br></div><div class="">
            &lt;mailto:<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a> &lt;mailto:<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a>&gt;&gt;&gt; wrote:<br>
                       &gt;<br>
                       &gt; &gt; I compiled it with: x86_64 Linux, gfortran<br>
            compiler with<br>
                      gcc   (dmpar). dmpar - distributed memory option.<br>
                       &gt; &gt;<br>
                       &gt; &gt; Attached is the self-generated configuration<br>
            file. The<br>
                      architecture specification settings start at line<br>
            107. I didn&#39;t<br>
                      use Open MPI (shared memory option).<br>
                       &gt; &gt;<br>
                       &gt; &gt;<br>
                       &gt; &gt; On Mon, Apr 14, 2014 at 1:23 PM, Dave Goodell<br>
            (dgoodell)<br>
                      &lt;<a href="mailto:dgoodell@cisco.com" target="_blank">dgoodell@cisco.com</a> &lt;mailto:<a href="mailto:dgoodell@cisco.com" target="_blank">dgoodell@cisco.com</a>&gt;<br></div><div class="">
            &lt;mailto:<a href="mailto:dgoodell@cisco.com" target="_blank">dgoodell@cisco.com</a> &lt;mailto:<a href="mailto:dgoodell@cisco.com" target="_blank">dgoodell@cisco.com</a>&gt;&gt;&gt; wrote:<br>
                       &gt; &gt; On Apr 14, 2014, at 12:15 PM, Djordje Romanic<br>
                      &lt;<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a> &lt;mailto:<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a>&gt;<br></div><div class="">
            &lt;mailto:<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a> &lt;mailto:<a href="mailto:djordje8@gmail.com" target="_blank">djordje8@gmail.com</a>&gt;&gt;&gt; wrote:<br>
                       &gt; &gt;<br>
                       &gt; &gt; &gt; When I start wrf with mpirun -np 4<br>
            ./wrf.exe, I get this:<br>
                       &gt; &gt; &gt;<br></div>
            ------------------------------<u></u>__-------------------<div class=""><br>
                       &gt; &gt; &gt;  starting wrf task            0  of<br>
                1<br>
                       &gt; &gt; &gt;  starting wrf task            0  of<br>
                1<br>
                       &gt; &gt; &gt;  starting wrf task            0  of<br>
                1<br>
                       &gt; &gt; &gt;  starting wrf task            0  of<br>
                1<br>
                       &gt; &gt; &gt;<br></div>
            ------------------------------<u></u>__-------------------<div class=""><br>
                       &gt; &gt; &gt; This indicates that it is not using 4<br>
            processors, but 1.<br>
                       &gt; &gt; &gt;<br>
                       &gt; &gt; &gt; Any idea what might be the problem?<br>
                       &gt; &gt;<br>
                       &gt; &gt; It could be that you compiled WRF with a<br>
            different MPI<br>
                      implementation than you are using to run it (e.g.,<br>
            MPICH vs.<br>
                      Open MPI).<br>
                       &gt; &gt;<br>
                       &gt; &gt; -Dave<br>
                       &gt; &gt;<br></div>
                       &gt; &gt; ______________________________<u></u>___________________<br>
                       &gt; &gt; users mailing list<br>
                       &gt; &gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
            &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt; &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
            &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;&gt;<br>
<br>
                       &gt; &gt;<br>
            <a href="http://www.open-mpi.org/__mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/__<u></u>mailman/listinfo.cgi/users</a><br>
            &lt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a>&gt;<br>
                       &gt; &gt;<br>
                       &gt; &gt;<br>
            &lt;configure.wrf&gt;_______________<u></u>______________________________<u></u>______<br>
                       &gt; &gt; users mailing list<br>
                       &gt; &gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
            &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt; &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
            &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;&gt;<br>
<br>
                       &gt; &gt;<br>
            <a href="http://www.open-mpi.org/__mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/__<u></u>mailman/listinfo.cgi/users</a><div class=""><br>
            &lt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a>&gt;<br>
                       &gt;<br>
                       &gt;<br>
                       &gt; --<br>
                       &gt; Jeff Squyres<br>
                       &gt; <a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a> &lt;mailto:<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;<br></div>
            &lt;mailto:<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a> &lt;mailto:<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;&gt;<div class=""><br>
<br>
                       &gt; For corporate legal information go to:<br></div>
            <a href="http://www.cisco.com/web/__about/doing_business/legal/__cri/" target="_blank">http://www.cisco.com/web/__<u></u>about/doing_business/legal/__<u></u>cri/</a><br>
            &lt;<a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/<u></u>about/doing_business/legal/<u></u>cri/</a>&gt;<br>
                       &gt;<br>
                       &gt; ______________________________<u></u>___________________<div class=""><br>
                       &gt; users mailing list<br>
                       &gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br></div>
            &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;&gt;<br>
<br>
                       &gt;<br>
            <a href="http://www.open-mpi.org/__mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/__<u></u>mailman/listinfo.cgi/users</a><br>
            &lt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a>&gt;<br>
                       &gt;<br>
                       &gt; ______________________________<u></u>___________________<div class=""><br>
                       &gt; users mailing list<br>
                       &gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br></div>
            &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;&gt;<br>
<br>
                       &gt;<br>
            <a href="http://www.open-mpi.org/__mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/__<u></u>mailman/listinfo.cgi/users</a><div class=""><br>
            &lt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a>&gt;<br>
<br>
<br>
                      --<br>
                      Jeff Squyres<br>
            <a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a> &lt;mailto:<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;<br></div>
            &lt;mailto:<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a> &lt;mailto:<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;&gt;<div class=""><br>
<br>
                      For corporate legal information go to:<br></div>
            <a href="http://www.cisco.com/web/__about/doing_business/legal/__cri/" target="_blank">http://www.cisco.com/web/__<u></u>about/doing_business/legal/__<u></u>cri/</a><br>
            &lt;<a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/<u></u>about/doing_business/legal/<u></u>cri/</a>&gt;<br>
<br>
                      ______________________________<u></u>___________________<div class=""><br>
                      users mailing list<br>
            <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br></div>
            &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;&gt;<br>
<br>
            <a href="http://www.open-mpi.org/__mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/__<u></u>mailman/listinfo.cgi/users</a><br>
            &lt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a>&gt;<br>
<br>
<br>
<br>
<br>
<br>
            ______________________________<u></u>___________________<div class=""><br>
            users mailing list<br>
            <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br></div>
            <a href="http://www.open-mpi.org/__mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/__<u></u>mailman/listinfo.cgi/users</a><br>
            &lt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a>&gt;<br>
<br>
<br>
            ______________________________<u></u>___________________<div class=""><br>
            users mailing list<br>
            <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br></div>
            <a href="http://www.open-mpi.org/__mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/__<u></u>mailman/listinfo.cgi/users</a><br>
            &lt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a>&gt;<br>
<br>
            ______________________________<u></u>___________________<div class=""><br>
            users mailing list<br>
            <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br></div>
            <a href="http://www.open-mpi.org/__mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/__<u></u>mailman/listinfo.cgi/users</a><br>
            &lt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a>&gt;<br>
<br>
<br>
<br>
<br>
    ______________________________<u></u>___________________<div class=""><br>
    users mailing list<br>
    <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br></div>
    <a href="http://www.open-mpi.org/__mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/__<u></u>mailman/listinfo.cgi/users</a><div class=""><br>
    &lt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a>&gt;<br>
<br>
<br>
<br>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
<br>
</div></blockquote><div class="HOEnZb"><div class="h5">
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br></div>

