Hi guys<br><br>Our apologies - the rshd launcher isn&#39;t supposed to be in a release branch. We&#39;ve removed it for the next release.<br><br>Sorry for the problem... :-(<br><br><br><div class="gmail_quote">On Fri, Feb 17, 2012 at 11:42 AM, Brian McNally <span dir="ltr">&lt;<a href="mailto:bmcnally@uw.edu">bmcnally@uw.edu</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Dave,<br>
<br>
Thanks for the suggestion, adding &quot;-mca plm ^rshd&quot; did force mpirun to spawn things via qrsh rather than SSH. My problem is solved!<br>
<br>
--<br>
Brian McNally<div class="HOEnZb"><div class="h5"><br>
<br>
On 02/16/2012 03:05 AM, Dave Love wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Brian McNally&lt;<a href="mailto:bmcnally@uw.edu" target="_blank">bmcnally@uw.edu</a>&gt;  writes:<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Hi Dave,<br>
<br>
I looked through the INSTALL, VERSION, NEWS, and README files in the<br>
1.5.4 openmpi tarball but didn&#39;t see what you were referring to.<br>
</blockquote>
<br>
I can&#39;t access the web site, but there&#39;s an item in the notes on the<br>
download page about the bug.  It must also be in the mail archive in a<br>
thread with me included.<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Are<br>
you suggesting that I launch mpirun similar to this?<br>
<br>
    mpirun -mca plm ^rshd ...?<br>
</blockquote>
<br>
Yes, or put it in openmpi-mca-params.conf.  It&#39;s harmless with 1.4.<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
What I meant by &quot;the same parallel environment setup&quot; was that the PE<br>
in SGE was defined the same way:<br>
<br>
$ qconf -sp orte<br>
pe_name            orte<br>
slots              9999<br>
user_lists         NONE<br>
xuser_lists        NONE<br>
start_proc_args    /bin/true<br>
stop_proc_args     /bin/true<br>
allocation_rule    $round_robin<br>
control_slaves     TRUE<br>
job_is_first_task  FALSE<br>
urgency_slots      min<br>
accounting_summary FALSE<br>
<br>
Even though I have RHEL 5 and RHEL 6 nodes in the same cluster they<br>
never run the same MPI job; it&#39;s always either all RHEL 5 nodes or all<br>
RHEL 6.<br>
</blockquote>
<br>
OK.  (I&#39;d expect a separate PE for each node set to enforce that.)<br>
<br>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
</blockquote>
______________________________<u></u>_________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/<u></u>mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

