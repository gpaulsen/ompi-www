<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">I know this commit could be a little hard to parse, but I have updated the mpirun man page on the trunk and will port the change over to the 1.8 series tomorrow. FWIW, I’ve provided the link to the commit below so you can “preview” it.<div class=""><br class=""></div><div class=""><a href="https://github.com/open-mpi/ompi/commit/f9d620e3a772cdeddd40b4f0789cf59c75b44868" class="">https://github.com/open-mpi/ompi/commit/f9d620e3a772cdeddd40b4f0789cf59c75b44868</a></div><div class=""><br class=""></div><div class="">HTH</div><div class="">Ralph</div><div class=""><br class=""></div><div class=""><br class=""></div><div class=""><div><blockquote type="cite" class=""><div class="">On Oct 16, 2014, at 9:43 AM, Gus Correa &lt;<a href="mailto:gus@ldeo.columbia.edu" class="">gus@ldeo.columbia.edu</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class="">Hi Ralph<br class=""><br class="">Yes, I know the process placement features are powerful.<br class="">They were already very good in 1.6, even in 1.4,<br class="">and I just tried the new 1.8<br class="">"-map-by l2cache" (works nicely on Opteron 6300).<br class=""><br class="">Unfortunately I couldn't keep track, test, and use the 1.7 series.<br class="">I did that in the previous "odd/new feature" series (1.3, 1.5).<br class="">However, my normal workload require that<br class="">I focus my attention on the "even/stable" series<br class="">(less fun, more production).<br class="">Hence I hopped directly from 1.6 to 1.8,<br class="">although I read a number of mailing list postings about the new<br class="">style of process placement.<br class=""><br class="">Pestering you again about documentation (last time for now):<br class="">The mpiexec man page also seems to need an update.<br class="">That is probably the first place people look for information<br class="">about runtime features.<br class="">For instance, the process placement examples still<br class="">use deprecated parameters and mpiexec options:<br class="">-bind-to-core, rmaps_base_schedule_policy, orte_process_binding, etc.<br class=""><br class="">Thank you,<br class="">Gus Correa<br class=""><br class="">On 10/15/2014 11:10 PM, Ralph Castain wrote:<br class=""><blockquote type="cite" class=""><br class="">On Oct 15, 2014, at 11:46 AM, Gus Correa &lt;<a href="mailto:gus@ldeo.columbia.edu" class="">gus@ldeo.columbia.edu</a><br class="">&lt;<a href="mailto:gus@ldeo.columbia.edu" class="">mailto:gus@ldeo.columbia.edu</a>&gt;&gt; wrote:<br class=""><br class=""><blockquote type="cite" class="">Thank you Ralph and Jeff for the help!<br class=""><br class="">Glad to hear the segmentation fault is reproducible and will be fixed.<br class=""><br class="">In any case, one can just avoid the old parameter name<br class="">(rmaps_base_schedule_policy),<br class="">and use instead the new parameter name<br class="">(rmaps_base_mapping_policy)<br class="">without any problem in OMPI 1.8.3.<br class=""><br class=""></blockquote><br class="">Fix is in the nightly 1.8 tarball - I'll release a 1.8.4 soon to cover<br class="">the problem.<br class=""><br class=""><blockquote type="cite" class="">**<br class=""><br class="">Thanks Ralph for sending the new (OMPI 1.8)<br class="">parameter names for process binding.<br class=""><br class="">My recollection is that sometime ago somebody (Jeff perhaps?)<br class="">posted here a link to a presentation (PDF or PPT) explaining the<br class="">new style of process binding, but I couldn't find it in the<br class="">list archives.<br class="">Maybe the link could be part of the FAQ (if not already there)?<br class=""></blockquote><br class="">I don't think it is, but I'll try to add it over the next day or so.<br class=""><br class=""><blockquote type="cite" class=""><br class="">**<br class=""><br class="">The Open MPI runtime environment is really great.<br class="">However, to take advantage of it one often has to do parameter guessing,<br class="">and to do time consuming tests by trial and error,<br class="">because the main sources of documentation are<br class="">the terse output of ompi_info, and several sparse<br class="">references in the FAQ.<br class="">(Some of them outdated?)<br class=""><br class="">In addition, the runtime environment has evolved over time,<br class="">which is certainly a good thing.<br class="">However, along with this evolution, several runtime parameters<br class="">changed both name and functionality, new ones were introduced,<br class="">old ones were deprecated, which can be somewhat confusing,<br class="">and can lead to an ineffective use of the runtime environment.<br class="">(In 1.8.3 I was using several deprecated parameters from 1.6.5<br class="">that seem to be silently ignored at runtime.<br class="">I only noticed the problem because that segmentation fault happened.)<br class=""><br class="">I know asking for thorough documentation is foolish,<br class=""></blockquote><br class="">Not really - it is something we need to get better about :-(<br class=""><br class=""><blockquote type="cite" class="">but I guess a simple table of runtime parameter names and valid values<br class="">in the FAQ, maybe sorted by their purpose/function, along with a few<br class="">examples of use, could help a lot.<br class="">Some of this material is now spread across several FAQ, but not so<br class="">easy to find/compare.<br class="">That doesn't need to be a comprehensive table, but commonly used<br class="">items like selecting the btl, selecting interfaces,<br class="">dealing with process binding,<br class="">modifying/enriching the stdout/sterr output<br class="">(tagging output, increasing verbosity, etc),<br class="">probably have their place there.<br class=""></blockquote><br class="">Yeah, we fell down on this one. The changes were announced with each<br class="">step in the 1.7 series, but if you step from 1.6 directly to 1.8, you'll<br class="">get caught flat-footed. We honestly didn't think of that case, and so we<br class="">mentally assumed that "of course people have been following the series -<br class="">they know what happened".<br class=""><br class="">You know what they say about those who "assume" :-/<br class=""><br class="">I'll try to get something into the FAQ about the entire new mapping,<br class="">ranking, and binding system. It is actually VERY powerful, allowing you<br class="">to specify pretty much any placement pattern you can imagine and bind it<br class="">to whatever level you desire. It was developed in response to requests<br class="">from researchers who wanted to explore application performance versus<br class="">placement strategies - but we provided some simplified options to<br class="">support more common usage patterns.<br class=""><br class=""><br class=""><blockquote type="cite" class=""><br class=""><br class="">Many thanks,<br class="">Gus Correa<br class=""><br class=""><br class="">On 10/15/2014 11:12 AM, Jeff Squyres (jsquyres) wrote:<br class=""><blockquote type="cite" class="">We talked off-list -- fixed this on master and just filed<br class=""><a href="https://github.com/open-mpi/ompi-release/pull/33" class="">https://github.com/open-mpi/ompi-release/pull/33</a> to get this into the<br class="">v1.8 branch.<br class=""><br class=""><br class="">On Oct 14, 2014, at 7:39 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" class="">rhc@open-mpi.org</a><br class="">&lt;<a href="mailto:rhc@open-mpi.org" class="">mailto:rhc@open-mpi.org</a>&gt;&gt; wrote:<br class=""><br class=""><blockquote type="cite" class=""><br class="">On Oct 14, 2014, at 5:32 PM, Gus Correa &lt;<a href="mailto:gus@ldeo.columbia.edu" class="">gus@ldeo.columbia.edu</a><br class="">&lt;<a href="mailto:gus@ldeo.columbia.edu" class="">mailto:gus@ldeo.columbia.edu</a>&gt;&gt; wrote:<br class=""><br class=""><blockquote type="cite" class="">Dear Open MPI fans and experts<br class=""><br class="">This is just a note in case other people run into the same problem.<br class=""><br class="">I just built Open MPI 1.8.3.<br class="">As usual I put my old settings on openmpi-mca-params.conf,<br class="">with no further thinking.<br class="">Then I compiled the connectivity_c.c program and tried<br class="">to run it with mpiexec.<br class="">That is a routine that never failed before.<br class=""><br class="">Bummer!<br class="">I've got a segmentation fault right away.<br class=""></blockquote><br class="">Strange &nbsp;- it works fine from the cmd line:<br class=""><br class="">07:27:04 &nbsp;(v1.8) /home/common/openmpi/ompi-release$ mpirun -n 1 -mca<br class="">rmaps_base_schedule_policy core hostname<br class="">--------------------------------------------------------------------------<br class="">A deprecated MCA variable value was specified in the environment or<br class="">on the command line. &nbsp;Deprecated MCA variables should be avoided;<br class="">they may disappear in future releases.<br class=""><br class=""> Deprecated variable: rmaps_base_schedule_policy<br class=""> New variable: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rmaps_base_mapping_policy<br class="">--------------------------------------------------------------------------<br class="">bend001<br class=""><br class="">HOWEVER, I can replicate that behavior when it is in the default<br class="">params file! I don't see the immediate cause of the difference, but<br class="">will investigate.<br class=""><br class=""><blockquote type="cite" class=""><br class="">After some head scratching, checking my environment, etc,<br class="">I thought I might have configured OMPI incorrectly.<br class="">Hence, I tried to get information from ompi_info.<br class="">Oh well, ompi_info also segfaulted!<br class=""><br class="">It took me a while to realize that the runtime parameter<br class="">configuration file was the culprit.<br class=""><br class="">When I inserted the runtime parameter settings one by one,<br class="">the segfault came with this one:<br class=""><br class="">rmaps_base_schedule_policy = core<br class=""><br class="">Ompi_info (when I got it to work) told me that the parameter above<br class="">is now a deprecated synonym of:<br class=""><br class="">rmaps_base_mapping_policy = core<br class=""><br class="">In any case, the old synonym doesn't work and makes ompi_info and<br class="">mpiexec segfault (and I'd guess anything else that requires the<br class="">OMPI runtime components).<br class="">Only the new parameter name works.<br class=""></blockquote><br class="">That's because the segfault is happening in the printing of the<br class="">deprecation warning.<br class=""><br class=""><blockquote type="cite" class=""><br class="">***<br class=""><br class="">I am also missing in the ompi_info output the following<br class="">(OMPI 1.6.5) parameters (not reported by ompi_info --all --all):<br class=""><br class=""></blockquote><br class="">1) orte_process_binding &nbsp;===&gt; hwloc_base_binding_policy<br class=""><br class="">2) orte_report_bindings &nbsp;&nbsp;===&gt; hwloc_base_report_bindings<br class=""><br class="">3) opal_paffinity_alone &nbsp;===&gt; gone, use<br class="">hwloc_base_binding_policy=none if you don't want any binding<br class=""><br class=""><blockquote type="cite" class=""><br class="">Are they gone forever?<br class=""><br class="">Are there replacements for them, with approximately the same<br class="">functionality?<br class=""><br class="">Is there a list comparing the new (1.8) vs. old (1.6)<br class="">OMPI runtime parameters, and/or any additional documentation<br class="">about the new style of OMPI 1.8 runtime parameters?<br class=""></blockquote><br class="">Will try to add this to the web site<br class=""><br class=""><blockquote type="cite" class=""><br class="">Since there seems to have been a major revamping of the OMPI<br class="">runtime parameters, that would be a great help.<br class=""><br class="">Thank you,<br class="">Gus Correa<br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a> &lt;<a href="mailto:users@open-mpi.org" class="">mailto:users@open-mpi.org</a>&gt;<br class="">Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this post:<br class=""><a href="http://www.open-mpi.org/community/lists/users/2014/10/25497.php" class="">http://www.open-mpi.org/community/lists/users/2014/10/25497.php</a><br class=""></blockquote><br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a> &lt;<a href="mailto:users@open-mpi.org" class="">mailto:users@open-mpi.org</a>&gt;<br class="">Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" class="">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this post:<br class=""><a href="http://www.open-mpi.org/community/lists/users/2014/10/25498.php" class="">http://www.open-mpi.org/community/lists/users/2014/10/25498.php</a><br class=""></blockquote><br class=""><br class=""></blockquote><br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a> &lt;<a href="mailto:users@open-mpi.org" class="">mailto:users@open-mpi.org</a>&gt;<br class="">Subscription:http://<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" class="">www.open-mpi.org/mailman/listinfo.cgi/users</a><br class="">Link to this<br class="">post:http://<a href="http://www.open-mpi.org/community/lists/users/2014/10/25501.php" class="">www.open-mpi.org/community/lists/users/2014/10/25501.php</a><br class=""></blockquote><br class=""><br class=""><br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2014/10/25503.php<br class=""><br class=""></blockquote><br class="">_______________________________________________<br class="">users mailing list<br class=""><a href="mailto:users@open-mpi.org" class="">users@open-mpi.org</a><br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/users<br class="">Link to this post: http://www.open-mpi.org/community/lists/users/2014/10/25508.php<br class=""></div></blockquote></div><br class=""></div></body></html>
