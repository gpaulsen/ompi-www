<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    Peter,<br>
    <br>
    a patch is available at
<a class="moz-txt-link-freetext" href="https://github.com/ggouaillardet/ompi-release/commit/0b62eabcae403b95274ce55973a7ce29483d0c98.patch">https://github.com/ggouaillardet/ompi-release/commit/0b62eabcae403b95274ce55973a7ce29483d0c98.patch</a><br>
    <br>
    it is now under review<br>
    <br>
    Cheers,<br>
    <br>
    Gilles<br>
    <br>
    <div class="moz-cite-prefix">On 2/2/2016 11:22 PM, Gilles
      Gouaillardet wrote:<br>
    </div>
    <blockquote
cite="mid:CAAkFZ5uYzA_caw=FLu3LK=OJG_yy8npmYhOo0zaJ3UyMDGXzNw@mail.gmail.com"
      type="cite">Thanks Peter,
      <div><br>
      </div>
      <div>this is just a workaround for a bug we just identified, the
        fix will come soon</div>
      <div><br>
      </div>
      <div>Cheers,</div>
      <div><br>
      </div>
      <div>Gilles<br>
        <br>
        On Tuesday, February 2, 2016, Peter Wind &lt;<a
          moz-do-not-send="true" href="mailto:peter.wind@met.no"><a class="moz-txt-link-abbreviated" href="mailto:peter.wind@met.no">peter.wind@met.no</a></a>&gt;
        wrote:<br>
        <blockquote class="gmail_quote" style="margin:0 0 0
          .8ex;border-left:1px #ccc solid;padding-left:1ex">
          <div>
            <div
style="font-family:arial,helvetica,sans-serif;font-size:12pt;color:#000000">
              <div>That worked!<br>
              </div>
              <div><br>
              </div>
              <div>i.e with the changed you proposed the code gives the
                right result.<br>
              </div>
              <div><br>
              </div>
              <div>That was efficient work, thank you Gilles :)<br>
              </div>
              <div><br>
              </div>
              <div>Best wishes,<br>
              </div>
              <div>Peter<br>
              </div>
              <div><br>
              </div>
              <div><br>
              </div>
              <hr>
              <blockquote style="border-left:2px solid
#1010ff;margin-left:5px;padding-left:5px;color:#000;font-weight:normal;font-style:normal;text-decoration:none;font-family:Helvetica,Arial,sans-serif;font-size:12pt">Thanks
                Peter,
                <div><br>
                </div>
                <div>that is quite unexpected ...</div>
                <div><br>
                </div>
                <div>let s try an other workaround, can you replace</div>
                <div><br>
                </div>
                <div>
                  <pre style="word-wrap:break-word">integer            :: comm_group</pre>
                  <span style="font-family:monospace" face="monospace"><span style="white-space:pre-wrap">with</span></span></div>
                <div><span style="font-family:monospace"
                    face="monospace"><span style="white-space:pre-wrap">
</span></span></div>
                <div>
                  <pre style="word-wrap:break-word">integer            :: comm_group, comm_tmp</pre>
                  <pre style="word-wrap:break-word">
</pre>
                  <pre style="word-wrap:break-word">and</pre>
                </div>
                <div>
                  <pre style="word-wrap:break-word">call MPI_COMM_SPLIT(comm, irank*2/num_procs, irank, comm_group, ierr)</pre>
                  <pre style="word-wrap:break-word">
</pre>
                  <pre style="word-wrap:break-word">with</pre>
                  <pre style="word-wrap:break-word">
</pre>
                  <pre style="word-wrap:break-word"><pre style="word-wrap:break-word">call MPI_COMM_SPLIT(comm, irank*2/num_procs, irank, comm_tmp, ierr)</pre><pre style="word-wrap:break-word">if (irank &lt; (num_procs/2)) then</pre><pre style="word-wrap:break-word">    comm_group = comm_tmp</pre><pre style="word-wrap:break-word">else</pre><pre style="word-wrap:break-word">    call MPI_Comm_dup(comm_tmp, comm_group, ierr)<span></span></pre><pre style="word-wrap:break-word">endif</pre><pre style="word-wrap:break-word">
</pre><pre style="word-wrap:break-word">
</pre></pre>
                  if it works, I will make a fix tomorrow when I can
                  access my workstation.</div>
                <div>if not, can you please run</div>
                mpirun --mca osc_base_verbose 100 ...
                <div>and post the output ?</div>
                <div><br>
                </div>
                <div>I will then try to reproduce the issue and
                  investigate it</div>
                <div><br>
                </div>
                <div>Cheers,</div>
                <div><br>
                </div>
                <div>Gilles</div>
                <div><br>
                  <div>On Tuesday, February 2, 2016, Peter Wind &lt;<a
                      moz-do-not-send="true"><a class="moz-txt-link-abbreviated" href="mailto:peter.wind@met.no">peter.wind@met.no</a></a>&gt;
                    wrote:<br>
                    <blockquote class="gmail_quote" style="margin:0 0 0
                      .8ex;border-left:1px #ccc solid;padding-left:1ex">
                      <div>
                        <div
style="font-family:arial,helvetica,sans-serif;font-size:12pt;color:#000000">
                          <div>Thanks Gilles,<br>
                          </div>
                          <div><br>
                          </div>
                          <div>I get the following output (I guess it is
                            not what you wanted?).<br>
                          </div>
                          <div><br>
                          </div>
                          <div>Peter<br>
                          </div>
                          <div><br>
                          </div>
                          <div><br>
                          </div>
                          <div>$ mpirun --mca osc pt2pt -np 4 a.out<br>
--------------------------------------------------------------------------<br>
                            A requested component was not found, or was
                            unable to be opened.  This<br>
                            means that this component is either not
                            installed or is unable to be<br>
                            used on your system (e.g., sometimes this
                            means that shared libraries<br>
                            that the component requires are unable to be
                            found/loaded).  Note that<br>
                            Open MPI stopped checking at the first
                            component that it did not find.<br>
                            <div><br>
                            </div>
                            Host:      stallo-2.local<br>
                            Framework: osc<br>
                            Component: pt2pt<br>
--------------------------------------------------------------------------<br>
--------------------------------------------------------------------------<br>
                            It looks like MPI_INIT failed for some
                            reason; your parallel process is<br>
                            likely to abort.  There are many reasons
                            that a parallel process can<br>
                            fail during MPI_INIT; some of which are due
                            to configuration or environment<br>
                            problems.  This failure appears to be an
                            internal failure; here's some<br>
                            additional information (which may only be
                            relevant to an Open MPI<br>
                            developer):<br>
                            <div><br>
                            </div>
                              ompi_osc_base_open() failed<br>
                              --&gt; Returned "Not found" (-13) instead
                            of "Success" (0)<br>
--------------------------------------------------------------------------<br>
                            *** An error occurred in MPI_Init<br>
                            *** on a NULL communicator<br>
                            *** MPI_ERRORS_ARE_FATAL (processes in this
                            communicator will now abort,<br>
                            ***    and potentially your MPI job)<br>
                            [stallo-2.local:38415] Local abort before
                            MPI_INIT completed successfully; not able to
                            aggregate error messages, and not able to
                            guarantee that all other processes were
                            killed!<br>
                            *** An error occurred in MPI_Init<br>
                            *** on a NULL communicator<br>
                            *** MPI_ERRORS_ARE_FATAL (processes in this
                            communicator will now abort,<br>
                            ***    and potentially your MPI job)<br>
                            [stallo-2.local:38418] Local abort before
                            MPI_INIT completed successfully; not able to
                            aggregate error messages, and not able to
                            guarantee that all other processes were
                            killed!<br>
                            *** An error occurred in MPI_Init<br>
                            *** on a NULL communicator<br>
                            *** MPI_ERRORS_ARE_FATAL (processes in this
                            communicator will now abort,<br>
                            ***    and potentially your MPI job)<br>
                            [stallo-2.local:38416] Local abort before
                            MPI_INIT completed successfully; not able to
                            aggregate error messages, and not able to
                            guarantee that all other processes were
                            killed!<br>
                            *** An error occurred in MPI_Init<br>
                            *** on a NULL communicator<br>
                            *** MPI_ERRORS_ARE_FATAL (processes in this
                            communicator will now abort,<br>
                            ***    and potentially your MPI job)<br>
                            [stallo-2.local:38417] Local abort before
                            MPI_INIT completed successfully; not able to
                            aggregate error messages, and not able to
                            guarantee that all other processes were
                            killed!<br>
-------------------------------------------------------<br>
                            Primary job  terminated normally, but 1
                            process returned<br>
                            a non-zero exit code.. Per user-direction,
                            the job has been aborted.<br>
-------------------------------------------------------<br>
--------------------------------------------------------------------------<br>
                            mpirun detected that one or more processes
                            exited with non-zero status, thus causing<br>
                            the job to be terminated. The first process
                            to do so was:<br>
                            <div><br>
                            </div>
                              Process name: [[52507,1],0]<br>
                              Exit code:    1<br>
--------------------------------------------------------------------------<br>
                            [stallo-2.local:38410] 3 more processes have
                            sent help message help-mca-base.txt /
                            find-available:not-valid<br>
                            [stallo-2.local:38410] Set MCA parameter
                            "orte_base_help_aggregate" to 0 to see all
                            help / error messages<br>
                            [stallo-2.local:38410] 2 more processes have
                            sent help message help-mpi-runtime /
                            mpi_init:startup:internal-failure<br>
                            <div><br>
                            </div>
                          </div>
                          <div><br>
                          </div>
                          <hr>
                          <blockquote style="border-left:2px solid
#1010ff;margin-left:5px;padding-left:5px;color:#000;font-weight:normal;font-style:normal;text-decoration:none;font-family:Helvetica,Arial,sans-serif;font-size:12pt">Peter,
                            <div><br>
                            </div>
                            <div>at first glance, your test program
                              looks correct.</div>
                            <div><br>
                            </div>
                            <div>can you please try to run</div>
                            <div>mpirun --mca osc pt2pt -np 4 ...</div>
                            <div><br>
                            </div>
                            <div>I  might have identified a bug with the
                              sm osc component.</div>
                            <div><br>
                            </div>
                            <div>Cheers,</div>
                            <div><br>
                            </div>
                            <div>Gilles<br>
                              <div><br>
                              </div>
                              On Tuesday, February 2, 2016, Peter Wind
                              &lt;<a moz-do-not-send="true">peter.wind@met.no</a>&gt;
                              wrote:<br>
                              <blockquote class="gmail_quote"
                                style="margin:0 0 0 .8ex;border-left:1px
                                #ccc solid;padding-left:1ex">Enclosed is
                                a short (&lt; 100 lines) fortran code
                                example that uses shared memory.<br>
                                It seems to me it behaves wrongly if
                                openmpi is used.<br>
                                Compiled with SGI/mpt , it gives the
                                right result.<br>
                                <br>
                                To fail, the code must be run on a
                                single node.<br>
                                It creates two groups of 2 processes
                                each. Within each group memory is
                                shared.<br>
                                The error is that the two groups get the
                                same memory allocated, but they should
                                not.<br>
                                <br>
                                Tested with openmpi 1.8.4, 1.8.5, 1.10.2
                                and gfortran, intel 13.0, intel 14.0<br>
                                all fail.<br>
                                <br>
                                The call:<br>
                                   call
                                MPI_Win_allocate_shared(win_size,
                                disp_unit, MPI_INFO_NULL, comm_group,
                                cp1, win, ierr)<br>
                                <br>
                                Should allocate memory only within the
                                group. But when the other group
                                allocates memory, the pointers from the
                                two groups point to the same address in
                                memory.<br>
                                <br>
                                Could you please confirm that this is
                                the wrong behaviour?<br>
                                <br>
                                Best regards,<br>
                                Peter Wind</blockquote>
                            </div>
                            <br>
_______________________________________________<br>
                            users mailing list<br>
                            <a moz-do-not-send="true">users@open-mpi.org</a><br>
                            Subscription: <a moz-do-not-send="true"
                              href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
                              target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                            Link to this post: <a
                              moz-do-not-send="true"
                              href="http://www.open-mpi.org/community/lists/users/2016/02/28429.php"
                              target="_blank"><a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/02/28429.php">http://www.open-mpi.org/community/lists/users/2016/02/28429.php</a></a><br>
                          </blockquote>
                          <div><br>
                          </div>
                        </div>
                      </div>
                    </blockquote>
                  </div>
                </div>
                <br>
                _______________________________________________<br>
                users mailing list<br>
                <a moz-do-not-send="true"
                  href="javascript:_e(%7B%7D,'cvml','users@open-mpi.org');"
                  target="_blank">users@open-mpi.org</a><br>
                Subscription: <a moz-do-not-send="true"
                  href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
                  target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
                Link to this post: <a moz-do-not-send="true"
                  href="http://www.open-mpi.org/community/lists/users/2016/02/28431.php"
                  target="_blank">http://www.open-mpi.org/community/lists/users/2016/02/28431.php</a></blockquote>
              <div><br>
              </div>
            </div>
          </div>
        </blockquote>
      </div>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/users/2016/02/28436.php">http://www.open-mpi.org/community/lists/users/2016/02/28436.php</a></pre>
    </blockquote>
    <br>
  </body>
</html>

