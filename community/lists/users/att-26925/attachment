<div dir="ltr">scif is a OFA device from Intel.<div>can you please select export MXM_IB_PORTS=mlx4_0:1 explicitly and retry</div></div><div class="gmail_extra"><br><div class="gmail_quote">On Mon, May 25, 2015 at 8:26 PM, Timur Ismagilov <span dir="ltr">&lt;<a href="mailto:tismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div><p>Hi, Mike,<br>that is what i have:<br></p><p>$ echo $LD_LIBRARY_PATH | tr &quot;:&quot; &quot;\n&quot;<br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/fca/lib               <br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/hcoll/lib             <br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/mxm/lib               <br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/ompi-mellanox-v1.8/lib<br> +intel compiler paths<br><br>$ echo $OPAL_PREFIX                                                                     <br>/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/ompi-mellanox-v1.8<br><br>I don&#39;t use LD_PRELOAD.<br><br>In the attached file(ompi_info.out) you will find the output of ompi_info -l 9  command.<br><br><strong>P.S</strong>. <br>node1 $ ./mxm_perftest<br>node2 $  ./mxm_perftest node1  -t send_lat<br>[1432568685.067067] [node151:87372:0]         shm.c:65   MXM  WARN  Could not open the KNEM device file $t /dev/knem : No such file or directory. Won&#39;t use knem.         <strong>( I don&#39;t have knem)</strong><br>[1432568685.069699] [node151:87372:0]      ib_dev.c:531  MXM  WARN  skipping device scif0 (vendor_id/par$_id = 0x8086/0x0) - not a Mellanox device                               <strong>(???)</strong><br>Failed to create endpoint: No such device<br><br>$  ibv_devinfo                                         <br>hca_id: mlx4_0                                                  <br>        transport:                      InfiniBand (0)          <br>        fw_ver:                         2.10.600                <br>        node_guid:                      0002:c903:00a1:13b0     <br>        sys_image_guid:                 0002:c903:00a1:13b3     <br>        vendor_id:                      0x02c9                  <br>        vendor_part_id:                 4099                    <br>        hw_ver:                         0x0                     <br>        board_id:                       MT_1090120019           <br>        phys_port_cnt:                  2                       <br>                port:   1                                       <br>                        state:                  PORT_ACTIVE (4) <br>                        max_mtu:                4096 (5)        <br>                        active_mtu:             4096 (5)        <br>                        sm_lid:                 1               <br>                        port_lid:               83              <br>                        port_lmc:               0x00            <br>                                                                <br>                port:   2                                       <br>                        state:                  PORT_DOWN (1)   <br>                        max_mtu:                4096 (5)        <br>                        active_mtu:             4096 (5)        <br>                        sm_lid:                 0               <br>                        port_lid:               0               <br>                        port_lmc:               0x00            <br><br><span style="font-family:arial,helvetica,sans-serif">Best regards,</span><br><span style="font-family:arial,helvetica,sans-serif">Timur.</span><br><br></p><p><br>Понедельник, 25 мая 2015, 19:39 +03:00 от Mike Dubman &lt;<a href="mailto:miked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt;:<br>
</p><div><div class="h5"><blockquote style="border-left:1px solid #0857a6;margin:10px;padding:0 0 0 10px">
	<div>
	



    











	
	


	
	
	

	

	
	

	

	
	



<div>
	
 	<div>
		
		
            <div><div dir="ltr">Hi Timur,<div>seems that yalla component was not found in your OMPI tree.</div><div>can it be that your mpirun is not from hpcx? Can you please check LD_LIBRARY_PATH,PATH, LD_PRELOAD and OPAL_PREFIX that it is pointing to the right mpirun?</div><div><br></div><div>Also, could you please check that yalla is present in the ompi_info -l 9 output?</div><div><br></div><div>Thanks</div></div><div><br><div>On Mon, May 25, 2015 at 7:11 PM, Timur Ismagilov <span dir="ltr">&lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt;</span> wrote:<br><blockquote style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div>I can password-less ssh to all nodes:<br>base$ ssh node1<br>node1$ssh node2<br>Last login: Mon May 25 18:41:23 <br>node2$ssh node3<br>Last login: Mon May 25 16:25:01<br>node3$ssh node4<br>Last login: Mon May 25 16:27:04<br>node4$<br><br>Is this correct?<br><br>In ompi-1.9 i do not have no-tree-spawn problem.<br><br><br>Понедельник, 25 мая 2015, 9:04 -07:00 от Ralph Castain &lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3arhc@open%2dmpi.org" target="_blank">rhc@open-mpi.org</a>&gt;:<div><div><br>
<blockquote style="border-left:1px solid #0857a6;margin:10px;padding:0 0 0 10px">
	<div>
	



    











	
	


	
	
	

	

	
	

	

	
	



<div>
	
 	<div>
		
		
            <div>I can’t speak to the mxm problem, but the no-tree-spawn issue indicates that you don’t have password-less ssh authorized between the compute nodes<div><br></div><div><br><div><blockquote type="cite"><div>On May 25, 2015, at 8:55 AM, Timur Ismagilov &lt;<a href="https://e.mail.ru/compose/?mailto=mailto%3atismagilov@mail.ru" target="_blank">tismagilov@mail.ru</a>&gt; wrote:</div><br><div>
<div><p>Hello!<br><br>I use ompi-v1.8.4 from hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2;<br>OFED-1.5.4.1;<br>CentOS release 6.2;<br>infiniband 4x FDR<br><br><br><br>I have two problems:<br><strong>1. I can not use mxm</strong>:<br><strong><span style="text-decoration:underline">1.a) $mpirun --mca pml cm --mca mtl mxm -host node5,node14,node28,node29 -mca plm_rsh_no_tree_spawn 1 -np 4 ./hello</span> </strong><br>--------------------------------------------------------------------------                               <br>A requested component was not found, or was unable to be opened.  This                                   <br>means that this component is either not installed or is unable to be                                     <br>used on your system (e.g., sometimes this means that shared libraries                                    <br>that the component requires are unable to be found/loaded).  Note that                                   <br>Open MPI stopped checking at the first component that it did not find.                                   <br>                                                                                                         <br>Host:      node14                                                                                        <br>Framework: pml                                                                                           <br>Component: yalla                                                                                         <br>--------------------------------------------------------------------------                               <br>*** An error occurred in MPI_Init                                                                        <br>--------------------------------------------------------------------------                               <br>It looks like MPI_INIT failed for some reason; your parallel process is                                  <br>likely to abort.  There are many reasons that a parallel process can                                     <br>fail during MPI_INIT; some of which are due to configuration or environment                              <br>problems.  This failure appears to be an internal failure; here&#39;s some                                   <br>additional information (which may only be relevant to an Open MPI                                        <br>developer):                                                                                              <br>                                                                                                         <br>  mca_pml_base_open() failed                                                                             <br>  --&gt; Returned &quot;Not found&quot; (-13) instead of &quot;Success&quot; (0)                                                <br>--------------------------------------------------------------------------                               <br>*** on a NULL communicator                                                                               <br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,                                 <br>***    and potentially your MPI job)                                                                     <br>*** An error occurred in MPI_Init                                                                        <br>[node28:102377] Local abort before MPI_INIT completed successfully; not able to aggregate error messages,<br> and not able to guarantee that all other processes were killed!                                         <br>*** on a NULL communicator                                                                               <br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,                                 <br>***    and potentially your MPI job)                                                                     <br>[node29:105600] Local abort before MPI_INIT completed successfully; not able to aggregate error messages,<br> and not able to guarantee that all other processes were killed!                                         <br>*** An error occurred in MPI_Init                                                                        <br>*** on a NULL communicator                                                                               <br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,                                 <br>***    and potentially your MPI job)                                                                     <br>[node5:102409] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, <br>and not able to guarantee that all other processes were killed!                                          <br>*** An error occurred in MPI_Init                                                                        <br>*** on a NULL communicator                                                                               <br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,                                 <br>***    and potentially your MPI job)                                                                     <br>[node14:85284] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, <br>and not able to guarantee that all other processes were killed!                                          <br>-------------------------------------------------------                                                  <br>Primary job  terminated normally, but 1 process returned                                                 <br>a non-zero exit code.. Per user-direction, the job has been aborted.                                     <br>-------------------------------------------------------                                                  <br>--------------------------------------------------------------------------                               <br>mpirun detected that one or more processes exited with non-zero status, thus causing                     <br>the job to be terminated. The first process to do so was:                                                <br>                                                                                                         <br>  Process name: [[9372,1],2]<br>  Exit code:    1                                                                                        <br>--------------------------------------------------------------------------                               <br>[login:08295] 3 more processes have sent help message help-mca-base.txt / find-available:not-valid       <br>[login:08295] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages         <br>[login:08295] 3 more processes have sent help message help-mpi-runtime / mpi_init:startup:internal-failur<br>e                                                                                                        <br><br><strong><span style="text-decoration:underline">1.b $mpirun --mca pml yalla -host node5,node14,node28,node29 -mca plm_rsh_no_tree_spawn 1 -np 4 ./hello</span> </strong><br>--------------------------------------------------------------------------                              <br>A requested component was not found, or was unable to be opened.  This                                  <br>means that this component is either not installed or is unable to be                                    <br>used on your system (e.g., sometimes this means that shared libraries                                   <br>that the component requires are unable to be found/loaded).  Note that                                  <br>Open MPI stopped checking at the first component that it did not find.                                  <br>                                                                                                        <br>Host:      node5                                                                                        <br>Framework: pml                                                                                          <br>Component: yalla                                                                                        <br>--------------------------------------------------------------------------                              <br>*** An error occurred in MPI_Init                                                                       <br>*** on a NULL communicator                                                                              <br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,                                <br>***    and potentially your MPI job)                                                                    <br>[node5:102449] Local abort before MPI_INIT completed successfully; not able to aggregate error messages,<br>and not able to guarantee that all other processes were killed!                                         <br>--------------------------------------------------------------------------                              <br>It looks like MPI_INIT failed for some reason; your parallel process is                                 <br>likely to abort.  There are many reasons that a parallel process can                                    <br>fail during MPI_INIT; some of which are due to configuration or environment                             <br>problems.  This failure appears to be an internal failure; here&#39;s some                                  <br>additional information (which may only be relevant to an Open MPI                                       <br>developer):                                                                                             <br>                                                                                                        <br>  mca_pml_base_open() failed                                                                            <br>  --&gt; Returned &quot;Not found&quot; (-13) instead of &quot;Success&quot; (0)                                               <br>--------------------------------------------------------------------------                              <br>-------------------------------------------------------                                                 <br>Primary job  terminated normally, but 1 process returned                                                <br>a non-zero exit code.. Per user-direction, the job has been aborted.                                    <br>-------------------------------------------------------                                                 <br>*** An error occurred in MPI_Init                                                                       <br>*** on a NULL communicator                                                                              <br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,                                <br>***    and potentially your MPI job)                                                                    <br>[node14:85325] Local abort before MPI_INIT completed successfully; not able to aggregate error messages,<br>and not able to guarantee that all other processes were killed!                                         <br>--------------------------------------------------------------------------                              <br>mpirun detected that one or more processes exited with non-zero status, thus causing                    <br>the job to be terminated. The first process to do so was:                                               <br>                                                                                                        <br>  Process name: [[9619,1],0]                                                                            <br>  Exit code:    1                                                                                       <br>--------------------------------------------------------------------------                              <br>[login:08552] 1 more process has sent help message help-mca-base.txt / find-available:not-valid         <br>[login:08552] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages        <br><strong><br>2. I can not remove <strong>-mca plm_rsh_no_tree_spawn 1 from mpirun cmd line</strong>:</strong><br>$mpirun -host node5,node14,node28,node29 -np 4 ./hello<br>sh: -c: line 0: syntax error near unexpected token `--tree-spawn&#39;                                        <br>sh: -c: line 0: `( test ! -r ./.profile || . ./.profile; OPAL_PREFIX=/gpfs/NETHOME/oivt1/nicevt/itf/sourc<br>es/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/ompi-mellanox-v1.8 ; export OPAL_PREFIX; PATH=/gpfs/NETHOME/o<br>ivt1/nicevt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/ompi-mellanox-v1.8/bin:$PATH ; export PA<br>TH ; LD_LIBRARY_PATH=/gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/ompi<br>-mellanox-v1.8/lib:$LD_LIBRARY_PATH ; export LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/gpfs/NETHOME/oivt1/nice<br>vt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/ompi-mellanox-v1.8/lib:$DYLD_LIBRARY_PATH ; expor<br>t DYLD_LIBRARY_PATH ;   /gpfs/NETHOME/oivt1/nicevt/itf/sources/hpcx-v1.3.0-327-icc-OFED-1.5.3-redhat6.2/o<br>mpi-mellanox-v1.8/bin/orted --hnp-topo-sig 2N:2S:2L3:16L2:16L1:16C:32H:x86_64 -mca ess &quot;env&quot; -mca orte_es<br>s_jobid &quot;625606656&quot; -mca orte_ess_vpid 3 -mca orte_ess_num_procs &quot;5&quot; -mca orte_parent_uri &quot;625606656.1;tc<br><a>p://10.65.0.105,10.64.0.105,10.67.0.105:56862</a>&quot; -mca orte_hnp_uri &quot;625606656.0;<a>tcp://10.65.0.2,10.67.0.2,8</a><br>3.149.214.101,<a href="http://10.64.0.2:54893" target="_blank">10.64.0.2:54893</a>&quot; --mca pml &quot;yalla&quot; -mca plm_rsh_no_tree_spawn &quot;0&quot; -mca plm &quot;rsh&quot; ) --tree-s<br>pawn&#39;                                                                                                    <br>--------------------------------------------------------------------------                               <br>ORTE was unable to reliably start one or more daemons.                                                   <br>This usually is caused by:                                                                               <br>                                                                                                         <br>* not finding the required libraries and/or binaries on                                                  <br>  one or more nodes. Please check your PATH and LD_LIBRARY_PATH                                          <br>  settings, or configure OMPI with --enable-orterun-prefix-by-default                                    <br>                                                                                                         <br>* lack of authority to execute on one or more specified nodes.                                           <br>  Please verify your allocation and authorities.                                                         <br>                                                                                                         <br>* the inability to write startup files into /tmp (--tmpdir/orte_tmpdir_base).                            <br>  Please check with your sys admin to determine the correct location to use.                             <br>                                                                                                         <br>*  compilation of the orted with dynamic libraries when static are required                              <br>  (e.g., on Cray). Please check your configure cmd line and consider using                               <br>  one of the contrib/platform definitions for your system type.                                          <br>                                                                                                         <br>* an inability to create a connection back to mpirun due to a                                            <br>  lack of common network interfaces and/or no route found between                                        <br>  them. Please check network connectivity (including firewalls                                           <br>  and network routing requirements).                                                                     <br>--------------------------------------------------------------------------                               <br>mpirun: abort is already in progress...hit ctrl-c again to forcibly terminate                          <br>                                                                                                         <br></p><div><span style="font-family:arial,helvetica,sans-serif">Thank you for your comments.</span></div><div><span style="font-family:arial,helvetica,sans-serif"> </span></div><div><span style="font-family:arial,helvetica,sans-serif">Best regards,<br>Timur.<br></span></div><div><span style="font-family:arial,helvetica,sans-serif"> </span><br></div><p><br><br></p></div>
_______________________________________________<br>users mailing list<br><a href="https://e.mail.ru/compose/?mailto=mailto%3ausers@open%2dmpi.org" target="_blank">users@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/05/26919.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/05/26919.php</a><br></div></blockquote></div><br></div></div>
            
        
		
	</div>

	
</div>


</div>
</blockquote>
<br>
<br> <br></div></div></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="https://e.mail.ru/compose/?mailto=mailto%3ausers@open%2dmpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/05/26922.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/05/26922.php</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br><div><div dir="ltr"><br><div>Kind Regards,</div><div><br></div><div>M.</div></div></div>
</div>
</div>
            
        
		
	</div>

	
</div>


</div>
</blockquote>
<br>
<br> <br></div></div></div>
</blockquote></div><br><br clear="all"><div><br></div>-- <br><div class="gmail_signature"><div dir="ltr"><br><div>Kind Regards,</div><div><br></div><div>M.</div></div></div>
</div>
