<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><br><div><div>On 15-Aug-09, at 1:03 AM, Alan wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite">Thanks Warner,<div><br></div><div>This is frustrating... I read the ticket. 6 months already and 2 releases postponed... Frankly, I am very skeptical that this will be fixed for 1.3.4. I really hope so, but when 1.3.4 will be released?</div> <div><br></div><div>I have to think about going with 1.2.x and possible disruptions in my configuration (I use Fink) or wait.</div><div><br></div><div>And I offered myself to test any nightly snapshot claiming this bug is fixed.</div></blockquote><div><br></div><div>Hi Alan,</div><div><br></div><div>Its not too hard to get PBS/torque up and running. &nbsp;</div><div><br></div><div>The&nbsp;OS/X-specific (?) issues&nbsp;I&nbsp;had:</div><div><br></div><div>1)&nbsp;/etc/hosts&nbsp;had&nbsp;to&nbsp;have&nbsp;the server explicitly listed on each of the nodes.</div><div>2) $usecp had to be set in mom_config</div><div>3) $restricted had to be set on the nodes in mom_priv/config to accept calls from the server. &nbsp;</div><div><br></div><div>I think that is it. &nbsp;Of course if you are already using xgrid on these machines for other uses it won't play well with PBS, but otherwise all you are missing is the cute tachometer display. &nbsp;</div><div><br></div><div>Cheers, &nbsp;Jody</div><div><br></div><blockquote type="cite"><div>Cheers,</div><div>Alan<br><br><div class="gmail_quote">On Fri, Aug 14, 2009 at 17:20, Warner Yuen <span dir="ltr">&lt;<a href="mailto:wyuen@apple.com">wyuen@apple.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;"> Hi Alan,<br> <br> Xgrid support for Open MPI is currently broken in the latest version of Open MPI. See the ticket below. However, I believe that Xgrid still works with one of the earlier 1.2 &nbsp;versions of Open MPI. I don't recall for sure, but I think that it's Open MPI 1.2.3.<br> <br> #1777: Xgrid support is broken in the v1.3 series<br> ---------------------+------------------------------------------------------<br> Reporter: &nbsp;jsquyres &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp;Owner: &nbsp;brbarret<br> &nbsp; Type: &nbsp;defect &nbsp; &nbsp;| &nbsp; &nbsp; &nbsp; Status: &nbsp;accepted<br> Priority: &nbsp;major &nbsp; &nbsp; | &nbsp; &nbsp;Milestone: &nbsp;Open MPI 1.3.4<br> Version: &nbsp;trunk &nbsp; &nbsp; | &nbsp; Resolution:<br> Keywords: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|<br> ---------------------+------------------------------------------------------<br> Changes (by bbenton):<br> <br> &nbsp;* milestone: &nbsp;Open MPI 1.3.3 =&gt; Open MPI 1.3.4<br> <br> <br> Warner Yuen<br> Scientific Computing<br> Consulting Engineer<br> Apple, Inc.<br> email: <a href="mailto:wyuen@apple.com" target="_blank">wyuen@apple.com</a><br> Tel: 408.718.2859<br> <br> <br> <br> <br> On Aug 14, 2009, at 6:21 AM, <a href="mailto:users-request@open-mpi.org" target="_blank">users-request@open-mpi.org</a> wrote:<br> <br> <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"> <br> Message: 1<br> Date: Fri, 14 Aug 2009 14:21:30 +0100<br> From: Alan &lt;<a href="mailto:alanwilter@gmail.com" target="_blank">alanwilter@gmail.com</a>&gt;<br> Subject: [OMPI users] openmpi with xgrid<br> To: <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> Message-ID:<br> &nbsp; &nbsp; &nbsp; &nbsp;&lt;<a href="mailto:cf58c8d00908140621v18d384f2wef97ee80ca3ded0c@mail.gmail.com" target="_blank">cf58c8d00908140621v18d384f2wef97ee80ca3ded0c@mail.gmail.com</a>&gt;<br> Content-Type: text/plain; charset="utf-8"<div><div></div><div class="h5"><br> <br> Hi there,<br> I saw that <a href="http://www.open-mpi.org/community/lists/users/2007/08/3900.php" target="_blank">http://www.open-mpi.org/community/lists/users/2007/08/3900.php</a>.<br> <br> I use fink, and so I changed the <a href="http://openmpi.info" target="_blank">openmpi.info</a> file in order to get openmpi<br> with xgrid support.<br> <br> As you can see:<br> amadeus[2081]:~/Downloads% /sw/bin/ompi_info<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Package: Open MPI <a href="mailto:root@amadeus.local">root@amadeus.local</a> Distribution<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Open MPI: 1.3.3<br> &nbsp;Open MPI SVN revision: r21666<br> &nbsp;Open MPI release date: Jul 14, 2009<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Open RTE: 1.3.3<br> &nbsp;Open RTE SVN revision: r21666<br> &nbsp;Open RTE release date: Jul 14, 2009<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; OPAL: 1.3.3<br> &nbsp; &nbsp; &nbsp;OPAL SVN revision: r21666<br> &nbsp; &nbsp; &nbsp;OPAL release date: Jul 14, 2009<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ident string: 1.3.3<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Prefix: /sw<br> Configured architecture: x86_64-apple-darwin9<br> &nbsp; &nbsp; &nbsp; &nbsp; Configure host: amadeus.local<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Configured by: root<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Configured on: Fri Aug 14 12:58:12 BST 2009<br> &nbsp; &nbsp; &nbsp; &nbsp; Configure host: amadeus.local<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Built by:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Built on: Fri Aug 14 13:07:46 BST 2009<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Built host: amadeus.local<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; C bindings: yes<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; C++ bindings: yes<br> &nbsp; &nbsp; Fortran77 bindings: yes (single underscore)<br> &nbsp; &nbsp; Fortran90 bindings: yes<br> Fortran90 bindings size: small<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; C compiler: gcc<br> &nbsp; &nbsp;C compiler absolute: /sw/var/lib/fink/path-prefix-10.6/gcc<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; C++ compiler: g++<br> &nbsp;C++ compiler absolute: /sw/var/lib/fink/path-prefix-10.6/g++<br> &nbsp; &nbsp; Fortran77 compiler: gfortran<br> &nbsp;Fortran77 compiler abs: /sw/bin/gfortran<br> &nbsp; &nbsp; Fortran90 compiler: gfortran<br> &nbsp;Fortran90 compiler abs: /sw/bin/gfortran<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;C profiling: yes<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;C++ profiling: yes<br> &nbsp; &nbsp;Fortran77 profiling: yes<br> &nbsp; &nbsp;Fortran90 profiling: yes<br> &nbsp; &nbsp; &nbsp; &nbsp; C++ exceptions: no<br> &nbsp; &nbsp; &nbsp; &nbsp; Thread support: posix (mpi: no, progress: no)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Sparse Groups: no<br> &nbsp;Internal debug support: no<br> &nbsp; &nbsp;MPI parameter check: runtime<br> Memory profiling support: no<br> Memory debugging support: no<br> &nbsp; &nbsp; &nbsp; &nbsp;libltdl support: yes<br> &nbsp;Heterogeneous support: no<br> mpirun default --prefix: no<br> &nbsp; &nbsp; &nbsp; &nbsp;MPI I/O support: yes<br> &nbsp; &nbsp; &nbsp;MPI_WTIME support: gettimeofday<br> Symbol visibility support: yes<br> &nbsp;FT Checkpoint support: no &nbsp;(checkpoint thread: no)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA backtrace: execinfo (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA paffinity: darwin (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA carto: auto_detect (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA carto: file (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA maffinity: first_use (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA timer: darwin (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp;MCA installdirs: env (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp;MCA installdirs: config (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA dpm: orte (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA pubsub: orte (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA allocator: basic (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA allocator: bucket (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA coll: basic (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA coll: hierarch (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA coll: inter (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA coll: self (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA coll: sm (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA coll: sync (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA coll: tuned (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA io: romio (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA mpool: fake (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA mpool: rdma (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA mpool: sm (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA pml: cm (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA pml: csum (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA pml: ob1 (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA pml: v (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA bml: r2 (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA rcache: vma (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA btl: self (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA btl: sm (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA btl: tcp (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA topo: unity (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA osc: pt2pt (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA osc: rdma (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA iof: hnp (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA iof: orted (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA iof: tool (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA oob: tcp (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA odls: default (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA ras: slurm (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA rmaps: rank_file (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA rmaps: round_robin (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA rmaps: seq (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA rml: oob (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA routed: binomial (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA routed: direct (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA routed: linear (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA plm: rsh (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA plm: slurm (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA plm: xgrid (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA filem: rsh (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MCA errmgr: default (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA ess: env (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA ess: hnp (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA ess: singleton (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA ess: slurm (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA ess: tool (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA grpcomm: bad (MCA v2.0, API v2.0, Component v1.3.3)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;MCA grpcomm: basic (MCA v2.0, API v2.0, Component v1.3.3)<br> <br> All seemed fine and I also have xgrid controller and agent running in my<br> laptop, and then when I tried:<br> <br> /sw/bin/om-mpirun -c 2 mpiapp &nbsp;# hello world example for mpi<br> [amadeus.local:40293] [[804,0],0] ORTE_ERROR_LOG: Unknown error: 1 in file<br> src/plm_xgrid_module.m at line 119<br> [amadeus.local:40293] [[804,0],0] ORTE_ERROR_LOG: Unknown error: 1 in file<br> src/plm_xgrid_module.m at line 153<br> --------------------------------------------------------------------------<br> om-mpirun was unable to start the specified application as it encountered an<br> error.<br> More information may be available above.<br> --------------------------------------------------------------------------<br> 2009-08-14 14:16:19.715 om-mpirun[40293:10b] *** Terminating app due to<br> uncaught exception 'NSInvalidArgumentException', reason: '***<br> -[NSKVONotifying_XGConnection&lt;0x1001164b0&gt; finalize]: called when collecting<br> not enabled'<br> 2009-08-14 14:16:19.716 om-mpirun[40293:10b] Stack: (<br> &nbsp; 140735390096156,<br> &nbsp; 140735366109391,<br> &nbsp; 140735390122388,<br> &nbsp; 4295943988,<br> &nbsp; 4295939168,<br> &nbsp; 4295171139,<br> &nbsp; 4295883300,<br> &nbsp; 4295025321,<br> &nbsp; 4294973498,<br> &nbsp; 4295401605,<br> &nbsp; 4295345774,<br> &nbsp; 4295056598,<br> &nbsp; 4295116412,<br> &nbsp; 4295119970,<br> &nbsp; 4295401605,<br> &nbsp; 4294972881,<br> &nbsp; 4295401605,<br> &nbsp; 4295345774,<br> &nbsp; 4295056598,<br> &nbsp; 4295172615,<br> &nbsp; 4295938185,<br> &nbsp; 4294971936,<br> &nbsp; 4294969401,<br> &nbsp; 4294969340<br> )<br> terminate called after throwing an instance of 'NSException'<br> [amadeus:40293] *** Process received signal ***<br> [amadeus:40293] Signal: Abort trap (6)<br> [amadeus:40293] Signal code: &nbsp;(0)<br> [amadeus:40293] [ 0] 2 &nbsp; libSystem.B.dylib<br> 0x00000000831443fa _sigtramp + 26<br> [amadeus:40293] [ 1] 3 &nbsp; ???<br> 0x000000005fbfb1e8 0x0 + 1606398440<br> [amadeus:40293] [ 2] 4 &nbsp; libstdc++.6.dylib<br> 0x00000000827f2085 _ZN9__gnu_cxx27__verbose_terminate_handlerEv + 377<br> [amadeus:40293] [ 3] 5 &nbsp; libobjc.A.dylib<br> 0x0000000081811adf objc_end_catch + 280<br> [amadeus:40293] [ 4] 6 &nbsp; libstdc++.6.dylib<br> 0x00000000827f0425 __gxx_personality_v0 + 1259<br> [amadeus:40293] [ 5] 7 &nbsp; libstdc++.6.dylib<br> 0x00000000827f045b _ZSt9terminatev + 19<br> [amadeus:40293] [ 6] 8 &nbsp; libstdc++.6.dylib<br> 0x00000000827f054c __cxa_rethrow + 0<br> [amadeus:40293] [ 7] 9 &nbsp; libobjc.A.dylib<br> 0x0000000081811966 objc_exception_rethrow + 0<br> [amadeus:40293] [ 8] 10 &nbsp;CoreFoundation<br> 0x0000000082ef8194 _CF_forwarding_prep_0 + 5700<br> [amadeus:40293] [ 9] 11 &nbsp;mca_plm_xgrid.so<br> 0x00000000000ee734 orte_plm_xgrid_finalize + 4884<br> [amadeus:40293] [10] 12 &nbsp;mca_plm_xgrid.so<br> 0x00000000000ed460 orte_plm_xgrid_finalize + 64<br> [amadeus:40293] [11] 13 &nbsp;libopen-rte.0.dylib<br> 0x0000000000031c43 orte_plm_base_close + 195<br> [amadeus:40293] [12] 14 &nbsp;mca_ess_hnp.so<br> 0x00000000000dfa24 0x0 + 916004<br> [amadeus:40293] [13] 15 &nbsp;libopen-rte.0.dylib<br> 0x000000000000e2a9 orte_finalize + 89<br> [amadeus:40293] [14] 16 &nbsp;om-mpirun<br> 0x000000000000183a start + 4210<br> [amadeus:40293] [15] 17 &nbsp;libopen-pal.0.dylib<br> 0x000000000006a085 opal_event_add_i + 1781<br> [amadeus:40293] [16] 18 &nbsp;libopen-pal.0.dylib<br> 0x000000000005c66e opal_progress + 142<br> [amadeus:40293] [17] 19 &nbsp;libopen-rte.0.dylib<br> 0x0000000000015cd6 orte_trigger_event + 70<br> [amadeus:40293] [18] 20 &nbsp;libopen-rte.0.dylib<br> 0x000000000002467c orte_daemon_recv + 4332<br> [amadeus:40293] [19] 21 &nbsp;libopen-rte.0.dylib<br> 0x0000000000025462 orte_daemon_cmd_processor + 722<br> [amadeus:40293] [20] 22 &nbsp;libopen-pal.0.dylib<br> 0x000000000006a085 opal_event_add_i + 1781<br> [amadeus:40293] [21] 23 &nbsp;om-mpirun<br> 0x00000000000015d1 start + 3593<br> [amadeus:40293] [22] 24 &nbsp;libopen-pal.0.dylib<br> 0x000000000006a085 opal_event_add_i + 1781<br> [amadeus:40293] [23] 25 &nbsp;libopen-pal.0.dylib<br> 0x000000000005c66e opal_progress + 142<br> [amadeus:40293] [24] 26 &nbsp;libopen-rte.0.dylib<br> 0x0000000000015cd6 orte_trigger_event + 70<br> [amadeus:40293] [25] 27 &nbsp;libopen-rte.0.dylib<br> 0x0000000000032207 orte_plm_base_launch_failed + 135<br> [amadeus:40293] [26] 28 &nbsp;mca_plm_xgrid.so<br> 0x00000000000ed089 orte_plm_xgrid_spawn + 89<br> [amadeus:40293] [27] 29 &nbsp;om-mpirun<br> 0x0000000000001220 start + 2648<br> [amadeus:40293] [28] 30 &nbsp;om-mpirun<br> 0x0000000000000839 start + 113<br> [amadeus:40293] [29] 31 &nbsp;om-mpirun<br> 0x00000000000007fc start + 52<br> [amadeus:40293] *** End of error message ***<br> [1] &nbsp; &nbsp;40293 abort &nbsp; &nbsp; &nbsp;/sw/bin/om-mpirun -c 2 mpiapp<br> <br> <br> Is there anyone using openmpi with xgrid successfully keen to share his/her<br> experience? I am not new to xgrid or mpi, but to both integrated I must say<br> that I am in uncharted waters.<br> <br> Any help would be very appreciated.<br> <br> Many thanks in advance,<br> Alan<br> -- <br> Alan Wilter S. da Silva, D.Sc. - CCPN Research Associate<br> Department of Biochemistry, University of Cambridge.<br> 80 Tennis Court Road, Cambridge CB2 1GA, UK.<br> <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"> <a href="http://www.bio.cam.ac.uk/~awd28" target="_blank">http://www.bio.cam.ac.uk/~awd28</a>&lt;&lt;<br> </blockquote></blockquote></div></div> -------------- next part --------------<br> HTML attachment scrubbed and removed<br> <br> ------------------------------<br> <br> _______________________________________________<br> users mailing list<br> <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> <br> End of users Digest, Vol 1318, Issue 2<br> **************************************<br> </blockquote> <br> _______________________________________________<br> users mailing list<br> <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br> </blockquote></div><br><br clear="all"><br>-- <br>Alan Wilter S. da Silva, D.Sc. - CCPN Research Associate<br>Department of Biochemistry, University of Cambridge.<br>80 Tennis Court Road, Cambridge CB2 1GA, UK.<br>&gt;&gt;<a href="http://www.bio.cam.ac.uk/~awd28">http://www.bio.cam.ac.uk/~awd28</a>&lt;&lt;<br> </div> _______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></body></html>
