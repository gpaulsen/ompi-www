Jeff,<br><br>An update of what I did. Apparently, one of my lab mates installed another version of OpenMPI manually and it clashed with the OpenMPI I installed from the Ubuntu repository. I manually identified the files installed and deleted them. After I installed OpenMPI from Ubuntu repository, my &quot;mpirun.openmpi&quot; works!<br>
<br><div class="gmail_quote">On Fri, Jul 16, 2010 at 4:41 PM, TH Chew <span dir="ltr">&lt;<a href="mailto:teonghan@gmail.com">teonghan@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">
Jeff,<br><br>Thanks for the suggestion. Been looking into it and although, I installed the same OpenMPI version. But somehow, another software (Discovery Studio) was installed on birg-desktop-10, causing the mpirun to be messed up (since Discovery Studio also install some kind of mpirun/mpiexec). I type &quot;mpirun.openmpi --version&quot; on birg-desktop-10, the output is:<br>

<br>####################################################<br>birg@birg-desktop-10:~$ mpirun.openmpi --version<br>mpirun.openmpi: symbol lookup error: mpirun.openmpi: undefined symbol: orted_cmd_line<br>####################################################<br>

<br>and when I type on other machine<br><br>####################################################<br>birg@birg-frontnode:~/Desktop/nfs_shared$ mpirun.openmpi --version<br>mpirun.openmpi (OpenRTE) 1.4.1<br><br>Report bugs to <a href="http://www.open-mpi.org/community/help/" target="_blank">http://www.open-mpi.org/community/help/</a><br>

####################################################<br><br>I am now uninstalling Discovery Studio and see whether it works or not.<br><br>Thanks again.<div><div></div><div class="h5"><br><br><div class="gmail_quote">On Thu, Jul 15, 2010 at 7:15 PM, Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 204, 204); padding-left: 1ex;">This usually means that you have mis-matched versions of Open MPI across your machines.  Double check that you have the same version of Open MPI installed on all the machines that you&#39;ll be running (e.g., perhaps birg-desktop-10 has a different version?).<br>


<div><div></div><div><br>
<br>
On Jul 15, 2010, at 5:18 AM, TH Chew wrote:<br>
<br>
&gt; Hi all,<br>
&gt;<br>
&gt; I am setting up a 7+1 nodes cluster for MD simulation, specifically using GROMACS. I am using Ubuntu Lucid 64-bit on all machines. Installed gromacs, gromacs-openmpi, and gromacs-mpich from the repository. MPICH version of gromacs runs fine without any error. However, when I ran OpenMPI version of gromacs by<br>


&gt;<br>
&gt; ###########################################################################<br>
&gt; mpirun.openmpi -np 8 -wdir /home/birg/Desktop/nfs/ -hostfile ~/Desktop/mpi_settings/hostfile mdrun_mpi.openmpi -v<br>
&gt; ###########################################################################<br>
&gt;<br>
&gt; an error occur, something like this<br>
&gt;<br>
&gt; ###########################################################################<br>
&gt; [birg-desktop-10:02101] Error: unknown option &quot;--daemonize&quot;<br>
&gt; Usage: orted [OPTION]...<br>
&gt; Start an Open RTE Daemon<br>
&gt;<br>
&gt;    --bootproxy &lt;arg0&gt;    Run as boot proxy for &lt;job-id&gt;<br>
&gt; -d|--debug               Debug the OpenRTE<br>
&gt; -d|--spin                Have the orted spin until we can connect a debugger<br>
&gt;                          to it<br>
&gt;    --debug-daemons       Enable debugging of OpenRTE daemons<br>
&gt;    --debug-daemons-file  Enable debugging of OpenRTE daemons, storing output<br>
&gt;                          in files<br>
&gt;    --gprreplica &lt;arg0&gt;   Registry contact information.<br>
&gt; -h|--help                This help message<br>
&gt;    --mpi-call-yield &lt;arg0&gt;<br>
&gt;                          Have MPI (or similar) applications call yield when<br>
&gt;                          idle<br>
&gt;    --name &lt;arg0&gt;         Set the orte process name<br>
&gt;    --no-daemonize        Don&#39;t daemonize into the background<br>
&gt;    --nodename &lt;arg0&gt;     Node name as specified by host/resource<br>
&gt;                          description.<br>
&gt;    --ns-nds &lt;arg0&gt;       set sds/nds component to use for daemon (normally<br>
&gt;                          not needed)<br>
&gt;    --nsreplica &lt;arg0&gt;    Name service contact information.<br>
&gt;    --num_procs &lt;arg0&gt;    Set the number of process in this job<br>
&gt;    --persistent          Remain alive after the application process<br>
&gt;                          completes<br>
&gt;    --report-uri &lt;arg0&gt;   Report this process&#39; uri on indicated pipe<br>
&gt;    --scope &lt;arg0&gt;        Set restrictions on who can connect to this<br>
&gt;                          universe<br>
&gt;    --seed                Host replicas for the core universe services<br>
&gt;    --set-sid             Direct the orted to separate from the current<br>
&gt;                          session<br>
&gt;    --tmpdir &lt;arg0&gt;       Set the root for the session directory tree<br>
&gt;    --universe &lt;arg0&gt;     Set the universe name as<br>
&gt;                          username@hostname:universe_name for this<br>
&gt;                          application<br>
&gt;    --vpid_start &lt;arg0&gt;   Set the starting vpid for this job<br>
&gt; --------------------------------------------------------------------------<br>
&gt; A daemon (pid 5598) died unexpectedly with status 251 while attempting<br>
&gt; to launch so we are aborting.<br>
&gt;<br>
&gt; There may be more information reported by the environment (see above).<br>
&gt;<br>
&gt; This may be because the daemon was unable to find all the needed shared<br>
&gt; libraries on the remote node. You may set your LD_LIBRARY_PATH to have the<br>
&gt; location of the shared libraries on the remote nodes and this will<br>
&gt; automatically be forwarded to the remote nodes.<br>
&gt; --------------------------------------------------------------------------<br>
&gt; --------------------------------------------------------------------------<br>
&gt; mpirun.openmpi noticed that the job aborted, but has no info as to the process<br>
&gt; that caused that situation.<br>
&gt; --------------------------------------------------------------------------<br>
&gt; --------------------------------------------------------------------------<br>
&gt; mpirun.openmpi was unable to cleanly terminate the daemons on the nodes shown<br>
&gt; below. Additional manual cleanup may be required - please refer to<br>
&gt; the &quot;orte-clean&quot; tool for assistance.<br>
&gt; --------------------------------------------------------------------------<br>
&gt;     birg-desktop-04 - daemon did not report back when launched<br>
&gt;     birg-desktop-07 - daemon did not report back when launched<br>
&gt;     birg-desktop-10 - daemon did not report back when launched<br>
&gt; ###########################################################################<br>
&gt;<br>
&gt; It is strange that it only happen on one of the compute node (birg-desktop-10). If I remove birg-desktop-10 from the hostfile, I can run OpenMPI gromacs successfully. Any idea?<br>
&gt;<br>
&gt; Thanks.<br>
&gt;<br>
&gt; --<br>
&gt; Regards,<br>
&gt; THChew<br>
</div></div>&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a><br>
For corporate legal information go to:<br>
<a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</blockquote></div><br><br clear="all"><br></div></div><font color="#888888">-- <br>Regards,<br>THChew<br>
</font></blockquote></div><br><br clear="all"><br>-- <br>Regards,<br>THChew<br>

