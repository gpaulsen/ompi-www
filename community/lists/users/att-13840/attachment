<br><br><div class="gmail_quote">On Wed, Jul 28, 2010 at 11:09 AM, Gus Correa <span dir="ltr">&lt;<a href="mailto:gus@ldeo.columbia.edu">gus@ldeo.columbia.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">

Hi Cristobal<br>
<br>
In case you are not using full path name for mpiexec/mpirun,<br>
what does &quot;which mpirun&quot; say?<br></blockquote><div><br></div><div>--&gt; $which mpirun</div><div>      /opt/openmpi-1.4.2</div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">


<br>
Often times this is a source of confusion, old versions may<br>
be first on the PATH.<br>
<br>
Gus<br></blockquote><div><br></div><div>openMPI version problem is now gone, i can confirm that the version is consistent now :), thanks.</div><div><br></div><div>however, i keep getting this kernel crash randomnly when i execute with -np higher than 5</div>

<div>these are Xeons, with Hyperthreading On, is that a problem??</div><div><br></div><div>im trying to locate the kernel error on logs, but after rebooting a crash, the error is not in the kern.log (neither kern.log.1).</div>

<div>all i remember is that it starts with &quot;Kernel BUG...&quot;</div><div>and somepart it mentions a certain CPU X, where that cpu can be any from 0 to 15 (im testing only in main node).  Someone knows where the log of kernel error could be?</div>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
<br>
Cristobal Navarro wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div></div><div class="h5">
<br>
On Tue, Jul 27, 2010 at 7:29 PM, Gus Correa &lt;<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a> &lt;mailto:<a href="mailto:gus@ldeo.columbia.edu" target="_blank">gus@ldeo.columbia.edu</a>&gt;&gt; wrote:<br>


<br>
    Hi Cristobal<br>
<br>
    Does it run only on the head node alone?<br>
    (Fuego? Agua? Acatenango?)<br>
    Try to put only the head node on the hostfile and execute with mpiexec.<br>
<br>
--&gt; i will try only with the head node, and post results back <br>
    This may help sort out what is going on.<br>
    Hopefully it will run on the head node.<br>
<br>
    Also, do you have Infinband connecting the nodes?<br>
    The error messages refer to the openib btl (i.e. Infiniband),<br>
    and complains of<br>
<br>
<br>
no we are just using normal network 100MBit/s , since i am just testing yet.<br>
<br>
<br>
    &quot;perhaps a missing symbol, or compiled for a different<br>
    version of Open MPI?&quot;.<br>
    It sounds as a mixup of versions/builds.<br>
<br>
<br>
--&gt; i agree, somewhere there must be the remains of the older version <br>
<br>
    Did you configure/build OpenMPI from source, or did you install<br>
    it with apt-get?<br>
    It may be easier/less confusing to install from source.<br>
    If you did, what configure options did you use?<br>
<br>
<br>
--&gt;i installed from source, ./configure --prefix=/opt/openmpi-1.4.2 --with-sge --without-xgid --disable--static <br>
<br>
    Also, as for the OpenMPI runtime environment,<br>
    it is not enough to set it on<br>
    the command line, because it will be effective only on the head node.<br>
    You need to either add them to the PATH and LD_LIBRARY_PATH<br>
    on your .bashrc/.cshrc files (assuming these files and your home<br>
    directory are *also* shared with the nodes via NFS),<br>
    or use the --prefix option of mpiexec to point to the OpenMPI main<br>
    directory.<br>
<br>
<br>
yes, all nodes have their PATH and LD_LIBRARY_PATH set up properly inside the login scripts ( .bashrc in my case  ) <br>
<br>
    Needless to say, you need to check and ensure that the OpenMPI<br>
    directory (and maybe your home directory, and your work directory)<br>
    is (are)<br>
    really mounted on the nodes.<br>
<br>
<br>
--&gt; yes, doublechecked that they are <br>
<br>
    I hope this helps,<br>
<br>
<br>
--&gt; thanks really! <br>
<br>
    Gus Correa<br>
<br>
    Update: i just reinstalled openMPI, with the same parameters, and it<br>
    seems that the problem has gone, i couldnt test entirely but when i<br>
    get back to lab ill confirm. <br>
<br>
best regards! Cristobal<br>
<br>
<br></div></div><div class="im">
------------------------------------------------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></blockquote><div><div></div><div class="h5">
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

