<div class="nodecontent"><p>Hi,<br> I have set up an Xgrid including
one laptop and 7 Mac mini nodes (all are duo core machines). I have
also installed openMPI (openmpi 1.2.1) on all nodes. The laptop node
(hostname: sib) has three roles: agent, controller and client, all the
other nodes are only agents. </p>
<p> When I started &quot;mpirun -n 8 /bin/hostname&quot; on my laptop node
terminal, it shows all 8 nodes&#39; hostnames correctly. It seems that
xgrid works fine. </p>
<p>Then I wanted to run a simple mpi code. The source code &quot;Hello.c&quot;
has been compiled (use mpicc) and the excuatalbe &quot;Hello&quot; has been
copied to each node under same path(I have also tested they all run
properly on each of the local nodes.). when I asked for 1 or 2
processors to run the job, xgrid worked fine, but when I asked for 3 or
more processors, all jobs were failed. Following are the commands and
the results/messages that I got. </p>
<p>Can anybody help me out? </p>
<p>*************************************<br>
running &quot;hostname&quot; and the results, they looks good.<br>
*************************************<br>
sib:sharcnet$ mpirun -n 8 /bin/hostname<br>
node2<br>
node8<br>
node4<br>
node5<br>
node3<br>
node7<br>
sib<br>
node6</p>
<p>*************************************<br>
 the simple mpi program Hello.c source code<br>
*************************************<br>
#include <br>
#include </p>
<p>int main(int argc, char *argv[]) {<br>
  int numprocs, rank, namelen;<br>
  char processor_name[MPI_MAX_PROCESSOR_NAME];</p>
<p>  MPI_Init(&amp;argc, &amp;argv);<br>
  MPI_Comm_size(MPI_COMM_WORLD, &amp;numprocs);<br>
  MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);<br>
  MPI_Get_processor_name(processor_name, &amp;namelen);</p>
<p>  printf(&quot;Process %d on %s out of %d\n&quot;, rank, processor_name, numprocs);</p>
<p>  MPI_Finalize();<br>
}</p>
<p>*************************************<br>
ask for 1 and 2 processors to run &quot;Hello&quot;<br>
and the results are all good<br>
*************************************<br>
sib:sharcnet$ mpirun -n 1 ~/openMPI_sutuff/Hello<br>
Process 0 on sib out of 1</p>
<p>sib:sharcnet$ mpiurun -n 2 ~/openMPI_stuff/Hello<br>
Process 1 on node2 out of 2<br>
Process 0 on sib out of 2</p>
<p>*************************************<br>
Here is the problem when<br>
ask for 3 processors to run the job,<br>
following are all the messages I got<br>
*************************************</p>
<p>sib:sharcnet$ mpirun -n 3 ~/openMPI_stuff/Hello</p>
<p>Process 0.1.1 is unable to reach 0.1.2 for MPI communication.<br>
If you specified the use of a BTL component, you may have<br>
forgotten a component (such as &quot;self&quot;) in the list of<br>
usable components.</p>
<p>Process 0.1.2 is unable to reach 0.1.1 for MPI communication.<br>
If you specified the use of a BTL component, you may have<br>
forgotten a component (such as &quot;self&quot;) in the list of<br>
usable components.</p>
<p>It looks like MPI_INIT failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during MPI_INIT; some of which are due to configuration or environment<br>
problems.  This failure appears to be an internal failure; here&#39;s some<br>
additional information (which may only be relevant to an Open MPI<br>
developer):</p>
<p>  PML add procs failed<br>
  --&gt; Returned &quot;Unreachable&quot; (-12) instead of &quot;Success&quot; (0)</p>
<p>*** An error occurred in MPI_Init<br>
*** before MPI was initialized<br>
*** MPI_ERRORS_ARE_FATAL (goodbye)</p>
<p>It looks like MPI_INIT failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during MPI_INIT; some of which are due to configuration or environment<br>
problems.  This failure appears to be an internal failure; here&#39;s some<br>
additional information (which may only be relevant to an Open MPI<br>
developer):</p>
<p>  PML add procs failed<br>
  --&gt; Returned &quot;Unreachable&quot; (-12) instead of &quot;Success&quot; (0)</p>
<p>*** An error occurred in MPI_Init<br>
*** before MPI was initialized<br>
*** MPI_ERRORS_ARE_FATAL (goodbye)<br>
mpirun noticed that job rank 0 with PID 817 on node xgrid-node-0 exited on signal 15 (Terminated). </p>
<p>sib:sharcnet$ </p>
</div>

