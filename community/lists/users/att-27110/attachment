<html dir="ltr">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=Windows-1252">
<style type="text/css" id="owaParaStyle"></style>
</head>
<body fpstyle="1" ocsi="0">
<div style="direction: ltr;font-family: Tahoma;color: #000000;font-size: 10pt;"><span style="font-size: 13.3333330154419px;">Howard,</span>
<div style="font-size: 13.3333330154419px;"><br>
</div>
<div style="font-size: 13.3333330154419px;">I do not run into a problem when I have one parent spawning many children (tested up to 100 children ranks), but am seeing the problem when I have, for example, 8 parents launching 10 children each.</div>
<div style="font-size: 13.3333330154419px;"><br>
</div>
<div style="font-size: 13.3333330154419px;">- Ken</div>
<div style="font-family: Times New Roman; color: #000000; font-size: 16px">
<hr tabindex="-1">
<div id="divRpF72885" style="direction: ltr;"><font face="Tahoma" size="2" color="#000000"><b>From:</b> users [users-bounces@open-mpi.org] on behalf of Howard Pritchard [hppritcha@gmail.com]<br>
<b>Sent:</b> Thursday, June 11, 2015 2:36 PM<br>
<b>To:</b> Open MPI Users<br>
<b>Subject:</b> Re: [OMPI users] orted segmentation fault in pmix on master<br>
</font><br>
</div>
<div></div>
<div>
<div dir="ltr">Hi Ken,
<div><br>
</div>
<div>Could you post the output of your ompi_info?</div>
<div><br>
</div>
<div>I have PrgEnv-gnu/5.2.56 and gcc/4.9.2 loaded in my env on nersc system.&nbsp; Following configure line:</div>
<div><br>
</div>
<div>./configure --enable-mpi-java --prefix=my_favorite_install_location</div>
<div><br>
</div>
<div>The general rule of thumb on cray's with master (not with older versions though) is you should be able to</div>
<div>do a ./configure (install location) and you're ready to go, no need for complicated platform files, etc.</div>
<div>to just build vanilla.</div>
<div><br>
</div>
<div>As you're probably guessing, I'm going to say it works for me, at least up to 68 slave ranks.</div>
<div><br>
</div>
<div>I do notice there's some glitch with the mapping of the ranks though.&nbsp; The binding logic seems</div>
<div>to think there's oversubscription of cores even when there should not be.&nbsp; I had to use the</div>
<div><br>
</div>
<div>--bind-to none</div>
<div><br>
</div>
<div>option on the command line once I asked for more than 22 slave ranks. &nbsp;edison system has</div>
<div>has 24 cores/node.</div>
<div><br>
</div>
<div>Howard</div>
<div><br>
</div>
<div><br>
</div>
<div class="gmail_extra"><br>
<div class="gmail_quote">2015-06-11 12:10 GMT-06:00 Leiter, Kenneth W CIV USARMY ARL (US)
<span dir="ltr">&lt;<a href="mailto:kenneth.w.leiter2.civ@mail.mil" target="_blank">kenneth.w.leiter2.civ@mail.mil</a>&gt;</span>:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex; border-left:1px #ccc solid; padding-left:1ex">
I will try on a non-cray machine as well.<br>
<br>
- Ken<br>
<br>
-----Original Message-----<br>
From: users [mailto:<a href="mailto:users-bounces@open-mpi.org" target="_blank">users-bounces@open-mpi.org</a>] On Behalf Of Howard Pritchard<br>
Sent: Thursday, June 11, 2015 12:21 PM<br>
To: Open MPI Users<br>
Subject: Re: [OMPI users] orted segmentation fault in pmix on master<br>
<br>
Hello Ken,<br>
<br>
Could you give the details of the allocation request (qsub args) as well as the mpirun command line args? I'm trying to reproduce on the nersc system.<br>
<br>
It would be interesting if you have access to a similar size non-cray cluster if you get the same problems.<br>
<br>
Howard<br>
<br>
<br>
2015-06-11 9:13 GMT-06:00 Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a> &lt;mailto:<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt; &gt;:<br>
<br>
<br>
&nbsp; &nbsp; &nbsp; &nbsp; I don’t have a Cray, but let me see if I can reproduce this on something else<br>
<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; On Jun 11, 2015, at 7:26 AM, Leiter, Kenneth W CIV USARMY ARL (US) &lt;<a href="mailto:kenneth.w.leiter2.civ@mail.mil" target="_blank">kenneth.w.leiter2.civ@mail.mil</a> &lt;mailto:<a href="mailto:kenneth.w.leiter2.civ@mail.mil" target="_blank">kenneth.w.leiter2.civ@mail.mil</a>&gt;
 &gt; wrote:<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt;<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; Hello,<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt;<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; I am attempting to use the openmpi development master for a code that uses<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; dynamic process management (i.e. MPI_Comm_spawn) on our Cray XC40 at the<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; Army Research Laboratory. After reading through the mailing list I came to<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; the conclusion that the master branch is the only hope for getting this to<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; work on the newer Cray machines.<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt;<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; To test I am using the cpi-master.c cpi-worker.c example. The test works<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; when executing on a small number of processors, five or less, but begins to<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; fail with segmentation faults in orted when using more processors. Even with<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; five or fewer processors, I am spreading the computation to more than one<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; node. I am using the cray ugni btl through the alps scheduler.<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt;<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; I get a core file from orted and have the seg fault tracked down to<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; pmix_server_process_msgs.c:420 where req-&gt;proxy is NULL. I have tried<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; reading the code to understand how this happens, but am unsure. I do see<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; that in the if statement where I take the else branch, the other branch<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; specifically checks &quot;if (NULL == req-&gt;proxy)&quot; - however, no such check is<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; done the the else branch.<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt;<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; I have debug output dumped for the failing runs. I can provide the output<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; along with ompi_info output and config.log to anyone who is interested.<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt;<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; - Ken Leiter<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt;<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; _______________________________________________<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; users mailing list<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&nbsp; &nbsp; &nbsp; &nbsp; &gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/06/27094.php" rel="noreferrer" target="_blank">
http://www.open-mpi.org/community/lists/users/2015/06/27094.php</a><br>
<br>
&nbsp; &nbsp; &nbsp; &nbsp; _______________________________________________<br>
&nbsp; &nbsp; &nbsp; &nbsp; users mailing list<br>
&nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a> &lt;mailto:<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a>&gt;<br>
&nbsp; &nbsp; &nbsp; &nbsp; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&nbsp; &nbsp; &nbsp; &nbsp; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/06/27095.php" rel="noreferrer" target="_blank">
http://www.open-mpi.org/community/lists/users/2015/06/27095.php</a><br>
<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/06/27103.php" rel="noreferrer" target="_blank">
http://www.open-mpi.org/community/lists/users/2015/06/27103.php</a><br>
</blockquote>
</div>
<br>
</div>
</div>
</div>
</div>
</div>
</body>
</html>

