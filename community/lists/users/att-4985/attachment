Thank you Josh, that&#39;s interesting. I&#39;ll have a look.<br>--Oleg<br><br><div class="gmail_quote">On Feb 5, 2008 2:39 PM, Josh Hursey &lt;<a href="mailto:jjhursey@open-mpi.org">jjhursey@open-mpi.org</a>&gt; wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">Oleg,<br><br>Interesting work. You mentioned late in your email that you believe<br>that adding support for piggybacking to the MPI standard would be the<br>
best solution. As you may know, the MPI Forum has reconvened and there<br>is a working group for Fault Tolerance. This working group is<br>discussing a piggybacking interface proposal for the standard, amongst<br>other things. If you are interested in contributing to this<br>
conversation you can find the mailing list here:<br> &nbsp;<a href="http://lists.cs.uiuc.edu/mailman/listinfo/mpi3-ft" target="_blank">http://lists.cs.uiuc.edu/mailman/listinfo/mpi3-ft</a><br><br>Best,<br><font color="#888888">Josh<br>
</font><div><div></div><div class="Wj3C7c"><br>On Feb 5, 2008, at 4:58 AM, Oleg Morajko wrote:<br><br>&gt; Hi,<br>&gt;<br>&gt; I&#39;ve been working on MPI piggyback technique as a part of my PhD work.<br>&gt;<br>&gt; Although MPI does not provide a native support, there are several<br>
&gt; different<br>&gt; solutions to transmit piggyback data over every MPI communication.<br>&gt; You may<br>&gt; find a brief overview in papers [1, 2]. This includes copying the<br>&gt; original<br>&gt; message and the extra data to a bigger buffer, sending additional<br>
&gt; message or<br>&gt; changing the sendtype to a dynamically created wrapper datatype that<br>&gt; contains a pointer to the original data and the piggyback data. I<br>&gt; have tried<br>&gt; all mechanisms and they work, but considering the overhead, there is<br>
&gt; no &quot;the<br>&gt; best&quot; technique that outperforms the others in all scenarios. Jeff<br>&gt; Squyres<br>&gt; had interesting comments on this subject before (in this mailing<br>&gt; list).<br>&gt;<br>&gt; Finally after some benchmarking, I have implemented *a *hybrid<br>
&gt; technique<br>&gt; that combines existing mechanisms. For small, point-to-point messages<br>&gt; datatype wrapping seems to be the less intrusive, at least considering<br>&gt; OpenMPI implementation of derived datatypes. For large, point-to-point<br>
&gt; messages, experiments confirmed that sending an additional message<br>&gt; is much<br>&gt; cheaper than wrapping (and besides the intrusion is small as we are<br>&gt; already<br>&gt; sending a large message). Moreover, the implementation may<br>
&gt; interleave the<br>&gt; original send with an asynchronous send of piggyback data. This<br>&gt; optimization<br>&gt; partially hides the latency of additional send and lowers overall<br>&gt; intrusion.<br>&gt; The &nbsp;same criteria can be applied for collective operations, except<br>
&gt; barrier<br>&gt; and reduce operations. As the former does not transmit any data and<br>&gt; the<br>&gt; latter transforms the data, the only solution is to send additional<br>&gt; messages.<br>&gt;<br>&gt; There is a penalty of course. Especially for collective operations<br>
&gt; with very<br>&gt; small messages the intrusion may reach 15% and that&#39;s a lot. It than<br>&gt; decreases down to 0.1% for bigger messages, but anyway it&#39;s still<br>&gt; there. I<br>&gt; don&#39;t know what are your requirements/expectations for that issue.<br>
&gt; The only<br>&gt; work that reported lower overheads is [3] but they added native<br>&gt; piggyback<br>&gt; support by changing underlying MPI implementation.<br>&gt;<br>&gt; I think the best possible option is to add piggyback support for MPI<br>
&gt; as a<br>&gt; part of the standard. A growing number of runtime tools use this<br>&gt; functionality for multiple reasons and certainly PMPI itself is not<br>&gt; enough.<br>&gt; References of interest:<br>&gt;<br>&gt; &nbsp; -<br>
&gt;<br>&gt; &nbsp; [1] Shende, S., Malony, A., Morris, A., Wolf, F. &quot;Performance<br>&gt; &nbsp; Profiling Overhead Compensation for MPI Programs&quot;. 12th EuroPVM-MPI<br>&gt; &nbsp; Conference, LNCS, vol. 3666, pp. 359-367, 2005. &nbsp;They review various<br>
&gt; &nbsp; techniques and &nbsp;come up with datatype wrapping.<br>&gt;<br>&gt; &nbsp; -<br>&gt;<br>&gt; &nbsp; [2] Schulz, M., &quot;Extracting Critical Path Graphs from MPI<br>&gt; &nbsp; Applications&quot;. Cluster Computing 2005, IEEE International, pp. 1-10,<br>
&gt; &nbsp; September 2005. They use datatype wrapping.<br>&gt; &nbsp; - [3] Jeffrey Vetter, &quot;Dynamic Statistical Profiling of<br>&gt; Communication<br>&gt; &nbsp; Activity in Distributed Applications&quot;. They add support for<br>
&gt; piggyback at MPI<br>&gt; &nbsp; implementation level and report very low overheads (no surprise).<br>&gt;<br>&gt; Regards,<br>&gt; Oleg Morajko<br>&gt;<br>&gt;<br>&gt; On Feb 1, 2008 5:08 PM, Aurélien Bouteiller &lt;<a href="mailto:bouteill@eecs.utk.edu">bouteill@eecs.utk.edu</a>&gt;<br>
&gt; wrote:<br>&gt;<br>&gt;&gt; I don&#39;t know of any work in that direction for now. Indeed, we plan<br>&gt;&gt; to<br>&gt;&gt; eventually integrate at least causal message logging in the pml-v,<br>&gt;&gt; which also includes piggybacking. Therefore we are open for<br>
&gt;&gt; collaboration with you on this matter. Please let us know :)<br>&gt;&gt;<br>&gt;&gt; Aurelien<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt; Le 1 févr. 08 à 09:51, Thomas Ropars a écrit :<br>&gt;&gt;<br>&gt;&gt;&gt; Hi,<br>
&gt;&gt;&gt;<br>&gt;&gt;&gt; I&#39;m currently working on optimistic message logging and I would like<br>&gt;&gt;&gt; to<br>&gt;&gt;&gt; implement an optimistic message logging protocol in OpenMPI.<br>&gt;&gt;&gt; Optimistic<br>
&gt;&gt;&gt; message logging protocols piggyback information about dependencies<br>&gt;&gt;&gt; between processes on the application messages to be able to find a<br>&gt;&gt;&gt; consistent global state after a failure. That&#39;s why I&#39;m interested<br>
&gt;&gt;&gt; in<br>&gt;&gt;&gt; the problem of piggybacking information on MPI messages.<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; Is there some works on this problem at the moment ?<br>&gt;&gt;&gt; Has anyone already implemented some mechanisms in OpenMPI to<br>
&gt;&gt;&gt; piggyback<br>&gt;&gt;&gt; data on MPI messages?<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; Regards,<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; Thomas<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; Oleg Morajko wrote:<br>&gt;&gt;&gt;&gt; Hi,<br>&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; I&#39;m developing a causality chain tracking library and need a<br>&gt;&gt;&gt;&gt; mechanism<br>&gt;&gt;&gt;&gt; to attach an extra data to every MPI message, so called piggyback<br>&gt;&gt;&gt;&gt; mechanism.<br>
&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; As far as I know there are a few solutions to this problem from<br>&gt;&gt;&gt;&gt; which<br>&gt;&gt;&gt;&gt; the two fundamental ones are the following:<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; &nbsp; * Dynamic datatype wrapping - if a user MPI_Send, let&#39;s say 1024<br>
&gt;&gt;&gt;&gt; &nbsp; &nbsp; doubles, the wrapped send call implementation dynamically<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; creates a derived datatype that is a structure composed of a<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; pointer to 1024 doubles and extra fields to be piggybacked. The<br>
&gt;&gt;&gt;&gt; &nbsp; &nbsp; datatype is constructed with absolute addresses to avoid<br>&gt;&gt;&gt;&gt; copying<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; the original buffer. The receivers side creates the equivalent<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; datatype to receive the original data and extra data. The<br>
&gt;&gt;&gt;&gt; &nbsp; &nbsp; performance of this solution depends on the how good is derived<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; data type handling, but seems to be lightweight.<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; &nbsp; * Sending extra data in a separate message -- seems this can have<br>
&gt;&gt;&gt;&gt; &nbsp; &nbsp; much more significant overhead<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; Do you know any other portable solution?<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; I have implemented the first solution for P2P operations and it<br>
&gt;&gt;&gt;&gt; works<br>&gt;&gt;&gt;&gt; pretty well. However there are problems with collective operations.<br>&gt;&gt;&gt;&gt; There are 2 classes of collective calls that are problematic:<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; &nbsp;1. Single receiver calls, like MPI_Gather. The sender tasks in<br>
&gt;&gt;&gt;&gt; &nbsp; &nbsp; gather can be handled in the same way as a normal send, a data<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; item is wrapped and extra data is piggybacked with the message.<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; The problem is at the receiver side when a root gathers N data<br>
&gt;&gt;&gt;&gt; &nbsp; &nbsp; items that must be received in an array big enough to receive<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; all items strided by datatype extent.<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; In particular, it seems impossible to construct a datatype that<br>
&gt;&gt;&gt;&gt; &nbsp; &nbsp; contains data item and extra data (i.e. structure type with<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; absolute addresses) AND make an array of these datatypes<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; separated by a fixed extent. For example: data item to receive<br>
&gt;&gt;&gt;&gt; &nbsp; &nbsp; from every process is a vector of 1024 doubles. Extra data is a<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; single integer. User provides a receive buffer with place for N<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; * 1024 * double. The library allocates an array of N integers<br>
&gt;&gt;&gt;&gt; to<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; receive piggybacked data. How to construct a datatype that can<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; be used to receive data in MPI_Gather?<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; &nbsp;2. MPI_Reduce calls. There is no problem with datatypes as the<br>
&gt;&gt;&gt;&gt; &nbsp; &nbsp; receiver gets the single data item and not an array as in<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; previous case. The problem is the reduction operator itself<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; (MPI_Op) because these operators do not work with wrapped data<br>
&gt;&gt;&gt;&gt; &nbsp; &nbsp; types. So I can create a new operator to recognize the wrapped<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; data type that extracts the original data (skipping extra data)<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; and performs the original reduction. The point is how to invoke<br>
&gt;&gt;&gt;&gt; &nbsp; &nbsp; the original reduction on an existing datatype. I have found<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; that Open MPI calls internally ompi_op_reduce(op, inbuf, rbuf,<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; count, dtype) this solves a problem. However this makes the<br>
&gt;&gt;&gt;&gt; code<br>&gt;&gt;&gt;&gt; &nbsp; &nbsp; MPI-implementation dependent. Any idea on more portable<br>&gt;&gt;&gt;&gt; options?<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; Thank you in advance for any comment.<br>
&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; --Oleg<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;<br>&gt;&gt; ------------------------------------------------------------------------<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt;&gt; users mailing list<br>&gt;&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt; _______________________________________________<br>&gt;&gt;&gt; users mailing list<br>&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;<br>&gt;&gt;<br>&gt;&gt; --<br>&gt;&gt; Dr. Aurélien Bouteiller<br>&gt;&gt; Sr. Research Associate - Innovative Computing Laboratory<br>&gt;&gt; Suite 350, 1122 Volunteer Boulevard<br>&gt;&gt; Knoxville, TN 37996<br>
&gt;&gt; 865 974 6321<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt;<br>&gt;&gt; _______________________________________________<br>&gt;&gt; users mailing list<br>&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><br>_______________________________________________<br>
users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div>
</div></blockquote></div><br>

