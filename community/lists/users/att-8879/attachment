<p>Hi,</p><p>The first &quot;crash&quot; is OK, since your rankfile has ranks 0 and 1 defined, while n=1, which means only rank 0 is present and can be allocated.<br></p><p>NP must be &gt;= the largest rank in rankfile. </p>
<p></p><p>What exactly are you trying to do ?</p><p>I tried to recreate your seqv but all I got was</p><p>~/work/svn/ompi/trunk/build_x86-64/install/bin/mpirun --hostfile hostfile.0 -rf rankfile.0 -n 1 hostname : -rf rankfile.1 -n 1 hostname<br>
[witch19:30798] mca: base: component_find: paffinity &quot;mca_paffinity_linux&quot; uses an MCA interface that is not recognized (component MCA v1.0.0 != supported MCA v2.0.0) -- ignored<br>--------------------------------------------------------------------------<br>
It looks like opal_init failed for some reason; your parallel process is<br>likely to abort.  There are many reasons that a parallel process can<br>fail during opal_init; some of which are due to configuration or<br>environment problems.  This failure appears to be an internal failure;<br>
here&#39;s some additional information (which may only be relevant to an<br>Open MPI developer):<br><br>  opal_carto_base_select failed<br>  --&gt; Returned value -13 instead of OPAL_SUCCESS<br>--------------------------------------------------------------------------<br>
[witch19:30798] [[INVALID],INVALID] ORTE_ERROR_LOG: Not found in file ../../orte/runtime/orte_init.c at line 78<br>[witch19:30798] [[INVALID],INVALID] ORTE_ERROR_LOG: Not found in file ../../orte/orted/orted_main.c at line 344<br>
--------------------------------------------------------------------------<br>A daemon (pid 11629) died unexpectedly with status 243 while attempting<br>to launch so we are aborting.<br><br>There may be more information reported by the environment (see above).<br>
<br>This may be because the daemon was unable to find all the needed shared<br>libraries on the remote node. You may set your LD_LIBRARY_PATH to have the<br>location of the shared libraries on the remote nodes and this will<br>
automatically be forwarded to the remote nodes.<br>--------------------------------------------------------------------------<br>--------------------------------------------------------------------------<br>mpirun noticed that the job aborted, but has no info as to the process<br>
that caused that situation.<br>--------------------------------------------------------------------------<br>mpirun: clean termination accomplished<br><br><br></p><p></p><p>Lenny.</p><p></p><p></p><br><div><span class="gmail_quote">On 4/10/09, <b class="gmail_sendername">Geoffroy Pignot</b> &lt;<a href="mailto:geopignot@gmail.com">geopignot@gmail.com</a>&gt; wrote:</span><blockquote class="gmail_quote" style="margin:0;margin-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex">
Hi ,<br><br>I am currently testing the process affinity capabilities of openmpi and I would like to know if the rankfile behaviour I will describe below is normal or not ?<br><br>cat hostfile.0<br>r011n002 slots=4<br>r011n003 slots=4<br>

<br>cat rankfile.0<br>rank 0=r011n002 slot=0<br>rank 1=r011n003 slot=1<br><br>##################################################################################<br><br>mpirun --hostfile hostfile.0 -rf rankfile.0 -n 2  hostname ### OK<br>

r011n002<br>r011n003 <br><br>##################################################################################<br>but <br>mpirun --hostfile hostfile.0 -rf rankfile.0 -n 1 hostname : -n 1 hostname ### CRASHED<br><i> --------------------------------------------------------------------------<br>

Error, invalid rank (1) in the rankfile (rankfile.0)<br>--------------------------------------------------------------------------<br>[r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in file rmaps_rank_file.c at line 404<br>

[r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in file base/rmaps_base_map_job.c at line 87<br>[r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in file base/plm_base_launch_support.c at line 77<br>

[r011n002:25129] [[63976,0],0] ORTE_ERROR_LOG: Bad parameter in file plm_rsh_module.c at line 985<br>--------------------------------------------------------------------------<br>A daemon (pid unknown) died unexpectedly on signal 1  while attempting to<br>

launch so we are aborting.<br><br>There may be more information reported by the environment (see above).<br><br>This may be because the daemon was unable to find all the needed shared<br>libraries on the remote node. You may set your LD_LIBRARY_PATH to have the<br>

location of the shared libraries on the remote nodes and this will<br>automatically be forwarded to the remote nodes.<br>--------------------------------------------------------------------------<br>--------------------------------------------------------------------------<br>

orterun noticed that the job aborted, but has no info as to the process<br>that caused that situation.<br>--------------------------------------------------------------------------<br>orterun: clean termination accomplished<br>

</i><br>It seems that the rankfile option is not propagted to the second command line ; there is no global understanding of the ranking inside a mpirun command.<br><br>
##################################################################################<br><br>Assuming that , I tried to provide a rankfile to each command line: <br> <br>cat rankfile.0<br>
rank 0=r011n002 slot=0<br><br>
cat rankfile.1<br>

rank 0=r011n003 slot=1<br><br>mpirun --hostfile hostfile.0 -rf rankfile.0 -n 1 hostname : -rf rankfile.1 -n 1 hostname ### CRASHED<br><i>[r011n002:28778] *** Process received signal ***<br>[r011n002:28778] Signal: Segmentation fault (11)<br>

[r011n002:28778] Signal code: Address not mapped (1)<br>[r011n002:28778] Failing at address: 0x34<br>[r011n002:28778] [ 0] [0xffffe600]<br>[r011n002:28778] [ 1] /tmp/HALMPI/openmpi-1.3.1/lib/libopen-rte.so.0(orte_odls_base_default_get_add_procs_data+0x55d) [0x5557decd]<br>

[r011n002:28778] [ 2] /tmp/HALMPI/openmpi-1.3.1/lib/libopen-rte.so.0(orte_plm_base_launch_apps+0x117) [0x555842a7]<br>[r011n002:28778] [ 3] /tmp/HALMPI/openmpi-1.3.1/lib/openmpi/mca_plm_rsh.so [0x556098c0]<br>[r011n002:28778] [ 4] /tmp/HALMPI/openmpi-1.3.1/bin/orterun [0x804aa27]<br>

[r011n002:28778] [ 5] /tmp/HALMPI/openmpi-1.3.1/bin/orterun [0x804a022]<br>[r011n002:28778] [ 6] /lib/libc.so.6(__libc_start_main+0xdc) [0x9f1dec]<br>[r011n002:28778] [ 7] /tmp/HALMPI/openmpi-1.3.1/bin/orterun [0x8049f71]<br>

[r011n002:28778] *** End of error message ***<br>Segmentation fault (core dumped)</i><br><br><br><br>I hope that I&#39;ve found a bug because it would be very important for me to have this kind of capabiliy .<br>Launch a multiexe mpirun command line and be able to bind my exes and sockets together.<br>

<br>Thanks in advance for your help<br><br>Geoffroy <br><br><br><br><br><br><br><br><br><br><br><div class="gmail_quote">2009/4/9  <span dir="ltr">&lt;<a href="mailto:users-request@open-mpi.org" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">users-request@open-mpi.org</a>&gt;</span><br>

<blockquote class="gmail_quote" style="border-left:1px solid rgb(204, 204, 204);margin:0pt 0pt 0pt 0.8ex;padding-left:1ex">Send users mailing list submissions to<br>
        <a href="mailto:users@open-mpi.org" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">users@open-mpi.org</a><br>
<br>
To subscribe or unsubscribe via the World Wide Web, visit<br>
        <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
or, via email, send a message with subject or body &#39;help&#39; to<br>
        <a href="mailto:users-request@open-mpi.org" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">users-request@open-mpi.org</a><br>
<br>
You can reach the person managing the list at<br>
        <a href="mailto:users-owner@open-mpi.org" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">users-owner@open-mpi.org</a><br>
<br>
When replying, please edit your Subject line so it is more specific<br>
than &quot;Re: Contents of users digest...&quot;<br>
<br>
<br>
Today&#39;s Topics:<br>
<br>
   1. mpirun self,sm (Robert Kubrick)<br>
   2. Re: mpirun self,sm (Ralph Castain)<br>
   3. shared libraries issue compiling 1.3.1/intel 10.1.022<br>
      (Francesco Pietra)<br>
<br>
<br>
----------------------------------------------------------------------<br>
<br>
Message: 1<br>
Date: Thu, 9 Apr 2009 00:15:03 -0400<br>
From: Robert Kubrick &lt;<a href="mailto:robertkubrick@gmail.com" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">robertkubrick@gmail.com</a>&gt;<br>
Subject: [OMPI users] mpirun self,sm<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">users@open-mpi.org</a>&gt;<br>
Message-ID: &lt;<a href="mailto:99AB3654-DD6A-4E96-94AC-B741073821ED@gmail.com" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">99AB3654-DD6A-4E96-94AC-B741073821ED@gmail.com</a>&gt;<br>
Content-Type: text/plain; charset=US-ASCII; delsp=yes; format=flowed<br>
<br>
How is this possible?<br>
<br>
dx:~&gt; mpirun -v -np 2 --mca btl self,sm --host dx,sx hostname<br>
dx<br>
sx<br>
<br>
dx:~&gt; netstat -i<br>
Kernel Interface table<br>
Iface   MTU Met   RX-OK RX-ERR RX-DRP RX-OVR   TX-OK TX-ERR TX-DRP TX-<br>
OVR Flg<br>
eth0   1500   0  998755      0      0      0 1070323      0<br>
0      0 BMRU<br>
eth1   1500   0   85352      0      0      0  379993      0<br>
0      0 BMRU<br>
ib0   65520   0     204      0      0      0     155      0<br>
5      0 BMRU<br>
lo    16436   0 1648874      0      0      0 1648874      0<br>
0      0 LRU<br>
<br>
I want to force an error with the first command above to make sure<br>
that my btl parameters are used correctly, but it looks like ompi<br>
runs hostname on the remote machine regardless.<br>
<br>
<br>
------------------------------<br>
<br>
Message: 2<br>
Date: Thu, 9 Apr 2009 02:16:08 -0600<br>
From: Ralph Castain &lt;<a href="mailto:rhc@lanl.gov" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">rhc@lanl.gov</a>&gt;<br>
Subject: Re: [OMPI users] mpirun self,sm<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">users@open-mpi.org</a>&gt;<br>
Message-ID: &lt;<a href="mailto:FF3FCDB6-3E23-41F6-88BC-7D4F327E40DC@lanl.gov" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">FF3FCDB6-3E23-41F6-88BC-7D4F327E40DC@lanl.gov</a>&gt;<br>
Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes<br>
<br>
hostname never calls MPI_Init, and therefore never initializes the BTL<br>
subsystem. Therefore, hostname will always run correctly.<br>
<br>
mpirun is not an MPI process, nor are the daemons used by OMPI to<br>
launch the MPI job. Thus, they also do not call MPI_Init, and<br>
therefore do not see the BTL subsystem.<br>
<br>
So this example will run just fine. You need to run an MPI application<br>
to cause it to fail.<br>
<br>
Ralph<br>
<br>
<br>
On Apr 8, 2009, at 10:15 PM, Robert Kubrick wrote:<br>
<br>
&gt; How is this possible?<br>
&gt;<br>
&gt; dx:~&gt; mpirun -v -np 2 --mca btl self,sm --host dx,sx hostname<br>
&gt; dx<br>
&gt; sx<br>
&gt;<br>
&gt; dx:~&gt; netstat -i<br>
&gt; Kernel Interface table<br>
&gt; Iface   MTU Met   RX-OK RX-ERR RX-DRP RX-OVR   TX-OK TX-ERR TX-DRP<br>
&gt; TX-OVR Flg<br>
&gt; eth0   1500   0  998755      0      0      0 1070323      0<br>
&gt; 0      0 BMRU<br>
&gt; eth1   1500   0   85352      0      0      0  379993      0<br>
&gt; 0      0 BMRU<br>
&gt; ib0   65520   0     204      0      0      0     155      0<br>
&gt; 5      0 BMRU<br>
&gt; lo    16436   0 1648874      0      0      0 1648874      0<br>
&gt; 0      0 LRU<br>
&gt;<br>
&gt; I want to force an error with the first command above to make sure<br>
&gt; that my btl parameters are used correctly, but it looks like ompi<br>
&gt; runs hostname on the remote machine regardless.<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
<br>
------------------------------<br>
<br>
Message: 3<br>
Date: Thu, 9 Apr 2009 17:31:16 +0200<br>
From: Francesco Pietra &lt;<a href="mailto:chiendarret@gmail.com" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">chiendarret@gmail.com</a>&gt;<br>
Subject: [OMPI users] shared libraries issue compiling 1.3.1/intel<br>
        10.1.022<br>
To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">users@open-mpi.org</a>&gt;<br>
Message-ID:<br>
        &lt;<a href="mailto:b87c422a0904090831q56a98e67w5000c90a94bf8a37@mail.gmail.com" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">b87c422a0904090831q56a98e67w5000c90a94bf8a37@mail.gmail.com</a>&gt;<br>

Content-Type: text/plain; charset=UTF-8<br>
<br>
Hi:<br>
As failure to find &quot;limits.h&quot; in my attempted compilations of Amber of<br>
th fast few days (amd64 lenny, openmpi 1.3.1, intel compilers<br>
10.1.015) is probably (or I hope so) a bug of the version used of<br>
intel compilers (I made with debian the same observations reported<br>
for gentoo, <a href="http://software.intel.com/en-us/forums/intel-c-compiler/topic/59886/" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">http://software.intel.com/en-us/forums/intel-c-compiler/topic/59886/</a>).<br>

<br>
I made a deb package of 10.1.022, icc and ifort.<br>
<br>
./configure CC icc, CXX icp, F77 and FC ifort --with-libnuma=/usr (not<br>
/usr/lib so that the numa.h issue is not raised), &quot;make clean&quot;, and<br>
&quot;mak install&quot; gave no error signals. However, the compilation tests in<br>
the examples did not pass and I really don&#39;t understand why.<br>
<br>
The error, with both connectivity_c and hello_c (I was operating on<br>
the parallel computer deb64 directly and have access to everything<br>
there) was failure to find a shared library, libimf.so<br>
<br>
# dpkg --search libimf.so<br>
   /opt/intel/fce/10.1.022/lib/libimf.so  (the same for cce)<br>
<br>
which path seems to be correctly sourced by iccvars.sh and<br>
ifortvars.sh (incidentally, both files are -rw-r--r-- root root;<br>
correct that non executable?)<br>
<br>
echo $LD_LIBRARY_PATH<br>
returned, inter alia,<br>
/opt/intel/mkl/<a href="http://10.1.2.024/lib/em64t:/opt/intel/mkl/10.1.2.024/lib/em64t:/opt/intel/cce/10.1.022/lib:/opt/intel/fce/10.1.022/lib" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">10.1.2.024/lib/em64t:/opt/intel/mkl/10.1.2.024/lib/em64t:/opt/intel/cce/10.1.022/lib:/opt/intel/fce/10.1.022/lib</a><br>


(why twice the mkl?)<br>
<br>
I surely miss to understand something fundamental. Hope other eyes see better<br>
<br>
A kind person elsewhere suggested me on passing &quot;The use of -rpath<br>
during linking is highly recommended as opposed to setting<br>
LD_LIBRARY_PATH at run time, not the least because it hardcodes the<br>
paths to the &quot;right&quot; library files in the executables themselves&quot;<br>
Should this be relevant to the present issue, where to learn about<br>
-rpath linking?<br>
<br>
thanks<br>
francesco pietra<br>
<br>
<br>
------------------------------<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
End of users Digest, Vol 1197, Issue 1<br>
**************************************<br>
</blockquote></div><br>
<br>_______________________________________________<br>
users mailing list<br>
<a onclick="return top.js.OpenExtLink(window,event,this)" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a onclick="return top.js.OpenExtLink(window,event,this)" href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></div><br>

