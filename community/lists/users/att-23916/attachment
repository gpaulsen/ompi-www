<div dir="ltr"><div class="gmail_default" style="font-family:tahoma,sans-serif"><br></div><br><div class="gmail_quote">---------- Forwarded message ----------<br>From: <b class="gmail_sendername">Jeff Squyres (jsquyres)</b> <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span><br>
Date: Fri, Mar 21, 2014 at 3:05 PM<br>Subject: Re: problem for multiple clusters using mpirun<br>To: Hamid Saeed &lt;<a href="mailto:e.hamidsaeed@gmail.com">e.hamidsaeed@gmail.com</a>&gt;<br><br><br>Please reply on the mailing list; more people can reply that way, and the answers to your questions become google-able for people with similar questions.<br>

<div class="HOEnZb"><div class="h5"><br>
<br>
On Mar 21, 2014, at 10:03 AM, Hamid Saeed &lt;<a href="mailto:e.hamidsaeed@gmail.com">e.hamidsaeed@gmail.com</a>&gt; wrote:<br>
<br>
&gt; Hello Jeff,<br>
&gt;<br>
&gt; Sorry to bother you again.<br>
&gt;<br>
&gt; I think i have a tcp connection. As for as i know my cluster is not configured for Infiniband (IB).<br>
&gt;<br>
&gt; but even for tcp connections.<br>
&gt;<br>
&gt; mpirun -n 2 -host master,node001 --mca btl tcp,sm,self ./helloworldmpi<br>
&gt; mpirun -n 2 -host master,node001 ./helloworldmpi<br>
&gt;<br>
&gt; These line are not working they output<br>
&gt; Error like<br>
&gt; [btl_tcp_endpoint.c:655:mca_btl_tcp_endpoint_complete_connect] connect() to xx.xxx.x.xxx failed: Connection refused (111)<br>
&gt;<br>
&gt;<br>
&gt; at the program hangs up until i press<br>
&gt; ctrl + c.<br>
&gt; n Fri, Mar 21, 2014 at 2:47 PM, Hamid Saeed &lt;<a href="mailto:e.hamidsaeed@gmail.com">e.hamidsaeed@gmail.com</a>&gt; wrote:<br>
&gt;<br>
&gt; Hello,<br>
&gt;<br>
&gt; Thanks for the answer.<br>
&gt;<br>
&gt; Can you kindly explain what does IB connection means?<br>
&gt;<br>
&gt; thanks<br>
&gt;<br>
&gt; regards<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; On Fri, Mar 21, 2014 at 2:44 PM, Jeff Squyres (jsquyres) &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br>
&gt; Was Ralph&#39;s answer not enough? &nbsp;I think he hit the nail on the head...<br>
&gt;<br>
&gt;<br>
&gt; On Mar 21, 2014, at 9:29 AM, Hamid Saeed &lt;<a href="mailto:e.hamidsaeed@gmail.com">e.hamidsaeed@gmail.com</a>&gt; wrote:<br>
&gt;<br>
&gt; &gt; Hello:<br>
&gt; &gt;<br>
&gt; &gt; I have learnt about mpi from you using different web portals.<br>
&gt; &gt; I hope you can help me in solving this problem too.<br>
&gt; &gt;<br>
&gt; &gt; &nbsp; &nbsp; &nbsp; &bull; I can compile my helloworld.c program using mpicc and I have confirmed that the script runs correctly on another working cluster, so the local paths are set up correctly I think and the script definitely works.<br>

&gt; &gt;<br>
&gt; &gt; &nbsp; &nbsp; &nbsp; &bull; If I execute mpirun from my master node, and using only the master node, helloworld executes correctly:<br>
&gt; &gt;<br>
&gt; &gt; mpirun -n 1 -host master --mca btl sm,openib,self ./helloworldmpi<br>
&gt; &gt; hello world from process 0 of 1<br>
&gt; &gt;<br>
&gt; &gt; &nbsp; &nbsp; &nbsp; &bull; If I execute mpirun from my master node, using only the worker node, helloworld executes correctly:<br>
&gt; &gt;<br>
&gt; &gt; mpirun -n 1 -host node001 --mca btl sm,openib,self./helloworldmpi<br>
&gt; &gt; hello world from process 0 of 1<br>
&gt; &gt;<br>
&gt; &gt; Now, my problem is that if I try to run helloworld on both nodes, I get an error:<br>
&gt; &gt;<br>
&gt; &gt; mpirun -n 2 -host master,node001 --mca btl openib,self ./helloworldmpi<br>
&gt; &gt; --------------------------------------------------------------------------<br>
&gt; &gt; At least one pair of MPI processes are unable to reach each other for<br>
&gt; &gt; MPI communications. &nbsp;This means that no Open MPI device has indicated<br>
&gt; &gt; that it can be used to communicate between these processes. &nbsp;This is<br>
&gt; &gt; an error; Open MPI requires that all MPI processes be able to reach<br>
&gt; &gt; each other. &nbsp;This error can sometimes be the result of forgetting to<br>
&gt; &gt; specify the &quot;self&quot; BTL.<br>
&gt; &gt;<br>
&gt; &gt; &nbsp; Process 1 ([[5228,1],0]) is on host: hsaeed<br>
&gt; &gt; &nbsp; Process 2 ([[5228,1],1]) is on host: node001<br>
&gt; &gt; &nbsp; BTLs attempted: self<br>
&gt; &gt;<br>
&gt; &gt; Your MPI job is now going to abort; sorry.<br>
&gt; &gt; --------------------------------------------------------------------------<br>
&gt; &gt; --------------------------------------------------------------------------<br>
&gt; &gt; It looks like MPI_INIT failed for some reason; your parallel process is<br>
&gt; &gt; likely to abort. &nbsp;There are many reasons that a parallel process can<br>
&gt; &gt; fail during MPI_INIT; some of which are due to configuration or environment<br>
&gt; &gt; problems. &nbsp;This failure appears to be an internal failure; here&#39;s some<br>
&gt; &gt; additional information (which may only be relevant to an Open MPI<br>
&gt; &gt; developer):<br>
&gt; &gt;<br>
&gt; &gt; &nbsp; PML add procs failed<br>
&gt; &gt; &nbsp; --&gt; Returned &quot;Unreachable&quot; (-12) instead of &quot;Success&quot; (0)<br>
&gt; &gt; --------------------------------------------------------------------------<br>
&gt; &gt; *** The MPI_Init() function was called before MPI_INIT was invoked.<br>
&gt; &gt; *** This is disallowed by the MPI standard.<br>
&gt; &gt; *** Your MPI job will now abort.<br>
&gt; &gt; Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>
&gt; &gt; --------------------------------------------------------------------------<br>
&gt; &gt; mpirun has exited due to process rank 0 with PID 7037 on<br>
&gt; &gt; node xxxx exiting without calling &quot;finalize&quot;. This may<br>
&gt; &gt; have caused other processes in the application to be<br>
&gt; &gt; terminated by signals sent by mpirun (as reported here).<br>
&gt; &gt; --------------------------------------------------------------------------<br>
&gt; &gt; *** The MPI_Init() function was called before MPI_INIT was invoked.<br>
&gt; &gt; *** This is disallowed by the MPI standard.<br>
&gt; &gt; *** Your MPI job will now abort.<br>
&gt; &gt; Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>
&gt; &gt; 1 more process has sent help message help-mca-bml-r2.txt / unreachable proc<br>
&gt; &gt; Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>
&gt; &gt; 1 more process has sent help message help-mpi-runtime<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; i tried using<br>
&gt; &gt; mpirun -n 2 -host master,node001 --mca btl tcp,sm,self ./helloworldmpi<br>
&gt; &gt; mpirun -n 2 -host master,node001 --mca btl o<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; penib,tcp,<br>
&gt; &gt; self ./helloworldmpi<br>
&gt; &gt; etc..<br>
&gt; &gt;<br>
&gt; &gt; But no flag is works.<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; Can some one reply with the idea.<br>
&gt; &gt;<br>
&gt; &gt; Thanks in Advance.<br>
&gt; &gt;<br>
&gt; &gt; Regards--<br>
&gt; &gt; --<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; Hamid Saeed<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt;<br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Jeff Squyres<br>
&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt;<br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
</div></div></div><br><br clear="all"><div><br></div>-- <br><div dir="ltr"><div dir="ltr"><p style="color:rgb(34,34,34);margin:0cm 0cm 0.0001pt;font-size:11pt;font-family:Calibri,sans-serif"><span lang="EN-US">_______________________________________________<u></u><u></u></span></p>
<p style="color:rgb(34,34,34);margin:0cm 0cm 0.0001pt"><span lang="EN-US"><font face="tahoma, sans-serif">Hamid Saeed</font></span></p><div><font face="tahoma, sans-serif"><font color="#000000">CoSynth GmbH &amp; Co. KG<br>
Escherweg 2 - 26121 Oldenburg - Germany</font><br></font></div><p style="margin:0cm 0cm 0.0001pt"><font face="tahoma, sans-serif">Tel&nbsp;<a value="+494419722288" style="color:rgb(17,85,204)">+49 441 9722 738</a>&nbsp;| Fax -278<br>
<a href="http://www.cosynth.com/" style="color:rgb(17,85,204)" target="_blank">http://www.cosynth.com</a></font><br></p><p style="margin:0cm 0cm 0.0001pt;font-size:11pt;font-family:Calibri,sans-serif"><span style="font-size:11pt">______________________________</span><span style="font-size:11pt">_________________</span></p>
</div></div>
</div>

