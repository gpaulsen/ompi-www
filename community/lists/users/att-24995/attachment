<div dir="ltr">You can improve performance by using --bind-to socket or --bind-to numa as this will keep the process inside the same memory region. You can also help separate the jobs by using the --cpuset to tell each job which cpus it should use - we&#39;ll stay within that envelope.<div>
<br></div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Tue, Aug 12, 2014 at 8:33 AM, Reuti <span dir="ltr">&lt;<a href="mailto:reuti@staff.uni-marburg.de" target="_blank">reuti@staff.uni-marburg.de</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Am 12.08.2014 um 16:57 schrieb Antonio Rago:<br>
<div class=""><br>
&gt; Brilliant, this works!<br>
&gt; However I’ve to say that it seems that it seems that code becomes slightly less performing.<br>
&gt; Is there a way to instruct mpirun on which core to use, and maybe create this map automatically with grid engine?<br>
<br>
</div>In the open source version of SGE the requested core binding is only a soft request. The Univa version can handle this as a hard request though, as the scheduler will do the assignment and knows which cores are used. I have no information whether this will be forwarded to Open MPI automatically. I assume not, and it must be read out of the machine file (there ought to be an extra column for it in their version) and feed to Open MPI by some measures.<br>

<br>
-- Reuti<br>
<div><div class="h5"><br>
<br>
&gt; Thanks in advance<br>
&gt; Antonio<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; On 12 Aug 2014, at 14:10, Jeff Squyres (jsquyres) &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br>
&gt;<br>
&gt;&gt; The quick and dirty answer is that in the v1.8 series, Open MPI started binding MPI processes to cores by default.<br>
&gt;&gt;<br>
&gt;&gt; When you run 2 independent jobs on the same machine in the way in which you described, the two jobs won&#39;t have knowledge of each other, and therefore they will both starting binging MPI processes starting with logical core 0.<br>

&gt;&gt;<br>
&gt;&gt; The easy workaround is to disable bind-to-core behavior.  For example, &quot;mpirun --bind-to none ...&quot;.  In this way, the OS will (more or less) load balance your MPI jobs to available cores (assuming you don&#39;t run more MPI processes than cores).<br>

&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; On Aug 12, 2014, at 7:05 AM, Antonio Rago &lt;<a href="mailto:antonio.rago@plymouth.ac.uk">antonio.rago@plymouth.ac.uk</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt;&gt; Dear mailing list<br>
&gt;&gt;&gt; I’m running into trouble in the configuration of the small cluster I’m managing.<br>
&gt;&gt;&gt; I’ve installed openmpi-1.8.1 with gcc 4.7 on a Centos 6.5 with infiniband support.<br>
&gt;&gt;&gt; Compile and installation were all ok and i can compile and actually run parallel jobs, both directly or by submitting them with the queue manager (gridengine).<br>
&gt;&gt;&gt; My problem is that when two different subsets of two job end on the same node, they will not spread equally and use all the cores of the node but instead they will run on a common subset of cores leaving some other totally empty.<br>

&gt;&gt;&gt; For example two 4 core jobs on a 8 core node will result in only 4 core running on the node (all of them being oversubscribed) and the other 4 cores being empty.<br>
&gt;&gt;&gt; Clearly there must be an error in the way I’ve configured stuffs but i cannot find any hint on how to solve the problem.<br>
&gt;&gt;&gt; I’ve tried to do different map (map by core or by slot) but I’ve never succeeded.<br>
&gt;&gt;&gt; Could you give a me suggestion on this issue?<br>
&gt;&gt;&gt; Regards<br>
&gt;&gt;&gt; Antonio<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; ________________________________<br>
&gt;&gt;&gt; [<a href="http://www.plymouth.ac.uk/images/email_footer.gif" target="_blank">http://www.plymouth.ac.uk/images/email_footer.gif</a>]&lt;<a href="http://www.plymouth.ac.uk/worldclass" target="_blank">http://www.plymouth.ac.uk/worldclass</a>&gt;<br>

&gt;&gt;&gt;<br>
&gt;&gt;&gt; This email and any files with it are confidential and intended solely for the use of the recipient to whom it is addressed. If you are not the intended recipient then copying, distribution or other use of the information contained is strictly prohibited and you should not rely on it. If you have received this email in error please let the sender know immediately and delete it from your system(s). Internet emails are not necessarily secure. While we take every care, Plymouth University accepts no responsibility for viruses and it is your responsibility to scan emails and their attachments. Plymouth University does not accept responsibility for any changes made after it was sent. Nothing in this email or its attachments constitutes an order for goods or services unless accompanied by an official order form.<br>

&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; users mailing list<br>
&gt;&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/24986.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/24986.php</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; Jeff Squyres<br>
&gt;&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt;&gt; For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; users mailing list<br>
&gt;&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/24991.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/24991.php</a><br>
&gt;<br>
&gt; ________________________________<br>
&gt; [<a href="http://www.plymouth.ac.uk/images/email_footer.gif" target="_blank">http://www.plymouth.ac.uk/images/email_footer.gif</a>]&lt;<a href="http://www.plymouth.ac.uk/worldclass" target="_blank">http://www.plymouth.ac.uk/worldclass</a>&gt;<br>

&gt;<br>
&gt; This email and any files with it are confidential and intended solely for the use of the recipient to whom it is addressed. If you are not the intended recipient then copying, distribution or other use of the information contained is strictly prohibited and you should not rely on it. If you have received this email in error please let the sender know immediately and delete it from your system(s). Internet emails are not necessarily secure. While we take every care, Plymouth University accepts no responsibility for viruses and it is your responsibility to scan emails and their attachments. Plymouth University does not accept responsibility for any changes made after it was sent. Nothing in this email or its attachments constitutes an order for goods or services unless accompanied by an official order form.<br>

&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/24992.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/24992.php</a><br>
&gt;<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2014/08/24994.php" target="_blank">http://www.open-mpi.org/community/lists/users/2014/08/24994.php</a><br>
</blockquote></div><br></div>

