<html><body><div style="color:#000; background-color:#fff; font-family:Courier New, courier, monaco, monospace, sans-serif;font-size:10pt"><div><span>Hello Ole</span></div><div><br><span></span></div><div><span>I ran your program on open-mpi-1.4.2&nbsp; five times, and all five times, it finished successfully.</span></div><div><br><span></span></div><div><span>So, I think the problem was with the version of mpi.</span></div><div><br><span></span></div><div><span>Output from your program is attached. I ran on 3 nodes:</span></div><div><br><span></span></div><div><span>$home/OpenMPI-1.4.2/bin/mpirun -np 3 -v --output-filename mpi_testfile ./mpi_test</span></div><div><br><span></span></div><div><span>So, maybe this helps you.</span></div><div><br><span></span></div><div><span>Best,</span></div><div><br><span></span></div><div><span>Devendra Rai<br></span></div><div><br></div><div style="font-family: Courier New, courier, monaco, monospace, sans-serif;
 font-size: 10pt;"><div style="font-family: times new roman, new york, times, serif; font-size: 12pt;"><font face="Arial" size="2"><hr size="1"><b><span style="font-weight:bold;">From:</span></b> Ole Nielsen &lt;ole.moller.nielsen@gmail.com&gt;<br><b><span style="font-weight: bold;">To:</span></b> users@open-mpi.org<br><b><span style="font-weight: bold;">Sent:</span></b> Monday, 19 September 2011, 10:59<br><b><span style="font-weight: bold;">Subject:</span></b> [OMPI users] MPI hangs on multiple nodes<br></font><br><div id="yiv1880761681">The test program is available here:<br><a rel="nofollow" target="_blank" href="http://code.google.com/p/pypar/source/browse/source/mpi_test.c">http://code.google.com/p/pypar/source/browse/source/mpi_test.c</a><br><br>Hopefully, someone can help us troubleshoot why communications stop when multiple nodes are involved and CPU usage goes to 100% for as long as we leave the program running.<br>
<br>Many thanks<br>Ole Nielsen<br><br><br><div class="yiv1880761681gmail_quote">---------- Forwarded message ----------<br>From: <b class="yiv1880761681gmail_sendername">Ole Nielsen</b> <span dir="ltr">&lt;<a rel="nofollow" ymailto="mailto:ole.moller.nielsen@gmail.com" target="_blank" href="mailto:ole.moller.nielsen@gmail.com">ole.moller.nielsen@gmail.com</a>&gt;</span><br>
Date: Mon, Sep 19, 2011 at 3:39 PM<br>Subject: Re: MPI hangs on multiple nodes<br>To: <a rel="nofollow" ymailto="mailto:users@open-mpi.org" target="_blank" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><br><br>Further to the posting below, I can report that the test program (attached - this time correctly) is chewing up CPU time on both compute nodes for as long as I care to let it continue.<br>
It would appear that MPI_Receive which is the next command after the print statements in the test program.<br>
<br>Has anyone else seen this behavior or can anyone give me a hint on how to troubleshoot.<div class="yiv1880761681im"><br>Cheers and thanks<br>Ole Nielsen<br><br></div>Output:<div class="yiv1880761681im"><br>nielso@alamba:~/sandpit/pypar/source$ mpirun --hostfile /etc/mpihosts --host node17,node18 --npernode 2 a.out <br>

Number of processes = 4<br>Test repeated 3 times for reliability<br></div><div class="yiv1880761681im">I am process 2 on node node18<br>P2: Waiting to receive from to P1<br></div><div class="yiv1880761681im">I am process 0 on node node17<br>Run 1 of 3<br>
P0: Sending to P1<br>I am process 1 on node node17<br>
P1: Waiting to receive from to P0<br></div><div class="yiv1880761681im">I am process 3 on node node18<br>P3: Waiting to receive from to P2<br></div>P0: Waiting to receive from P3<div class="yiv1880761681im"><br>P1: Sending to to P2<br></div><div class="yiv1880761681im">
P1: Waiting to receive from to P0<br></div>P2: Sending to to P3<div class="yiv1880761681im"><br>
P0: Received from to P3<br>Run 2 of 3<br>P0: Sending to P1<br></div>P3: Sending to to P0<div class="yiv1880761681im"><br>P3: Waiting to receive from to P2<br></div><div class="yiv1880761681im">P2: Waiting to receive from to P1<br></div><div class="yiv1880761681im">
P1: Sending to to P2<br></div>P0: Waiting to receive from P3<div><div></div><div class="yiv1880761681h5"><br><br>
<br><br><br><br><br>&nbsp;<br><br><div class="yiv1880761681gmail_quote">On Mon, Sep 19, 2011 at 11:04 AM, Ole Nielsen <span dir="ltr">&lt;<a rel="nofollow" ymailto="mailto:ole.moller.nielsen@gmail.com" target="_blank" href="mailto:ole.moller.nielsen@gmail.com">ole.moller.nielsen@gmail.com</a>&gt;</span> wrote:<br>
<blockquote class="yiv1880761681gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204, 204, 204);padding-left:1ex;">
<div><div></div><div><br><div class="yiv1880761681gmail_quote">Hi all<br><br>We have been using OpenMPI for many years with Ubuntu on our 20-node cluster. Each node has 2 quad cores, so we usually run up to 8 processes on each node up to a maximum of 160 processes.<br>



<br>However, we just upgraded the cluster to Ubuntu 11.04 with Open MPI 1.4.3 and and have come across a strange behavior where mpi programs run perfectly well when confined to one node but hangs during communication across multiple nodes. We have no idea why and would like some help in debugging this. A small MPI test program is attached and typical output shown below.<br>



<br>Hope someone can help us<br>Cheers and thanks<br>Ole Nielsen<br><br>-------------------- Test output across two nodes (This one hangs) --------------------------<br>
nielso@alamba:~/sandpit/pypar/source$ mpirun --hostfile /etc/mpihosts --host node17,node18 --npernode 2 a.out <br>
Number of processes = 4<br>
Test repeated 3 times for reliability<br>
I am process 1 on node node17<br>
P1: Waiting to receive from to P0<br>
I am process 0 on node node17<br>
Run 1 of 3<br>
P0: Sending to P1<br>
I am process 2 on node node18<br>
P2: Waiting to receive from to P1<br>
I am process 3 on node node18<br>
P3: Waiting to receive from to P2<br>
P1: Sending to to P2<br>
<br><br>-------------------- Test output within one node (This one is OK) --------------------------<br>nielso@alamba:~/sandpit/pypar/source$ mpirun --hostfile /etc/mpihosts --host node17 --npernode 4 a.out <br>Number of processes = 4<br>



Test repeated 3 times for reliability<br>I am process 2 on node node17<br>P2: Waiting to receive from to P1<br>I am process 0 on node node17<br>Run 1 of 3<br>P0: Sending to P1<br>I am process 1 on node node17<br>P1: Waiting to receive from to P0<br>



I am process 3 on node node17<br>P3: Waiting to receive from to P2<br>P1: Sending to to P2<br>P2: Sending to to P3<br>P1: Waiting to receive from to P0<br>P2: Waiting to receive from to P1<br>P3: Sending to to P0<br>P0: Received from to P3<br>



Run 2 of 3<br>P0: Sending to P1<br>P3: Waiting to receive from to P2<br>P1: Sending to to P2<br>P2: Sending to to P3<br>P1: Waiting to receive from to P0<br>P3: Sending to to P0<br>P2: Waiting to receive from to P1<br>P0: Received from to P3<br>



Run 3 of 3<br>P0: Sending to P1<br>P3: Waiting to receive from to P2<br>P1: Sending to to P2<br>P2: Sending to to P3<br>P1: Done<br>P2: Done<br>P3: Sending to to P0<br>P0: Received from to P3<br>P0: Done<br>P3: Done<br><br>



<br>
</div><br>
</div></div></blockquote></div><br>
</div></div></div><br>
</div><br>_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br></div></div></div></body></html>
