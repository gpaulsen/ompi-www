<div dir="ltr">btw, ompi master now calls ibv_fork_init() before initializing btl/mtl/oob frameworks and all fork fears should be addressed.<div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Fri, Apr 24, 2015 at 4:37 AM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Disable the memory manager / don&#39;t use leave pinned.  Then you can fork/exec without fear (because only MPI will have registered memory -- it&#39;ll never leave user buffers registered after MPI communications finish).<br>
<div><div class="h5"><br>
<br>
&gt; On Apr 23, 2015, at 9:25 PM, Howard Pritchard &lt;<a href="mailto:hppritcha@gmail.com">hppritcha@gmail.com</a>&gt; wrote:<br>
&gt;<br>
&gt; Jeff<br>
&gt;<br>
&gt; this is kind of a lanl thing. Jack and I are working offline.  any suggestions about openib and fork/exec may be useful however...and don&#39;t say no to fork/exec not at least if you dream of mpi in the data center.<br>
&gt;<br>
&gt; On Apr 23, 2015 10:49 AM, &quot;Galloway, Jack D&quot; &lt;<a href="mailto:jackg@lanl.gov">jackg@lanl.gov</a>&gt; wrote:<br>
&gt; I am using a “homecooked” cluster at LANL, ~500 cores.  There are a whole bunch of fortran system calls doing the copying and pasting.  The full code is attached here, a bunch of if-then statements for user options.  Thanks for the help.<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; --Jack Galloway<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; From: users [mailto:<a href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>] On Behalf Of Howard Pritchard<br>
&gt; Sent: Thursday, April 23, 2015 8:15 AM<br>
&gt; To: Open MPI Users<br>
&gt; Subject: Re: [OMPI users] MPI_Finalize not behaving correctly, orphaned processes<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; Hi Jack,<br>
&gt;<br>
&gt; Are you using a system at LANL? Maybe I could try to reproduce the problem on the system you are using.  The system call stuff adds a certain bit of zest to the problem.  does the app make fortran system calls to do the copying and pasting?<br>
&gt;<br>
&gt; Howard<br>
&gt;<br>
&gt; On Apr 22, 2015 4:24 PM, &quot;Galloway, Jack D&quot; &lt;<a href="mailto:jackg@lanl.gov">jackg@lanl.gov</a>&gt; wrote:<br>
&gt;<br>
&gt; I have an MPI program that is fairly straight forward, essentially &quot;initialize, 2 sends from master to slaves, 2 receives on slaves, do a bunch of system calls for copying/pasting then running a serial code on each mpi task, tidy up and mpi finalize&quot;.<br>
&gt;<br>
&gt; This seems straightforward, but I&#39;m not getting mpi_finalize to work correctly. Below is a snapshot of the program, without all the system copy/paste/call external code which I&#39;ve rolled up in &quot;do codish stuff&quot; type statements.<br>
&gt;<br>
&gt; program mpi_finalize_break<br>
&gt;<br>
&gt; !&lt;variable declarations&gt;<br>
&gt;<br>
&gt; call MPI_INIT(ierr)<br>
&gt;<br>
&gt; icomm = MPI_COMM_WORLD<br>
&gt;<br>
&gt; call MPI_COMM_SIZE(icomm,nproc,ierr)<br>
&gt;<br>
&gt; call MPI_COMM_RANK(icomm,rank,ierr)<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; !&lt;do codish stuff for a while&gt;<br>
&gt;<br>
&gt; if (rank == 0) then<br>
&gt;<br>
&gt;     !&lt;set up some stuff then call MPI_SEND in a loop over number of slaves&gt;<br>
&gt;<br>
&gt;     call MPI_SEND(numat,1,MPI_INTEGER,n,0,icomm,ierr)<br>
&gt;<br>
&gt;     call MPI_SEND(n_to_add,1,MPI_INTEGER,n,0,icomm,ierr)<br>
&gt;<br>
&gt; else<br>
&gt;<br>
&gt;     call MPI_Recv(begin_mat,1,MPI_INTEGER,0,0,icomm,status,ierr)<br>
&gt;<br>
&gt;     call MPI_Recv(nrepeat,1,MPI_INTEGER,0,0,icomm,status,ierr)<br>
&gt;<br>
&gt;     !&lt;do codish stuff for a while&gt;<br>
&gt;<br>
&gt; endif<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; print*, &quot;got here4&quot;, rank<br>
&gt;<br>
&gt; call MPI_BARRIER(icomm,ierr)<br>
&gt;<br>
&gt; print*, &quot;got here5&quot;, rank, ierr<br>
&gt;<br>
&gt; call MPI_FINALIZE(ierr)<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; print*, &quot;got here6&quot;<br>
&gt;<br>
&gt; end program mpi_finalize_break<br>
&gt;<br>
&gt; Now the problem I am seeing occurs around the &quot;got here4&quot;, &quot;got here5&quot; and &quot;got here6&quot; statements. I get the appropriate number of print statements with corresponding ranks for &quot;got here4&quot;, as well as &quot;got here5&quot;. Meaning, the master and all the slaves (rank 0, and all other ranks) got to the barrier call, through the barrier call, and to MPI_FINALIZE, reporting 0 for ierr on all of them. However, when it gets to &quot;got here6&quot;, after the MPI_FINALIZE I&#39;ll get all kinds of weird behavior. Sometimes I&#39;ll get one less &quot;got here6&quot; than I expect, or sometimes I&#39;ll get eight less (it varies), however the program hangs forever, never closing and leaves an orphaned process on one (or more) of the compute nodes.<br>
&gt;<br>
&gt; I am running this on an infiniband backbone machine, with the NFS server shared over infiniband (nfs-rdma). I&#39;m trying to determine how the MPI_BARRIER call works fine, yet MPI_FINALIZE ends up with random orphaned runs (not the same node, nor the same number of orphans every time). I&#39;m guessing it is related to the various system calls to cp, mv, ./run_some_code, cp, mv but wasn&#39;t sure if it may be related to the speed of infiniband too, as all this happens fairly quickly. I could have wrong intuition as well. Anybody have thoughts? I could put the whole code if helpful, but this condensed version I believe captures it. I&#39;m running openmpi1.8.4 compiled against ifort 15.0.2 , with Mellanox adapters running firmware 2.9.1000.  This is the mellanox firmware available through yum with centos 6.5, 2.6.32-504.8.1.el6.x86_64.<br>
&gt;<br>
&gt; ib0       Link encap:InfiniBand  HWaddr 80:00:00:48:FE:80:00:00:00:00:00:00:00:00:00:00:00:00:00:00<br>
&gt;<br>
&gt;           inet addr:192.168.6.254  Bcast:192.168.6.255  Mask:255.255.255.0<br>
&gt;<br>
&gt;           inet6 addr: fe80::202:c903:57:e7fd/64 Scope:Link<br>
&gt;<br>
&gt;           UP BROADCAST RUNNING MULTICAST  MTU:2044  Metric:1<br>
&gt;<br>
&gt;           RX packets:10952 errors:0 dropped:0 overruns:0 frame:0<br>
&gt;<br>
&gt;           TX packets:9805 errors:0 dropped:625413 overruns:0 carrier:0<br>
&gt;<br>
&gt;           collisions:0 txqueuelen:256<br>
&gt;<br>
&gt;           RX bytes:830040 (810.5 KiB)  TX bytes:643212 (628.1 KiB)<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; hca_id: mlx4_0<br>
&gt;<br>
&gt;         transport:                      InfiniBand (0)<br>
&gt;<br>
&gt;         fw_ver:                         2.9.1000<br>
&gt;<br>
&gt;         node_guid:                      0002:c903:0057:e7fc<br>
&gt;<br>
&gt;         sys_image_guid:                 0002:c903:0057:e7ff<br>
&gt;<br>
&gt;         vendor_id:                      0x02c9<br>
&gt;<br>
&gt;         vendor_part_id:                 26428<br>
&gt;<br>
&gt;         hw_ver:                         0xB0<br>
&gt;<br>
&gt;         board_id:                       MT_0D90110009<br>
&gt;<br>
&gt;         phys_port_cnt:                  1<br>
&gt;<br>
&gt;                 port:   1<br>
&gt;<br>
&gt;                         state:                  PORT_ACTIVE (4)<br>
&gt;<br>
&gt;                         max_mtu:                4096 (5)<br>
&gt;<br>
&gt;                         active_mtu:             4096 (5)<br>
&gt;<br>
&gt;                         sm_lid:                 1<br>
&gt;<br>
&gt;                         port_lid:               2<br>
&gt;<br>
&gt;                         port_lmc:               0x00<br>
&gt;<br>
&gt;                         link_layer:             InfiniBand<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; This problem only occurs in this simple implementation, thus my thinking it is tied to the system calls.  I run several other, much larger, much more robust MPI codes without issue on the machine.  Thanks for the help.<br>
&gt;<br>
&gt; --Jack<br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/04/26765.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/04/26765.php</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/04/26772.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/04/26772.php</a><br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div>&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/04/26775.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/04/26775.php</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<span class=""><br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</span>Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2015/04/26776.php" target="_blank">http://www.open-mpi.org/community/lists/users/2015/04/26776.php</a></blockquote></div><br><br clear="all"><div><br></div>-- <br><div class="gmail_signature"><div dir="ltr"><br><div>Kind Regards,</div><div><br></div><div>M.</div></div></div>
</div>

