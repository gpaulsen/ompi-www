Hi,<br><div class="gmail_quote"><br>Currently I&#39;m having trouble to scale an MPI job beyond a certain limit.<br>So I&#39;m running an MPI hello example to test beyond 1024 but it failed with the following error with 2048 processes.<br>
It worked fine with 1024 processes.  I have enough file descriptor limit (65536) defined for each process.<br><br>I appreciate if anyone gives me any suggestions. <br>
I&#39;m running (Open MPI) 1.4.3<br><br>[x-01-06-a:25989] [[37568,0],69] ORTE_ERROR_LOG: Data unpack had inadequate space in file base/odls_base_default_fns.c at line 335<br>[x-01-06-b:09532] [[37568,0],74] ORTE_ERROR_LOG: Data unpack had inadequate space in file base/odls_base_default_fns.c at line 335<br>

--------------------------------------------------------------------------<br>mpirun noticed that the job aborted, but has no info as to the process<br>that caused that situation.<br>--------------------------------------------------------------------------<br>

[x-03-20-b:23316] *** Process received signal ***<br>[x-03-20-b:23316] Signal: Segmentation fault (11)<br>[x-03-20-b:23316] Signal code: Address not mapped (1)<br>[x-03-20-b:23316] Failing at address: 0x6c<br>[x-03-20-b:23316] [ 0] /lib64/libpthread.so.0 [0x310860ee90]<br>

[x-03-20-b:23316] [ 1] /usr/local/MPI/openmpi-1.4.3/lib/libopen-rte.so.0(orte_plm_base_app_report_launch+0x230) [0x7f0dbe0c5010]<br>[x-03-20-b:23316] [ 2] /usr/local/MPI/openmpi-1.4.3/lib/libopen-pal.so.0 [0x7f0dbde5c8f8]<br>

[x-03-20-b:23316] [ 3] mpirun [0x403bbe]<br>[x-03-20-b:23316] [ 4] /usr/local/MPI/openmpi-1.4.3/lib/libopen-pal.so.0 [0x7f0dbde5c8f8]<br>[x-03-20-b:23316] [ 5] /usr/local/MPI/openmpi-1.4.3/lib/libopen-pal.so.0(opal_progress+0x99) [0x7f0dbde50e49]<br>

[x-03-20-b:23316] [ 6] /usr/local/MPI/openmpi-1.4.3/lib/libopen-rte.so.0(orte_trigger_event+0x42) [0x7f0dbe0a7ca2]<br>[x-03-20-b:23316] [ 7] /usr/local/MPI/openmpi-1.4.3/lib/libopen-rte.so.0(orte_plm_base_app_report_launch+0x22d) [0x7f0dbe0c500d]<br>

[x-03-20-b:23316] [ 8] /usr/local/MPI/openmpi-1.4.3/lib/libopen-pal.so.0 [0x7f0dbde5c8f8]<br>[x-03-20-b:23316] [ 9] /usr/local/MPI/openmpi-1.4.3/lib/libopen-pal.so.0(opal_progress+0x99) [0x7f0dbde50e49]<br>[x-03-20-b:23316] [10] /usr/local/MPI/openmpi-1.4.3/lib/libopen-rte.so.0(orte_plm_base_launch_apps+0x23d) [0x7f0dbe0c5ddd]<br>

[x-03-20-b:23316] [11] /usr/local/MPI/openmpi-1.4.3/lib/openmpi/mca_plm_rsh.so [0x7f0dbd41d679]<br>[x-03-20-b:23316] [12] mpirun [0x40373f]<br>[x-03-20-b:23316] [13] mpirun [0x402a1c]<br>[x-03-20-b:23316] [14] /lib64/libc.so.6(__libc_start_main+0xfd) [0x3107e1ea2d]<br>

[x-03-20-b:23316] [15] mpirun [0x402939]<br>[x-03-20-b:23316] *** End of error message ***<br>[x-01-06-a:25989] [[37568,0],69]-[[37568,0],0] mca_oob_tcp_msg_recv: readv failed: Connection reset by peer (104)<br>[x-01-06-b:09532] [[37568,0],74]-[[37568,0],0] mca_oob_tcp_msg_recv: readv failed: Connection reset by peer (104)<br>

./sge_jsb.sh: line 9: 23316 Segmentation fault      (core dumped) mpirun -np $NSLOTS ./hello_openmpi.exe<br><br>
</div><br>

