<html><head><style type="text/css"><!-- DIV {margin:0px;} --></style></head><body><div style="font-family:times new roman,new york,times,serif;font-size:12pt"><div>Hi,<br>all are linked.<br>what should I find ? anything different?<br>thank`s <br>and sorry for all....<br></div><div style="font-family: times new roman,new york,times,serif; font-size: 12pt;"><br><div style="font-family: arial,helvetica,sans-serif; font-size: 13px;"><font face="Tahoma" size="2"><hr size="1"><b><span style="font-weight: bold;">De:</span></b> "Addepalli, Srirangam V" &lt;srirangam.v.addepalli@ttu.edu&gt;<br><b><span style="font-weight: bold;">Para:</span></b> Open MPI Users &lt;users@open-mpi.org&gt;<br><b><span style="font-weight: bold;">Enviadas:</span></b> Terça-feira, 8 de Junho de 2010 13:59:08<br><b><span style="font-weight: bold;">Assunto:</span></b> Re: [OMPI users] Res:  Res:  Res:  Gromacs run in parallel<br></font><br>Hello,<br><br>ldd&nbsp; `which
 mdrun_mpi`<br><br>should give you which libraries the binary is looking for.&nbsp; What does the above command do for your build.<br><br>I had a user who had a serial mdrun in his path and it did the same.<br><br>Rangam<br><br>________________________________________<br>From: <a ymailto="mailto:users-bounces@open-mpi.org" href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a> [<a ymailto="mailto:users-bounces@open-mpi.org" href="mailto:users-bounces@open-mpi.org">users-bounces@open-mpi.org</a>] On Behalf Of lauren [<a ymailto="mailto:owenlany@yahoo.com.br" href="mailto:owenlany@yahoo.com.br">owenlany@yahoo.com.br</a>]<br>Sent: Tuesday, June 08, 2010 11:36 AM<br>To: Open MPI Users<br>Subject: [OMPI users] Res:&nbsp; Res:&nbsp; Res:&nbsp; Gromacs run in parallel<br><br>Hi,<br>I did it and it match.....<br>mdrun and mpiexec at the same place.<br>seems ok...<br>1 more suggestion?<br><br>thank
 you,<br><br><br><br><br><br>________________________________<br>De: Carsten Kutzner &lt;<a ymailto="mailto:ckutzne@gwdg.de" href="mailto:ckutzne@gwdg.de">ckutzne@gwdg.de</a>&gt;<br>Para: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Enviadas: Terça-feira, 8 de Junho de 2010 13:12:35<br>Assunto: Re: [OMPI users] Res: Res: Gromacs run in parallel<br><br>Ok,<br><br>1. type 'which mdrun' to see where the mdrun executable resides.<br>2. type ldd 'which mdrun' to find out against which mpi library it is linked<br>3. type which mpirun (or which mpiexec, whatever you use) to verify that<br>this is the right mpi launcher for your mdrun.<br>4. If the MPI's do not match, either use the right mpiexec or recompile<br>gromacs with the current mpi.<br><br>Carsten<br><br><br>On Jun 8, 2010, at 5:50 PM, lauren wrote:<br><br>I saw<br>Host: &lt;somename&gt; pid: &lt;somepid&gt; nodeid: 0 nnodes:
 1<br><br>really it`s running in 1 node<br>and All of you really undestood my problem, thanks<br><br>But how can I fix it.<br>How can I run 1 job in 4 nodes...?<br>I really need help,<br>I took a look in my files and erase all the errors and the implementations seem corect.<br>&gt;From the beginning, please.<br>`case all tutorials only explain the same thing that look right.<br>And thanks very much for this help!<br><br><br><br>________________________________<br>De: Jeff Squyres &lt;<a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&lt;mailto:<a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;&gt;<br>Para: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&lt;mailto:<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;&gt;<br>Enviadas: Terça-feira, 8 de
 Junho de 2010 10:30:03<br>Assunto: Re: [OMPI users] Res: Gromacs run in parallel<br><br>No, I'm sorry -- I wasn't clear.&nbsp; What I meant was, that if you run:<br><br>&nbsp; mpirun -np 4 my_mpi_application<br><br>1. If you see a single, 4-process MPI job (regardless of how many nodes/servers it's spread across), then all is good.&nbsp; This is what you want.<br><br>2. But if you're seeing 4 independent 1-process MPI jobs (again, regardless of how many nodes/servers they are spread across), it's possible that you compiled your application with MPI implementation X and then used the "mpirun" from MPI implementation Y.<br><br>You will need X==Y to make it work properly -- i.e., to see case #1, above.&nbsp; I mention this because your first post mentioned that you're seeing the same job run 4 times.&nbsp; This implied to me that you are running into case #2.&nbsp; If I misunderstood your problem, then ignore me and forgive the noise.<br><br><br><br>On Jun
 8, 2010, at 9:20 AM, Carsten Kutzner wrote:<br><br>&gt; On Jun 8, 2010, at 3:06 PM, Jeff Squyres wrote:<br>&gt;<br>&gt; &gt; I know nothing about Gromacs, but you might want to ensure that your Gromacs was compiled with Open MPI.&nbsp; A common symptom of "mpirun -np 4 my_mpi_application" running 4 1-process MPI jobs (instead of 1 4-process MPI job) is that you compiled my_mpi_application with one MPI implementation, but then used the mpirun from a different MPI implementation.<br>&gt; &gt;<br>&gt; Hi,<br>&gt;<br>&gt; this can be checked by looking at the Gromacs output file md.log. The second line should<br>&gt; read something like<br>&gt;<br>&gt; Host: &lt;somename&gt; pid: &lt;somepid&gt; nodeid: 0 nnodes: 4<br>&gt;<br>&gt; Lauren, you will want to ensure that nnodes is 4 in your case, and not 1.<br>&gt;<br>&gt; You can also easily test that without any input file by typing<br>&gt;<br>&gt; mpirun -np 4 mdrun -h<br>&gt;<br>&gt; and then should
 see<br>&gt;<br>&gt; NNODES=4, MYRANK=1, HOSTNAME=&lt;...&gt;<br>&gt; NNODES=4, MYRANK=2, HOSTNAME=&lt;...&gt;<br>&gt; NNODES=4, MYRANK=3, HOSTNAME=&lt;...&gt;<br>&gt; NNODES=4, MYRANK=4, HOSTNAME=&lt;...&gt;<br>&gt; ...<br>&gt;<br>&gt;<br>&gt; Carsten<br>&gt;<br>&gt;<br>&gt; &gt;<br>&gt; &gt; On Jun 8, 2010, at 8:59 AM, lauren wrote:<br>&gt; &gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; The version of Gromacs is 4.0.7.<br>&gt; &gt;&gt; This is the first time that I using Gromacs, then excuse me if I'm nonsense.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Wich part of md.log output&nbsp; should I post?<br>&gt; &gt;&gt; after or before the input description?<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; thanks for all,<br>&gt; &gt;&gt; and sorry<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; De: Carsten Kutzner &lt;<a ymailto="mailto:ckutzne@gwdg.de" href="mailto:ckutzne@gwdg.de">ckutzne@gwdg.de</a>&lt;mailto:<a ymailto="mailto:ckutzne@gwdg.de"
 href="mailto:ckutzne@gwdg.de">ckutzne@gwdg.de</a>&gt;&gt;<br>&gt; &gt;&gt; Para: Open MPI Users &lt;<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&lt;mailto:<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;&gt;<br>&gt; &gt;&gt; Enviadas: Domingo, 6 de Junho de 2010 9:51:26<br>&gt; &gt;&gt; Assunto: Re: [OMPI users] Gromacs run in parallel<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Hi,<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; which version of Gromacs is this? Could you post the first lines of<br>&gt; &gt;&gt; the md.log output file?<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Carsten<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; On Jun 5, 2010, at 10:23 PM, lauren wrote:<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;&gt; sorry my english..<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; I want to know how can I run&nbsp; Gromancs in parallel!<br>&gt; &gt;&gt;&gt; Because when I used<br>&gt;
 &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; mdrun &amp;<br>&gt; &gt;&gt;&gt; mpiexec -np 4 mdrun_mpi -v -deffnm em<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; to run the minimization in 4 cores &gt; all cores make the same job, again!<br>&gt; &gt;&gt;&gt; They don't run together.<br>&gt; &gt;&gt;&gt; I want all in parallel make the job faster.<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; what could be wrong?<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; thank's a lot!<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; _______________________________________________<br>&gt; &gt;&gt;&gt; users mailing list<br>&gt; &gt;&gt;&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&lt;mailto:<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>&gt; &gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"
 target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; _______________________________________________<br>&gt; &gt;&gt; users mailing list<br>&gt; &gt;&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&lt;mailto:<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>&gt; &gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt; --<br>&gt; &gt; Jeff Squyres<br>&gt; &gt; <a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&lt;mailto:<a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>&gt; &gt; For corporate legal information go to:<br>&gt; &gt; <a
 href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt; _______________________________________________<br>&gt; &gt; users mailing list<br>&gt; &gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&lt;mailto:<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<br>&gt;<br>&gt; --<br>&gt; Dr. Carsten Kutzner<br>&gt; Max Planck Institute for Biophysical Chemistry<br>&gt; Theoretical and Computational Biophysics<br>&gt; Am Fassberg 11, 37077 Goettingen, Germany<br>&gt; Tel. +49-551-2012313, Fax: +49-551-2012302<br>&gt; <a href="http://www.mpibpc.mpg.de/home/grubmueller/ihp/ckutzne"
 target="_blank">http://www.mpibpc.mpg.de/home/grubmueller/ihp/ckutzne</a><br>&gt;<br>&gt;<br>&gt;<br>&gt;<br>&gt;<br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&lt;mailto:<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt;<br><br><br>--<br>Jeff Squyres<br><a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&lt;mailto:<a ymailto="mailto:jsquyres@cisco.com" href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>For corporate legal information go to:<br><a href="http://www.cisco.com/web/about/doing_business/legal/cri/"
 target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br><br><br>_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&lt;mailto:<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br> _______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&lt;mailto:<a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br><br><br>--<br>Dr. Carsten Kutzner<br>Max
 Planck Institute for Biophysical Chemistry<br>Theoretical and Computational Biophysics<br>Am Fassberg 11, 37077 Goettingen, Germany<br>Tel. +49-551-2012313, Fax: +49-551-2012302<br><a href="http://www.mpibpc.mpg.de/home/grubmueller/ihp/ckutzne" target="_blank">http://www.mpibpc.mpg.de/home/grubmueller/ihp/ckutzne</a><br><br><br><br><br><br><br><br>_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></div>
</div><br>



      &nbsp;</body></html>
