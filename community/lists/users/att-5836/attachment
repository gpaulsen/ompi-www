Hi Jeff,<br>thanks for you reply. I did understand previous questions
about RDMA. Ever with SKaMPI, i tried to run with mpi_leave_pinned = 1,
as you have suggested.&nbsp; But also in this case, execution time is very
similar to previous case.<br>
<br>Does it means that SKaMPI, reallocates buffer every time ? For
example, with &quot;MPI_Bcast-length&quot; test, over 128 procs,&nbsp; the collective
is repeated about 28 times, increasing buffer size for each step by
internal formula, and finale buffer size =2097152&nbsp; K.<br>
<br>Since there aren&#39;t advantages with leave_pinned = 1, it means that
SKaMPI doesn&#39;t allocates buffer of 2097152 K initially, but it
allocates small buffer and reallocates buffer every time, with more
large size. Is it possible? If no, which is the cause of similar
performance?<br>
<br>Another question: RDMA pipeline protocol for long messages, in OpenMPI 1.2.6 is setting by default? <br><br><div class="gmail_quote">2008/6/6 Gabriele Fatigati &lt;<a href="mailto:gabriele.fatigati@gmail.com">gabriele.fatigati@gmail.com</a>&gt;:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">Hi Jeff,<br>thanks for you reply. I did understand previous questions about RDMA. Ever with SKaMPI, i tried to run with mpi_leave_pinned = 1, as you have suggested.&nbsp; But also in this case, execution time is very similar to previous case.<br>

<br>Does it means that SKaMPI, reallocates buffer every time ? For example, with &quot;MPI_Bcast-length&quot; test, over 128 procs,&nbsp; the collective is repeated about 28 times, increasing buffer size for each step by internal formula, and finale buffer size =2097152&nbsp; K.<br>

<br>Since there aren&#39;t advantages with leave_pinned = 1, it means that SKaMPI doesn&#39;t allocates buffer of 2097152 K initially, but it allocates small buffer and reallocates buffer every time, with more large size. Is it possible? If no, which is the cause of similar performance?<br>

<br>Another question: RDMA pipeline protocol for long messages, in OpenMPI 1.2.6 is setting by default? <br><br><div class="gmail_quote">2008/6/6 Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;:<div>
<div></div><div class="Wj3C7c"><br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">Note that &quot;eager&quot; RDMA is only used for short messages -- it&#39;s not<br>
really relevant to whether the same user buffers are re-used or not<br>
(the mpi_leave_pinned parameter for long messages is only useful if<br>
long buffers are re-used). &nbsp;See this FAQ item:<br>
<br>
<a href="http://www.open-mpi.org/faq/?category=openfabrics#ib-small-message-rdma" target="_blank">http://www.open-mpi.org/faq/?category=openfabrics#ib-small-message-rdma</a><br>
<br>
For benchmarks (like SKAMPI) that re-use long buffers, you might want<br>
to use the mpi_leave_pinned MCA parameter:<br>
<br>
<a href="http://www.open-mpi.org/faq/?category=openfabrics#large-message-leave-pinned" target="_blank">http://www.open-mpi.org/faq/?category=openfabrics#large-message-leave-pinned</a><br>
<a href="http://www.open-mpi.org/faq/?category=tuning#running-perf-numbers" target="_blank">http://www.open-mpi.org/faq/?category=tuning#running-perf-numbers</a><br>
<div><div></div><div><br>
<br>
On Jun 5, 2008, at 9:47 AM, Gabriele Fatigati wrote:<br>
<br>
&gt;<br>
&gt; Hi,<br>
&gt; i&#39;m testing SKaMPI Benchmark on IBM Blade System over Infiniband.<br>
&gt; Current version of OpenMPI is 1.2.6<br>
&gt; I have tried to disable RDMA setting btl_openib_use_eager_rdma = 0.<br>
&gt; But, i have noted that, in MPI collectives execution time, there are<br>
&gt; few difference beetween RDMA active and none. Before tests, I<br>
&gt; expected that with RDMA off, excecution time was more long.<br>
&gt;<br>
&gt; So, i suppose that SKaMPI benchmark does continues reallocation of<br>
&gt; buffers that forbid benefits of RDMA protocol. Indeed, if initial<br>
&gt; buffer address change every time, we have to do very much<br>
&gt; registration of memory pages afterwards decay of perfomance.<br>
&gt;<br>
&gt; I used RDMA pipeline protocol. This protocol should makes no<br>
&gt; assumption about the application reuse of source and target buffers.<br>
&gt; But, is it every true?<br>
&gt; Parameters net are explained below.<br>
&gt;<br>
&gt; MCA btl: parameter &quot;btl_openib_mpool&quot; (current value: &quot;rdma&quot;)<br>
&gt; MCA btl: parameter &quot;btl_openib_ib_max_rdma_dst<br>
&gt; _ops&quot; (current value: &quot;4&quot;)<br>
&gt; MCA btl: parameter &quot;btl_openib_use_eager_rdma&quot; (current value: &quot;1&quot;)<br>
&gt; MCA btl: parameter &quot;btl_openib_eager_rdma_threshold&quot; (current value:<br>
&gt; &quot;16&quot;)<br>
&gt; MCA btl: parameter &quot;btl_openib_max_eager_rdma&quot; (current value: &quot;16&quot;)<br>
&gt; MCA btl: parameter &quot;btl_openib_eager_rdma_num&quot; (current value: &quot;16&quot;)<br>
&gt; MCA btl: parameter &quot;btl_openib_min_rdma_size&quot; (current value:<br>
&gt; &quot;1048576&quot;)<br>
&gt; MCA btl: parameter &quot;btl_openib_max_rdma_size&quot; (current value:<br>
&gt; &quot;1048576&quot;)<br>
&gt;<br>
&gt; --<br>
&gt; Gabriele Fatigati<br>
&gt;<br>
&gt; CINECA Systems &amp; Tecnologies Department<br>
&gt;<br>
&gt; Supercomputing Group<br>
&gt;<br>
&gt; Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>
&gt;<br>
&gt; <a href="http://www.cineca.it" target="_blank">www.cineca.it</a> Tel: +39 051 6171722<br>
&gt;<br>
</div></div>&gt; <a href="mailto:g.fatigati@cineca.it" target="_blank">g.fatigati@cineca.it</a> _______________________________________________<br>
&gt; users mailing list<br>
&gt; <a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<font color="#888888"><br>
<br>
--<br>
Jeff Squyres<br>
Cisco Systems<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org" target="_blank">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
</font></blockquote></div></div></div><div><div></div><div class="Wj3C7c"><br><br clear="all"><br>-- <br>Gabriele Fatigati<br><br>CINECA Systems &amp; Tecnologies Department<br><br>Supercomputing Group<br><br>Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>
<br><a href="http://www.cineca.it" target="_blank">www.cineca.it</a> Tel: +39 051 6171722<br>
<br><a href="mailto:g.fatigati@cineca.it" target="_blank">g.fatigati@cineca.it</a> 
</div></div></blockquote></div><br><br clear="all"><br>-- <br>Gabriele Fatigati<br><br>CINECA Systems &amp; Tecnologies Department<br><br>Supercomputing Group<br><br>Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>
<br><a href="http://www.cineca.it">www.cineca.it</a> Tel: +39 051 6171722<br><br><a href="mailto:g.fatigati@cineca.it">g.fatigati@cineca.it</a> 

