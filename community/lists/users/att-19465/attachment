Hello Brian, <br><br>You do not have to be sorry. my code was not that clear.<br><br>My Open MPI version is 1.6, the network is Ethernet. <br><br>About the MPI_Alloc_mem, I tried with malloc also, but was getting the same seg-fault.<br>
<br>Thanks,<br>Ziaul<br><br><div class="gmail_quote">On Wed, Jun 6, 2012 at 1:35 PM, Barrett, Brian W <span dir="ltr">&lt;<a href="mailto:bwbarre@sandia.gov" target="_blank">bwbarre@sandia.gov</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Ziaul -<br>
<br>
You&#39;re right, I totally misread the code, sorry about that.  What version<br>
of Open MPI are you using and over what network?<br>
<br>
As an aside, there&#39;s no point in using MPI_ALLOC_MEM for the displacement<br>
arrays.  MPI_ALLOC_MEM is really only advantageous for allocating<br>
communication buffers.  Since you know the displacement arrays aren&#39;t<br>
going to be used for communication, you&#39;re just tying up (potentially<br>
scarce) network resources by using MPI_ALLOC_MEM there.<br>
<br>
Biran<br>
<div class="HOEnZb"><div class="h5"><br>
On 6/6/12 11:24 AM, &quot;Ziaul Haque Olive&quot; &lt;<a href="mailto:mzh.olive@gmail.com">mzh.olive@gmail.com</a>&gt; wrote:<br>
<br>
&gt;Hello Brian,<br>
&gt;<br>
&gt;Actually, I am not modifying the local communication buffer that contains<br>
&gt;the data. I am modifying the the buffer that contains the indices of the<br>
&gt;data buffer(source_disp and target_disp).<br>
&gt;<br>
&gt;in MPICH2 this is not a problem. I am not sure about Open MPI.<br>
&gt;<br>
&gt;Thanks,<br>
&gt;Ziaul<br>
&gt;<br>
&gt;On Wed, Jun 6, 2012 at 1:05 PM, Barrett, Brian W &lt;<a href="mailto:bwbarre@sandia.gov">bwbarre@sandia.gov</a>&gt;<br>
&gt;wrote:<br>
&gt;<br>
&gt;Ziaul -<br>
&gt;<br>
&gt;Your program is erroneous; you can not modify the local communication<br>
&gt;buffer of an MPI_ACCUMULATE call until after the next synchronization call<br>
&gt;(Section 11.3 of MPI 2.2).  In your example, that would be after the<br>
&gt;MPI_FENCE call following the call to MPI_ACCUMULATE.<br>
&gt;<br>
&gt;Brian<br>
&gt;<br>
&gt;On 6/6/12 9:44 AM, &quot;Ziaul Haque Olive&quot; &lt;<a href="mailto:mzh.olive@gmail.com">mzh.olive@gmail.com</a>&gt; wrote:<br>
&gt;<br>
&gt;&gt;Hello,<br>
&gt;&gt;<br>
&gt;&gt;I am not sure, if my code is correct according to Open MPI(v1.6). the<br>
&gt;&gt; code is given as follows, I am doing MPI one-sided communication inside<br>
&gt;&gt; a function - data_transfer. this function is being called inside a<br>
&gt;&gt;fence epoch. inside data_transfer, I am allocating memory for<br>
&gt;&gt;non-contiguous data, creating derived data type, using this datatype in<br>
&gt;&gt;MPI_Accumulate, and after calling MPI_Accumulate, freeing the indexed<br>
&gt;&gt;data type and also freeing the memory containing indices for indexed<br>
&gt;&gt;data type. is it okay that I am freeing memory for derived datatype<br>
&gt;&gt;before the closing<br>
&gt;&gt;fence?<br>
&gt;&gt;<br>
&gt;&gt;I am getting segmentation fault with this code. if I comment out the<br>
&gt;&gt;MPI_Accumulate call, then no seg-fault occurs.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;void data_transfer(void *data, int<br>
&gt;&gt;*sources_disp, int *targets_disp, int *target, int size, int *blength,<br>
&gt;&gt;int func, MPI_Op op, MPI_Win win, MPI_Datatype dtype){<br>
&gt;&gt;<br>
&gt;&gt;    int i,j, index;<br>
&gt;&gt;    int tmp_target;<br>
&gt;&gt;    int *flag;<br>
&gt;&gt;    int *source_disp;<br>
&gt;&gt;    int *target_disp;<br>
&gt;&gt;    MPI_Datatype source_type, target_type;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;    MPI_Alloc_mem( size*sizeof(int), MPI_INFO_NULL, &amp;source_disp);<br>
&gt;&gt;    MPI_Alloc_mem( size*sizeof(int), MPI_INFO_NULL, &amp;target_disp);<br>
&gt;&gt;    MPI_Alloc_mem( size*sizeof(int), MPI_INFO_NULL, &amp;flag );<br>
&gt;&gt;<br>
&gt;&gt;    memset(flag, 0, size*sizeof(int));<br>
&gt;&gt;<br>
&gt;&gt;    for(i=0;i&lt;size;i++){<br>
&gt;&gt;        if(flag[i]==0){<br>
&gt;&gt;            tmp_target = target[i];<br>
&gt;&gt;<br>
&gt;&gt;            index = 0;<br>
&gt;&gt;            for(j=i; j&lt;size; j++){<br>
&gt;&gt;                if(flag[j]==0 &amp;&amp; tmp_target == target[j] ){<br>
&gt;&gt;                    source_disp[index] = sources_disp[j];<br>
&gt;&gt;                    target_disp[index] = targets_disp[j];<br>
&gt;&gt;                    //printf(&quot;src, target disp %d  %d\n&quot;, j, disp[j]);<br>
&gt;&gt;                    index++;<br>
&gt;&gt;                    flag[j] = 1;<br>
&gt;&gt;                 }<br>
&gt;&gt;            }<br>
&gt;&gt;<br>
&gt;&gt;            MPI_Type_indexed(index, blength , source_disp, dtype,<br>
&gt;&gt;&amp;source_type);<br>
&gt;&gt;            MPI_Type_commit(&amp;source_type);<br>
&gt;&gt;            MPI_Type_indexed(index, blength , target_disp, dtype,<br>
&gt;&gt;&amp;target_type);<br>
&gt;&gt;            MPI_Type_commit(&amp;target_type);<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;            MPI_Accumulate( data, 1, source_type, tmp_target, 0, 1,<br>
&gt;&gt;target_type , op, win);<br>
&gt;&gt;<br>
&gt;&gt;            MPI_Type_free(&amp;source_type);<br>
&gt;&gt;            MPI_Type_free(&amp;target_type);<br>
&gt;&gt;        }<br>
&gt;&gt;    }<br>
&gt;&gt;    MPI_Free_mem(source_disp);<br>
&gt;&gt;    MPI_Free_mem(target_disp);<br>
&gt;&gt;    MPI_Free_mem(flag);<br>
&gt;&gt;<br>
&gt;&gt;}<br>
&gt;&gt;<br>
&gt;&gt;void main(){<br>
&gt;&gt;    int i;<br>
&gt;&gt;    while(i&lt;N){<br>
&gt;&gt;             MPI_Win_fence(MPI_MODE_NOPRECEDE, queue2_win);<br>
&gt;&gt;<br>
&gt;&gt;             data_transfer();<br>
&gt;&gt;<br>
&gt;&gt;             MPI_Win_fence(MPI_MODE_NOSUCCEED, queue2_win);<br>
&gt;&gt;    }<br>
&gt;&gt;}<br>
&gt;&gt;<br>
&gt;&gt;thanks,<br>
&gt;&gt;Ziaul<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;<br>
&gt;<br>
&gt;&gt;_______________________________________________<br>
&gt;&gt;users mailing list<br>
&gt;&gt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;&gt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt;--<br>
&gt;  Brian W. Barrett<br>
&gt;  Dept. 1423: Scalable System Software<br>
&gt;  Sandia National Laboratories<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;_______________________________________________<br>
&gt;users mailing list<br>
&gt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;_______________________________________________<br>
&gt;users mailing list<br>
&gt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
&gt;<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
<br>
<br>
--<br>
  Brian W. Barrett<br>
  Dept. 1423: Scalable System Software<br>
  Sandia National Laboratories<br>
<br>
<br>
<br>
<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
</div></div></blockquote></div><br>

