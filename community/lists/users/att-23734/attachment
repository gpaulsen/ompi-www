<html>
  <head>
    <meta content="text/html; charset=ISO-8859-1"
      http-equiv="Content-Type">
  </head>
  <body text="#000000" bgcolor="#FFFFFF">
    <div class="moz-cite-prefix">What's your mpirun or mpiexec
      command-line?<br>
      The error "BTLs attempted: self sm tcp" says that it didn't even
      try the MX BTL (for Open-MX). Did you use the MX MTL instead?<br>
      Are you sure that you actually use Open-MX when not mixing AMD and
      Intel nodes?<br>
      <br>
      Brice<br>
      <br>
      <br>
      <br>
      Le 02/03/2014 08:06, Victor a &eacute;crit&nbsp;:<br>
    </div>
    <blockquote
cite="mid:CAJTHAufNCyHBp=-4L4tK01A2VAK6CVPG3SZkojQvyXGCaEpoYg@mail.gmail.com"
      type="cite">
      <div dir="ltr">I got 4 x AMD A-10 6800K nodes on loan for a few
        months and added them to my existing Intel nodes.
        <div><br>
        </div>
        <div>All nodes share the relevant directories via NFS. I have
          OpenMPI 1.6.5 which was build with Open-MX 1.5.3 support
          networked via GbE.</div>
        <div><br>
        </div>
        <div>All nodes run Ubuntu 12.04.</div>
        <div><br>
        </div>
        <div>Problem:</div>
        <div><br>
        </div>
        <div>I can run a job EITHER on 4 x AMD nodes OR on 2 x Intel
          nodes, but I cannot run a job on any combination of an AMD and
          Intel node, ie. 1 x AMD node + 1 x Intel node = error below.</div>
        <div><br>
        </div>
        <div>The error that I get during job setup is:</div>
        <blockquote class="gmail_quote" style="margin:0px 0px 0px
0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><br>
--------------------------------------------------------------------------<br>
          At least one pair of MPI processes are unable to reach each
          other for<br>
          MPI communications. &nbsp;This means that no Open MPI device has
          indicated<br>
          that it can be used to communicate between these processes.
          &nbsp;This is<br>
          an error; Open MPI requires that all MPI processes be able to
          reach<br>
          each other. &nbsp;This error can sometimes be the result of
          forgetting to<br>
          specify the "self" BTL.<br>
          &nbsp; Process 1 ([[2229,1],1]) is on host: AMD-Node-1<br>
          &nbsp; Process 2 ([[2229,1],8]) is on host: Intel-Node-1<br>
          &nbsp; BTLs attempted: self sm tcp<br>
          Your MPI job is now going to abort; sorry.<br>
--------------------------------------------------------------------------<br>
--------------------------------------------------------------------------<br>
          MPI_INIT has failed because at least one MPI process is
          unreachable<br>
          from another. &nbsp;This *usually* means that an underlying
          communication<br>
          plugin -- such as a BTL or an MTL -- has either not loaded or
          not<br>
          allowed itself to be used. &nbsp;Your MPI job will now abort.<br>
          You may wish to try to narrow down the problem;<br>
          &nbsp;* Check the output of ompi_info to see which BTL/MTL plugins
          are<br>
          &nbsp; &nbsp;available.<br>
          &nbsp;* Run your application with MPI_THREAD_SINGLE.<br>
          &nbsp;* Set the MCA parameter btl_base_verbose to 100 (or
          mtl_base_verbose,<br>
          &nbsp; &nbsp;if using MTL-based communications) to see exactly which<br>
          &nbsp; &nbsp;communication plugins were considered and/or discarded.<br>
--------------------------------------------------------------------------<br>
          [AMD-Node-1:3932] *** An error occurred in MPI_Init<br>
          [AMD-Node-1:3932] *** on a NULL communicator<br>
          [AMD-Node-1:3932] *** Unknown error<br>
          [AMD-Node-1:3932] *** MPI_ERRORS_ARE_FATAL: your MPI job will
          now abort<br>
--------------------------------------------------------------------------<br>
          An MPI process is aborting at a time when it cannot guarantee
          that all<br>
          of its peer processes in the job will be killed properly. &nbsp;You
          should<br>
          double check that everything has shut down cleanly.<br>
          &nbsp; Reason: &nbsp; &nbsp; Before MPI_INIT completed<br>
          &nbsp; Local host: AMD-Node-1<br>
          &nbsp; PID: &nbsp; &nbsp; &nbsp; &nbsp;3932<br>
--------------------------------------------------------------------------</blockquote>
        <div><br>
        </div>
        <div><br>
        </div>
        <div>What I would like to know is, is it actually difficult
          (impossible) to mix AMD and Intel machines in the same cluster
          and have them run the same job, or am I missing something
          obvious, or not so obvious when it comes to the communication
          stack on the Intel nodes for example.&nbsp;</div>
        <div><br>
        </div>
        <div>I set up the AMD nodes just yesterday, but I used the same
          OpenMPI and Open-MX versions, however I may have inadvertently
          done something different, so I am thinking (hoping) that it is
          possible to run such a heterogeneous cluster, and that all I
          need to do is ensure that all OpenMPI modules are correctly
          installed on all nodes.</div>
        <div><br>
        </div>
        <div>I need the extra 32 Gb RAM and the AMD nodes bring as I
          need to validate our CFD application, and our additional Intel
          nodes are still not here (ETA 2 weeks).</div>
        <div><br>
        </div>
        <div>Thank you,</div>
        <div>
          <br>
        </div>
        <div>Victor</div>
      </div>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:users@open-mpi.org">users@open-mpi.org</a>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></pre>
    </blockquote>
    <br>
  </body>
</html>

