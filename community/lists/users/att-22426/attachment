<html>
  <head>

    <meta http-equiv="content-type" content="text/html; charset=ISO-8859-1">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    Hello everyone,<br>
    <br>
    I have installed openmpi 1.5.4 on 11 node cluster using "yum install
    openmpi openmpi-devel" and everything seems to be working fine. For
    testing I am using this test program<br>
    <br>
    //******************************************************************<br>
    <br>
    <b>$ cat test.cpp</b><br>
    <br>
    #include &lt;stdio.h&gt;<br>
    #include &lt;mpi.h&gt;<br>
    <br>
    int main (int argc, char *argv[])<br>
    {<br>
    &nbsp; int id, np;<br>
    &nbsp; char name[MPI_MAX_PROCESSOR_NAME];<br>
    &nbsp; int namelen;<br>
    &nbsp; int i;<br>
    <br>
    &nbsp; MPI_Init (&amp;argc, &amp;argv);<br>
    <br>
    &nbsp; MPI_Comm_size (MPI_COMM_WORLD, &amp;np);<br>
    &nbsp; MPI_Comm_rank (MPI_COMM_WORLD, &amp;id);<br>
    &nbsp; MPI_Get_processor_name (name, &amp;namelen);<br>
    <br>
    &nbsp; printf ("This is Process %2d out of %2d running on host %s\n", id,
    np, name);<br>
    <br>
    &nbsp; MPI_Finalize ();<br>
    <br>
    &nbsp; return (0);<br>
    }<br>
    <br>
    //******************************************************************<br>
    <br>
    and my hosts file look like this:<br>
    <br>
    <b>$ cat mpi_hostfile</b><br>
    <br>
    # The Hostfile for Open MPI<br>
    <br>
    # specify number of slots for processes to run locally.<br>
    #localhost slots=12<br>
    #x.x.x.16 slots=12 max-slots=12<br>
    #x.x.x.17 slots=12 max-slots=12<br>
    #x.x.x.18 slots=12 max-slots=12<br>
    #x.x.1x.19 slots=12 max-slots=12<br>
    #x.x.x.20 slots=12 max-slots=12<br>
    #x.x.x.55 slots=46 max-slots=46<br>
    #x.x.x.56 slots=46 max-slots=46<br>
    <br>
    x.x.x.22 slots=15 max-slots=15<br>
    x.x.x.24 slots=2 max-slots=2<br>
    x.x.x.26 slots=14 max-slots=14<br>
    x.x.x.28 slots=16 max-slots=16<br>
    x.x.x.29 slots=14 max-slots=14<br>
    x.x.x.30 slots=16 max-slots=16<br>
    x.x.x.41 slots=46 max-slots=46<br>
    x.x.x.101 slots=46 max-slots=46<br>
    x.x.x.100 slots=46 max-slots=46<br>
    x.x.x.102 slots=22 max-slots=22<br>
    x.x.x.103 slots=22 max-slots=22<br>
    <br>
    # The following slave nodes are available to this machine:<br>
    x.x.x.24<br>
    x.x.x.26<br>
    x.x.x.28<br>
    x.x.x.29<br>
    x.x.x.30<br>
    x.x.x.41<br>
    x.x.x.101<br>
    x.x.x.100<br>
    x.x.x.102<br>
    x.x.x.103<br>
    <br>
    //******************************************************************<br>
    <br>
    this is how my .bashrc looks like on each node:<br>
    <br>
    <b>$ cat ~/.bashrc</b><br>
    <br>
    # .bashrc<br>
    <br>
    # Source global definitions<br>
    if [ -f /etc/bashrc ]; then<br>
    &nbsp;&nbsp;&nbsp; . /etc/bashrc<br>
    fi<br>
    <br>
    # User specific aliases and functions<br>
    umask 077<br>
    <br>
    export PSM_SHAREDCONTEXTS_MAX=20<br>
    <br>
    #export PATH=/usr/lib64/openmpi/bin${PATH:+:$PATH}<br>
    export PATH=/usr/OPENMPI/openmpi-1.7.2/bin${PATH:+:$PATH}<br>
    <br>
    #export
LD_LIBRARY_PATH=/usr/lib64/openmpi/lib${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}<br>
    export
LD_LIBRARY_PATH=/usr/OPENMPI/openmpi-1.7.2/lib${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}<br>
    <br>
    export PATH="$PATH":/bin/:/usr/lib/:/usr/lib:/usr:/usr/<br>
    <br>
    //******************************************************************<br>
    <br>
    <b>$ mpic++ test.cpp -o test</b><br>
    <br>
    <b>$ mpirun -d --display-map -np 10 --hostfile mpi_hostfile --bynode
      ./test</b><br>
    <br>
    //******************************************************************<br>
    <br>
    These nodes are running 2.6.32-358.2.1.el6.x86_64 release<br>
    <br>
    <b>$ </b><b>uname</b><br>
    Linux<br>
    <b>$ </b><b>uname -r</b><br>
    2.6.32-358.2.1.el6.x86_64<br>
    <b>$ cat /etc/issue</b><br>
    CentOS release 6.4 (Final)<br>
    Kernel \r on an \m<br>
    <br>
    //******************************************************************<br>
    <br>
    Now, if I install openmpi 1.7.2 on each node separately then I can
    only use it on either first 7 nodes or last 4 nodes but not on all
    of them.&nbsp; <br>
    <br>
    //******************************************************************<br>
    <br>
    <b>$ gunzip -c openmpi-1.7.2.tar.gz | tar xf -</b><b><br>
    </b><b><br>
    </b><b>$ cd openmpi-1.7.2</b><b><br>
    </b><b>&nbsp;&nbsp;&nbsp; </b><b><br>
    </b><b>$ ./configure --prefix=/usr/OPENMPI/openmpi-1.7.2
      --enable-event-thread-support --enable-opal-multi-threads
      --enable-orte-progress-threads --enable-mpi-thread-multiple</b><b><br>
    </b><b><br>
    </b><b>$ make all install</b><br>
    <br>
    //******************************************************************<br>
    <br>
    This is the error message that i am receiving:<br>
    <br>
    <br>
    <b>$ mpirun -d --display-map -np 10 --hostfile mpi_hostfile --bynode
      ./test</b><br>
    <br>
    [SERVER-2:05284] procdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-2_0/50535/0/0<br>
    [SERVER-2:05284] jobdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-2_0/50535/0<br>
    [SERVER-2:05284] top: openmpi-sessions-mpidemo@SERVER-2_0<br>
    [SERVER-2:05284] tmp: /tmp<br>
    CentOS release 6.4 (Final)<br>
    Kernel \r on an \m<br>
    CentOS release 6.4 (Final)<br>
    Kernel \r on an \m<br>
    CentOS release 6.4 (Final)<br>
    Kernel \r on an \m<br>
    [SERVER-3:28993] procdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-3_0/50535/0/1<br>
    [SERVER-3:28993] jobdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-3_0/50535/0<br>
    [SERVER-3:28993] top: openmpi-sessions-mpidemo@SERVER-3_0<br>
    [SERVER-3:28993] tmp: /tmp<br>
    CentOS release 6.4 (Final)<br>
    Kernel \r on an \m<br>
    CentOS release 6.4 (Final)<br>
    Kernel \r on an \m<br>
    [SERVER-6:09087] procdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-6_0/50535/0/4<br>
    [SERVER-6:09087] jobdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-6_0/50535/0<br>
    [SERVER-6:09087] top: openmpi-sessions-mpidemo@SERVER-6_0<br>
    [SERVER-6:09087] tmp: /tmp<br>
    [SERVER-7:32563] procdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-7_0/50535/0/5<br>
    [SERVER-7:32563] jobdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-7_0/50535/0<br>
    [SERVER-7:32563] top: openmpi-sessions-mpidemo@SERVER-7_0<br>
    [SERVER-7:32563] tmp: /tmp<br>
    [SERVER-4:15711] procdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-4_0/50535/0/2<br>
    [SERVER-4:15711] jobdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-4_0/50535/0<br>
    [SERVER-4:15711] top: openmpi-sessions-mpidemo@SERVER-4_0<br>
    [SERVER-4:15711] tmp: /tmp<br>
    [sv-1:45701] procdir: /tmp/openmpi-sessions-mpidemo@sv-1_0/50535/0/8<br>
    [sv-1:45701] jobdir: /tmp/openmpi-sessions-mpidemo@sv-1_0/50535/0<br>
    [sv-1:45701] top: openmpi-sessions-mpidemo@sv-1_0<br>
    [sv-1:45701] tmp: /tmp<br>
    CentOS release 6.4 (Final)<br>
    Kernel \r on an \m<br>
    [sv-3:08352] procdir: /tmp/openmpi-sessions-mpidemo@sv-3_0/50535/0/9<br>
    [sv-3:08352] jobdir: /tmp/openmpi-sessions-mpidemo@sv-3_0/50535/0<br>
    [sv-3:08352] top: openmpi-sessions-mpidemo@sv-3_0<br>
    [sv-3:08352] tmp: /tmp<br>
    [SERVER-5:12534] procdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-5_0/50535/0/3<br>
    [SERVER-5:12534] jobdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-5_0/50535/0<br>
    [SERVER-5:12534] top: openmpi-sessions-mpidemo@SERVER-5_0<br>
    [SERVER-5:12534] tmp: /tmp<br>
    [SERVER-14:08399] procdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-14_0/50535/0/6<br>
    [SERVER-14:08399] jobdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-14_0/50535/0<br>
    [SERVER-14:08399] top: openmpi-sessions-mpidemo@SERVER-14_0<br>
    [SERVER-14:08399] tmp: /tmp<br>
    [sv-4:11802] procdir:
    /tmp/openmpi-sessions-mpidemo@sv-4_0/50535/0/10<br>
    [sv-4:11802] jobdir: /tmp/openmpi-sessions-mpidemo@sv-4_0/50535/0<br>
    [sv-4:11802] top: openmpi-sessions-mpidemo@sv-4_0<br>
    [sv-4:11802] tmp: /tmp<br>
    [sv-2:07503] procdir: /tmp/openmpi-sessions-mpidemo@sv-2_0/50535/0/7<br>
    [sv-2:07503] jobdir: /tmp/openmpi-sessions-mpidemo@sv-2_0/50535/0<br>
    [sv-2:07503] top: openmpi-sessions-mpidemo@sv-2_0<br>
    [sv-2:07503] tmp: /tmp<br>
    <br>
    &nbsp;Mapper requested: NULL&nbsp; Last mapper: round_robin&nbsp; Mapping policy:
    BYNODE&nbsp; Ranking policy: NODE&nbsp; Binding policy: NONE[NODE]&nbsp; Cpu set:
    NULL&nbsp; PPR: NULL<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num new daemons: 0&nbsp;&nbsp;&nbsp; New daemon starting vpid INVALID<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num nodes: 10<br>
    <br>
    &nbsp;Data for node: SERVER-2&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Launch id: -1&nbsp;&nbsp;&nbsp; State: 2<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Daemon: [[50535,0],0]&nbsp;&nbsp;&nbsp; Daemon launched: True<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots: 15&nbsp;&nbsp;&nbsp; Slots in use: 1&nbsp;&nbsp;&nbsp; Oversubscribed: FALSE<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots allocated: 15&nbsp;&nbsp;&nbsp; Max slots: 15<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Username on node: NULL<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num procs: 1&nbsp;&nbsp;&nbsp; Next node_rank: 1<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Data for proc: [[50535,1],0]<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; Pid: 0&nbsp;&nbsp;&nbsp; Local rank: 0&nbsp;&nbsp;&nbsp; Node rank: 0&nbsp;&nbsp;&nbsp; App rank: 0<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; State: INITIALIZED&nbsp;&nbsp;&nbsp; Restarts: 0&nbsp;&nbsp;&nbsp; App_context: 0&nbsp;&nbsp;&nbsp;
    Locale: 0-15&nbsp;&nbsp;&nbsp; Binding: NULL[0]<br>
    <br>
    &nbsp;Data for node: x.x.x.24&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Launch id: -1&nbsp;&nbsp;&nbsp; State: 0<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Daemon: [[50535,0],1]&nbsp;&nbsp;&nbsp; Daemon launched: False<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots: 3&nbsp;&nbsp;&nbsp; Slots in use: 1&nbsp;&nbsp;&nbsp; Oversubscribed: FALSE<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots allocated: 3&nbsp;&nbsp;&nbsp; Max slots: 2<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Username on node: NULL<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num procs: 1&nbsp;&nbsp;&nbsp; Next node_rank: 1<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Data for proc: [[50535,1],1]<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; Pid: 0&nbsp;&nbsp;&nbsp; Local rank: 0&nbsp;&nbsp;&nbsp; Node rank: 0&nbsp;&nbsp;&nbsp; App rank: 1<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; State: INITIALIZED&nbsp;&nbsp;&nbsp; Restarts: 0&nbsp;&nbsp;&nbsp; App_context: 0&nbsp;&nbsp;&nbsp;
    Locale: 0-7&nbsp;&nbsp;&nbsp; Binding: NULL[0]<br>
    <br>
    &nbsp;Data for node: x.x.x.26&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Launch id: -1&nbsp;&nbsp;&nbsp; State: 0<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Daemon: [[50535,0],2]&nbsp;&nbsp;&nbsp; Daemon launched: False<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots: 15&nbsp;&nbsp;&nbsp; Slots in use: 1&nbsp;&nbsp;&nbsp; Oversubscribed: FALSE<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots allocated: 15&nbsp;&nbsp;&nbsp; Max slots: 14<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Username on node: NULL<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num procs: 1&nbsp;&nbsp;&nbsp; Next node_rank: 1<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Data for proc: [[50535,1],2]<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; Pid: 0&nbsp;&nbsp;&nbsp; Local rank: 0&nbsp;&nbsp;&nbsp; Node rank: 0&nbsp;&nbsp;&nbsp; App rank: 2<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; State: INITIALIZED&nbsp;&nbsp;&nbsp; Restarts: 0&nbsp;&nbsp;&nbsp; App_context: 0&nbsp;&nbsp;&nbsp;
    Locale: 0-7&nbsp;&nbsp;&nbsp; Binding: NULL[0]<br>
    <br>
    &nbsp;Data for node: x.x.x.28&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Launch id: -1&nbsp;&nbsp;&nbsp; State: 0<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Daemon: [[50535,0],3]&nbsp;&nbsp;&nbsp; Daemon launched: False<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots: 17&nbsp;&nbsp;&nbsp; Slots in use: 1&nbsp;&nbsp;&nbsp; Oversubscribed: FALSE<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots allocated: 17&nbsp;&nbsp;&nbsp; Max slots: 16<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Username on node: NULL<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num procs: 1&nbsp;&nbsp;&nbsp; Next node_rank: 1<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Data for proc: [[50535,1],3]<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; Pid: 0&nbsp;&nbsp;&nbsp; Local rank: 0&nbsp;&nbsp;&nbsp; Node rank: 0&nbsp;&nbsp;&nbsp; App rank: 3<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; State: INITIALIZED&nbsp;&nbsp;&nbsp; Restarts: 0&nbsp;&nbsp;&nbsp; App_context: 0&nbsp;&nbsp;&nbsp;
    Locale: 0-7&nbsp;&nbsp;&nbsp; Binding: NULL[0]<br>
    <br>
    &nbsp;Data for node: x.x.x.29&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Launch id: -1&nbsp;&nbsp;&nbsp; State: 0<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Daemon: [[50535,0],4]&nbsp;&nbsp;&nbsp; Daemon launched: False<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots: 15&nbsp;&nbsp;&nbsp; Slots in use: 1&nbsp;&nbsp;&nbsp; Oversubscribed: FALSE<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots allocated: 15&nbsp;&nbsp;&nbsp; Max slots: 14<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Username on node: NULL<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num procs: 1&nbsp;&nbsp;&nbsp; Next node_rank: 1<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Data for proc: [[50535,1],4]<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; Pid: 0&nbsp;&nbsp;&nbsp; Local rank: 0&nbsp;&nbsp;&nbsp; Node rank: 0&nbsp;&nbsp;&nbsp; App rank: 4<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; State: INITIALIZED&nbsp;&nbsp;&nbsp; Restarts: 0&nbsp;&nbsp;&nbsp; App_context: 0&nbsp;&nbsp;&nbsp;
    Locale: 0-7&nbsp;&nbsp;&nbsp; Binding: NULL[0]<br>
    <br>
    &nbsp;Data for node: x.x.x.30&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Launch id: -1&nbsp;&nbsp;&nbsp; State: 0<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Daemon: [[50535,0],5]&nbsp;&nbsp;&nbsp; Daemon launched: False<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots: 17&nbsp;&nbsp;&nbsp; Slots in use: 1&nbsp;&nbsp;&nbsp; Oversubscribed: FALSE<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots allocated: 17&nbsp;&nbsp;&nbsp; Max slots: 16<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Username on node: NULL<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num procs: 1&nbsp;&nbsp;&nbsp; Next node_rank: 1<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Data for proc: [[50535,1],5]<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; Pid: 0&nbsp;&nbsp;&nbsp; Local rank: 0&nbsp;&nbsp;&nbsp; Node rank: 0&nbsp;&nbsp;&nbsp; App rank: 5<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; State: INITIALIZED&nbsp;&nbsp;&nbsp; Restarts: 0&nbsp;&nbsp;&nbsp; App_context: 0&nbsp;&nbsp;&nbsp;
    Locale: 0-7&nbsp;&nbsp;&nbsp; Binding: NULL[0]<br>
    <br>
    &nbsp;Data for node: x.x.x.41&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Launch id: -1&nbsp;&nbsp;&nbsp; State: 0<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Daemon: [[50535,0],6]&nbsp;&nbsp;&nbsp; Daemon launched: False<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots: 47&nbsp;&nbsp;&nbsp; Slots in use: 1&nbsp;&nbsp;&nbsp; Oversubscribed: FALSE<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots allocated: 47&nbsp;&nbsp;&nbsp; Max slots: 46<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Username on node: NULL<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num procs: 1&nbsp;&nbsp;&nbsp; Next node_rank: 1<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Data for proc: [[50535,1],6]<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; Pid: 0&nbsp;&nbsp;&nbsp; Local rank: 0&nbsp;&nbsp;&nbsp; Node rank: 0&nbsp;&nbsp;&nbsp; App rank: 6<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; State: INITIALIZED&nbsp;&nbsp;&nbsp; Restarts: 0&nbsp;&nbsp;&nbsp; App_context: 0&nbsp;&nbsp;&nbsp;
    Locale: 0-7&nbsp;&nbsp;&nbsp; Binding: NULL[0]<br>
    <br>
    &nbsp;Data for node: x.x.x.101&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Launch id: -1&nbsp;&nbsp;&nbsp; State: 0<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Daemon: [[50535,0],7]&nbsp;&nbsp;&nbsp; Daemon launched: False<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots: 47&nbsp;&nbsp;&nbsp; Slots in use: 1&nbsp;&nbsp;&nbsp; Oversubscribed: FALSE<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots allocated: 47&nbsp;&nbsp;&nbsp; Max slots: 46<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Username on node: NULL<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num procs: 1&nbsp;&nbsp;&nbsp; Next node_rank: 1<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Data for proc: [[50535,1],7]<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; Pid: 0&nbsp;&nbsp;&nbsp; Local rank: 0&nbsp;&nbsp;&nbsp; Node rank: 0&nbsp;&nbsp;&nbsp; App rank: 7<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; State: INITIALIZED&nbsp;&nbsp;&nbsp; Restarts: 0&nbsp;&nbsp;&nbsp; App_context: 0&nbsp;&nbsp;&nbsp;
    Locale: 0-7&nbsp;&nbsp;&nbsp; Binding: NULL[0]<br>
    <br>
    &nbsp;Data for node: x.x.x.100&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Launch id: -1&nbsp;&nbsp;&nbsp; State: 0<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Daemon: [[50535,0],8]&nbsp;&nbsp;&nbsp; Daemon launched: False<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots: 47&nbsp;&nbsp;&nbsp; Slots in use: 1&nbsp;&nbsp;&nbsp; Oversubscribed: FALSE<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots allocated: 47&nbsp;&nbsp;&nbsp; Max slots: 46<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Username on node: NULL<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num procs: 1&nbsp;&nbsp;&nbsp; Next node_rank: 1<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Data for proc: [[50535,1],8]<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; Pid: 0&nbsp;&nbsp;&nbsp; Local rank: 0&nbsp;&nbsp;&nbsp; Node rank: 0&nbsp;&nbsp;&nbsp; App rank: 8<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; State: INITIALIZED&nbsp;&nbsp;&nbsp; Restarts: 0&nbsp;&nbsp;&nbsp; App_context: 0&nbsp;&nbsp;&nbsp;
    Locale: 0-7&nbsp;&nbsp;&nbsp; Binding: NULL[0]<br>
    <br>
    &nbsp;Data for node: x.x.x.102&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Launch id: -1&nbsp;&nbsp;&nbsp; State: 0<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Daemon: [[50535,0],9]&nbsp;&nbsp;&nbsp; Daemon launched: False<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots: 23&nbsp;&nbsp;&nbsp; Slots in use: 1&nbsp;&nbsp;&nbsp; Oversubscribed: FALSE<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num slots allocated: 23&nbsp;&nbsp;&nbsp; Max slots: 22<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Username on node: NULL<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Num procs: 1&nbsp;&nbsp;&nbsp; Next node_rank: 1<br>
    &nbsp;&nbsp;&nbsp;&nbsp; Data for proc: [[50535,1],9]<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; Pid: 0&nbsp;&nbsp;&nbsp; Local rank: 0&nbsp;&nbsp;&nbsp; Node rank: 0&nbsp;&nbsp;&nbsp; App rank: 9<br>
    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; State: INITIALIZED&nbsp;&nbsp;&nbsp; Restarts: 0&nbsp;&nbsp;&nbsp; App_context: 0&nbsp;&nbsp;&nbsp;
    Locale: 0-7&nbsp;&nbsp;&nbsp; Binding: NULL[0]<br>
    [sv-1:45712] procdir: /tmp/openmpi-sessions-mpidemo@sv-1_0/50535/1/8<br>
    [sv-1:45712] jobdir: /tmp/openmpi-sessions-mpidemo@sv-1_0/50535/1<br>
    [sv-1:45712] top: openmpi-sessions-mpidemo@sv-1_0<br>
    [sv-1:45712] tmp: /tmp<br>
    [SERVER-14:08412] procdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-14_0/50535/1/6<br>
    [SERVER-14:08412] jobdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-14_0/50535/1<br>
    [SERVER-14:08412] top: openmpi-sessions-mpidemo@SERVER-14_0<br>
    [SERVER-14:08412] tmp: /tmp<br>
    [SERVER-2:05291] procdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-2_0/50535/1/0<br>
    [SERVER-2:05291] jobdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-2_0/50535/1<br>
    [SERVER-2:05291] top: openmpi-sessions-mpidemo@SERVER-2_0<br>
    [SERVER-2:05291] tmp: /tmp<br>
    [SERVER-4:15726] procdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-4_0/50535/1/2<br>
    [SERVER-4:15726] jobdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-4_0/50535/1<br>
    [SERVER-4:15726] top: openmpi-sessions-mpidemo@SERVER-4_0<br>
    [SERVER-4:15726] tmp: /tmp<br>
    [SERVER-6:09100] procdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-6_0/50535/1/4<br>
    [SERVER-6:09100] jobdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-6_0/50535/1<br>
    [SERVER-6:09100] top: openmpi-sessions-mpidemo@SERVER-6_0<br>
    [SERVER-6:09100] tmp: /tmp<br>
    [SERVER-7:32576] procdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-7_0/50535/1/5<br>
    [SERVER-7:32576] jobdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-7_0/50535/1<br>
    [SERVER-7:32576] top: openmpi-sessions-mpidemo@SERVER-7_0<br>
    [SERVER-7:32576] tmp: /tmp<br>
    [sv-3:08363] procdir: /tmp/openmpi-sessions-mpidemo@sv-3_0/50535/1/9<br>
    [sv-3:08363] jobdir: /tmp/openmpi-sessions-mpidemo@sv-3_0/50535/1<br>
    [sv-3:08363] top: openmpi-sessions-mpidemo@sv-3_0<br>
    [sv-3:08363] tmp: /tmp<br>
    [sv-2:07514] procdir: /tmp/openmpi-sessions-mpidemo@sv-2_0/50535/1/7<br>
    [sv-2:07514] jobdir: /tmp/openmpi-sessions-mpidemo@sv-2_0/50535/1<br>
    [sv-2:07514] top: openmpi-sessions-mpidemo@sv-2_0<br>
    [sv-2:07514] tmp: /tmp<br>
    [SERVER-5:12548] procdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-5_0/50535/1/3<br>
    [SERVER-5:12548] jobdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-5_0/50535/1<br>
    [SERVER-5:12548] top: openmpi-sessions-mpidemo@SERVER-5_0<br>
    [SERVER-5:12548] tmp: /tmp<br>
    [SERVER-3:29009] procdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-3_0/50535/1/1<br>
    [SERVER-3:29009] jobdir:
    /tmp/openmpi-sessions-mpidemo@SERVER-3_0/50535/1<br>
    [SERVER-3:29009] top: openmpi-sessions-mpidemo@SERVER-3_0<br>
    [SERVER-3:29009] tmp: /tmp<br>
    &nbsp; MPIR_being_debugged = 0<br>
    &nbsp; MPIR_debug_state = 1<br>
    &nbsp; MPIR_partial_attach_ok = 1<br>
    &nbsp; MPIR_i_am_starter = 0<br>
    &nbsp; MPIR_forward_output = 0<br>
    &nbsp; MPIR_proctable_size = 10<br>
    &nbsp; MPIR_proctable:<br>
    &nbsp;&nbsp;&nbsp; (i, host, exe, pid) = (0, SERVER-2,
    /usr2/mpidemo/dev/DISTRIBUTED_COMPUTING/./test, 5291)<br>
    &nbsp;&nbsp;&nbsp; (i, host, exe, pid) = (1, x.x.x.24,
    /usr2/mpidemo/dev/DISTRIBUTED_COMPUTING/./test, 29009)<br>
    &nbsp;&nbsp;&nbsp; (i, host, exe, pid) = (2, x.x.x.26,
    /usr2/mpidemo/dev/DISTRIBUTED_COMPUTING/./test, 15726)<br>
    &nbsp;&nbsp;&nbsp; (i, host, exe, pid) = (3, x.x.x.28,
    /usr2/mpidemo/dev/DISTRIBUTED_COMPUTING/./test, 12548)<br>
    &nbsp;&nbsp;&nbsp; (i, host, exe, pid) = (4, x.x.x.29,
    /usr2/mpidemo/dev/DISTRIBUTED_COMPUTING/./test, 9100)<br>
    &nbsp;&nbsp;&nbsp; (i, host, exe, pid) = (5, x.x.x.30,
    /usr2/mpidemo/dev/DISTRIBUTED_COMPUTING/./test, 32576)<br>
    &nbsp;&nbsp;&nbsp; (i, host, exe, pid) = (6, x.x.x.41,
    /usr2/mpidemo/dev/DISTRIBUTED_COMPUTING/./test, 8412)<br>
    &nbsp;&nbsp;&nbsp; (i, host, exe, pid) = (7, x.x.x.101,
    /usr2/mpidemo/dev/DISTRIBUTED_COMPUTING/./test, 7514)<br>
    &nbsp;&nbsp;&nbsp; (i, host, exe, pid) = (8, x.x.x.100,
    /usr2/mpidemo/dev/DISTRIBUTED_COMPUTING/./test, 45712)<br>
    &nbsp;&nbsp;&nbsp; (i, host, exe, pid) = (9, x.x.x.102,
    /usr2/mpidemo/dev/DISTRIBUTED_COMPUTING/./test, 8363)<br>
    MPIR_executable_path: NULL<br>
    MPIR_server_arguments: NULL<br>
--------------------------------------------------------------------------<br>
    It looks like MPI_INIT failed for some reason; your parallel process
    is<br>
    likely to abort.&nbsp; There are many reasons that a parallel process can<br>
    fail during MPI_INIT; some of which are due to configuration or
    environment<br>
    problems.&nbsp; This failure appears to be an internal failure; here's
    some<br>
    additional information (which may only be relevant to an Open MPI<br>
    developer):<br>
    <br>
    &nbsp; PML add procs failed<br>
    &nbsp; --&gt; Returned "Error" (-1) instead of "Success" (0)<br>
--------------------------------------------------------------------------<br>
    [SERVER-2:5291] *** An error occurred in MPI_Init<br>
    [SERVER-2:5291] *** reported by process
    [140508871983105,140505560121344]<br>
    [SERVER-2:5291] *** on a NULL communicator<br>
    [SERVER-2:5291] *** Unknown error<br>
    [SERVER-2:5291] *** MPI_ERRORS_ARE_FATAL (processes in this
    communicator will now abort,<br>
    [SERVER-2:5291] ***&nbsp;&nbsp;&nbsp; and potentially your MPI job)<br>
--------------------------------------------------------------------------<br>
    An MPI process is aborting at a time when it cannot guarantee that
    all<br>
    of its peer processes in the job will be killed properly.&nbsp; You
    should<br>
    double check that everything has shut down cleanly.<br>
    <br>
    &nbsp; Reason:&nbsp;&nbsp;&nbsp;&nbsp; Before MPI_INIT completed<br>
    &nbsp; Local host: SERVER-2<br>
    &nbsp; PID:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5291<br>
--------------------------------------------------------------------------<br>
    [sv-1][[50535,1],8][btl_openib_proc.c:157:mca_btl_openib_proc_create]
    [btl_openib_proc.c:157] ompi_modex_recv failed for peer
    [[50535,1],0]<br>
    [sv-3][[50535,1],9][btl_openib_proc.c:157:mca_btl_openib_proc_create]
    [btl_openib_proc.c:157] ompi_modex_recv failed for peer
    [[50535,1],0]<br>
    [sv-3][[50535,1],9][btl_tcp_proc.c:128:mca_btl_tcp_proc_create]
    mca_base_modex_recv: failed with return value=-13<br>
    [sv-3][[50535,1],9][btl_tcp_proc.c:128:mca_btl_tcp_proc_create]
    mca_base_modex_recv: failed with return value=-13<br>
    [sv-1][[50535,1],8][btl_tcp_proc.c:128:mca_btl_tcp_proc_create]
    mca_base_modex_recv: failed with return value=-13<br>
    [sv-1][[50535,1],8][btl_tcp_proc.c:128:mca_btl_tcp_proc_create]
    mca_base_modex_recv: failed with return value=-13<br>
--------------------------------------------------------------------------<br>
    At least one pair of MPI processes are unable to reach each other
    for<br>
    MPI communications.&nbsp; This means that no Open MPI device has
    indicated<br>
    that it can be used to communicate between these processes.&nbsp; This is<br>
    an error; Open MPI requires that all MPI processes be able to reach<br>
    each other.&nbsp; This error can sometimes be the result of forgetting to<br>
    specify the "self" BTL.<br>
    <br>
    &nbsp; Process 1 ([[50535,1],8]) is on host: sv-1<br>
    &nbsp; Process 2 ([[50535,1],0]) is on host: SERVER-2<br>
    &nbsp; BTLs attempted: openib self sm tcp<br>
    <br>
    Your MPI job is now going to abort; sorry.<br>
--------------------------------------------------------------------------<br>
--------------------------------------------------------------------------<br>
    MPI_INIT has failed because at least one MPI process is unreachable<br>
    from another.&nbsp; This *usually* means that an underlying communication<br>
    plugin -- such as a BTL or an MTL -- has either not loaded or not<br>
    allowed itself to be used.&nbsp; Your MPI job will now abort.<br>
    <br>
    You may wish to try to narrow down the problem;<br>
    <br>
    &nbsp;* Check the output of ompi_info to see which BTL/MTL plugins are<br>
    &nbsp;&nbsp; available.<br>
    &nbsp;* Run your application with MPI_THREAD_SINGLE.<br>
    &nbsp;* Set the MCA parameter btl_base_verbose to 100 (or
    mtl_base_verbose,<br>
    &nbsp;&nbsp; if using MTL-based communications) to see exactly which<br>
    &nbsp;&nbsp; communication plugins were considered and/or discarded.<br>
--------------------------------------------------------------------------<br>
    [sv-2][[50535,1],7][btl_openib_proc.c:157:mca_btl_openib_proc_create]
    [btl_openib_proc.c:157] ompi_modex_recv failed for peer
    [[50535,1],0]<br>
    [sv-2][[50535,1],7][btl_tcp_proc.c:128:mca_btl_tcp_proc_create]
    mca_base_modex_recv: failed with return value=-13<br>
    [sv-2][[50535,1],7][btl_tcp_proc.c:128:mca_btl_tcp_proc_create]
    mca_base_modex_recv: failed with return value=-13<br>
    [SERVER-2:05284] sess_dir_finalize: proc session dir not empty -
    leaving<br>
    [SERVER-2:05284] sess_dir_finalize: proc session dir not empty -
    leaving<br>
    [sv-4:11802] sess_dir_finalize: job session dir not empty - leaving<br>
    [SERVER-14:08399] sess_dir_finalize: job session dir not empty -
    leaving<br>
    [SERVER-6:09087] sess_dir_finalize: proc session dir not empty -
    leaving<br>
    [SERVER-6:09087] sess_dir_finalize: proc session dir not empty -
    leaving<br>
    [SERVER-4:15711] sess_dir_finalize: proc session dir not empty -
    leaving<br>
    [SERVER-4:15711] sess_dir_finalize: proc session dir not empty -
    leaving<br>
    [SERVER-6:09087] sess_dir_finalize: job session dir not empty -
    leaving<br>
    exiting with status 0<br>
    [SERVER-7:32563] sess_dir_finalize: proc session dir not empty -
    leaving<br>
    [SERVER-7:32563] sess_dir_finalize: proc session dir not empty -
    leaving<br>
    [SERVER-5:12534] sess_dir_finalize: proc session dir not empty -
    leaving<br>
    [SERVER-5:12534] sess_dir_finalize: proc session dir not empty -
    leaving<br>
    [SERVER-7:32563] sess_dir_finalize: job session dir not empty -
    leaving<br>
    exiting with status 0<br>
    exiting with status 0<br>
    exiting with status 0<br>
    [SERVER-4:15711] sess_dir_finalize: job session dir not empty -
    leaving<br>
    [SERVER-3:28993] sess_dir_finalize: proc session dir not empty -
    leaving<br>
    exiting with status 0<br>
    [SERVER-3:28993] sess_dir_finalize: proc session dir not empty -
    leaving<br>
    [sv-3:08352] sess_dir_finalize: proc session dir not empty - leaving<br>
    [sv-3:08352] sess_dir_finalize: job session dir not empty - leaving<br>
    [sv-1:45701] sess_dir_finalize: proc session dir not empty - leaving<br>
    [sv-1:45701] sess_dir_finalize: job session dir not empty - leaving<br>
    exiting with status 0<br>
    exiting with status 0<br>
    [sv-2:07503] sess_dir_finalize: proc session dir not empty - leaving<br>
    [sv-2:07503] sess_dir_finalize: job session dir not empty - leaving<br>
    exiting with status 0<br>
    [SERVER-5:12534] sess_dir_finalize: job session dir not empty -
    leaving<br>
    exiting with status 0<br>
    [SERVER-3:28993] sess_dir_finalize: job session dir not empty -
    leaving<br>
    exiting with status 0<br>
--------------------------------------------------------------------------<br>
    mpirun has exited due to process rank 6 with PID 8412 on<br>
    node x.x.x.41 exiting improperly. There are three reasons this could
    occur:<br>
    <br>
    1. this process did not call "init" before exiting, but others in<br>
    the job did. This can cause a job to hang indefinitely while it
    waits<br>
    for all processes to call "init". By rule, if one process calls
    "init",<br>
    then ALL processes must call "init" prior to termination.<br>
    <br>
    2. this process called "init", but exited without calling
    "finalize".<br>
    By rule, all processes that call "init" MUST call "finalize" prior
    to<br>
    exiting or it will be considered an "abnormal termination"<br>
    <br>
    3. this process called "MPI_Abort" or "orte_abort" and the mca
    parameter<br>
    orte_create_session_dirs is set to false. In this case, the run-time
    cannot<br>
    detect that the abort call was an abnormal termination. Hence, the
    only<br>
    error message you will receive is this one.<br>
    <br>
    This may have caused other processes in the application to be<br>
    terminated by signals sent by mpirun (as reported here).<br>
    <br>
    You can avoid this message by specifying -quiet on the mpirun
    command line.<br>
    <br>
--------------------------------------------------------------------------<br>
    [SERVER-2:05284] 6 more processes have sent help message
    help-mpi-runtime / mpi_init:startup:internal-failure<br>
    [SERVER-2:05284] Set MCA parameter "orte_base_help_aggregate" to 0
    to see all help / error messages<br>
    [SERVER-2:05284] 9 more processes have sent help message
    help-mpi-errors.txt / mpi_errors_are_fatal unknown handle<br>
    [SERVER-2:05284] 9 more processes have sent help message
    help-mpi-runtime.txt / ompi mpi abort:cannot guarantee all killed<br>
    [SERVER-2:05284] 2 more processes have sent help message
    help-mca-bml-r2.txt / unreachable proc<br>
    [SERVER-2:05284] 2 more processes have sent help message
    help-mpi-runtime / mpi_init:startup:pml-add-procs-fail<br>
    [SERVER-2:05284] sess_dir_finalize: job session dir not empty -
    leaving<br>
    exiting with status 1<br>
    <br>
    //******************************************************************<br>
    <br>
    Any feedback will be helpful. Thank you!<br>
    <br>
    Mr. Beans<br>
  </body>
</html>

