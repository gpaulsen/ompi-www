Hi,<br><br>A fortran application is installed with Intel Fortran 10.1, MKL-10 and
Openmpi-1.3.3 on a Rocks-5.1 HPC Linux cluster. The jobs are not
scaling when more than one node is used. The cluster has Intel Quad
core Xeon (E5472) @ 3.00GHz Dual processor (total 8 cores per node,
16GB RAM) and Infiniband interconnectivity.<br>

<br>Here are some of the timings:<br><br>12 cores (Node 1: 8 cores, Node2: 4 cores)  --  No progress in the job <br> 8 cores (Node 1: 8 cores)                        <div class="gmail_quote">   -- 21 hours (38 CG move steps)<br>


 4 cores (Node 1: 4 cores)                           -- 25 hours <br>12 cores (Node 1, Node 2, Node 3: 4cores each) -- No progress <br><br> <br>   Later to check, whether Open MPI is using IB or not, I used --mca btl openib. But the job failed with following error message:<br>
# cat /home1/g03/apps_test/amber/test16/err.352.job16 <br>--------------------------------------------------------------------------<br>At least one pair of MPI processes are unable to reach each other for<br>MPI communications.  This means that no Open MPI device has indicated<br>
that it can be used to communicate between these processes.  This is<br>an error; Open MPI requires that all MPI processes be able to reach<br>each other.  This error can sometimes be the result of forgetting to<br>specify the &quot;self&quot; BTL.<br>
<br>  Process 1 ([[23671,1],12]) is on host: compute-0-12.local<br>  Process 2 ([[23671,1],12]) is on host: compute-0-12.local<br>  BTLs attempted: openib<br><br>Your MPI job is now going to abort; sorry.<br>--------------------------------------------------------------------------<br>
--------------------------------------------------------------------------<br>It looks like MPI_INIT failed for some reason; your parallel process is<br>likely to abort.  There are many reasons that a parallel process can<br>
fail during MPI_INIT; some of which are due to configuration or environment<br>problems.  This failure appears to be an internal failure; here&#39;s some<br>additional information (which may only be relevant to an Open MPI<br>
developer):<br><br>  PML add procs failed<br>  --&gt; Returned &quot;Unreachable&quot; (-12) instead of &quot;Success&quot; (0)<br>--------------------------------------------------------------------------<br>*** An error occurred in MPI_Init<br>
*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[compute-0-12.local:5496] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>
*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[compute-0-5.local:6916] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>
*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>[compute-0-5.local:6914] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>
*** An error occurred in MPI_Init<br>*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>
[compute-0-5.local:6915] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>[compute-0-5.local:6913] Abort before MPI_INIT completed successfully; not able to guarantee that all other processes were killed!<br>
*** An error occurred in MPI_Init<br>*** before MPI was initialized<br>*** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)<br>mpirun has exited due to process rank 12 with PID 5496 on<br>node compute-0-12.local exiting without calling &quot;finalize&quot;. This may<br>
have caused other processes in the application to be<br>terminated by signals sent by mpirun (as reported here).<br>--------------------------------------------------------------------------<br>[compute-0-5.local:06910] 15 more processes have sent help message help-mca-bml-r2.txt / unreachable proc<br>
[compute-0-5.local:06910] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages<br>[compute-0-5.local:06910] 15 more processes have sent help message help-mpi-runtime / mpi_init:startup:internal-failure<br>
--------------------------------------------------------------------------<br>At least one pair of MPI processes are unable to reach each other for<br>MPI communications.  This means that no Open MPI device has indicated<br>
that it can be used to communicate between these processes.  This is<br>an error; Open MPI requires that all MPI processes be able to reach<br>each other.  This error can sometimes be the result of forgetting to<br>specify the &quot;self&quot; BTL.<br>
<br>  Process 1 ([[23958,1],2]) is on host: compute-0-5.local<br>  Process 2 ([[23958,1],2]) is on host: compute-0-5.local<br>  BTLs attempted: openib<br>   <br>Then added &#39;self&#39; to --mca btl openib,. With this it started running, but I can make sure its not using IB as I observed it from the netstat -i command.<br>
<br>1st Snap:<br><br>Every 2.0s: netstat -i                                                                                                               Sun Oct 11 15:29:29 2009<br><br>Kernel Interface table<br>Iface       MTU Met    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg<br>
eth0       1500   0  1847619      0     0    0  2073010    0      0      0 BMRU<br>ib0      65520   0     708      0     0    0      509    0      5      0 BMRU<br>lo        16436   0     5731      0     0    0     5731    0      0      0 LRU<br>
<br>2nd Snap:<br><br>Every 2.0s: netstat -i                                                                                                               Sun Oct 11 15:29:57 2009<br><br>Kernel Interface table<br>Iface       MTU Met    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg<br>
eth0       1500   0  1847647      0     0    0  2073073    0      0      0 BMRU<br>ib0      65520   0     708      0     0    0      509    0      5      0 BMRU<br>lo        16436   0     5731      0     0    0     5731    0      0      0 LRU<br>
<br>Why OpenMPI is not able to use IB?<br><br>The ldd to the executable shows, no IB libraries are linked. Is this the reason:<br>ldd /opt/apps/siesta/siesta_mpi<br>    /opt/intel/mkl/<a href="http://10.0.5.025/lib/em64t/libmkl_intel_lp64.so">10.0.5.025/lib/em64t/libmkl_intel_lp64.so</a> (0x00002aaaaaaad000)<br>
    /opt/intel/mkl/<a href="http://10.0.5.025/lib/em64t/libmkl_intel_thread.so">10.0.5.025/lib/em64t/libmkl_intel_thread.so</a> (0x00002aaaaadc2000)<br>    /opt/intel/mkl/<a href="http://10.0.5.025/lib/em64t/libmkl_core.so">10.0.5.025/lib/em64t/libmkl_core.so</a> (0x00002aaaab2ad000)<br>
    libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00000034a6200000)<br>    libmpi_f90.so.0 =&gt; /opt/mpi/openmpi/1.3.3/intel/lib/libmpi_f90.so.0 (0x00002aaaab4a0000)<br>    libmpi_f77.so.0 =&gt; /opt/mpi/openmpi/1.3.3/intel/lib/libmpi_f77.so.0 (0x00002aaaab6a3000)<br>
    libmpi.so.0 =&gt; /opt/mpi/openmpi/1.3.3/intel/lib/libmpi.so.0 (0x00002aaaab8db000)<br>    libopen-rte.so.0 =&gt; /opt/mpi/openmpi/1.3.3/intel/lib/libopen-rte.so.0 (0x00002aaaabbaa000)<br>    libopen-pal.so.0 =&gt; /opt/mpi/openmpi/1.3.3/intel/lib/libopen-pal.so.0 (0x00002aaaabe07000)<br>
    libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00000034a5e00000)<br>    libnsl.so.1 =&gt; /lib64/libnsl.so.1 (0x00000034a8200000)<br>    libutil.so.1 =&gt; /lib64/libutil.so.1 (0x00000034a6600000)<br>    libifport.so.5 =&gt; /opt/intel/fce/10.1.008/lib/libifport.so.5 (0x00002aaaac09a000)<br>
    libifcoremt.so.5 =&gt; /opt/intel/fce/10.1.008/lib/libifcoremt.so.5 (0x00002aaaac1d0000)<br>    libimf.so =&gt; /opt/intel/cce/10.1.018/lib/libimf.so (0x00002aaaac401000)<br>    libsvml.so =&gt; /opt/intel/cce/10.1.018/lib/libsvml.so (0x00002aaaac766000)<br>
    libm.so.6 =&gt; /lib64/libm.so.6 (0x00000034a6e00000)<br>    libguide.so =&gt; /opt/intel/mkl/<a href="http://10.0.5.025/lib/em64t/libguide.so">10.0.5.025/lib/em64t/libguide.so</a> (0x00002aaaac8f1000)<br>    libintlc.so.5 =&gt; /opt/intel/cce/10.1.018/lib/libintlc.so.5 (0x00002aaaaca65000)<br>
    libc.so.6 =&gt; /lib64/libc.so.6 (0x00000034a5a00000)<br>    libgcc_s.so.1 =&gt; /lib64/libgcc_s.so.1 (0x00000034a7e00000)<br>    /lib64/ld-linux-x86-64.so.2 (0x00000034a5600000)<br><br>With the help of Open MPI FAQ:<br>
<br># /opt/mpi/openmpi/1.3.3/intel/bin/ompi_info --param btl openib<br>                 MCA btl: parameter &quot;btl_base_verbose&quot; (current value: &quot;0&quot;, data source: default value)<br>                          Verbosity level of the BTL framework<br>
                 MCA btl: parameter &quot;btl&quot; (current value: &lt;none&gt;, data source: default value)<br>                          Default selection set of components for the btl framework (&lt;none&gt; means use all components that can be found)<br>
                 MCA btl: parameter &quot;btl_openib_verbose&quot; (current value: &quot;0&quot;, data source: default value)<br>                          Output some verbose OpenIB BTL information (0 = no output, nonzero = output)<br>
                 MCA btl: parameter &quot;btl_openib_warn_no_device_params_found&quot; (current value: &quot;1&quot;, data source: default value, synonyms:<br>                          btl_openib_warn_no_hca_params_found)<br>
                          Warn when no device-specific parameters are found in the INI file specified by the btl_openib_device_param_files MCA parameter (0 =<br>                          do not warn; any other value = warn)<br>
                 MCA btl: parameter &quot;btl_openib_warn_no_hca_params_found&quot; (current value: &quot;1&quot;, data source: default value, deprecated, synonym of:<br>                          btl_openib_warn_no_device_params_found)<br>
                          Warn when no device-specific parameters are found in the INI file specified by the btl_openib_device_param_files MCA parameter (0 =<br>                          do not warn; any other value = warn)<br>
                 MCA btl: parameter &quot;btl_openib_warn_default_gid_prefix&quot; (current value: &quot;1&quot;, data source: default value)<br>                          Warn when there is more than one active ports and at least one of them connected to the network with only default GID prefix<br>
                          configured (0 = do not warn; any other value = warn)<br>                 MCA btl: parameter &quot;btl_openib_warn_nonexistent_if&quot; (current value: &quot;1&quot;, data source: default value)<br>
                          Warn if non-existent devices and/or ports are specified in the btl_openib_if_[in|ex]clude MCA parameters (0 = do not warn; any<br>                          other value = warn)<br><br>During Open MPI install I&#39;ve used --with-openib=/usr. So I believe its compiled with IB support.<br>
<br>The IB utilities such as ibv_rc_pingpong are working fine.<br><br>I&#39;m not getting why its OMPI is not using IB? Please help me to resolve this issue.<br><br>Thanks<br></div>

