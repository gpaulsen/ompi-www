<html><head><meta http-equiv="Content-Type" content="text/html charset=us-ascii"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">Really shouldn't matter - this is clearly a bug in OMPI if it is doing mapping as you describe. Out of curiosity, have you tried the 1.7 series? Does it behave the same?<div><br></div><div>I can take a look at the code later today and try to figure out what happened.</div><div><br><div><div>On Nov 22, 2013, at 9:56 AM, Jason Gans &lt;<a href="mailto:jgans@lanl.gov">jgans@lanl.gov</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;">On 11/22/13 10:47 AM, Reuti wrote:<br><blockquote type="cite">Hi,<br><br>Am 22.11.2013 um 17:32 schrieb Gans, Jason D:<br><br><blockquote type="cite">I would like to run an instance of my application on every *core* of a small cluster. I am using Torque 2.5.12 to run jobs on the cluster. The cluster in question is a heterogeneous collection of machines that are all past their prime. Specifically, the number of cores ranges from 2-8. Here is the Torque "nodes" file:<br><br>n0000 np=2<br>n0001 np=2<br>n0002 np=8<br>n0003 np=8<br>n0004 np=2<br>n0005 np=2<br>n0006 np=2<br>n0007 np=4<br><br>When I use openmpi-1.6.3, I can oversubscribe nodes but the tasks are allocated to nodes without regard to the number of cores on each node (specified by the "np=xx" in the nodes file). For example, when I run "mpirun -np 24 hostname", mpirun places three instances of "hostname" on each node, despite the fact that some nodes only have two processors and some have more.<br></blockquote>You submitted the job itself by requesting 24 cores for it too?<br><br>-- Reuti<br></blockquote>Since there are only 8 Torque nodes in the cluster, I submitted the job by requesting 8 nodes, i.e. "qsub -I -l nodes=8".<br><blockquote type="cite"><br><br><blockquote type="cite">Is there a way to have OpenMPI "gracefully" oversubscribe nodes by allocating instances based on the "np=xx" information in the Torque nodes file? It this a Torque problem?<br><br>p.s. I do get the desired behavior when I run *without* Torque and specify the following machine file to mpirun:<br><br>n0000 slots=2<br>n0001 slots=2<br>n0002 slots=8<br>n0003 slots=8<br>n0004 slots=2<br>n0005 slots=2<br>n0006 slots=2<br>n0007 slots=4<br><br>Regards,<br><br>Jason<br><br><br><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users<br></blockquote><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></blockquote></div><br></div></body></html>
