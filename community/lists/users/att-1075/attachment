Thank you, Brian:<br><br>It worked.<br><br>Thank you, again.<br><br><div><span class="gmail_quote">On 4/16/06, <b class="gmail_sendername">Brian Barrett</b> &lt;<a href="mailto:brbarret@open-mpi.org">brbarret@open-mpi.org</a>
&gt; wrote:</span><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">On Apr 16, 2006, at 8:18 AM, Sang Chul Choi wrote:<br><br>&gt; 1. I could not find any document except FAQ and mailing list
<br>&gt; for Open MPI. Is there any user manual or something like that?<br>&gt; Or, the LAM MPI's manual can be used instead?<br><br>Unfortunately, at this time, the only documentation available for<br>Open MPI is the FAQ and the mailing list.&nbsp;&nbsp;There are some fairly
<br>significant differences between Open MPI and LAM/MPI, so while the<br>LAM/MPI manuals could be a start, there are some fairly significant<br>differences.<br><br>&gt; 2. Another question is about installation.<br>&gt; If I want to use rsh/ssh for Open MPI, do I have to install
<br>&gt; Open MPI on all master and slave nodes? Or, should I use<br>&gt; something like NSF file system so that even though I installed<br>&gt; Open MPI on only master node, all the other salve nodes could<br>&gt; see Open MPI installation in the master node?
<br><br>Like LAM/MPI, Open MPI doesn't really care on this point.&nbsp;&nbsp;This is<br>also somewhat of a religious point -- people seem to have strong<br>opinions either way.&nbsp;&nbsp;The advantage of the NFS approach is that it<br>makes it trivial to keep the software installs in sync on all the
<br>nodes.&nbsp;&nbsp;The advantage of the installation on local disk approach is<br>that there is significantly less strain on the NFS server during<br>process startup.&nbsp;&nbsp;&nbsp;&nbsp;For development, I tend to go with the NFS<br>approach, since I'm constantly updating my installation.&nbsp;&nbsp;For large
<br>cluster production installs, I prefer the installation on each node<br>approach.&nbsp;&nbsp;But unless your cluster is really large (or your NFS<br>server is really slow) either approach should work.<br><br>&gt; The error I have is from I wanted to run a program on two slave
<br>&gt; nodes but shell complained that there is no orted. It is true<br>&gt; that there is no installation of Open MPI on each slave node.<br><br>Yes, that is the expected error if you try to run on a node without<br>an Open MPI installation.&nbsp;&nbsp;If you ensure that a copy of Open MPI is
<br>installed (and in your path) on each node, your problem should go away.<br><br><br>Hope this helps,<br><br>Brian<br><br>--<br>&nbsp;&nbsp; Brian Barrett<br>&nbsp;&nbsp; Open MPI developer<br>&nbsp;&nbsp; <a href="http://www.open-mpi.org/">http://www.open-mpi.org/
</a><br><br><br>_______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users
</a><br></blockquote></div><br><br clear="all"><br>-- <br>=============================== <br>Live, Learn, and Love! <br>E-mail : goshng at empal dot com <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;goshng at gmail dot com&nbsp;&nbsp;<br>Home : +1-919-468-2578 
<br>Address : 1528 Macalpine Circle <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Morrisville, NC 27560 <br>===============================

