When I was doing presales, the vast majority of our small to middle size procurements were for a three years duration.<div>sometimes, the maintenance was extended for one year, but the cluster was generally replaced after three years.</div><div>I can understand the fastest clusters might last longer (5 years for e xample) because of the huge initial investment, especially if it takes a year or so for the system to be fully ready for production.</div><div>past 5 years, and imho, a cluster made of commodities components does a better job as a heater compared to number crunching.</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<br><br>On Tuesday, March 22, 2016, Jeff Hammond &lt;<a href="mailto:jeff.science@gmail.com">jeff.science@gmail.com</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><br><div class="gmail_extra"><br><div class="gmail_quote">On Mon, Mar 21, 2016 at 6:06 AM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;gilles.gouaillardet@gmail.com&#39;);" target="_blank">gilles.gouaillardet@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Durga,<div><br></div><div>currently, the average life expectancy of a cluster is 3 years.</div></blockquote><div><br></div><div>By average life expectancy, do you mean the average time to upgrade?  DOE supercomputers usually run for 5-6 years, and some HPC systems run for 6-9 years.  I know that some folks upgrade their clusters more often than every 3 years, but I&#39;ve never heard of an HPC system that was used for less than 3 years.</div><div> </div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div>si if you have to architect a cluster out of off the shelf components, I would recommend</div><div>you take the &quot;best&quot; components available today or to be released in a very near future.</div><div>so many things can happen in 10 years, so I can only suggest you do not lock in yourself with a given vendor.</div><div><br></div></blockquote><div><br></div><div>Indeed, just write code that relies upon open standards and then buy the hardware that supports those open standards best at any given point in time.</div><div><br></div><div>MPI is, of course, the best open standard to which you should be writing applications, and I am absolutely certain that MPI will be supported by interconnect products in 10 years, if for no other reason than why Fortran 77 is widely supported nearly 40 years after its introduction :-)<br></div><br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div></div><div>&quot;best&quot; should be understood as &quot;best match with your needs and your budget&quot;.</div><div><br></div></blockquote><div><br></div><div>And relying upon MPI rather than some hardware technology is the best way to optimize your budget, because MPI is supported by a huge range of networks, both commodity and custom.</div><div> </div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div></div><div>as a general though, market is both rational and irrational, so the best engineered technology might not always prevail. and I do not know the magic recipe to guarantee success.</div><div><br></div></blockquote><div><br></div><div>For example, VHS vs Betamax :-)<br><br>Jeff</div><div> </div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div></div><div>Cheers,</div><div><br></div><div>Gilles<br><br>On Monday, March 21, 2016, dpchoudh . &lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;dpchoudh@gmail.com&#39;);" target="_blank">dpchoudh@gmail.com</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><div><div>Hello all<br><br></div>I don&#39;t mean this to be a political conversation, but more of a research type.<br><br></div>From what I have been observing, some of the interconnects that had very good technological features as well as popularity in the past have basically gone down the history book and some others, with comparable feature set, have gained (although I won&#39;t put any names here, neither of these are commodity gigabit Ethernet).<br><br></div>Any comments on what drives these factors? Put another way, if I am to architect a system consisting of commodity nodes today, how can I reasonably be sure that the interconnect will be a good choice, in all sense of the word &#39;good&#39;, say, 10 years down the road?<br><div><div><div><div><div><br></div><div>Thanks<br></div><div>Durga<br></div><div><br clear="all"><div><div><div dir="ltr"><div>We learn from history that we never learn from history.<br></div></div></div></div>
</div></div></div></div></div></div>
</blockquote></div>
<br>_______________________________________________<br>
users mailing list<br>
<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;users@open-mpi.org&#39;);" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/03/28769.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/users/2016/03/28769.php</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br><div>Jeff Hammond<br><a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;jeff.science@gmail.com&#39;);" target="_blank">jeff.science@gmail.com</a><br><a href="http://jeffhammond.github.io/" target="_blank">http://jeffhammond.github.io/</a></div>
</div></div>
</blockquote></div>

