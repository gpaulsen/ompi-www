
<br><font size=2 face="sans-serif">I will add to what Terry said by mentioning
that the MPI implementation has no awareness of ordinary POSIX or Fortran
disk I/O routines. &nbsp;It cannot help on those.</font>
<br>
<br><font size=2 face="sans-serif">Any automated help the MPI implementation
can provide would only apply to MPI_File_xxx &nbsp;disk I/O. &nbsp;These
are implemented by the MPI library. </font>
<br>
<br><font size=2 face="sans-serif">It is possible for MPI-IO to be implemented
in a way that lets a single process or the set of process on a node act
as the disk i/O agents for the entire job but someone else will need to
tell you if OpenMPI can do this, &nbsp;I think OpenMPI built on the ROMIO
MPI-IO implementation and based on my outdated knowledge of ROMIO, I would
be a bit surprised if it has his option.</font>
<br>
<br>
<br><font size=2 face="sans-serif">Dick Treumann &nbsp;- &nbsp;MPI Team
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <br>
IBM Systems &amp; Technology Group<br>
Dept X2ZA / MS P963 -- 2455 South Road -- Poughkeepsie, NY 12601<br>
Tele (845) 433-7846 &nbsp; &nbsp; &nbsp; &nbsp; Fax (845) 433-8363<br>
</font>
<br>
<br>
<br>
<table width=100%>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">From:</font>
<td><font size=1 face="sans-serif">Terry Frankcombe &lt;terry@chem.gu.se&gt;</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">To:</font>
<td><font size=1 face="sans-serif">Open MPI Users &lt;users@open-mpi.org&gt;</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Date:</font>
<td><font size=1 face="sans-serif">09/29/2010 09:50 PM</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Subject:</font>
<td><font size=1 face="sans-serif">Re: [OMPI users] a question about [MPI]IO
on systems &nbsp; &nbsp; &nbsp; &nbsp;without &nbsp; &nbsp; &nbsp;
&nbsp;network filesystem</font>
<tr valign=top>
<td><font size=1 color=#5f5f5f face="sans-serif">Sent by:</font>
<td><font size=1 face="sans-serif">users-bounces@open-mpi.org</font></table>
<br>
<hr noshade>
<br>
<br>
<br><tt><font size=2>Hi Paul<br>
<br>
I think you should clarify whether you mean you want you application to<br>
send all it's data back to a particular rank, which then does all IO (in<br>
which case the answer is any MPI implementation can do this... it's a<br>
matter of how you code the app), or if you want the application to know<br>
nothing about it, but have the system somehow intercept all IO and make<br>
it magically appear at a particular node (much harder).<br>
<br>
<br>
On Wed, 2010-09-29 at 11:34 +0200, Paul Kapinos wrote:<br>
&gt; Dear OpenMPI developer,<br>
&gt; <br>
&gt; We have a question about the possibility to use MPI IO (and possible
<br>
&gt; regular I/O) on clusters which does *not* have a common filesystem
<br>
&gt; (network filesystem) on all nodes.<br>
&gt; <br>
&gt; A common filesystem is mainly NOT a hard precondition to use OpenMPI:<br>
&gt; </font></tt><a href="http://www.open-mpi.org/faq/?category=running#do-i-need-a-common-filesystem"><tt><font size=2>http://www.open-mpi.org/faq/?category=running#do-i-need-a-common-filesystem</font></tt></a><tt><font size=2><br>
&gt; <br>
&gt; <br>
&gt; Say, we have a (diskless? equipped with very small disks?) cluster,
on <br>
&gt; which only one node have access to a filesystem.<br>
&gt; <br>
&gt; Is it possible to configure/run OpenMPI in a such way, that only _one_
<br>
&gt; process (e.g. master) performs real disk I/O, and other processes
sends <br>
&gt; the data to the master which works as an agent?<br>
&gt; <br>
&gt; Of course this would impacts the performance, because all data must
be <br>
&gt; send over network, and the master may became a bottleneck. But is
such <br>
&gt; scenario - IO of all processes bundled to one &nbsp;process - practicable
at all?<br>
&gt; <br>
&gt; <br>
&gt; Best wishes<br>
&gt; Paul<br>
&gt; <br>
&gt; <br>
&gt; <br>
&gt; _______________________________________________<br>
&gt; users mailing list<br>
&gt; users@open-mpi.org<br>
&gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
<br>
_______________________________________________<br>
users mailing list<br>
users@open-mpi.org<br>
</font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/users</font></tt></a><tt><font size=2><br>
</font></tt>
<br>
<br>
