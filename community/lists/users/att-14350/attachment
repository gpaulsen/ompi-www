<table cellspacing="0" cellpadding="0" border="0" ><tr><td valign="top" style="font: inherit;">I have have&nbsp;successfully&nbsp;used a perl program to start mpirun and record its PID<div>The monitor can then watch the output from MPI and terminate the mpirun command with a series of kills or something if it is having trouble.<br><br></div><div>One method of doing this is to prefix all legal output from your MPI program with a known short string, if the monitor does not see this string prefixed on a line, it can terminate MPI, check available nodes and recast the job&nbsp;accordingly</div><div><br></div><div>Hope this helps,</div><div>Randolph</div><div><br>--- On <b>Fri, 24/9/10, Joshua Hursey <i>&lt;jjhursey@open-mpi.org&gt;</i></b> wrote:<br><blockquote style="border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;"><br>From: Joshua Hursey &lt;jjhursey@open-mpi.org&gt;<br>Subject: Re: [OMPI users] Running on crashing
 nodes<br>To: "Open MPI Users" &lt;users@open-mpi.org&gt;<br>Received: Friday, 24 September, 2010, 10:18 PM<br><br><div class="plainMail">As one of the Open MPI developers actively working on the MPI layer stabilization/recover feature set, I don't think we can give you a specific timeframe for availability, especially availability in a stable release. Once the initial functionality is finished, we will open it up for user testing by making a public branch available. After addressing the concerns highlighted by public testing, we will attempt to work this feature into the mainline trunk and eventual release.<br><br>Unfortunately it is difficult to assess the time needed to go through these development stages. What I can tell you is that the work to this point on the MPI layer is looking promising, and that as soon as we feel that the code is ready we will make it available to the public for further testing.<br><br>-- Josh<br><br>On Sep 24, 2010, at 3:37
 AM, Andrei Fokau wrote:<br><br>&gt; Ralph, could you tell us when this functionality will be available in the stable version? A rough estimate will be fine.<br>&gt; <br>&gt; <br>&gt; On Fri, Sep 24, 2010 at 01:24, Ralph Castain &lt;<a ymailto="mailto:rhc@open-mpi.org" href="/mc/compose?to=rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>&gt; In a word, no. If a node crashes, OMPI will abort the currently-running job if it had processes on that node. There is no current ability to "ride-thru" such an event.<br>&gt; <br>&gt; That said, there is work being done to support "ride-thru". Most of that is in the current developer's code trunk, and more is coming, but I wouldn't consider it production-quality just yet.<br>&gt; <br>&gt; Specifically, the code that does what you specify below is done and works. It is recovery of the MPI job itself (collectives, lost messages, etc.) that remains to be completed.<br>&gt; <br>&gt; <br>&gt; On Thu, Sep 23, 2010 at
 7:22 AM, Andrei Fokau &lt;<a ymailto="mailto:andrei.fokau@neutron.kth.se" href="/mc/compose?to=andrei.fokau@neutron.kth.se">andrei.fokau@neutron.kth.se</a>&gt; wrote:<br>&gt; Dear users,<br>&gt; <br>&gt; Our cluster has a number of nodes which have high probability to crash, so it happens quite often that calculations stop due to one node getting down. May be you know if it is possible to block the crashed nodes during run-time when running with OpenMPI? I am asking about principal possibility to program such behavior. Does OpenMPI allow such dynamic checking? The scheme I am curious about is the following:<br>&gt; <br>&gt; 1. A code starts its tasks via mpirun on several nodes<br>&gt; 2. At some moment one node gets down<br>&gt; 3. The code realizes that the node is down (the results are lost) and excludes it from the list of nodes to run its tasks on<br>&gt; 4. At later moment the user restarts the crashed node<br>&gt; 5. The code notices that the
 node is up again, and puts it back to the list of active nodes<br>&gt; <br>&gt; <br>&gt; Regards,<br>&gt; Andrei<br>&gt; <br>&gt; <br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a ymailto="mailto:users@open-mpi.org" href="/mc/compose?to=users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; <br>&gt; <br>&gt; _______________________________________________<br>&gt; users mailing list<br>&gt; <a ymailto="mailto:users@open-mpi.org" href="/mc/compose?to=users@open-mpi.org">users@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>&gt; <br>&gt; &lt;ATT00001..txt&gt;<br><br>------------------------------------<br>Joshua Hursey<br>Postdoctoral Research Associate<br>Oak Ridge
 National Laboratory<br><a href="http://www.cs.indiana.edu/~jjhursey" target="_blank">http://www.cs.indiana.edu/~jjhursey</a><br><br><br>_______________________________________________<br>users mailing list<br><a ymailto="mailto:users@open-mpi.org" href="/mc/compose?to=users@open-mpi.org">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></div></blockquote></div></td></tr></table><br>



      &nbsp;
