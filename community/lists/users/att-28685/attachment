Gary,<div><br></div><div>I previously missed the fact you are running on a 10GbE network, and I still assumes you are not running a debug build.</div><div><br></div><div>maybe you need to increase send/recv buffer sizes</div><div>ompi_info --all | grep btl_tcp</div><div>will list the parameters that can be tuned,</div><div>then you can</div><div>mpirun --mca btl_tcp_&lt;name&gt; &lt;value&gt;</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<br><div><br>On Friday, March 11, 2016, Jackson, Gary L. &lt;<a href="mailto:Gary.Jackson@jhuapl.edu">Gary.Jackson@jhuapl.edu</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">



<div style="word-wrap:break-word;color:rgb(0,0,0);font-size:14px;font-family:Calibri,sans-serif">
<div>
<div>I re-ran all experiments with 1.10.2 configured the way you specified. My results are here:</div>
<div><br>
</div>
<div><a href="https://www.dropbox.com/s/4v4jaxe8sflgymj/collected.pdf?dl=0" target="_blank">https://www.dropbox.com/s/4v4jaxe8sflgymj/collected.pdf?dl=0</a></div>
<div><br>
</div>
<div>Some remarks:</div>
<ol>
<li>OpenMPI had poor performance relative to raw TCP and IMPI across all MTUs.</li><li>Those issues appeared at larger message sizes.</li><li>Intel MPI and raw TCP were comparable across message sizes and MTUs.</li></ol>
<div>
<div>With respect to some other concerns:</div>
<ol>
<li>I verified that the MTU values I&#39;m using are correct with tracepath.</li><li>I am using a placement group.</li></ol>
<div>-- </div>
<div>Gary Jackson</div>
</div>
</div>
<div><br>
</div>
<span>
<div style="font-family:Calibri;font-size:11pt;text-align:left;color:black;BORDER-BOTTOM:medium none;BORDER-LEFT:medium none;PADDING-BOTTOM:0in;PADDING-LEFT:0in;PADDING-RIGHT:0in;BORDER-TOP:#b5c4df 1pt solid;BORDER-RIGHT:medium none;PADDING-TOP:3pt">
<span style="font-weight:bold">From: </span>users &lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;users-bounces@open-mpi.org&#39;);" target="_blank">users-bounces@open-mpi.org</a>&gt; on behalf of Gilles Gouaillardet &lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;gilles@rist.or.jp&#39;);" target="_blank">gilles@rist.or.jp</a>&gt;<br>
<span style="font-weight:bold">Reply-To: </span>Open MPI Users &lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;users@open-mpi.org&#39;);" target="_blank">users@open-mpi.org</a>&gt;<br>
<span style="font-weight:bold">Date: </span>Tuesday, March 8, 2016 at 11:07 PM<br>
<span style="font-weight:bold">To: </span>Open MPI Users &lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;users@open-mpi.org&#39;);" target="_blank">users@open-mpi.org</a>&gt;<br>
<span style="font-weight:bold">Subject: </span>Re: [OMPI users] Poor performance on Amazon EC2 with TCP<br>
</div>
<div><br>
</div>
<div>
<div bgcolor="#FFFFFF" text="#000000">Jackson,<br>
<br>
one more thing, how did you build openmpi ?<br>
<br>
if you built from git (and without VPATH), then --enable-debug is automatically set, and this is hurting performance.<br>
if not already done, i recommend you download the latest openmpi tarball (1.10.2) and<br>
./configure --with-platform=contrib/platform/optimized --prefix=...<br>
last but not least, you can<br>
mpirun --mca mpi_leave_pinned 1 &lt;your benchmark&gt;<br>
(that being said, i am not sure this is useful with TCP networks ...)<br>
<br>
Cheers,<br>
<br>
Gilles<br>
<br>
<br>
<br>
<div>On 3/9/2016 11:34 AM, Rayson Ho wrote:<br>
</div>
<blockquote type="cite">
<div dir="ltr">If you are using instance types that support SR-IOV (aka. &quot;enhanced networking&quot; in AWS), then turn it on. We saw huge differences when SR-IOV is enabled
<div>
<div class="gmail_extra"><br>
</div>
<div class="gmail_extra"><a href="http://blogs.scalablelogic.com/2013/12/enhanced-networking-in-aws-cloud.html" target="_blank">http://blogs.scalablelogic.com/2013/12/enhanced-networking-in-aws-cloud.html</a><br>
</div>
<div class="gmail_extra"><a href="http://blogs.scalablelogic.com/2014/01/enhanced-networking-in-aws-cloud-part-2.html" target="_blank">http://blogs.scalablelogic.com/2014/01/enhanced-networking-in-aws-cloud-part-2.html</a><br>
</div>
<div class="gmail_extra"><br>
</div>
<div class="gmail_extra">Make sure you start your instances with a placement group -- otherwise, the instances can be data centers apart!<br>
</div>
<div class="gmail_extra"><br>
</div>
<div class="gmail_extra">And check that jumbo frames are enabled properly:</div>
<div class="gmail_extra"><br>
</div>
<div class="gmail_extra"><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/network_mtu.html" target="_blank">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/network_mtu.html</a></div>
<div class="gmail_extra"><br>
</div>
<div class="gmail_extra">But still, it is interesting that Intel MPI is getting a 2X speedup with the same setup! Can you post the raw numbers so that we can take a deeper look??</div>
<div class="gmail_extra"><br clear="all">
<div>
<div>Rayson<br>
<br>
==================================================<br>
Open Grid Scheduler - The Official Open Source Grid Engine<br>
<a href="http://gridscheduler.sourceforge.net/" target="_blank">http://gridscheduler.sourceforge.net/</a><br>
<a href="http://gridscheduler.sourceforge.net/GridEngine/GridEngineCloud.html" target="_blank">http://gridscheduler.sourceforge.net/GridEngine/GridEngineCloud.html</a></div>
</div>
<div class="gmail_extra"><br>
</div>
<div class="gmail_extra"><br>
</div>
<div class="gmail_extra"><br>
</div>
<br>
<div class="gmail_quote">On Tue, Mar 8, 2016 at 9:08 AM, Jackson, Gary L. <span dir="ltr">
&lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;Gary.Jackson@jhuapl.edu&#39;);" target="_blank"></a><a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;Gary.Jackson@jhuapl.edu&#39;);" target="_blank">Gary.Jackson@jhuapl.edu</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">
<div style="word-wrap:break-word;color:rgb(0,0,0);font-size:14px;font-family:Calibri,sans-serif">
<div><br>
</div>
<div>I&#39;ve built OpenMPI 1.10.1 on Amazon EC2. Using NetPIPE, I&#39;m seeing about half the performance for MPI over TCP as I do with raw TCP. Before I start digging in to this more deeply, does anyone know what might cause that?</div>
<div><br>
</div>
<div>For what it&#39;s worth, I see the same issues with MPICH, but I do not see it with Intel MPI.</div>
<span><font color="#888888">
<div><br>
</div>
<div>
<div>-- </div>
<div>Gary Jackson</div>
<div><br>
</div>
</div>
</font></span></div>
<br>
_______________________________________________<br>
users mailing list<br>
<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;users@open-mpi.org&#39;);" target="_blank">users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" rel="noreferrer" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/03/28659.php" rel="noreferrer" target="_blank">
http://www.open-mpi.org/community/lists/users/2016/03/28659.php</a><br>
</blockquote>
</div>
<br>
</div>
</div>
</div>
<br>
<fieldset></fieldset> <br>
<pre>_______________________________________________
users mailing list
<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;users@open-mpi.org&#39;);" target="_blank">users@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/03/28665.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/03/28665.php</a></pre>
</blockquote>
<br>
</div>
</div>
</span>
</div>

</blockquote></div></div>

