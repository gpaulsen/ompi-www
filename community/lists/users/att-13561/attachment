<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">Are there multiple interfaces on your nodes? I'm wondering if we are using a different network than the one where you opened these ports.<div><br></div><div>You'll get quite a bit of output, but you can turn on debug output in the oob itself with -mca oob_tcp_verbose xx. The higher the number, the more you get.</div><div><br></div><div><br><div><div>On Jul 10, 2010, at 11:14 AM, Robert Walters wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><table cellspacing="0" cellpadding="0" border="0"><tbody><tr><td valign="top" style="font: inherit;">Hello again,<br><br>I believe my administrator has opened the ports I requested. The problem I am having now is that OpenMPI is not listening to my defined port assignments in openmpi-mca-params.conf (looks like permission 644 on those files should it be 755?)<br><br>When I perform netstat -ltnup I see that orted is listening 14 processes in tcp but scaterred in the 26000ish port range when I specified 60001-60016 in the mca-params file. Is there a parameter I am missing? In any case I am still hanging as mentioned originally even with the port forwarding enabled and specifications in mca-param enabled. <br><br>Any other ideas on what might be causing the hang? Is there a more verbose mode I can employ to see more deeply into the issue? I have run --debug-daemons and --mca plm_base_verbose 99.<br><br>Thanks!<br>--- On <b>Tue, 7/6/10, Robert Walters
 <i>&lt;<a href="mailto:raw19896@yahoo.com">raw19896@yahoo.com</a>&gt;</i></b> wrote:<br><blockquote style="border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;"><br>From: Robert Walters &lt;<a href="mailto:raw19896@yahoo.com">raw19896@yahoo.com</a>&gt;<br>Subject: Re: [OMPI users] OpenMPI Hangs, No Error<br>To: "Open MPI Users" &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Date: Tuesday, July 6, 2010, 5:41 PM<br><br><div id="yiv1356572678"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td style="font: inherit;" valign="top">Thanks for your expeditious responses, Ralph.<br><br>Just to confirm with you, I should change openmpi-mca-params.conf to include:<br><br>oob_tcp_port_min_v4 = (My minimum port in the range)<br>oob_tcp_port_range_v4 = (My port range)<br>btl_tcp_port_min_v4 = (My minimum port in the range)<br>btl_tcp_port_range_v4 = (My port range)<br><br>correct?<br><br>Also, for a cluster of around 32-64 processes (8 processors per node), how wide of a range will I require? I've noticed some entries in
 the mailing list suggesting you need a few to get started and then it opens as necessary. Will I be safe with 20 or should I go for 100? <br><br>Thanks again for all of your help!<br><br>--- On <b>Tue, 7/6/10, Ralph Castain <i>&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;</i></b> wrote:<br><blockquote style="border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;"><br>From:
 Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;<br>Subject: Re: [OMPI users] OpenMPI Hangs, No Error<br>To: "Open MPI Users" &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br>Date: Tuesday, July 6, 2010, 5:31 PM<br><br><div id="yiv795991271">Problem isn't with ssh - the problem is that the daemons need to open a TCP connection back to the machine where mpirun is running. If the firewall blocks that connection, then we can't run.<div><br></div><div>If you can get a range of ports opened, then you can specify the ports OMPI should use for this purpose. If the sysadmin won't allow even that, then you are pretty well hosed.</div><div><br></div><div><br><div><div>On Jul 6, 2010, at 2:23 PM, Robert Walters wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td style="font: inherit;" valign="top">Yes, there is a system firewall. I don't think the sysadmin will allow it to go disabled. Each Linux machine
 has the built-in RHEL firewall. SSH is enabled through the firewall though.<br><br>--- On <b>Tue, 7/6/10, Ralph Castain <i>&lt;<a rel="nofollow">rhc@open-mpi.org</a>&gt;</i></b> wrote:<br><blockquote style="border-left: 2px solid rgb(16, 16, 255); margin-left: 5px; padding-left: 5px;"><br>From: Ralph Castain &lt;<a rel="nofollow">rhc@open-mpi.org</a>&gt;<br>Subject: Re: [OMPI users] OpenMPI Hangs, No Error<br>To: "Open MPI Users" &lt;<a rel="nofollow">users@open-mpi.org</a>&gt;<br>Date: Tuesday, July 6, 2010, 4:19 PM<br><br><div id="yiv1514504972">It looks like the remote daemon is starting - is there a firewall in the way?<div><br><div><div>On Jul 6, 2010, at 2:04 PM, Robert Walters
 wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td style="font: inherit;" valign="top"><div id="yiv62228572">Hello all,<br><br>I am using OpenMPI 1.4.2 on RHEL. I have a cluster of AMD Opteron's and right now I am just working on getting OpenMPI itself up and running. I have a successful configure and make all install. LD_LIBRARY_PATH and PATH variables were correctly edited. mpirun -np 8 hello_c successfully works on all machines. I have setup my two test machines with DSA key pairs that successfully work with each other.<br><br>The problem comes when I initiate my hostfile to attempt to communicate across machines. The hostfile is setup correctly with &lt;host_name&gt; &lt;slots&gt; &lt;max-slots&gt;. When running with all verbose options enabled "mpirun --mca plm_base_verbose 99 --debug-daemons --mca btl_base_verbose 30 --mca oob_base_verbose 99 --mca
 pml_base_verbose 99 -hostfile hostfile -np 16 hello_c" I receive the following text
 output.<br><br>[machine1:03578] mca: base: components_open: Looking for plm components<br>[machine1:03578] mca: base: components_open: opening plm components<br>[machine1:03578] mca: base: components_open: found loaded component rsh<br>[machine1:03578] mca: base: components_open: component rsh has no register function<br>[machine1:03578] mca: base: components_open: component rsh open function successful<br>[machine1:03578] mca: base: components_open: found loaded component slurm<br>[machine1:03578] mca: base: components_open: component slurm has no register function<br>[machine1:03578] mca: base: components_open: component slurm open function successful<br>[machine1:03578] mca:base:select: Auto-selecting plm components<br>[machine1:03578] mca:base:select:(&nbsp; plm) Querying component [rsh]<br>[machine1:03578] mca:base:select:(&nbsp; plm) Query of component [rsh] set priority to 10<br>[machine1:03578] mca:base:select:(&nbsp; plm) Querying component
 [slurm]<br>[machine1:03578] mca:base:select:(&nbsp; plm) Skipping component [slurm]. Query failed to return a module<br>[machine1:03578] mca:base:select:(&nbsp; plm) Selected component [rsh]<br>[machine1:03578] mca: base: close: component slurm closed<br>[machine1:03578] mca: base: close: unloading component slurm<br>[machine1:03578] mca: base: components_open: Looking for oob components<br>[machine1:03578] mca: base: components_open: opening oob components<br>[machine1:03578] mca: base: components_open: found loaded component tcp<br>[machine1:03578] mca: base: components_open: component tcp has no register function<br>[machine1:03578] mca: base: components_open: component tcp open function successful<br>Daemon was launched on machine2- beginning to initialize<br>[machine2:01962] mca: base: components_open: Looking for oob components<br>[machine2:01962] mca: base: components_open: opening oob components<br>[machine2:01962] mca: base: components_open:
 found loaded component tcp<br>[machine2:01962] mca: base: components_open: component tcp has no register function<br>[machine2:01962] mca: base: components_open: component tcp open function successful<br>Daemon [[1418,0],1] checking in as pid 1962 on host machine2<br>Daemon [[1418,0],1] not using static ports<br><br>At this point the system hangs indefinitely. While running top on the machine2 terminal, I see several things come up briefly. These items are: sshd (root), tcsh (myuser), orted (myuser), and mcstransd (root). I was wondering if sshd needs to be initiated by myuser? It is currently turned off in sshd_config through UsePAM yes. This was setup by the sysadmin but it can be worked around if this is necessary.<br><br>So in summary, mpirun works on each machine individually, but hangs when initiated through a hostfile or with the -host flag. ./configure with defaults and --prefix. LD_LIBRARY_PATH and PATH set up correctly. Any help is
 appreciated. Thanks!<br><br></div></td></tr></tbody></table><br>

      _______________________________________________<br>users mailing list<br><a rel="nofollow">users@open-mpi.org</a><br><a rel="nofollow" target="_blank" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote></div><br></div></div><br>-----Inline Attachment Follows-----<br><br><div class="plainMail">_______________________________________________<br>users mailing list<br><a rel="nofollow">users@open-mpi.org</a><br><a rel="nofollow" target="_blank" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></blockquote></td></tr></tbody></table><br>







      _______________________________________________<br>users mailing list<br><a rel="nofollow">users@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></blockquote></div><br></div></div><br>-----Inline Attachment Follows-----<br><br><div class="plainMail">_______________________________________________<br>users mailing list<br><a rel="nofollow">users@open-mpi.org</a><br><a rel="nofollow" target="_blank" href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a></div></blockquote></td></tr></tbody></table><br>

      </div></blockquote></td></tr></tbody></table><br>

      _______________________________________________<br>users mailing list<br><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/users</blockquote></div><br></div></body></html>
