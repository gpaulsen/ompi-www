#======================================================================
# ARM sample configuration
#======================================================================

[MTT]
# JMS Do you need any proxies?
#http_proxy = http://proxy-sjc-2.cisco.com:8080
#ftp_proxy = http://proxy-sjc-2.cisco.com:8080
#https_proxy = http://proxy-sjc-2.cisco.com:8080

description = ARM MPI testing machines

min_disk_free = 2%
min_disk_free_wait = 30

# JMS Turn this to 0 when you're ready
trial = 1

#======================================================================
# MPI get phase
#======================================================================

# JMS I included testing for the SVN trunk and v1.5 SVN branch

[MPI get: ompi-nightly-trunk]
mpi_details = ompi-general

module = OMPI_Snapshot
ompi_snapshot_url = http://www.open-mpi.org/nightly/trunk
ompi_snapshot_version_file = &getenv("HOME")/mtt-versions/trunk&getenv("MTT_VERSION_FILE_SUFFIX").txt

#----------------------------------------------------------------------

[MPI get: ompi-nightly-v1.5]
mpi_details = ompi-general

module = OMPI_Snapshot
ompi_snapshot_url = http://www.open-mpi.org/nightly/v1.5
ompi_snapshot_version_file = &getenv("HOME")/mtt-versions/v1.5&getenv("MTT_VERSION_FILE_SUFFIX").txt

#======================================================================
# Install MPI phase
#======================================================================

# JMS Just gcc -- set whatever configure flags you need.

[MPI install: GNU-standard]
mpi_get = all
save_stdout_on_success = 1
merge_stdout_stderr = 0

module = OMPI
ompi_vpath_mode = none
ompi_make_all_arguments = -j 4
ompi_make_check = 1
ompi_compiler_name = gnu
ompi_compiler_version = &get_gcc_version()
ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default

#======================================================================
# MPI run details
#======================================================================

[MPI Details: ompi-general]

exec = mpirun --leave-session-attached -np &test_np() --mca orte_startup_timeout 10000 @mca@ &test_executable() &test_argv()

parameters = &MPI::OMPI::find_mpirun_params(&test_command_line(), \
                                            &test_executable())
network = &MPI::OMPI::find_network(&test_command_line(), &test_executable())

mca = &enumerate ( \
        "--mca btl sm,tcp,self", \
        "--mca btl tcp,self")

#======================================================================
# Test get phase
#======================================================================

# JMS I suggest commenting out all Test Get sections except Trivial to
# get started.  E.g., [SKIP Test Get: ibm], and so on.

[Test get: trivial]
module = Trivial

#----------------------------------------------------------------------

[Test get: intel]
module = SCM
scm_module = SVN
scm_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/intel_tests

#----------------------------------------------------------------------

[Test get: ibm]
module = SCM
scm_module = SVN
scm_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/ibm
scm_post_copy = <<EOT
./autogen.sh
EOT

#----------------------------------------------------------------------

[Test get: onesided]
module = SCM
scm_module = SVN
scm_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/onesided
scm_post_copy = <<EOT
./autogen.sh
EOT

#----------------------------------------------------------------------

[Test get: mpicxx]
module = SCM
scm_module = SVN
scm_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/cxx-test-suite
scm_post_copy = <<EOT
./autogen.sh
EOT

#----------------------------------------------------------------------

[Test get: imb]
module = SCM
scm_module = SVN
scm_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/IMB_3.2

#----------------------------------------------------------------------

[Test get: netpipe]
module = SCM
scm_module = SVN
scm_url = https://svn.open-mpi.org/svn/ompi-tests/trunk/NetPIPE_3.6.2

#======================================================================
# Test build phase
#======================================================================

[Test build: trivial]
test_get = trivial
save_stdout_on_success = 1
merge_stdout_stderr = 1

module = Trivial

#----------------------------------------------------------------------

[Test build: intel]
test_get = intel
save_stdout_on_success = 1
merge_stdout_stderr = 1

module = Intel_OMPI_Tests
intel_ompi_tests_buildfile = all_tests_no_perf

#----------------------------------------------------------------------

[Test build: ibm]
test_get = ibm
save_stdout_on_success = 1
merge_stdout_stderr = 1

module = Shell
shell_build_command = <<EOT
./configure
make
EOT

#----------------------------------------------------------------------

[Test build: onesided]
test_get = onesided
save_stdout_on_success = 1
merge_stdout_stderr = 1
stderr_save_lines = 100

module = Shell
shell_build_command = <<EOT
./configure
make
EOT

#----------------------------------------------------------------------

[Test build: mpicxx]
test_get = mpicxx
save_stdout_on_success = 1
merge_stdout_stderr = 1

module = Shell
shell_build_command = <<EOT
./configure CC=mpicc CXX=mpic++
make
EOT

#----------------------------------------------------------------------

[Test build: imb]
test_get = imb
save_stdout_on_success = 1
merge_stdout_stderr = 1

module = Shell
shell_build_command = <<EOT
cd src
make clean IMB-MPI1 IMB-EXT
EOT

#======================================================================
# Test Run phase
#======================================================================

# This section is not used directly; it is included in others.
[Defaults Test run]
pass = &and(&test_wifexited(), &eq(&test_wexitstatus(), 0))
skipped = &and(&test_wifexited(), &eq(&test_wexitstatus(), 77))

save_stdout_on_pass = 1
merge_stdout_stderr = 1
stdout_save_lines = 100
stderr_save_lines = 100
report_after_n_results = 100 

np = &env_max_procs()

#----------------------------------------------------------------------

[Test run: trivial]
include_section = Defaults Test run

test_build = trivial
timeout = &max(10, &test_np())
skipped = 0

specify_module = Simple
simple_first:tests = &find_executables(".")

#----------------------------------------------------------------------

[Test run: intel]
include_section = Defaults Test run

test_build = intel
timeout = &max(30, &multiply(10, &test_np()))
np = &min("60", &env_max_procs())

specify_module = Simple
simple_successful:tests = &find_executables("src")

simple_failures:tests = &find_executables(&prepend("src/", &cat("supposed_to_fail")))
simple_failures:pass = &and(&test_wifexited(), &ne(&test_wexitstatus(), 0))
simple_failures:exclusive = 1
simple_failures:timeout = &env_max_procs()

# These tests sleep for 90 seconds (!) before attempting to process
# any messages
simple_really_slow:tests = src/MPI_Isend_flood_c src/MPI_Send_flood_c
simple_really_slow:pass = &and(&test_wifexited(), &eq(&test_wexitstatus(), 0))
simple_really_slow:exclusive = 1
simple_really_slow:timeout = &sum(120, &multiply(5, &test_np()))

# This test collectively sleeps for 26 seconds *per MCW rank*
simple_coll_slow:tests = src/MPI_collective_overlap_c
simple_coll_slow:pass = &and(&test_wifexited(), &eq(&test_wexitstatus(), 0))
simple_coll_slow:exclusive = 1
simple_coll_slow:timeout = &multiply(35, &test_np()))

#------------------------------------------------------------------------

[Test run: ibm]
include_section = Defaults Test run

test_build = ibm
timeout = &max(30, &multiply(10, &test_np()))

specify_module = Simple
simple_first:np = &env_max_procs()
simple_first:tests = &find_executables("collective", "communicator", \
                                       "datatype", "dynamic", "environment", \
                                       "group", "info", "io", "onesided", \
                                       "pt2pt", "random", "topology")

# Tests that are supposed to fail
simple_fail:tests = environment/abort environment/final
simple_fail:pass = &and(&test_wifexited(), &ne(&test_wexitstatus(), 0))
simple_fail:exclusive = 1
simple_fail:timeout = &env_max_procs()

# Spawn tests; need to run very few
simple_spawns:tests  = dynamic/spawn dynamic/spawn_multiple
simple_spawns:np = 3
simple_spawns:pass = &and(&test_wifexited(), &eq(&test_wexitstatus(),0))
simple_spawns:exclusive = 1
simple_spawns:timeout = &multiply(5,&env_max_procs())

# Big loop o' spawns
loop_spawn:tests = dynamic/loop_spawn
loop_spawn:np = 1
loop_spawn:pass = &and(&test_wifexited(), &eq(&test_wexitstatus(),0))
loop_spawn:exclusive = 1
loop_spawn:timeout = 600

# THREAD_MULTIPLE test will fail with the openib btl because it
# deactivates itself in the presence of THREAD_MULTIPLE.  So just skip
# it.  loop_child is the target for loop_spawn, so we don't need to
# run it (although it'll safely pass if you run it by itself).
simple_skip:tests = environment/init_thread_multiple dynamic/loop_child
simple_skip:exclusive = 1
simple_skip:do_not_run = 1

#----------------------------------------------------------------------

[Test run: onesided]
include_section = Defaults Test run

test_build = onesided
timeout = &max(30, &multiply(10, &test_np()))

specify_module = Simple
simple_pass:tests = &cat("run_list")

# This test is killing my nodes for some reason
simple_skip:tests = test_dan1
simple_skip:exclusive = 1
simple_skip:do_not_run = 1

#----------------------------------------------------------------------
     
[Test run: mpicxx]
include_section = Defaults Test run

test_build = mpicxx
timeout = &max(30, &multiply(10, &test_np()))

specify_module = Simple
simple_pass:tests = src/mpi2c++_test src/mpi2c++_dynamics_test

#----------------------------------------------------------------------

[Test run: imb-correctness]
include_section = Defaults Test run

test_build = imb
timeout = &max(2800, &multiply(50, &test_np()))
np = &min("32", &env_max_procs())

specify_module = Simple
simple_pass:tests = src/IMB-MPI1 src/IMB-EXT

#======================================================================
# Reporter phase
#======================================================================

[Reporter: IU database]
module = MTTDatabase

mttdatabase_realm = OMPI
mttdatabase_username = arm
# JMS fill in your database password here
mttdatabase_password = 
# JMS fill in your platform name here
mttdatabase_platform = ARM cluster
# JMS fill in your hostnames here
mttdatabase_hostname = fill in your hostnames here
mttdatabase_url = https://www.open-mpi.org/mtt/submit/
mttdatabase_debug_filename = mttdb_debug_file
mttdatabase_keep_debug_files = 1
mttdatabase_debug_server = 1

#----------------------------------------------------------------------

[SKIP Reporter: send email]
module = Email
email_to = someone@example.com
email_subject = MPI test results: $phase / $section

#----------------------------------------------------------------------

# This is a backup for while debugging MTT; it also writes results to
# a local text file
[SKIP Reporter: text file backup]
module = TextFile

textfile_filename = arm-$phase-$section-$mpi_name-$mpi_version.txt
textfile_separator = >>>>----------------------------------------------------------<<<<

