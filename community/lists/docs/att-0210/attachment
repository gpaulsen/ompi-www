<html>
  <head>

    <meta http-equiv="content-type" content="text/html; charset=utf-8">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    I have a GPUDirect test-case that appears to work correctly with
    OpenMPI 1.8.2 and CUDA 6.5.<br>
    It uses MPI_BCAST plus MPI_PACK &amp; MPI_UNPACK with GPU-located
    data, programmed in OpenACC with PGI 14.9.<br>
    The OpenMPI FAQ entry<br>
    <blockquote><a
        href="http://www.open-mpi.org/faq/?category=all#mpi-cuda-support"><tt>http://www.open-mpi.org/faq/?category=all#mpi-cuda-support</tt><br>
      </a></blockquote>
    states that<br>
    <blockquote>Here is the list
      of APIs that currently support sending and receiving CUDA device
      memory.
      <p><em>MPI_Send, MPI_Bsend, MPI_Ssend, MPI_Rsend, MPI_Isend,
          MPI_Ibsend, MPI_Issend, MPI_Irsend, MPI_Send_init,
          MPI_Bsend_init, MPI_Ssend_init, MPI_Rsend_init, MPI_Recv,
          MPI_Irecv, MPI_Recv_init, MPI_Sendrecv, MPI_Bcast, MPI_Gather,
          MPI_Gatherv, MPI_Allgather, MPI_Allgatherv, MPI_Alltoall,
          MPI_Alltoallv, MPI_Scatter, MPI_Scatterv</em>
      </p>
    </blockquote>
    implying that the MPI_PACK &amp; MPI_UNPACK aren't supported, but
    the FAQ also only references OpenMPI up to 1.7.4 and CUDA up to 6.0.<br>
    Are these MPI_PACK &amp; MPI_UNPACK supported with the OpenMPI 1.8.*
    releases?<br>
    Thanks,<br>
    <br>
                    Carl Ponder<br>
                    HPC Applications Performance<br>
                    NVIDIA Developer Technology<br>
    <br>
  
<DIV>
<HR>
</DIV>
<DIV>This email message is for the sole use of the intended recipient(s) and may 
contain confidential information.&nbsp; Any unauthorized review, use, disclosure 
or distribution is prohibited.&nbsp; If you are not the intended recipient, 
please contact the sender by reply email and destroy all copies of the original 
message. </DIV>
<DIV>
<HR>
</DIV>
</body>
</html>

