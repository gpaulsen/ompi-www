[ntch-2837:32610] [[11953,1],1] ORTE_ERROR_LOG: Error in file ../../orte/util/nidmap.c at line 106
[ntch-2837:32610] [[11953,1],1] ORTE_ERROR_LOG: Error in file ../../../../../orte/mca/ess/env/ess_env_module.c at line 154
[ntch-2837:32609] [[11953,1],0] ORTE_ERROR_LOG: Error in file ../../orte/util/nidmap.c at line 106
[ntch-2837:32609] [[11953,1],0] ORTE_ERROR_LOG: Error in file ../../../../../orte/mca/ess/env/ess_env_module.c at line 154
*** An error occurred in MPI_Init
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[ntch-2837:32610] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
*** An error occurred in MPI_Init
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[ntch-2837:32609] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  orte_util_nidmap_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  orte_ess_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
forrtl: error (78): process killed (SIGTERM)
Image              PC                Routine            Line        Source             
libopen-pal.so.6   00002B07A45517F0  Unknown               Unknown  Unknown
libopen-pal.so.6   00002B07A45512A7  Unknown               Unknown  Unknown
libopen-pal.so.6   00002B07A4563662  Unknown               Unknown  Unknown
libopen-pal.so.6   00002B07A4562964  Unknown               Unknown  Unknown
libopen-pal.so.6   00002B07A45706A2  Unknown               Unknown  Unknown
libopen-pal.so.6   00002B07A4570BE4  Unknown               Unknown  Unknown
libopen-pal.so.6   00002B07A45709FC  Unknown               Unknown  Unknown
libopen-rte.so.7   00002B07A42C56E2  Unknown               Unknown  Unknown
mca_ess_env.so     00002B07A75C56A3  Unknown               Unknown  Unknown
libopen-rte.so.7   00002B07A429EEFB  Unknown               Unknown  Unknown
libmpi.so.1        00002B07A3379C2C  Unknown               Unknown  Unknown
libmpi.so.1        00002B07A339B75F  Unknown               Unknown  Unknown
libmpi_mpifh.so.2  00002B07A311F967  Unknown               Unknown  Unknown
hw                 0000000000406D4C  Unknown               Unknown  Unknown
hw                 0000000000406CFC  Unknown               Unknown  Unknown
libc.so.6          00002B07A3B01EAD  Unknown               Unknown  Unknown
hw                 0000000000406BD9  Unknown               Unknown  Unknown
forrtl: error (78): process killed (SIGTERM)
Image              PC                Routine            Line        Source             
libpthread.so.0    00002B3B8251A8AD  Unknown               Unknown  Unknown
libopen-rte.so.7   00002B3B82F2DBD3  Unknown               Unknown  Unknown
mca_routed_binomi  00002B3B86812AF9  Unknown               Unknown  Unknown
libopen-rte.so.7   00002B3B82F0A65A  Unknown               Unknown  Unknown
mca_ess_env.so     00002B3B8620A6A3  Unknown               Unknown  Unknown
libopen-rte.so.7   00002B3B82EE3EFB  Unknown               Unknown  Unknown
libmpi.so.1        00002B3B81FBEC2C  Unknown               Unknown  Unknown
libmpi.so.1        00002B3B81FE075F  Unknown               Unknown  Unknown
libmpi_mpifh.so.2  00002B3B81D64967  Unknown               Unknown  Unknown
hw                 0000000000406D4C  Unknown               Unknown  Unknown
hw                 0000000000406CFC  Unknown               Unknown  Unknown
libc.so.6          00002B3B82746EAD  Unknown               Unknown  Unknown
hw                 0000000000406BD9  Unknown               Unknown  Unknown
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[11953,1],0]
  Exit code:    1
--------------------------------------------------------------------------