Dear Jeff, <div><br></div><div>I don&#39;t think is a simply out of memory since NUMA node has 48 GB, and I&#39;m allocating just 8 GB.<br><br><div class="gmail_quote">2012/9/5 Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span><br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Perhaps you simply have run out of memory on that NUMA node, and therefore the malloc failed.  Check &quot;numactl --hardware&quot;, for example.<br>

<br>
You might want to check the output of numastat to see if one or more of your NUMA nodes have run out of memory.<br>
<div><div class="h5"><br>
<br>
On Sep 5, 2012, at 12:58 PM, Gabriele Fatigati wrote:<br>
<br>
&gt; I&#39;ve reproduced the problem in a small MPI + OpenMP code.<br>
&gt;<br>
&gt; The error is the same: after some memory bind, gives &quot;Cannot allocate memory&quot;.<br>
&gt;<br>
&gt; Thanks.<br>
&gt;<br>
&gt; 2012/9/5 Gabriele Fatigati &lt;<a href="mailto:g.fatigati@cineca.it">g.fatigati@cineca.it</a>&gt;<br>
&gt; Downscaling the matrix size, binding works well, but the memory available is enought also using more big matrix, so I&#39;m a bit confused.<br>
&gt;<br>
&gt; Using the same big matrix size without binding the code works well, so how I can explain this behaviour?<br>
&gt;<br>
&gt; Maybe hwloc_set_area_membind_nodeset introduces other extra allocation that are resilient after the call?<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; 2012/9/5 Brice Goglin &lt;<a href="mailto:Brice.Goglin@inria.fr">Brice.Goglin@inria.fr</a>&gt;<br>
&gt; An internal malloc failed then. That would explain why your malloc failed too.<br>
&gt; It looks like you malloc&#39;ed too much memory in your program?<br>
&gt;<br>
&gt; Brice<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; Le 05/09/2012 15:56, Gabriele Fatigati a écrit :<br>
&gt;&gt; An update:<br>
&gt;&gt;<br>
&gt;&gt; placing strerror(errno) after hwloc_set_area_membind_nodeset  gives: &quot;Cannot allocate memory&quot;<br>
&gt;&gt;<br>
&gt;&gt; 2012/9/5 Gabriele Fatigati &lt;<a href="mailto:g.fatigati@cineca.it">g.fatigati@cineca.it</a>&gt;<br>
&gt;&gt; Hi,<br>
&gt;&gt;<br>
&gt;&gt; I&#39;ve noted that hwloc_set_area_membind_nodeset return -1 but errno is not equal to EXDEV or ENOSYS. I supposed that these two case was the two unique possibly.<br>
&gt;&gt;<br>
&gt;&gt; From the hwloc documentation:<br>
&gt;&gt;<br>
&gt;&gt; -1 with errno set to ENOSYS if the action is not supported<br>
&gt;&gt; -1 with errno set to EXDEV if the binding cannot be enforced<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Any other binding failure reason? The memory available is enought.<br>
&gt;&gt;<br>
&gt;&gt; 2012/9/5 Brice Goglin &lt;<a href="mailto:Brice.Goglin@inria.fr">Brice.Goglin@inria.fr</a>&gt;<br>
&gt;&gt; Hello Gabriele,<br>
&gt;&gt;<br>
&gt;&gt; The only limit that I would think of is the available physical memory on each NUMA node (numactl -H will tell you how much of each NUMA node memory is still available).<br>
&gt;&gt; malloc usually only fails (it returns NULL?) when there no *virtual* memory anymore, that&#39;s different. If you don&#39;t allocate tons of terabytes of virtual memory, this shouldn&#39;t happen easily.<br>
&gt;&gt;<br>
&gt;&gt; Brice<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Le 05/09/2012 14:27, Gabriele Fatigati a écrit :<br>
&gt;&gt;&gt; Dear Hwloc users and developers,<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; I&#39;m using hwloc 1.4.1 on a multithreaded program in a Linux platform, where each thread bind many non contiguos pieces of a big matrix using in a very intensive way hwloc_set_area_membind_nodeset function:<br>

&gt;&gt;&gt;<br>
&gt;&gt;&gt; hwloc_set_area_membind_nodeset(topology, punt+offset, len, nodeset, HWLOC_MEMBIND_BIND, HWLOC_MEMBIND_THREAD | HWLOC_MEMBIND_MIGRATE);<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Binding seems works well, since the returned code from function is 0 for every calls.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; The problems is that after binding, a simple little new malloc fails, without any apparent reason.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Disabling memory binding, the allocations works well.  Is there any knows problem if  hwloc_set_area_membind_nodeset is used intensively?<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Is there some operating system limit for memory pages binding?<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Thanks in advance.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; --<br>
&gt;&gt;&gt; Ing. Gabriele Fatigati<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; HPC specialist<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; SuperComputing Applications and Innovation Department<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; <a href="http://www.cineca.it" target="_blank">www.cineca.it</a>                    Tel:   <a href="tel:%2B39%20051%206171722" value="+390516171722">+39 051 6171722</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; g.fatigati [AT] <a href="http://cineca.it" target="_blank">cineca.it</a><br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; hwloc-users mailing list<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; <a href="mailto:hwloc-users@open-mpi.org">hwloc-users@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; Ing. Gabriele Fatigati<br>
&gt;&gt;<br>
&gt;&gt; HPC specialist<br>
&gt;&gt;<br>
&gt;&gt; SuperComputing Applications and Innovation Department<br>
&gt;&gt;<br>
&gt;&gt; Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>
&gt;&gt;<br>
&gt;&gt; <a href="http://www.cineca.it" target="_blank">www.cineca.it</a>                    Tel:   <a href="tel:%2B39%20051%206171722" value="+390516171722">+39 051 6171722</a><br>
&gt;&gt;<br>
&gt;&gt; g.fatigati [AT] <a href="http://cineca.it" target="_blank">cineca.it</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; Ing. Gabriele Fatigati<br>
&gt;&gt;<br>
&gt;&gt; HPC specialist<br>
&gt;&gt;<br>
&gt;&gt; SuperComputing Applications and Innovation Department<br>
&gt;&gt;<br>
&gt;&gt; Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>
&gt;&gt;<br>
&gt;&gt; <a href="http://www.cineca.it" target="_blank">www.cineca.it</a>                    Tel:   <a href="tel:%2B39%20051%206171722" value="+390516171722">+39 051 6171722</a><br>
&gt;&gt;<br>
&gt;&gt; g.fatigati [AT] <a href="http://cineca.it" target="_blank">cineca.it</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Ing. Gabriele Fatigati<br>
&gt;<br>
&gt; HPC specialist<br>
&gt;<br>
&gt; SuperComputing Applications and Innovation Department<br>
&gt;<br>
&gt; Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>
&gt;<br>
&gt; <a href="http://www.cineca.it" target="_blank">www.cineca.it</a>                    Tel:   <a href="tel:%2B39%20051%206171722" value="+390516171722">+39 051 6171722</a><br>
&gt;<br>
&gt; g.fatigati [AT] <a href="http://cineca.it" target="_blank">cineca.it</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Ing. Gabriele Fatigati<br>
&gt;<br>
&gt; HPC specialist<br>
&gt;<br>
&gt; SuperComputing Applications and Innovation Department<br>
&gt;<br>
&gt; Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>
&gt;<br>
&gt; <a href="http://www.cineca.it" target="_blank">www.cineca.it</a>                    Tel:   <a href="tel:%2B39%20051%206171722" value="+390516171722">+39 051 6171722</a><br>
&gt;<br>
&gt; g.fatigati [AT] <a href="http://cineca.it" target="_blank">cineca.it</a><br>
</div></div>&gt; &lt;main_hybrid_bind_mem.c&gt;_______________________________________________<br>
<div class="im HOEnZb">&gt; hwloc-users mailing list<br>
&gt; <a href="mailto:hwloc-users@open-mpi.org">hwloc-users@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a><br>
<br>
<br>
--<br>
</div><span class="HOEnZb"><font color="#888888">Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
</font></span><div class="HOEnZb"><div class="h5"><br>
<br>
_______________________________________________<br>
hwloc-users mailing list<br>
<a href="mailto:hwloc-users@open-mpi.org">hwloc-users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a><br>
</div></div></blockquote></div><br><br clear="all"><div><br></div>-- <br>Ing. Gabriele Fatigati<br><br>HPC specialist<br><br>SuperComputing Applications and Innovation Department<br><br>Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>
<br><a href="http://www.cineca.it" target="_blank">www.cineca.it</a>                    Tel:   +39 051 6171722<br><br>g.fatigati [AT] <a href="http://cineca.it" target="_blank">cineca.it</a>           <br>
</div>

