<html>
  <head>
    <meta content="text/html; charset=ISO-8859-1"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    Le 06/09/2012 14:51, Gabriele Fatigati a &eacute;crit&nbsp;:
    <blockquote
cite="mid:CAJNPZUWwDoDUDRNkmmY1N57SmXtm=XeMffZCgRfup4kGwr5miQ@mail.gmail.com"
      type="cite">Hi Brice,
      <div><br>
      </div>
      <div>the initial grep is:</div>
      <div><br>
      </div>
      <div>numa_policy &nbsp; &nbsp; &nbsp; &nbsp;65671 &nbsp;65952 &nbsp; &nbsp; 24 &nbsp;144 &nbsp; &nbsp;1 : tunables
        &nbsp;120 &nbsp; 60 &nbsp; &nbsp;8 : slabdata &nbsp; &nbsp;458 &nbsp; &nbsp;458 &nbsp; &nbsp; &nbsp;0</div>
      <div><br>
      </div>
      <div>When set_membind fails is:</div>
      <div><br>
      </div>
      <div>numa_policy &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;482 &nbsp; 1152 &nbsp; &nbsp; 24 &nbsp;144 &nbsp; &nbsp;1 : tunables
        &nbsp;120 &nbsp; 60 &nbsp; &nbsp;8 : slabdata &nbsp; &nbsp; &nbsp;8 &nbsp; &nbsp; &nbsp;8 &nbsp; &nbsp;288</div>
      <div><br>
      </div>
      <div>What does it means?</div>
    </blockquote>
    <br>
    The first number is the number of active objects. That means 65000
    mempolicy objects were in use on the first line.<br>
    (I wonder if you swapped the lines, I expected higher numbers at the
    end of the run)<br>
    <br>
    Anyway, having 65000 mempolicies in use is a lot. And that would
    somehow correspond to the number of set_area_membind that succeeed
    before one fails. So the kernel might indeed fail to merge those.<br>
    <br>
    That said, these objects are small (24bytes here if I am reading
    things correctly), so we're talking about 1,6MB only here. So
    there's still something else eating all the memory. /proc/meminfo
    (MemFree) and numactl -H should again help.<br>
    <br>
    Brice<br>
    <br>
    <br>
    <blockquote
cite="mid:CAJNPZUWwDoDUDRNkmmY1N57SmXtm=XeMffZCgRfup4kGwr5miQ@mail.gmail.com"
      type="cite">
      <div><br>
      </div>
      <div><br>
        <br>
        <div class="gmail_quote">
          2012/9/6 Brice Goglin <span dir="ltr">&lt;<a
              moz-do-not-send="true" href="mailto:Brice.Goglin@inria.fr"
              target="_blank">Brice.Goglin@inria.fr</a>&gt;</span><br>
          <blockquote class="gmail_quote" style="margin:0 0 0
            .8ex;border-left:1px #ccc solid;padding-left:1ex">
            <div bgcolor="#FFFFFF" text="#000000"> Le 06/09/2012 12:19,
              Gabriele Fatigati a &eacute;crit&nbsp;:
              <div class="im">
                <blockquote type="cite">I did't find any strange number
                  in /proc/meminfo.
                  <div><br>
                  </div>
                  <div>I've noted that the program fails exactly
                    every&nbsp;65479&nbsp;hwloc_set_area_membind. So It sounds
                    like some kernel limit. You can check that also just
                    one thread.</div>
                  <div><br>
                  </div>
                  <div>Maybe never has not noted them &nbsp;because usually
                    we bind a large amount of contiguos memory few
                    times, instead of small and non contiguos pieces of
                    memory many and many times.. :(</div>
                </blockquote>
                <br>
              </div>
              If you have root access, try (as root)<br>
              &nbsp;&nbsp;&nbsp; watch -n 1 grep numa_policy /proc/slabinfo<br>
              Put a sleep(10) in your program when set_area_membind()
              fails, and don't let your program exit before you can read
              the content of /proc/slabinfo.<span class="HOEnZb"><font
                  color="#888888"><br>
                  <br>
                  Brice</font></span>
              <div>
                <div class="h5"><br>
                  <br>
                  <br>
                  <br>
                  <blockquote type="cite">
                    <div><br>
                      <div class="gmail_quote"> 2012/9/6 Brice Goglin <span
                          dir="ltr">&lt;<a moz-do-not-send="true"
                            href="mailto:Brice.Goglin@inria.fr"
                            target="_blank">Brice.Goglin@inria.fr</a>&gt;</span><br>
                        <blockquote class="gmail_quote" style="margin:0
                          0 0 .8ex;border-left:1px #ccc
                          solid;padding-left:1ex"> Le 06/09/2012 10:44,
                          Samuel Thibault a &eacute;crit :<br>
                          <div>&gt; Gabriele Fatigati, le Thu 06 Sep
                            2012 10:12:38 +0200, a &eacute;crit :<br>
                            &gt;&gt; mbind
                            hwloc_linux_set_area_membind() &nbsp;fails:<br>
                            &gt;&gt;<br>
                            &gt;&gt; Error from HWLOC mbind: Cannot
                            allocate memory<br>
                            &gt; Ok. mbind is not really supposed to
                            allocate much memory, but it still<br>
                            &gt; does allocate some, to record the
                            policy<br>
                            &gt;<br>
                            &gt;&gt; // &nbsp; &nbsp; &nbsp; &nbsp;hwloc_obj_t obj =
                            hwloc_get_obj_by_type(topology,
                            HWLOC_OBJ_NODE, tid);<br>
                            &gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; hwloc_obj_t obj =
                            hwloc_get_obj_by_type(topology,
                            HWLOC_OBJ_PU, tid);<br>
                            &gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; hwloc_cpuset_t cpuset =
                            hwloc_bitmap_dup(obj-&gt;cpuset);<br>
                            &gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp;
                            hwloc_bitmap_singlify(cpuset);<br>
                            &gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; hwloc_set_cpubind(topology,
                            cpuset, HWLOC_CPUBIND_THREAD);<br>
                            &gt;&gt;<br>
                            &gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; for( i = chunk*tid; i &lt;
                            len; i+=PAGE_SIZE) {<br>
                            &gt;&gt; // &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; res =
                            hwloc_set_area_membind_nodeset(topology,
                            &amp;array[i], PAGE_SIZE, obj-&gt;nodeset,
                            HWLOC_MEMBIND_BIND, HWLOC_MEMBIND_THREAD);<br>
                            &gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;res =
                            hwloc_set_area_membind(topology,
                            &amp;array[i], PAGE_SIZE, cpuset,
                            HWLOC_MEMBIND_BIND, HWLOC_MEMBIND_THREAD);<br>
                            &gt; and I'm afraid that calling
                            set_area_membind for each page might be too<br>
                            &gt; dense: the kernel is probably
                            allocating a memory policy record for each<br>
                            &gt; page, not being able to merge adjacent
                            equal policies.<br>
                            &gt;<br>
                            <br>
                          </div>
                          It's supposed to merge VMA with same policies
                          (from what I understand in<br>
                          the code), but I don't know if that actually
                          works.<br>
                          Maybe Gabriele found a kernel bug :)<br>
                          <span><font color="#888888"><br>
                              Brice<br>
                            </font></span>
                          <div>
                            <div><br>
_______________________________________________<br>
                              hwloc-users mailing list<br>
                              <a moz-do-not-send="true"
                                href="mailto:hwloc-users@open-mpi.org"
                                target="_blank">hwloc-users@open-mpi.org</a><br>
                              <a moz-do-not-send="true"
                                href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users"
                                target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a><br>
                            </div>
                          </div>
                        </blockquote>
                      </div>
                      <br>
                      <br clear="all">
                      <div><br>
                      </div>
                      -- <br>
                      Ing. Gabriele Fatigati<br>
                      <br>
                      HPC specialist<br>
                      <br>
                      SuperComputing Applications and Innovation
                      Department<br>
                      <br>
                      Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>
                      <br>
                      <a moz-do-not-send="true"
                        href="http://www.cineca.it" target="_blank">www.cineca.it</a>&nbsp;
                      &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Tel:&nbsp;&nbsp; <a
                        moz-do-not-send="true"
                        href="tel:%2B39%20051%206171722"
                        value="+390516171722" target="_blank">+39 051
                        6171722</a><br>
                      <br>
                      g.fatigati [AT] <a moz-do-not-send="true"
                        href="http://cineca.it" target="_blank">cineca.it</a>&nbsp;
                      &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <br>
                    </div>
                    <br>
                    <fieldset></fieldset>
                    <br>
                    <pre>_______________________________________________
hwloc-users mailing list
<a moz-do-not-send="true" href="mailto:hwloc-users@open-mpi.org" target="_blank">hwloc-users@open-mpi.org</a>
<a moz-do-not-send="true" href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a></pre>
                  </blockquote>
                  <br>
                </div>
              </div>
            </div>
            <br>
            _______________________________________________<br>
            hwloc-users mailing list<br>
            <a moz-do-not-send="true"
              href="mailto:hwloc-users@open-mpi.org">hwloc-users@open-mpi.org</a><br>
            <a moz-do-not-send="true"
              href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users"
              target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a><br>
          </blockquote>
        </div>
        <br>
        <br clear="all">
        <div><br>
        </div>
        -- <br>
        Ing. Gabriele Fatigati<br>
        <br>
        HPC specialist<br>
        <br>
        SuperComputing Applications and Innovation Department<br>
        <br>
        Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>
        <br>
        <a moz-do-not-send="true" href="http://www.cineca.it"
          target="_blank">www.cineca.it</a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Tel:&nbsp;&nbsp;
        +39 051 6171722<br>
        <br>
        g.fatigati [AT] <a moz-do-not-send="true"
          href="http://cineca.it" target="_blank">cineca.it</a>&nbsp; &nbsp; &nbsp; &nbsp;
        &nbsp;&nbsp; <br>
      </div>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
hwloc-users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:hwloc-users@open-mpi.org">hwloc-users@open-mpi.org</a>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a></pre>
    </blockquote>
    <br>
  </body>
</html>

