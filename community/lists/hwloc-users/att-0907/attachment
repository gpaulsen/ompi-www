<div dir="ltr"><div>Hallo,<br><br><br></div><div>I initially posted this at <a href="mailto:users@open-mpi.org">users@open-mpi.org</a>.<br><br></div>We seem to be unable to to set the cpu binding on a cluster consisting of Dell M420/M610 systems:<br>
<div><br>[jallan@hpc21 ~]$ cat report-bindings.sh #!/bin/sh<br><br>bitmap=`hwloc-bind --get -p`<br>

friendly=`hwloc-calc -p -H socket.core.pu $bitmap`<br><br>echo &quot;MCW rank $OMPI_COMM_WORLD_RANK (`hostname`): $friendly&quot;<br>exit 0 <br><br><br>[jallan@hpc27 ~]$ hwloc-bind -v  socket:0.core:0 -l ./report-bindings.sh <br>


using object #0 depth 2 below cpuset 0x000000ff<br>using object #0 depth 6 below cpuset 0x00000080<br>adding 0x00000080 to 0x0<br>adding 0x00000080 to 0x0<br>assuming the command starts at ./report-bindings.sh<br>binding on cpu set 0x00000080<br>


MCW rank  (hpc27): Socket:0.Core:10.PU:7<br>[jallan@hpc27 ~]$ hwloc-bind -v  socket:1.core:0 -l ./report-bindings.sh <br>object #1 depth 2 (type socket) below cpuset 0x000000ff does not exist<br>adding 0x0 to 0x0<br>assuming the command starts at ./report-bindings.sh<br>


MCW rank  (hpc27): Socket:0.Core:10.PU:7<br><br><br>The topology of this system looks a bit strange:<br><br>[jallan@hpc21 ~]$ lstopo --no-io<br>Machine (24GB)<br> NUMANode L#0 (P#0 24GB)<br> NUMANode L#1 (P#1) + Socket L#0 + L3 L#0 (15MB) + L2 L#0 (256KB) + L1<br>


L#0 (32KB) + Core L#0 + PU L#0 (P#11)<br>[jallan@hpc21 ~]$ <br><br><br>Using Open MPI 1.4.4:<br><br><a href="http://pastebin.com/VsZS2q3R" target="_blank">http://pastebin.com/VsZS2q3R</a><br><br>For some reason the binding cannot be set. We also tried Open MPI 1.6.5 and 1.7.3 with similar results.<br>


<br>This is the output from a local SMP system:<br><br>[panos@demo ~]$ 
hwloc-bind -v  socket:1.core:0 -l ./report-bindings.sh using object #1 
depth 2 below cpuset 0x00000003 using object #0 depth 6 below cpuset 
0x00000002 adding 0x00000002 to 0x0 adding 0x00000002 to 0x0 assuming 
the command starts at ./report-bindings.sh binding on cpu set 0x00000002
 MCW rank  (demo): Socket:1.Core:0.PU:1 [panos@demo ~]$ hwloc-bind -v  
socket:0.core:0 -l ./report-bindings.sh using object #0 depth 2 below 
cpuset 0x00000003 using object #0 depth 6 below cpuset 0x00000001 adding
 0x00000001 to 0x0 adding 0x00000001 to 0x0 assuming the command starts 
at ./report-bindings.sh binding on cpu set 0x00000001 MCW rank  (demo): 
Socket:0.Core:0.PU:0<br>
<br><br>The MPI binding output is formatted a bit different as this nodes runs Open MPI 1.6.5:<br><br>[panos@demo ~]$ `which  mpiexec` --report-bindings --bind-to-core<br>--bycore -mca btl ^openib -np 4   -hostfile ./hplnodes2 -x<br>


LD_LIBRARY_PATH -x PATH    /cm/shared/apps/hpl/2.1/xhpl<br>[demo:25615] 
MCW rank 0 bound to socket 0[core 0]: [B][.] [demo:25615] MCW rank 2 
bound to socket 1[core 0]: [.][B] [node003:08454] MCW rank 1 bound to 
socket 0[core 0]: [B .] [node003:08454] MCW rank 3 bound to socket 
0[core 1]: [. B] [panos@demo ~]$ module load hwloc<br>
<br><br><br>[panos@demo ~]$ lstopo -l<br>Machine (4095MB)<br> NUMANode L#0 (P#0 2048MB) + Socket L#0 + L2 L#0 (1024KB) + L1d L#0<br>(64KB) + L1i L#0 (64KB) + Core L#0 + PU L#0 (P#0)<br> NUMANode L#1 (P#1 2048MB) + Socket L#1 + L2 L#1 (1024KB) + L1d L#1<br>


(64KB) + L1i L#1 (64KB) + Core L#1 + PU L#1 (P#1)<br><br></div><div>Any help will be appreciated.<br><br></div><div>Kind Regards,<br></div>  Panos Labropoulos</div>

