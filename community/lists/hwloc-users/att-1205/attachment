<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    Here's the fixed XML. For the record, for each NUMA node, I extended
    the cpusets of the L3 to match the container NUMA node, and moved
    all L2 objects as children of that L3.<br>
    Now you may load that XML instead of the native discovery by setting
    HWLOC_XMLFILE=leo2.xml in your environment.<br>
    Brice<br>
    <br>
    <br>
    <br>
    <div class="moz-cite-prefix">Le 27/10/2015 10:08, Fabian Wein a
      écrit :<br>
    </div>
    <blockquote cite="mid:562F3F2B.9030408@fau.de" type="cite">Brice,
      <br>
      <br>
      thank you very much for the offer. I attached the xml file
      <br>
      ..
      <br>
      <br>
      * hwloc 1.11.1 has encountered what looks like an error from the
      operating system.
      <br>
      *
      <br>
      * L3 (cpuset 0x000003f0) intersects with NUMANode (P#0 cpuset
      0x0000003f) without inclusion!
      <br>
      * Error occurred in topology.c line 981
      <br>
      *
      <br>
      ..
      <br>
      <br>
      So if you can affort the time, I apprechiate it very much!
      <br>
      <br>
      Fabian
      <br>
      <br>
      <br>
      <br>
      On 10/27/2015 09:52 AM, Brice Goglin wrote:
      <br>
      <blockquote type="cite">Hello
        <br>
        <br>
        This bug is about L3 cache locality only, everything else should
        be
        <br>
        fine, including cache sizes. Few applications use that locality
        <br>
        information, so I assume it doesn't matter for PETSc scaling.
        <br>
        We can work around the bug by loading a XML topology. There's no
        easy
        <br>
        way to build that correct XML, but I can do it manually if you
        send your
        <br>
        current broken topology (lstopo foo.xml and send this foo.xml).
        <br>
        <br>
        Brice
        <br>
        <br>
        <br>
        <br>
        Le 27/10/2015 09:43, Fabian Wein a écrit :
        <br>
        <blockquote type="cite">Hello,
          <br>
          <br>
          I'm new to the list and new to the mpi-business, too.
          <br>
          <br>
          Our 4*12 Opteron 6238 system is very similar to the one from
          the
          <br>
          original poster and I get the same error message.
          <br>
          Any use in posting my logs?
          <br>
          <br>
          I compiled the latest hwloc, no change. our System is Ubunut
          14.4 LTS
          <br>
          with kernel 3.13. and our bios is not updated.
          <br>
          <br>
          The system scales very fine with OpenMP but fails to give any
          <br>
          realistic scaling using PETSc (both for the standard
          <br>
          streaming benchmark and quick tests with a given application).
          <br>
          <br>
          As far as I understand the system is fine, just the
          information
          <br>
          gathering fails, right?!
          <br>
          <br>
          Do you know if the hwloc issue relates with our poor PETSc
          scaling? Is
          <br>
          there a way to configure the topology
          <br>
          manually?
          <br>
          <br>
          To me it appears that an bios update wouldn't help, right?! I
          wouldn't
          <br>
          try it if it is not nesessary. I'm a user with sudo accesss,
          <br>
          not an administrator but we have no admin for the system.
          <br>
          <br>
          Thanks,
          <br>
          <br>
          Fabian
          <br>
          _______________________________________________
          <br>
          hwloc-users mailing list
          <br>
          <a class="moz-txt-link-abbreviated" href="mailto:hwloc-users@open-mpi.org">hwloc-users@open-mpi.org</a>
          <br>
          Subscription:
          <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a>
          <br>
          Link to this post:
          <br>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/hwloc-users/2015/10/1201.php">http://www.open-mpi.org/community/lists/hwloc-users/2015/10/1201.php</a>
          <br>
        </blockquote>
        <br>
        _______________________________________________
        <br>
        hwloc-users mailing list
        <br>
        <a class="moz-txt-link-abbreviated" href="mailto:hwloc-users@open-mpi.org">hwloc-users@open-mpi.org</a>
        <br>
        Subscription:
        <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a>
        <br>
        Link to this post:
        <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/hwloc-users/2015/10/1204.php">http://www.open-mpi.org/community/lists/hwloc-users/2015/10/1204.php</a>
        <br>
        <br>
      </blockquote>
      <br>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
hwloc-users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:hwloc-users@open-mpi.org">hwloc-users@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/hwloc-users/2015/10/1205.php">http://www.open-mpi.org/community/lists/hwloc-users/2015/10/1205.php</a></pre>
    </blockquote>
    <br>
  </body>
</html>

