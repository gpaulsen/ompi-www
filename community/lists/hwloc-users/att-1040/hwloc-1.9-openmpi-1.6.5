[ntch-2837:32575] [[11984,1],0] ORTE_ERROR_LOG: Error in file ../../orte/util/nidmap.c at line 148
[ntch-2837:32575] [[11984,1],0] ORTE_ERROR_LOG: Error in file ../../../../../orte/mca/ess/env/ess_env_module.c at line 174
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  orte_util_nidmap_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
[ntch-2837:32575] [[11984,1],0] ORTE_ERROR_LOG: Error in file ../../orte/runtime/orte_init.c at line 128
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  orte_ess_set_name failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: orte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[ntch-2837:32576] [[11984,1],1] ORTE_ERROR_LOG: Error in file ../../orte/util/nidmap.c at line 148
[ntch-2837:32576] [[11984,1],1] ORTE_ERROR_LOG: Error in file ../../../../../orte/mca/ess/env/ess_env_module.c at line 174
[ntch-2837:32575] *** An error occurred in MPI_Init
[ntch-2837:32575] *** on a NULL communicator
[ntch-2837:32575] *** Unknown error
[ntch-2837:32575] *** MPI_ERRORS_ARE_FATAL: your MPI job will now abort
--------------------------------------------------------------------------
An MPI process is aborting at a time when it cannot guarantee that all
of its peer processes in the job will be killed properly.  You should
double check that everything has shut down cleanly.

  Reason:     Before MPI_INIT completed
  Local host: ntch-2837
  PID:        32575
--------------------------------------------------------------------------
[ntch-2837:32576] [[11984,1],1] ORTE_ERROR_LOG: Error in file ../../orte/runtime/orte_init.c at line 128
[ntch-2837:32577] [[11984,1],2] ORTE_ERROR_LOG: Error in file ../../orte/util/nidmap.c at line 148
[ntch-2837:32577] [[11984,1],2] ORTE_ERROR_LOG: Error in file ../../../../../orte/mca/ess/env/ess_env_module.c at line 174
[ntch-2837:32577] [[11984,1],2] ORTE_ERROR_LOG: Error in file ../../orte/runtime/orte_init.c at line 128
forrtl: error (78): process killed (SIGTERM)
Image              PC                Routine            Line        Source             
libc.so.6          00002B3B601925F0  Unknown               Unknown  Unknown
libc.so.6          00002B3B601BC304  Unknown               Unknown  Unknown
mca_oob_tcp.so     00002B3B63087353  Unknown               Unknown  Unknown
mca_rml_oob.so     00002B3B62E7B8B2  Unknown               Unknown  Unknown
libmpi.so.1        00002B3B5ED56AE9  Unknown               Unknown  Unknown
mca_ess_env.so     00002B3B62C776FE  Unknown               Unknown  Unknown
libmpi.so.1        00002B3B5ED3408B  Unknown               Unknown  Unknown
libmpi.so.1        00002B3B5ECF3C36  Unknown               Unknown  Unknown
libmpi.so.1        00002B3B5ED0C303  Unknown               Unknown  Unknown
libmpi_f77.so.1    00002B3B5EA82CC7  Unknown               Unknown  Unknown
hw                 0000000000406F4C  Unknown               Unknown  Unknown
hw                 0000000000406EFC  Unknown               Unknown  Unknown
libc.so.6          00002B3B60104EAD  Unknown               Unknown  Unknown
hw                 0000000000406DD9  Unknown               Unknown  Unknown
--------------------------------------------------------------------------
mpirun has exited due to process rank 0 with PID 32575 on
node ntch-2837 exiting improperly. There are two reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

This may have caused other processes in the application to be
terminated by signals sent by mpirun (as reported here).
--------------------------------------------------------------------------
[ntch-2837:32574] 2 more processes have sent help message help-orte-runtime.txt / orte_init:startup:internal-failure
[ntch-2837:32574] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[ntch-2837:32574] 2 more processes have sent help message help-orte-runtime / orte_init:startup:internal-failure
[ntch-2837:32574] 2 more processes have sent help message help-mpi-runtime / mpi_init:startup:internal-failure
[ntch-2837:32574] 2 more processes have sent help message help-mpi-errors.txt / mpi_errors_are_fatal unknown handle
[ntch-2837:32574] 2 more processes have sent help message help-mpi-runtime.txt / ompi mpi abort:cannot guarantee all killed
