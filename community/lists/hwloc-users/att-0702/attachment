<html>
  <head>
    <meta content="text/html; charset=ISO-8859-1"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    Le 06/09/2012 12:19, Gabriele Fatigati a &eacute;crit&nbsp;:
    <blockquote
cite="mid:CAJNPZUX1yAZwMr54pqVh_fzDBk=N71wLG1LJ-2LA1cAn-RNWrg@mail.gmail.com"
      type="cite">I did't find any strange number in /proc/meminfo.
      <div><br>
      </div>
      <div>I've noted that the program fails exactly
        every&nbsp;65479&nbsp;hwloc_set_area_membind. So It sounds like some
        kernel limit. You can check that also just one thread.</div>
      <div><br>
      </div>
      <div>Maybe never has not noted them &nbsp;because usually we bind a
        large amount of contiguos memory few times, instead of small and
        non contiguos pieces of memory many and many times.. :(</div>
    </blockquote>
    <br>
    If you have root access, try (as root)<br>
    &nbsp;&nbsp;&nbsp; watch -n 1 grep numa_policy /proc/slabinfo<br>
    Put a sleep(10) in your program when set_area_membind() fails, and
    don't let your program exit before you can read the content of
    /proc/slabinfo.<br>
    <br>
    Brice<br>
    <br>
    <br>
    <br>
    <blockquote
cite="mid:CAJNPZUX1yAZwMr54pqVh_fzDBk=N71wLG1LJ-2LA1cAn-RNWrg@mail.gmail.com"
      type="cite">
      <div><br>
        <div class="gmail_quote">
          2012/9/6 Brice Goglin <span dir="ltr">&lt;<a
              moz-do-not-send="true" href="mailto:Brice.Goglin@inria.fr"
              target="_blank">Brice.Goglin@inria.fr</a>&gt;</span><br>
          <blockquote class="gmail_quote" style="margin:0 0 0
            .8ex;border-left:1px #ccc solid;padding-left:1ex">
            Le 06/09/2012 10:44, Samuel Thibault a &eacute;crit :<br>
            <div class="im">&gt; Gabriele Fatigati, le Thu 06 Sep 2012
              10:12:38 +0200, a &eacute;crit :<br>
              &gt;&gt; mbind hwloc_linux_set_area_membind() &nbsp;fails:<br>
              &gt;&gt;<br>
              &gt;&gt; Error from HWLOC mbind: Cannot allocate memory<br>
              &gt; Ok. mbind is not really supposed to allocate much
              memory, but it still<br>
              &gt; does allocate some, to record the policy<br>
              &gt;<br>
              &gt;&gt; // &nbsp; &nbsp; &nbsp; &nbsp;hwloc_obj_t obj =
              hwloc_get_obj_by_type(topology, HWLOC_OBJ_NODE, tid);<br>
              &gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; hwloc_obj_t obj =
              hwloc_get_obj_by_type(topology, HWLOC_OBJ_PU, tid);<br>
              &gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; hwloc_cpuset_t cpuset =
              hwloc_bitmap_dup(obj-&gt;cpuset);<br>
              &gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; hwloc_bitmap_singlify(cpuset);<br>
              &gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; hwloc_set_cpubind(topology, cpuset,
              HWLOC_CPUBIND_THREAD);<br>
              &gt;&gt;<br>
              &gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; for( i = chunk*tid; i &lt; len;
              i+=PAGE_SIZE) {<br>
              &gt;&gt; // &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; res =
              hwloc_set_area_membind_nodeset(topology, &amp;array[i],
              PAGE_SIZE, obj-&gt;nodeset, HWLOC_MEMBIND_BIND,
              HWLOC_MEMBIND_THREAD);<br>
              &gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;res =
              hwloc_set_area_membind(topology, &amp;array[i], PAGE_SIZE,
              cpuset, HWLOC_MEMBIND_BIND, HWLOC_MEMBIND_THREAD);<br>
              &gt; and I'm afraid that calling set_area_membind for each
              page might be too<br>
              &gt; dense: the kernel is probably allocating a memory
              policy record for each<br>
              &gt; page, not being able to merge adjacent equal
              policies.<br>
              &gt;<br>
              <br>
            </div>
            It's supposed to merge VMA with same policies (from what I
            understand in<br>
            the code), but I don't know if that actually works.<br>
            Maybe Gabriele found a kernel bug :)<br>
            <span class="HOEnZb"><font color="#888888"><br>
                Brice<br>
              </font></span>
            <div class="HOEnZb">
              <div class="h5"><br>
                _______________________________________________<br>
                hwloc-users mailing list<br>
                <a moz-do-not-send="true"
                  href="mailto:hwloc-users@open-mpi.org">hwloc-users@open-mpi.org</a><br>
                <a moz-do-not-send="true"
                  href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users"
                  target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a><br>
              </div>
            </div>
          </blockquote>
        </div>
        <br>
        <br clear="all">
        <div><br>
        </div>
        -- <br>
        Ing. Gabriele Fatigati<br>
        <br>
        HPC specialist<br>
        <br>
        SuperComputing Applications and Innovation Department<br>
        <br>
        Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>
        <br>
        <a moz-do-not-send="true" href="http://www.cineca.it"
          target="_blank">www.cineca.it</a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Tel:&nbsp;&nbsp;
        +39 051 6171722<br>
        <br>
        g.fatigati [AT] <a moz-do-not-send="true"
          href="http://cineca.it" target="_blank">cineca.it</a>&nbsp; &nbsp; &nbsp; &nbsp;
        &nbsp;&nbsp; <br>
      </div>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
hwloc-users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:hwloc-users@open-mpi.org">hwloc-users@open-mpi.org</a>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a></pre>
    </blockquote>
    <br>
  </body>
</html>

