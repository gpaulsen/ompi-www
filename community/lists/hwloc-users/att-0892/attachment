<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="HTML Tidy for Windows (vers 25 March 2009), see www.w3.org">
<meta name="Generator" content="MS Exchange Server version 14.02.0342.001">
<title>[EXTERNAL] Re: [hwloc-users] Many queries creating slow performance</title>
</head>
<body>
Hey Jeff,<br>
<br>
It's not in OpenMPI or MPICH :(. It's a custom library which is not MPI aware making it difficult to share the topology query. Ill see if we can get a stand alone piece of code.<br>
<br>
From earlier posts it sounds like OpenMPI queries once per physical node so probably won't have this problem. I'm guessing MPICH would do something similar?<br>
<br>
S.<br>
<br>
<br>
<br>
Sent with Good (www.good.com)<br>
<br>
<br>
-----Original Message-----<br>
<b>From:&nbsp;</b>Jeff Hammond [<a href="mailto:jhammond@alcf.anl.gov">jhammond@alcf.anl.gov</a>]<br>
<b>Sent:&nbsp;</b>Tuesday, March 05, 2013 07:17 PM Mountain Standard Time<br>
<b>To:&nbsp;</b>Hardware locality user list<br>
<b>Subject:&nbsp;</b>[EXTERNAL] Re: [hwloc-users] Many queries creating slow performance<br>
<br>
<!-- Converted from text/plain format -->
<p><font size="2">Si - Is your code that calls hwloc part of MPICH or OpenMPI or<br>
something that can be made standalone and shared?<br>
<br>
Brice - Do you have access to a MIC system for testing?&nbsp; Write me<br>
offline if you don't and I'll see what I can do to help.<br>
<br>
If this affects MPICH i.e. Hydra, then I'm sure Intel will be<br>
committed to helping fix it since Intel MPI is using Hydra as the<br>
launcher on systems like Stampede.<br>
<br>
Best,<br>
<br>
Jeff<br>
<br>
On Tue, Mar 5, 2013 at 3:05 PM, Brice Goglin &lt;Brice.Goglin@inria.fr&gt; wrote:<br>
&gt; Just tested on a 96-core shared-memory machine. Running OpenMPI 1.6 mpiexec<br>
&gt; lstopo, here's the execution time (mpiexec launch time is 0.2-0.4s)<br>
&gt;<br>
&gt; 1 rank :&nbsp; 0.2s<br>
&gt; 8 ranks:&nbsp; 0.3-0.5s depending on binding (packed or scatter)<br>
&gt; 24ranks:&nbsp; 0.8-3.7s depending on binding<br>
&gt; 48ranks:&nbsp; 2.8-8.0s depending on binding<br>
&gt; 96ranks: 14.2s<br>
&gt;<br>
&gt; 96ranks from a single XML file: 0.4s (negligible against mpiexec launch<br>
&gt; time)<br>
&gt;<br>
&gt; Brice<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; Le 05/03/2013 20:23, Simon Hammond a Ã©crit :<br>
&gt;<br>
&gt; Hi HWLOC users,<br>
&gt;<br>
&gt; We are seeing some significant performance problems using HWLOC 1.6.2 on<br>
&gt; Intel's MIC products. In one of our configurations we create 56 MPI ranks,<br>
&gt; each rank then queries the topology of the MIC card before creating threads.<br>
&gt; We are noticing that if we run 56 MPI ranks as opposed to one the calls to<br>
&gt; query the topology in HWLOC are very slow, runtime goes from seconds to<br>
&gt; minutes (and upwards).<br>
&gt;<br>
&gt; We guessed that this might be caused by the kernel serializing access to the<br>
&gt; /proc filesystem but this is just a hunch.<br>
&gt;<br>
&gt; Has anyone had this problem and found an easy way to change the library /<br>
&gt; calls to HWLOC so that the slow down is not experienced? Would you describe<br>
&gt; this as a bug?<br>
&gt;<br>
&gt; Thanks for your help.<br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Simon Hammond<br>
&gt;<br>
&gt; 1-(505)-845-7897 / MS-1319<br>
&gt; Scalable Computer Architectures<br>
&gt; Sandia National Laboratories, NM<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; hwloc-users mailing list<br>
&gt; hwloc-users@open-mpi.org<br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a><br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; hwloc-users mailing list<br>
&gt; hwloc-users@open-mpi.org<br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a><br>
<br>
<br>
<br>
--<br>
Jeff Hammond<br>
Argonne Leadership Computing Facility<br>
University of Chicago Computation Institute<br>
jhammond@alcf.anl.gov / (630) 252-5381<br>
<a href="http://www.linkedin.com/in/jeffhammond">http://www.linkedin.com/in/jeffhammond</a><br>
<a href="https://wiki.alcf.anl.gov/parts/index.php/User:Jhammond">https://wiki.alcf.anl.gov/parts/index.php/User:Jhammond</a><br>
<br>
_______________________________________________<br>
hwloc-users mailing list<br>
hwloc-users@open-mpi.org<br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a><br>
<br>
</font></p>
</body>
</html>
