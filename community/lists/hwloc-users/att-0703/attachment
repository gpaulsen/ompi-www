Hi Brice,<div><br></div><div>the initial grep is:</div><div><br></div><div>numa_policy        65671  65952     24  144    1 : tunables  120   60    8 : slabdata    458    458      0</div><div><br></div><div>When set_membind fails is:</div>
<div><br></div><div>numa_policy          482   1152     24  144    1 : tunables  120   60    8 : slabdata      8      8    288</div><div><br></div><div>What does it means?</div><div><br></div><div><br><br><div class="gmail_quote">
2012/9/6 Brice Goglin <span dir="ltr">&lt;<a href="mailto:Brice.Goglin@inria.fr" target="_blank">Brice.Goglin@inria.fr</a>&gt;</span><br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">

  
    
  
  <div bgcolor="#FFFFFF" text="#000000">
    Le 06/09/2012 12:19, Gabriele Fatigati a écrit :
    <div class="im"><blockquote type="cite">I did&#39;t find any strange number in /proc/meminfo.
      <div><br>
      </div>
      <div>I&#39;ve noted that the program fails exactly
        every 65479 hwloc_set_area_membind. So It sounds like some
        kernel limit. You can check that also just one thread.</div>
      <div><br>
      </div>
      <div>Maybe never has not noted them  because usually we bind a
        large amount of contiguos memory few times, instead of small and
        non contiguos pieces of memory many and many times.. :(</div>
    </blockquote>
    <br></div>
    If you have root access, try (as root)<br>
        watch -n 1 grep numa_policy /proc/slabinfo<br>
    Put a sleep(10) in your program when set_area_membind() fails, and
    don&#39;t let your program exit before you can read the content of
    /proc/slabinfo.<span class="HOEnZb"><font color="#888888"><br>
    <br>
    Brice</font></span><div><div class="h5"><br>
    <br>
    <br>
    <br>
    <blockquote type="cite">
      <div><br>
        <div class="gmail_quote">
          2012/9/6 Brice Goglin <span dir="ltr">&lt;<a href="mailto:Brice.Goglin@inria.fr" target="_blank">Brice.Goglin@inria.fr</a>&gt;</span><br>
          <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
            Le 06/09/2012 10:44, Samuel Thibault a écrit :<br>
            <div>&gt; Gabriele Fatigati, le Thu 06 Sep 2012
              10:12:38 +0200, a écrit :<br>
              &gt;&gt; mbind hwloc_linux_set_area_membind()  fails:<br>
              &gt;&gt;<br>
              &gt;&gt; Error from HWLOC mbind: Cannot allocate memory<br>
              &gt; Ok. mbind is not really supposed to allocate much
              memory, but it still<br>
              &gt; does allocate some, to record the policy<br>
              &gt;<br>
              &gt;&gt; //        hwloc_obj_t obj =
              hwloc_get_obj_by_type(topology, HWLOC_OBJ_NODE, tid);<br>
              &gt;&gt;         hwloc_obj_t obj =
              hwloc_get_obj_by_type(topology, HWLOC_OBJ_PU, tid);<br>
              &gt;&gt;         hwloc_cpuset_t cpuset =
              hwloc_bitmap_dup(obj-&gt;cpuset);<br>
              &gt;&gt;         hwloc_bitmap_singlify(cpuset);<br>
              &gt;&gt;         hwloc_set_cpubind(topology, cpuset,
              HWLOC_CPUBIND_THREAD);<br>
              &gt;&gt;<br>
              &gt;&gt;         for( i = chunk*tid; i &lt; len;
              i+=PAGE_SIZE) {<br>
              &gt;&gt; //           res =
              hwloc_set_area_membind_nodeset(topology, &amp;array[i],
              PAGE_SIZE, obj-&gt;nodeset, HWLOC_MEMBIND_BIND,
              HWLOC_MEMBIND_THREAD);<br>
              &gt;&gt;              res =
              hwloc_set_area_membind(topology, &amp;array[i], PAGE_SIZE,
              cpuset, HWLOC_MEMBIND_BIND, HWLOC_MEMBIND_THREAD);<br>
              &gt; and I&#39;m afraid that calling set_area_membind for each
              page might be too<br>
              &gt; dense: the kernel is probably allocating a memory
              policy record for each<br>
              &gt; page, not being able to merge adjacent equal
              policies.<br>
              &gt;<br>
              <br>
            </div>
            It&#39;s supposed to merge VMA with same policies (from what I
            understand in<br>
            the code), but I don&#39;t know if that actually works.<br>
            Maybe Gabriele found a kernel bug :)<br>
            <span><font color="#888888"><br>
                Brice<br>
              </font></span>
            <div>
              <div><br>
                _______________________________________________<br>
                hwloc-users mailing list<br>
                <a href="mailto:hwloc-users@open-mpi.org" target="_blank">hwloc-users@open-mpi.org</a><br>
                <a href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a><br>
              </div>
            </div>
          </blockquote>
        </div>
        <br>
        <br clear="all">
        <div><br>
        </div>
        -- <br>
        Ing. Gabriele Fatigati<br>
        <br>
        HPC specialist<br>
        <br>
        SuperComputing Applications and Innovation Department<br>
        <br>
        Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br>
        <br>
        <a href="http://www.cineca.it" target="_blank">www.cineca.it</a>                    Tel:  
        <a href="tel:%2B39%20051%206171722" value="+390516171722" target="_blank">+39 051 6171722</a><br>
        <br>
        g.fatigati [AT] <a href="http://cineca.it" target="_blank">cineca.it</a>       
           <br>
      </div>
      <br>
      <fieldset></fieldset>
      <br>
      <pre>_______________________________________________
hwloc-users mailing list
<a href="mailto:hwloc-users@open-mpi.org" target="_blank">hwloc-users@open-mpi.org</a>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a></pre>
    </blockquote>
    <br>
  </div></div></div>

<br>_______________________________________________<br>
hwloc-users mailing list<br>
<a href="mailto:hwloc-users@open-mpi.org">hwloc-users@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br>Ing. Gabriele Fatigati<br>
<br>HPC specialist<br><br>SuperComputing Applications and Innovation Department<br><br>Via Magnanelli 6/3, Casalecchio di Reno (BO) Italy<br><br><a href="http://www.cineca.it" target="_blank">www.cineca.it</a>                    Tel:   +39 051 6171722<br>
<br>g.fatigati [AT] <a href="http://cineca.it" target="_blank">cineca.it</a>           <br>
</div>

