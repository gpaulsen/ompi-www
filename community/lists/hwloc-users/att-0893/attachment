<html>
  <head>
    <meta content="text/html; charset=ISO-8859-1"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    Aside from the idea of saving the topology to a XML file before
    running the job, you could also:<br>
    * rank 0 load the topology as usual<br>
    * rank 0 saves it to a XML buffer
    (hwloc_topology_export_xmlbuffer()) then MPI_Broadcast() to other
    ranks<br>
    * those ranks would just load a hwloc topology from the received XML
    buffer (hwloc_topology_set_xmlbuffer()).<br>
    <br>
    Brice<br>
    <br>
    <br>
    <br>
    Le 06/03/2013 03:53, Hammond, Simon David (-EXP) a &eacute;crit&nbsp;:
    <blockquote
cite="mid:74A0D537FED93B4EA49EA78E5A3905C30535D8F1@EXMB02.srn.sandia.gov"
      type="cite">
      <meta http-equiv="Content-Type" content="text/html;
        charset=ISO-8859-1">
      <meta name="generator" content="HTML Tidy for Windows (vers 25
        March 2009), see www.w3.org">
      <meta name="Generator" content="MS Exchange Server version
        14.02.0342.001">
      <title>[EXTERNAL] Re: [hwloc-users] Many queries creating slow
        performance</title>
      Hey Jeff,<br>
      <br>
      It's not in OpenMPI or MPICH :(. It's a custom library which is
      not MPI aware making it difficult to share the topology query. Ill
      see if we can get a stand alone piece of code.<br>
      <br>
      From earlier posts it sounds like OpenMPI queries once per
      physical node so probably won't have this problem. I'm guessing
      MPICH would do something similar?<br>
      <br>
      S.<br>
      <br>
      <br>
      <br>
      Sent with Good (<a class="moz-txt-link-abbreviated" href="http://www.good.com">www.good.com</a>)<br>
      <br>
      <br>
      -----Original Message-----<br>
      <b>From:&nbsp;</b>Jeff Hammond [<a moz-do-not-send="true"
        href="mailto:jhammond@alcf.anl.gov">jhammond@alcf.anl.gov</a>]<br>
      <b>Sent:&nbsp;</b>Tuesday, March 05, 2013 07:17 PM Mountain Standard
      Time<br>
      <b>To:&nbsp;</b>Hardware locality user list<br>
      <b>Subject:&nbsp;</b>[EXTERNAL] Re: [hwloc-users] Many queries creating
      slow performance<br>
      <br>
      <!-- Converted from text/plain format -->
      <p><font size="2">Si - Is your code that calls hwloc part of MPICH
          or OpenMPI or<br>
          something that can be made standalone and shared?<br>
          <br>
          Brice - Do you have access to a MIC system for testing?&nbsp; Write
          me<br>
          offline if you don't and I'll see what I can do to help.<br>
          <br>
          If this affects MPICH i.e. Hydra, then I'm sure Intel will be<br>
          committed to helping fix it since Intel MPI is using Hydra as
          the<br>
          launcher on systems like Stampede.<br>
          <br>
          Best,<br>
          <br>
          Jeff<br>
          <br>
          On Tue, Mar 5, 2013 at 3:05 PM, Brice Goglin
          <a class="moz-txt-link-rfc2396E" href="mailto:Brice.Goglin@inria.fr">&lt;Brice.Goglin@inria.fr&gt;</a> wrote:<br>
          &gt; Just tested on a 96-core shared-memory machine. Running
          OpenMPI 1.6 mpiexec<br>
          &gt; lstopo, here's the execution time (mpiexec launch time is
          0.2-0.4s)<br>
          &gt;<br>
          &gt; 1 rank :&nbsp; 0.2s<br>
          &gt; 8 ranks:&nbsp; 0.3-0.5s depending on binding (packed or
          scatter)<br>
          &gt; 24ranks:&nbsp; 0.8-3.7s depending on binding<br>
          &gt; 48ranks:&nbsp; 2.8-8.0s depending on binding<br>
          &gt; 96ranks: 14.2s<br>
          &gt;<br>
          &gt; 96ranks from a single XML file: 0.4s (negligible against
          mpiexec launch<br>
          &gt; time)<br>
          &gt;<br>
          &gt; Brice<br>
          &gt;<br>
          &gt;<br>
          &gt;<br>
          &gt; Le 05/03/2013 20:23, Simon Hammond a &eacute;crit :<br>
          &gt;<br>
          &gt; Hi HWLOC users,<br>
          &gt;<br>
          &gt; We are seeing some significant performance problems using
          HWLOC 1.6.2 on<br>
          &gt; Intel's MIC products. In one of our configurations we
          create 56 MPI ranks,<br>
          &gt; each rank then queries the topology of the MIC card
          before creating threads.<br>
          &gt; We are noticing that if we run 56 MPI ranks as opposed to
          one the calls to<br>
          &gt; query the topology in HWLOC are very slow, runtime goes
          from seconds to<br>
          &gt; minutes (and upwards).<br>
          &gt;<br>
          &gt; We guessed that this might be caused by the kernel
          serializing access to the<br>
          &gt; /proc filesystem but this is just a hunch.<br>
          &gt;<br>
          &gt; Has anyone had this problem and found an easy way to
          change the library /<br>
          &gt; calls to HWLOC so that the slow down is not experienced?
          Would you describe<br>
          &gt; this as a bug?<br>
          &gt;<br>
          &gt; Thanks for your help.<br>
          &gt;<br>
          &gt;<br>
          &gt; --<br>
          &gt; Simon Hammond<br>
          &gt;<br>
          &gt; 1-(505)-845-7897 / MS-1319<br>
          &gt; Scalable Computer Architectures<br>
          &gt; Sandia National Laboratories, NM<br>
          &gt;<br>
          &gt;<br>
          &gt;<br>
          &gt;<br>
          &gt;<br>
          &gt;<br>
          &gt; _______________________________________________<br>
          &gt; hwloc-users mailing list<br>
          &gt; <a class="moz-txt-link-abbreviated" href="mailto:hwloc-users@open-mpi.org">hwloc-users@open-mpi.org</a><br>
          &gt; <a moz-do-not-send="true"
            href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a><br>
          &gt;<br>
          &gt;<br>
          &gt;<br>
          &gt; _______________________________________________<br>
          &gt; hwloc-users mailing list<br>
          &gt; <a class="moz-txt-link-abbreviated" href="mailto:hwloc-users@open-mpi.org">hwloc-users@open-mpi.org</a><br>
          &gt; <a moz-do-not-send="true"
            href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a><br>
          <br>
          <br>
          <br>
          --<br>
          Jeff Hammond<br>
          Argonne Leadership Computing Facility<br>
          University of Chicago Computation Institute<br>
          <a class="moz-txt-link-abbreviated" href="mailto:jhammond@alcf.anl.gov">jhammond@alcf.anl.gov</a> / (630) 252-5381<br>
          <a moz-do-not-send="true"
            href="http://www.linkedin.com/in/jeffhammond">http://www.linkedin.com/in/jeffhammond</a><br>
          <a moz-do-not-send="true"
            href="https://wiki.alcf.anl.gov/parts/index.php/User:Jhammond">https://wiki.alcf.anl.gov/parts/index.php/User:Jhammond</a><br>
          <br>
          _______________________________________________<br>
          hwloc-users mailing list<br>
          <a class="moz-txt-link-abbreviated" href="mailto:hwloc-users@open-mpi.org">hwloc-users@open-mpi.org</a><br>
          <a moz-do-not-send="true"
            href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a><br>
          <br>
        </font></p>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
hwloc-users mailing list
<a class="moz-txt-link-abbreviated" href="mailto:hwloc-users@open-mpi.org">hwloc-users@open-mpi.org</a>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users">http://www.open-mpi.org/mailman/listinfo.cgi/hwloc-users</a></pre>
    </blockquote>
    <br>
  </body>
</html>

