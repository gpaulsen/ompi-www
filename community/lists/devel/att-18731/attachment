Folks,<div><br></div><div>currently, there is a global lock in the OpenMPI glue for ROMIO</div><div><br></div><div>that can causes some hangs with MPI_THREAD_MULTIPLE</div><div><br></div><div>I could not find such global locks in MPICH, nor a good rationale for having then in OpenMPI.</div><div>At this stage, I can only consider this is legacy stuff that should simply be removed.</div><div><br></div><div>Does anyone remember why we used global locks in the first place ?</div><div>Is there any rationale for using global locks (or even a finer grain locking) in the ROMIO glue ?</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<br><br>---------- Forwarded message ----------<br>From: <b>Sebastian Rettenberger</b> &lt;<a href="mailto:rettenbs@in.tum.de">rettenbs@in.tum.de</a>&gt;<br>Date: Tuesday, March 29, 2016<br>Subject: [OMPI users] Collective MPI-IO + MPI_THREAD_MULTIPLE<br>To: Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br><br><br>Hi,<br>
<br>
thanks, the patch works for me. I will do some further tests and report back if I find another problem.<br>
<br>
Best regards,<br>
Sebastian<br>
<br>
On 03/25/2016 01:58 AM, Gilles Gouaillardet wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Sebastian,<br>
<br>
at first glance, the global lock in romio glue is not necessary.<br>
<br>
feel free to give the attached patch a try<br>
(it works with your example, and i made no further testing)<br>
<br>
Cheers,<br>
<br>
Gilles<br>
<br>
<br>
On 3/25/2016 9:26 AM, Gilles Gouaillardet wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Sebastian,<br>
<br>
thanks for the info.<br>
<br>
bottom line, the global lock is in the OpenMPI glue for ROMIO.<br>
<br>
i will check what kind of locking (if any) is done in mpich<br>
<br>
Cheers,<br>
<br>
Gilles<br>
<br>
On 3/24/2016 11:30 PM, Sebastian Rettenberger wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Hi,<br>
<br>
I tested this on my desktop machine. Thus, one node, two tasks.<br>
It deadlock appears on the local file system and on the nfs mount.<br>
<br>
The MPICH version I tested was 3.2.<br>
<br>
However, as far as I know, locking is part of the MPI library and not<br>
ROMIO.<br>
<br>
Best regards,<br>
Sebastian<br>
<br>
On 03/24/2016 03:19 PM, Gilles Gouaillardet wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Sebastian,<br>
<br>
in openmpi 1.10, the default io component is romio from mpich 3.0.4.<br>
<br>
how many tasks, how many nodes and which file system are you running<br>
on ?<br>
<br>
Cheers,<br>
<br>
Gilles<br>
<br>
On Thursday, March 24, 2016, Sebastian Rettenberger<br>
&lt;<a>rettenbs@in.tum.de</a>&gt;<br>
wrote:<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Hi,<br>
<br>
I tried to run the attached program with OpenMPI. It works well<br>
with MPICH<br>
and Intel MPI but I get a deadlock when using OpenMPI.<br>
I am using OpenMPI 1.10.0 with support for MPI_THREAD_MULTIPLE.<br>
<br>
It seems like ROMIO uses global locks in OpenMPI which is a problem if<br>
multiple threads want to do collective I/O.<br>
<br>
Any idea how one can get around this issue?<br>
<br>
Best regards,<br>
Sebastian<br>
<br>
--<br>
Sebastian Rettenberger, M.Sc.<br>
Technische Universit채t M체nchen<br>
Department of Informatics<br>
Chair of Scientific Computing<br>
Boltzmannstrasse 3, 85748 Garching, Germany<br>
<a href="http://www5.in.tum.de/" target="_blank">http://www5.in.tum.de/</a><br>
<br>
</blockquote>
<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a>users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post:<br>
<a href="http://www.open-mpi.org/community/lists/users/2016/03/28819.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/03/28819.php</a><br>
<br>
</blockquote>
<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a>users@open-mpi.org</a><br>
Subscription:<a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this<br>
post:<a href="http://www.open-mpi.org/community/lists/users/2016/03/28820.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/03/28820.php</a><br>
</blockquote>
<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a>users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post:<br>
<a href="http://www.open-mpi.org/community/lists/users/2016/03/28825.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/03/28825.php</a><br>
</blockquote>
<br>
<br>
<br>
<br>
_______________________________________________<br>
users mailing list<br>
<a>users@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/users" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/users/2016/03/28826.php" target="_blank">http://www.open-mpi.org/community/lists/users/2016/03/28826.php</a><br>
<br>
</blockquote>
<br>
-- <br>
Sebastian Rettenberger, M.Sc.<br>
Technische Universit채t M체nchen<br>
Department of Informatics<br>
Chair of Scientific Computing<br>
Boltzmannstrasse 3, 85748 Garching, Germany<br>
<a href="http://www5.in.tum.de/" target="_blank">http://www5.in.tum.de/</a><br>
<br>
<br></div>

