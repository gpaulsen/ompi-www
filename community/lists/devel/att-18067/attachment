<div dir="ltr">Might not - there has been a very large amount of change over the last few months, and I confess I haven&#39;t been checking the DVM regularly. So let me take a step back and look at that code.<div><br></div><div>I&#39;ll also include the extensions you requested on the other email - I didn&#39;t forget them, just somewhat overwhelmed lately</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Thu, Sep 17, 2015 at 11:39 AM, Mark Santcroos <span dir="ltr">&lt;<a href="mailto:mark.santcroos@rutgers.edu" target="_blank">mark.santcroos@rutgers.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><span class=""><br>
&gt; On 17 Sep 2015, at 20:34 , Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt;<br>
&gt; Ouch - this is on current master HEAD?<br>
<br>
</span>Yep!<br>
<span class=""><br>
&gt; I&#39;m on travel right now, but I&#39;ll be back Fri evening and can look at it this weekend. Probably something silly that needs to be fixed.<br>
<br>
</span>Thanks!<br>
<br>
Obviously I didn&#39;t check every single version between March and now, but its safe to assume that it didn&#39;t work in between either I guess.<br>
<div><div class="h5"><br>
<br>
&gt;<br>
&gt;<br>
&gt; On Thu, Sep 17, 2015 at 11:30 AM, Mark Santcroos &lt;<a href="mailto:mark.santcroos@rutgers.edu">mark.santcroos@rutgers.edu</a>&gt; wrote:<br>
&gt; Hi (Ralph),<br>
&gt;<br>
&gt; Over the last months I have been focussing on exec throughput, and not so much on the application payload (read: mainly using /bin/sleep ;-)<br>
&gt; As things are stabilising now, I returned my attention to &quot;real&quot; applications.<br>
&gt; To discover that launching MPI applications (build with the same Open MPI version) within a DVM doesn&#39;t work anymore (see error below).<br>
&gt;<br>
&gt; I&#39;ve been doing a binary search, but that turned out to be not so trivial because of other problems in the window of the change.<br>
&gt; So far I&#39;ve narrowed it down to:<br>
&gt;<br>
&gt; 64ec498 - mar 5 - works on my laptop (but not on the Crays)<br>
&gt; b67b361 - mar 28 - works once per DVM launch on my laptop, but consecutive orte-submits get a connect error<br>
&gt; b209c9e - March 30 - same MPI_Init issue as HEAD<br>
&gt;<br>
&gt; Going further into mid-March I run into build issues with verb, runtime issues with default binding complaining about missing libnumactl, runtime tcp oob errors, etc.<br>
&gt; So I don&#39;t know whether the binary search will yield much more than I was able to dig up now.<br>
&gt;<br>
&gt; What can I do to get closer to debugging the actual issue?<br>
&gt;<br>
&gt; Thanks!<br>
&gt;<br>
&gt; Mark<br>
&gt;<br>
&gt;<br>
&gt; OMPI_PREFIX=/Users/mark/proj/openmpi/installed/HEAD<br>
&gt; OMPI_MCA_orte_hnp_uri=723386368.0;usock;tcp://<a href="http://192.168.0.103:56672" rel="noreferrer" target="_blank">192.168.0.103:56672</a><br>
&gt; OMPI_MCA_ess=tool<br>
&gt; [netbook:70703] Job [11038,3] has launched<br>
&gt; --------------------------------------------------------------------------<br>
&gt; It looks like MPI_INIT failed for some reason; your parallel process is<br>
&gt; likely to abort.  There are many reasons that a parallel process can<br>
&gt; fail during MPI_INIT; some of which are due to configuration or environment<br>
&gt; problems.  This failure appears to be an internal failure; here&#39;s some<br>
&gt; additional information (which may only be relevant to an Open MPI<br>
&gt; developer):<br>
&gt;<br>
&gt;   ompi_mpi_init: ompi_rte_init failed<br>
&gt;   --&gt; Returned &quot;(null)&quot; (-43) instead of &quot;Success&quot; (0)<br>
&gt; --------------------------------------------------------------------------<br>
&gt; *** An error occurred in MPI_Init<br>
&gt; *** on a NULL communicator<br>
&gt; *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>
&gt; ***    and potentially your MPI job)<br>
&gt; [netbook:70704] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/09/18064.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/09/18064.php</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</div></div>&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/09/18065.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/09/18065.php</a><br>
<span class=""><br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</span>Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/09/18066.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/09/18066.php</a><br>
</blockquote></div><br></div>

