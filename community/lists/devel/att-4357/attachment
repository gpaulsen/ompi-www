<p>Hi George,&nbsp;</p><p>I got seqv with IMB PingPong test starting from r18850 ( including trunk )</p><p>r18849 works fine.</p><p>Best Regards</p><p>Lenny.</p><p></p><p>/home/USERS/lenny/OMPI_ORTE_18850/bin/mpirun -np 2 -H witch2,witch3 ./IMB-MPI1_18850 PingPong<br>
</p>#---------------------------------------------------<br>#    Intel (R) MPI Benchmark Suite V3.0v modified by Voltaire, MPI-1 part<br>#---------------------------------------------------<br># Date                  : Tue Jul 15 15:11:30 2008<br>
# Machine               : x86_64<br># System                : Linux<br># Release               : 2.6.16.46-0.12-smp<br># Version               : #1 SMP Thu May 17 14:00:09 UTC 2007<br># MPI Version           : 2.0<br># MPI Thread Environment: MPI_THREAD_SINGLE<br>
<br>#<br># Minimum message length in bytes:   0<br># Maximum message length in bytes:   67108864<br>#<br># MPI_Datatype                   :   MPI_BYTE<br># MPI_Datatype for reductions    :   MPI_FLOAT<br># MPI_Op                         :   MPI_SUM<br>
#<br>#<br><br># List of Benchmarks to run:<br><br># PingPong<br>[witch3:32461] *** Process received signal ***<br>[witch3:32461] Signal: Segmentation fault (11)<br>[witch3:32461] Signal code: Address not mapped (1)<br>[witch3:32461] Failing at address: 0x20<br>
[witch3:32461] [ 0] /lib64/libpthread.so.0 [0x2b514fcedc10]<br>[witch3:32461] [ 1] /home/USERS/lenny/OMPI_ORTE_18850/lib/openmpi/mca_pml_ob1.so [0x2b51510b416a]<br>[witch3:32461] [ 2] /home/USERS/lenny/OMPI_ORTE_18850/lib/openmpi/mca_pml_ob1.so [0x2b51510b4661]<br>
[witch3:32461] [ 3] /home/USERS/lenny/OMPI_ORTE_18850/lib/openmpi/mca_pml_ob1.so [0x2b51510b180e]<br>[witch3:32461] [ 4] /home/USERS/lenny/OMPI_ORTE_18850/lib/openmpi/mca_btl_openib.so [0x2b5151811c22]<br>[witch3:32461] [ 5] /home/USERS/lenny/OMPI_ORTE_18850/lib/openmpi/mca_btl_openib.so [0x2b51518132e9]<br>
[witch3:32461] [ 6] /home/USERS/lenny/OMPI_ORTE_18850/lib/openmpi/mca_bml_r2.so [0x2b51512c412f]<br>[witch3:32461] [ 7] /home/USERS/lenny/OMPI_ORTE_18850/lib/libopen-pal.so.0(opal_progress+0x5a) [0x2b514f71268a]<br>[witch3:32461] [ 8] /home/USERS/lenny/OMPI_ORTE_18850/lib/openmpi/mca_pml_ob1.so [0x2b51510af0f5]<br>
[witch3:32461] [ 9] /home/USERS/lenny/OMPI_ORTE_18850/lib/libmpi.so.0(PMPI_Recv+0x13b) [0x2b514f47941b]<br>[witch3:32461] [10] ./IMB-MPI1_18850(IMB_pingpong+0x1a1) [0x4073cd]<br>[witch3:32461] [11] ./IMB-MPI1_18850(IMB_warm_up+0x2d) [0x405e49]<br>
[witch3:32461] [12] ./IMB-MPI1_18850(main+0x394) [0x4034d4]<br>[witch3:32461] [13] /lib64/libc.so.6(__libc_start_main+0xf4) [0x2b514fe14154]<br>[witch3:32461] [14] ./IMB-MPI1_18850 [0x4030a9]<br>[witch3:32461] *** End of error message ***<br>
mpirun: killing job...<br><br>--------------------------------------------------------------------------<br>mpirun was unable to cleanly terminate the daemons on the nodes shown<br>below. Additional manual cleanup may be required - please refer to<br>
the &quot;orte-clean&quot; tool for assistance.<br>--------------------------------------------------------------------------<br>&nbsp;       witch2<br>&nbsp;       witch3<br><br><br>

