<html><head><meta http-equiv="Content-Type" content="text/html charset=windows-1252"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Nov 5, 2014, at 6:11 PM, Gilles Gouaillardet &lt;<a href="mailto:gilles.gouaillardet@iferc.org" class="">gilles.gouaillardet@iferc.org</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class="">
  
    <meta content="text/html; charset=ISO-8859-1" http-equiv="Content-Type" class="">
  
  <div text="#000000" bgcolor="#FFFFFF" class="">
    <div class="moz-cite-prefix">Elena,<br class="">
      <br class="">
      the first case (-mca btl tcp,self) crashing is a bug and i will
      have a look at it.<br class="">
      <br class="">
      the second case (-mca sm,self) is a feature : the sm btl cannot be
      used with tasks<br class="">
      having different jobids (this is the case after a spawn), and
      obviously, self cannot be used also,<br class="">
      so the behaviour and error message is correct.<br class="">
      /* i am not aware of any plans to make the sm btl work with tasks
      from different jobids */\<br class=""></div></div></div></blockquote><div><br class=""></div><div>That is correct - I’m also unaware of any plans to extend it at this point, though IIRC Nathan at one time mentioned perhaps extending vader for that purpose</div><div><br class=""></div><blockquote type="cite" class=""><div class=""><div text="#000000" bgcolor="#FFFFFF" class=""><div class="moz-cite-prefix">
      <br class="">
      the third case (-mca openib,self) is more controversial ...<br class="">
      i previously posted
      <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/devel/2014/10/16136.php">http://www.open-mpi.org/community/lists/devel/2014/10/16136.php</a><br class="">
      what happens in your case (simple_spawn) is the openib modex is
      sent with PMIX_REMOTE,<br class="">
      that means openib btl cannot be used between tasks on the same
      node.<br class="">
      i am still waiting for some feedback since i cannot figure out
      whether this is a feature or an<br class="">
      undesired side effect / bug<br class=""></div></div></div></blockquote><div><br class=""></div>I believe it is a bug - I provided some initial values for the modex scope with the expectation (and request when we committed it) that people would review and modify them as appropriate. I recall setting the openib scope as “remote” only because I wasn’t aware of anyone using it for local comm. Since Mellanox obviously is testing for that case, a scope of PMIX_GLOBAL would be more appropriate</div><div><br class=""><blockquote type="cite" class=""><div class=""><div text="#000000" bgcolor="#FFFFFF" class=""><div class="moz-cite-prefix">
      <br class="">
      the last cast (-mca ^sm,openib) does make sense to me :<br class="">
      the tcp and self btls are used and they work just like they
      should.<br class="">
      <br class="">
      bottom line, i will investigate the first crash, wait for feedback
      about the openib btl.<br class="">
      <br class="">
      Cheers,<br class="">
      <br class="">
      Gilles<br class="">
      <br class="">
      On 2014/11/06 1:08, Elena Elkina wrote:<br class="">
    </div>
    <blockquote cite="mid:CANhOtjb_Ht=w=96dJLxf8YLSsBCT-jqrczRteZTJ3sGe4=EzMw@mail.gmail.com" type="cite" class="">
      <pre wrap="" class="">Hi,

It looks like there is a problem in trunk which reproduces with
simple_spawn test (orte/test/mpi/simple_spawn.c). It seems to be a n issue
with pmix. It doesn't reproduce with default set of btls. But it reproduces
with several btls specified. For example,

salloc -N5 $OMPI_HOME/install/bin/mpirun -np 33 --map-by node -mca coll ^ml
-display-map -mca orte_debug_daemons true --leave-session-attached
--debug-daemons -mca pml ob1 -mca btl *tcp,self*
./orte/test/mpi/simple_spawn

gets

simple_spawn: ../../ompi/group/group_init.c:215:
ompi_group_increment_proc_count: Assertion `((0xdeafbeedULL &lt;&lt; 32) +
0xdeafbeedULL) == ((opal_object_t *) (proc_pointer))-&gt;obj_magic_id' failed.
[<a href="http://sputnik3.vbench.com" class="">sputnik3.vbench.com</a>:28888] [[41877,0],3] orted_cmd: exit cmd, but proc
[[41877,1],2] is alive
[sputnik5][[41877,1],29][../../../../../opal/mca/btl/tcp/btl_tcp_endpoint.c:675:mca_btl_tcp_endpoint_complete_connect]
connect() to 192.168.1.42 failed: Connection refused (111)

salloc -N1 $OMPI_HOME/install/bin/mpirun -np 3 --map-by node -mca coll ^ml
-display-map -mca orte_debug_daemons true --leave-session-attached
--debug-daemons -mca pml ob1 -mca btl *sm,self* ./orte/test/mpi/simple_spawn

fails with

At least one pair of MPI processes are unable to reach each other for
MPI communications.  This means that no Open MPI device has indicated
that it can be used to communicate between these processes.  This is
an error; Open MPI requires that all MPI processes be able to reach
each other.  This error can sometimes be the result of forgetting to
specify the "self" BTL.

  Process 1 ([[59481,2],0]) is on host: sputnik1
  Process 2 ([[59481,1],0]) is on host: sputnik1
  BTLs attempted: self sm

Your MPI job is now going to abort; sorry.
--------------------------------------------------------------------------
[<a href="http://sputnik1.vbench.com" class="">sputnik1.vbench.com</a>:22156] [[59481,1],2] ORTE_ERROR_LOG: Unreachable in
file ../../../../../ompi/mca/dpm/orte/dpm_orte.c at line 485


salloc -N1 $OMPI_HOME/install/bin/mpirun -np 3 --map-by node -mca coll ^ml
-display-map -mca orte_debug_daemons true --leave-session-attached
--debug-daemons -mca pml ob1 -mca btl *openib,self*
 ./orte/test/mpi/simple_spawn

also doesn't work:
At least one pair of MPI processes are unable to reach each other for
MPI communications.  This means that no Open MPI device has indicated
that it can be used to communicate between these processes.  This is
an error; Open MPI requires that all MPI processes be able to reach
each other.  This error can sometimes be the result of forgetting to
specify the "self" BTL.

  Process 1 ([[60046,1],13]) is on host: sputnik4
  Process 2 ([[60046,2],1]) is on host: sputnik4
  BTLs attempted: openib self

Your MPI job is now going to abort; sorry.
--------------------------------------------------------------------------
[<a href="http://sputnik4.vbench.com" class="">sputnik4.vbench.com</a>:25476] [[60046,1],3] ORTE_ERROR_LOG: Unreachable in
file ../../../../../ompi/mca/dpm/orte/dpm_orte.c at line 485


*But* combination ^sm,openib seems to work.

I tried different revisions from the beginning of October. It reproduces on
them.

Best regards,
Elena

</pre>
      <br class="">
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br class="">
      <pre wrap="" class="">_______________________________________________
devel mailing list
<a class="moz-txt-link-abbreviated" href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/devel/2014/11/16202.php">http://www.open-mpi.org/community/lists/devel/2014/11/16202.php</a></pre>
    </blockquote>
    <br class="">
  </div>

_______________________________________________<br class="">devel mailing list<br class=""><a href="mailto:devel@open-mpi.org" class="">devel@open-mpi.org</a><br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/devel<br class="">Link to this post: http://www.open-mpi.org/community/lists/devel/2014/11/16223.php</div></blockquote></div><br class=""></body></html>
