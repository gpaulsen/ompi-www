<div dir="ltr">HI George,<div><br></div><div>I&#39;d say commit cf377db82 explains the vanishing of the bandwidth metric as well as the mis-labeling of the latency metric.</div><div><br></div><div>Howard</div><div><br></div>







</div><div class="gmail_extra"><br><div class="gmail_quote">2015-02-10 18:41 GMT-07:00 George Bosilca <span dir="ltr">&lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank">bosilca@icl.utk.edu</a>&gt;</span>:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Somehow one of the most basic information about the capabilities of the BTLs (bandwidth) disappeared from the MCA parameters and the one left (latency) was mislabeled. This mishap not only prevented the communication engine from correctly ordering the BTL for small messages (the latency bound part), but also introduced undesirable bias on the load-balance between multiple devices logic (the bandwidth part).<div><br></div><div>I just pushed a fix  in master <a href="https://github.com/open-mpi/ompi/commit/e173f9b0c0c63c3ea24b8d8bc0ebafe1f1736acb" target="_blank">https://github.com/open-mpi/ompi/commit/e173f9b0c0c63c3ea24b8d8bc0ebafe1f1736acb</a>. Once validated this should be moved over the 1.8 branch.<div><br></div><div>Dave do you think it is possible to renew your experiment with the current master ?</div><div><br></div><div>  Thanks,</div><div>    George.</div><div><br></div><div><br></div></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Mon, Feb 9, 2015 at 2:57 PM, Dave Turner <span dir="ltr">&lt;<a href="mailto:drdaveturner@gmail.com" target="_blank">drdaveturner@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Gilles,<div><br></div><div>     I tried running with <span style="font-size:12.8000001907349px">btl_openib_cpc_include rdmacm and saw no change.</span></div><div><span style="font-size:12.8000001907349px"><br></span></div><div><span style="font-size:12.8000001907349px">      Let&#39;s simplify the problem by forgetting about the channel bonding.  </span></div><div><span style="font-size:12.8000001907349px">If I just do an aggregate test of 16 cores on one machine talking to 16 on</span></div><div><span style="font-size:12.8000001907349px">a second machine without any settings changed from the default install</span></div><div><span style="font-size:12.8000001907349px">of OpenMPI, I see that RoCE over the 10 Gbps link is used for small messages</span></div><div><span style="font-size:12.8000001907349px">then it switches over to QDR IB for large messages.  I don&#39;t see channel bonding</span></div><div><span style="font-size:12.8000001907349px">for large messages, but can turn this on with the btl_tcp_exclusivity parameter.</span></div><div><span style="font-size:12.8000001907349px"><br></span></div><div><span style="font-size:12.8000001907349px">     I think there are 2 problems here, both related to the fact that QDR IB link and RoCE </span></div><div><span style="font-size:12.8000001907349px">both use the same openib btl.  The first problem is that the slower RoCE link is being chosen</span></div><div><span style="font-size:12.8000001907349px">for small messages, which does lower performance significantly.  The second problem</span></div><div><span style="font-size:12.8000001907349px">is that I don&#39;t think there are parameters to allow for tuning of multiple openib btl&#39;s</span></div><div><span style="font-size:12.8000001907349px">to manually select one over the other.  </span></div><span><font color="#888888"><div><span style="font-size:12.8000001907349px"><br></span></div><div><span style="font-size:12.8000001907349px">                       Dave</span></div></font></span></div><div><div><div class="gmail_extra"><br><div class="gmail_quote">On Fri, Feb 6, 2015 at 8:24 PM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles.gouaillardet@gmail.com" target="_blank">gilles.gouaillardet@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Dave,<br><br>These settings tell ompi to use native infiniband on the ib qdr port and tcpo/ip on the other port.<br><br>From the faq, roce is implemented in the openib btl<br><a href="http://www.open-mpi.org/faq/?category=openfabrics#ompi-over-roce" target="_blank">http://www.open-mpi.org/faq/?category=openfabrics#ompi-over-roce</a><br><br>Did you use <br>--mca btl_openib_cpc_include rdmacm<br>in  your first tests ?<br><br>I had some second thougths about the bandwidth values, and imho they should be 327680 and 81920 because of the 8/10 encoding<br>(And that being said, that should not change the measured performance)<br><br>Also, could you try again by forcing the same btl_tcp_latency and btl_openib_latency ?<br><br>Cheers,<br><br>Gilles<br><br>Dave Turner &lt;<a href="mailto:drdaveturner@gmail.com" target="_blank">drdaveturner@gmail.com</a>&gt; wrote:<br><div dir="ltr">George,<div><br></div><div>     I can check with my guys on Monday but I think the bandwidth parameters </div><div>are the defaults.  I did alter these to 40960 and 10240 as someone else </div><div>suggested to me.  The attached graph shows the base red line, along with</div><div>the manual balanced blue line and auto balanced green line (0&#39;s for both).</div><div>This shift lower suggests to me that the higher TCP latency is being pulled in.</div><div>I&#39;m not sure why the curves are shifted right.</div><div><br></div><div>                        Dave</div></div><div class="gmail_extra"><br><div class="gmail_quote">On Fri, Feb 6, 2015 at 5:32 PM, George Bosilca <span dir="ltr">&lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank">bosilca@icl.utk.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div>Dave,</div><div><br></div><div>Based on your ompi_info.all the following bandwidth are reported on your system:</div><div> <br></div><div>                MCA btl: parameter &quot;btl_openib_bandwidth&quot; (current value: &quot;4&quot;, data source: default, level: 5 tuner/detail, type: unsigned)</div><div>                          Approximate maximum bandwidth of interconnect (0 = auto-detect value at run-time [not supported in all BTL modules], &gt;= 1 = bandwidth in Mbps)</div><div><br></div><div><div>                 MCA btl: parameter &quot;btl_tcp_bandwidth&quot; (current value: &quot;100&quot;, data source: default, level: 5 tuner/detail, type: unsigned)</div><div>                          Approximate maximum bandwidth of interconnect (0 = auto-detect value at run-time [not supported in all BTL modules], &gt;= 1 = bandwidth in Mbps)</div></div><div><br></div><div>This basically states that on your system the default values for these parameters are wrong, your TCP network being much faster than the IB. This explains the somewhat unexpected decision of OMPI.</div><div><br></div><div>As a possible solution I suggest you set these bandwidth values to something more meaningful (directly in your configuration file). As an example,</div><div><br></div><div>btl_openib_bandwidth = 40000<br></div><div>btl_tcp_bandwidth = 10000<br></div><div><br></div><div>make more sense based on your HPC system description.</div><div><br></div><div>  George.</div><div><br></div><div><br></div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote"><div><div>On Fri, Feb 6, 2015 at 5:37 PM, Dave Turner <span dir="ltr">&lt;<a href="mailto:drdaveturner@gmail.com" target="_blank">drdaveturner@gmail.com</a>&gt;</span> wrote:<br></div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div><div dir="ltr"><div><br></div>     We have nodes in our HPC system that have 2 NIC&#39;s, <div>one being QDR IB and the second being a slower 10 Gbps card</div><div>configured for both RoCE and TCP.  Aggregate bandwidth </div><div>tests with 20 cores on one node yelling at 20 cores on a second</div><div>node (attached roce.ib.aggregate.pdf) show that without tuning</div><div>the slower RoCE interface is being used for small messages</div><div>then QDR IB is used for larger messages (red line).  Tuning</div><div>the tcp_exclusivity to 1024 to match the openib_exclusivity </div><div>adds another 20 Gbps of bidirectional bandwidth to the high end (green line),</div><div>and I&#39;m guessing this is TCP traffic and not RoCE.</div><div><br></div><div>     So by default the slower interface is being chosen on the low end, and</div><div>I don&#39;t think there are tunable parameters to allow me to choose the </div><div>QDR interface as the default.  Going forward we&#39;ll probably just disable </div><div>RoCE on these nodes and go with QDR IB plus 10 Gbps TCP for large messages.   </div><div><br></div><div>      However, I do think these issues will come up more in the future.</div><div>With the low latency of RoCE matching IB, there are more opportunities</div><div>to do channel bonding or allowing multiple interfaces for aggregate traffic</div><div>for even smaller message sizes. </div><span><font color="#888888"><div><br></div><div>                Dave Turner</div><div><div><br></div>-- <br><div><div dir="ltr">Work:     <a href="mailto:DaveTurner@ksu.edu" target="_blank">DaveTurner@ksu.edu</a>     <a href="tel:%28785%29%20532-7791" value="+17855327791" target="_blank">(785) 532-7791</a><div>             118 Nichols Hall, Manhattan KS  66502<br>Home:    <a href="mailto:DrDaveTurner@gmail.com" target="_blank">DrDaveTurner@gmail.com</a><br>              cell: <a href="tel:%28785%29%20770-5929" value="+17857705929" target="_blank">(785) 770-5929</a><br></div></div></div>
</div></font></span></div>
<br></div></div>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/02/16951.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/02/16951.php</a><br></blockquote></div><br></div><span><font color="#888888">
</font></span></blockquote></div><span><font color="#888888"><br><br clear="all"><span class="HOEnZb"><font color="#888888"><div><br></div>-- <br><div><div dir="ltr">Work:     <a href="mailto:DaveTurner@ksu.edu" target="_blank">DaveTurner@ksu.edu</a>     <a href="tel:%28785%29%20532-7791" value="+17855327791" target="_blank">(785) 532-7791</a><div>             118 Nichols Hall, Manhattan KS  66502<br>Home:    <a href="mailto:DrDaveTurner@gmail.com" target="_blank">DrDaveTurner@gmail.com</a><br>              cell: <a href="tel:%28785%29%20770-5929" value="+17857705929" target="_blank">(785) 770-5929</a><br></div></div></div>
</font></span></font></span></div><span class="HOEnZb"><font color="#888888">
</font></span></blockquote></div><span class="HOEnZb"><font color="#888888"><br><br clear="all"><div><br></div>-- <br><div><div dir="ltr">Work:     <a href="mailto:DaveTurner@ksu.edu" target="_blank">DaveTurner@ksu.edu</a>     <a href="tel:%28785%29%20532-7791" value="+17855327791" target="_blank">(785) 532-7791</a><div>             118 Nichols Hall, Manhattan KS  66502<br>Home:    <a href="mailto:DrDaveTurner@gmail.com" target="_blank">DrDaveTurner@gmail.com</a><br>              cell: <a href="tel:%28785%29%20770-5929" value="+17857705929" target="_blank">(785) 770-5929</a><br></div></div></div>
</font></span></div><span class="HOEnZb"><font color="#888888">
</font></span></div></div><span class="HOEnZb"><font color="#888888"><br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/02/16963.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/02/16963.php</a><br></font></span></blockquote></div><br></div>
<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/02/16965.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/02/16965.php</a><br></blockquote></div><br></div>

