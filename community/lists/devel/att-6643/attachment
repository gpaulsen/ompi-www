<html><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">The problem is that the two mpiruns don't know about each other, and therefore the second mpirun doesn't know that another mpirun has already used socket 0.<div><br></div><div>We hope to change that at some point in the future.</div><div><br></div><div>Ralph</div><div><br></div><div><br><div><div>On Aug 17, 2009, at 4:02 AM, Lenny Verkhovsky wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr"><div>In the multi job environment, can't we just start binding processes on the first avaliable and unused socket?</div><div>I mean first job/user will start binding itself from socket 0,&nbsp;</div><div>the next job/user will start binding itself from socket 2, for instance .</div> <div></div><div>Lenny.</div><br><div class="gmail_quote">On Mon, Aug 17, 2009 at 6:02 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;"> <div style="word-wrap:break-word"><br><div><div class="im"><div>On Aug 16, 2009, at 8:16 PM, Eugene Loh wrote:</div><br><blockquote type="cite"> <div bgcolor="#ffffff" text="#000000"> Chris Samuel wrote: <blockquote cite="http://mid3844979.1418711250471183148.JavaMail.root@mail.vpac.org" type="cite">  <pre>----- "Eugene Loh" <a href="mailto:Eugene.Loh@Sun.COM" target="_blank">&lt;Eugene.Loh@Sun.COM&gt;</a> wrote:
  </pre>  <blockquote type="cite">    <pre>This is an important discussion.
    </pre>  </blockquote>  <pre>Indeed! My big fear is that people won't pick up the significance
of the change and will complain about performance regressions
in the middle of an OMPI stable release cycle.</pre>  <blockquote type="cite">    <pre>2) The proposed OMPI bind-to-socket default is less severe. In the
general case, it would allow multiple jobs to bind in the same way
without oversubscribing any core or socket. (This comment added to
the trac ticket.)
    </pre>  </blockquote>  <pre>That's a nice clarification, thanks. I suspect though that the
same issue we have with MVAPICH would occur if two 4 core jobs
both bound themselves to the first socket.
  </pre> </blockquote> Okay, so let me point out a second distinction from MVAPICH:&nbsp; the default policy would be to spread out over sockets.<br> <br> Let's say you have two sockets, with four cores each.&nbsp; Let's say you submit two four-core jobs.&nbsp; The first job would put two processes on the first socket and two processes on the second.&nbsp; The second job would do the same.&nbsp; The loading would be even.<br> <br> I'm not saying there couldn't be problems.&nbsp; It's just that MVAPICH2 (at least what I looked at) has multiple shortfalls.&nbsp; The binding is to fill up one socket after another (which decreases memory bandwidth per process and increases chances of collisions with other jobs) and binding is to core (increasing chances of oversubscribing cores).&nbsp; The proposed OMPI behavior distributes over sockets (improving memory bandwidth per process and reducing collisions with other jobs) and binding is to sockets (reducing changes of oversubscribing cores, whether due to other MPI jobs or due to multithreaded processes).&nbsp; So, the proposed OMPI behavior mitigates the problems.<br> <br> It would be even better to have binding selections adapt to other bindings on the system.<br> <br> In any case, regardless of what the best behavior is, I appreciate the point about changing behavior in the middle of a stable release.&nbsp; Arguably, leaving significant performance on the table in typical situations is a bug that warrants fixing even in the middle of a release, but I won't try to settle that debate here.<br> </div></blockquote><div><br></div></div>I think the problem here, Eugene, is that performance benchmarks are far from the typical application. We have repeatedly seen this - optimizing for benchmarks frequently makes applications run less efficiently. So I concur with Chris on this one - let's not go -too- benchmark happy and hurt the regular users.</div> <div><br></div><div>Here at LANL, binding to-socket instead of to-core hurts performance by ~5-10%, depending on the specific application. Of course, either binding method is superior to no binding at all...</div><div><br> </div><div>UNLESS you have a threaded application, in which case -any- binding can be highly detrimental to performance.</div><div><br></div><div>So going slow on this makes sense. If we provide the capability, but leave it off by default, then people can test it against real applications and see the impact. Then we can better assess the right default settings.</div> <div><br></div><font color="#888888"><div>Ralph</div><div><br></div><div><br></div></font><div><blockquote type="cite"><div class="im"><div bgcolor="#ffffff" text="#000000"> <blockquote cite="http://mid3844979.1418711250471183148.JavaMail.root@mail.vpac.org" type="cite">  <blockquote type="cite">    <pre>3) Defaults (if I understand correctly) can be set differently
on each cluster.
    </pre>  </blockquote>  <pre>Yes, but the defaults should be sensible for the majority of
clusters.  If the majority do indeed share nodes between jobs
then I would suggest that the default should be off and the
minority who don't share nodes should have to enable it.
  </pre> </blockquote> In debates on this subject, I've heard people argue that:<br> <br> *) Though nodes are getting fatter, most are still thin.<br> <br> *) Resource managers tend to space share the cluster.<br> </div> </div><div class="im">  _______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></div> </blockquote></div><br></div><br>_______________________________________________<br> devel mailing list<br> <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br></blockquote></div><br></div> _______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/devel</blockquote></div><br></div></body></html>
