<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    Thanks for clarification. I will go via new btl module path.<br>
    I used -btl self,tcp in past, to get things working (when dealing
    with exec and fork problems). So at the moment, Open MPI runs fine,
    we were able to run some test jobs, to get some preliminary
    performance measurements etc. Only the cleanup part was still
    problematic (and I lack(ed) the Open MPI knowledge to be able to
    understand, how _should_ be things working).<br>
    <br>
    BR Justin<br>
    <br>
    <div class="moz-cite-prefix">On 16. 12. 2015 12:20, Gilles
      Gouaillardet wrote:<br>
    </div>
    <blockquote
cite="mid:CAAkFZ5v6DcT_yh-M191Pnn8pcodN7paMjw3zayJg3dCj76_F=g@mail.gmail.com"
      type="cite">Justin,
      <div><br>
      </div>
      <div>knem allows a process to write into the address space of an
        other process, to do zero copy.</div>
      <div>in the case of osv, threads can simply do a memcpy(), and I
        doubt knew is even available.</div>
      <div>so a new btl that uses memcpy would be optimal on osv.</div>
      <div><br>
      </div>
      <div>one option is to starts from the vader btl, and replace knem
        invocation with memcpy()</div>
      <div>an other option could be to extend the self btl</div>
      <div><br>
      </div>
      <div>but once again, this is for performance only, using tcp btl
        only should be enough to get things work.</div>
      <div><br>
      </div>
      <div>Cheers,</div>
      <div><br>
      </div>
      <div>Gilles<br>
        <br>
        On Wednesday, December 16, 2015, Justin Cinkelj &lt;<a
          moz-do-not-send="true" href="mailto:justin.cinkelj@xlab.si"><a class="moz-txt-link-abbreviated" href="mailto:justin.cinkelj@xlab.si">justin.cinkelj@xlab.si</a></a>&gt;
        wrote:<br>
        <blockquote class="gmail_quote" style="margin:0 0 0
          .8ex;border-left:1px #ccc solid;padding-left:1ex">
          <div bgcolor="#FFFFFF" text="#000000"> Vader is for intra-node
            communication only, right? So for inter-node communication
            some other mechanism will be used anyway.<br>
            Why would be even better to write a new btl? To avoid memcpy
            (knem would use it, if I understand you correctly; I guess
            code assumes that multiple processes on same node have
            isolated address spaces).<br>
            <br>
            Fork + execve was one of first problems, yes. I replaced
            that with OSv specific calls (ignore fork, and instead of
            execve start given binary in new thread). The global
            variables required OSv modification - the guys from <a
              moz-do-not-send="true" href="http://osv.io/"
              target="_blank"><a class="moz-txt-link-freetext" href="http://osv.io/">http://osv.io/</a></a> took care of that (I
            was surprised that at the end, the patches were really small
            and elegant). So while there are no real processes, new
            binary / ELF file is loaded at different address then the
            rest of OS - so it has separate global variables, and
            separate environ too. Other resources like file descriptors
            are still shared.<br>
            <br>
            BR Justin<br>
            <br>
            <div>On 15. 12. 2015 14:55, Gilles Gouaillardet wrote:<br>
            </div>
            <blockquote type="cite">Justin,
              <div><br>
              </div>
              <div>at first glance, vader should be symmetric (e.g.
                call opal_shmem_segment_dettach() instead of munmap()</div>
              <div>Nathan, can you please comment ?</div>
              <div><br>
              </div>
              <div>using tid instead of pid should also do the trick</div>
              <div><br>
              </div>
              <div>that being said, a more elegant approach would be to
                create a new module in the shmem framework</div>
              <div>basically, create = malloc, attach = return the
                malloc'ed address, detach = noop, destroy = free</div>
              <div><br>
              </div>
              <div>and an even better approach would be to write your
                own btl that replaces vader.</div>
              <div>basically, vader can use the knem module to write
                into an other process address space.</div>
              <div>since your os is thread only, knem invocation would
                become a simple memcpy.</div>
              <div><br>
              </div>
              <div>makes sense ?</div>
              <div><br>
              </div>
              <div><br>
              </div>
              <div>as a side note,</div>
              <div>ompi uses global variables, and orted forks and exec
                MPI tasks after setting some environment variables. it
                seems porting ompi to this new os was not so painful,
                and I would have expected some issues with the global
                variables, and some race conditions with the
                environment.</div>
              <div>did you already solve these issues ?</div>
              <div><br>
              </div>
              <div>Cheers,</div>
              <div><br>
              </div>
              <div>Gilles<br>
                <br>
                On Tuesday, December 15, 2015, Justin Cinkelj &lt;<a
                  moz-do-not-send="true"
                  href="javascript:_e(%7B%7D,'cvml','justin.cinkelj@xlab.si');"
                  target="_blank"><a class="moz-txt-link-abbreviated" href="mailto:justin.cinkelj@xlab.si">justin.cinkelj@xlab.si</a></a>&gt; wrote:<br>
                <blockquote class="gmail_quote" style="margin:0 0 0
                  .8ex;border-left:1px #ccc solid;padding-left:1ex">I'm
                  trying to port Open MPI to OS with threads instead of
                  processes. Currently, during MPI_Finalize, I get
                  attempt to call munmap first with address of
                  0x200000c00000 and later 0x200000c00008.<br>
                  <br>
                  mca_btl_vader_component_close():<br>
                  munmap (mca_btl_vader_component.my_segment,
                  mca_btl_vader_component.segment_size)<br>
                  <br>
                  mca_btl_vader_component_init():<br>
                  if(MCA_BTL_VADER_XPMEM !=
                  mca_btl_vader_component.single_copy_mechanism) {<br>
                    opal_shmem_segment_create
                  (&amp;component-&gt;seg_ds, sm_file,
                  component-&gt;segment_size);<br>
                    component-&gt;my_segment = opal_shmem_segment_attach
                  (&amp;component-&gt;seg_ds);<br>
                  } else {<br>
                    mmap (NULL, component-&gt;segment_size, PROT_READ |
                  PROT_WRITE, MAP_ANONYMOUS | MAP_SHARED, -1, 0);<br>
                  }<br>
                  <br>
                  But opal_shmem_segment_attach (from mmap module) ends
                  with:<br>
                      /* update returned base pointer with an offset
                  that hides our stuff */<br>
                      return (ds_buf-&gt;seg_base_addr +
                  sizeof(opal_shmem_seg_hdr_t));<br>
                  <br>
                  So mca_btl_vader_component_close() should in that case
                  call opal_shmem_segment_dettach() instead of munmap.<br>
                  Or actually, as at that point shmem_mmap module
                  cleanup code is already done, vader could/should just
                  skip cleanup part?<br>
                  <br>
                  Maybe I should ask first how does that setup/cleanup
                  work on normal Linux system?<br>
                  Is mmap called twice, and vader and shmem_mmap module
                  each uses different address (so vader munmap is indeed
                  required in that case)?<br>
                  <br>
                  Second question.<br>
                  With two threads in one process, I got attempt to
                  opal_shmem_segment_dettach() and munmap() on same
                  mmap-ed address, from both threads. I 'fixed' that by
                  replacing "ds_buf-&gt;seg_cpid = getpid()" with
                  gettid(), and then each thread munmap-s only address
                  allocated by itself. Is that correct? Or is it
                  possible, that the second thread might still try to
                  access data at that address?<br>
                  <br>
                  BR Justin<br>
                  <br>
                  _______________________________________________<br>
                  devel mailing list<br>
                  <a moz-do-not-send="true">devel@open-mpi.org</a><br>
                  Subscription: <a moz-do-not-send="true"
                    href="http://www.open-mpi.org/mailman/listinfo.cgi/devel"
                    target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
                  Link to this post: <a moz-do-not-send="true"
                    href="http://www.open-mpi.org/community/lists/devel/2015/12/18417.php"
                    target="_blank">http://www.open-mpi.org/community/lists/devel/2015/12/18417.php</a><br>
                </blockquote>
              </div>
              <br>
              <fieldset></fieldset>
              <br>
              <pre>_______________________________________________
devel mailing list
<a moz-do-not-send="true" href="javascript:_e(%7B%7D,'cvml','devel@open-mpi.org');" target="_blank">devel@open-mpi.org</a>
Subscription: <a moz-do-not-send="true" href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a>
Link to this post: <a moz-do-not-send="true" href="http://www.open-mpi.org/community/lists/devel/2015/12/18418.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/12/18418.php</a></pre>
            </blockquote>
            <br>
          </div>
        </blockquote>
      </div>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
devel mailing list
<a class="moz-txt-link-abbreviated" href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/devel/2015/12/18427.php">http://www.open-mpi.org/community/lists/devel/2015/12/18427.php</a></pre>
    </blockquote>
    <br>
  </body>
</html>

