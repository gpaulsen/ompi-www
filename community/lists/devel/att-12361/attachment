<html><head><meta http-equiv="Content-Type" content="text/html charset=iso-8859-1"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">We are working towards thread safety, but nowhere near ready yet.&nbsp;<div><br><div><div>On May 6, 2013, at 3:39 AM, Hugo Daniel Meyer &lt;<a href="mailto:meyer.hugo@gmail.com">meyer.hugo@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr">Sorry, i've sent the message without finishing it.<div><br></div><div><span style="font-family:arial,sans-serif;font-size:13px">Hello to @ll.</span><div style="font-family:arial,sans-serif;font-size:13px">
<br></div><div style="font-family:arial,sans-serif;font-size:13px">I'm not sure if this is the correct list to post this question, but maybe i'm dealing with a bug.</div><div style="font-family:arial,sans-serif;font-size:13px">
<br></div><div style="font-family:arial,sans-serif;font-size:13px">I have develop an event logging mechanism where application processes connect to event loggers (using MPI_Lookup, MPI_open_port, MPI_Comm_Connect, MPI_Comm_Accept, etc) that are part of another MPI application.</div>
<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">Well, i have develop my own vprotocol where once a process receive a message try to establish a connection with an event logger which is a thread that belongs to another mpi application.&nbsp;</div>
<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">The event logger application consists in one mpi process per node with multiple threads waiting for connections of MPI processes from the main application.&nbsp;</div>
<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">I'm suspecting that there is a problem with the critical regions when processes try to connect with the threads of the event logger.&nbsp;</div>
<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">I'm attaching two short examples that i have written in order to show the problem. First, i launch the event-logger application:</div>
<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div><div><font face="arial, sans-serif">mpirun -n 2 --machinefile machinefile2-th --report-uri URIFILE ./test-thread</font></div><div><br></div><div style="">
Then i launch the example as this:</div><div style=""><br></div><div style="">mpirun -n 16 --machinefile machine16 --ompi-server file:URIFILE ./thread_logger_connect<br></div><div style=""><br></div><div style="">I have obtained this output:</div>
<div style=""><br></div><div style=""><div><i><font color="#0000ff">Published: radic_eventlog[1,6], ret=0</font></i></div><div><i><font color="#0000ff">[clus2:16104] [[39125,1],1] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file dpm_orte.c at line 315</font></i></div>
<div><i><font color="#0000ff">[clus2:16104] [[39125,1],1] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file dpm_orte.c at line 315</font></i></div><div><i><font color="#0000ff">[clus2:16104] *** An error occurred in MPI_Comm_accept</font></i></div>
<div><i><font color="#0000ff">[clus2:16104] *** on communicator MPI_COMM_SELF</font></i></div><div><i><font color="#0000ff">[clus2:16104] *** MPI_ERR_UNKNOWN: unknown error</font></i></div><div><i><font color="#0000ff">[clus2:16104] *** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)</font></i></div>
<div><i><font color="#0000ff">--------------------------------------------------------------------------</font></i></div><div><i><font color="#0000ff">mpirun has exited due to process rank 1 with PID 16104 on</font></i></div>
<div><i><font color="#0000ff">node clus2 exiting improperly. There are two reasons this could occur:</font></i></div><div><i><font color="#0000ff"><br></font></i></div><div><i><font color="#0000ff">1. this process did not call "init" before exiting, but others in</font></i></div>
<div><i><font color="#0000ff">the job did. This can cause a job to hang indefinitely while it waits</font></i></div><div><i><font color="#0000ff">for all processes to call "init". By rule, if one process calls "init",</font></i></div>
<div><i><font color="#0000ff">then ALL processes must call "init" prior to termination.</font></i></div><div><i><font color="#0000ff"><br></font></i></div><div><i><font color="#0000ff">2. this process called "init", but exited without calling "finalize".</font></i></div>
<div><i><font color="#0000ff">By rule, all processes that call "init" MUST call "finalize" prior to</font></i></div><div><i><font color="#0000ff">exiting or it will be considered an "abnormal termination"</font></i></div>
<div><i><font color="#0000ff"><br></font></i></div><div><i><font color="#0000ff">This may have caused other processes in the application to be</font></i></div><div><i><font color="#0000ff">terminated by signals sent by mpirun (as reported here).</font></i></div>
<div><br></div><div style=""><br></div></div></div><div style="font-family:arial,sans-serif;font-size:13px">If i use mutex in order to serialized the access to MPI_Comm_Accept, the behavior is ok, but shoudn't the MPI_comm_accept be thread safe?</div>
</div><div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">Best regards.</div><div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">
Hugo Meyer</div><div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">P.d.: This occurs with openmpi1.5.1 and also with also with an old version of the trunk (1.7).</div>
</div><div class="gmail_extra"><br><br><div class="gmail_quote">2013/5/6 Hugo Daniel Meyer <span dir="ltr">&lt;<a href="mailto:meyer.hugo@gmail.com" target="_blank">meyer.hugo@gmail.com</a>&gt;</span><br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div dir="ltr">Hello to @ll.<div><br></div><div>I'm not sure if this is the correct list to post this question, but maybe i'm dealing with a bug.</div><div><br></div><div>I have develop an event logging mechanism where application processes connect to event loggers (using MPI_Lookup, MPI_open_port, MPI_Comm_Connect, MPI_Comm_Accept, etc) that are part of another MPI application.</div>


<div><br></div><div>Well, i have develop my own vprotocol where once a process receive a message try to establish a connection with an event logger which is a thread that belongs to another mpi application.&nbsp;</div>
<div><br></div><div>The event logger application consists in one mpi process per node with multiple threads waiting for connections of MPI processes from the main application.&nbsp;</div><div><br></div><div>I'm suspecting that there is a problem with the critical regions when processes try to connect with the threads of the event logger.&nbsp;</div>

<div><br></div><div>I'm attaching two short examples that i have written in order to show the problem. First, i launch the event-logger application:</div><div><br></div><div><br></div><div><br>
</div><div>If i use mutex in order to serialized the access to MPI_Comm_Accept,</div><div><br></div>
<div><br></div><div><br></div></div>
</blockquote></div><br></div>
<span>&lt;event_logger.c&gt;</span><span>&lt;main-mpi-app.c&gt;</span>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/devel</blockquote></div><br></div></body></html>
