Michael,<div><br></div><div>byte swapping only occurs if you invoke MPI_Pack_external and MPI_Unpack_external on little endianness systems.</div><div><br></div><div>MPI_Pack and MPI_Unpack uses the same engine that MPI_Send and MPI_Recv and this does not involve any byte swapping if both ends have the same endianness.</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<br><br>On Friday, February 12, 2016, Michael Rezny &lt;<a href="mailto:michael.rezny@monash.edu">michael.rezny@monash.edu</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">Hi,<div>oh, that is good news! The process is meant to be implementing &quot;receiver makes right&quot; which is good news for efficiency.</div><div><br></div><div>But, in the second case, without --enable-heterogeneous, are you saying that on little-endian machines, byte swapping</div><div>is meant to always occur? That seems most odd. I would have thought that if one only wants to work and then to configure</div><div>OpenMPI for this mode, then there is no need to check at the receiving end whether byte-swapping is needed or not. It will be assumed</div><div>that both sender and receiver are agreed on the format, whatever it is. On a homogeneous little-endian HPC cluster one would not want</div><div>the extra overhead of two conversions for every packed message.</div><div><br></div><div>Is it possible that the assert has been implemented incorrectly in this case?</div><div><br></div><div>There is absolutely no urgency with regard to a fix. Thanks to your quick response, we now understand what is causing</div><div>the problem and are in the process of implementing a test in ./configure to determine if the bug is present, and if so,</div><div>add a compiler flag to switch to using MPI_Pack and MPI_Unpack.</div><div><br></div><div>It would be good if you would be kind enough to let me know when a fix is available and I will download, build,</div><div>and test it on our application. Then this version can be installed as the default.</div><div><br></div><div>Once again, many thanks for your prompt and most helpful responses.</div><div><br></div><div>warmest regards</div><div>MIke</div><div><br></div><div><div><div>On 12/02/2016, at 7:03 PM, Gilles Gouaillardet wrote:</div><br><blockquote type="cite">
  
    
  
  <div bgcolor="#FFFFFF" text="#000000">
    Michael,<br>
    <br>
    i&#39;d like to correct what i wrote earlier<br>
    <br>
    in heterogeneous clusters, data is sent &quot;as is&quot; (e.g. no byte
    swapping) and it is byte swapped when received and only if needed.<br>
    <br>
    with --enable-heterogeneous, MPI_Unpack_external is working, but
    MPI_Pack_external is broken<br>
    (e.g. no byte swapping occurs on little endian arch) since we
    internall use the similar mechanism used to send data. that is a bug
    and i will work on that.<br>
    <br>
    without --enable-heterogeneous, MPI_Pack_external nor
    MPI_Unpack_external do any byte swapping and they<br>
    are both broken. fwiw, it you configure&#39;d with --enable-debug, you
    would have ran into an assert error (e.g. crash).<br>
    <br>
    i will work on a fix, but it might take some time before it is ready<br>
    <br>
    Cheers,<br>
    <br>
    Gilles<br>
    <div>On 2/11/2016 6:16 PM, Gilles
      Gouaillardet wrote:<br>
    </div>
    <blockquote type="cite">Michael,
      <div><br>
      </div>
      <div>MPI_Pack_external must convert data to big endian, so it can
        be dumped into a file, and be read correctly on big and little
        endianness arch, and with any MPI flavor.</div>
      <div><br>
      </div>
      <div>if you use only one MPI library on one arch, or if data is
        never read/written from/to a file, then it is more efficient to
        MPI_Pack.</div>
      <div><br>
      </div>
      <div>openmpi is optimized and the data is swapped only when
        needed.</div>
      <div>so if your cluster is little endian only, MPI_Send and
        MPI_Recv will never byte swap data internally.</div>
      if both ends have different endianness, data is sent in big endian
      format and byte swapped when received only if needed.
      <div>generally speaking, a send/recv requires zero or one byte
        swap.</div>
      <div><br>
      </div>
      <div>fwiw, we previously had a claim that debian nor Ubuntu have a
        maintainer for openmpi, which would explain why an obsolete
        version is shipped. I made a few researchs and could not find
        any evidence openmpi is no more maintained.</div>
      <div><br>
      </div>
      <div>Cheers,</div>
      <div><br>
      </div>
      <div>Gilles<br>
        <div><br>
        </div>
        <div><br>
          <div><br>
            On Thursday, February 11, 2016, Michael Rezny &lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;michael.rezny@monash.edu&#39;);" target="_blank"></a><a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;michael.rezny@monash.edu&#39;);" target="_blank">michael.rezny@monash.edu</a>&gt;
            wrote:<br>
            <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
              <div style="word-wrap:break-word">Hi Gilles,
                <div>thanks for thinking about this in more detail.</div>
                <div>
                  <div><br>
                  </div>
                  <div>I understand what you are saying, but your
                    comments raise some questions in my mind:</div>
                  <div><br>
                  </div>
                  <div>If one is in a homogeneous cluster, is it
                    important that, in the case of little-endian, that
                    the data be</div>
                  <div>converted to extern32 format (big-endian), only
                    to be always converted at the receiving rank</div>
                  <div>back to little-endian?</div>
                  <div><br>
                  </div>
                  <div>This would seem to be inefficient, especially if
                    the site has no need for external MPI access.</div>
                  <div><br>
                  </div>
                  <div>So, does --enable-heterogeneous do more than put
                    MPI routines using &quot;extern32&quot; into straight
                    pass-through?</div>
                  <div><br>
                  </div>
                  <div>Back in the old days of PVM, all messages were
                    converted into network order. This had severe
                    performance impacts</div>
                  <div>on little-endian clusters.</div>
                  <div><br>
                  </div>
                  <div>So much so that a clever way of getting around
                    this was an implementation of &quot;receiver makes right&quot;
                    in which</div>
                  <div>all data was sent in the native format of the
                    sending rank. The receiving rank analysed the
                    message to determine if</div>
                  <div>a conversion was necessary. In those days with
                    Cray format data, it could be more complicated than
                    just byte swapping.</div>
                  <div><br>
                  </div>
                  <div>So in essence, how is a balance struck between
                    supporting heterogenous architectures and maximum
                    performance</div>
                  <div>with codes where message passing performance is
                    critical?</div>
                  <div><br>
                  </div>
                  <div>As a follow up, since I am now at home, this same
                    problem also exists with the Ubuntu 15.10 OpenMP
                    packages</div>
                  <div>which surprisingly are still at 1.6.5, same as
                    14.04.</div>
                  <div><br>
                  </div>
                  <div>Again, downloading, building, and using the
                    latest stable version of OpenMP solved the problem.</div>
                  <div><br>
                  </div>
                  <div>kindest regards</div>
                  <div>Mike</div>
                  <div><br>
                  </div>
                  <div><br>
                  </div>
                  <div>
                    <div>
                      <div>On 11/02/2016, at 7:31 PM, Gilles
                        Gouaillardet wrote:</div>
                      <br>
                      <blockquote type="cite">Michael,
                        <div><br>
                        </div>
                        <div>I think it is worst than that ...</div>
                        <div><br>
                        </div>
                        <div>without --enable-heterogeneous, it seems
                          the data is not correctly packed</div>
                        <div>(e.g. it is not converted to big endian),
                          at least on a x86_64 arch.</div>
                        <div>unpack looks broken too, but pack followed
                          by unpack does work.</div>
                        <div>that means if you are reading data
                          correctly written in external32e format,</div>
                        <div>it will not be correctly unpacked.</div>
                        <div><br>
                        </div>
                        <div>with --enable-heterogeneous, it is only
                          half broken</div>
                        <div>(I do not know yet whether pack or unpack
                          is broken ...)</div>
                        <div>and pack followed by unpack does not work.</div>
                        <div><br>
                        </div>
                        <div>I will double check that tomorrow</div>
                        <div><br>
                        </div>
                        <div>Cheers,</div>
                        <div><br>
                        </div>
                        <div>Gilles<br>
                          <br>
                          On Thursday, February 11, 2016, Michael Rezny
                          &lt;<a>michael.rezny@monash.edu</a>&gt;
                          wrote:<br>
                          <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
                            <div dir="ltr">
                              <div>
                                <div>
                                  <div>
                                    <div>
                                      <div>
                                        <div>
                                          <div>
                                            <div>
                                              <div>Hi Ralph,<br>
                                              </div>
                                              you are indeed correct.
                                              However, many of our users<br>
                                            </div>
                                            have workstations such as
                                            me, with OpenMPI provided by
                                            installing a package.<br>
                                          </div>
                                          <div>So we don&#39;t know what has
                                            been configured.<br>
                                          </div>
                                          <div><br>
                                          </div>
                                          Then we have failures, since,
                                          for instance, Ubuntu 14.04 by
                                          default appears to have been
                                          built<br>
                                        </div>
                                        with heterogeneous support! The
                                        other (working) machine is a
                                        large HPC, and it seems OpenMPI
                                        was built<br>
                                      </div>
                                      <div>without heterogeneous
                                        support.<br>
                                      </div>
                                      <div><br>
                                      </div>
                                      Currently we work around the
                                      problem for packing and unpacking
                                      by having a compiler switch<br>
                                    </div>
                                    that will switch between calls to
                                    pack/unpack_external and pac/unpack.<br>
                                    <br>
                                  </div>
                                  It is only now we started to track
                                  down what the problem actually is.<br>
                                  <br>
                                </div>
                                kindest regards<br>
                              </div>
                              Mike<br>
                            </div>
                            <div class="gmail_extra"><br>
                              <div class="gmail_quote">On 11 February
                                2016 at 15:54, Ralph Castain <span dir="ltr">&lt;<a></a><a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;rhc@open-mpi.org&#39;);" target="_blank">rhc@open-mpi.org</a>&gt;</span>
                                wrote:<br>
                                <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
                                  <div style="word-wrap:break-word">Out
                                    of curiosity: if both systems are
                                    Intel, they why are you enabling
                                    hetero? You don’t need it in that
                                    scenario.
                                    <div><br>
                                    </div>
                                    <div>Admittedly, we do need to fix
                                      the bug - just trying to
                                      understand why you are configuring
                                      that way.</div>
                                    <div><br>
                                    </div>
                                    <div><br>
                                      <div>
                                        <blockquote type="cite">
                                          <div>On Feb 10, 2016, at 8:46
                                            PM, Michael Rezny &lt;<a></a><a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;michael.rezny@monash.edu&#39;);" target="_blank">michael.rezny@monash.edu</a>&gt;
                                            wrote:</div>
                                          <br>
                                          <div>
                                            <div dir="ltr">
                                              <div>
                                                <div>
                                                  <div>
                                                    <div>
                                                      <div>
                                                        <div>Hi Gilles,<br>
                                                        </div>
                                                        I can confirm
                                                        that with a
                                                        fresh download
                                                        and build from
                                                        source for
                                                        OpenMPI 1.10.2<br>
                                                      </div>
                                                      <div>with
                                                        --enable-heterogeneous<br>
                                                      </div>
                                                      the unpacked ints
                                                      are the wrong
                                                      endian.<br>
                                                      <br>
                                                    </div>
                                                    However, without
                                                    --enable-heterogeneous,
                                                    the unpacked ints
                                                    are correct.<br>
                                                    <br>
                                                  </div>
                                                  So, this problem still
                                                  exists in
                                                  heterogeneous builds
                                                  with OpenMPI version
                                                  1.10.2.<br>
                                                  <br>
                                                </div>
                                                kindest regards<br>
                                              </div>
                                              Mike<br>
                                            </div>
                                            <div class="gmail_extra"><br>
                                              <div class="gmail_quote">On
                                                11 February 2016 at
                                                14:48, Gilles
                                                Gouaillardet <span dir="ltr">&lt;<a></a><a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;gilles.gouaillardet@gmail.com&#39;);" target="_blank">gilles.gouaillardet@gmail.com</a>&gt;</span>
                                                wrote:<br>
                                                <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Michael,
                                                  <div><br>
                                                  </div>
                                                  <div>does your two
                                                    systems have the
                                                    same endianness ?</div>
                                                  <div><br>
                                                  </div>
                                                  <div>do you know how
                                                    openmpi was
                                                    configure&#39;d on both
                                                    systems ?</div>
                                                  <div>(is
                                                    --enable-heterogeneous
                                                    enabled or disabled
                                                    on both systems ?)</div>
                                                  <div><br>
                                                  </div>
                                                  <div>fwiw, openmpi
                                                    1.6.5 is old now and
                                                    no more maintained.</div>
                                                  <div>I strongly
                                                    encourage you to use
                                                    openmpi 1.10.2</div>
                                                  <div><br>
                                                  </div>
                                                  <div>Cheers,</div>
                                                  <div><br>
                                                  </div>
                                                  <div>Gilles</div>
                                                  <div><br>
                                                    On Thursday,
                                                    February 11, 2016,
                                                    Michael Rezny &lt;<a></a><a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;michael.rezny@monash.edu&#39;);" target="_blank">michael.rezny@monash.edu</a>&gt; wrote:<br>
                                                    <blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
                                                      <div dir="ltr">
                                                        <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>
                                                          <div>Hi,<br>
                                                          </div>
                                                          I am running
                                                          Ubuntu 14.04
                                                          LTS with
                                                          OpenMPI 1.6.5
                                                          and gcc 4.8.4<br>
                                                          <br>
                                                          </div>
                                                          On a single
                                                          rank program
                                                          which just
                                                          packs and
                                                          unpacks two
                                                          ints using
                                                          MPI_Pack_external
                                                          and
                                                          MPI_Unpack_external<br>
                                                          </div>
                                                          the unpacked
                                                          ints are in
                                                          the wrong
                                                          endian order.<br>
                                                          <br>
                                                          </div>
                                                          However, on a
                                                          HPC, (not
                                                          Ubuntu), using
                                                          OpenMPI 1.6.5
                                                          and gcc 4.8.4
                                                          the unpacked
                                                          ints are
                                                          correct.<br>
                                                          <br>
                                                          </div>
                                                          <div>Is it
                                                          possible to
                                                          get some
                                                          assistance to
                                                          track down
                                                          what is going
                                                          on?<br>
                                                          </div>
                                                          <div><br>
                                                          </div>
                                                          Here is the
                                                          output from
                                                          the program:<br>
                                                          <br>
                                                           <span style="font-family:monospace,monospace">~/tests/mpi/Pack
                                                          test1<br>
                                                          send data
                                                          000004d2
                                                          0000162e <br>
                                                          MPI_Pack_external:
                                                          0<br>
                                                          buffer size: 8<br>
                                                          MPI_unpack_external:
                                                          0<br>
                                                          recv data
                                                          d2040000
                                                          2e160000 </span><br>
                                                          <br>
                                                        </div>
                                                        And here is the
                                                        source code:<br>
                                                        <br>
                                                        <span style="font-family:monospace,monospace">#include
&lt;stdio.h&gt;<br>
                                                          #include
                                                          &lt;mpi.h&gt;<br>
                                                          <br>
                                                          int main(int
                                                          argc, char
                                                          *argv[]) {<br>
                                                            int
                                                          numRanks,
                                                          myRank, error;<br>
                                                          <br>
                                                            int
                                                          send_data[2] =
                                                          {1234, 5678};<br>
                                                            int
                                                          recv_data[2];<br>
                                                          <br>
                                                            MPI_Aint
                                                          buffer_size =
                                                          1000;<br>
                                                            char
                                                          buffer[buffer_size];<br>
                                                          <br>
                                                           
                                                          MPI_Init(&amp;argc,
                                                          &amp;argv);<br>
                                                           
                                                          MPI_Comm_size(MPI_COMM_WORLD,
&amp;numRanks);<br>
                                                           
                                                          MPI_Comm_rank(MPI_COMM_WORLD,
                                                          &amp;myRank);<br>
                                                          <br>
                                                            printf(&quot;send
                                                          data %08x %08x
                                                          \n&quot;,
                                                          send_data[0],
                                                          send_data[1]);<br>
                                                          <br>
                                                            MPI_Aint
                                                          position = 0;<br>
                                                            error =
                                                          MPI_Pack_external(&quot;external32&quot;,
                                                          (void*)
                                                          send_data, 2,
                                                          MPI_INT,<br>
                                                                   
                                                          buffer,
                                                          buffer_size,
                                                          &amp;position);<br>
                                                           
                                                          printf(&quot;MPI_Pack_external:
                                                          %d\n&quot;, error);<br>
                                                          <br>
                                                           
                                                          printf(&quot;buffer
                                                          size: %d\n&quot;,
                                                          (int)
                                                          position);<br>
                                                          <br>
                                                            position =
                                                          0;<br>
                                                            error =
                                                          MPI_Unpack_external(&quot;external32&quot;,
                                                          buffer,
                                                          buffer_size,
                                                          &amp;position,<br>
                                                                   
                                                          recv_data, 2,
                                                          MPI_INT);<br>
                                                           
                                                          printf(&quot;MPI_unpack_external:
                                                          %d\n&quot;, error);<br>
                                                          <br>
                                                            printf(&quot;recv
                                                          data %08x %08x
                                                          \n&quot;,
                                                          recv_data[0],
                                                          recv_data[1]);<br>
                                                          <br>
                                                           
                                                          MPI_Finalize();<br>
                                                          <br>
                                                            return 0;<br>
                                                          }<br>
                                                        </span><br>
                                                        <br>
                                                      </div>
                                                    </blockquote>
                                                  </div>
                                                  <br>
_______________________________________________<br>
                                                  devel mailing list<br>
                                                  <a></a><a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;devel@open-mpi.org&#39;);" target="_blank">devel@open-mpi.org</a><br>
                                                  Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank"></a><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
                                                  Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/02/18573.php" rel="noreferrer" target="_blank"></a><a href="http://www.open-mpi.org/community/lists/devel/2016/02/18573.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/02/18573.php</a><br>
                                                </blockquote>
                                              </div>
                                              <br>
                                            </div>
_______________________________________________<br>
                                            devel mailing list<br>
                                            <a>devel@open-mpi.org</a><br>
                                            Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank"></a><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
                                            Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/02/18575.php" target="_blank"></a><a href="http://www.open-mpi.org/community/lists/devel/2016/02/18575.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/02/18575.php</a></div>
                                        </blockquote>
                                      </div>
                                      <br>
                                    </div>
                                  </div>
                                  <br>
_______________________________________________<br>
                                  devel mailing list<br>
                                  <a>devel@open-mpi.org</a><br>
                                  Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank"></a><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
                                  Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/02/18576.php" rel="noreferrer" target="_blank"></a><a href="http://www.open-mpi.org/community/lists/devel/2016/02/18576.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/02/18576.php</a><br>
                                </blockquote>
                              </div>
                              <br>
                            </div>
                          </blockquote>
                        </div>
                        _______________________________________________<br>
                        devel mailing list<br>
                        <a>devel@open-mpi.org</a><br>
                        Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
                        Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/02/18579.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/02/18579.php</a></blockquote>
                    </div>
                    <br>
                  </div>
                </div>
              </div>
            </blockquote>
          </div>
        </div>
      </div>
      <br>
      <fieldset></fieldset>
      <br>
      <pre>_______________________________________________
devel mailing list
<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;devel@open-mpi.org&#39;);" target="_blank">devel@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/02/18582.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/02/18582.php</a></pre>
    </blockquote>
    <br>
  </div>

_______________________________________________<br>devel mailing list<br><a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;devel@open-mpi.org&#39;);" target="_blank">devel@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/02/18591.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/02/18591.php</a></blockquote></div><br></div></div></blockquote></div>

