<html xmlns:v="urn:schemas-microsoft-com:vml" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:w="urn:schemas-microsoft-com:office:word" xmlns:m="http://schemas.microsoft.com/office/2004/12/omml" xmlns="http://www.w3.org/TR/REC-html40"><head><meta http-equiv=Content-Type content="text/html; charset=us-ascii"><meta name=Generator content="Microsoft Word 14 (filtered medium)"><style><!--
/* Font Definitions */
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:Tahoma;
	panose-1:2 11 6 4 3 5 4 4 2 4;}
@font-face
	{font-family:Consolas;
	panose-1:2 11 6 9 2 2 4 3 2 4;}
/* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin:0in;
	margin-bottom:.0001pt;
	font-size:12.0pt;
	font-family:"Times New Roman","serif";
	color:black;}
a:link, span.MsoHyperlink
	{mso-style-priority:99;
	color:blue;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{mso-style-priority:99;
	color:purple;
	text-decoration:underline;}
pre
	{mso-style-priority:99;
	mso-style-link:"HTML Preformatted Char";
	margin:0in;
	margin-bottom:.0001pt;
	font-size:10.0pt;
	font-family:"Courier New";
	color:black;}
p.MsoAcetate, li.MsoAcetate, div.MsoAcetate
	{mso-style-priority:99;
	mso-style-link:"Balloon Text Char";
	margin:0in;
	margin-bottom:.0001pt;
	font-size:8.0pt;
	font-family:"Tahoma","sans-serif";
	color:black;}
span.HTMLPreformattedChar
	{mso-style-name:"HTML Preformatted Char";
	mso-style-priority:99;
	mso-style-link:"HTML Preformatted";
	font-family:Consolas;
	color:black;}
span.EmailStyle19
	{mso-style-type:personal-reply;
	font-family:"Calibri","sans-serif";
	color:#1F497D;}
span.BalloonTextChar
	{mso-style-name:"Balloon Text Char";
	mso-style-priority:99;
	mso-style-link:"Balloon Text";
	font-family:"Tahoma","sans-serif";
	color:black;}
.MsoChpDefault
	{mso-style-type:export-only;
	font-size:10.0pt;}
@page WordSection1
	{size:8.5in 11.0in;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
	{page:WordSection1;}
--></style><!--[if gte mso 9]><xml>
<o:shapedefaults v:ext="edit" spidmax="1026" />
</xml><![endif]--><!--[if gte mso 9]><xml>
<o:shapelayout v:ext="edit">
<o:idmap v:ext="edit" data="1" />
</o:shapelayout></xml><![endif]--></head><body bgcolor=white lang=EN-US link=blue vlink=purple><div class=WordSection1><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>Yes, the step outlined in your second bullet is no longer necessary.&nbsp; <o:p></o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>Rolf<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><div style='border:none;border-left:solid blue 1.5pt;padding:0in 0in 0in 4.0pt'><div><div style='border:none;border-top:solid #B5C4DF 1.0pt;padding:3.0pt 0in 0in 0in'><p class=MsoNormal><b><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif";color:windowtext'>From:</span></b><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif";color:windowtext'> devel-bounces@open-mpi.org [mailto:devel-bounces@open-mpi.org] <b>On Behalf Of </b>Sebastian Rinke<br><b>Sent:</b> Tuesday, January 17, 2012 5:22 PM<br><b>To:</b> Open MPI Developers<br><b>Subject:</b> Re: [OMPI devel] GPUDirect v1 issues<o:p></o:p></span></p></div></div><p class=MsoNormal><o:p>&nbsp;</o:p></p><p class=MsoNormal>Thank you very much. I will try setting the environment variable and if required also use the 4.1 RC2 version.<br><br>To clarify things a little bit for me, to set up my machine with GPUDirect v1 I did the following:<br><br>* Install RHEL 5.4<br>* Use the kernel with GPUDirect support<br>* Use the MLNX OFED stack with GPUDirect support<br>* Install the CUDA developer driver<br><br>Does using CUDA&nbsp; &gt;= 4.0&nbsp; make one of the above steps&nbsp; redundant?<br><br>I.e., RHEL or different kernel or MLNX OFED stack with GPUDirect support is&nbsp; not needed any more?<br><br>Sebastian.<br><br>Rolf vandeVaart wrote: <o:p></o:p></p><pre>I ran your test case against Open MPI 1.4.2 and CUDA 4.1 RC2 and it worked fine.&nbsp; I do not have a machine right now where I can load CUDA 4.0 drivers.<o:p></o:p></pre><pre>Any chance you can try CUDA 4.1 RC2?&nbsp; There were some improvements in the support (you do not need to set an environment variable for one)<o:p></o:p></pre><pre> <a href="http://developer.nvidia.com/cuda-toolkit-41">http://developer.nvidia.com/cuda-toolkit-41</a><o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>There is also a chance that setting the environment variable as outlined in this link may help you.<o:p></o:p></pre><pre><a href="http://forums.nvidia.com/index.php?showtopic=200629">http://forums.nvidia.com/index.php?showtopic=200629</a><o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>However, I cannot explain why MVAPICH would work and Open MPI would not.&nbsp; <o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>Rolf<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>&nbsp; <o:p></o:p></pre><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><pre>-----Original Message-----<o:p></o:p></pre><pre>From: <a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a> [<a href="mailto:devel-bounces@open-mpi.org">mailto:devel-bounces@open-mpi.org</a>]<o:p></o:p></pre><pre>On Behalf Of Sebastian Rinke<o:p></o:p></pre><pre>Sent: Tuesday, January 17, 2012 12:08 PM<o:p></o:p></pre><pre>To: Open MPI Developers<o:p></o:p></pre><pre>Subject: Re: [OMPI devel] GPUDirect v1 issues<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>I use CUDA 4.0 with MVAPICH2 1.5.1p1 and Open MPI 1.4.2.<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>Attached you find a little test case which is based on the GPUDirect v1 test<o:p></o:p></pre><pre>case (mpi_pinned.c).<o:p></o:p></pre><pre>In that program the sender splits a message into chunks and sends them<o:p></o:p></pre><pre>separately to the receiver which posts the corresponding recvs. It is a kind of<o:p></o:p></pre><pre>pipelining.<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>In mpi_pinned.c:141 the offsets into the recv buffer are set.<o:p></o:p></pre><pre>For the correct offsets, i.e. increasing them, it blocks with Open MPI.<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>Using line 142 instead (offset = 0) works.<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>The tarball attached contains a Makefile where you will have to adjust<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>* CUDA_INC_DIR<o:p></o:p></pre><pre>* CUDA_LIB_DIR<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>Sebastian<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>On Jan 17, 2012, at 4:16 PM, Kenneth A. Lloyd wrote:<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre> &nbsp;&nbsp;&nbsp;<o:p></o:p></pre><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><pre>Also, which version of MVAPICH2 did you use?<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>I've been pouring over Rolf's OpenMPI CUDA RDMA 3 (using CUDA 4.1 r2)<o:p></o:p></pre><pre>vis MVAPICH-GPU on a small 3 node cluster. These are wickedly interesting.<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>Ken<o:p></o:p></pre><pre>-----Original Message-----<o:p></o:p></pre><pre>From: <a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a> [<a href="mailto:devel-bounces@open">mailto:devel-bounces@open</a>-<o:p></o:p></pre><pre>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <o:p></o:p></pre></blockquote><pre>mpi.org]<o:p></o:p></pre><pre>&nbsp;&nbsp;&nbsp; <o:p></o:p></pre><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><pre>On Behalf Of Rolf vandeVaart<o:p></o:p></pre><pre>Sent: Tuesday, January 17, 2012 7:54 AM<o:p></o:p></pre><pre>To: Open MPI Developers<o:p></o:p></pre><pre>Subject: Re: [OMPI devel] GPUDirect v1 issues<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>I am not aware of any issues.&nbsp; Can you send me a test program and I<o:p></o:p></pre><pre>can try it out?<o:p></o:p></pre><pre>Which version of CUDA are you using?<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>Rolf<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <o:p></o:p></pre><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><pre>-----Original Message-----<o:p></o:p></pre><pre>From: <a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a> [<a href="mailto:devel-bounces@open">mailto:devel-bounces@open</a>-<o:p></o:p></pre><pre>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <o:p></o:p></pre></blockquote></blockquote><pre>mpi.org]<o:p></o:p></pre><pre>&nbsp;&nbsp;&nbsp; <o:p></o:p></pre><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><pre>On Behalf Of Sebastian Rinke<o:p></o:p></pre><pre>Sent: Tuesday, January 17, 2012 8:50 AM<o:p></o:p></pre><pre>To: Open MPI Developers<o:p></o:p></pre><pre>Subject: [OMPI devel] GPUDirect v1 issues<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>Dear all,<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>I'm using GPUDirect v1 with Open MPI 1.4.3 and experience blocking<o:p></o:p></pre><pre>MPI_SEND/RECV to block forever.<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>For two subsequent MPI_RECV, it hangs if the recv buffer pointer of<o:p></o:p></pre><pre>the second recv points to somewhere, i.e. not at the beginning, in<o:p></o:p></pre><pre>the recv buffer (previously allocated with cudaMallocHost()).<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>I tried the same with MVAPICH2 and did not see the problem.<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>Does anybody know about issues with GPUDirect v1 using Open MPI?<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>Thanks for your help,<o:p></o:p></pre><pre>Sebastian<o:p></o:p></pre><pre>_______________________________________________<o:p></o:p></pre><pre>devel mailing list<o:p></o:p></pre><pre><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><o:p></o:p></pre><pre><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><o:p></o:p></pre><pre>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <o:p></o:p></pre></blockquote></blockquote></blockquote><pre>-----------------------------------------------------------------------------------<o:p></o:p></pre><pre>This email message is for the sole use of the intended recipient(s) and may contain<o:p></o:p></pre><pre>confidential information.&nbsp; Any unauthorized review, use, disclosure or distribution<o:p></o:p></pre><pre>is prohibited.&nbsp; If you are not the intended recipient, please contact the sender by<o:p></o:p></pre><pre>reply email and destroy all copies of the original message.<o:p></o:p></pre><pre>-----------------------------------------------------------------------------------<o:p></o:p></pre><pre><o:p>&nbsp;</o:p></pre><pre>_______________________________________________<o:p></o:p></pre><pre>devel mailing list<o:p></o:p></pre><pre><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><o:p></o:p></pre><pre><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><o:p></o:p></pre><pre>&nbsp; <o:p></o:p></pre><p class=MsoNormal><o:p>&nbsp;</o:p></p></div></div></body></html>
