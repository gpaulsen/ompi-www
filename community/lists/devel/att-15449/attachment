<html><head><meta http-equiv="Content-Type" content="text/html charset=us-ascii"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">Okay, I fixed those two and will release rc4 once the coll/ml fix has been reviewed. Thanks<div><br><div><div>On Aug 1, 2014, at 2:46 AM, Mike Dubman &lt;<a href="mailto:miked@dev.mellanox.co.il">miked@dev.mellanox.co.il</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr">Also, latest commit into openib (origin/v1.8 <a href="https://svn.open-mpi.org/trac/ompi/changeset/32391">https://svn.open-mpi.org/trac/ompi/changeset/32391</a>) broke something:<div><br></div><div><pre class="" style="white-space: pre-wrap; word-wrap: break-word; margin-top: 0px; margin-bottom: 0px; font-size: 11px;"><span class=""><b>11:45:01</b> </span>+ timeout -s SIGSEGV 3m /scrap/jenkins/workspace/OMPI-vendor/label/hpctest/ompi_install1/bin/mpirun -np 8 -mca pml ob1 -mca btl self,openib /scrap/jenkins/workspace/OMPI-vendor/label/hpctest/ompi_install1/examples/hello_usempi
<span class=""><b>11:45:01</b> </span>--------------------------------------------------------------------------
<span class=""><b>11:45:01</b> </span>WARNING: There are more than one active ports on host 'hpctest', but the
<span class=""><b>11:45:01</b> </span>default subnet GID prefix was detected on more than one of these
<span class=""><b>11:45:01</b> </span>ports.  If these ports are connected to different physical IB
<span class=""><b>11:45:01</b> </span>networks, this configuration will fail in Open MPI.  This version of
<span class=""><b>11:45:01</b> </span>Open MPI requires that every physically separate IB subnet that is
<span class=""><b>11:45:01</b> </span>used between connected MPI processes must have different subnet ID
<span class=""><b>11:45:01</b> </span>values.
<span class=""><b>11:45:01</b> </span>
<span class=""><b>11:45:01</b> </span>Please see this FAQ entry for more details:
<span class=""><b>11:45:01</b> </span>
<span class=""><b>11:45:01</b> </span>  <a href="http://www.open-mpi.org/faq/?category=openfabrics#ofa-default-subnet-gid" style="color:rgb(92,53,102)">http://www.open-mpi.org/faq/?category=openfabrics#ofa-default-subnet-gid</a>
<span class=""><b>11:45:01</b> </span>
<span class=""><b>11:45:01</b> </span>NOTE: You can turn off this warning by setting the MCA parameter
<span class=""><b>11:45:01</b> </span>      btl_openib_warn_default_gid_prefix to 0.
<span class=""><b>11:45:01</b> </span>--------------------------------------------------------------------------
<span class=""><b>11:45:01</b> </span>--------------------------------------------------------------------------
<span class=""><b>11:45:01</b> </span>WARNING: No queue pairs were defined in the btl_openib_receive_queues
<span class=""><b>11:45:01</b> </span>MCA parameter.  At least one queue pair must be defined.  The
<span class=""><b>11:45:01</b> </span>OpenFabrics (openib) BTL will therefore be deactivated for this run.
<span class=""><b>11:45:01</b> </span>
<span class=""><b>11:45:01</b> </span>  Local host: hpctest
<span class=""><b>11:45:01</b> </span>--------------------------------------------------------------------------
<span class=""><b>11:45:01</b> </span>--------------------------------------------------------------------------
<span class=""><b>11:45:01</b> </span>At least one pair of MPI processes are unable to reach each other for
<span class=""><b>11:45:01</b> </span>MPI communications.  This means that no Open MPI device has indicated
<span class=""><b>11:45:01</b> </span>that it can be used to communicate between these processes.  This is
<span class=""><b>11:45:01</b> </span>an error; Open MPI requires that all MPI processes be able to reach
<span class=""><b>11:45:01</b> </span>each other.  This error can sometimes be the result of forgetting to
<span class=""><b>11:45:01</b> </span>specify the "self" BTL.
<span class=""><b>11:45:01</b> </span>
<span class=""><b>11:45:01</b> </span>  Process 1 ([[55281,1],1]) is on host: hpctest
<span class=""><b>11:45:01</b> </span>  Process 2 ([[55281,1],0]) is on host: hpctest
<span class=""><b>11:45:01</b> </span>  BTLs attempted: self
<span class=""><b>11:45:01</b> </span>
<span class=""><b>11:45:01</b> </span>Your MPI job is now going to abort; sorry.
<span class=""><b>11:45:01</b> </span>--------------------------------------------------------------------------
<span class=""><b>11:45:01</b> </span>--------------------------------------------------------------------------
<span class=""><b>11:45:01</b> </span>MPI_INIT has failed because at least one MPI process is unreachable
<span class=""><b>11:45:01</b> </span>from another.  This *usually* means that an underlying communication
<span class=""><b>11:45:01</b> </span>plugin -- such as a BTL or an MTL -- has either not loaded or not
<span class=""><b>11:45:01</b> </span>allowed itself to be used.  Your MPI job will now abort.
<span class=""><b>11:45:01</b> </span>
<span class=""><b>11:45:01</b> </span>You may wish to try to narrow down the problem;
<span class=""><b>11:45:01</b> </span>
<span class=""><b>11:45:01</b> </span> * Check the output of ompi_info to see which BTL/MTL plugins are
<span class=""><b>11:45:01</b> </span>   available.
<span class=""><b>11:45:01</b> </span> * Run your application with MPI_THREAD_SINGLE.
<span class=""><b>11:45:01</b> </span> * Set the MCA parameter btl_base_verbose to 100 (or mtl_base_verbose,
<span class=""><b>11:45:01</b> </span>   if using MTL-based communications) to see exactly which
<span class=""><b>11:45:01</b> </span>   communication plugins were considered and/or discarded.
<span class=""><b>11:45:01</b> </span>--------------------------------------------------------------------------
<span class=""><b>11:45:01</b> </span>*** An error occurred in MPI_Init
<span class=""><b>11:45:01</b> </span>*** on a NULL communicator
<span class=""><b>11:45:01</b> </span>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
<span class=""><b>11:45:01</b> </span>***    and potentially your MPI job)
<span class=""><b>11:45:01</b> </span>[hpctest:2761] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
<span class=""><b>11:45:01</b> </span>*** An error occurred in MPI_Init
<span class=""><b>11:45:01</b> </span>*** on a NULL communicator
<span class=""><b>11:45:01</b> </span>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
<span class=""><b>11:45:01</b> </span>***    and potentially your MPI job)
<span class=""><b>11:45:01</b> </span>[hpctest:2757] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
<span class=""><b>11:45:01</b> </span>*** An error occurred in MPI_Init
<span class=""><b>11:45:01</b> </span>*** on a NULL communicator
<span class=""><b>11:45:01</b> </span>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
<span class=""><b>11:45:01</b> </span>***    and potentially your MPI job)
<span class=""><b>11:45:01</b> </span>[hpctest:2751] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
<span class=""><b>11:45:01</b> </span>*** An error occurred in MPI_Init
<span class=""><b>11:45:01</b> </span>*** on a NULL communicator
<span class=""><b>11:45:01</b> </span>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
<span class=""><b>11:45:01</b> </span>***    and potentially your MPI job)
<span class=""><b>11:45:01</b> </span>[hpctest:2752] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
<span class=""><b>11:45:01</b> </span>*** An error occurred in MPI_Init
<span class=""><b>11:45:01</b> </span>*** on a NULL communicator
<span class=""><b>11:45:01</b> </span>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
<span class=""><b>11:45:01</b> </span>***    and potentially your MPI job)
<span class=""><b>11:45:01</b> </span>[hpctest:2753] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
<span class=""><b>11:45:01</b> </span>*** An error occurred in MPI_Init
<span class=""><b>11:45:01</b> </span>*** on a NULL communicator
<span class=""><b>11:45:01</b> </span>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
<span class=""><b>11:45:01</b> </span>***    and potentially your MPI job)
<span class=""><b>11:45:01</b> </span>[hpctest:2755] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
<span class=""><b>11:45:01</b> </span>*** An error occurred in MPI_Init
<span class=""><b>11:45:01</b> </span>*** on a NULL communicator
<span class=""><b>11:45:01</b> </span>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
<span class=""><b>11:45:01</b> </span>***    and potentially your MPI job)
<span class=""><b>11:45:01</b> </span>[hpctest:2759] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
<span class=""><b>11:45:01</b> </span>*** An error occurred in MPI_Init
<span class=""><b>11:45:01</b> </span>*** on a NULL communicator
<span class=""><b>11:45:01</b> </span>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
<span class=""><b>11:45:01</b> </span>***    and potentially your MPI job)
<span class=""><b>11:45:01</b> </span>[hpctest:2763] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!</pre></div></div>
<div class="gmail_extra"><br><br><div class="gmail_quote">On Fri, Aug 1, 2014 at 11:00 AM, Paul Hargrove <span dir="ltr">&lt;<a href="mailto:phhargrove@lbl.gov" target="_blank">phhargrove@lbl.gov</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Note that the Solaris unresolved alloca problem George fixed in&nbsp;r32388 is still present in 1.8.2rc3.<div>
I have manually confirmed that the same patch resolves the problem in 1.8.2rc3.</div><div><br></div><div>
-Paul</div></div><div class="gmail_extra"><div><div class="h5"><br><br><div class="gmail_quote">On Thu, Jul 31, 2014 at 9:44 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Usual place - this is a last-chance check, so please hit it. Main change from rc2 is the repairs to the Fortran binding config logic<br>


<br>
<a href="http://www.open-mpi.org/software/ompi/v1.8/" target="_blank">http://www.open-mpi.org/software/ompi/v1.8/</a><br>
<br>
Ralph<br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/08/15433.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/08/15433.php</a><br>
</blockquote></div><br><br clear="all"><div><br></div></div></div><span class="HOEnZb"><font color="#888888">-- <br><font face="courier new, monospace"><div>Paul H. Hargrove &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href="mailto:PHHargrove@lbl.gov" target="_blank">PHHargrove@lbl.gov</a></div>

<div>Future Technologies Group</div><div>Computer and Data Sciences Department &nbsp; &nbsp; Tel: <a href="tel:%2B1-510-495-2352" value="+15104952352" target="_blank">+1-510-495-2352</a></div><div>Lawrence Berkeley National Laboratory &nbsp; &nbsp; Fax: <a href="tel:%2B1-510-486-6900" value="+15104866900" target="_blank">+1-510-486-6900</a></div>
</font>
</font></span></div>
<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/08/15440.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/08/15440.php</a><br></blockquote></div><br></div>
_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/devel<br>Link to this post: http://www.open-mpi.org/community/lists/devel/2014/08/15444.php</blockquote></div><br></div></body></html>
