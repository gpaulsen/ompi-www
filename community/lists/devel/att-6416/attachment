Hmmm...I believe I made a mis-statement. Shocking to those who know me, I am sure! :-)<br><br>Just to correct my comments: OMPI knows how many &quot;slots&quot; have been
allocated to us, but not which &quot;cores&quot;. So I&#39;ll assign the correct
number of procs to each node, but they won&#39;t know that we were
allocated cores 2 and 4 (for example), as opposed to some other
combination.<br>
<br>
When we subsequently bind, we bind to logical cpus based on our node
rank - i.e., what rank I am relative to my local peers on this node.
PLPA then translates that into a physical core.<br><br>My guess is that you are correct and PLPA isn&#39;t looking to see specifically -which- cores were allocated to the job, but instead is simply translating logical cpu=0 to the first physical core in the node.<br>
<br>The test I asked you to run, though, will confirm this. Please do let us know as this is definitely something we should fix.<br><br>Thanks!<br>Ralph<br><br><br><div class="gmail_quote">On Wed, Jul 15, 2009 at 6:11 AM, Chris Samuel <span dir="ltr">&lt;<a href="mailto:csamuel@vpac.org">csamuel@vpac.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><br>
----- &quot;Ralph Castain&quot; &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
<br>
Hi Ralph,<br>
<div class="im"><br>
&gt; Interesting. No, we don&#39;t take PLPA cpu sets into account when<br>
&gt; retrieving the allocation.<br>
<br>
</div>Understood.<br>
<div class="im"><br>
&gt; Just to be clear: from an OMPI perspective, I don&#39;t think this is an<br>
&gt; issue of binding, but rather an issue of allocation. If we knew we had<br>
&gt; been allocated only a certain number of cores on a node, then we would<br>
&gt; only map that many procs to the node. When we subsequently &quot;bind&quot;, we<br>
&gt; should then bind those procs to the correct cores (I think).<br>
<br>
</div>Hmm, OpenMPI should already know this from the PBS TM API when<br>
launching the job, we&#39;ve never had to get our users to specify<br>
how many procs per node to start (and they will generally have<br>
no idea how many to ask for in advance as they are at the mercy<br>
of the scheduler, unless they select a whole nodes with ppn=8).</blockquote><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><br>
<div class="im"><br>
&gt; Could you check this? You can run a trivial job using the<br>
&gt; -npernode x option, where x matched the #cores you were<br>
&gt; allocated on the nodes.<br>
&gt;<br>
&gt; If you do this, do we bind to the correct cores?<br>
<br>
</div>I&#39;ll give this a shot tomorrow when I&#39;m back in the office<br>
(just checking email late at night here), I&#39;ll try it under<br>
strace to to see what it tries to sched_setaffinity() to.<br>
<div class="im"><br>
&gt; If we do, then that would confirm that we just aren&#39;t<br>
&gt; picking up the right number of cores allocated to us.<br>
&gt; If it is wrong, then this is a PLPA issue where it<br>
&gt; isn&#39;t binding to the right core.<br>
<br>
</div>Interesting, will let you know the test results tomorrow!<br>
<div><div></div><div class="h5"><br>
cheers,<br>
Chris<br>
--<br>
Christopher Samuel - (03) 9925 4751 - Systems Manager<br>
 The Victorian Partnership for Advanced Computing<br>
 P.O. Box 201, Carlton South, VIC 3053, Australia<br>
VPAC is a not-for-profit Registered Research Agency<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</div></div></blockquote></div><br>

