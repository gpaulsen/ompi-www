Though I did not repeat it, I assumed --mca btl tcp,self is always used, as described in the initial email<div><br></div><div>Cheers,</div><div><br></div><div>Gilles<br><br>On Monday, January 25, 2016, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">I believe the performance penalty will still always be greater than zero, however, as the TCP stack is smart enough to take an optimized path when doing a loopback as opposed to inter-node communication.<div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Mon, Jan 25, 2016 at 4:28 AM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;gilles.gouaillardet@gmail.com&#39;);" target="_blank">gilles.gouaillardet@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Federico,<div><br></div><div>I did not expect 0% degradation, since you are now comparing two different cases</div><div>1 orted means tasks are bound on sockets</div><div>16 orted means tasks are not bound.</div><div><br></div><div>a quick way to improve things is to use a wrapper that binds MPI tasks</div><div>mpirun --bind-to none wrapper.sh skampi</div><div><br></div><div>wrapper.sh can use environment variable to retrieve the rank id</div><div>(PMI(X)_RANK iirc) and then bind the tasks with taskset or helicopter utils</div><div><br></div><div>mpirun --tag-output grep Cpus_allowed_list /proc/self/status</div><div>with 1 orted should return the same output than</div><div>mpirun --tag-output --bind-to none wrapper.sh grep CPUs_allowed_list /proc/self/status</div><div>with 16 orted</div><div><br></div><div>when wrapper.sh works fine, skampi degradation should be smaller with 16 orted</div><div><div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<br><br>On Monday, January 25, 2016, Federico Reghenzani &lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;federico1.reghenzani@mail.polimi.it&#39;);" target="_blank">federico1.reghenzani@mail.polimi.it</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Thank you Gilles, you&#39;re right, with --bind-to none we have ~ 15% of degradation rather than 50%.<div><br></div><div>It&#39;s much better now, but I think it should be (in theory) around 0%.</div><div>The benchmark is MPI bound (the standard benchmark provided with SkaMPI), it tests these functions: <span style="color:rgb(0,0,0);font-family:Arial;font-size:13px;white-space:pre-wrap">MPI_Bcast, </span><span style="color:rgb(0,0,0);font-family:Arial;font-size:13px;white-space:pre-wrap">MPI_Barrier, </span><span style="color:rgb(0,0,0);font-family:Arial;font-size:13px;white-space:pre-wrap">MPI_Reduce, </span><span style="color:rgb(0,0,0);font-family:Arial;font-size:13px;white-space:pre-wrap">MPI_Allreduce, </span><span style="color:rgb(0,0,0);font-family:Arial;font-size:13px;white-space:pre-wrap">MPI_Alltoall, </span><span style="color:rgb(0,0,0);font-family:Arial;font-size:13px;white-space:pre-wrap">MPI_Gather, </span><span style="color:rgb(0,0,0);font-family:Arial;font-size:13px;white-space:pre-wrap">MPI_Scatter, </span><span style="color:rgb(0,0,0);font-family:Arial;font-size:13px;white-space:pre-wrap">MPI_Scan, MPI_Send/Recv</span></div><div class="gmail_extra"><br></div><div class="gmail_extra">Cheers,</div><div class="gmail_extra">Federico<br clear="all"><div><div><div dir="ltr"><div><div dir="ltr"><div><div dir="ltr"><div>__</div><div><span style="font-size:12.8px">Federico Reghenzani</span></div><div><font size="1">M.Eng. Student @ Politecnico di Milano</font></div><div><span style="font-size:x-small">Computer Science and Engineering</span></div><div><br></div><div><br></div></div></div></div></div></div></div></div>
<br><div class="gmail_quote">2016-01-25 12:17 GMT+01:00 Gilles Gouaillardet <span dir="ltr">&lt;<a>gilles.gouaillardet@gmail.com</a>&gt;</span>:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Federico,<div><br></div><div>unless you already took care of that, I would guess all 16 orted bound their children MPI tasks on socket 0</div><div><br></div><div>can you try</div><div>mpirun --bind-to none ...</div><div><br></div><div>btw, is your benchmark application cpu bound ? memory bound ? MPI bound ?</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<div><div><br><br>On Monday, January 25, 2016, Federico Reghenzani &lt;<a>federico1.reghenzani@mail.polimi.it</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Hello,<div><br></div><div>we have executed a benchmark (SkaMPI) on the same machine (32 core Intel Xeon 86_64) with these two configurations:</div><div>- 1 orted with 16 processes with BTL forced to TCP (--mca btl self,tcp)</div><div>- 16 orted with each 1 process (that uses TCP)</div><div><br></div><div>We use a custom RAS to allow multiple orted on the same machine (I know that it seems non-sense to have multiple orteds on the same machine for the same application, but we are doing some experiments for migration).</div><div><br></div><div>Initially we have expected approximately the same performance in both cases (we have 16 processes communicating via TCP in both cases), but we have a degradation of 50%, and we are sure that is not an overhead due to orteds initialization.<br></div><div><br></div><div>Do you have any idea how can multiple orteds influence the processess performance?</div><div><br></div><div><br></div><div>Cheers,</div><div>Federico<br clear="all"><div><div><div dir="ltr"><div><div dir="ltr"><div><div dir="ltr"><div>__</div><div><span style="font-size:12.8px">Federico Reghenzani</span></div><div><font size="1">M.Eng. Student @ Politecnico di Milano</font></div><div><span style="font-size:x-small">Computer Science and Engineering</span></div><div><br></div><div><br></div></div></div></div></div></div></div></div>
</div></div>
</blockquote></div></div></div>
<br>_______________________________________________<br>
devel mailing list<br>
<a>devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/01/18499.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/01/18499.php</a><br></blockquote></div><br></div></div>
</blockquote></div>
</div></div><br>_______________________________________________<br>
devel mailing list<br>
<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;devel@open-mpi.org&#39;);" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/01/18501.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/01/18501.php</a><br></blockquote></div><br></div>
</blockquote></div>

