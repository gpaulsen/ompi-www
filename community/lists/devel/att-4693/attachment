<p>Hi George,</p><p>It seems like some data corruption in Reduce_scatter function</p><p>I discovered it when added -DCHECK to IMB benchmark, and it seemed to be there for ages.</p><p>it runs with voltaire MPI, but failes with OMPI. you will get a seqv with IMB3.1 and error with IMB3.0</p>
<p></p><p>host#VER=TRUNK ; /home/USERS/lenny/OMPI_ORTE_${VER}/bin/mpirun -np 2 -H witch8  /home/BENCHMARKS/PALLAS/IMB_3.0v/src/IMB-MPI1_${VER} Reduce_scatter<br></p><p>#---------------------------------------------------<br>
#    Intel (R) MPI Benchmark Suite V3.0v modified by Voltaire, MPI-1 part<br>#---------------------------------------------------<br># Date                  : Tue Sep 23 18:05:35 2008<br># Machine               : x86_64<br>
# System                : Linux<br># Release               : 2.6.16.46-0.12-smp<br># Version               : #1 SMP Thu May 17 14:00:09 UTC 2007<br># MPI Version           : 2.0<br># MPI Thread Environment: MPI_THREAD_SINGLE<br>
<br>#<br># Minimum message length in bytes:   0<br># Maximum message length in bytes:   67108864<br>#<br># MPI_Datatype                   :   MPI_BYTE<br># MPI_Datatype for reductions    :   MPI_FLOAT<br># MPI_Op                         :   MPI_SUM<br>
#<br>#<br><br># List of Benchmarks to run:<br><br># Reduce_scatter<br><br>#-----------------------------------------------------------------------------<br># Benchmarking Reduce_scatter<br># #processes = 2<br>#-----------------------------------------------------------------------------<br>
#Benchmarking        #procs       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]      defects<br>Reduce_scatter  2                   0         1000         0.05         0.05         0.05         0.00<br>0: Error Reduce_scatter, size = 4, sample #0<br>
Process 0: Got invalid buffer:<br>Buffer entry: 817291591680.000000<br>pos: 0<br>Process 0: Expected    buffer:<br>Buffer entry: 0.000000<br>Reduce_scatter  2                   4         1000         0.98         1.06         1.02         1.00<br>
Application error code 1 occurred<br>[witch8:10190] MPI_ABORT invoked on rank 0 in communicator MPI_COMM_WORLD with errorcode 17<br>--------------------------------------------------------------------------<br>mpirun has exited due to process rank 0 with PID 10190 on<br>
node witch8 exiting without calling &quot;finalize&quot;. This may<br>have caused other processes in the application to be<br>terminated by signals sent by mpirun (as reported here).<br>--------------------------------------------------------------------------<br>
<br><br></p>

