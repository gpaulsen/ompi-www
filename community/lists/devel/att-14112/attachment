<div dir="ltr">Hello<div><br></div><div>I found that SLURM installations that use cgroup plugin and have TaskAffinity=yes in cgroup.conf have problems with OpenMPI: all processes on non-launch node are assigned on one core. This leads to quite poor performance.</div>
<div>The problem can be seen only if using mpirun to start parallel application in batch script. For example: <b>mpirun ./mympi</b></div><div>If using srun with PMI affinity is setted properly: <b>srun ./mympi.</b><br></div>
<div><br></div><div>Close look shows that the reason lies in the way Open MPI use srun to launch ORTE daemons. Here is example of the command line:</div><div><b>srun</b> <b>--nodes=1</b> <b>--ntasks=1</b> --kill-on-bad-exit --nodelist=node02 <b>orted</b> -mca ess slurm -mca orte_ess_jobid 3799121920 -mca orte_ess_vpid <br>
</div><div> </div><div>Saying <b>--nodes=1</b> <b>--ntasks=1</b> to SLURM means that you want to start one task and (with TaskAffinity=yes) it will be binded to one core. Next orted use this affinity as base for all spawned branch processes. If I understand correctly the problem behind using srun is that if you say <b>srun</b> <b>--nodes=1</b> <b>--ntasks=4</b> - then SLURM will spawn 4 independent orted processes binded to different cores which is not what we really need.</div>
<div><br></div><div>I found that disabling of cpu binding as a fast hack works good for cgroup plugin. Since job runs inside a group which has core access restrictions, spawned branch processes are executed under nodes scheduler control on all allocated cores. The command line looks like this:</div>
<div><div>srun <b>--cpu_bind=none</b> --nodes=1 --ntasks=1 --kill-on-bad-exit --nodelist=node02 orted -mca ess slurm -mca orte_ess_jobid 3799121920 -mca orte_ess_vpid </div></div><div><br></div><div>This solution will probably won&#39;t work with SLURM task/affinity plugin. Also it may be a bad idea when strong affinity desirable.</div>
<div><br></div><div>My patch to stable Open MPI version (1.6.5) is attached to this e-mail. I will try to make more reliable solution but I need more time and beforehand would like to know opinion of Open MPI developers.</div>
<div><div><br></div>-- <br>С Уважением, Поляков Артем Юрьевич<br>Best regards, Artem Y. Polyakov
</div></div>

