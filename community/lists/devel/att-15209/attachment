<div dir="ltr"><div class="gmail_extra"><div class="gmail_quote">On Mon, Jul 21, 2014 at 1:41 PM, Yossi Etigin <span dir="ltr">&lt;<a href="mailto:yosefe@mellanox.com" target="_blank">yosefe@mellanox.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">





<div lang="EN-US" link="blue" vlink="purple">
<div>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d">Right, but:<u></u><u></u></span></p>
<p><u></u><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d"><span>1.<span style="font:7.0pt &quot;Times New Roman&quot;">      
</span></span></span><u></u><span dir="LTR"></span><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d">IMHO the rte_barrier in the wrong place (in the trunk)</span></p></div>
</div></blockquote><div><br></div><div>In the trunk we have the rte_barrier prior to del_proc, which is what I would have expected: quiescence the BTLs by reaching a point where everybody agree that no more MPI messages will be exchanged, and then delete the BTLs.</div>
<div> </div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div lang="EN-US" link="blue" vlink="purple"><div><p><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d"><u></u><u></u></span></p>

<p><u></u><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d"><span>2.<span style="font:7.0pt &quot;Times New Roman&quot;">      
</span></span></span><u></u><span dir="LTR"></span><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d">In addition to the rte_barrier, need also mpi_barrier</span></p></div>
</div></blockquote><div>Care for providing a reasoning for this barrier? Why and where should it be placed?</div><div><br></div><div>  George.</div><div><br></div><div><br></div><div> </div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div lang="EN-US" link="blue" vlink="purple"><div><p><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d"><u></u><u></u></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d"><u></u> <u></u></span></p>
<p class="MsoNormal"><b><span style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;">From:</span></b><span style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;"> devel [mailto:<a href="mailto:devel-bounces@open-mpi.org" target="_blank">devel-bounces@open-mpi.org</a>]
<b>On Behalf Of </b>George Bosilca<br>
<b>Sent:</b> Monday, July 21, 2014 8:19 PM<br>
<b>To:</b> Open MPI Developers</span></p><div><div class="h5"><br>
<b>Subject:</b> Re: [OMPI devel] barrier before calling del_procs<u></u><u></u></div></div><p></p><div><div class="h5">
<p class="MsoNormal"><u></u> <u></u></p>
<div>
<p class="MsoNormal">There was a long thread of discussion on why we must use an rte_barrier and not an mpi_barrier during the finalize. Basically, we long as we have connectionless unreliable BTLs we need an external mechanism to ensure complete tear-down
 of the entire infrastructure. Thus, we need to rely on an rte_barrier not because it guarantees the correctness of the code, but because it provides enough time to all processes to flush all HPC traffic.<u></u><u></u></p>

<div>
<p class="MsoNormal"><u></u> <u></u></p>
</div>
<div>
<p class="MsoNormal">  George.<u></u><u></u></p>
</div>
<div>
<p class="MsoNormal"><u></u> <u></u></p>
</div>
</div>
<div>
<p class="MsoNormal" style="margin-bottom:12.0pt"><u></u> <u></u></p>
<div>
<p class="MsoNormal">On Mon, Jul 21, 2014 at 1:10 PM, Yossi Etigin &lt;<a href="mailto:yosefe@mellanox.com" target="_blank">yosefe@mellanox.com</a>&gt; wrote:<u></u><u></u></p>
<p class="MsoNormal">I see. But in branch v1.8, in 31869, Ralph reverted the commit which moved del_procs after the barrier:<br>
  &quot;Revert r31851 until we can resolve how to close these leaks without causing the usnic BTL to fail during disconnect of intercommunicators<br>
   Refs #4643&quot;<br>
Also, we need an rte barrier after del_procs - because otherwise rankA could call pml_finalize() before rankB finishes disconnecting from rankA.<br>
<br>
I think the order in finalize should be like this:<br>
        1. mpi_barrier(world)<br>
        2. del_procs()<br>
        3. rte_barrier()<br>
        4. pml_finalize()<u></u><u></u></p>
<div>
<div>
<p class="MsoNormal"><br>
-----Original Message-----<br>
From: Nathan Hjelm [mailto:<a href="mailto:hjelmn@lanl.gov" target="_blank">hjelmn@lanl.gov</a>]<br>
Sent: Monday, July 21, 2014 8:01 PM<br>
To: Open MPI Developers<br>
Cc: Yossi Etigin<br>
Subject: Re: [OMPI devel] barrier before calling del_procs<br>
<br>
I should add that it is an rte barrier and not an MPI barrier for technical reasons.<br>
<br>
-Nathan<br>
<br>
On Mon, Jul 21, 2014 at 09:42:53AM -0700, Ralph Castain wrote:<br>
&gt;    We already have an rte barrier before del procs<br>
&gt;<br>
&gt;    Sent from my iPhone<br>
&gt;    On Jul 21, 2014, at 8:21 AM, Yossi Etigin &lt;<a href="mailto:yosefe@mellanox.com" target="_blank">yosefe@mellanox.com</a>&gt; wrote:<br>
&gt;<br>
&gt;      Hi,<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;      We get occasional hangs with MTL/MXM during finalize, because a global<br>
&gt;      synchronization is needed before calling del_procs.<br>
&gt;<br>
&gt;      e.g rank A may call del_procs() and disconnect from rank B, while rank B<br>
&gt;      is still working.<br>
&gt;<br>
&gt;      What do you think about adding an MPI barrier on COMM_WORLD before<br>
&gt;      calling del_procs()?<br>
&gt;<br>
&gt;<br>
<br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; Link to this post:<br>
&gt; <a href="http://www.open-mpi.org/community/lists/devel/2014/07/15204.php" target="_blank">
http://www.open-mpi.org/community/lists/devel/2014/07/15204.php</a><br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><u></u><u></u></p>
</div>
</div>
<p class="MsoNormal">Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/07/15206.php" target="_blank">
http://www.open-mpi.org/community/lists/devel/2014/07/15206.php</a><u></u><u></u></p>
</div>
<p class="MsoNormal"><u></u> <u></u></p>
</div>
</div></div></div>
</div>

<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/07/15208.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/07/15208.php</a><br></blockquote></div><br></div></div>

