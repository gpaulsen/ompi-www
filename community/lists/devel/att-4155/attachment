<html><head><style type="text/css"><!-- DIV {margin:0px;} --></style></head><body><div style="font-family:times new roman, new york, times, serif;font-size:10pt">Hi again... I was on a break from Xensocket stuff.... This time some general questions...<br><br>Forgive me for the question.... its&nbsp; a quick one and related to some of my development work on Xen, I will explain the rationale after the question. What if I have multiple Ethernet cards (say 5) on two of my quad core machines.&nbsp; The IP addresses (and the subnets of course) are <br>Machine A&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Machine B<br>eth0 is y.y.1.a &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; y.y.1.z &nbsp;&nbsp;&nbsp; <br>eth1 is
 y.y.4.b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y.y.4.y<br>eth2 is y.y.4.c&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br>eth3 is y.y.4.d&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...<br><div>&nbsp;...<br>Now from the FAQ's (Refer 9: How does Open MPI know which TCP addresses are routable to each other?) it is clear that if I want to run a job on multiple ethernets, I can use --mca btl_tcp_if_include&nbsp; eth0,eth1. This will run the job on two of the subnets utilizing both the Ethernet cards. Is it doing some sort of load balancing? or some round robin mechanism? What part of code is responsible for this
 work?<br><br>Now what if I want to run the job like --mca btl_tcp_if_include eth1,eth2,eth3,eth4. Notice that all of these ethNs are on same subnet. Even in the FAQ's (which mostly answers our lame questions)&nbsp; its not entirely clear how communication will be done.&nbsp; Each process will have tcp_num_btls equal to interfaces, but then what? Is it some sort of load balancing or similar stuff which is not clear in tcpdump?<br><br>Another related question is what if I want to run 8 process job (on 2x4 cluster) and want to pin a process to an network interface. OpenMPI to my understanding does not give any control of allocating IP to a process (like MPICH) or is there some magical --mca thingie. I think only way to go is adding routing tables... am i thinking in right direction? If yes, then the performance of my boxes decrease when i trying to force the routing (obviously something terrible with my configuration)<br>&nbsp;Its related to my Xen
 (virtualization) work. We are in a scenario, where all the virtual machines on one Xen host need to use eth2 (which is virtualized but optimized for intra-domain communication) and for communication outside the physical machine (i.e. to other Xen hosts)&nbsp; we want to use eth1. Is 'route add' the only way again?<br><br>I will ask Xensocket BTL related questions later :)<br><br></div>Best Regards and thanks in advance,<br>Muhammad Atif<div><br></div></div><br>

      </body></html>
