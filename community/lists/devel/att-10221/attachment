<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta content="text/html;charset=ISO-8859-1" http-equiv="Content-Type">
  <title></title>
</head>
<body bgcolor="#ffffff" text="#000000">
Thank you very much. I will try setting the environment variable and if
required also use the 4.1 RC2 version.<br>
<br>
To clarify things a little bit for me, to set up my machine with
GPUDirect v1 I did the following:<br>
<br>
* Install RHEL 5.4<br>
* Use the kernel with GPUDirect support<br>
* Use the MLNX OFED stack with GPUDirect support<br>
* Install the CUDA developer driver<br>
<br>
Does using CUDA&nbsp; &gt;= 4.0&nbsp; make one of the above steps&nbsp; redundant?<br>
<br>
I.e., RHEL or different kernel or MLNX OFED stack with GPUDirect
support is&nbsp; not needed any more?<br>
<br>
Sebastian.<br>
<br>
Rolf vandeVaart wrote:
<blockquote
 cite="mid:3AF945EBF4D3EC41AFE44EED9B0585F34C09AC7863@HQMAIL02.nvidia.com"
 type="cite">
  <pre wrap="">I ran your test case against Open MPI 1.4.2 and CUDA 4.1 RC2 and it worked fine.  I do not have a machine right now where I can load CUDA 4.0 drivers.
Any chance you can try CUDA 4.1 RC2?  There were some improvements in the support (you do not need to set an environment variable for one)
 <a class="moz-txt-link-freetext" href="http://developer.nvidia.com/cuda-toolkit-41">http://developer.nvidia.com/cuda-toolkit-41</a>

There is also a chance that setting the environment variable as outlined in this link may help you.
<a class="moz-txt-link-freetext" href="http://forums.nvidia.com/index.php?showtopic=200629">http://forums.nvidia.com/index.php?showtopic=200629</a>

However, I cannot explain why MVAPICH would work and Open MPI would not.  

Rolf

  </pre>
  <blockquote type="cite">
    <pre wrap="">-----Original Message-----
From: <a class="moz-txt-link-abbreviated" href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a> [<a class="moz-txt-link-freetext" href="mailto:devel-bounces@open-mpi.org">mailto:devel-bounces@open-mpi.org</a>]
On Behalf Of Sebastian Rinke
Sent: Tuesday, January 17, 2012 12:08 PM
To: Open MPI Developers
Subject: Re: [OMPI devel] GPUDirect v1 issues

I use CUDA 4.0 with MVAPICH2 1.5.1p1 and Open MPI 1.4.2.

Attached you find a little test case which is based on the GPUDirect v1 test
case (mpi_pinned.c).
In that program the sender splits a message into chunks and sends them
separately to the receiver which posts the corresponding recvs. It is a kind of
pipelining.

In mpi_pinned.c:141 the offsets into the recv buffer are set.
For the correct offsets, i.e. increasing them, it blocks with Open MPI.

Using line 142 instead (offset = 0) works.

The tarball attached contains a Makefile where you will have to adjust

* CUDA_INC_DIR
* CUDA_LIB_DIR

Sebastian

On Jan 17, 2012, at 4:16 PM, Kenneth A. Lloyd wrote:

    </pre>
    <blockquote type="cite">
      <pre wrap="">Also, which version of MVAPICH2 did you use?

I've been pouring over Rolf's OpenMPI CUDA RDMA 3 (using CUDA 4.1 r2)
vis MVAPICH-GPU on a small 3 node cluster. These are wickedly interesting.

Ken
-----Original Message-----
From: <a class="moz-txt-link-abbreviated" href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a> [<a class="moz-txt-link-freetext" href="mailto:devel-bounces@open">mailto:devel-bounces@open</a>-
      </pre>
    </blockquote>
    <pre wrap="">mpi.org]
    </pre>
    <blockquote type="cite">
      <pre wrap="">On Behalf Of Rolf vandeVaart
Sent: Tuesday, January 17, 2012 7:54 AM
To: Open MPI Developers
Subject: Re: [OMPI devel] GPUDirect v1 issues

I am not aware of any issues.  Can you send me a test program and I
can try it out?
Which version of CUDA are you using?

Rolf

      </pre>
      <blockquote type="cite">
        <pre wrap="">-----Original Message-----
From: <a class="moz-txt-link-abbreviated" href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a> [<a class="moz-txt-link-freetext" href="mailto:devel-bounces@open">mailto:devel-bounces@open</a>-
        </pre>
      </blockquote>
    </blockquote>
    <pre wrap="">mpi.org]
    </pre>
    <blockquote type="cite">
      <blockquote type="cite">
        <pre wrap="">On Behalf Of Sebastian Rinke
Sent: Tuesday, January 17, 2012 8:50 AM
To: Open MPI Developers
Subject: [OMPI devel] GPUDirect v1 issues

Dear all,

I'm using GPUDirect v1 with Open MPI 1.4.3 and experience blocking
MPI_SEND/RECV to block forever.

For two subsequent MPI_RECV, it hangs if the recv buffer pointer of
the second recv points to somewhere, i.e. not at the beginning, in
the recv buffer (previously allocated with cudaMallocHost()).

I tried the same with MVAPICH2 and did not see the problem.

Does anybody know about issues with GPUDirect v1 using Open MPI?

Thanks for your help,
Sebastian
_______________________________________________
devel mailing list
<a class="moz-txt-link-abbreviated" href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a>
        </pre>
      </blockquote>
    </blockquote>
  </blockquote>
  <pre wrap=""><!---->-----------------------------------------------------------------------------------
This email message is for the sole use of the intended recipient(s) and may contain
confidential information.  Any unauthorized review, use, disclosure or distribution
is prohibited.  If you are not the intended recipient, please contact the sender by
reply email and destroy all copies of the original message.
-----------------------------------------------------------------------------------

_______________________________________________
devel mailing list
<a class="moz-txt-link-abbreviated" href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>
<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a>
  </pre>
</blockquote>
<br>
</body>
</html>

