<p>I really would like to help, but I am not sure how much time will I have in the very near future ( we are expecting a babygirl delivery ).&nbsp;<br></p><br><div><span class="gmail_quote">On 8/6/08, <b class="gmail_sendername">Open MPI</b> &lt;<a href="mailto:bugs@open-mpi.org">bugs@open-mpi.org</a>&gt; wrote:</span><blockquote class="gmail_quote" style="margin:0;margin-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex">
#1435: Crash on PPC (with SMT off) when using mpi_paffinity alone<br> -------------------+--------------------------------------------------------<br> <br>Reporter:&nbsp;&nbsp;jnysal&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner:&nbsp;&nbsp;rhc<br> <br>&nbsp;&nbsp;&nbsp;&nbsp;Type:&nbsp;&nbsp;defect&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Status:&nbsp;&nbsp;new<br>
 Priority:&nbsp;&nbsp;major&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;Milestone:&nbsp;&nbsp;Open MPI 1.3<br> <br> Version:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp; Resolution:<br> Keywords:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br> -------------------+--------------------------------------------------------<br> Changes (by rhc):<br>
 <br>&nbsp;&nbsp;* owner:&nbsp;&nbsp;jnysal =&gt; rhc<br>&nbsp;&nbsp;* status:&nbsp;&nbsp;assigned =&gt; new<br> <br> <br> Comment:<br> <br>&nbsp;&nbsp;Several of us have had a telecon on this subject, and have a proposed<br>&nbsp;&nbsp;solution:<br> <br>&nbsp;&nbsp;The real root of the problem here is that we never clearly delineated<br>
&nbsp;&nbsp;between physical and logical processors in OMPI. Instead, there was an<br>&nbsp;&nbsp;implicit assumption that the two were one-and-the-same. Thus, if a user<br>&nbsp;&nbsp;specified a slot_list, we just directly dumped that into the paffinity<br>
&nbsp;&nbsp;subsystem.<br> <br>&nbsp;&nbsp;Unfortunately, when we use paffinity_alone and automatically map the ranks<br>&nbsp;&nbsp;to processors, we again just passed the info the paffinity subsystem -<br>&nbsp;&nbsp;without clearly indicating whether this was a physical processor or<br>
&nbsp;&nbsp;logical processor.<br> <br>&nbsp;&nbsp;Our feeling is that we need to cleanly handle both physical and logical<br>&nbsp;&nbsp;processor specifications. Accordingly, we propose to do the following:<br> <br>&nbsp;&nbsp;1. modify the opal_paffinity_base_get API to add a boolean flag indicating<br>
&nbsp;&nbsp;we want logical (true) or physical (false) processor id&#39;s in the returned<br>&nbsp;&nbsp;cpumask<br> <br>&nbsp;&nbsp;2. modify the opal_paffinity_base_set API to add a boolean flag indicating<br>&nbsp;&nbsp;we provided logical (true) or physical (false) processor id&#39;s in the<br>
&nbsp;&nbsp;cpumask<br> <br>&nbsp;&nbsp;3. modify the opal_paffinity linux and solaris components to do the<br>&nbsp;&nbsp;necessary mapping to handle the two cases so that we bind or return data<br>&nbsp;&nbsp;according to the new flag<br> <br>&nbsp;&nbsp;4. modify ompi_mpi_init so that mpi_paffinity_alone indicates the<br>
&nbsp;&nbsp;automatic binding is to be done on the basis of logical processor id&#39;s<br> <br>&nbsp;&nbsp;5. modify the syntax of the slot_list mca param so that it defaults to<br>&nbsp;&nbsp;logical processor ids, but allows the user to prepend the specification<br>
&nbsp;&nbsp;with a &quot;P&quot; or &quot;p&quot; to indicate these are physical processor id&#39;s. This will<br>&nbsp;&nbsp;also be applied to the parsing of the rank_file mapping file.<br> <br>&nbsp;&nbsp;6. modify the places that utilize that param to handle the new syntax,<br>
&nbsp;&nbsp;including the opal_paffinity_base_slot_list_set and its companion<br>&nbsp;&nbsp;functions<br> <br>&nbsp;&nbsp;7. update the documentation to reflect the changed syntax<br> <br>&nbsp;&nbsp;Terry has volunteered to modify the paffinity components. Ralph will do<br>
&nbsp;&nbsp;the ORTE-level stuff and mpi_init, and likely the slot_list stuff too<br>&nbsp;&nbsp;(unless Lenny has time and is willing to help there?). This will be done<br>&nbsp;&nbsp;on a new Hg branch that Ralph will create - will post the access info here<br>
&nbsp;&nbsp;later today.<br> <br>&nbsp;&nbsp;Any comments? Please post soon so we don&#39;t go too far down path before we<br>&nbsp;&nbsp;hear them!<br> <br><br> --<br> Ticket URL: &lt;<a href="https://svn.open-mpi.org/trac/ompi/ticket/1435#comment:18">https://svn.open-mpi.org/trac/ompi/ticket/1435#comment:18</a>&gt;<br>
 <br>Open MPI &lt;<a href="http://www.open-mpi.org/">http://www.open-mpi.org/</a>&gt;<br> <br> <br> _______________________________________________<br> bugs mailing list<br> <a href="mailto:bugs@open-mpi.org">bugs@open-mpi.org</a><br>
 <a href="http://www.open-mpi.org/mailman/listinfo.cgi/bugs">http://www.open-mpi.org/mailman/listinfo.cgi/bugs</a><br> </blockquote></div><br>

