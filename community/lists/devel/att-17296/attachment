<div dir="ltr"><div><div>Thanks, Jeff. I think Devendar and me are observing the same issue. We&#39;re talking about the same cluster. And I agree with Ralph it must be just a print out error since latency test shows that actual binding seems to be correct.<br><br></div>Best regards,<br></div>Elena<br><div><div><br></div></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Tue, Apr 21, 2015 at 6:17 PM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">+1<br>
<br>
Devendar, you seem to be reporting a different issue than Elena...?  FWIW: Open MPI has always used logical CPU numbering.  As far as I can tell from your output, it looks like Open MPI did the Right Thing with your examples.<br>
<br>
Elena&#39;s example seemed to show conflicting cpu numbering -- where OMPI said it would bind a process and then where it actually bound it.  Ralph mentioned to me that he would look at this as soon as he could; he thinks it might just be an error in the printf output (and that the binding is actually occurring in the right location).<br>
<div><div class="h5"><br>
<br>
<br>
&gt; On Apr 20, 2015, at 9:48 PM, <a href="mailto:tmishima@jcity.maeda.co.jp">tmishima@jcity.maeda.co.jp</a> wrote:<br>
&gt;<br>
&gt; Hi Devendar,<br>
&gt;<br>
&gt; As far as I know, the report-bindings option shows the logical<br>
&gt; cpu order. On the other hand, you are talking about physical one,<br>
&gt; I guess.<br>
&gt;<br>
&gt; Regards,<br>
&gt; Tetsuya Mishima<br>
&gt;<br>
&gt; 2015/04/21 9:04:37、&quot;devel&quot;さんは「Re: [OMPI devel] binding output<br>
&gt; error」で書きました<br>
&gt;&gt; HT is not enabled.  All node are same topo . This is reproducible even on<br>
&gt; single node.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; I ran osu latency to see if it is really is mapped to other socket or not<br>
&gt; with –map-by socket.  It looks likes mapping is correct as per latency<br>
&gt; test.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; $mpirun -np 2 -report-bindings -map-by<br>
&gt; socket  /hpc/local/benchmarks/hpc-stack-icc/install/ompi-mellanox-v1.8/tests/osu-micro-benchmarks-4.4.1/osu_latency<br>
&gt;<br>
&gt;&gt;<br>
&gt;&gt; [clx-orion-001:10084] MCW rank 0 bound to socket 0[core 0[hwt 0]]:<br>
&gt; [B/././././././././././././.][./././././././././././././.]<br>
&gt;&gt;<br>
&gt;&gt; [clx-orion-001:10084] MCW rank 1 bound to socket 1[core 14[hwt 0]]:<br>
&gt; [./././././././././././././.][B/././././././././././././.]<br>
&gt;&gt;<br>
&gt;&gt; # OSU MPI Latency Test v4.4.1<br>
&gt;&gt;<br>
&gt;&gt; # Size          Latency (us)<br>
&gt;&gt;<br>
&gt;&gt; 0                       0.50<br>
&gt;&gt;<br>
&gt;&gt; 1                       0.50<br>
&gt;&gt;<br>
&gt;&gt; 2                       0.50<br>
&gt;&gt;<br>
&gt;&gt; 4                       0.49<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; $mpirun -np 2 -report-bindings -cpu-set<br>
&gt; 1,7 /hpc/local/benchmarks/hpc-stack-icc/install/ompi-mellanox-v1.8/tests/osu-micro-benchmarks-4.4.1/osu_latency<br>
&gt;<br>
&gt;&gt;<br>
&gt;&gt; [clx-orion-001:10155] MCW rank 0 bound to socket 0[core 1[hwt 0]]:<br>
&gt; [./B/./././././././././././.][./././././././././././././.]<br>
&gt;&gt;<br>
&gt;&gt; [clx-orion-001:10155] MCW rank 1 bound to socket 0[core 7[hwt 0]]:<br>
&gt; [./././././././B/./././././.][./././././././././././././.]<br>
&gt;&gt;<br>
&gt;&gt; # OSU MPI Latency Test v4.4.1<br>
&gt;&gt;<br>
&gt;&gt; # Size          Latency (us)<br>
&gt;&gt;<br>
&gt;&gt; 0                       0.23<br>
&gt;&gt;<br>
&gt;&gt; 1                       0.24<br>
&gt;&gt;<br>
&gt;&gt; 2                       0.23<br>
&gt;&gt;<br>
&gt;&gt; 4                       0.22<br>
&gt;&gt;<br>
&gt;&gt; 8                       0.23<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Both hwloc and /proc/cpuinfo indicates following cpu numbering<br>
&gt;&gt;<br>
&gt;&gt; socket 0 cpus: 0 1 2 3 4 5 6 14 15 16 17 18 19 20<br>
&gt;&gt;<br>
&gt;&gt; socket 1 cpus: 7 8 9 10 11 12 13 21 22 23 24 25 26 27<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; $hwloc-info -f<br>
&gt;&gt;<br>
&gt;&gt; Machine (256GB)<br>
&gt;&gt;<br>
&gt;&gt;   NUMANode L#0 (P#0 128GB) + Socket L#0 + L3 L#0 (35MB)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#0 (256KB) + L1 L#0 (32KB) + Core L#0 + PU L#0 (P#0)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#1 (256KB) + L1 L#1 (32KB) + Core L#1 + PU L#1 (P#1)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#2 (256KB) + L1 L#2 (32KB) + Core L#2 + PU L#2 (P#2)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#3 (256KB) + L1 L#3 (32KB) + Core L#3 + PU L#3 (P#3)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#4 (256KB) + L1 L#4 (32KB) + Core L#4 + PU L#4 (P#4)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#5 (256KB) + L1 L#5 (32KB) + Core L#5 + PU L#5 (P#5)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#6 (256KB) + L1 L#6 (32KB) + Core L#6 + PU L#6 (P#6)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#7 (256KB) + L1 L#7 (32KB) + Core L#7 + PU L#7 (P#14)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#8 (256KB) + L1 L#8 (32KB) + Core L#8 + PU L#8 (P#15)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#9 (256KB) + L1 L#9 (32KB) + Core L#9 + PU L#9 (P#16)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#10 (256KB) + L1 L#10 (32KB) + Core L#10 + PU L#10 (P#17)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#11 (256KB) + L1 L#11 (32KB) + Core L#11 + PU L#11 (P#18)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#12 (256KB) + L1 L#12 (32KB) + Core L#12 + PU L#12 (P#19)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#13 (256KB) + L1 L#13 (32KB) + Core L#13 + PU L#13 (P#20)<br>
&gt;&gt;<br>
&gt;&gt;   NUMANode L#1 (P#1 128GB) + Socket L#1 + L3 L#1 (35MB)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#14 (256KB) + L1 L#14 (32KB) + Core L#14 + PU L#14 (P#7)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#15 (256KB) + L1 L#15 (32KB) + Core L#15 + PU L#15 (P#8)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#16 (256KB) + L1 L#16 (32KB) + Core L#16 + PU L#16 (P#9)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#17 (256KB) + L1 L#17 (32KB) + Core L#17 + PU L#17 (P#10)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#18 (256KB) + L1 L#18 (32KB) + Core L#18 + PU L#18 (P#11)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#19 (256KB) + L1 L#19 (32KB) + Core L#19 + PU L#19 (P#12)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#20 (256KB) + L1 L#20 (32KB) + Core L#20 + PU L#20 (P#13)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#21 (256KB) + L1 L#21 (32KB) + Core L#21 + PU L#21 (P#21)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#22 (256KB) + L1 L#22 (32KB) + Core L#22 + PU L#22 (P#22)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#23 (256KB) + L1 L#23 (32KB) + Core L#23 + PU L#23 (P#23)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#24 (256KB) + L1 L#24 (32KB) + Core L#24 + PU L#24 (P#24)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#25 (256KB) + L1 L#25 (32KB) + Core L#25 + PU L#25 (P#25)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#26 (256KB) + L1 L#26 (32KB) + Core L#26 + PU L#26 (P#26)<br>
&gt;&gt;<br>
&gt;&gt;     L2 L#27 (256KB) + L1 L#27 (32KB) + Core L#27 + PU L#27 (P#27)<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; So, Is --reporting-binding shows one more level of logical CPU numbering?<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; -Devendar<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; From:devel [mailto:<a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a>] On Behalf Of Ralph Castain<br>
&gt;&gt; Sent: Monday, April 20, 2015 3:52 PM<br>
&gt;&gt; To: Open MPI Developers<br>
&gt;&gt; Subject: Re: [OMPI devel] binding output error<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Also, was this with HT&#39;s enabled? I&#39;m wondering if the print code is<br>
&gt; incorrectly computing the core because it isn&#39;t correctly accounting for HT<br>
&gt; cpus.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; On Mon, Apr 20, 2015 at 3:49 PM, Jeff Squyres (jsquyres)<br>
&gt; &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; Ralph&#39;s the authority on this one, but just to be sure: are all nodes the<br>
&gt; same topology? E.g., does adding &quot;--hetero-nodes&quot; to the mpirun command<br>
&gt; line fix the problem?<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;&gt; On Apr 20, 2015, at 9:29 AM, Elena Elkina &lt;<a href="mailto:elena.elkina@itseez.com">elena.elkina@itseez.com</a>&gt;<br>
&gt; wrote:<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Hi guys,<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; I faced with an issue on our cluster related to mapping &amp; binding<br>
&gt; policies on 1.8.5.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; The matter is that --report-bindings output doesn&#39;t correspond to the<br>
&gt; locale. It looks like there is a mistake on the output itself, because it<br>
&gt; just puts serial core number while that core can be<br>
&gt;&gt; on another socket. For example,<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; mpirun -np 2 --display-devel-map --report-bindings --map-by socket<br>
&gt; hostname<br>
&gt;&gt;&gt;   Data for JOB [43064,1] offset 0<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;   Mapper requested: NULL  Last mapper: round_robin  Mapping policy:<br>
&gt; BYSOCKET  Ranking policy: SOCKET<br>
&gt;&gt;&gt;   Binding policy: CORE  Cpu set: NULL  PPR: NULL  Cpus-per-rank: 1<br>
&gt;&gt;&gt;        Num new daemons: 0      New daemon starting vpid INVALID<br>
&gt;&gt;&gt;        Num nodes: 1<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;   Data for node: clx-orion-001         Launch id: -1   State: 2<br>
&gt;&gt;&gt;        Daemon: [[43064,0],0]   Daemon launched: True<br>
&gt;&gt;&gt;        Num slots: 28   Slots in use: 2 Oversubscribed: FALSE<br>
&gt;&gt;&gt;        Num slots allocated: 28 Max slots: 0<br>
&gt;&gt;&gt;        Username on node: NULL<br>
&gt;&gt;&gt;        Num procs: 2    Next node_rank: 2<br>
&gt;&gt;&gt;        Data for proc: [[43064,1],0]<br>
&gt;&gt;&gt;                Pid: 0  Local rank: 0   Node rank: 0    App rank: 0<br>
&gt;&gt;&gt;                State: INITIALIZED      Restarts: 0     App_context: 0<br>
&gt; Locale: 0-6,14-20       Bind location: 0        Binding: 0<br>
&gt;&gt;&gt;        Data for proc: [[43064,1],1]<br>
&gt;&gt;&gt;                Pid: 0  Local rank: 1   Node rank: 1    App rank: 1<br>
&gt;&gt;&gt;                State: INITIALIZED      Restarts: 0     App_context: 0<br>
&gt; Locale: 7-13,21-27      Bind location: 7        Binding: 7<br>
&gt;&gt;&gt; [clx-orion-001:26951] MCW rank 0 bound to socket 0[core 0[hwt 0]]:<br>
&gt; [B/././././././././././././.][./././././././././././././.]<br>
&gt;&gt;&gt; [clx-orion-001:26951] MCW rank 1 bound to socket 1[core 14[hwt 0]]:<br>
&gt; [./././././././././././././.][B/././././././././././././.]<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; The second process should be bound at core 7 (not core 14).<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Another example:<br>
&gt;&gt;&gt; mpirun -np 8 --display-devel-map --report-bindings --map-by core<br>
&gt; hostname<br>
&gt;&gt;&gt;   Data for JOB [43202,1] offset 0<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;   Mapper requested: NULL  Last mapper: round_robin  Mapping policy:<br>
&gt; BYCORE  Ranking policy: CORE<br>
&gt;&gt;&gt;   Binding policy: CORE  Cpu set: NULL  PPR: NULL  Cpus-per-rank: 1<br>
&gt;&gt;&gt;        Num new daemons: 0      New daemon starting vpid INVALID<br>
&gt;&gt;&gt;        Num nodes: 1<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;   Data for node: clx-orion-001         Launch id: -1   State: 2<br>
&gt;&gt;&gt;        Daemon: [[43202,0],0]   Daemon launched: True<br>
&gt;&gt;&gt;        Num slots: 28   Slots in use: 8 Oversubscribed: FALSE<br>
&gt;&gt;&gt;        Num slots allocated: 28 Max slots: 0<br>
&gt;&gt;&gt;        Username on node: NULL<br>
&gt;&gt;&gt;        Num procs: 8    Next node_rank: 8<br>
&gt;&gt;&gt;        Data for proc: [[43202,1],0]<br>
&gt;&gt;&gt;                Pid: 0  Local rank: 0   Node rank: 0    App rank: 0<br>
&gt;&gt;&gt;                State: INITIALIZED      Restarts: 0     App_context: 0<br>
&gt; Locale: 0       Bind location: 0        Binding: 0<br>
&gt;&gt;&gt;        Data for proc: [[43202,1],1]<br>
&gt;&gt;&gt;                Pid: 0  Local rank: 1   Node rank: 1    App rank: 1<br>
&gt;&gt;&gt;                State: INITIALIZED      Restarts: 0     App_context: 0<br>
&gt; Locale: 1       Bind location: 1        Binding: 1<br>
&gt;&gt;&gt;        Data for proc: [[43202,1],2]<br>
&gt;&gt;&gt;                Pid: 0  Local rank: 2   Node rank: 2    App rank: 2<br>
&gt;&gt;&gt;                State: INITIALIZED      Restarts: 0     App_context: 0<br>
&gt; Locale: 2       Bind location: 2        Binding: 2<br>
&gt;&gt;&gt;        Data for proc: [[43202,1],3]<br>
&gt;&gt;&gt;                Pid: 0  Local rank: 3   Node rank: 3    App rank: 3<br>
&gt;&gt;&gt;                State: INITIALIZED      Restarts: 0     App_context: 0<br>
&gt; Locale: 3       Bind location: 3        Binding: 3<br>
&gt;&gt;&gt;        Data for proc: [[43202,1],4]<br>
&gt;&gt;&gt;                Pid: 0  Local rank: 4   Node rank: 4    App rank: 4<br>
&gt;&gt;&gt;                State: INITIALIZED      Restarts: 0     App_context: 0<br>
&gt; Locale: 4       Bind location: 4        Binding: 4<br>
&gt;&gt;&gt;        Data for proc: [[43202,1],5]<br>
&gt;&gt;&gt;                Pid: 0  Local rank: 5   Node rank: 5    App rank: 5<br>
&gt;&gt;&gt;                State: INITIALIZED      Restarts: 0     App_context: 0<br>
&gt; Locale: 5       Bind location: 5        Binding: 5<br>
&gt;&gt;&gt;        Data for proc: [[43202,1],6]<br>
&gt;&gt;&gt;                Pid: 0  Local rank: 6   Node rank: 6    App rank: 6<br>
&gt;&gt;&gt;                State: INITIALIZED      Restarts: 0     App_context: 0<br>
&gt; Locale: 6       Bind location: 6        Binding: 6<br>
&gt;&gt;&gt;        Data for proc: [[43202,1],7]<br>
&gt;&gt;&gt;                Pid: 0  Local rank: 7   Node rank: 7    App rank: 7<br>
&gt;&gt;&gt;                State: INITIALIZED      Restarts: 0     App_context: 0<br>
&gt; Locale: 14      Bind location: 14       Binding: 14<br>
&gt;&gt;&gt; [clx-orion-001:27069] MCW rank 0 bound to socket 0[core 0[hwt 0]]:<br>
&gt; [B/././././././././././././.][./././././././././././././.]<br>
&gt;&gt;&gt; [clx-orion-001:27069] MCW rank 1 bound to socket 0[core 1[hwt 0]]:<br>
&gt; [./B/./././././././././././.][./././././././././././././.]<br>
&gt;&gt;&gt; [clx-orion-001:27069] MCW rank 2 bound to socket 0[core 2[hwt 0]]:<br>
&gt; [././B/././././././././././.][./././././././././././././.]<br>
&gt;&gt;&gt; [clx-orion-001:27069] MCW rank 3 bound to socket 0[core 3[hwt 0]]:<br>
&gt; [./././B/./././././././././.][./././././././././././././.]<br>
&gt;&gt;&gt; [clx-orion-001:27069] MCW rank 4 bound to socket 0[core 4[hwt 0]]:<br>
&gt; [././././B/././././././././.][./././././././././././././.]<br>
&gt;&gt;&gt; [clx-orion-001:27069] MCW rank 5 bound to socket 0[core 5[hwt 0]]:<br>
&gt; [./././././B/./././././././.][./././././././././././././.]<br>
&gt;&gt;&gt; [clx-orion-001:27069] MCW rank 6 bound to socket 0[core 6[hwt 0]]:<br>
&gt; [././././././B/././././././.][./././././././././././././.]<br>
&gt;&gt;&gt; [clx-orion-001:27069] MCW rank 7 bound to socket 0[core 7[hwt 0]]:<br>
&gt; [./././././././B/./././././.][./././././././././././././.]<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Rank 7 should be bound at core 14 instead of core 7 since core 7 is at<br>
&gt; another socket.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Best regards,<br>
&gt;&gt;&gt; Elena<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; devel mailing list<br>
&gt;&gt;&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt;&gt;&gt; Subscription:  <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt;&gt;&gt; Link to this post:<br>
&gt; <a href="http://www.open-mpi.org/community/lists/devel/2015/04/17273.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/04/17273.php</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; Jeff Squyres<br>
&gt;&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt;&gt; For corporate legal information go to:<br>
&gt; <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; devel mailing list<br>
&gt;&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt;&gt; Subscription:  <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt;&gt; Link to this post:<br>
&gt; <a href="http://www.open-mpi.org/community/lists/devel/2015/04/17282.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/04/17282.php</a><br>
&gt;&gt;<br>
&gt;&gt;  _______________________________________________<br>
&gt;&gt; devel mailing list<br>
&gt;&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/develLink" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/develLink</a> to<br>
&gt; this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/04/17287.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/04/17287.php</a><br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/04/17291.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/04/17291.php</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</div></div>Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/04/17295.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/04/17295.php</a></blockquote></div><br></div>

