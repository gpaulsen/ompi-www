<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">Hmmm…this looks like it might be that problem we previously saw where the blocking recv hangs in a proc when the blocking send tries to send before the domain socket is actually ready, and so the send fails on the other end. As I recall, it was something to do with the socketoptions - and then Paul had a problem on some of his machines, and we backed it out?<div class=""><br class=""></div><div class="">I wonder if that’s what is biting us here again, and what we need is to either remove the blocking send/recv’s altogether, or figure out a way to wait until the socket is really ready.</div><div class=""><br class=""></div><div class="">Any thoughts?</div><div class=""><br class=""></div><div class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Oct 27, 2015, at 4:11 PM, George Bosilca &lt;<a href="mailto:bosilca@icl.utk.edu" class="">bosilca@icl.utk.edu</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><div dir="ltr" class="">It appear the branch solve the problem at least partially. I asked one of my students to hammer it pretty badly, and he reported that the deadlocks still occur. He also graciously provided some stacktraces:<div class=""><br class=""></div><div class=""><div class="">#0 &nbsp;0x00007f4bd5274aed in nanosleep () from /lib64/libc.so.6</div><div class="">#1 &nbsp;0x00007f4bd52a9c94 in usleep () from /lib64/libc.so.6</div><div class="">#2 &nbsp;0x00007f4bd2e42b00 in OPAL_PMIX_PMIX1XX_PMIx_Fence (procs=0x0, nprocs=0, info=0x7fff3c561960,&nbsp;</div><div class="">&nbsp; &nbsp; ninfo=1) at src/client/pmix_client_fence.c:100</div><div class="">#3 &nbsp;0x00007f4bd306e6d2 in pmix1_fence (procs=0x0, collect_data=1) at pmix1_client.c:306</div><div class="">#4 &nbsp;0x00007f4bd57d5cc3 in ompi_mpi_init (argc=3, argv=0x7fff3c561ea8, requested=3,&nbsp;</div><div class="">&nbsp; &nbsp; provided=0x7fff3c561d84) at runtime/ompi_mpi_init.c:644</div><div class="">#5 &nbsp;0x00007f4bd5813399 in PMPI_Init_thread (argc=0x7fff3c561d7c, argv=0x7fff3c561d70, required=3,&nbsp;</div><div class="">&nbsp; &nbsp; provided=0x7fff3c561d84) at pinit_thread.c:69</div><div class="">#6 &nbsp;0x0000000000401516 in main (argc=3, argv=0x7fff3c561ea8) at osu_mbw_mr.c:86</div></div><div class=""><br class=""></div><div class="">And another process:</div><div class=""><br class=""></div><div class=""><div class="">#0 &nbsp;0x00007f7b9d7d8bdc in recv () from /lib64/libpthread.so.0</div><div class="">#1 &nbsp;0x00007f7b9b0aa42d in opal_pmix_pmix1xx_pmix_usock_recv_blocking (sd=13, data=0x7ffd62139004 "",&nbsp;</div><div class="">&nbsp; &nbsp; size=4) at src/usock/usock.c:168</div><div class="">#2 &nbsp;0x00007f7b9b0af5d9 in recv_connect_ack (sd=13) at src/client/pmix_client.c:844</div><div class="">#3 &nbsp;0x00007f7b9b0b085e in usock_connect (addr=0x7ffd62139330) at src/client/pmix_client.c:1110</div><div class="">#4 &nbsp;0x00007f7b9b0acc24 in connect_to_server (address=0x7ffd62139330, cbdata=0x7ffd621390e0)</div><div class="">&nbsp; &nbsp; at src/client/pmix_client.c:181</div><div class="">#5 &nbsp;0x00007f7b9b0ad569 in OPAL_PMIX_PMIX1XX_PMIx_Init (proc=0x7f7b9b4e9b60)</div><div class="">&nbsp; &nbsp; at src/client/pmix_client.c:362</div><div class="">#6 &nbsp;0x00007f7b9b2dbd9d in pmix1_client_init () at pmix1_client.c:99</div><div class="">#7 &nbsp;0x00007f7b9b4eb95f in pmi_component_query (module=0x7ffd62139490, priority=0x7ffd6213948c)</div><div class="">&nbsp; &nbsp; at ess_pmi_component.c:90</div><div class="">#8 &nbsp;0x00007f7b9ce70ec5 in mca_base_select (type_name=0x7f7b9d20e059 "ess", output_id=-1,&nbsp;</div><div class="">&nbsp; &nbsp; components_available=0x7f7b9d431eb0, best_module=0x7ffd621394d0, best_component=0x7ffd621394d8,&nbsp;</div><div class="">&nbsp; &nbsp; priority_out=0x0) at mca_base_components_select.c:77</div><div class="">#9 &nbsp;0x00007f7b9d1a956b in orte_ess_base_select () at base/ess_base_select.c:40</div><div class="">#10 0x00007f7b9d160449 in orte_init (pargc=0x0, pargv=0x0, flags=32) at runtime/orte_init.c:219</div><div class="">#11 0x00007f7b9da4377a in ompi_mpi_init (argc=3, argv=0x7ffd621397f8, requested=3,&nbsp;</div><div class="">&nbsp; &nbsp; provided=0x7ffd621396d4) at runtime/ompi_mpi_init.c:488</div><div class="">#12 0x00007f7b9da81399 in PMPI_Init_thread (argc=0x7ffd621396cc, argv=0x7ffd621396c0, required=3,&nbsp;</div><div class="">&nbsp; &nbsp; provided=0x7ffd621396d4) at pinit_thread.c:69</div><div class="">#13 0x0000000000401516 in main (argc=3, argv=0x7ffd621397f8) at osu_mbw_mr.c:86</div></div><div class=""><br class=""></div><div class="">&nbsp; George.</div><div class=""><br class=""></div><div class=""><br class=""></div></div><div class="gmail_extra"><br class=""><div class="gmail_quote">On Tue, Oct 27, 2015 at 2:36 PM, Ralph Castain <span dir="ltr" class="">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank" class="">rhc@open-mpi.org</a>&gt;</span> wrote:<br class=""><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word" class="">I haven’t been able to replicate this when using the branch in this PR:<div class=""><br class=""></div><div class=""><a href="https://github.com/open-mpi/ompi/pull/1073" target="_blank" class="">https://github.com/open-mpi/ompi/pull/1073</a></div><div class=""><br class=""></div><div class="">Would you mind giving it a try? It fixes some other race conditions and might pick this one up too.</div><div class=""><div class="h5"><div class=""><br class=""></div><div class=""><br class=""><div class=""><blockquote type="cite" class=""><div class="">On Oct 27, 2015, at 10:04 AM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank" class="">rhc@open-mpi.org</a>&gt; wrote:</div><br class=""><div class=""><div style="word-wrap:break-word" class="">Okay, I’ll take a look - I’ve been chasing a race condition that might be related<div class=""><br class=""><div class=""><blockquote type="cite" class=""><div class="">On Oct 27, 2015, at 9:54 AM, George Bosilca &lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank" class="">bosilca@icl.utk.edu</a>&gt; wrote:</div><br class=""><div class=""><div dir="ltr" class="">No, it's using 2 nodes.<div class="">&nbsp; George.</div><div class=""><br class=""></div></div><div class="gmail_extra"><br class=""><div class="gmail_quote">On Tue, Oct 27, 2015 at 12:35 PM, Ralph Castain <span dir="ltr" class="">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank" class="">rhc@open-mpi.org</a>&gt;</span> wrote:<br class=""><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word" class="">Is this on a single node?<div class=""><br class=""><div class=""><blockquote type="cite" class=""><div class=""><div class=""><div class="">On Oct 27, 2015, at 9:25 AM, George Bosilca &lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank" class="">bosilca@icl.utk.edu</a>&gt; wrote:</div><br class=""></div></div><div class=""><div class=""><div class=""><div dir="ltr" class="">I get intermittent deadlocks wit the latest trunk. The smallest reproducer is a shell for loop around a small (2 processes) short (20 seconds) MPI application. After few tens of iterations the MPI_Init will deadlock with the following backtrace:<div class=""><br class=""></div><div class=""><div style="font-size:12.8px" class="">#0&nbsp; 0x00007fa94b5d9aed in nanosleep () from /lib64/libc.so.6</div><div style="font-size:12.8px" class="">#1&nbsp; 0x00007fa94b60ec94 in usleep () from /lib64/libc.so.6</div><div style="font-size:12.8px" class="">#2&nbsp; 0x00007fa94960ba08 in OPAL_PMIX_PMIX1XX_PMIx_Fence (procs=0x0, nprocs=0, info=0x7ffd7934fb90,&nbsp;</div><div style="font-size:12.8px" class="">&nbsp; &nbsp; ninfo=1) at src/client/pmix_client_fence.c:100</div><div style="font-size:12.8px" class="">#3&nbsp; 0x00007fa9498376a2 in pmix1_fence (procs=0x0, collect_data=1) at pmix1_client.c:305</div><div style="font-size:12.8px" class="">#4&nbsp; 0x00007fa94bb39ba4 in ompi_mpi_init (argc=3, argv=0x7ffd793500a8, requested=3,&nbsp;</div><div style="font-size:12.8px" class="">&nbsp; &nbsp; provided=0x7ffd7934ff94) at runtime/ompi_mpi_init.c:645</div><div style="font-size:12.8px" class="">#5&nbsp; 0x00007fa94bb77281 in PMPI_Init_thread (argc=0x7ffd7934ff8c, argv=0x7ffd7934ff80, required=3,&nbsp;</div><div style="font-size:12.8px" class="">&nbsp; &nbsp; provided=0x7ffd7934ff94) at pinit_thread.c:69</div><div style="font-size:12.8px" class="">#6&nbsp; 0x000000000040150f in main (argc=3, argv=0x7ffd793500a8) at osu_mbw_mr.c:86</div></div><div style="font-size:12.8px" class=""><br class=""></div><div style="font-size:12.8px" class="">On my machines this is reproducible at 100% after anywhere between 50 and 100 iterations.</div><div style="font-size:12.8px" class=""><br class=""></div><div style="font-size:12.8px" class="">&nbsp; Thanks,</div><div style="font-size:12.8px" class="">&nbsp; &nbsp; George.</div><div style="font-size:12.8px" class=""><br class=""></div></div></div></div>
_______________________________________________<br class="">devel mailing list<br class=""><a href="mailto:devel@open-mpi.org" target="_blank" class="">devel@open-mpi.org</a><br class="">Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br class="">Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/10/18280.php" target="_blank" class="">http://www.open-mpi.org/community/lists/devel/2015/10/18280.php</a></div></blockquote></div><br class=""></div></div><br class="">_______________________________________________<br class="">
devel mailing list<br class="">
<a href="mailto:devel@open-mpi.org" target="_blank" class="">devel@open-mpi.org</a><br class="">
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br class="">
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/10/18281.php" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/community/lists/devel/2015/10/18281.php</a><br class=""></blockquote></div><br class=""></div>
_______________________________________________<br class="">devel mailing list<br class=""><a href="mailto:devel@open-mpi.org" target="_blank" class="">devel@open-mpi.org</a><br class="">Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br class="">Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/10/18282.php" target="_blank" class="">http://www.open-mpi.org/community/lists/devel/2015/10/18282.php</a></div></blockquote></div><br class=""></div></div></div></blockquote></div><br class=""></div></div></div></div><br class="">_______________________________________________<br class="">
devel mailing list<br class="">
<a href="mailto:devel@open-mpi.org" class="">devel@open-mpi.org</a><br class="">
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br class="">
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/10/18284.php" rel="noreferrer" target="_blank" class="">http://www.open-mpi.org/community/lists/devel/2015/10/18284.php</a><br class=""></blockquote></div><br class=""></div>
_______________________________________________<br class="">devel mailing list<br class=""><a href="mailto:devel@open-mpi.org" class="">devel@open-mpi.org</a><br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/devel<br class="">Link to this post: http://www.open-mpi.org/community/lists/devel/2015/10/18292.php</div></blockquote></div><br class=""></div></body></html>
