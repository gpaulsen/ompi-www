<div dir="ltr"><div class="gmail_quote">I think it&#39;s a sm bug again I tested with the latest revision, I think it was r19588 ( before Jeff shuted the svn down).</div><div class="gmail_quote"></div><div class="gmail_quote">
I run the mpi_p test ( BW between pairs of nodes ) with many nodes and it got stacked, it also works without sm. &nbsp;I am sorry I couldn&#39;t test it earlier.</div><div class="gmail_quote"></div><div class="gmail_quote"># i=1 ; while [ 1 ] ; do echo &quot;  ******************  i=$i ******** &quot;; /home/USERS/lenny/OMPI_ORTE_TRUNK/bin/mpirun -np 84 -hostfile hostfile  /home/USERS/lenny/TESTS/TRUNK/mpi_p1_4_TRUNK  -t bw ; let i=i+1; sleep 1 ; done<br>
&nbsp; ******************  i=1 ********<br>BW (84) (size min max avg)  1048576     660.152249      2075.115025     1325.838953<br>&nbsp; ******************  i=2 ********<br>[stucked]<br><br></div><div class="gmail_quote"></div><div class="gmail_quote">
p.s. I will be on vacation until 5-Oct, I hope to fallow mails and run few tests.</div><div class="gmail_quote"></div><div class="gmail_quote">Best Regards</div><div class="gmail_quote">Lenny.</div><div class="gmail_quote">
</div><div class="gmail_quote">On Thu, Sep 25, 2008 at 6:44 PM, Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
Note that there *are* other changes to the openib BTL in that branch besides just the CPC (meaning: changing the CPC meant changing other things as well).<br>
<br>
So if you can run with the trunk and you can&#39;t run with this branch, then there may be something different wrong with the hg tree other than just the RDMA CM stuff...<br>
<br>
Let me know what you find.<div><div class="Wj3C7c"><br>
<br>
<br>
On Sep 25, 2008, at 9:21 AM, Lenny Verkhovsky wrote:<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
after few more tests is seems like -mca btl_openib_cpc_include oob hangs too.<br>
<br>
so, maybe it&#39;s something environmental.<br>
<br>
let me recheck it.<br>
<br>
<br>
On 9/25/08, Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt; wrote: On Sep 25, 2008, at 7:25 AM, Lenny Verkhovsky wrote:<br>
<br>
I have RDMACM got hanged on np=16 ( dual core dual cpu).<br>
<br>
<br>
Yuck. &nbsp;I&#39;ve run all of the intel tests at 32 procs (4ppn). &nbsp;What exactly did you run and where exactly did it hang? &nbsp;Can you get stack traces?<br>
<br>
it seems like it got hanged on the last machine ( witch1,witch2,witch3,witch4)<br>
<br>
when I ctrl-c the mpirun, I got defunct procs on the last machine.<br>
<br>
#ps -ef |grep &nbsp;mpi<br>
root 5321 5320 98 14:09 ? 00:03:47 [mpi_p_TRUNK_rdm] &lt;defunct&gt;<br>
root 5322 5320 98 14:09 ? 00:03:47 [mpi_p_TRUNK_rdm] &lt;defunct&gt;<br>
root 5323 5320 98 14:09 ? 00:03:47 [mpi_p_TRUNK_rdm] &lt;defunct&gt;<br>
root 5324 5320 98 14:09 ? 00:03:47 [mpi_p_TRUNK_rdm] &lt;defunct&gt;<br>
<br>
<br>
Are you seeing ORTE problems?<br>
<br>
-- <br>
Jeff Squyres<br>
Cisco Systems<br>
<br>
<br>
</blockquote>
<br>
<br></div></div><font color="#888888">
-- <br>
Jeff Squyres<br>
Cisco Systems<br>
<br>
</font></blockquote></div><br></div>

