<div dir="ltr">Hi,<div><br></div><div style>It looks like there is a problem in trunk which reproduces with simple_spawn test (orte/test/mpi/simple_spawn.c). It seems to be a n issue with pmix. It doesn&#39;t reproduce with default set of btls. But it reproduces with several btls specified. For example,</div><div style> </div><div style>salloc -N5 $OMPI_HOME/install/bin/mpirun -np 33 --map-by node -mca coll ^ml -display-map -mca orte_debug_daemons true --leave-session-attached --debug-daemons -mca pml ob1 -mca btl <b>tcp,self</b> ./orte/test/mpi/simple_spawn<br></div><div style><br></div><div style>gets</div><div style><br></div><div style><div><div>simple_spawn: ../../ompi/group/group_init.c:215: ompi_group_increment_proc_count: Assertion `((0xdeafbeedULL &lt;&lt; 32) + 0xdeafbeedULL) == ((opal_object_t *) (proc_pointer))-&gt;obj_magic_id&#39; failed.<br></div><div>[<a href="http://sputnik3.vbench.com:28888/" target="_blank">sputnik3.vbench.com:28888</a>] [[41877,0],3] orted_cmd: exit cmd, but proc [[41877,1],2] is alive</div><div>[sputnik5][[41877,1],29][../../../../../opal/mca/btl/tcp/btl_tcp_endpoint.c:675:mca_btl_tcp_endpoint_complete_connect] connect() to 192.168.1.42 failed: Connection refused (111)</div></div><div><br></div></div><div style>salloc -N1 $OMPI_HOME/install/bin/mpirun -np 3 --map-by node -mca coll ^ml -display-map -mca orte_debug_daemons true --leave-session-attached --debug-daemons -mca pml ob1 -mca btl <b>sm,self</b> ./orte/test/mpi/simple_spawn<br></div><div style><br></div><div style>fails with</div><div style><br></div><div style><div>At least one pair of MPI processes are unable to reach each other for</div><div>MPI communications.  This means that no Open MPI device has indicated</div><div>that it can be used to communicate between these processes.  This is</div><div>an error; Open MPI requires that all MPI processes be able to reach</div><div>each other.  This error can sometimes be the result of forgetting to</div><div>specify the &quot;self&quot; BTL.</div><div><br></div><div>  Process 1 ([[59481,2],0]) is on host: sputnik1</div><div>  Process 2 ([[59481,1],0]) is on host: sputnik1</div><div>  BTLs attempted: self sm</div><div><br></div><div>Your MPI job is now going to abort; sorry.</div><div>--------------------------------------------------------------------------</div><div>[<a href="http://sputnik1.vbench.com:22156">sputnik1.vbench.com:22156</a>] [[59481,1],2] ORTE_ERROR_LOG: Unreachable in file ../../../../../ompi/mca/dpm/orte/dpm_orte.c at line 485</div><div><br></div><div><br></div><div>salloc -N1 $OMPI_HOME/install/bin/mpirun -np 3 --map-by node -mca coll ^ml -display-map -mca orte_debug_daemons true --leave-session-attached --debug-daemons -mca pml ob1 -mca btl <b>openib,self</b> ./orte/test/mpi/simple_spawn<br></div></div><div style><div><br></div><div style>also doesn&#39;t work:</div><div style><div>At least one pair of MPI processes are unable to reach each other for</div><div>MPI communications.  This means that no Open MPI device has indicated</div><div>that it can be used to communicate between these processes.  This is</div><div>an error; Open MPI requires that all MPI processes be able to reach</div><div>each other.  This error can sometimes be the result of forgetting to</div><div>specify the &quot;self&quot; BTL.</div><div><br></div><div>  Process 1 ([[60046,1],13]) is on host: sputnik4</div><div>  Process 2 ([[60046,2],1]) is on host: sputnik4</div><div>  BTLs attempted: openib self</div><div><br></div><div>Your MPI job is now going to abort; sorry.</div><div>--------------------------------------------------------------------------</div><div>[<a href="http://sputnik4.vbench.com:25476">sputnik4.vbench.com:25476</a>] [[60046,1],3] ORTE_ERROR_LOG: Unreachable in file ../../../../../ompi/mca/dpm/orte/dpm_orte.c at line 485</div><div><br></div></div><div style><br></div><div style><b>But</b> combination ^sm,openib seems to work.</div><div><br></div><div style>I tried different revisions from the beginning of October. It reproduces on them.</div><div style><br></div><div style>Best regards,</div><div style>Elena</div><div><br></div></div></div>

