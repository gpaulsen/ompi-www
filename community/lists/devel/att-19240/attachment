<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    <p>Howard,</p>
    <p><br>
    </p>
    <p>did you bump both btl_tcp_rndv_eager_limit and
      btl_tcp_eager_limit ?</p>
    <p>you might also need to bump btl_tcp_sndbuf, btl_tcp_rcvbuf and
      btl_tcp_max_send_size to get the max performance out of your 100Gb
      ethernet cards</p>
    <p>last but not least, you might also need to bump btl_tcp_links to
      saturate your network (that is likely a good thing when running 1
      task per node, but that can lead to decreased performance when
      running several tasks per node)<br>
    </p>
    <p>Cheers,</p>
    <p><br>
    </p>
    <p>Gilles<br>
    </p>
    <br>
    <div class="moz-cite-prefix">On 7/19/2016 6:57 AM, Howard Pritchard
      wrote:<br>
    </div>
    <blockquote
cite="mid:CAF1Cqj7D380DTAa4R_Pb_juyedLK4uUeocSrQ7jNnxsSKz2f1w@mail.gmail.com"
      type="cite">
      <div dir="ltr">Hi Folks,
        <div><br>
        </div>
        <div>I have a cluster with some 100 Gb ethernet cards</div>
        <div>installed.  What we are noticing if we force Open MPI
          1.10.3</div>
        <div>to go through the TCP BTL (rather than yalla)  is that</div>
        <div>the performance of osu_bw once the TCP BTL switches</div>
        <div>from eager to rendezvous (&gt; 32KB)</div>
        <div>falls off a cliff, going from about 1.6 GB/sec to 233
          MB/sec</div>
        <div>and stays that way out to 4 MB message lengths.</div>
        <div><br>
        </div>
        <div>There's nothing wrong with the IP stack (iperf -P4 gives </div>
        <div>63 Gb/sec).</div>
        <div><br>
        </div>
        <div>So, my questions are </div>
        <div><br>
        </div>
        <div>1) is this performance expected for the TCP BTL when in</div>
        <div>rendezvous mode?</div>
        <div>2) is there some way to get more like the single socket</div>
        <div>performance obtained with iperf for large messages (~16
          Gb/sec).</div>
        <div><br>
        </div>
        <div>We tried adjusting the tcp_btl_rendezvous threshold but
          that doesn't</div>
        <div>appear to actually be adjustable from the mpirun command
          line.</div>
        <div><br>
        </div>
        <div>Thanks for any suggestions,</div>
        <div><br>
        </div>
        <div>Howard</div>
        <div><br>
        </div>
        <div><br>
        </div>
        <div><br>
        </div>
      </div>
      <br>
      <fieldset class="mimeAttachmentHeader"></fieldset>
      <br>
      <pre wrap="">_______________________________________________
devel mailing list
<a class="moz-txt-link-abbreviated" href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="https://www.open-mpi.org/mailman/listinfo.cgi/devel">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/devel/2016/07/19237.php">http://www.open-mpi.org/community/lists/devel/2016/07/19237.php</a></pre>
    </blockquote>
    <br>
  </body>
</html>

