<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">Hi Gilles,<div>thanks for thinking about this in more detail.</div><div><div><br></div><div>I understand what you are saying, but your comments raise some questions in my mind:</div><div><br></div><div>If one is in a homogeneous cluster, is it important that, in the case of little-endian, that the data be</div><div>converted to extern32 format (big-endian), only to be always converted at the receiving rank</div><div>back to little-endian?</div><div><br></div><div>This would seem to be inefficient, especially if the site has no need for external MPI access.</div><div><br></div><div>So, does --enable-heterogeneous do more than put MPI routines using "extern32" into straight pass-through?</div><div><br></div><div>Back in the old days of PVM, all messages were converted into network order. This had severe performance impacts</div><div>on little-endian clusters.</div><div><br></div><div>So much so that a clever way of getting around this was an implementation of "receiver makes right" in which</div><div>all data was sent in the native format of the sending rank. The receiving rank analysed the message to determine if</div><div>a conversion was necessary. In those days with Cray format data, it could be more complicated than just byte swapping.</div><div><br></div><div>So in essence, how is a balance struck between supporting heterogenous architectures and maximum performance</div><div>with codes where message passing performance is critical?</div><div><br></div><div>As a follow up, since I am now at home, this same problem also exists with the Ubuntu 15.10 OpenMP packages</div><div>which surprisingly are still at 1.6.5, same as 14.04.</div><div><br></div><div>Again, downloading, building, and using the latest stable version of OpenMP solved the problem.</div><div><br></div><div>kindest regards</div><div>Mike</div><div><br></div><div><br></div><div><div><div>On 11/02/2016, at 7:31 PM, Gilles Gouaillardet wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite">Michael,<div><br></div><div>I think it is worst than that ...</div><div><br></div><div>without --enable-heterogeneous, it seems the data is not correctly packed</div><div>(e.g. it is not converted to big endian), at least on a x86_64 arch.</div><div>unpack looks broken too, but pack followed by unpack does work.</div><div>that means if you are reading data correctly written in external32e format,</div><div>it will not be correctly unpacked.</div><div><br></div><div>with --enable-heterogeneous, it is only half broken</div><div>(I&nbsp;do not know yet whether pack or unpack is broken ...)</div><div>and pack followed by unpack does not work.</div><div><br></div><div>I will double check that tomorrow</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<br><br>On Thursday, February 11, 2016, Michael Rezny &lt;<a href="mailto:michael.rezny@monash.edu">michael.rezny@monash.edu</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><div><div><div><div><div><div><div><div>Hi Ralph,<br></div>you are indeed correct. However, many of our users<br></div>have workstations such as me, with OpenMPI provided by installing a package.<br></div><div>So we don't know what has been configured.<br></div><div><br></div>Then we have failures, since, for instance, Ubuntu 14.04 by default appears to have been built<br></div>with heterogeneous support! The other (working) machine is a large HPC, and it seems OpenMPI was built<br></div><div>without heterogeneous support.<br></div><div><br></div>Currently we work around the problem for packing and unpacking by having a compiler switch<br></div>that will switch between calls to pack/unpack_external and pac/unpack.<br><br></div>It is only now we started to track down what the problem actually is.<br><br></div>kindest regards<br></div>Mike<br></div><div class="gmail_extra"><br><div class="gmail_quote">On 11 February 2016 at 15:54, Ralph Castain <span dir="ltr">&lt;<a href="javascript:_e(%7B%7D,'cvml','rhc@open-mpi.org');" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">Out of curiosity: if both systems are Intel, they why are you enabling hetero? You don’t need it in that scenario.<div><br></div><div>Admittedly, we do need to fix the bug - just trying to understand why you are configuring that way.</div><div><br></div><div><br><div><blockquote type="cite"><div>On Feb 10, 2016, at 8:46 PM, Michael Rezny &lt;<a href="javascript:_e(%7B%7D,'cvml','michael.rezny@monash.edu');" target="_blank">michael.rezny@monash.edu</a>&gt; wrote:</div><br><div><div dir="ltr"><div><div><div><div><div><div>Hi Gilles,<br></div>I can confirm that with a fresh download and build from source for OpenMPI 1.10.2<br></div><div>with --enable-heterogeneous<br></div>the unpacked ints are the wrong endian.<br><br></div>However, without --enable-heterogeneous, the unpacked ints are correct.<br><br></div>So, this problem still exists in heterogeneous builds with OpenMPI version 1.10.2.<br><br></div>kindest regards<br></div>Mike<br></div><div class="gmail_extra"><br><div class="gmail_quote">On 11 February 2016 at 14:48, Gilles Gouaillardet <span dir="ltr">&lt;<a href="javascript:_e(%7B%7D,'cvml','gilles.gouaillardet@gmail.com');" target="_blank">gilles.gouaillardet@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Michael,<div><br></div><div>does your two systems have the same endianness ?</div><div><br></div><div>do you know how openmpi was configure'd on both systems ?</div><div>(is --enable-heterogeneous enabled or disabled on both systems ?)</div><div><br></div><div>fwiw, openmpi 1.6.5 is old now and no more maintained.</div><div>I strongly encourage you to use openmpi 1.10.2</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles</div><div><br>On Thursday, February 11, 2016, Michael Rezny &lt;<a href="javascript:_e(%7B%7D,'cvml','michael.rezny@monash.edu');" target="_blank">michael.rezny@monash.edu</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><div><div><div><div><div>Hi,<br></div>I am running Ubuntu 14.04 LTS with OpenMPI 1.6.5 and gcc 4.8.4<br><br></div>On a single rank program which just packs and unpacks two ints using MPI_Pack_external and MPI_Unpack_external<br></div>the unpacked ints are in the wrong endian order.<br><br></div>However, on a HPC, (not Ubuntu), using OpenMPI 1.6.5 and gcc 4.8.4 the unpacked ints are correct.<br><br></div><div>Is it possible to get some assistance to track down what is going on?<br></div><div><br></div>Here is the output from the program:<br><br>&nbsp;<span style="font-family:monospace,monospace">~/tests/mpi/Pack test1<br>send data 000004d2 0000162e <br>MPI_Pack_external: 0<br>buffer size: 8<br>MPI_unpack_external: 0<br>recv data d2040000 2e160000 </span><br><br></div>And here is the source code:<br><br><span style="font-family:monospace,monospace">#include &lt;stdio.h&gt;<br>#include &lt;mpi.h&gt;<br><br>int main(int argc, char *argv[]) {<br>&nbsp; int numRanks, myRank, error;<br><br>&nbsp; int send_data[2] = {1234, 5678};<br>&nbsp; int recv_data[2];<br><br>&nbsp; MPI_Aint buffer_size = 1000;<br>&nbsp; char buffer[buffer_size];<br><br>&nbsp; MPI_Init(&amp;argc, &amp;argv);<br>&nbsp; MPI_Comm_size(MPI_COMM_WORLD, &amp;numRanks);<br>&nbsp; MPI_Comm_rank(MPI_COMM_WORLD, &amp;myRank);<br><br>&nbsp; printf("send data %08x %08x \n", send_data[0], send_data[1]);<br><br>&nbsp; MPI_Aint position = 0;<br>&nbsp; error = MPI_Pack_external("external32", (void*) send_data, 2, MPI_INT,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; buffer, buffer_size, &amp;position);<br>&nbsp; printf("MPI_Pack_external: %d\n", error);<br><br>&nbsp; printf("buffer size: %d\n", (int) position);<br><br>&nbsp; position = 0;<br>&nbsp; error = MPI_Unpack_external("external32", buffer, buffer_size, &amp;position,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; recv_data, 2, MPI_INT);<br>&nbsp; printf("MPI_unpack_external: %d\n", error);<br><br>&nbsp; printf("recv data %08x %08x \n", recv_data[0], recv_data[1]);<br><br>&nbsp; MPI_Finalize();<br><br>&nbsp; return 0;<br>}<br></span><br><br></div>
</blockquote></div>
<br>_______________________________________________<br>
devel mailing list<br>
<a href="javascript:_e(%7B%7D,'cvml','devel@open-mpi.org');" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/02/18573.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/02/18573.php</a><br></blockquote></div><br></div>
_______________________________________________<br>devel mailing list<br><a href="javascript:_e(%7B%7D,'cvml','devel@open-mpi.org');" target="_blank">devel@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/02/18575.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/02/18575.php</a></div></blockquote></div><br></div></div><br>_______________________________________________<br>
devel mailing list<br>
<a href="javascript:_e(%7B%7D,'cvml','devel@open-mpi.org');" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/02/18576.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/02/18576.php</a><br></blockquote></div><br></div>
</blockquote></div>
_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/devel<br>Link to this post: http://www.open-mpi.org/community/lists/devel/2016/02/18579.php</blockquote></div><br></div></div></body></html>
