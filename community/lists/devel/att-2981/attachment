<html><head><style type="text/css"><!-- DIV {margin:0px;} --></style></head><body><div style="font-family:times new roman, new york, times, serif;font-size:10pt">Thank you so much for the help and guidance. I think i partially understand the send process. To have a good feel of the OpenMPI code, if I try to exchange information through GPR, I am getting funny problem. Its funny as in, i know the reason, I just cant seem to locate it.<br><br>Just for reference, I am trying to port btl/tcp to xensockets. Now if i want to do modex send/recv , to my understanding, mca_btl_tcp_addr_t is used (ref code/function is mca_btl_tcp_component_exchange). For xensockets, I need to send only one additional integer remote_domU_id across to say all the peers (in refined code it would be specific to each domain, but i just want to have clear understanding before i move any further). Now I have changed the struct mca_btl_tcp_addr_t present in btl_tcp_addr.h and have added
 int r_domu_id. This makes the size of structure 12. Upon receive mca_btl_tcp_proc_create() gives an error after mca_pml_base_modex_recv() and at this statement if(0 != (size % sizeof(mca_btl_tcp_addr_t))) that size do not match. It is still expecting size 8, where as i have made the size 12.&nbsp; I am unable to pin point the exact location where the size 8 is still embedded. Any ideas?<br><br>Second question is regarding the receive part of openmpi. In my understanding, once Recv api is called, the control goes through PML layer and everything initializes there. However, I am unable to get a lock at the layer/file/function where the receive socket polling is done. There are callbacks, but where or how exactly the openMPI knows that message has in fact arrived. Any pointer will do :)<br><br><div>&nbsp;</div>Best Regards,<br>Muhammad Atif<div style="font-family: times new roman,new york,times,serif; font-size: 10pt;">PS: Sorry if my questions are too
 basic. <br><br><div style="font-family: times new roman,new york,times,serif; font-size: 12pt;">----- Original Message ----<br>From: Jeff Squyres &lt;jsquyres@cisco.com&gt;<br>To: Open MPI Developers &lt;devel@open-mpi.org&gt;<br>Sent: Friday, January 11, 2008 1:02:31 PM<br>Subject: Re: [OMPI devel] btl tcp port to xensocket<br><br>
<br>On Jan 10, 2008, at 8:40 PM, Muhammad Atif wrote:<br><br>&gt; Hi,<br>&gt; Thanks for such a detailed reply. You are right, we have partitioned
&nbsp; <br>&gt; (normalized) our system with Xen and have seen that virtualization&nbsp; <br>&gt; overhead is not that great (for some applications) as compared to&nbsp; <br>&gt; potential benefits that we can get. We have executed various&nbsp; <br>&gt; benchmarks on different network/cluster configuration of Xen and&nbsp; <br>&gt; Native linux and they are really encouraging. The only known problem
&nbsp; <br>&gt; is inter-domain communication of Xen which is quite poor (1/6 of the
&nbsp; <br>&gt; native memory transfer and not to mention 50%CPU utilization of&nbsp; <br>&gt; host). We have tested out Xensocket, and these sockets give us&nbsp; <br>&gt; really good performance boost in all respects.<br>&gt; Now that I am having a look at the complex yet wonderful&nbsp; <br>&gt; architecture of openmpi, can you guys give me some guidance on&nbsp; <br>&gt; couple of naive questions?<br>&gt;<br>&gt; 1- How do I view the console output of an mpi process which is not&nbsp; <br>&gt; at headnode? Do I have to have some parallel debugger? Or is there&nbsp; <br>&gt; any magical technique?<br><br>OMPI's run-time environment takes care of redirection stdout/stderr&nbsp; <br>from each MPI process to the stdout/stderr of mpirun for you (this is&nbsp; <br>another use of the "out of band" TCP channel that is setup between&nbsp; <br>mpirun and all the MPI processes).<br><br>&gt;<br>&gt; 2- How do i setup GPR?<br><br>You don't.&nbsp; The GPR is
 automatically instantiated in mpirun upon&nbsp; <br>startup.<br><br>&gt; say i have a struct foo, and all processes have at least one such&nbsp; <br>&gt; instance of foo. From what I gather, openmpi will create a linked&nbsp; <br>&gt; list of foo's that were passed on (though I am unable to pass one&nbsp; <br>&gt; on). Where do i have to define struct foo so that it can be&nbsp; <br>&gt; exchanged b/w the processes? I know its a lame question, but I think
&nbsp; <br>&gt; i am getting lost in the sea. :(<br><br>I assume you're asking about the modex.<br><br>Every BTL defines its own data that is passed around in the modex.&nbsp; It
&nbsp; <br>is assumed that only modules of the same BTL type will be able to read/
 <br>understand that data.&nbsp; The modex just treats the data as a blob; all&nbsp; <br>the modex blobs are gathered into mpirun and then broadcast out to&nbsp; <br>every MPI process (I said scatter in my previous mail; broadcast is&nbsp; <br>more accurate).<br><br>So when you modex_send, you just pass a pointer to a chunk of memory&nbsp; <br>and a length (e.g., a pointer to a struct instance and a length).&nbsp;  <br>When you modex_receive, you can just dereference the blob that you&nbsp; <br>return as the same struct type as you modex_send'ed previously&nbsp; <br>(because you can only receive blobs from BTL modules that are the same
&nbsp; <br>type as you, and therefore the data they sent is the same type of data
&nbsp; <br>that you sent).<br><br>You can do more complex things in the modex if you need to, of&nbsp; <br>course.&nbsp; For example, we're changing the openib BTL to send variable&nbsp; <br>length data in the modex, but that requires a bit more setup and I&nbsp; <br>suspect you don't need to do this.<br><br>&gt;<br>&gt; Best Regards,<br>&gt; Muhammad Atif<br>&gt; PS: I am totally new to MPI internals. So if at all we decide to go&nbsp; <br>&gt; ahead with the project, I would be regular bugger in the list.<br><br>That's what we're here for.&nbsp; We don't always reply immediately, but we
&nbsp; <br>try.&nbsp; :-)<br><br>&gt;<br>&gt; ----- Original Message ----<br>&gt; From: Adrian Knoth &lt;<a ymailto="mailto:adi@drcomp.erfurt.thur.de" href="mailto:adi@drcomp.erfurt.thur.de">adi@drcomp.erfurt.thur.de</a>&gt;<br>&gt; To: Open MPI Developers &lt;<a ymailto="mailto:devel@open-mpi.org" href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>&gt;<br>&gt; Sent: Thursday, January 10, 2008 1:24:01 AM<br>&gt; Subject: Re: [OMPI devel] btl tcp port to xensocket<br>&gt;<br>&gt; On Tue, Jan 08, 2008 at 10:51:45PM -0800, Muhammad Atif wrote:<br>&gt;<br>&gt; &gt; I am planning to port tcp component to xensocket, which is a fast<br>&gt; &gt; interdomain communication mechanism for guest domains in Xen. I may<br>&gt;<br>&gt; Just to get things right: You first partition your SMP/Multicore&nbsp; <br>&gt; system<br>&gt; with Xen, and then want to re-combine it later for MPI communication?<br>&gt;<br>&gt; Wouldn't it be easier to leave the unpartitioned
 host alone and use<br>&gt; shared memory communication instead?<br>&gt;<br>&gt; &gt; As per design, and the fact that these sockets are not normal&nbsp; <br>&gt; sockets,<br>&gt; &gt; I have to pass certain information (basically memory references,&nbsp; <br>&gt; guest<br>&gt; &gt; domain info etc) to other peers once sockets have been created. I<br>&gt;<br>&gt; There's ORTE, the runtime environment. It employs OOB/tcp to have a
 so<br>&gt; called out-of-band channel. ORTE also provides a general purpose<br>&gt; registry (GPR).<br>&gt;<br>&gt; Once a TCP connection between the headnode process and all other
 peers<br>&gt; is established, you can store your required information in the GPR.<br>&gt;<br>&gt; &gt; understand that mca_pml_base_modex_send and recv (or simply using<br>&gt; &gt; mca_btl_tcp_component_exchange) can be used to exchange
 information,<br>&gt;<br>&gt; Use mca_pml_base_modex_send (now ompi_modex_send) and encode your<br>&gt; required information. It's getting stored in the GPR. Read it back&nbsp; <br>&gt; with<br>&gt; mca_pml_base_modex_recv (ompi_modex_recv), as it is done in<br>&gt; mca_btl_tcp_component_exchange and mca_btl_tcp_proc_create.<br>&gt;<br>&gt; &gt; but I cannot seem to get them to communicate. So to put my&nbsp; <br>&gt; question in<br>&gt; &gt; a very simple way..... I want to create a socket structure&nbsp; <br>&gt; containing<br>&gt; &gt; necessary information, and then pass it to all other peers before<br>&gt; &gt; start of actual mpi communication. What is the easiest way to do
 it.<br>&gt;<br>&gt;<br>&gt; Quite the same way. mca_btl_tcp_component_exchange assembles the<br>&gt; required information and stores it in the GPR by calling<br>&gt; ompi_modex_send.<br>&gt;<br>&gt; mca_btl_tcp_proc_create (think of "the other peers") reads this<br>&gt; information into local context.<br>&gt;<br>&gt;<br>&gt; I guess you might want to copy btl/tcp to let's say btl/xen, so you&nbsp; <br>&gt; can<br>&gt; modify internal structures, if required. Perhaps xensockets don't
 need<br>&gt; IP addresses, as they are actually memory sockets.<br>&gt;<br>&gt; However, you'll still need TCP communication between Xen guests for&nbsp; <br>&gt; the<br>&gt; OOB channel.<br>&gt;<br>&gt;<br>&gt; As mentioned above, I'm not sure if it's reasonable to use Xen and
 MPI<br>&gt; at all. Virtualization overhead might decrease your performance, and<br>&gt; that's usually the last thing you want to have when using MPI ;)<br>&gt;<br>&gt;<br>&gt; HTH<br>&gt;<br>&gt; -- <br>&gt; Cluster and Metacomputing Working Group<br>&gt; Friedrich-Schiller-Universität Jena, Germany<br>&gt;<br>&gt; private: <a href="http://adi.thur.de" target="_blank">http://adi.thur.de</a><br>&gt; _______________________________________________<br>&gt; devel mailing list<br>&gt; <a ymailto="mailto:devel@open-mpi.org" href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>&gt;<br>&gt;<br>&gt; Be a better friend, newshound, and know-it-all with Yahoo! Mobile.&nbsp; <br>&gt; Try it now._______________________________________________<br>&gt; devel mailing list<br>&gt; <a ymailto="mailto:devel@open-mpi.org"
 href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br><br><br>-- <br>Jeff Squyres<br>Cisco Systems<br><br><br>_______________________________________________<br>devel mailing list<br><a ymailto="mailto:devel@open-mpi.org" href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br></div><br></div></div><br>

      <hr size=1>Never miss a thing.  <a href="http://us.rd.yahoo.com/evt=51438/*http://www.yahoo.com/r/hs"> Make Yahoo your homepage.</a>

</body></html>
