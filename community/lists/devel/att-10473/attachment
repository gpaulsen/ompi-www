
<br><tt><font size=2>devel-bounces@open-mpi.org wrote on 02/17/2012 08:36:54
AM:<br>
<br>
&gt; De : Brice Goglin &lt;Brice.Goglin@inria.fr&gt;</font></tt>
<br><tt><font size=2>&gt; A : devel@open-mpi.org</font></tt>
<br><tt><font size=2>&gt; Date : 02/17/2012 08:37 AM</font></tt>
<br><tt><font size=2>&gt; Objet : Re: [OMPI devel] btl/openib: get_ib_dev_distance
doesn't see<br>
&gt; processes as bound if the job has been launched by srun</font></tt>
<br><tt><font size=2>&gt; Envoyé par : devel-bounces@open-mpi.org</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; Le 16/02/2012 14:16, nadia.derbey@bull.net a écrit : </font></tt>
<br><tt><font size=2>&gt; Hi Jeff, <br>
&gt; <br>
&gt; Sorry for the delay, but my victim with 2 ib devices had been stolen
;-) <br>
&gt; <br>
&gt; So, I ported the patch on the v1.5 branch and finally could test it.
<br>
&gt; <br>
&gt; Actually, there is no opal_hwloc_base_get_topology() in v1.5 so I
had to set <br>
&gt; the hwloc flags in ompi_mpi_init() and orte_odls_base_open() (i.e.
the places<br>
&gt; where opal_hwloc_topology is initialized). <br>
&gt; <br>
&gt; With the new flag set, hwloc_get_nbobjs_by_type(opal_hwloc_topology,<br>
&gt; HWLOC_OBJ_CORE) <br>
&gt; is now seeing the actual number of cores on the node (instead of 1
when our <br>
&gt; cpuset is a singleton). <br>
&gt; <br>
&gt; Since opal_paffinity_base_get_processor_info() calls <br>
&gt; module_get_processor_info() <br>
&gt; (in hwloc/paffinity_hwloc_module.c), which in turn calls <br>
&gt; hwloc_get_nbobjs_by_type(), <br>
&gt; we are now getting the right number of cores in get_ib_dev_distance().
<br>
&gt; <br>
&gt; So we are looping over the exact number of cores, looking for a <br>
&gt; potential binding. <br>
&gt; <br>
&gt; So as a conclusion, there's no need for any other patch: the fix youcommitted<br>
&gt; was the only one needed to fix the issue. </font></tt>
<br><tt><font size=2>&gt; <br>
&gt; I didn't follow this entire thread in details, but I am feeling that<br>
&gt; something is wrong here. The flag fixes your problem indeed, but I
<br>
&gt; think it may break binding too. It's basically making all <br>
&gt; &quot;unavailable resources&quot; available. So the binding code may
end up <br>
&gt; trying to bind processes on cores that it can't actually use.</font></tt>
<br>
<br><tt><font size=2>It's true that if we have a resource manager that
can allocate for us</font></tt>
<br><tt><font size=2>say a single socket within a node, the binding part
OMPI might go out</font></tt>
<br><tt><font size=2>of its actual boundaries.</font></tt>
<br><tt><font size=2><br>
&gt; <br>
&gt; If srun gives you the first cores of the machine, it works fine <br>
&gt; because OMPI tries to use the first cores and those are available.
<br>
&gt; But did you ever try when srun gives the second socket only for <br>
&gt; instance? Or whichever part of the machine that does not contain the<br>
&gt; first cores ?</font></tt>
<br>
<br><tt><font size=2>But I have to look for the proper option in slurm:
I don't know if slurm </font></tt>
<br><tt><font size=2>allows for such a fine grained allocation. I have
to look for the option</font></tt>
<br><tt><font size=2>that enables to allocate socket X (X!=0).</font></tt>
<br>
<br><tt><font size=2>&gt; I think OMPI will still try to bind on the first
cores<br>
&gt; if the flag is set, but those are not available for binding.<br>
&gt; <br>
&gt; Unless I am missing something, the proper fix would be to have two
<br>
&gt; instances of the topology. One with the entire machine (for people
<br>
&gt; that really want to consult all physical resources), and one for the<br>
&gt; really available part of machine (mostly used for binding).</font></tt>
<br>
<br><tt><font size=2>Agreed! </font></tt>
<br>
<br><tt><font size=2>Regards,</font></tt>
<br><tt><font size=2>Nadia<br>
&gt; <br>
&gt; Brice<br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; devel@open-mpi.org<br>
&gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/devel</font></tt></a>
<br>
