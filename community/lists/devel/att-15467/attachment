<div dir="ltr">Paul,<div><br></div><div>about the second point :</div><div>mmap is called with the MAP_FIXED flag, before the fix, the</div><div>required address was not aligned on a page size and hence</div><div>mmap failed.</div>
<div>the mmap failure was immediatly handled, but for some reasons</div><div>i did not fully investigate yet, this failure was not correctly propagated,</div><div>leading to a SIGSEGV later in lmngr_register (if i remember correctly)</div>
<div><br></div><div>i will add this to my todo list : investigate why the error is not correctly<br><div class="gmail_extra">propagated and handled.</div><div class="gmail_extra"><br></div><div class="gmail_extra">Cheers,</div>
<div class="gmail_extra"><br></div><div class="gmail_extra">Gilles<br><br><div class="gmail_quote">On Sat, Aug 2, 2014 at 6:05 AM, Paul Hargrove <span dir="ltr">&lt;<a href="mailto:phhargrove@lbl.gov" target="_blank">phhargrove@lbl.gov</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Regarding review of the coll/ml fix:<div><br></div><div>While the fix Gilles worked out overnight proved sufficient on Solaris/SPARC, Linux/PPC64 and Linux/IA64, I had two concerns:</div>
<div><br></div><div>
1) As I already voiced on the list, I am concerned with the portability of _SC_PAGESIZE vs _SC_PAGE_SIZE (vs get_pagesize()).</div><div><br></div><div>2) Though I have not tried to trace the code, the fact that fixing the alignment prevents a SEGV strongly suggests that there was a mmap (or something else sensitive to page size) call failing. Â So, there should probably be a check added for failure of that call to produce a cleaner failure than SEGV.</div>

<div><br></div><div>Just my USD 0.02.</div><div>-Paul</div></div><div class="gmail_extra"><br><br><div class="gmail_quote"><div><div class="h5">On Fri, Aug 1, 2014 at 6:39 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>

</div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div class="h5"><div style="word-wrap:break-word">Okay, I fixed those two and will release rc4 once the coll/ml fix has been reviewed. Thanks<div>

<br><div><div>On Aug 1, 2014, at 2:46 AM, Mike Dubman &lt;<a href="mailto:miked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt; wrote:</div><br><blockquote type="cite"><div dir="ltr">Also, latest commit into openib (origin/v1.8 <a href="https://svn.open-mpi.org/trac/ompi/changeset/32391" target="_blank">https://svn.open-mpi.org/trac/ompi/changeset/32391</a>) broke something:<div>

<br></div><div><pre style="white-space:pre-wrap;word-wrap:break-word;margin-top:0px;margin-bottom:0px;font-size:11px"><span><b>11:45:01</b> </span>+ timeout -s SIGSEGV 3m /scrap/jenkins/workspace/OMPI-vendor/label/hpctest/ompi_install1/bin/mpirun -np 8 -mca pml ob1 -mca btl self,openib /scrap/jenkins/workspace/OMPI-vendor/label/hpctest/ompi_install1/examples/hello_usempi
<span><b>11:45:01</b> </span>--------------------------------------------------------------------------
<span><b>11:45:01</b> </span>WARNING: There are more than one active ports on host &#39;hpctest&#39;, but the
<span><b>11:45:01</b> </span>default subnet GID prefix was detected on more than one of these
<span><b>11:45:01</b> </span>ports.  If these ports are connected to different physical IB
<span><b>11:45:01</b> </span>networks, this configuration will fail in Open MPI.  This version of
<span><b>11:45:01</b> </span>Open MPI requires that every physically separate IB subnet that is
<span><b>11:45:01</b> </span>used between connected MPI processes must have different subnet ID
<span><b>11:45:01</b> </span>values.
<span><b>11:45:01</b> </span>
<span><b>11:45:01</b> </span>Please see this FAQ entry for more details:
<span><b>11:45:01</b> </span>
<span><b>11:45:01</b> </span>  <a href="http://www.open-mpi.org/faq/?category=openfabrics#ofa-default-subnet-gid" style="color:rgb(92,53,102)" target="_blank">http://www.open-mpi.org/faq/?category=openfabrics#ofa-default-subnet-gid</a>
<span><b>11:45:01</b> </span>
<span><b>11:45:01</b> </span>NOTE: You can turn off this warning by setting the MCA parameter
<span><b>11:45:01</b> </span>      btl_openib_warn_default_gid_prefix to 0.
<span><b>11:45:01</b> </span>--------------------------------------------------------------------------
<span><b>11:45:01</b> </span>--------------------------------------------------------------------------
<span><b>11:45:01</b> </span>WARNING: No queue pairs were defined in the btl_openib_receive_queues
<span><b>11:45:01</b> </span>MCA parameter.  At least one queue pair must be defined.  The
<span><b>11:45:01</b> </span>OpenFabrics (openib) BTL will therefore be deactivated for this run.
<span><b>11:45:01</b> </span>
<span><b>11:45:01</b> </span>  Local host: hpctest
<span><b>11:45:01</b> </span>--------------------------------------------------------------------------
<span><b>11:45:01</b> </span>--------------------------------------------------------------------------
<span><b>11:45:01</b> </span>At least one pair of MPI processes are unable to reach each other for
<span><b>11:45:01</b> </span>MPI communications.  This means that no Open MPI device has indicated
<span><b>11:45:01</b> </span>that it can be used to communicate between these processes.  This is
<span><b>11:45:01</b> </span>an error; Open MPI requires that all MPI processes be able to reach
<span><b>11:45:01</b> </span>each other.  This error can sometimes be the result of forgetting to
<span><b>11:45:01</b> </span>specify the &quot;self&quot; BTL.
<span><b>11:45:01</b> </span>
<span><b>11:45:01</b> </span>  Process 1 ([[55281,1],1]) is on host: hpctest
<span><b>11:45:01</b> </span>  Process 2 ([[55281,1],0]) is on host: hpctest
<span><b>11:45:01</b> </span>  BTLs attempted: self
<span><b>11:45:01</b> </span>
<span><b>11:45:01</b> </span>Your MPI job is now going to abort; sorry.
<span><b>11:45:01</b> </span>--------------------------------------------------------------------------
<span><b>11:45:01</b> </span>--------------------------------------------------------------------------
<span><b>11:45:01</b> </span>MPI_INIT has failed because at least one MPI process is unreachable
<span><b>11:45:01</b> </span>from another.  This *usually* means that an underlying communication
<span><b>11:45:01</b> </span>plugin -- such as a BTL or an MTL -- has either not loaded or not
<span><b>11:45:01</b> </span>allowed itself to be used.  Your MPI job will now abort.
<span><b>11:45:01</b> </span>
<span><b>11:45:01</b> </span>You may wish to try to narrow down the problem;
<span><b>11:45:01</b> </span>
<span><b>11:45:01</b> </span> * Check the output of ompi_info to see which BTL/MTL plugins are
<span><b>11:45:01</b> </span>   available.
<span><b>11:45:01</b> </span> * Run your application with MPI_THREAD_SINGLE.
<span><b>11:45:01</b> </span> * Set the MCA parameter btl_base_verbose to 100 (or mtl_base_verbose,
<span><b>11:45:01</b> </span>   if using MTL-based communications) to see exactly which
<span><b>11:45:01</b> </span>   communication plugins were considered and/or discarded.
<span><b>11:45:01</b> </span>--------------------------------------------------------------------------
<span><b>11:45:01</b> </span>*** An error occurred in MPI_Init
<span><b>11:45:01</b> </span>*** on a NULL communicator
<span><b>11:45:01</b> </span>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
<span><b>11:45:01</b> </span>***    and potentially your MPI job)
<span><b>11:45:01</b> </span>[hpctest:2761] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
<span><b>11:45:01</b> </span>*** An error occurred in MPI_Init
<span><b>11:45:01</b> </span>*** on a NULL communicator
<span><b>11:45:01</b> </span>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
<span><b>11:45:01</b> </span>***    and potentially your MPI job)
<span><b>11:45:01</b> </span>[hpctest:2757] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
<span><b>11:45:01</b> </span>*** An error occurred in MPI_Init
<span><b>11:45:01</b> </span>*** on a NULL communicator
<span><b>11:45:01</b> </span>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
<span><b>11:45:01</b> </span>***    and potentially your MPI job)
<span><b>11:45:01</b> </span>[hpctest:2751] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
<span><b>11:45:01</b> </span>*** An error occurred in MPI_Init
<span><b>11:45:01</b> </span>*** on a NULL communicator
<span><b>11:45:01</b> </span>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
<span><b>11:45:01</b> </span>***    and potentially your MPI job)
<span><b>11:45:01</b> </span>[hpctest:2752] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
<span><b>11:45:01</b> </span>*** An error occurred in MPI_Init
<span><b>11:45:01</b> </span>*** on a NULL communicator
<span><b>11:45:01</b> </span>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
<span><b>11:45:01</b> </span>***    and potentially your MPI job)
<span><b>11:45:01</b> </span>[hpctest:2753] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
<span><b>11:45:01</b> </span>*** An error occurred in MPI_Init
<span><b>11:45:01</b> </span>*** on a NULL communicator
<span><b>11:45:01</b> </span>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
<span><b>11:45:01</b> </span>***    and potentially your MPI job)
<span><b>11:45:01</b> </span>[hpctest:2755] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
<span><b>11:45:01</b> </span>*** An error occurred in MPI_Init
<span><b>11:45:01</b> </span>*** on a NULL communicator
<span><b>11:45:01</b> </span>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
<span><b>11:45:01</b> </span>***    and potentially your MPI job)
<span><b>11:45:01</b> </span>[hpctest:2759] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
<span><b>11:45:01</b> </span>*** An error occurred in MPI_Init
<span><b>11:45:01</b> </span>*** on a NULL communicator
<span><b>11:45:01</b> </span>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
<span><b>11:45:01</b> </span>***    and potentially your MPI job)
<span><b>11:45:01</b> </span>[hpctest:2763] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!</pre></div></div>
<div class="gmail_extra"><br><br><div class="gmail_quote">On Fri, Aug 1, 2014 at 11:00 AM, Paul Hargrove <span dir="ltr">&lt;<a href="mailto:phhargrove@lbl.gov" target="_blank">phhargrove@lbl.gov</a>&gt;</span> wrote:<br>


<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Note that the Solaris unresolved alloca problem George fixed inÂ r32388 is still present in 1.8.2rc3.<div>


I have manually confirmed that the same patch resolves the problem in 1.8.2rc3.</div><div><br></div><div>
-Paul</div></div><div class="gmail_extra"><div><div><br><br><div class="gmail_quote">On Thu, Jul 31, 2014 at 9:44 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>



<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Usual place - this is a last-chance check, so please hit it. Main change from rc2 is the repairs to the Fortran binding config logic<br>




<br>
<a href="http://www.open-mpi.org/software/ompi/v1.8/" target="_blank">http://www.open-mpi.org/software/ompi/v1.8/</a><br>
<br>
Ralph<br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/08/15433.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/08/15433.php</a><br>
</blockquote></div><br><br clear="all"><div><br></div></div></div><span><font color="#888888">-- <br><font face="courier new, monospace"><div>Paul H. Hargrove Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â <a href="mailto:PHHargrove@lbl.gov" target="_blank">PHHargrove@lbl.gov</a></div>



<div>Future Technologies Group</div><div>Computer and Data Sciences Department Â  Â  Tel: <a href="tel:%2B1-510-495-2352" value="+15104952352" target="_blank">+1-510-495-2352</a></div><div>Lawrence Berkeley National Laboratory Â  Â  Fax: <a href="tel:%2B1-510-486-6900" value="+15104866900" target="_blank">+1-510-486-6900</a></div>


</font>
</font></span></div>
<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/08/15440.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/08/15440.php</a><br></blockquote></div><br></div>
_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>

Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/08/15444.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/08/15444.php</a></blockquote></div><br></div></div><br>_______________________________________________<br>


devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/08/15449.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/08/15449.php</a><br></blockquote></div><div class=""><br><br clear="all">
<div>
<br></div>-- <br><font face="courier new, monospace"><div>Paul H. Hargrove Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â <a href="mailto:PHHargrove@lbl.gov" target="_blank">PHHargrove@lbl.gov</a></div><div>Future Technologies Group</div><div>

Computer and Data Sciences Department Â  Â  Tel: <a href="tel:%2B1-510-495-2352" value="+15104952352" target="_blank">+1-510-495-2352</a></div><div>Lawrence Berkeley National Laboratory Â  Â  Fax: <a href="tel:%2B1-510-486-6900" value="+15104866900" target="_blank">+1-510-486-6900</a></div>
</font>
</div></div>
<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/08/15464.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/08/15464.php</a><br></blockquote></div><br></div></div></div>

