<div dir="ltr">I pushed a slightly better patch for the TCP BTL (54ddb0aece0892dcdb1a1293a3bd3902b5f3acdc). The correct scheme would be to OBJ_RETAIN the proc once it is attached to the btl_proc and release it upon destruction of the btl_proc. However, for some obscure reason this doesn&#39;t quite works, as the main opel_proc is apparently already destroyed when we try to finalize the TCP BTL.<div><br></div><div>  George.</div><div><br></div><div>PS: Sorry Dave I also pushed a master branch merge ...</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Thu, Nov 6, 2014 at 1:10 AM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles.gouaillardet@iferc.org" target="_blank">gilles.gouaillardet@iferc.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
  
    
  
  <div text="#000000" bgcolor="#FFFFFF">
    <div>Ralph,<br>
      <br>
      i updated the MODEX flag to PMIX_GLOBAL<br>
<a href="https://github.com/open-mpi/ompi/commit/d542c9ff2dc57ca5d260d0578fd5c1c556c598c7" target="_blank">https://github.com/open-mpi/ompi/commit/d542c9ff2dc57ca5d260d0578fd5c1c556c598c7</a><br>
      <br>
      Elena, <br>
      <br>
      i was able to reproduce the issue (salloc -N 5 mpirun -np 2 is
      enough).<br>
      i was &quot;lucky&quot; to reproduce the issue : it happened because one of
      node was misconfigured<br>
      with two interfaces in the same subnet (!)<br>
      <br>
      could you please give a try to the attached patch ?<br>
      i did not commit it because i do not know if this is the right fix
      or a simple workaround<br>
      /* for example, should the opal_proc_t be OBJ_RETAINed before
      invoking add_procs and<br>
      then be OBJ_RELEASEd by the btl add_proc if it is unreachable ? */<br>
      <br>
      Cheers,<br>
      <br>
      Gilles<span class=""><br>
      <br>
      On 2014/11/06 12:46, Ralph Castain wrote:<br>
    </span></div>
    <blockquote type="cite"><span class="">
      <pre></pre>
      <blockquote type="cite">
        <pre>On Nov 5, 2014, at 6:11 PM, Gilles Gouaillardet <a href="mailto:gilles.gouaillardet@iferc.org" target="_blank">&lt;gilles.gouaillardet@iferc.org&gt;</a> wrote:

Elena,

the first case (-mca btl tcp,self) crashing is a bug and i will have a look at it.

the second case (-mca sm,self) is a feature : the sm btl cannot be used with tasks
having different jobids (this is the case after a spawn), and obviously, self cannot be used also,
so the behaviour and error message is correct.
/* i am not aware of any plans to make the sm btl work with tasks from different jobids */\
</pre>
      </blockquote>
      <pre>That is correct - I’m also unaware of any plans to extend it at this point, though IIRC Nathan at one time mentioned perhaps extending vader for that purpose

</pre>
      </span><blockquote type="cite">
        <pre>the third case (-mca openib,self) is more controversial ...
i previously posted <a href="http://www.open-mpi.org/community/lists/devel/2014/10/16136.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/10/16136.php</a> <a href="http://www.open-mpi.org/community/lists/devel/2014/10/16136.php" target="_blank">&lt;http://www.open-mpi.org/community/lists/devel/2014/10/16136.php&gt;</a><div><div class="h5">
what happens in your case (simple_spawn) is the openib modex is sent with PMIX_REMOTE,
that means openib btl cannot be used between tasks on the same node.
i am still waiting for some feedback since i cannot figure out whether this is a feature or an
undesired side effect / bug
</div></div></pre>
      </blockquote><div><div class="h5">
      <pre>I believe it is a bug - I provided some initial values for the modex scope with the expectation (and request when we committed it) that people would review and modify them as appropriate. I recall setting the openib scope as “remote” only because I wasn’t aware of anyone using it for local comm. Since Mellanox obviously is testing for that case, a scope of PMIX_GLOBAL would be more appropriate

</pre>
      </div></div><blockquote type="cite"><div><div class="h5">
        <pre>the last cast (-mca ^sm,openib) does make sense to me :
the tcp and self btls are used and they work just like they should.

bottom line, i will investigate the first crash, wait for feedback about the openib btl.

Cheers,

Gilles

On 2014/11/06 1:08, Elena Elkina wrote:
</pre>
        </div></div><blockquote type="cite">
          <pre><div><div class="h5">Hi,

It looks like there is a problem in trunk which reproduces with
simple_spawn test (orte/test/mpi/simple_spawn.c). It seems to be a n issue
with pmix. It doesn&#39;t reproduce with default set of btls. But it reproduces
with several btls specified. For example,

salloc -N5 $OMPI_HOME/install/bin/mpirun -np 33 --map-by node -mca coll ^ml
-display-map -mca orte_debug_daemons true --leave-session-attached
--debug-daemons -mca pml ob1 -mca btl *tcp,self*
./orte/test/mpi/simple_spawn

gets

simple_spawn: ../../ompi/group/group_init.c:215:
ompi_group_increment_proc_count: Assertion `((0xdeafbeedULL &lt;&lt; 32) +
0xdeafbeedULL) == ((opal_object_t *) (proc_pointer))-&gt;obj_magic_id&#39; failed.
[<a href="http://sputnik3.vbench.com:28888" target="_blank">sputnik3.vbench.com:28888</a>] [[41877,0],3] orted_cmd: exit cmd, but proc
[[41877,1],2] is alive
[sputnik5][[41877,1],29][../../../../../opal/mca/btl/tcp/btl_tcp_endpoint.c:675:mca_btl_tcp_endpoint_complete_connect]
connect() to 192.168.1.42 failed: Connection refused (111)

salloc -N1 $OMPI_HOME/install/bin/mpirun -np 3 --map-by node -mca coll ^ml
-display-map -mca orte_debug_daemons true --leave-session-attached
--debug-daemons -mca pml ob1 -mca btl *sm,self* ./orte/test/mpi/simple_spawn

fails with

At least one pair of MPI processes are unable to reach each other for
MPI communications.  This means that no Open MPI device has indicated
that it can be used to communicate between these processes.  This is
an error; Open MPI requires that all MPI processes be able to reach
each other.  This error can sometimes be the result of forgetting to
specify the &quot;self&quot; BTL.

  Process 1 ([[59481,2],0]) is on host: sputnik1
  Process 2 ([[59481,1],0]) is on host: sputnik1
  BTLs attempted: self sm

Your MPI job is now going to abort; sorry.
--------------------------------------------------------------------------
[<a href="http://sputnik1.vbench.com:22156" target="_blank">sputnik1.vbench.com:22156</a>] [[59481,1],2] ORTE_ERROR_LOG: Unreachable in
file ../../../../../ompi/mca/dpm/orte/dpm_orte.c at line 485


salloc -N1 $OMPI_HOME/install/bin/mpirun -np 3 --map-by node -mca coll ^ml
-display-map -mca orte_debug_daemons true --leave-session-attached
--debug-daemons -mca pml ob1 -mca btl *openib,self*
 ./orte/test/mpi/simple_spawn

also doesn&#39;t work:
At least one pair of MPI processes are unable to reach each other for
MPI communications.  This means that no Open MPI device has indicated
that it can be used to communicate between these processes.  This is
an error; Open MPI requires that all MPI processes be able to reach
each other.  This error can sometimes be the result of forgetting to
specify the &quot;self&quot; BTL.

  Process 1 ([[60046,1],13]) is on host: sputnik4
  Process 2 ([[60046,2],1]) is on host: sputnik4
  BTLs attempted: openib self

Your MPI job is now going to abort; sorry.
--------------------------------------------------------------------------
[<a href="http://sputnik4.vbench.com:25476" target="_blank">sputnik4.vbench.com:25476</a>] [[60046,1],3] ORTE_ERROR_LOG: Unreachable in
file ../../../../../ompi/mca/dpm/orte/dpm_orte.c at line 485


*But* combination ^sm,openib seems to work.

I tried different revisions from the beginning of October. It reproduces on
them.

Best regards,
Elena



_______________________________________________
devel mailing list
</div></div><a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a> <a href="mailto:devel@open-mpi.org" target="_blank">&lt;mailto:devel@open-mpi.org&gt;</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a> <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">&lt;http://www.open-mpi.org/mailman/listinfo.cgi/devel&gt;</a>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/11/16202.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/11/16202.php</a> <a href="http://www.open-mpi.org/community/lists/devel/2014/11/16202.php" target="_blank">&lt;http://www.open-mpi.org/community/lists/devel/2014/11/16202.php&gt;</a>
</pre>
        </blockquote><span class="">
        <pre>_______________________________________________
devel mailing list
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/11/16223.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/11/16223.php</a>
</pre>
      </span></blockquote>
      <pre>
</pre>
      <br>
      <fieldset></fieldset>
      <br>
      <pre><span class="">_______________________________________________
devel mailing list
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></span>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/11/16225.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/11/16225.php</a></pre>
    </blockquote>
    <br>
  </div>

<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/11/16226.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/11/16226.php</a><br></blockquote></div><br></div>

