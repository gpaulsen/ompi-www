<div dir="ltr">Hi Folks,<div><br></div><div>I have a cluster with some 100 Gb ethernet cards</div><div>installed.  What we are noticing if we force Open MPI 1.10.3</div><div>to go through the TCP BTL (rather than yalla)  is that</div><div>the performance of osu_bw once the TCP BTL switches</div><div>from eager to rendezvous (&gt; 32KB)</div><div>falls off a cliff, going from about 1.6 GB/sec to 233 MB/sec</div><div>and stays that way out to 4 MB message lengths.</div><div><br></div><div>There&#39;s nothing wrong with the IP stack (iperf -P4 gives </div><div>63 Gb/sec).</div><div><br></div><div>So, my questions are </div><div><br></div><div>1) is this performance expected for the TCP BTL when in</div><div>rendezvous mode?</div><div>2) is there some way to get more like the single socket</div><div>performance obtained with iperf for large messages (~16 Gb/sec).</div><div><br></div><div>We tried adjusting the tcp_btl_rendezvous threshold but that doesn&#39;t</div><div>appear to actually be adjustable from the mpirun command line.</div><div><br></div><div>Thanks for any suggestions,</div><div><br></div><div>Howard</div><div><br></div><div><br></div><div><br></div></div>

