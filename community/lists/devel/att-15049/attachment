Ralph,<br><br>That makes perfect sense.<br><br>What about FCA_IS_LOCAL_PROCESS ?<br>Shall we keep it or shall we use directly OPAL_PROC_ON_LOCAL_NODE directly ?<br><br>Cheers<br><br>Gilles<br><br>Ralph Castain &lt;rhc@open-mpi.org&gt; wrote:<br><div dir="ltr">Hi Gilles<div><br></div><div>We discussed this at the devel conference this morning. The root cause of the problem is a test in coll/ml that we feel is incorrect - it basically checks to see if the proc itself is bound, and then assumes that all other procs are similarly bound. This in fact is never guaranteed to be true as someone could use the rank_file method to specify that some procs are to be left unbound, while others are to be bound to specified cpus.</div>
<div><br></div><div>Nathan has looked at that check before and believes it isn&#39;t necessary. All coll/ml really needs to know is that the two procs share the same node, and the current locality algorithm will provide that information. We have asked him to &quot;fix&quot; the coll/ml selection logic to resolve that situation.</div>
<div><br></div><div>After then discussing the various locality definitions, it was our feeling that the current definition is probably the better one unless you have a reason for changing it other than coll/ml. If so, we&#39;d be happy to revisit the proposal.</div>
<div><br></div><div>Make sense?</div><div>Ralph</div><div><br></div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Tue, Jun 24, 2014 at 3:24 AM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles.gouaillardet@iferc.org" target="_blank">gilles.gouaillardet@iferc.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">WHAT: semantic change of opal_hwloc_base_get_relative_locality<br>
<br>
WHY:  make is closer to what coll/ml expects.<br>
<br>
      Currently, opal_hwloc_base_get_relative_locality means &quot;at what level do these procs share cpus&quot;<br>
      however, coll/ml is using it as &quot;at what level are these procs commonly bound&quot;.<br>
<br>
      it is important to note that if a task is bound to all the available cpus, locality should<br>
      be set to OPAL_PROC_ON_NODE only.<br>
      /* e.g. on a single socket Sandy Bridge system, use OPAL_PROC_ON_NODE instead of OPAL_PROC_ON_L3CACHE */<br>
<br>
      This has been initially discussed in the devel mailing list<br>
      <a href="http://www.open-mpi.org/community/lists/devel/2014/06/15030.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/06/15030.php</a><br>
<br>
      as advised by Ralph, i browsed the source code looking for how the (ompi_proc_t *)-&gt;proc_flags is used.<br>
      so far, it is mainly used to figure out wether the proc is on the same node or not.<br>
<br>
      notable exceptions are :<br>
       a) ompi/mca/sbgp/basesmsocket/sbgp_basesmsocket_component.c : OPAL_PROC_ON_LOCAL_SOCKET<br>
       b) ompi/mca/coll/fca/coll_fca_module.c and oshmem/mca/scoll/fca/scoll_fca_module.c : FCA_IS_LOCAL_PROCESS<br>
<br>
      about a) the new definition fixes a hang in coll/ml<br>
      about b) FCA_IS_LOCAL_SOCKET looks like legacy code /* i could only found OMPI_PROC_FLAG_LOCAL in v1.3 */<br>
      so this macro can be simply removed and replaced with OPAL_PROC_ON_LOCAL_NODE<br>
<br>
      at this stage, i cannot find any objection not to do the described change.<br>
      please report if any and/or feel free to comment.<br>
<br>
WHERE: see the two attached patches<br>
<br>
TIMEOUT: June 30th, after the Open MPI developers meeting in Chicago, June 24-26.<br>
         The RFC will become final only after the meeting.<br>
         /* Ralph already added this topic to the agenda */<br>
<br>
Thanks<br>
<span class="HOEnZb"><font color="#888888"><br>
Gilles<br>
<br>
</font></span><br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/06/15046.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/06/15046.php</a><br></blockquote></div><br></div>
