<div dir="ltr">In Open MPI a process only retrieve information about a peer if they communicate. Thus, the add_proc is called from the two sides of a connection establishment, when locally a connection is decided or when a network packet requires a the existence of a proc (for the initiator of the connection). Thus, the modex_recv is called deep inside the BTL, usually when the proc is created (TCP has a specific function for this mca_btl_tcp_proc_create).<div><br></div><div>More inline.<br><div class="gmail_extra"><br><div class="gmail_quote">On Thu, Apr 28, 2016 at 2:07 AM, dpchoudh . <span dir="ltr">&lt;<a href="mailto:dpchoudh@gmail.com" target="_blank">dpchoudh@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid;border-left-color:rgb(204,204,204);padding-left:1ex"><div dir="ltr"><div><div><div><div>Hello all<br><br></div>I am struggling with this issue for last few days and thought it would be prudent to ask for help from people who have way more experience than I do.<br><br></div>There are two questions, interrelated in my mind, but may not be so in reality. Question 2 is the issue I am struggling with, and question 1 sort of leads to it.<br><br></div>1. I see that both in openib and tcp BTL (the two kind of hardware I have access to) a modex send happens, but a matching modex receive never happens. Is it because of some kind of optimization? (In my case, both IP NICs are in the same IP subnet and both IB NICs are in the same IB subnet) Or am I not understanding something? How do the processes figure out their peer information without a modex receive?<br><br></div>The place in code where the modex receive is called is in btl_add_procs(). However, it looks like in both the above BTLs, this method is never called. Is that expected?<br></div></blockquote><div><br></div><div>It is called upon communications. Force an MPI_Barrier in your example, and you will see the add_proc called for a subset of the peers (depending on the barrier algorithm).</div><div> </div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid;border-left-color:rgb(204,204,204);padding-left:1ex"><div dir="ltr"><br><div><div><div><div><div><div><div>2. This is the real question is this:<br></div><div>I am writing a BTL for a proprietary RDMA NIC (named &#39;lf&#39; in the code) that has no routing capability in protocol, and hence no concept of subnets. An HCA simply needs to be plugged in to the switch and it can see the whole network. However, there is a VLAN like partition (similar to IB partitions)<br></div><div>Given this (and as a first cut, every node is in the same partition, so even this complexity is eliminated), there is not much use for a modex exchange, but I added one anyway just with the partition key.<br><br></div><div>What I see is that the component open, register and init are all successful, but r2 bml still does not choose this network and thus OMPI aborts because of lack of full reachability.<br></div></div></div></div></div></div></div></div></blockquote><div><br></div><div>The BML should have selected your network if your _open returned OPAL_SUCCESS and your _init returned a module (or a list of).</div><div> </div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid;border-left-color:rgb(204,204,204);padding-left:1ex"><div dir="ltr"><div><div><div><div><div><div><div><br></div><div>This is my command line:<br>sudo /usr/local/bin/mpirun --allow-run-as-root -hostfile ~/hostfile -np 2 -mca btl self,lf -mca btl_base_verbose 100 -mca bml_base_verbose 100 ./mpitest<br><br></div><div>(&#39;mpitest&#39; is a trivial &#39;hello world&#39; program plus ONE MPI_Send()/MPI_Recv() to test in-band communication. The sudo is required because currently the driver requires root permission; I was told that this will be fixed. The hostfile has 2 hosts, named b-2 and b-3, with back-to-back connection on this &#39;lf&#39; HCA)<br><br></div><div>The output of this command is as follows; I have added my comments to explain it a bit.<br><br></div><div>&lt;Output from OMPI logging mechanism&gt;<br></div><div><span style="font-family:monospace,monospace">[b-2:21062] mca: base: components_register: registering framework bml components<br>[b-2:21062] mca: base: components_register: found loaded component r2<br>[b-2:21062] mca: base: components_register: component r2 register function successful<br>[b-2:21062] mca: base: components_open: opening bml components<br>[b-2:21062] mca: base: components_open: found loaded component r2<br>[b-2:21062] mca: base: components_open: component r2 open function successful<br>[b-2:21062] mca: base: components_register: registering framework btl components<br>[b-2:21062] mca: base: components_register: found loaded component self<br>[b-2:21062] mca: base: components_register: component self register function successful<br>[b-2:21062] mca: base: components_register: found loaded component lf<br>[b-2:21062] mca: base: components_register: component lf register function successful<br>[b-2:21062] mca: base: components_open: opening btl components<br>[b-2:21062] mca: base: components_open: found loaded component self<br>[b-2:21062] mca: base: components_open: component self open function successful<br>[b-2:21062] mca: base: components_open: found loaded component lf<br></span></div></div></div></div></div></div></div></div></blockquote><div><br></div><div>Your _open return OPAL_SUCCESS, the BTL is on.</div><div> </div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid;border-left-color:rgb(204,204,204);padding-left:1ex"><div dir="ltr"><div><div><div><div><div><div><div><span style="font-family:monospace,monospace"></span></div><div><span style="font-family:arial,helvetica,sans-serif"><br>&lt;Debugging output from the HCA driver&gt;</span><span style="font-family:monospace,monospace"><br></span></div><div><span style="font-family:monospace,monospace">lf_group_lib.c:442: _lf_open: _lf_open(&quot;MPI_0&quot;,0x842,0x1b6,4096,0)</span><br><br>&lt;Output from OMPI logging mechanism, continued&gt;<br><span style="font-family:monospace,monospace">[b-2:21062] mca: base: components_open: component lf open function successful<br>[b-2:21062] select: initializing btl component self<br>[b-2:21062] select: init of component self returned success<br>[b-2:21062] select: initializing btl component lf</span><br><span style="font-family:arial,helvetica,sans-serif"><br>&lt;Debugging output from the HCA driver&gt;<br></span><span style="font-family:monospace,monospace">Created group on b-2</span><br><br>&lt;Output from OMPI logging mechanism, continued&gt;<br><span style="font-family:monospace,monospace">[b-2:21062] select: init of component lf returned success<br>[b-3:07672] mca: base: components_register: registering framework bml components<br>[b-3:07672] mca: base: components_register: found loaded component r2<br>[b-3:07672] mca: base: components_register: component r2 register function successful<br>[b-3:07672] mca: base: components_open: opening bml components<br>[b-3:07672] mca: base: components_open: found loaded component r2<br>[b-3:07672] mca: base: components_open: component r2 open function successful<br>[b-3:07672] mca: base: components_register: registering framework btl components<br>[b-3:07672] mca: base: components_register: found loaded component self<br>[b-3:07672] mca: base: components_register: component self register function successful<br>[b-3:07672] mca: base: components_register: found loaded component lf<br>[b-3:07672] mca: base: components_register: component lf register function successful<br>[b-3:07672] mca: base: components_open: opening btl components<br>[b-3:07672] mca: base: components_open: found loaded component self<br>[b-3:07672] mca: base: components_open: component self open function successful<br>[b-3:07672] mca: base: components_open: found loaded component lf<br>[b-3:07672] mca: base: components_open: component lf open function successful<br>[b-3:07672] select: initializing btl component self<br>[b-3:07672] select: init of component self returned success<br>[b-3:07672] select: initializing btl component lf<br></span><br><span style="font-family:arial,helvetica,sans-serif">&lt;Debugging output from the HCA driver&gt;<br></span><span style="font-family:monospace,monospace">lf_group_lib.c:442: _lf_open: _lf_open(&quot;MPI_0&quot;,0x842,0x1b6,4096,0)<br>Created group on b-3</span><br><br>&lt;Output from OMPI logging mechanism, continued&gt;<br><span style="font-family:monospace,monospace">[b-3:07672] select: init of component lf returned success<br></span></div></div></div></div></div></div></div></div></blockquote><div><br></div><div>Based on this output I assume that your BTL has returned something else than NULL.</div><div> </div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid;border-left-color:rgb(204,204,204);padding-left:1ex"><div dir="ltr"><div><div><div><div><div><div><div><span style="font-family:monospace,monospace">[b-2:21062] mca: bml: Using self btl for send to [[6866,1],0] on node b-2<br>[b-3:07672] mca: bml: Using self btl for send to [[6866,1],1] on node b-3</span></div></div></div></div></div></div></div></div></blockquote><div><br></div><div>This is definitively not good (aka. self should never be selected for communication with another process) but I guess it is a side effect of the fact that we lost our connectivity map (thus any BTL is assumed as being able to communicate with anybody else).</div><div><br></div><div>The problem might be coming from here. Is your BTL setting the exclusivity flag ? What is your BTL priority ? I would look more in details at the mca_bml_r2_endpoint_add_btl function.</div><div><br></div><div>  George.</div><div><br></div>







<div><br></div><div> </div><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid;border-left-color:rgb(204,204,204);padding-left:1ex"><div dir="ltr"><div><div><div><div><div><div><div><br><br></div><div>&lt;Output from the &#39;mpitest&#39; MPI program: out-of-band-I/O&gt;<br></div><div><span style="font-family:monospace,monospace">Hello from b-2<br>The world has 2 nodes<br>My rank is 0<br>Hello from b-3</span><br><br></div><div>&lt;Output frm OMPI&gt;<br></div><div><span style="font-family:monospace,monospace">--------------------------------------------------------------------------<br>At least one pair of MPI processes are unable to reach each other for<br>MPI communications.  This means that no Open MPI device has indicated<br>that it can be used to communicate between these processes.  This is<br>an error; Open MPI requires that all MPI processes be able to reach<br>each other.  This error can sometimes be the result of forgetting to<br>specify the &quot;self&quot; BTL.<br><br>  Process 1 ([[6866,1],0]) is on host: b-2<br>  Process 2 ([[6866,1],1]) is on host: 10.4.70.12<br>  BTLs attempted: self<br><br>Your MPI job is now going to abort; sorry.<br>--------------------------------------------------------------------------</span><br><br>&lt;Output from the &#39;mpitest&#39; MPI program: out-of-band-I/O, continued&gt;<br><span style="font-family:monospace,monospace">The world has 2 nodes<br>My rank is 1</span><br><br>&lt;Output from OMPI logging mechanism, continued&gt;<br><span style="font-family:monospace,monospace">[b-2:21062] *** An error occurred in MPI_Send<br>[b-2:21062] *** reported by process [140385751007233,21474836480]<br>[b-2:21062] *** on communicator MPI_COMM_WORLD<br>[b-2:21062] *** MPI_ERR_INTERN: internal error<br>[b-2:21062] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>[b-2:21062] ***    and potentially your MPI job)</span><br>[durga@b-2 ~]$<br><br></div><div>As you can see, the lf network is not being chosen for communication. Without a modex exchange, how can that happen? Or, in a nutshell, what do I need to do?<br><br></div><div>Thanks a lot in advance<br></div><div>Durga<br><br></div><div><br clear="all"><div><div><div dir="ltr"><div>1% of the executables have 99% of CPU privilege!<br></div>Userspace code! Unite!! Occupy the kernel!!!<br></div></div></div>
</div></div></div></div></div></div></div></div>
<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/04/18827.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/04/18827.php</a><br></blockquote></div><br></div></div></div>

