<HTML>
<HEAD>
<TITLE>Re: [OMPI devel] [OMPI svn] svn:open-mpi r19600</TITLE>
</HEAD>
<BODY>
<FONT FACE="Calibri, Verdana, Helvetica, Arial"><SPAN STYLE='font-size:11pt'>Let me make the point that adding a data structure is much less destabilization to the tree than the routine day-to-day changes that go on in the tree.<BR>
<BR>
Rich<BR>
<BR>
<BR>
On 9/23/08 6:24 AM, &quot;Terry D. Dontje&quot; &lt;<a href="Terry.Dontje@Sun.COM">Terry.Dontje@Sun.COM</a>&gt; wrote:<BR>
<BR>
</SPAN></FONT><BLOCKQUOTE><FONT FACE="Calibri, Verdana, Helvetica, Arial"><SPAN STYLE='font-size:11pt'>Jeff Squyres wrote:<BR>
&gt; I think the point is that as a group, we consciously, deliberately,<BR>
&gt; and painfully decided not to support multi-cluster. &nbsp;And as a result,<BR>
&gt; we ripped out a lot of supporting code. &nbsp;Starting down this path again<BR>
&gt; will likely result in a) re-opening all the discussions, b) re-adding<BR>
&gt; a lot of code (or code effectively similar to what was there before). <BR>
&gt; Let's not forget that there were many unsolved problems surrounding<BR>
&gt; multi-cluster last time, too.<BR>
&gt;<BR>
&gt; It was also pointed out in Ralph's mails that, at least from the<BR>
&gt; descriptions provided, adding the field in orte_node_t does not<BR>
&gt; actually solve the problem that ORNL is trying to solve.<BR>
&gt;<BR>
&gt; If we, as a group, decide to re-add all this stuff, then a) recognize<BR>
&gt; that we are flip-flopping *again* on this issue, and b) it will take a<BR>
&gt; lot of coding effort to do so. &nbsp;I do think that since this was a group<BR>
&gt; decision last time, it should be a group decision this time, too. &nbsp;If<BR>
&gt; this does turn out to be as large of a sub-project as described, I<BR>
&gt; would be opposed to the development occurring on the trunk; hg trees<BR>
&gt; are perfect for this kind of stuff.<BR>
&gt;<BR>
&gt; I personally have no customers who are doing cross-cluster kinds of<BR>
&gt; things, so I don't personally care if cross-cluster functionality<BR>
&gt; works its way [back] in. &nbsp;But I recognize that OMPI core members are<BR>
&gt; investigating it. &nbsp;So the points I'm making are procedural; I have no<BR>
&gt; real dog in this fight...<BR>
&gt;<BR>
&gt;<BR>
I agree with Jeff that this is perfect for an hg tree. &nbsp;Though I also<BR>
don't have a dog in this fight but I have a cat that would rather stay<BR>
comfortably sleeping and not have someone step on its tail :-). &nbsp;&nbsp;In<BR>
other words knock yourself out but please don't destabilize the trunk. <BR>
Of course that begs the question what happens when the hg tree is done<BR>
and working?<BR>
<BR>
--td<BR>
<BR>
&gt; On Sep 22, 2008, at 4:40 PM, George Bosilca wrote:<BR>
&gt;<BR>
&gt;&gt; Ralph,<BR>
&gt;&gt;<BR>
&gt;&gt; There is NO need to have this discussion again, it was painful enough<BR>
&gt;&gt; last time. From my perspective I do not understand why are you making<BR>
&gt;&gt; so much noise on this one. How a 4 lines change in some ALPS specific<BR>
&gt;&gt; files (Cray system very specific to ORNL) can generate more than 3 A4<BR>
&gt;&gt; pages of emails, is still something out of my perception.<BR>
&gt;&gt;<BR>
&gt;&gt; If they want to do multi-cluster and they do not break anything in<BR>
&gt;&gt; ORTE/OMPI and they do not ask other people to do it for them why<BR>
&gt;&gt; trying to stop them ?<BR>
&gt;&gt;<BR>
&gt;&gt; &nbsp;george.<BR>
&gt;&gt;<BR>
&gt;&gt; On Sep 22, 2008, at 3:59 PM, Ralph Castain wrote:<BR>
&gt;&gt;<BR>
&gt;&gt;&gt; There was a very long drawn-out discussion about this early in 2007.<BR>
&gt;&gt;&gt; Rather than rehash all that, I'll try to summarize it here. It may<BR>
&gt;&gt;&gt; get confusing - it helped a whole lot to be in a room with a<BR>
&gt;&gt;&gt; whiteboard. There were also presentations on the subject - I believe<BR>
&gt;&gt;&gt; the slides may still be in the docs repository.<BR>
&gt;&gt;&gt;<BR>
&gt;&gt;&gt; Because terminology quickly gets confusing, we adopted a slightly<BR>
&gt;&gt;&gt; different one for these discussions. We talk about OMPI being a<BR>
&gt;&gt;&gt; &quot;single cell&quot; system - i.e., jobs executed via mpirun can only span<BR>
&gt;&gt;&gt; nodes that are reachable by that mpirun. In a typical managed<BR>
&gt;&gt;&gt; environment, a cell aligns quite well with a &quot;cluster&quot;. In an<BR>
&gt;&gt;&gt; unmanaged environment where the user provides a hostfile, the cell<BR>
&gt;&gt;&gt; will contain all nodes specified in the hostfile.<BR>
&gt;&gt;&gt;<BR>
&gt;&gt;&gt; We don't filter or abort for non-matching hostnames - if mpirun can<BR>
&gt;&gt;&gt; launch on that node, then great. What we don't support is asking<BR>
&gt;&gt;&gt; mpirun to remotely execute another mpirun on the frontend of another<BR>
&gt;&gt;&gt; cell in order to launch procs on the nodes in -that- cell, nor do we<BR>
&gt;&gt;&gt; ask mpirun to in any way manage (or even know about) any procs<BR>
&gt;&gt;&gt; running on a remote cell.<BR>
&gt;&gt;&gt;<BR>
&gt;&gt;&gt; I see what you are saying about the ALPS node name. However, the<BR>
&gt;&gt;&gt; field you want to add doesn't have anything to do with<BR>
&gt;&gt;&gt; accept/connect. The orte_node_t object is used solely by mpirun to<BR>
&gt;&gt;&gt; keep track of the node pool it controls - i.e., the nodes upon which<BR>
&gt;&gt;&gt; it is launching jobs. Thus, the mpirun on cluster A will have<BR>
&gt;&gt;&gt; &quot;nidNNNN&quot; entries it got from its allocation, and the mpirun on<BR>
&gt;&gt;&gt; cluster B will have &quot;nidNNNN&quot; entries it got from its allocation -<BR>
&gt;&gt;&gt; but the two mpiruns will never exchange that information, nor will<BR>
&gt;&gt;&gt; the mpirun on cluster A ever have a need to know the node entries<BR>
&gt;&gt;&gt; for cluster B. Each mpirun launches and manages procs -only- on the<BR>
&gt;&gt;&gt; nodes in its own allocation.<BR>
&gt;&gt;&gt;<BR>
&gt;&gt;&gt; I agree you will have issues when doing the connect/accept modex as<BR>
&gt;&gt;&gt; the nodenames are exchanged and are no longer unique in your<BR>
&gt;&gt;&gt; scenario. However, that info stays in the &nbsp;ompi_proc_t - it never<BR>
&gt;&gt;&gt; gets communicated to the ORTE layer as we couldn't care less down<BR>
&gt;&gt;&gt; there about the remote procs since they are under the control of a<BR>
&gt;&gt;&gt; different mpirun. So if you need to add a cluster id field for this<BR>
&gt;&gt;&gt; purpose, it needs to go in ompi_proc_t - not in the orte structures.<BR>
&gt;&gt;&gt;<BR>
&gt;&gt;&gt; And for that, you probably need to discuss it with the MPI team as<BR>
&gt;&gt;&gt; changes to ompi_proc_t will likely generate considerable discussion.<BR>
&gt;&gt;&gt;<BR>
&gt;&gt;&gt; FWIW: this is one reason I warned Galen about the problems in<BR>
&gt;&gt;&gt; reviving multi-cluster operations again. We used to deal with<BR>
&gt;&gt;&gt; multi-cells in the process name itself, but all that support has<BR>
&gt;&gt;&gt; been removed from OMPI.<BR>
&gt;&gt;&gt;<BR>
&gt;&gt;&gt; Hope that helps<BR>
&gt;&gt;&gt; Ralph<BR>
&gt;&gt;&gt;<BR>
&gt;&gt;&gt; On Sep 22, 2008, at 1:39 PM, Matney Sr, Kenneth D. wrote:<BR>
&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; I may be opening a can of worms...<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; But, what prevents a user from running across clusters in a &quot;normal<BR>
&gt;&gt;&gt;&gt; OMPI&quot;, i.e., non-ALPS environment? &nbsp;When he puts hosts into his<BR>
&gt;&gt;&gt;&gt; hostfile, does it parse and abort/filter non-matching hostnames? &nbsp;The<BR>
&gt;&gt;&gt;&gt; problem for ALPS based systems is that nodes are addressed via NID,PID<BR>
&gt;&gt;&gt;&gt; pairs at the portals level. &nbsp;Thus, these are unique only within a<BR>
&gt;&gt;&gt;&gt; cluster. &nbsp;In point of fact, I could rewrite all of the ALPS support to<BR>
&gt;&gt;&gt;&gt; identify the nodes by &quot;cluster_id&quot;.NID. &nbsp;It would be a bit inefficient<BR>
&gt;&gt;&gt;&gt; within a cluster because, we would have to extract the NID from this<BR>
&gt;&gt;&gt;&gt; syntax as we go down to the portals layer. &nbsp;It also would lead to a<BR>
&gt;&gt;&gt;&gt; larger degree of change within the OMPI ALPS code base. &nbsp;However, I<BR>
&gt;&gt;&gt;&gt; can<BR>
&gt;&gt;&gt;&gt; give ALPS-based systems the same feature set as the rest of the world.<BR>
&gt;&gt;&gt;&gt; It just is more efficient to use an additional pointer in the<BR>
&gt;&gt;&gt;&gt; orte_node_t structure and results is a far simpler code structure. <BR>
&gt;&gt;&gt;&gt; This<BR>
&gt;&gt;&gt;&gt; makes it easier to maintain.<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; The only thing that &quot;this change&quot; really does is to identify the<BR>
&gt;&gt;&gt;&gt; cluster<BR>
&gt;&gt;&gt;&gt; under which the ALPS allocation is made. &nbsp;If you are addressing a node<BR>
&gt;&gt;&gt;&gt; in another cluster, (e.g., via accept/connect), the clustername/NID<BR>
&gt;&gt;&gt;&gt; pair<BR>
&gt;&gt;&gt;&gt; is unique for ALPS as a hostname on a cluster node is unique between<BR>
&gt;&gt;&gt;&gt; clusters. &nbsp;If you do a gethostname() on a normal cluster node, you are<BR>
&gt;&gt;&gt;&gt; going to get mynameNNNNN, or something similar. &nbsp;If you do a<BR>
&gt;&gt;&gt;&gt; gethostname() on an ALPS node, you are going to get nidNNNNN; there is<BR>
&gt;&gt;&gt;&gt; no differentiation between cluster A and cluster B.<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; Perhaps, my earlier comment was not accurate. &nbsp;In reality, it provides<BR>
&gt;&gt;&gt;&gt; the same degree of identification for ALPS nodes as hostname provides<BR>
&gt;&gt;&gt;&gt; for normal clusters. &nbsp;From your perspective, it is immaterial that it<BR>
&gt;&gt;&gt;&gt; also would allow us to support our limited form of multi-cluster<BR>
&gt;&gt;&gt;&gt; support. &nbsp;However, of and by itself, it only provides the same<BR>
&gt;&gt;&gt;&gt; level of<BR>
&gt;&gt;&gt;&gt; identification as is done for other cluster nodes.<BR>
&gt;&gt;&gt;&gt; --<BR>
&gt;&gt;&gt;&gt; Ken<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; -----Original Message-----<BR>
&gt;&gt;&gt;&gt; From: Ralph Castain [<a href="mailto:rhc@lanl.gov">mailto:rhc@lanl.gov</a>]<BR>
&gt;&gt;&gt;&gt; Sent: Monday, September 22, 2008 2:33 PM<BR>
&gt;&gt;&gt;&gt; To: Open MPI Developers<BR>
&gt;&gt;&gt;&gt; Cc: Matney Sr, Kenneth D.<BR>
&gt;&gt;&gt;&gt; Subject: Re: [OMPI devel] [OMPI svn] svn:open-mpi r19600<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; The issue isn't with adding a string. The question is whether or not<BR>
&gt;&gt;&gt;&gt; OMPI is to support one job running across multiple clusters. We made a<BR>
&gt;&gt;&gt;&gt; conscious decision (after lengthy discussions on OMPI core and ORTE<BR>
&gt;&gt;&gt;&gt; mailing lists, plus several telecons) to not do so - we require that<BR>
&gt;&gt;&gt;&gt; the job execute on a single cluster, while allowing connect/accept to<BR>
&gt;&gt;&gt;&gt; occur between jobs on different clusters.<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; It is difficult to understand why we need a string (or our old &quot;cell<BR>
&gt;&gt;&gt;&gt; id&quot;) to tell us which cluster we are on if we are only following that<BR>
&gt;&gt;&gt;&gt; operating model. From the commit comment, and from what I know of the<BR>
&gt;&gt;&gt;&gt; system, the only rationale for adding such a designator is to shift<BR>
&gt;&gt;&gt;&gt; back to the one-mpirun-spanning-multiple-cluster model.<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; If we are now going to make that change, then it merits a similar<BR>
&gt;&gt;&gt;&gt; level of consideration as the last decision to move away from that<BR>
&gt;&gt;&gt;&gt; model. Making that move involves considerably more than just adding a<BR>
&gt;&gt;&gt;&gt; cluster id string. You may think that now, but the next step is<BR>
&gt;&gt;&gt;&gt; inevitably to bring back remote launch, killing jobs on all clusters<BR>
&gt;&gt;&gt;&gt; when one cluster has a problem, etc.<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; Before we go down this path and re-open Pandora's box, we should at<BR>
&gt;&gt;&gt;&gt; least agree that is what we intend to do...or agree on what hard<BR>
&gt;&gt;&gt;&gt; constraints we will place on multi-cluster operations. Frankly, I'm<BR>
&gt;&gt;&gt;&gt; tired of bouncing back-and-forth on even the most basic design<BR>
&gt;&gt;&gt;&gt; decisions.<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; Ralph<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt; On Sep 22, 2008, at 11:55 AM, Richard Graham wrote:<BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt; What Ken put in is what is needed for the limited multi-cluster<BR>
&gt;&gt;&gt;&gt;&gt; capabilities<BR>
&gt;&gt;&gt;&gt;&gt; we need, just one additional string. &nbsp;I don't think there is a need<BR>
&gt;&gt;&gt;&gt;&gt; for any<BR>
&gt;&gt;&gt;&gt;&gt; discussion of such a small change.<BR>
&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt; Rich<BR>
&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt; On 9/22/08 1:32 PM, &quot;Ralph Castain&quot; &lt;<a href="rhc@lanl.gov">rhc@lanl.gov</a>&gt; wrote:<BR>
&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt; We really should discuss that as a group first - there is quite a<BR>
&gt;&gt;&gt;&gt;&gt;&gt; bit<BR>
&gt;&gt;&gt;&gt;&gt;&gt; of code required to actually support multi-clusters that has been<BR>
&gt;&gt;&gt;&gt;&gt;&gt; removed.<BR>
&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt; Our operational model that was agreed to quite a while ago is that<BR>
&gt;&gt;&gt;&gt;&gt;&gt; mpirun can -only- extend over a single &quot;cell&quot;. You can<BR>
&gt;&gt;&gt;&gt;&gt;&gt; connect/accept<BR>
&gt;&gt;&gt;&gt;&gt;&gt; multiple mpiruns that are sitting on different cells, but you cannot<BR>
&gt;&gt;&gt;&gt;&gt;&gt; execute a single mpirun across multiple cells.<BR>
&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt; Please keep this on your own development branch for now. Bringing it<BR>
&gt;&gt;&gt;&gt;&gt;&gt; into the trunk will require discussion as this changes the operating<BR>
&gt;&gt;&gt;&gt;&gt;&gt; model, and has significant code consequences when we look at<BR>
&gt;&gt;&gt;&gt;&gt;&gt; abnormal<BR>
&gt;&gt;&gt;&gt;&gt;&gt; terminations, comm_spawn, etc.<BR>
&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt; Thanks<BR>
&gt;&gt;&gt;&gt;&gt;&gt; Ralph<BR>
&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt; On Sep 22, 2008, at 11:26 AM, Richard Graham wrote:<BR>
&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; This check in was in error - I had not realized that the checkout<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; was from<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; the 1.3 branch, so we will fix this, and put these into the trunk<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; (1.4). &nbsp;We<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; are going to bring in some limited multi-cluster support - limited<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; is the<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; operative word.<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Rich<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; On 9/22/08 12:50 PM, &quot;Jeff Squyres&quot; &lt;<a href="jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; I notice that Ken Matney (the committer) is not on the devel<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; list; I<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; added him explicitly to the CC line.<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Ken: please see below.<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; On Sep 22, 2008, at 12:46 PM, Ralph Castain wrote:<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Whoa! We made a decision NOT to support multi-cluster apps in<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; OMPI<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; over a year ago!<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Please remove this from 1.3 - we should discuss if/when this<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; would<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; even be allowed in the trunk.<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Thanks<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Ralph<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; On Sep 22, 2008, at 10:35 AM, <a href="matney@osl.iu.edu">matney@osl.iu.edu</a> wrote:<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Author: matney<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Date: 2008-09-22 12:35:54 EDT (Mon, 22 Sep 2008)<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; New Revision: 19600<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; URL: <a href="https://svn.open-mpi.org/trac/ompi/changeset/19600">https://svn.open-mpi.org/trac/ompi/changeset/19600</a><BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Log:<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Added member to orte_node_t to enable multi-cluster jobs in ALPS<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; scheduled systems (like Cray XT).<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Text files modified:<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; branches/v1.3/orte/runtime/orte_globals.h | &nbsp;&nbsp;&nbsp;&nbsp;4 ++++<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 1 files changed, 4 insertions(+), 0 deletions(-)<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Modified: branches/v1.3/orte/runtime/orte_globals.h<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; =<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; =<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; =<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; =<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; =<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; =<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; =<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; =<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; =<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; =<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; =<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; =<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; =<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; =================================================================<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; --- branches/v1.3/orte/runtime/orte_globals.h (original)<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +++ branches/v1.3/orte/runtime/orte_globals.h 2008-09-22<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 12:35:54<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; EDT (Mon, 22 Sep 2008)<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; @@ -222,6 +222,10 @@<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; /** Username on this node, if specified */<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; char *username;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; char *slot_list;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; + &nbsp;&nbsp;&nbsp;/** Clustername (machine name of cluster) on which this<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; node<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; + &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;resides. &nbsp;ALPS scheduled systems need this to enable<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; + &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;multi-cluster support. &nbsp;*/<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; + &nbsp;&nbsp;&nbsp;char *clustername;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; } orte_node_t;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ORTE_DECLSPEC OBJ_CLASS_DECLARATION(orte_node_t);<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; svn mailing list<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; <a href="svn@open-mpi.org">svn@open-mpi.org</a><BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/svn">http://www.open-mpi.org/mailman/listinfo.cgi/svn</a><BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; devel mailing list<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; <a href="devel@open-mpi.org">devel@open-mpi.org</a><BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; devel mailing list<BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; <a href="devel@open-mpi.org">devel@open-mpi.org</a><BR>
&gt;&gt;&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><BR>
&gt;&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt;&gt; _______________________________________________<BR>
&gt;&gt;&gt;&gt;&gt;&gt; devel mailing list<BR>
&gt;&gt;&gt;&gt;&gt;&gt; <a href="devel@open-mpi.org">devel@open-mpi.org</a><BR>
&gt;&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><BR>
&gt;&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;&gt;&gt; _______________________________________________<BR>
&gt;&gt;&gt;&gt;&gt; devel mailing list<BR>
&gt;&gt;&gt;&gt;&gt; <a href="devel@open-mpi.org">devel@open-mpi.org</a><BR>
&gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><BR>
&gt;&gt;&gt;&gt;<BR>
&gt;&gt;&gt;<BR>
&gt;&gt;&gt; _______________________________________________<BR>
&gt;&gt;&gt; devel mailing list<BR>
&gt;&gt;&gt; <a href="devel@open-mpi.org">devel@open-mpi.org</a><BR>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><BR>
&gt;&gt;<BR>
&gt;&gt; _______________________________________________<BR>
&gt;&gt; devel mailing list<BR>
&gt;&gt; <a href="devel@open-mpi.org">devel@open-mpi.org</a><BR>
&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><BR>
&gt;<BR>
&gt;<BR>
<BR>
_______________________________________________<BR>
devel mailing list<BR>
<a href="devel@open-mpi.org">devel@open-mpi.org</a><BR>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><BR>
<BR>
</SPAN></FONT></BLOCKQUOTE>
</BODY>
</HTML>


