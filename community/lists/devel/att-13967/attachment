<div dir="ltr">I am trying to test the trunk on an SGI UV (to validate Nathan&#39;s port of btl:vader to SGI&#39;s variant of xpmem).<div><br></div><div>At configure time, PBS&#39;s TM support was correctly located.</div><div>
<br></div><div>My PBS batch script includes</div><div><font face="courier new, monospace">  #PBS -l ncpus=16</font></div><div>because that is what this installation requires (not nodes, mppnodes, or anything like that).</div>
<div>One is allocating cpus on a large shared-memory machine, not a set of nodes in a cluster.</div><div><br></div><div>However, this appears to be causing mpirun to think I have just 1 slot:</div><div><br></div><div><div>
<font face="courier new, monospace">+ mpirun -np 2 ./ring_c</font></div><div><font face="courier new, monospace">--------------------------------------------------------------------------</font></div><div><font face="courier new, monospace">There are not enough slots available in the system to satisfy the 2 slots </font></div>
<div><font face="courier new, monospace">that were requested by the application:</font></div><div><font face="courier new, monospace">  ./ring_c</font></div><div><font face="courier new, monospace"><br></font></div><div><font face="courier new, monospace">Either request fewer slots for your application, or make more slots available</font></div>
<div><font face="courier new, monospace">for use.</font></div><div><font face="courier new, monospace">--------------------------------------------------------------------------</font></div></div><div><br></div><div>In case they contain useful info, here are the PBS env vars in the job:</div>
<div><br></div><div><div><font face="courier new, monospace">PBS_HT_NCPUS=32</font></div><div><font face="courier new, monospace">PBS_VERSION=TORQUE-2.3.13</font></div><div><font face="courier new, monospace">PBS_JOBNAME=qs</font></div>
<div><font face="courier new, monospace">PBS_ENVIRONMENT=PBS_BATCH</font></div><div><font face="courier new, monospace">PBS_HOME=/var/spool/torque</font></div><div><font face="courier new, monospace">PBS_O_WORKDIR=/usr/users/6/hargrove/SCRATCH/OMPI/openmpi-trunk-linux-x86_64-uv-trunk/BLD/examples</font></div>
<div><font face="courier new, monospace">PBS_PPN=16</font></div><div><font face="courier new, monospace">PBS_TASKNUM=1</font></div><div><font face="courier new, monospace">PBS_O_HOME=/usr/users/6/hargrove</font></div><div>
<font face="courier new, monospace">PBS_MOMPORT=15003</font></div><div><font face="courier new, monospace">PBS_O_QUEUE=debug</font></div><div><font face="courier new, monospace">PBS_O_LOGNAME=hargrove</font></div><div><font face="courier new, monospace">PBS_O_LANG=en_US.UTF-8</font></div>
<div><font face="courier new, monospace">PBS_JOBCOOKIE=9EEF5DF75FA705A241FEF66EDFE01C5B</font></div><div><font face="courier new, monospace">PBS_NODENUM=0</font></div><div><font face="courier new, monospace">PBS_O_SHELL=/usr/psc/shells/bash</font></div>
<div><font face="courier new, monospace">PBS_SERVER=<a href="http://tg-login1.blacklight.psc.teragrid.org">tg-login1.blacklight.psc.teragrid.org</a></font></div><div><font face="courier new, monospace">PBS_JOBID=<a href="http://314827.tg-login1.blacklight.psc.teragrid.org">314827.tg-login1.blacklight.psc.teragrid.org</a></font></div>
<div><font face="courier new, monospace">PBS_NCPUS=16</font></div><div><font face="courier new, monospace">PBS_O_HOST=<a href="http://tg-login1.blacklight.psc.teragrid.org">tg-login1.blacklight.psc.teragrid.org</a></font></div>
<div><font face="courier new, monospace">PBS_VNODENUM=0</font></div><div><font face="courier new, monospace">PBS_QUEUE=debug_r1</font></div><div><font face="courier new, monospace">PBS_O_MAIL=/var/mail/hargrove</font></div>
<div><font face="courier new, monospace">PBS_NODEFILE=/var/spool/torque/aux//<a href="http://314827.tg-login1.blacklight.psc.teragrid.org">314827.tg-login1.blacklight.psc.teragrid.org</a></font></div><div><font face="courier new, monospace">PBS_O_PATH=[...removed...]</font></div>
</div><div><br clear="all"><div>If any additional info is needed to help make mpirun &quot;just work&quot;, please let me know.</div><div><br></div><div>However, at this point I am mostly interested in any work-arounds that will let me run something other than a singleton on this system.</div>
<div><br></div><div>-Paul</div><div><br></div>-- <br><font face="courier new, monospace"><div>Paul H. Hargrove                          <a href="mailto:PHHargrove@lbl.gov" target="_blank">PHHargrove@lbl.gov</a></div><div>
Future Technologies Group</div><div>Computer and Data Sciences Department     Tel: +1-510-495-2352</div><div>Lawrence Berkeley National Laboratory     Fax: +1-510-486-6900</div></font>
</div></div>

