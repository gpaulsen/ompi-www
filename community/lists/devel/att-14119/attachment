<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">I'm not entirely comfortable with the solution, as the problem truly is that we are doing what you asked - i.e., if you tell Slurm to bind tasks to a single core, then we live within it. The problem with your proposed fix is that we override whatever the user may have actually wanted - e.g., if the user told Slurm to bind us to 4 cores, then we override that constraint.<div><br></div><div>If you can come up with a way that we can launch the orteds in a manner that respects whatever directive was given, while still providing added flexibility, then great. Otherwise, I would say the right solution is for users not to set TaskAffinity when using mpirun.</div><div><br></div><div><br><div><div>On Feb 12, 2014, at 2:42 AM, Artem Polyakov &lt;<a href="mailto:artpol84@gmail.com">artpol84@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr">Hello<div><br></div><div>I found that SLURM installations that use cgroup plugin and have&nbsp;TaskAffinity=yes in cgroup.conf have problems with OpenMPI: all processes on non-launch node are assigned on one core. This leads to quite poor performance.</div>
<div>The problem can be seen only if using mpirun to start parallel application in batch script. For example:&nbsp;<b>mpirun ./mympi</b></div><div>If using srun with PMI affinity is setted properly:&nbsp;<b>srun ./mympi.</b><br></div>
<div><br></div><div>Close look shows that the reason lies in the way Open MPI use srun to launch ORTE daemons. Here is example of the command line:</div><div><b>srun</b>&nbsp;<b>--nodes=1</b> <b>--ntasks=1</b> --kill-on-bad-exit --nodelist=node02 <b>orted</b> -mca ess slurm -mca orte_ess_jobid 3799121920 -mca orte_ess_vpid&nbsp;<br>
</div><div>&nbsp;</div><div>Saying&nbsp;<b>--nodes=1</b>&nbsp;<b>--ntasks=1</b>&nbsp;to SLURM means that you want to start one task and (with&nbsp;TaskAffinity=yes)&nbsp;it will be binded to one core. Next orted use this affinity as base for all spawned branch&nbsp;processes. If I understand correctly the problem behind using srun is that if you say&nbsp;<b>srun</b>&nbsp;<b>--nodes=1</b>&nbsp;<b>--ntasks=4</b>&nbsp;- then SLURM will spawn 4 independent orted processes binded to different cores which is not what we really need.</div>
<div><br></div><div>I found that disabling of cpu binding as a fast hack works good for cgroup plugin. Since job runs inside a group which has core access restrictions, spawned branch processes are executed under nodes scheduler control on all allocated cores.&nbsp;The command line looks like this:</div>
<div>srun <b>--cpu_bind=none</b> --nodes=1 --ntasks=1 --kill-on-bad-exit --nodelist=node02 orted -mca ess slurm -mca orte_ess_jobid 3799121920 -mca orte_ess_vpid&nbsp;</div><div><br></div><div>This solution will probably won't work with SLURM task/affinity plugin. Also it may be a bad idea when strong affinity desirable.</div>
<div><br></div><div>My patch to stable Open MPI version (1.6.5) is attached to this e-mail. I will try to make more reliable solution but I need more time and beforehand would like to know opinion of Open MPI developers.</div>
<div><div><br></div>-- <br>С Уважением, Поляков Артем Юрьевич<br>Best regards, Artem Y. Polyakov
</div></div>
<span>&lt;affinity.patch&gt;</span>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/devel</blockquote></div><br></div></body></html>
