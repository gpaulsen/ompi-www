<p dir="ltr">Jeff</p>
<p dir="ltr">I was wrong about this.  all the mtls except for portals4 register with opal progress in their comp init.</p>
<p dir="ltr">I dont see how this is a problem though as base select only invokes comp init on the selected mtl.  </p>
<p dir="ltr">Howard</p>
<p dir="ltr">----------</p>
<p dir="ltr">sent from my smart phonr so no good type.</p>
<p dir="ltr">Howard</p>
<div class="gmail_quote">On Jul 24, 2015 8:19 AM, &quot;Jeff Squyres (jsquyres)&quot; &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br type="attribution"><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Yohann --<br>
<br>
Can you have a look?<br>
<br>
<br>
&gt; On Jul 24, 2015, at 10:15 AM, Howard Pritchard &lt;<a href="mailto:hppritcha@gmail.com">hppritcha@gmail.com</a>&gt; wrote:<br>
&gt;<br>
&gt; looks like ofi mtl is being naughty.  its tje onlx mtl which registers with opal progress in component init method.<br>
&gt;<br>
&gt; ----------<br>
&gt;<br>
&gt; sent from my smart phonr so no good type.<br>
&gt;<br>
&gt; Howard<br>
&gt;<br>
&gt; On Jul 23, 2015 7:03 PM, &quot;Ralph Castain&quot; &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt; It looks like one of the MTL components is registering a progress call with the opal_progress thread, and then unloading when de-selected. Registering with opal_progress should only be done once the MTL has been selected and will run<br>
&gt;<br>
&gt;<br>
&gt;&gt; On Jul 23, 2015, at 5:05 PM, Paul Hargrove &lt;<a href="mailto:phhargrove@lbl.gov">phhargrove@lbl.gov</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; Yohann,<br>
&gt;&gt;<br>
&gt;&gt; With PR409 as it stands right now (commit 6daef310) I see no change to the behavior.<br>
&gt;&gt; I still get a SEGV below opal_progress() unless I use either<br>
&gt;&gt;    -mca mtl ^ofi<br>
&gt;&gt; OR<br>
&gt;&gt;    -mca pml cm<br>
&gt;&gt;<br>
&gt;&gt; A backtrace from gdb appears below.<br>
&gt;&gt;<br>
&gt;&gt; -Paul<br>
&gt;&gt;<br>
&gt;&gt; (gdb) where<br>
&gt;&gt; #0  0x00007f5bc7b59867 in ?? () from /lib64/libgcc_s.so.1<br>
&gt;&gt; #1  0x00007f5bc7b5a119 in _Unwind_Backtrace () from /lib64/libgcc_s.so.1<br>
&gt;&gt; #2  0x00007f5bcc9b08f6 in __backtrace (array=&lt;value optimized out&gt;, size=32)<br>
&gt;&gt;     at ../sysdeps/ia64/backtrace.c:110<br>
&gt;&gt; #3  0x00007f5bcc3483e1 in opal_backtrace_print (file=0x7f5bccc40880,<br>
&gt;&gt;     prefix=0x7fff6181d1f0 &quot;[pcp-f-5:05049] &quot;, strip=2)<br>
&gt;&gt;     at /scratch/phargrov/OMPI/openmpi-1.10.0rc2-linux-x86_64-sl6x/openmpi-1.10.0rc2/opal/mca/backtrace/execinfo/backtrace_execinfo.c:47<br>
&gt;&gt; #4  0x00007f5bcc3456a9 in show_stackframe (signo=11, info=0x7fff6181d770, p=0x7fff6181d640)<br>
&gt;&gt;     at /scratch/phargrov/OMPI/openmpi-1.10.0rc2-linux-x86_64-sl6x/openmpi-1.10.0rc2/opal/util/stacktrace.c:336<br>
&gt;&gt; #5  &lt;signal handler called&gt;<br>
&gt;&gt; #6  0x00007f5bc7717c58 in ?? ()<br>
&gt;&gt; #7  0x00007f5bcc2f567a in opal_progress ()<br>
&gt;&gt;     at /scratch/phargrov/OMPI/openmpi-1.10.0rc2-linux-x86_64-sl6x/openmpi-1.10.0rc2/opal/runtime/opal_progress.c:187<br>
&gt;&gt; #8  0x00007f5bccebbcb9 in ompi_mpi_init (argc=1, argv=0x7fff6181dd78, requested=0, provided=0x7fff6181dbf8)<br>
&gt;&gt;     at /scratch/phargrov/OMPI/openmpi-1.10.0rc2-linux-x86_64-sl6x/openmpi-1.10.0rc2/ompi/runtime/ompi_mpi_init.c:645<br>
&gt;&gt; #9  0x00007f5bccefbe77 in PMPI_Init (argc=0x7fff6181dc5c, argv=0x7fff6181dc50) at pinit.c:84<br>
&gt;&gt; #10 0x000000000040088e in main (argc=1, argv=0x7fff6181dd78) at ring_c.c:19<br>
&gt;&gt;<br>
&gt;&gt; (gdb) up 6<br>
&gt;&gt; #6  0x00007f5bc7717c58 in ?? ()<br>
&gt;&gt; (gdb) disass<br>
&gt;&gt; No function contains program counter for selected frame.<br>
&gt;&gt;<br>
&gt;&gt; On Thu, Jul 23, 2015 at 8:13 AM, Burette, Yohann &lt;<a href="mailto:yohann.burette@intel.com">yohann.burette@intel.com</a>&gt; wrote:<br>
&gt;&gt; Paul,<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; While looking at the issue, we noticed that we were missing some code that deals with MTL priorities.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; PR 409 (<a href="https://github.com/open-mpi/ompi-release/pull/409" rel="noreferrer" target="_blank">https://github.com/open-mpi/ompi-release/pull/409</a>) is attempting to fix that.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Hopefully, this will also fix the error you encountered.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Thanks again,<br>
&gt;&gt;<br>
&gt;&gt; Yohann<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; From: devel [mailto:<a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a>] On Behalf Of Paul Hargrove<br>
&gt;&gt; Sent: Wednesday, July 22, 2015 12:07 PM<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; To: Open MPI Developers<br>
&gt;&gt; Subject: Re: [OMPI devel] 1.10.0rc2<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Yohann,<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Things run fine with those additional flags.<br>
&gt;&gt;<br>
&gt;&gt; In fact, adding just &quot;--mca pml cm&quot; is sufficient to eliminate the SEGV.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; -Paul<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; On Wed, Jul 22, 2015 at 8:49 AM, Burette, Yohann &lt;<a href="mailto:yohann.burette@intel.com">yohann.burette@intel.com</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; Hi Paul,<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Thank you for doing all this testing!<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; About 1), it’s hard for me to see whether it’s a problem with mtl:ofi or with how OMPI selects the components to use.<br>
&gt;&gt;<br>
&gt;&gt; Could you please run your test again with “--mca mtl ofi --mca mtl_ofi_provider sockets --mca pml cm”?<br>
&gt;&gt;<br>
&gt;&gt; The idea is that if it still fails, then we have a problem with either mtl:ofi or the OFI/sockets provider. If it works, then there is an issue with how OMPI selects what component to use.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; I just tried 1.10.0rc2 with the latest libfabric (master) and it seems to work fine.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Yohann<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; From: devel [mailto:<a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a>] On Behalf Of Paul Hargrove<br>
&gt;&gt; Sent: Wednesday, July 22, 2015 1:05 AM<br>
&gt;&gt; To: Open MPI Developers<br>
&gt;&gt; Subject: Re: [OMPI devel] 1.10.0rc2<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; 1.10.0rc2 looks mostly good to me, but I still found some issues.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; 1) New to this round of testing, I have built mtl:ofi with gcc, pgi, icc, clang, open64 and studio compilers.<br>
&gt;&gt;<br>
&gt;&gt; I have only the sockets provider in libfaric (v1.0.0 and 1.1.0rc2).<br>
&gt;&gt;<br>
&gt;&gt; However, unless I pass &quot;-mca mtl ^ofi&quot; to mpirun I get a SEGV from a callback invoked in opal_progress().<br>
&gt;&gt;<br>
&gt;&gt; Gdb did not give a function name for the  callback, but the PC looks valid.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; 2) Of the several compilers I tried, only pgi-13.0 failed to compile mtl:ofi:<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;         /bin/sh ../../../../libtool  --tag=CC   --mode=compile pgcc -DHAVE_CONFIG_H -I. -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/ompi/mca/mtl/ofi -I../../../../opal/include -I../../../../orte/include -I../../../../ompi/include -I../../../../oshmem/include -I../../../../opal/mca/hwloc/hwloc191/hwloc/include/private/autogen -I../../../../opal/mca/hwloc/hwloc191/hwloc/include/hwloc/autogen  -I/usr/common/ftg/libfabric/1.1.0rc2p1/include -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2 -I../../../.. -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/opal/include -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/orte/include -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/ompi/include -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/oshmem/include   -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/opal/mca/hwloc/hwloc191/hwloc/include -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/BLD/opal/mca/hwloc/hwloc191/hwloc/include -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/opal/mca/event/libevent2021/libevent -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/opal/mca/event/libevent2021/libevent/include -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/BLD/opal/mca/event/libevent2021/libevent/include  -g  -c -o mtl_ofi_component.lo /global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/ompi/mca/mtl/ofi/mtl_ofi_component.c<br>
&gt;&gt;<br>
&gt;&gt; libtool: compile:  pgcc -DHAVE_CONFIG_H -I. -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/ompi/mca/mtl/ofi -I../../../../opal/include -I../../../../orte/include -I../../../../ompi/include -I../../../../oshmem/include -I../../../../opal/mca/hwloc/hwloc191/hwloc/include/private/autogen -I../../../../opal/mca/hwloc/hwloc191/hwloc/include/hwloc/autogen -I/usr/common/ftg/libfabric/1.1.0rc2p1/include -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2 -I../../../.. -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/opal/include -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/orte/include -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/ompi/include -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/oshmem/include -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/opal/mca/hwloc/hwloc191/hwloc/include -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/BLD/opal/mca/hwloc/hwloc191/hwloc/include -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/opal/mca/event/libevent2021/libevent -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/opal/mca/event/libevent2021/libevent/include -I/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/BLD/opal/mca/event/libevent2021/libevent/include -g -c /global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/ompi/mca/mtl/ofi/mtl_ofi_component.c  -fpic -DPIC -o .libs/mtl_ofi_component.o<br>
&gt;&gt;<br>
&gt;&gt; PGC-S-0060-opal_convertor_clone is not a member of this struct or union (/global/homes/h/hargrove/GSCRATCH/OMPI/openmpi-1.10.0rc2-linux-x86_64-pgi-13.10/openmpi-1.10.0rc2/ompi/mca/mtl/ofi/mtl_ofi_component.c: 51)<br>
&gt;&gt;<br>
&gt;&gt; pgcc-Fatal-/global/scratch2/sd/hargrove/pgi-13.10/linux86-64/13.10/bin/pgc TERMINATED by signal 11<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Since this ends with a SEGV in the compiler, I don&#39;t think this is an issue with the C code, just a plain compiler bug.<br>
&gt;&gt;<br>
&gt;&gt; At lease pgi-9.0-4 and pgi-10.9 compiled the code just fine.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; 3) As I noted in a separate email, there are some newly uncovered issues in the embedded hwloc w/ pgi and -m32.<br>
&gt;&gt;<br>
&gt;&gt; However, I had not tested such configurations previously, and all indications are that these issues have existed for a while.<br>
&gt;&gt;<br>
&gt;&gt; Brice is on vacation, so there will not be an official hwloc fix for this issue until next week at the earliest.<br>
&gt;&gt;<br>
&gt;&gt; [The upside is that I now have coverage for eight additional x86 configurations (true x86 or x86-64 w/ -m32).]<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; 4) I noticed a couple warnings somebody might want to investigate:<br>
&gt;&gt;<br>
&gt;&gt;   openmpi-1.10.0rc2/ompi/mca/btl/openib/connect/btl_openib_connect_udcm.c:2323:59: warning: format specifies type &#39;int&#39; but the argument has type &#39;struct ibv_qp *&#39; [-Wformat]<br>
&gt;&gt;<br>
&gt;&gt;   openmpi-1.10.0rc2/ompi/mca/btl/openib/connect/btl_openib_connect_udcm.c&quot;, line 2471: warning: improper pointer/integer combination: arg #3<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Also worth noting:<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; The ConnectX and ConnectIB XRC detection logic appears to be working as expected on multiple systems.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; I also have learned that pgi-9.0-4 is not a conforming C99 compiler when passed -m32, which is not Open MPI&#39;s fault.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; And as before...<br>
&gt;&gt;<br>
&gt;&gt; + I am currently without any SPARC platforms<br>
&gt;&gt;<br>
&gt;&gt; + Several qemu-emulated ARM and MIPS tests will complete by morning (though I have some ARM successes already)<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; -Paul<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; On Tue, Jul 21, 2015 at 12:29 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; Hey folks<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; 1.10.0rc2 is now out for review - excepting the library version numbers, this should be the final version. Please take a quick gander and let me know of any problems.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; <a href="http://www.open-mpi.org/software/ompi/v1.10/" rel="noreferrer" target="_blank">http://www.open-mpi.org/software/ompi/v1.10/</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Ralph<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; devel mailing list<br>
&gt;&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/07/17670.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/07/17670.php</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt;<br>
&gt;&gt; Paul H. Hargrove                          <a href="mailto:PHHargrove@lbl.gov">PHHargrove@lbl.gov</a><br>
&gt;&gt;<br>
&gt;&gt; Computer Languages &amp; Systems Software (CLaSS) Group<br>
&gt;&gt;<br>
&gt;&gt; Computer Science Department               Tel: <a href="tel:%2B1-510-495-2352" value="+15104952352">+1-510-495-2352</a><br>
&gt;&gt;<br>
&gt;&gt; Lawrence Berkeley National Laboratory     Fax: <a href="tel:%2B1-510-486-6900" value="+15104866900">+1-510-486-6900</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; devel mailing list<br>
&gt;&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/07/17681.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/07/17681.php</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt;<br>
&gt;&gt; Paul H. Hargrove                          <a href="mailto:PHHargrove@lbl.gov">PHHargrove@lbl.gov</a><br>
&gt;&gt;<br>
&gt;&gt; Computer Languages &amp; Systems Software (CLaSS) Group<br>
&gt;&gt;<br>
&gt;&gt; Computer Science Department               Tel: <a href="tel:%2B1-510-495-2352" value="+15104952352">+1-510-495-2352</a><br>
&gt;&gt;<br>
&gt;&gt; Lawrence Berkeley National Laboratory     Fax: <a href="tel:%2B1-510-486-6900" value="+15104866900">+1-510-486-6900</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; devel mailing list<br>
&gt;&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/07/17687.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/07/17687.php</a><br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; Paul H. Hargrove                          <a href="mailto:PHHargrove@lbl.gov">PHHargrove@lbl.gov</a><br>
&gt;&gt; Computer Languages &amp; Systems Software (CLaSS) Group<br>
&gt;&gt; Computer Science Department               Tel: <a href="tel:%2B1-510-495-2352" value="+15104952352">+1-510-495-2352</a><br>
&gt;&gt; Lawrence Berkeley National Laboratory     Fax: <a href="tel:%2B1-510-486-6900" value="+15104866900">+1-510-486-6900</a><br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; devel mailing list<br>
&gt;&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/07/17688.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/07/17688.php</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/07/17689.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/07/17689.php</a><br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/07/17690.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/07/17690.php</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" rel="noreferrer" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/07/17691.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/07/17691.php</a></blockquote></div>

