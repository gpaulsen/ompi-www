<div dir="ltr">Hi Folks,<div><br></div><div>First initial disclaimer - I&#39;ve looked through the open mpi faq and have been unable</div><div>so far an answer to my question below.<br></div><div><br></div><div>I&#39;ve been having a discussion with one of the other trilab folks about some issues with</div><div>using PSM within mvpaich where the default cpu affinity behavior of PSM can cause problems.</div><div>It turns out that the default behavior of PSM appears to be to set cpu affinity for a process</div><div>which calls psm_ep_open if process affinity has not already been set.  We&#39;re finding that</div><div>it is necesary to use the PSM_EP_OPEN_AFFINITY_SKIP setting in the affinity field</div><div>of the psm_opts struct that is passed to psm_ep_open in order to work around the problem.</div><div><br></div><div>The problem has to do with singleton processes.  If mvapich is using psm and multiple</div><div>singleton jobs are scheduled on a node, they all by default end up binding to core 0.</div><div>Setting the above option eliminates this problem.  </div><div><br></div><div>Could Open MPI also potentially have this same problem?  If so, I&#39;d want to add an mca param</div><div>to set this option before calling psm_ep_open within psm mtl.  Hmm.. maybe the ofi mtl</div><div>supporter should talk with the libfabric psm provider folks about this.</div><div><br></div><div>Thanks for any help,</div><div><br></div><div>Howard</div><div><br></div></div>

