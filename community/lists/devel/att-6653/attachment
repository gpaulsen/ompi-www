I don&#39;t disagree with your statements. However, I was addressing the specific question of two OpenMPI programs conflicting on process placement, not the overall question you are raising.<br><br>The issue of when/if to bind has been debated for a long time. I agree that having more options (bind-to-socket, bind-to-core, etc) makes sense and that the choice of a default is difficult, for all the reasons that have been raised in this thread.<br>
<br>At issue for us is that other MPIs -do- bind by default, thus creating an apparent performance advantage for themselves compared to us on standard benchmarks run &quot;out-of-the-box&quot;. We repeatedly get beat-up in papers and elsewhere over our performance, when many times the major difference is in the default binding. If we bind the same way they do, then the performance gap disappears or is minimal.<br>
<br>So this is why we are wrestling with this issue. I&#39;m not sure of the best compromise here, but I think people have raised good points on all sides. Unfortunately, there problem isn&#39;t a perfect answer... :-/<br>
<br>Certainly, I have no clue what it would be! Not that smart :-)<br>Ralph<br><br><br><div class="gmail_quote">On Mon, Aug 17, 2009 at 9:12 AM, N.M. Maclaren <span dir="ltr">&lt;<a href="mailto:nmm1@cam.ac.uk">nmm1@cam.ac.uk</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><div class="im">On Aug 17 2009, Ralph Castain wrote:<br>
<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
The problem is that the two mpiruns don&#39;t know about each other, and  therefore the second mpirun doesn&#39;t know that another mpirun has  already used socket 0.<br>
<br>
We hope to change that at some point in the future.<br>
</blockquote>
<br></div>
It won&#39;t help.  The problem is less likely to be that two jobs are running<br>
OpenMPI programs (that have been recently linked!), but that the other tasks<br>
are not OpenMPI at all.  I have mentioned daemons, kernel threads and so on,<br>
but think of shared-memory parallel programs (OpenMP etc.) and so on; a LOT<br>
of applications nowadays include some sort of threading.<br>
<br>
For the ordinary multi-user system, you don&#39;t want any form of binding. The scheduler is ricketty enough as it is, without confusing it further. That may change as the consequences of serious levels of multiple cores force that area to be improved, but don&#39;t hold your breath. And I haven&#39;t a clue which of the many directions scheduler design will go!<br>

<br>
I agree that having an option, and having it easy to experiment with, is the<br>
right way to go.  What the default should be is very much less clear.<br>
<br>
Regards,<br>
Nick Maclaren.<div><div></div><div class="h5"><br>
<br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</div></div></blockquote></div><br>

