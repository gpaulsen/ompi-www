Hi.<br><br>I am using IMB-3.1, an Intel MPI Benchmark tool with OpenMPI(v-1.2.5). In /IMB-3.1/src/make_mpich file, I had only given the decalartion for MPI_HOME, which takes care for CC, OPTFLAGS &amp; CLINKER. Building IMB_MPI1, IMP-EXT &amp; IMB-IO happens succesfully. <br>
<br>I get proper results of IMB Benchmark with command &quot;-np 1&quot; as mpirun IMB-MPI1, but for &quot;-np 2&quot;, I get below errors -<br><br>-----<br>[mukesh@n161 src]$ mpirun -np 2 IMB-MPI1<br>[n161:13390] *** Process received signal ***<br>
[n161:13390] Signal: Segmentation fault (11)<br>[n161:13390] Signal code: Address not mapped (1)<br>[n161:13390] Failing at address: (nil)<br>[n161:13390] [ 0] /lib64/tls/libpthread.so.0 [0x399e80c4f0]<br>[n161:13390] [ 1] /home/mukesh/openmpi/prefix/lib/openmpi/mca_btl_sm.so [0x2a9830f8b4]<br>
[n161:13390] [ 2] /home/mukesh/openmpi/prefix/lib/openmpi/mca_btl_sm.so [0x2a983109e3]<br>[n161:13390] [ 3] /home/mukesh/openmpi/prefix/lib/openmpi/mca_btl_sm.so(mca_btl_sm_component_progress+0xbc) [0x2a9830fc50]<br>[n161:13390] [ 4] /home/mukesh/openmpi/prefix/lib/openmpi/mca_bml_r2.so(mca_bml_r2_progress+0x4b) [0x2a97fce447]<br>
[n161:13390] [ 5] /home/mukesh/openmpi/prefix/lib/libopen-pal.so.0(opal_progress+0xbc) [0x2a958fc343]<br>[n161:13390] [ 6] /home/mukesh/openmpi/prefix/lib/openmpi/mca_oob_tcp.so(mca_oob_tcp_msg_wait+0x22) [0x2a962e9e22]<br>
[n161:13390] [ 7] /home/mukesh/openmpi/prefix/lib/openmpi/mca_oob_tcp.so(mca_oob_tcp_recv+0x677) [0x2a962f1aab]<br>[n161:13390] [ 8] /home/mukesh/openmpi/prefix/lib/libopen-rte.so.0(mca_oob_recv_packed+0x46) [0x2a9579d243]<br>
[n161:13390] [ 9] /home/mukesh/openmpi/prefix/lib/openmpi/mca_gpr_proxy.so(orte_gpr_proxy_put+0x2f3) [0x2a96508c8f]<br>[n161:13390] [10] /home/mukesh/openmpi/prefix/lib/libopen-rte.so.0(orte_smr_base_set_proc_state+0x425) [0x2a957c391d]<br>
[n161:13390] [11] /home/mukesh/openmpi/prefix/lib/libmpi.so.0(ompi_mpi_init+0xa1e) [0x2a9559f042]<br>[n161:13390] [12] /home/mukesh/openmpi/prefix/lib/libmpi.so.0(PMPI_Init_thread+0xcb) [0x2a955e1c5b]<br>[n161:13390] [13] IMB-MPI1(main+0x33) [0x403543]<br>
[n161:13390] [14] /lib64/tls/libc.so.6(__libc_start_main+0xdb) [0x399e11c3fb]<br>[n161:13390] [15] IMB-MPI1 [0x40347a]<br>[n161:13390] *** End of error message ***<br>[n161:13391] *** Process received signal ***<br>[n161:13391] Signal: Segmentation fault (11)<br>
[n161:13391] Signal code: Address not mapped (1)<br>[n161:13391] Failing at address: (nil)<br>[n161:13391] [ 0] /lib64/tls/libpthread.so.0 [0x399e80c4f0]<br>[n161:13391] [ 1] /home/mukesh/openmpi/prefix/lib/openmpi/mca_btl_sm.so [0x2a9830f8b4]<br>
[n161:13391] [ 2] /home/mukesh/openmpi/prefix/lib/openmpi/mca_btl_sm.so [0x2a983109e3]<br>[n161:13391] [ 3] /home/mukesh/openmpi/prefix/lib/openmpi/mca_btl_sm.so(mca_btl_sm_component_progress+0xbc) [0x2a9830fc50]<br>[n161:13391] [ 4] /home/mukesh/openmpi/prefix/lib/openmpi/mca_bml_r2.so(mca_bml_r2_progress+0x4b) [0x2a97fce447]<br>
[n161:13391] [ 5] /home/mukesh/openmpi/prefix/lib/libopen-pal.so.0(opal_progress+0xbc) [0x2a958fc343]<br>[n161:13391] [ 6] /home/mukesh/openmpi/prefix/lib/openmpi/mca_oob_tcp.so(mca_oob_tcp_msg_wait+0x22) [0x2a962e9e22]<br>
[n161:13391] [ 7] /home/mukesh/openmpi/prefix/lib/openmpi/mca_oob_tcp.so(mca_oob_tcp_recv+0x677) [0x2a962f1aab]<br>[n161:13391] [ 8] /home/mukesh/openmpi/prefix/lib/libopen-rte.so.0(mca_oob_recv_packed+0x46) [0x2a9579d243]<br>
[n161:13391] [ 9] /home/mukesh/openmpi/prefix/lib/libopen-rte.so.0 [0x2a9579e910]<br>[n161:13391] [10] /home/mukesh/openmpi/prefix/lib/libopen-rte.so.0(mca_oob_xcast+0x140) [0x2a9579d824]<br>[n161:13391] [11] /home/mukesh/openmpi/prefix/lib/libmpi.so.0(ompi_mpi_init+0xaf1) [0x2a9559f115]<br>
[n161:13391] [12] /home/mukesh/openmpi/prefix/lib/libmpi.so.0(PMPI_Init_thread+0xcb) [0x2a955e1c5b]<br>[n161:13391] [13] IMB-MPI1(main+0x33) [0x403543]<br>[n161:13391] [14] /lib64/tls/libc.so.6(__libc_start_main+0xdb) [0x399e11c3fb]<br>
[n161:13391] [15] IMB-MPI1 [0x40347a]<br>[n161:13391] *** End of error message ***<br><br>-----<br><br>Query#1: Any clue for above?<br><br>Query#2:&nbsp; How can I include seperate exe file and have the IMB for it, e.g, writing a hello.c with MPI elementary API calls, compiling with mpicc and performing IMB for the same exe.?<br>
<br>BR<br>

