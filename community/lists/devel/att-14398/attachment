<div dir="ltr"><div><div>1) By heterogeneous do you mean Derived Datatypes?<br></div>       MPJ Express&#39;s buffering layer handles this. It flattens the data into a ByteBuffer. In this way native device doesn&#39;t have to worry about Derived Datatypes (those things are handled at top layers). And an interesting thing, intuitively Java users would use the MPI.OBJECT if there is heterogeneous data to be sent (but yes, MPI.OBJECT is a slow case  ...)<br>

<br></div>       Currently same goes for user defined Op-functions. Those are handled at the top layers, i.e using MPJ Express&#39;s algorithms not native MPI&#39;s (but communication is native).<br><div><br></div><div>2) API changes: Do you envision to document the changes to something like a mpiJava 1.3 specs or something?<br>

</div><div><div><div><div class="gmail_extra"><br></div><div class="gmail_extra">3) New Benchmark Results:<br></div><div class="gmail_extra">       I did the benchmarking again with various configurations:<br></div><div class="gmail_extra">

<br>       i) Open MPI 1.7.4 C<br><br></div><div class="gmail_extra">       ii) MVAPICH2.2 C<br></div><div class="gmail_extra"><br>       iii) MPJ Express (using Open MPI - with arrays)<br></div><div class="gmail_extra">
<br>
       iv) Open MPI&#39;s Java Bindings (with a large user array -- the unoptimized case)<br></div><div class="gmail_extra"><br>       v) Open MPI&#39;s Java Bindings (with arrays, where size of the user array is equal to the data point, to be fair)<br>

</div><div class="gmail_extra"><br>       vi) MPJ Express (using MVAPICH2.2 - with arrays)<br></div><div class="gmail_extra"><br>       vii) Open MPI&#39;s Java Bindings (using MPI.new&lt;Type&gt;Buffer, ByteBuffer)<br></div>

<div class="gmail_extra"><br>       viii) MPJ Express (using Open MPI - with ByteBuffer, this is from the device layer of MPJ Express, this helps see how MPJ Express could perform if in future we add MPI.new&lt;Type&gt;Buffer like functionality)<br>

</div><div class="gmail_extra"><br>       ix) MPJ Express (using MVAPICH2.2 - with ByteBuffer) --&gt; currently I don&#39;t know how it performs better than Open MPI?<br></div><div class="gmail_extra"><br></div><div class="gmail_extra">

<br clear="all"><div><div dir="ltr"><span style="color:rgb(0,0,0)">Bibrak Qamar</span><br style="color:rgb(0,0,0)"><span style="color:rgb(0,0,0)"><br></span><font color="#888888"></font></div></div>
<br><br><div class="gmail_quote">On Mon, Mar 24, 2014 at 10:16 PM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex">

<div class="">On Mar 14, 2014, at 9:29 AM, Bibrak Qamar &lt;<a href="mailto:bibrakc@gmail.com">bibrakc@gmail.com</a>&gt; wrote:<br>
<br>
&gt; It works for Open MPI but for MPICH3 I have to comment the dlopen. Is there any way to tell the compiler if its using Open MPI (mpicc) then use dlopen else keep it commented? Or some thing else?<br>
<br>
</div>If Open MPI&#39;s mpi.h, we have defined OPEN_MPI.  You can therefore use #if defined (OPEN_MPI).<br>
<div class=""><br>
&gt; Yes, there are some places where we need to be sync with the internals of the native MPI implementation. These are in section 8.1.2 of MPI 2.1 (<a href="http://www.mpi-forum.org/docs/mpi-2.1/mpi21-report.pdf" target="_blank">http://www.mpi-forum.org/docs/mpi-2.1/mpi21-report.pdf</a>). For example the MPI_TAG_UB. For the pure Java devices of MPJ Express we have always used Integer.MAX_VALUE.<br>


&gt;<br>
&gt; Datatypes?<br>
&gt;<br>
&gt; MPJ Express uses an internal buffering layer to buffer the user data into a ByteBuffer. In this way for the native device we end up using the MPI_BYTE datatype most of the time. ByteBuffer simplifies matters since it is directly accessible from the native code.<br>


<br>
</div>Does that mean you can&#39;t do heterogeneous?  (not really a huge deal, since most people don&#39;t run heterogeneously, but something to consider)<br>
<div class=""><br>
&gt; With our current implementation there is one exception to it i.e. in the Reduce, Allreduce and Reduce_scatter where the native MPI implementation needs to know which Java datatype its going to process. Same goes for MPI.Op<br>


<br>
</div>And Accumulate and the other Op-using functions, right?<br>
<div class=""><br>
&gt; On Are your bindings similar in style/signature to ours?<br>
<br>
</div>No, they use the real datatypes.<br>
<div class=""><br>
&gt; I checked it and there are differences. MPJ Express (and FastMPJ also) implements the mpiJava 1.2 specifications. There is also MPJ API (this is very close to mpiJava 1.2 API).<br>
&gt;<br>
&gt; Example 1: Getting the rank and size of COMM_WORLD<br>
&gt;<br>
&gt; MPJ Express (the mpiJava 1.2 API):<br>
&gt;  public int Size() throws MPIException;<br>
&gt;  public int Rank() throws MPIException;<br>
&gt;<br>
&gt; MPJ API:<br>
&gt;  public int size() throws MPJException;<br>
&gt;  public int rank() throws MPJException;<br>
&gt;<br>
&gt; Open MPI&#39;s Java bindings:<br>
&gt;  public final int getRank() throws MPIException;<br>
&gt;  public final int getSize() throws MPIException;<br>
<br>
</div>Right -- we *started* with the old ideas, but then made the conscious choice to update the Java bindings in a few ways:<br>
<br>
- make them more like modern Java conventions (e.g., camel case, use verbs, etc.)<br>
- get rid of MPI.OBJECT<br>
- use modern, efficient Java practices<br>
- basically, we didn&#39;t want to be bound by any Java decisions that were made long ago that aren&#39;t necessarily relevant any more<br>
- and to be clear: we couldn&#39;t find many existing Java MPI codes, so compatibility with existing Java MPI codes was not a big concern<br>
<br>
One thing we didn&#39;t do was use bounce buffers for small messages, which shows up in your benchmarks.  We&#39;re considering adding that optimization, and others.<br>
<div class=""><br>
&gt; Example 2: Point-to-Point communication<br>
&gt;<br>
&gt; MPJ Express (the mpiJava 1.2 API):<br>
&gt;  public void Send(Object buf, int offset, int count, Datatype datatype, int dest, int tag) throws MPIException<br>
&gt;<br>
&gt;  public Status Recv(Object buf, int offset, int count, Datatype datatype,<br>
&gt;       int source, int tag) throws MPIException<br>
&gt;<br>
&gt; MPJ API:<br>
&gt;  public void send(Object buf, int offset, int count, Datatype datatype, int dest, int tag) throws MPJException;<br>
&gt;<br>
&gt;  public Status recv(Object buf, int offset, int count, Datatype datatype, int source, int tag) throws MPJException<br>
&gt;<br>
&gt; Open MPI&#39;s Java bindings:<br>
&gt;  public final void send(Object buf, int count, Datatype type, int dest, int tag) throws MPIException<br>
&gt;<br>
&gt;  public final Status recv(Object buf, int count, Datatype type, int source, int tag) throws MPIException<br>
&gt;<br>
&gt; Example 3: Collective communication<br>
&gt;<br>
&gt; MPJ Express (the mpiJava 1.2 API):<br>
&gt;  public void Bcast(Object buf, int offset, int count, Datatype type, int root)<br>
&gt;       throws MPIException;<br>
&gt;<br>
&gt; MPJ API:<br>
&gt;  public void bcast(Object buffer, int offset, int count, Datatype datatype, int root) throws MPJException;<br>
&gt;<br>
&gt; Open MPI&#39;s Java bindings:<br>
&gt;  public final void bcast(Object buf, int count, Datatype type, int root) throws MPIException;<br>
&gt;<br>
&gt;<br>
&gt; I couldn&#39;t find which API the Open MPI&#39;s Java bindings implement?<br>
<br>
</div>Our own.  :-)<br>
<div class=""><br>
&gt; But while reading your README.JAVA.txt and your code I realised that you are trying to avoid buffering overhead by giving the user the flexibility to directly allocate data onto a ByteBuffer using MPI.new&lt;Type&gt;Buffer, hence not following the mpiJava 1.2 specs (for communication operations)?<br>


<br>
</div>Correct.<br>
<div class=""><br>
&gt; On Performance Comparison<br>
&gt;<br>
&gt; Yes this is interesting, I have managed to do two kind of tests: Ping-Pong (Latency and Bandwidth) and Collective Communications (Bcast).<br>
&gt;<br>
&gt; Attached are graphs and the programs (testcases) that I used. The tests were done using Infiniband, more on the platform here <a href="http://www.nust.edu.pk/INSTITUTIONS/Centers/RCMS/AboutUs/facilities/screc/Pages/Resources.aspx" target="_blank">http://www.nust.edu.pk/INSTITUTIONS/Centers/RCMS/AboutUs/facilities/screc/Pages/Resources.aspx</a><br>


&gt;<br>
&gt; One reason for Open MPI&#39;s java bindings low performance (in the Bandwidth.png graph) is the way the test case was written (Bandwidth_OpenMPi.java). It allocates a total of 16M of byte array and uses the same array in send/recv for each data point (by varying count).<br>


&gt;<br>
&gt; This could be mainly because of the following code in mpi_Comm.c (let me know if I am mistaken)<br>
&gt;<br>
&gt; static void* getArrayPtr(void** bufBase, JNIEnv *env,<br>
&gt;                          jobject buf, int baseType, int offset)<br>
&gt; {<br>
&gt;     switch(baseType)<br>
&gt;     {<br>
&gt;            ...<br>
&gt;            ...<br>
&gt;           case 1: {<br>
&gt;             jbyte* els = (*env)-&gt;GetByteArrayElements(env, buf, NULL);<br>
&gt;             *bufBase = els;<br>
&gt;             return els + offset;<br>
&gt;         }<br>
&gt;            ...<br>
&gt;            ...<br>
&gt; }<br>
&gt;<br>
&gt; Get&lt;PrimitiveType&gt;ArrayElements routine every time gets the entire array even if the user wants to send some elements (the count). This might be one reason for Open MPI&#39; Java bindings to advocate for the MPI.new&lt;Type&gt;Buffer. The other reason is naturally the buffering overhead.<br>


<br>
</div>Yes.<br>
<br>
There&#39;s *always* going to be a penalty to pay if you don&#39;t use native buffers, just due to the nature of Java garbage collection, etc.<br>
<div class=""><br>
&gt; From the above experience, for the bandwidth of Bcast operation, I modified the test case to only allocate as much array as need for that Bcast and took the results. For a fairer comparison between MPJ Express and Open MPI&#39;s Java bindings I didn&#39;t use the MPI.new&lt;Type&gt;Buffer.<br>


<br>
</div>It would be interesting to see how using the native buffers compares, too -- i.e., are we correct in advocating for the use of native buffers?<br>
<div class=""><br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</div>Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/03/14384.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/03/14384.php</a><br>
</blockquote></div><br></div></div></div></div></div>

