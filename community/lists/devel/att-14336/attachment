<html><head><meta http-equiv="Content-Type" content="text/html charset=iso-8859-1"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">If you are going to use OMPI via JNI, then you have to load the OMPI library from within your code. This is a little tricky from Java as OMPI by default builds as a set of dynamic libraries, and each component is a dynamic library as well. The solution is to either build OMPI static, or to use lt_dladvise and friends to ensure the load paths are followed.<div><br></div><div><br><div><div>On Mar 12, 2014, at 3:40 AM, Bibrak Qamar &lt;<a href="mailto:bibrakc@gmail.com">bibrakc@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr">Hi all,<br><div class="gmail_quote"><div dir="ltr"><div><br>I am writing a new device for MPJ Express that uses a native MPI library for communication. Its based on JNI wrappers like the original mpiJava. The device works fine with MPICH3&nbsp; (and MVAPICH2.2). Here is the issue with loading Open MPI 1.7.4 from MPJ Express.<br>


<br></div><div>I have generated the following error message from a simple JNI to MPI application for clarity purposes and also to regenerate the error easily. I have attached the app for your consideration.<br></div><div>

<br><br>
</div><span><span style="font-family:courier new,monospace">[bibrak@localhost JNI_to_MPI]$ <b>mpirun -np 2 java -cp . -Djava.library.path=/home/</b><b>bibrak/work/JNI_to_MPI/ simpleJNI_MPI</b><br>[localhost.localdomain:29086] mca: base: component_find: unable to open /home/bibrak/work/installs/OpenMPI_installed/lib/openmpi/mca_shmem_mmap: /home/bibrak/work/installs/OpenMPI_installed/lib/openmpi/mca_shmem_mmap.so: undefined symbol: opal_show_help (ignored)<br>


[localhost.localdomain:29085] mca: base: component_find: unable to open /home/bibrak/work/installs/OpenMPI_installed/lib/openmpi/mca_shmem_mmap: /home/bibrak/work/installs/OpenMPI_installed/lib/openmpi/mca_shmem_mmap.so: undefined symbol: opal_show_help (ignored)<br>


[localhost.localdomain:29085] mca: base: component_find: unable to open /home/bibrak/work/installs/OpenMPI_installed/lib/openmpi/mca_shmem_posix: /home/bibrak/work/installs/OpenMPI_installed/lib/openmpi/mca_shmem_posix.so: undefined symbol: opal_shmem_base_framework (ignored)<br>


[localhost.localdomain:29086] mca: base: component_find: unable to open /home/bibrak/work/installs/OpenMPI_installed/lib/openmpi/mca_shmem_posix: /home/bibrak/work/installs/OpenMPI_installed/lib/openmpi/mca_shmem_posix.so: undefined symbol: opal_shmem_base_framework (ignored)<br>


[localhost.localdomain:29086] mca: base: component_find: unable to open /home/bibrak/work/installs/OpenMPI_installed/lib/openmpi/mca_shmem_sysv: /home/bibrak/work/installs/OpenMPI_installed/lib/openmpi/mca_shmem_sysv.so: undefined symbol: opal_show_help (ignored)<br>


--------------------------------------------------------------------------<br>It looks like opal_init failed for some reason; your parallel process is<br>likely to abort.&nbsp; There are many reasons that a parallel process can<br>


fail during opal_init; some of which are due to configuration or<br>environment problems.&nbsp; This failure appears to be an internal failure;<br>here's some additional information (which may only be relevant to an<br>Open MPI developer):<br>


<br>&nbsp; opal_shmem_base_select failed<br>&nbsp; --&gt; Returned value -1 instead of OPAL_SUCCESS<br>--------------------------------------------------------------------------<br>[localhost.localdomain:29085] mca: base: component_find: unable to open /home/bibrak/work/installs/OpenMPI_installed/lib/openmpi/mca_shmem_sysv: /home/bibrak/work/installs/OpenMPI_installed/lib/openmpi/mca_shmem_sysv.so: undefined symbol: opal_show_help (ignored)<br>


--------------------------------------------------------------------------<br>It looks like orte_init failed for some reason; your parallel process is<br>likely to abort.&nbsp; There are many reasons that a parallel process can<br>


fail during orte_init; some of which are due to configuration or<br>environment problems.&nbsp; This failure appears to be an internal failure;<br>here's some additional information (which may only be relevant to an<br>Open MPI developer):<br>


<br>&nbsp; opal_init failed<br>&nbsp; --&gt; Returned value Error (-1) instead of ORTE_SUCCESS<br>--------------------------------------------------------------------------<br>--------------------------------------------------------------------------<br>


It looks like MPI_INIT failed for some reason; your parallel process is<br>likely to abort.&nbsp; There are many reasons that a parallel process can<br>fail during MPI_INIT; some of which are due to configuration or environment<br>


problems.&nbsp; This failure appears to be an internal failure; here's some<br>additional information (which may only be relevant to an Open MPI<br>developer):<br><br>&nbsp; ompi_mpi_init: ompi_rte_init failed<br>&nbsp; --&gt; Returned "Error" (-1) instead of "Success" (0)<br>


--------------------------------------------------------------------------<br>*** An error occurred in MPI_Init<br>*** on a NULL communicator<br>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>


***&nbsp;&nbsp;&nbsp; and potentially your MPI job)<br>[localhost.localdomain:29086] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!<br>


--------------------------------------------------------------------------<br>It looks like opal_init failed for some reason; your parallel process is<br>likely to abort.&nbsp; There are many reasons that a parallel process can<br>


fail during opal_init; some of which are due to configuration or<br>environment problems.&nbsp; This failure appears to be an internal failure;<br>here's some additional information (which may only be relevant to an<br>Open MPI developer):<br>


<br>&nbsp; opal_shmem_base_select failed<br>&nbsp; --&gt; Returned value -1 instead of OPAL_SUCCESS<br>--------------------------------------------------------------------------<br>-------------------------------------------------------<br>


Primary job&nbsp; terminated normally, but 1 process returned<br>a non-zero exit code.. Per user-direction, the job has been aborted.<br>-------------------------------------------------------<br>--------------------------------------------------------------------------<br>


mpirun detected that one or more processes exited with non-zero status, thus causing<br>the job to be terminated. The first process to do so was:<br><br>&nbsp; Process name: [[41236,1],1]<br>&nbsp; Exit code:&nbsp;&nbsp;&nbsp; 1<br>--------------------------------------------------------------------------<br>


</span></span><br><div><div><br></div><div>This is a thread that I found where the Open MPI developers were having issues while integrating mpiJava into their stack.<br></div><div><br><a href="http://mail-archives.apache.org/mod_mbox/hadoop-common-dev/201202.mbox/%3C5EA543BD-A12E-4729-B66A-C746BC789EC3@open-mpi.org%3E" target="_blank">http://mail-archives.apache.org/mod_mbox/hadoop-common-dev/201202.mbox/%3C5EA543BD-A12E-4729-B66A-C746BC789EC3@open-mpi.org%3E</a><br>


<br></div><div> I don't have better understanding of the internals of Open MPI, so my question is how to use Open MPI using JNI wrappers? Any guidelines in this regard?<br>
</div><div><div><div dir="ltr"><span><br>Thanks<br>Bibrak<span class="HOEnZb"><font color="#888888"><br><br></font></span></span></div>
</div>
</div></div></div>
</div></div>
<span>&lt;JNI_to_MPI.tar.gz&gt;</span>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/devel<br>Link to this post: http://www.open-mpi.org/community/lists/devel/2014/03/14335.php</blockquote></div><br></div></body></html>
