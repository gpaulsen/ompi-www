<html><head><meta http-equiv="Content-Type" content="text/html charset=us-ascii"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">This is the email thread which sparked the problem:<div><br></div><div><a href="http://www.open-mpi.org/community/lists/devel/2014/07/15329.php">http://www.open-mpi.org/community/lists/devel/2014/07/15329.php</a></div><div><br></div><div>I actually tried to apply the original CMR and couldn't get it to work in the 1.8 branch - just kept having problems, so I pushed it off to 1.8.3. I'm leery to accept either of the current CMRs for two reasons: (a) none of the preceding changes is in the 1.8 series yet, and (b) it doesn't sound like we still have a complete solution.</div><div><br></div><div>Anyway, I just wanted to point to the original problem that was trying to be addressed.</div><div><br></div><div><br><div><div>On Aug 28, 2014, at 10:01 PM, Gilles Gouaillardet &lt;<a href="mailto:gilles.gouaillardet@iferc.org">gilles.gouaillardet@iferc.org</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite">Howard and Edgar,<br><br>i fixed a few bugs (r32639 and r32642)<br><br>the bug is trivial to reproduce with any mpi hello world program<br><br>mpirun -np 2 --mca btl openib,self hello_world<br><br>after setting the mca param in the $HOME/.openmpi/mca-params.conf<br><br>$ cat ~/.openmpi/mca-params.conf<br>btl_openib_receive_queues = S,12288,128,64,32:S,65536,128,64,3<br><br>good news is the program does not crash with a glory SIGSEGV any more<br>bad news is the program will (nicely) abort for an incorrect reason :<br><br>--------------------------------------------------------------------------<br>The Open MPI receive queue configuration for the OpenFabrics devices<br>on two nodes are incompatible, meaning that MPI processes on two<br>specific nodes were unable to communicate with each other. &nbsp;This<br>generally happens when you are using OpenFabrics devices from<br>different vendors on the same network. &nbsp;You should be able to use the<br>mca_btl_openib_receive_queues MCA parameter to set a uniform receive<br>queue configuration for all the devices in the MPI job, and therefore<br>be able to run successfully.<br><br> &nbsp;Local host: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;node0<br> &nbsp;Local adapter: &nbsp;&nbsp;&nbsp;mlx4_0 (vendor 0x2c9, part ID 4099)<br> &nbsp;Local queues: &nbsp;&nbsp;&nbsp;&nbsp;S,12288,128,64,32:S,65536,128,64,3<br><br> &nbsp;Remote host: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;node0<br> &nbsp;Remote adapter: &nbsp;&nbsp;(vendor 0x2c9, part ID 4099)<br> &nbsp;Remote queues: &nbsp;&nbsp;<br>P,128,256,192,128:S,2048,1024,1008,64:S,12288,1024,1008,64:S,65536,1024,1008,64<br><br>the root cause is the remote host did not send its receive_queues to the<br>local host<br>(and hence the local host believes the remote hosts uses the default value)<br><br>the logic was revamped vs v1.8, that is why v1.8 does not have such issue.<br><br>i am still thinking what should be the right fix :<br>- one option is to send the receive queues<br>- an other option would be to differenciate value overrided in<br>mca-params.conf (should be always ok) of value overrided in the .ini<br> &nbsp;(might want to double check local and remote values match)<br><br>Cheers,<br><br>Gilles<br><br>On 2014/08/29 7:02, Pritchard Jr., Howard wrote:<br><blockquote type="cite">Hi Edgar,<br><br>Could you send me your conf file? &nbsp;I'll try to reproduce it.<br><br>Maybe run with --mca btl_base_verbose 20 or something to<br>see what the code that is parsing this field in the conf file<br>is finding.<br><br><br>Howard<br><br><br>-----Original Message-----<br>From: devel [<a href="mailto:devel-bounces@open-mpi.org">mailto:devel-bounces@open-mpi.org</a>] On Behalf Of Edgar Gabriel<br>Sent: Thursday, August 28, 2014 3:40 PM<br>To: Open MPI Developers<br>Subject: Re: [OMPI devel] segfault in openib component on trunk<br><br>to add another piece of information that I just found, the segfault only occurs if I have a particular mca parameter set in my mca-params.conf file, namely<br><br>btl_openib_receive_queues = S,12288,128,64,32:S,65536,128,64,3<br><br>Has the syntax for this parameter changed, or should/can I get rid of it?<br><br>Thanks<br>Edgar<br><br>On 08/28/2014 04:19 PM, Edgar Gabriel wrote:<br><blockquote type="cite">we are having recently problems running trunk with openib component <br>enabled on one of our clusters. The problem occurs right in the <br>initialization part, here is the stack right before the segfault:<br><br>---snip---<br>(gdb) where<br>#0 &nbsp;mca_btl_openib_tune_endpoint (openib_btl=0x762a40,<br>endpoint=0x7d9660) at btl_openib.c:470<br>#1 &nbsp;0x00007f1062f105c4 in mca_btl_openib_add_procs (btl=0x762a40, <br>nprocs=2, procs=0x759be0, peers=0x762440, reachable=0x7fff22dd16f0) at<br>btl_openib.c:1093<br>#2 &nbsp;0x00007f106316102c in mca_bml_r2_add_procs (nprocs=2, <br>procs=0x759be0, reachable=0x7fff22dd16f0) at bml_r2.c:201<br>#3 &nbsp;0x00007f10615c0dd5 in mca_pml_ob1_add_procs (procs=0x70dc00,<br>nprocs=2) at pml_ob1.c:334<br>#4 &nbsp;0x00007f106823ed84 in ompi_mpi_init (argc=1, argv=0x7fff22dd1da8, <br>requested=0, provided=0x7fff22dd184c) at runtime/ompi_mpi_init.c:790<br>#5 &nbsp;0x00007f1068273a2c in MPI_Init (argc=0x7fff22dd188c,<br>argv=0x7fff22dd1880) at init.c:84<br>#6 &nbsp;0x00000000004008e7 in main (argc=1, argv=0x7fff22dd1da8) at<br>hello_world.c:13<br>---snip---<br><br><br>in line 538 of the file containing the mca_btl_openib_tune_endpoint <br>routine, the strcmp operation fails, because &nbsp;recv_qps is a NULL pointer.<br><br><br>---snip---<br><br>if(0 != strcmp(mca_btl_openib_component.receive_queues, recv_qps)) {<br><br>---snip---<br><br>Does anybody have an idea on what might be going wrong and how to <br>resolve it? Just to confirm, everything works perfectly with the 1.8 <br>series on that very same &nbsp;cluster<br><br>Thanks<br>Edgar<br>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/devel<br>Link to this post:<br>http://www.open-mpi.org/community/lists/devel/2014/08/15746.php<br></blockquote>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/devel<br>Link to this post: http://www.open-mpi.org/community/lists/devel/2014/08/15747.php<br>_______________________________________________<br>devel mailing list<br>devel@open-mpi.org<br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/devel<br>Link to this post: http://www.open-mpi.org/community/lists/devel/2014/08/15748.php<br></blockquote><br>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/devel<br>Link to this post: http://www.open-mpi.org/community/lists/devel/2014/08/15749.php<br></blockquote></div><br></div></body></html>
