<div dir="ltr">Hi, <div><br><div style>It looks like this fix resolved our problems as well.</div><div style><br></div><div style>Thanks,</div><div style>Elena</div></div></div><div class="gmail_extra"><br><br><div class="gmail_quote">
On Fri, May 30, 2014 at 4:58 PM, Rolf vandeVaart <span dir="ltr">&lt;<a href="mailto:rvandevaart@nvidia.com" target="_blank">rvandevaart@nvidia.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
This fixed all of my issues.  Thanks.  I will add that comment to ticket also.<br>
<div class=""><br>
&gt;-----Original Message-----<br>
&gt;From: devel [mailto:<a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a>] On Behalf Of George<br>
&gt;Bosilca<br>
&gt;Sent: Thursday, May 29, 2014 5:58 PM<br>
&gt;To: Open MPI Developers<br>
&gt;Subject: Re: [OMPI devel] regression with derived datatypes<br>
&gt;<br>
</div><div><div class="h5">&gt;r31904 should fix this issue. Please test it thoughtfully and report all issues.<br>
&gt;<br>
&gt;  George.<br>
&gt;<br>
&gt;<br>
&gt;On Fri, May 9, 2014 at 6:56 AM, Gilles Gouaillardet<br>
&gt;&lt;<a href="mailto:gilles.gouaillardet@iferc.org">gilles.gouaillardet@iferc.org</a>&gt; wrote:<br>
&gt;&gt; i opened #4610 <a href="https://svn.open-mpi.org/trac/ompi/ticket/4610" target="_blank">https://svn.open-mpi.org/trac/ompi/ticket/4610</a><br>
&gt;&gt; and attached a patch for the v1.8 branch<br>
&gt;&gt;<br>
&gt;&gt; i ran several tests from the intel_tests test suite and did not<br>
&gt;&gt; observe any regression.<br>
&gt;&gt;<br>
&gt;&gt; please note there are still issues when running with --mca btl<br>
&gt;&gt; scif,vader,self<br>
&gt;&gt;<br>
&gt;&gt; this might be an other issue, i will investigate more next week<br>
&gt;&gt;<br>
&gt;&gt; Gilles<br>
&gt;&gt;<br>
&gt;&gt; On 2014/05/09 18:08, Gilles Gouaillardet wrote:<br>
&gt;&gt;&gt; I ran some more investigations with --mca btl scif,self<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; i found that the previous patch i posted was complete crap and i<br>
&gt;&gt;&gt; apologize for it.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; on a brighter side, and imho, the issue only occurs if fragments are<br>
&gt;&gt;&gt; received (and then processed) out of order.<br>
&gt;&gt;&gt; /* i did not observe this with the tcp btl, but i always see that<br>
&gt;&gt;&gt; with the scif btl, i guess this can be observed too with openib+RDMA<br>
&gt;&gt;&gt; */<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; in this case only, opal_convertor_generic_simple_position(...) is<br>
&gt;&gt;&gt; invoked and does not set the pConvertor-&gt;pStack as expected by r31496<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; i will run some more tests from now<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; Gilles<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On 2014/05/08 2:23, George Bosilca wrote:<br>
&gt;&gt;&gt;&gt; Strange. The outcome and the timing of this issue seems to highlight a link<br>
&gt;with the other datatype-related issue you reported earlier, and as suggested<br>
&gt;by Ralph with Gilles scif+vader issue.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Generally speaking, the mechanism used to split the data in the case of<br>
&gt;multiple BTLs, is identical to the one used to split the data in fragments. So, if<br>
&gt;the culprit is in the splitting logic, one might see some weirdness as soon as<br>
&gt;we force the exclusive usage of the send protocol, with an unconventional<br>
&gt;fragment size.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; In other words using the following flags “—mca btl tcp,self —mca<br>
&gt;btl_tcp_flags 3 —mca btl_tcp_rndv_eager_limit 23 —mca btl_tcp_eager_limit<br>
&gt;23 —mca btl_tcp_max_send_size 23” should always transfer wrong data,<br>
&gt;even when only one single BTL is in play.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;   George.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; On May 7, 2014, at 13:11 , Rolf vandeVaart &lt;<a href="mailto:rvandevaart@nvidia.com">rvandevaart@nvidia.com</a>&gt;<br>
&gt;wrote:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; OK.  So, I investigated a little more.  I only see the issue when I am<br>
&gt;running with multiple ports enabled such that I have two openib BTLs<br>
&gt;instantiated.  In addition, large message RDMA has to be enabled.  If those<br>
&gt;conditions are not met, then I do not see the problem.  For example:<br>
&gt;&gt;&gt;&gt;&gt; FAILS:<br>
&gt;&gt;&gt;&gt;&gt; Ø  mpirun –np 2 –host host1,host2 –mca btl_openib_if_include<br>
&gt;&gt;&gt;&gt;&gt; mlx5_0:1,mlx5_0:2 –mca btl_openib_flags 3 MPI_Isend_ator_c<br>
&gt;&gt;&gt;&gt;&gt; PASS:<br>
&gt;&gt;&gt;&gt;&gt; Ø  mpirun –np 2 –host host1,host2 –mca btl_openib_if_include<br>
&gt;&gt;&gt;&gt;&gt; mlx5_0:1 –mca btl_openib_flags 3 MPI_Isend_ator_c Ø  mpirun –np 2<br>
&gt;&gt;&gt;&gt;&gt; –host host1,host2 –mca btl_openib_if_include_mlx5:0:1,mlx5_0:2 –mca<br>
&gt;&gt;&gt;&gt;&gt; btl_openib_flags 1 MPI_Isend_ator_c<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;&gt; So we must have some type of issue when we break up the message<br>
&gt;between the two openib BTLs.  Maybe someone else can confirm my<br>
&gt;observations?<br>
&gt;&gt;&gt;&gt;&gt; I was testing against the latest trunk.<br>
&gt;&gt;&gt;&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; devel mailing list<br>
&gt;&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt;&gt; Link to this post:<br>
&gt;&gt; <a href="http://www.open-mpi.org/community/lists/devel/2014/05/14766.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/05/14766.php</a><br>
&gt;_______________________________________________<br>
&gt;devel mailing list<br>
&gt;<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt;Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt;Link to this post: <a href="http://www.open-" target="_blank">http://www.open-</a><br>
&gt;<a href="http://mpi.org/community/lists/devel/2014/05/14910.php" target="_blank">mpi.org/community/lists/devel/2014/05/14910.php</a><br>
<br>
</div></div>-----------------------------------------------------------------------------------<br>
<div class="">This email message is for the sole use of the intended recipient(s) and may contain<br>
confidential information.  Any unauthorized review, use, disclosure or distribution<br>
is prohibited.  If you are not the intended recipient, please contact the sender by<br>
reply email and destroy all copies of the original message.<br>
</div>-----------------------------------------------------------------------------------<br>
<div class="">_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</div>Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/05/14912.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/05/14912.php</a></blockquote></div><br></div>

