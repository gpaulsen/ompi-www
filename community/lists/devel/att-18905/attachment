<div dir="ltr">Let me see if I can get access to a Power 7 machine internally, and I will try to replicate.</div><div class="gmail_extra"><br><div class="gmail_quote">On Tue, May 3, 2016 at 9:54 AM, Paul Hargrove <span dir="ltr">&lt;<a href="mailto:phhargrove@lbl.gov" target="_blank">phhargrove@lbl.gov</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Josh,<div><br></div><div>This is a Power7 (big-endian) system &quot;BlueBiou&quot; at Rice.</div><div><br></div><div>Xlc is 13.1 with no updates:</div><div><div><font face="monospace, monospace">$ xlc -qversion</font></div><div><font face="monospace, monospace">IBM XL C/C++ for Linux, V13.1</font></div><div><font face="monospace, monospace">Version: 13.01.0000.0000</font></div></div><div><br></div><div>I don&#39;t have privileged to install updates.</div><div><br></div><div>FWIW: I too can build with xlc for little-endian Power8.</div><div><div><font face="monospace, monospace">$ /opt/ibm/xlC/13.1.2/bin/xlc -qversion</font></div><div><font face="monospace, monospace">IBM XL C/C++ for Linux, V13.1.2 (5725-C73, 5765-J08)</font></div><div><font face="monospace, monospace">Version: 13.01.0002.0000</font></div></div><div><br></div><div>However, it is worth noting that my understanding from IBM docs is that the xlc for ppc64el is a VERY different compiler.</div><div>Specifically it uses the Clang front-end rather than IBM&#39;s own.</div><div><br></div><div>-Paul</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote"><div><div class="h5">On Tue, May 3, 2016 at 7:47 AM, Josh Hursey <span dir="ltr">&lt;<a href="mailto:jjhursey@open-mpi.org" target="_blank">jjhursey@open-mpi.org</a>&gt;</span> wrote:<br></div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div class="h5"><div dir="ltr">Paul,<div><br></div><div>What generation of the power arch are you using?</div><div><br></div><div>We have successfully built (a few weeks ago) with the xlc compiler 13.1.3 on a Power 8 (pcc64le). It might be related the big vs. little endian, but I wonder if it is something that was fixed in a point release of the xlc compiler. Are you able to upgrade to the latest compiler revision and try again?</div><div><br></div><div>(Internally, we are working on getting MTT nightly testing with the xl compilers - currently we are just reporting gcc results)</div><div><br></div><div>Thanks,</div><div>Josh</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote"><div><div>On Tue, May 3, 2016 at 3:11 AM, Paul Hargrove <span dir="ltr">&lt;<a href="mailto:phhargrove@lbl.gov" target="_blank">phhargrove@lbl.gov</a>&gt;</span> wrote:<br></div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div><div dir="ltr">For possible inclusion in README:<div>xlc-13.1.0 on Linux dies compiling the embedded hwloc in this rc (details below).</div><div>The same is true of the released hwloc 1.11.3</div><div><br></div><div>-Paul</div><div><br></div><div><div><font face="monospace, monospace">libtool: compile:  xlc -DHAVE_CONFIG_H -I. -I/gpfs-biou/phh1/OMPI/openmpi-2.0.0rc2-linux-ppc64-xlc-13.1/openmpi-2.0.0rc2/opal/mca/hwloc/hwloc1112/hwloc/src -I../../../../../../opal/include -I../../../../../../ompi/include -I../../../../../../oshmem/include -I../../../../../../opal/mca/hwloc/hwloc1112/hwloc/include/private/autogen -I../../../../../../opal/mca/hwloc/hwloc1112/hwloc/include/hwloc/autogen -I../../../../../../ompi/mpiext/cuda/c -I/gpfs-biou/phh1/OMPI/openmpi-2.0.0rc2-linux-ppc64-xlc-13.1/BLD/opal/mca/hwloc/hwloc1112/hwloc/include -I/gpfs-biou/phh1/OMPI/openmpi-2.0.0rc2-linux-ppc64-xlc-13.1/openmpi-2.0.0rc2/opal/mca/hwloc/hwloc1112/hwloc/include -DHWLOC_INSIDE_LIBHWLOC -DHWLOC_PLUGINS_PATH=\&quot;/gpfs-biou/phh1/OMPI/openmpi-2.0.0rc2-linux-ppc64-xlc-13.1/INST/lib/hwloc\&quot; -I/gpfs-biou/phh1/OMPI/openmpi-2.0.0rc2-linux-ppc64-xlc-13.1/openmpi-2.0.0rc2 -I../../../../../.. -I/gpfs-biou/phh1/OMPI/openmpi-2.0.0rc2-linux-ppc64-xlc-13.1/openmpi-2.0.0rc2/opal/include -I/gpfs-biou/phh1/OMPI/openmpi-2.0.0rc2-linux-ppc64-xlc-13.1/openmpi-2.0.0rc2/orte/include -I../../../../../../orte/include -I/gpfs-biou/phh1/OMPI/openmpi-2.0.0rc2-linux-ppc64-xlc-13.1/openmpi-2.0.0rc2/ompi/include -I/gpfs-biou/phh1/OMPI/openmpi-2.0.0rc2-linux-ppc64-xlc-13.1/openmpi-2.0.0rc2/oshmem/include -D_REENTRANT -I/gpfs-biou/phh1/OMPI/openmpi-2.0.0rc2-linux-ppc64-xlc-13.1/openmpi-2.0.0rc2/opal/mca/hwloc/hwloc1112/hwloc/include -I/gpfs-biou/phh1/OMPI/openmpi-2.0.0rc2-linux-ppc64-xlc-13.1/BLD/opal/mca/hwloc/hwloc1112/hwloc/include -I/gpfs-biou/phh1/OMPI/openmpi-2.0.0rc2-linux-ppc64-xlc-13.1/openmpi-2.0.0rc2/opal/mca/event/libevent2022/libevent -I/gpfs-biou/phh1/OMPI/openmpi-2.0.0rc2-linux-ppc64-xlc-13.1/openmpi-2.0.0rc2/opal/mca/event/libevent2022/libevent/include -I/gpfs-biou/phh1/OMPI/openmpi-2.0.0rc2-linux-ppc64-xlc-13.1/BLD/opal/mca/event/libevent2022/libevent/include -q64 -g -c /gpfs-biou/phh1/OMPI/openmpi-2.0.0rc2-linux-ppc64-xlc-13.1/openmpi-2.0.0rc2/opal/mca/hwloc/hwloc1112/hwloc/src/topology-xml-nolibxml.c -Wp,-qmakedep=gcc,-MF.deps/topology-xml-nolibxml.TPlo  -qpic -DPIC -o .libs/topology-xml-nolibxml.o</font></div><div><font face="monospace, monospace">/opt/ibm/xlC/13.1.0/bin/.orig/xlc: 1501-224 (S) fatal error in /opt/ibm/xlC/13.1.0/exe/xlcentry: signal 11 received</font></div><div><font face="monospace, monospace">make[4]: *** [topology-xml-nolibxml.lo] Error 1</font></div><span><font color="#888888"><div><br></div><div><br></div>-- <br><div><div dir="ltr"><div><font face="courier new, monospace"><div>Paul H. Hargrove                          <a href="mailto:PHHargrove@lbl.gov" target="_blank">PHHargrove@lbl.gov</a></div><div>Computer Languages &amp; Systems Software (CLaSS) Group</div><div>Computer Science Department               Tel: <a href="tel:%2B1-510-495-2352" value="+15104952352" target="_blank">+1-510-495-2352</a></div><div>Lawrence Berkeley National Laboratory     Fax: <a href="tel:%2B1-510-486-6900" value="+15104866900" target="_blank">+1-510-486-6900</a></div></font></div></div></div>
</font></span></div></div>
<br></div></div>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/05/18897.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/05/18897.php</a><br></blockquote></div><br></div>
<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br></div></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/05/18902.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/05/18902.php</a><br></blockquote></div><span class=""><br><br clear="all"><div><br></div>-- <br><div><div dir="ltr"><div><font face="courier new, monospace"><div>Paul H. Hargrove                          <a href="mailto:PHHargrove@lbl.gov" target="_blank">PHHargrove@lbl.gov</a></div><div>Computer Languages &amp; Systems Software (CLaSS) Group</div><div>Computer Science Department               Tel: <a href="tel:%2B1-510-495-2352" value="+15104952352" target="_blank">+1-510-495-2352</a></div><div>Lawrence Berkeley National Laboratory     Fax: <a href="tel:%2B1-510-486-6900" value="+15104866900" target="_blank">+1-510-486-6900</a></div></font></div></div></div>
</span></div>
<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/05/18904.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/05/18904.php</a><br></blockquote></div><br></div>

