Dave,<br><br>These settings tell ompi to use native infiniband on the ib qdr port and tcpo/ip on the other port.<br><br>From the faq, roce is implemented in the openib btl<br>http://www.open-mpi.org/faq/?category=openfabrics#ompi-over-roce<br><br>Did you use <br>--mca btl_openib_cpc_include rdmacm<br>in  your first tests ?<br><br>I had some second thougths about the bandwidth values, and imho they should be 327680 and 81920 because of the 8/10 encoding<br>(And that being said, that should not change the measured performance)<br><br>Also, could you try again by forcing the same btl_tcp_latency and btl_openib_latency ?<br><br>Cheers,<br><br>Gilles<br><br>Dave Turner &lt;drdaveturner@gmail.com&gt; wrote:<br><div dir="ltr">George,<div><br></div><div>     I can check with my guys on Monday but I think the bandwidth parameters </div><div>are the defaults.  I did alter these to 40960 and 10240 as someone else </div><div>suggested to me.  The attached graph shows the base red line, along with</div><div>the manual balanced blue line and auto balanced green line (0&#39;s for both).</div><div>This shift lower suggests to me that the higher TCP latency is being pulled in.</div><div>I&#39;m not sure why the curves are shifted right.</div><div><br></div><div>                        Dave</div></div><div class="gmail_extra"><br><div class="gmail_quote">On Fri, Feb 6, 2015 at 5:32 PM, George Bosilca <span dir="ltr">&lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank">bosilca@icl.utk.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div>Dave,</div><div><br></div><div>Based on your ompi_info.all the following bandwidth are reported on your system:</div><div> <br></div><div>                MCA btl: parameter &quot;btl_openib_bandwidth&quot; (current value: &quot;4&quot;, data source: default, level: 5 tuner/detail, type: unsigned)</div><div>                          Approximate maximum bandwidth of interconnect (0 = auto-detect value at run-time [not supported in all BTL modules], &gt;= 1 = bandwidth in Mbps)</div><div><br></div><div><div>                 MCA btl: parameter &quot;btl_tcp_bandwidth&quot; (current value: &quot;100&quot;, data source: default, level: 5 tuner/detail, type: unsigned)</div><div>                          Approximate maximum bandwidth of interconnect (0 = auto-detect value at run-time [not supported in all BTL modules], &gt;= 1 = bandwidth in Mbps)</div></div><div><br></div><div>This basically states that on your system the default values for these parameters are wrong, your TCP network being much faster than the IB. This explains the somewhat unexpected decision of OMPI.</div><div><br></div><div>As a possible solution I suggest you set these bandwidth values to something more meaningful (directly in your configuration file). As an example,</div><div><br></div><div>btl_openib_bandwidth = 40000<br></div><div>btl_tcp_bandwidth = 10000<br></div><div><br></div><div>make more sense based on your HPC system description.</div><div><br></div><div>  George.</div><div><br></div><div><br></div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote"><div><div class="h5">On Fri, Feb 6, 2015 at 5:37 PM, Dave Turner <span dir="ltr">&lt;<a href="mailto:drdaveturner@gmail.com" target="_blank">drdaveturner@gmail.com</a>&gt;</span> wrote:<br></div></div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div class="h5"><div dir="ltr"><div><br></div>     We have nodes in our HPC system that have 2 NIC&#39;s, <div>one being QDR IB and the second being a slower 10 Gbps card</div><div>configured for both RoCE and TCP.  Aggregate bandwidth </div><div>tests with 20 cores on one node yelling at 20 cores on a second</div><div>node (attached roce.ib.aggregate.pdf) show that without tuning</div><div>the slower RoCE interface is being used for small messages</div><div>then QDR IB is used for larger messages (red line).  Tuning</div><div>the tcp_exclusivity to 1024 to match the openib_exclusivity </div><div>adds another 20 Gbps of bidirectional bandwidth to the high end (green line),</div><div>and I&#39;m guessing this is TCP traffic and not RoCE.</div><div><br></div><div>     So by default the slower interface is being chosen on the low end, and</div><div>I don&#39;t think there are tunable parameters to allow me to choose the </div><div>QDR interface as the default.  Going forward we&#39;ll probably just disable </div><div>RoCE on these nodes and go with QDR IB plus 10 Gbps TCP for large messages.   </div><div><br></div><div>      However, I do think these issues will come up more in the future.</div><div>With the low latency of RoCE matching IB, there are more opportunities</div><div>to do channel bonding or allowing multiple interfaces for aggregate traffic</div><div>for even smaller message sizes. </div><span><font color="#888888"><div><br></div><div>                Dave Turner</div><div><div><br></div>-- <br><div><div dir="ltr">Work:     <a href="mailto:DaveTurner@ksu.edu" target="_blank">DaveTurner@ksu.edu</a>     <a href="tel:%28785%29%20532-7791" value="+17855327791" target="_blank">(785) 532-7791</a><div>             118 Nichols Hall, Manhattan KS  66502<br>Home:    <a href="mailto:DrDaveTurner@gmail.com" target="_blank">DrDaveTurner@gmail.com</a><br>              cell: <a href="tel:%28785%29%20770-5929" value="+17857705929" target="_blank">(785) 770-5929</a><br></div></div></div>
</div></font></span></div>
<br></div></div>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/02/16951.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/02/16951.php</a><br></blockquote></div><br></div>
</blockquote></div><br><br clear="all"><div><br></div>-- <br><div class="gmail_signature"><div dir="ltr">Work:     <a href="mailto:DaveTurner@ksu.edu" target="_blank">DaveTurner@ksu.edu</a>     (785) 532-7791<div>             118 Nichols Hall, Manhattan KS  66502<br>Home:    <a href="mailto:DrDaveTurner@gmail.com" target="_blank">DrDaveTurner@gmail.com</a><br>              cell: (785) 770-5929<br></div></div></div>
</div>
