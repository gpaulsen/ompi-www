<div dir="ltr">Ouch - this is on current master HEAD? I&#39;m on travel right now, but I&#39;ll be back Fri evening and can look at it this weekend. Probably something silly that needs to be fixed.<div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Thu, Sep 17, 2015 at 11:30 AM, Mark Santcroos <span dir="ltr">&lt;<a href="mailto:mark.santcroos@rutgers.edu" target="_blank">mark.santcroos@rutgers.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi (Ralph),<br>
<br>
Over the last months I have been focussing on exec throughput, and not so much on the application payload (read: mainly using /bin/sleep ;-)<br>
As things are stabilising now, I returned my attention to &quot;real&quot; applications.<br>
To discover that launching MPI applications (build with the same Open MPI version) within a DVM doesn&#39;t work anymore (see error below).<br>
<br>
I&#39;ve been doing a binary search, but that turned out to be not so trivial because of other problems in the window of the change.<br>
So far I&#39;ve narrowed it down to:<br>
<br>
64ec498 - mar 5 - works on my laptop (but not on the Crays)<br>
b67b361 - mar 28 - works once per DVM launch on my laptop, but consecutive orte-submits get a connect error<br>
b209c9e - March 30 - same MPI_Init issue as HEAD<br>
<br>
Going further into mid-March I run into build issues with verb, runtime issues with default binding complaining about missing libnumactl, runtime tcp oob errors, etc.<br>
So I don&#39;t know whether the binary search will yield much more than I was able to dig up now.<br>
<br>
What can I do to get closer to debugging the actual issue?<br>
<br>
Thanks!<br>
<br>
Mark<br>
<br>
<br>
OMPI_PREFIX=/Users/mark/proj/openmpi/installed/HEAD<br>
OMPI_MCA_orte_hnp_uri=723386368.0;usock;tcp://<a href="http://192.168.0.103:56672" rel="noreferrer" target="_blank">192.168.0.103:56672</a><br>
OMPI_MCA_ess=tool<br>
[netbook:70703] Job [11038,3] has launched<br>
--------------------------------------------------------------------------<br>
It looks like MPI_INIT failed for some reason; your parallel process is<br>
likely to abort.  There are many reasons that a parallel process can<br>
fail during MPI_INIT; some of which are due to configuration or environment<br>
problems.  This failure appears to be an internal failure; here&#39;s some<br>
additional information (which may only be relevant to an Open MPI<br>
developer):<br>
<br>
  ompi_mpi_init: ompi_rte_init failed<br>
  --&gt; Returned &quot;(null)&quot; (-43) instead of &quot;Success&quot; (0)<br>
--------------------------------------------------------------------------<br>
*** An error occurred in MPI_Init<br>
*** on a NULL communicator<br>
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<br>
***    and potentially your MPI job)<br>
[netbook:70704] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!<br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/09/18064.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/09/18064.php</a><br>
</blockquote></div><br></div>

