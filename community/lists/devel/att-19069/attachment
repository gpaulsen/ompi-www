<div dir="ltr"><br><div class="gmail_extra"><br><div class="gmail_quote">On Fri, Jun 3, 2016 at 11:10 PM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">That&#39;s disappointing / puzzling.<br>
<br>
Threads 4 and 5 look like they&#39;re in the PMIX / ORTE progress threads, respectively.<br>
<br>
But I don&#39;tt see any obvious signs of what thread 1, 2, 3 are for.  Huh.<br>
<br>
When is this hang happening -- during init?  Middle of the program?  During finalize?<br></blockquote><div><br></div><div>After finalize. As I said in my original email I se all the output the application is generating, and all processes (which are local as this happens on my laptop) are in zombie mode (Z+). This basically means whoever was supposed to get the SIGCHLD, didn&#39;t do it&#39;s job of cleaning them up.</div><div><br></div><div>  George.</div><div><br></div><div> </div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div><div class="h5"><br>
<br>
&gt; On Jun 2, 2016, at 6:00 PM, George Bosilca &lt;<a href="mailto:bosilca@icl.utk.edu">bosilca@icl.utk.edu</a>&gt; wrote:<br>
&gt;<br>
&gt; Sure, but they mostly look similar.<br>
&gt;<br>
&gt;   George.<br>
&gt;<br>
&gt;<br>
&gt; (lldb) thread list<br>
&gt; Process 76811 stopped<br>
&gt;   thread #1: tid = 0x272b40e, 0x00007fff93306de6 libsystem_kernel.dylib`__psynch_mutexwait + 10, queue = &#39;com.apple.main-thread&#39;, stop reason = signal SIGSTOP<br>
&gt;   thread #2: tid = 0x272b40f, 0x00007fff93306de6 libsystem_kernel.dylib`__psynch_mutexwait + 10<br>
&gt;   thread #3: tid = 0x272b410, 0x00007fff93306de6 libsystem_kernel.dylib`__psynch_mutexwait + 10<br>
&gt;   thread #4: tid = 0x272b411, 0x00007fff9330707a libsystem_kernel.dylib`__select + 10<br>
&gt; * thread #5: tid = 0x272b412, 0x00007fff9330707a libsystem_kernel.dylib`__select + 10<br>
&gt; (lldb)<br>
&gt;<br>
&gt;<br>
&gt; (lldb) thread select 1<br>
&gt; * thread #1: tid = 0x272b40e, 0x00007fff93306de6 libsystem_kernel.dylib`__psynch_mutexwait + 10, queue = &#39;com.apple.main-thread&#39;, stop reason = signal SIGSTOP<br>
&gt;     frame #0: 0x00007fff93306de6 libsystem_kernel.dylib`__psynch_mutexwait + 10<br>
&gt; libsystem_kernel.dylib`__psynch_mutexwait:<br>
&gt; -&gt;  0x7fff93306de6 &lt;+10&gt;: jae    0x7fff93306df0            ; &lt;+20&gt;<br>
&gt;     0x7fff93306de8 &lt;+12&gt;: movq   %rax, %rdi<br>
&gt;     0x7fff93306deb &lt;+15&gt;: jmp    0x7fff933017cd            ; cerror_nocancel<br>
&gt;     0x7fff93306df0 &lt;+20&gt;: retq<br>
&gt; (lldb) bt<br>
&gt; * thread #1: tid = 0x272b40e, 0x00007fff93306de6 libsystem_kernel.dylib`__psynch_mutexwait + 10, queue = &#39;com.apple.main-thread&#39;, stop reason = signal SIGSTOP<br>
&gt;   * frame #0: 0x00007fff93306de6 libsystem_kernel.dylib`__psynch_mutexwait + 10<br>
&gt;     frame #1: 0x00007fff9a000e4a libsystem_pthread.dylib`_pthread_mutex_lock_wait + 89<br>
&gt;     frame #2: 0x00007fff99ffe5f5 libsystem_pthread.dylib`_pthread_mutex_lock_slow + 300<br>
&gt;     frame #3: 0x00007fff8c2a00f8 libdyld.dylib`dyldGlobalLockAcquire() + 16<br>
&gt;     frame #4: 0x00007fff6ca8e177 dyld`ImageLoaderMachOCompressed::doBindFastLazySymbol(unsigned int, ImageLoader::LinkContext const&amp;, void (*)(), void (*)()) + 55<br>
&gt;     frame #5: 0x00007fff6ca78063 dyld`dyld::fastBindLazySymbol(ImageLoader**, unsigned long) + 90<br>
&gt;     frame #6: 0x00007fff8c2a0262 libdyld.dylib`dyld_stub_binder + 282<br>
&gt;     frame #7: 0x000000010a5b29b0 libopen-pal.0.dylib`obj_order_type + 3776<br>
&gt;<br>
&gt;<br>
&gt; (lldb) thread select 2<br>
&gt; * thread #2: tid = 0x272b40f, 0x00007fff93306de6 libsystem_kernel.dylib`__psynch_mutexwait + 10<br>
&gt;     frame #0: 0x00007fff93306de6 libsystem_kernel.dylib`__psynch_mutexwait + 10<br>
&gt; libsystem_kernel.dylib`__psynch_mutexwait:<br>
&gt; -&gt;  0x7fff93306de6 &lt;+10&gt;: jae    0x7fff93306df0            ; &lt;+20&gt;<br>
&gt;     0x7fff93306de8 &lt;+12&gt;: movq   %rax, %rdi<br>
&gt;     0x7fff93306deb &lt;+15&gt;: jmp    0x7fff933017cd            ; cerror_nocancel<br>
&gt;     0x7fff93306df0 &lt;+20&gt;: retq<br>
&gt; (lldb) bt<br>
&gt; * thread #2: tid = 0x272b40f, 0x00007fff93306de6 libsystem_kernel.dylib`__psynch_mutexwait + 10<br>
&gt;   * frame #0: 0x00007fff93306de6 libsystem_kernel.dylib`__psynch_mutexwait + 10<br>
&gt;     frame #1: 0x00007fff9a000e4a libsystem_pthread.dylib`_pthread_mutex_lock_wait + 89<br>
&gt;     frame #2: 0x00007fff99ffe5f5 libsystem_pthread.dylib`_pthread_mutex_lock_slow + 300<br>
&gt;     frame #3: 0x00007fff8c2a00f8 libdyld.dylib`dyldGlobalLockAcquire() + 16<br>
&gt;     frame #4: 0x00007fff6ca8e177 dyld`ImageLoaderMachOCompressed::doBindFastLazySymbol(unsigned int, ImageLoader::LinkContext const&amp;, void (*)(), void (*)()) + 55<br>
&gt;     frame #5: 0x00007fff6ca78063 dyld`dyld::fastBindLazySymbol(ImageLoader**, unsigned long) + 90<br>
&gt;     frame #6: 0x00007fff8c2a0262 libdyld.dylib`dyld_stub_binder + 282<br>
&gt;     frame #7: 0x000000010a5b29b0 libopen-pal.0.dylib`obj_order_type + 3776<br>
&gt;<br>
&gt;<br>
&gt; (lldb) thread select 3<br>
&gt; * thread #3: tid = 0x272b410, 0x00007fff93306de6 libsystem_kernel.dylib`__psynch_mutexwait + 10<br>
&gt;     frame #0: 0x00007fff93306de6 libsystem_kernel.dylib`__psynch_mutexwait + 10<br>
&gt; libsystem_kernel.dylib`__psynch_mutexwait:<br>
&gt; -&gt;  0x7fff93306de6 &lt;+10&gt;: jae    0x7fff93306df0            ; &lt;+20&gt;<br>
&gt;     0x7fff93306de8 &lt;+12&gt;: movq   %rax, %rdi<br>
&gt;     0x7fff93306deb &lt;+15&gt;: jmp    0x7fff933017cd            ; cerror_nocancel<br>
&gt;     0x7fff93306df0 &lt;+20&gt;: retq<br>
&gt; (lldb) bt<br>
&gt; * thread #3: tid = 0x272b410, 0x00007fff93306de6 libsystem_kernel.dylib`__psynch_mutexwait + 10<br>
&gt;   * frame #0: 0x00007fff93306de6 libsystem_kernel.dylib`__psynch_mutexwait + 10<br>
&gt;     frame #1: 0x00007fff9a000e4a libsystem_pthread.dylib`_pthread_mutex_lock_wait + 89<br>
&gt;     frame #2: 0x00007fff99ffe5f5 libsystem_pthread.dylib`_pthread_mutex_lock_slow + 300<br>
&gt;     frame #3: 0x00007fff8c2a00f8 libdyld.dylib`dyldGlobalLockAcquire() + 16<br>
&gt;     frame #4: 0x00007fff6ca8e177 dyld`ImageLoaderMachOCompressed::doBindFastLazySymbol(unsigned int, ImageLoader::LinkContext const&amp;, void (*)(), void (*)()) + 55<br>
&gt;     frame #5: 0x00007fff6ca78063 dyld`dyld::fastBindLazySymbol(ImageLoader**, unsigned long) + 90<br>
&gt;     frame #6: 0x00007fff8c2a0262 libdyld.dylib`dyld_stub_binder + 282<br>
&gt;     frame #7: 0x000000010a5b29b0 libopen-pal.0.dylib`obj_order_type + 3776<br>
&gt;<br>
&gt;<br>
&gt; (lldb) thread select 4<br>
&gt; * thread #4: tid = 0x272b411, 0x00007fff9330707a libsystem_kernel.dylib`__select + 10<br>
&gt;     frame #0: 0x00007fff9330707a libsystem_kernel.dylib`__select + 10<br>
&gt; libsystem_kernel.dylib`__select:<br>
&gt; -&gt;  0x7fff9330707a &lt;+10&gt;: jae    0x7fff93307084            ; &lt;+20&gt;<br>
&gt;     0x7fff9330707c &lt;+12&gt;: movq   %rax, %rdi<br>
&gt;     0x7fff9330707f &lt;+15&gt;: jmp    0x7fff933017f2            ; cerror<br>
&gt;     0x7fff93307084 &lt;+20&gt;: retq<br>
&gt; (lldb) bt<br>
&gt; * thread #4: tid = 0x272b411, 0x00007fff9330707a libsystem_kernel.dylib`__select + 10<br>
&gt;   * frame #0: 0x00007fff9330707a libsystem_kernel.dylib`__select + 10<br>
&gt;     frame #1: 0x000000010a9b1273 mca_pmix_pmix114.so`listen_thread(obj=0x0000000000000000) + 371 at pmix_server_listener.c:226<br>
&gt;     frame #2: 0x00007fff9a00099d libsystem_pthread.dylib`_pthread_body + 131<br>
&gt;     frame #3: 0x00007fff9a00091a libsystem_pthread.dylib`_pthread_start + 168<br>
&gt;     frame #4: 0x00007fff99ffe351 libsystem_pthread.dylib`thread_start + 13<br>
&gt;<br>
&gt;<br>
&gt; (lldb) thread select 5<br>
&gt; * thread #5: tid = 0x272b412, 0x00007fff9330707a libsystem_kernel.dylib`__select + 10<br>
&gt;     frame #0: 0x00007fff9330707a libsystem_kernel.dylib`__select + 10<br>
&gt; libsystem_kernel.dylib`__select:<br>
&gt; -&gt;  0x7fff9330707a &lt;+10&gt;: jae    0x7fff93307084            ; &lt;+20&gt;<br>
&gt;     0x7fff9330707c &lt;+12&gt;: movq   %rax, %rdi<br>
&gt;     0x7fff9330707f &lt;+15&gt;: jmp    0x7fff933017f2            ; cerror<br>
&gt;     0x7fff93307084 &lt;+20&gt;: retq<br>
&gt; (lldb) bt<br>
&gt; * thread #5: tid = 0x272b412, 0x00007fff9330707a libsystem_kernel.dylib`__select + 10<br>
&gt;   * frame #0: 0x00007fff9330707a libsystem_kernel.dylib`__select + 10<br>
&gt;     frame #1: 0x000000010a3c13cc libopen-rte.0.dylib`listen_thread_fn(obj=0x000000010a46e8c0) + 428 at listener.c:261<br>
&gt;     frame #2: 0x00007fff9a00099d libsystem_pthread.dylib`_pthread_body + 131<br>
&gt;     frame #3: 0x00007fff9a00091a libsystem_pthread.dylib`_pthread_start + 168<br>
&gt;     frame #4: 0x00007fff99ffe351 libsystem_pthread.dylib`thread_start + 13<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; On Fri, Jun 3, 2016 at 9:50 AM, Jeff Squyres (jsquyres) &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br>
&gt; George --<br>
&gt;<br>
&gt; You might want to get bt&#39;s from *all* the threads...?<br>
&gt;<br>
&gt;<br>
&gt; &gt; On Jun 2, 2016, at 5:31 PM, George Bosilca &lt;<a href="mailto:bosilca@icl.utk.edu">bosilca@icl.utk.edu</a>&gt; wrote:<br>
&gt; &gt;<br>
&gt; &gt; The timeout never triggers and when I attach to the mpirun process I see an extremely strange stack:<br>
&gt; &gt;<br>
&gt; &gt; (lldb) bt<br>
&gt; &gt; * thread #1: tid = 0x272b40e, 0x00007fff93306de6 libsystem_kernel.dylib`__psynch_mutexwait + 10, queue = &#39;com.apple.main-thread&#39;, stop reason = signal SIGSTOP<br>
&gt; &gt;   * frame #0: 0x00007fff93306de6 libsystem_kernel.dylib`__psynch_mutexwait + 10<br>
&gt; &gt;     frame #1: 0x00007fff9a000e4a libsystem_pthread.dylib`_pthread_mutex_lock_wait + 89<br>
&gt; &gt;     frame #2: 0x00007fff99ffe5f5 libsystem_pthread.dylib`_pthread_mutex_lock_slow + 300<br>
&gt; &gt;     frame #3: 0x00007fff8c2a00f8 libdyld.dylib`dyldGlobalLockAcquire() + 16<br>
&gt; &gt;     frame #4: 0x00007fff6ca8e177 dyld`ImageLoaderMachOCompressed::doBindFastLazySymbol(unsigned int, ImageLoader::LinkContext const&amp;, void (*)(), void (*)()) + 55<br>
&gt; &gt;     frame #5: 0x00007fff6ca78063 dyld`dyld::fastBindLazySymbol(ImageLoader**, unsigned long) + 90<br>
&gt; &gt;     frame #6: 0x00007fff8c2a0262 libdyld.dylib`dyld_stub_binder + 282<br>
&gt; &gt;     frame #7: 0x000000010a5b29b0 libopen-pal.0.dylib`obj_order_type + 3776<br>
&gt; &gt;<br>
&gt; &gt; This seems to indicate that we are trying to access a function from a dylib that has been or is in the process of being unloaded.<br>
&gt; &gt;<br>
&gt; &gt;   George.<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; On Thu, Jun 2, 2016 at 8:34 AM, Nathan Hjelm &lt;<a href="mailto:hjelmn@me.com">hjelmn@me.com</a>&gt; wrote:<br>
&gt; &gt; The osc hang is fixed by a PR to fix bugs in start in cm and ob1. See #1729.<br>
&gt; &gt;<br>
&gt; &gt; -Nathan<br>
&gt; &gt;<br>
&gt; &gt; On Jun 2, 2016, at 5:17 AM, Gilles Gouaillardet &lt;<a href="mailto:gilles.gouaillardet@gmail.com">gilles.gouaillardet@gmail.com</a>&gt; wrote:<br>
&gt; &gt;<br>
&gt; &gt;&gt; fwiw,<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; the onsided/c_fence_lock test from the ibm test suite hangs<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; (mpirun -np 2 ./c_fence_lock)<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; i ran a git bisect and it incriminates commit b90c83840f472de3219b87cd7e1a364eec5c5a29<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; commit b90c83840f472de3219b87cd7e1a364eec5c5a29<br>
&gt; &gt;&gt; Author: bosilca &lt;<a href="mailto:bosilca@users.noreply.github.com">bosilca@users.noreply.github.com</a>&gt;<br>
&gt; &gt;&gt; Date:   Tue May 24 18:20:51 2016 -0500<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;     Refactor the request completion (#1422)<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;     * Remodel the request.<br>
&gt; &gt;&gt;     Added the wait sync primitive and integrate it into the PML and MTL<br>
&gt; &gt;&gt;     infrastructure. The multi-threaded requests are now significantly<br>
&gt; &gt;&gt;     less heavy and less noisy (only the threads associated with completed<br>
&gt; &gt;&gt;     requests are signaled).<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;     * Fix the condition to release the request.<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; I also noted a warning is emitted when running only one task<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; ./c_fence_lock<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; but I did not git bisect, so that might not be related<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; Cheers,<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; Gilles<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; On Thursday, June 2, 2016, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt; &gt;&gt; Yes, please! I’d like to know what mpirun thinks is happening - if you like, just set the —timeout N —report-state-on-timeout flags and tell me what comes out<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;&gt; On Jun 1, 2016, at 7:57 PM, George Bosilca &lt;<a href="mailto:bosilca@icl.utk.edu">bosilca@icl.utk.edu</a>&gt; wrote:<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; I don&#39;t think it matters. I was running the IBM collective and pt2pt tests, but each time it deadlocked was in a different test. If you are interested in some particular values, I would be happy to attach a debugger next time it happens.<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;   George.<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; On Wed, Jun 1, 2016 at 10:47 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt; &gt;&gt;&gt; What kind of apps are they? Or does it matter what you are running?<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; &gt; On Jun 1, 2016, at 7:37 PM, George Bosilca &lt;<a href="mailto:bosilca@icl.utk.edu">bosilca@icl.utk.edu</a>&gt; wrote:<br>
&gt; &gt;&gt;&gt; &gt;<br>
&gt; &gt;&gt;&gt; &gt; I have a seldomly occurring deadlock on a OS X laptop if I use more than 2 processes). It is coming up once every 200 runs or so.<br>
&gt; &gt;&gt;&gt; &gt;<br>
&gt; &gt;&gt;&gt; &gt; Here is what I could gather from my experiments: All the MPI processes seem to have correctly completed (I get all the expected output and the MPI processes are in a waiting state), but somehow the mpirun does not detect their completion. As a result, mpirun never returns.<br>
&gt; &gt;&gt;&gt; &gt;<br>
&gt; &gt;&gt;&gt; &gt;   George.<br>
&gt; &gt;&gt;&gt; &gt;<br>
&gt; &gt;&gt;&gt; &gt; _______________________________________________<br>
&gt; &gt;&gt;&gt; &gt; devel mailing list<br>
&gt; &gt;&gt;&gt; &gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; &gt;&gt;&gt; &gt; Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; &gt;&gt;&gt; &gt; Searchable archives: <a href="http://www.open-mpi.org/community/lists/devel/2016/06/19054.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/06/19054.php</a><br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; _______________________________________________<br>
&gt; &gt;&gt;&gt; devel mailing list<br>
&gt; &gt;&gt;&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; &gt;&gt;&gt; Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; &gt;&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/06/19054.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/06/19054.php</a><br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; _______________________________________________<br>
&gt; &gt;&gt;&gt; devel mailing list<br>
&gt; &gt;&gt;&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; &gt;&gt;&gt; Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; &gt;&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/06/19055.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/06/19055.php</a><br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; _______________________________________________<br>
&gt; &gt;&gt; devel mailing list<br>
&gt; &gt;&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; &gt;&gt; Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; &gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/06/19059.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/06/19059.php</a><br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; devel mailing list<br>
&gt; &gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; &gt; Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; &gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/06/19060.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/06/19060.php</a><br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; devel mailing list<br>
&gt; &gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; &gt; Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; &gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/06/19061.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/06/19061.php</a><br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Jeff Squyres<br>
&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" rel="noreferrer" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/06/19062.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/06/19062.php</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</div></div>&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/06/19063.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/06/19063.php</a><br>
<span class=""><br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" rel="noreferrer" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</span>Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/06/19066.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/06/19066.php</a></blockquote></div><br></div></div>

