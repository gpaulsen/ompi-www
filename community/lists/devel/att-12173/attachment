<div dir="ltr">I&#39;ve tried the new rc. Here is what I got:<div><br></div><div style>1) I&#39;ve successfully built it with intel-13.1 and gcc-4.7.2. But I&#39;ve failed while using open64-4.5.2 and ekopath-5.0.0 (pathscale). The problems are in the fortran part. In each case I&#39;ve used the following configuration line:</div>
<div style>CC=$CC CXX=$CXX F77=$F77 FC=$FC ./configure --prefix=$prefix --with-knem=$knem_path</div><div style>Open64 failed during configuration with the following:</div><div style><div>*** Fortran compiler</div><div>checking whether we are using the GNU Fortran compiler... yes</div>
<div>checking whether openf95 accepts -g... yes</div><div>configure: WARNING: Open MPI now ignores the F77 and FFLAGS environment variables; only the FC and FCFLAGS environment variables are used.</div><div>checking whether ln -s works... yes</div>
<div>checking if Fortran compiler works... yes</div><div>checking for extra arguments to build a shared library... none needed</div><div>checking for Fortran flag to compile .f files... none</div><div>checking for Fortran flag to compile .f90 files... none</div>
<div>checking to see if Fortran compilers need additional linker flags... none</div><div>checking  external symbol convention... double underscore</div><div>checking if C and Fortran are link compatible... yes</div><div>checking to see if Fortran compiler likes the C++ exception flags... skipped (no C++ exceptions flags)</div>
<div>checking to see if mpifort compiler needs additional linker flags... none</div><div>checking if Fortran compiler supports CHARACTER... yes</div><div>checking size of Fortran CHARACTER... 1</div><div>checking for C type corresponding to CHARACTER... char</div>
<div>checking alignment of Fortran CHARACTER... 1</div><div>checking for corresponding KIND value of CHARACTER... C_SIGNED_CHAR</div><div>checking KIND value of Fortran C_SIGNED_CHAR... no ISO_C_BINDING -- fallback</div><div>
checking Fortran value of selected_int_kind(4)... no</div><div>configure: WARNING: Could not determine KIND value of C_SIGNED_CHAR</div><div>configure: WARNING: See config.log for more details</div><div>configure: error: Cannot continue</div>
<div><br></div><div style>Ekopath failed during make with the following error:</div><div style><div> PPFC     mpi-f08-sizeof.lo</div><div>  PPFC     mpi-f08.lo</div><div>In file included from mpi-f08.F90:37:</div><div>mpi-f-interfaces-bind.h:1908: warning: extra tokens at end of #endif directive</div>
<div>mpi-f-interfaces-bind.h:2957: warning: extra tokens at end of #endif directive</div><div>In file included from mpi-f08.F90:38:</div><div>pmpi-f-interfaces-bind.h:1911: warning: extra tokens at end of #endif directive</div>
<div>pmpi-f-interfaces-bind.h:2963: warning: extra tokens at end of #endif directive</div><div>pathf95-1044 pathf95: INTERNAL OMPI_OP_CREATE_F, File = mpi-f-interfaces-bind.h, Line = 955, Column = 29 </div><div>  Internal : Unexpected ATP_PGM_UNIT in check_interoperable_pgm_unit()</div>
<div>make[2]: *** [mpi-f08.lo] Error 1</div><div>make[2]: Leaving directory `/tmp/mpi_install_tmp1400/openmpi-1.7rc8/ompi/mpi/fortran/use-mpi-f08&#39;</div><div>make[1]: *** [all-recursive] Error 1</div><div>make[1]: Leaving directory `/tmp/mpi_install_tmp1400/openmpi-1.7rc8/ompi&#39;</div>
<div>make: *** [all-recursive] Error 1</div><div><br></div><div style>It seems to be different from the error I got last time with rc7. And again I&#39;m not a fortran guy to understand this error. I&#39;ve used the following version of the compiler: <a href="http://c591116.r16.cf2.rackcdn.com/ekopath/nightly/Linux/ekopath-2013-02-26-installer.run">http://c591116.r16.cf2.rackcdn.com/ekopath/nightly/Linux/ekopath-2013-02-26-installer.run</a></div>
<div style><br></div><div style>2) I&#39;ve ran a couple of tests (IMB) with the new version. I ran this on a system consisting of 10 nodes with Intel SB processor and fdr ConnectX3 infiniband adapters.</div><div style>First I&#39;ve tried the following parameters:</div>
<div style><div>mpirun -np $NP -hostfile hosts --mca btl openib,sm,self --bind-to-core -npernode 16 --mca mpi_leave_pinned 1 ./IMB-MPI1 -npmin $NP -mem 4G $COLL</div><div style>This combination complained about mca_leave_pinned. The same line works for 1.6.3. Is something different in the new release and I&#39;ve missed it?</div>
<div style><div>--------------------------------------------------------------------------</div><div>A process attempted to use the &quot;leave pinned&quot; MPI feature, but no</div><div>memory registration hooks were found on the system at run time.  This</div>
<div>may be the result of running on a system that does not support memory</div><div>hooks or having some other software subvert Open MPI&#39;s use of the</div><div>memory hooks.  You can disable Open MPI&#39;s use of memory hooks by</div>
<div>setting both the mpi_leave_pinned and mpi_leave_pinned_pipeline MCA</div><div>parameters to 0.</div><div><br></div><div>Open MPI will disable any transports that are attempting to use the</div><div>leave pinned functionality; your job may still run, but may fall back</div>
<div>to a slower network transport (such as TCP).</div><div><br></div><div>  Mpool name: grdma</div><div>  Process:    [[13305,1],1]</div><div>  Local host: b23</div><div>--------------------------------------------------------------------------</div>
<div>--------------------------------------------------------------------------</div><div>WARNING: There is at least one OpenFabrics device found but there are</div><div>no active ports detected (or Open MPI was unable to use them).  This</div>
<div>is most certainly not what you wanted.  Check your cables, subnet</div><div>manager configuration, etc.  The openib BTL will be ignored for this</div><div>job.</div><div><br></div><div>  Local host: b23</div><div>--------------------------------------------------------------------------</div>
<div>--------------------------------------------------------------------------</div><div>At least one pair of MPI processes are unable to reach each other for</div><div>MPI communications.  This means that no Open MPI device has indicated</div>
<div>that it can be used to communicate between these processes.  This is</div><div>an error; Open MPI requires that all MPI processes be able to reach</div><div>each other.  This error can sometimes be the result of forgetting to</div>
<div>specify the &quot;self&quot; BTL.</div><div><br></div><div>  Process 1 ([[13305,1],0]) is on host: b22</div><div>  Process 2 ([[13305,1],1]) is on host: b23</div><div>  BTLs attempted: self sm</div><div><br></div><div>
Your MPI job is now going to abort; sorry.</div><div>...</div><div><br></div><div style>Then I ran a couple of P2P and collective tests. In general the performance improved compared to 1.6.3. But there are several cases where it got worse. Perhaps I need to use some tuning, could you please tell me what parameters would suite me better then the default.</div>
<div style>Here is what I got for PingPong and PingPing in 1.7rc8 (the above parameters changed to have &quot;-npernode 1&quot;):</div><div style><div>#---------------------------------------------------</div><div># Benchmarking PingPong </div>
<div># #processes = 2 </div><div>#---------------------------------------------------</div><div>       #bytes #repetitions      t[usec]   Mbytes/sec</div><div>            0         1000         1.39         0.00</div><div>
            1         1000         1.50         0.64</div><div>            2         1000         1.10         1.73</div><div>            4         1000         1.10         3.46</div><div>            8         1000         1.12         6.80</div>
<div>           16         1000         1.12        13.62</div><div>           32         1000         1.14        26.75</div><div>           64         1000         1.18        51.92</div><div>          128         1000         1.73        70.42</div>
<div>          256         1000         1.85       132.04</div><div>          512         1000         1.98       247.16</div><div>         1024         1000         2.26       431.52</div><div>         2048         1000         2.85       684.58</div>
<div>         4096         1000         3.49      1118.63</div><div>         8192         1000         4.48      1741.96</div><div>        16384         1000         9.58      1630.92</div><div>        32768         1000        14.27      2189.46</div>
<div>        65536          640        23.03      2713.71</div><div>       131072          320        35.55      3515.73</div><div>       262144          160        57.65      4336.77</div><div>       524288           80       101.42      4930.05</div>
<div>      1048576           40       188.00      5319.18</div><div>      2097152           20       521.70      3833.61</div><div>      4194304           10      1118.20      3577.19</div><div><br></div><div>#---------------------------------------------------</div>
<div># Benchmarking PingPing </div><div># #processes = 2 </div><div>#---------------------------------------------------</div><div>       #bytes #repetitions      t[usec]   Mbytes/sec</div><div>            0         1000         1.26         0.00</div>
<div>            1         1000         1.32         0.72</div><div>            2         1000         1.32         1.44</div><div>            4         1000         1.35         2.84</div><div>            8         1000         1.38         5.53</div>
<div>           16         1000         1.13        13.51</div><div>           32         1000         1.13        26.96</div><div>           64         1000         1.17        51.95</div><div>          128         1000         1.72        70.96</div>
<div>          256         1000         1.80       135.63</div><div>          512         1000         1.94       251.17</div><div>         1024         1000         2.23       437.51</div><div>         2048         1000         2.88       677.47</div>
<div>         4096         1000         3.49      1119.28</div><div>         8192         1000         4.75      1643.41</div><div>        16384         1000         9.90      1578.12</div><div>        32768         1000        14.54      2149.25</div>
<div>        65536          640        24.04      2599.79</div><div>       131072          320        37.00      3378.35</div><div>       262144          160        60.25      4149.39</div><div>       524288           80       105.74      4728.77</div>
<div>      1048576           40       196.73      5083.23</div><div>      2097152           20       785.79      2545.20</div><div>      4194304           10      1790.19      2234.40</div><div><br></div><div style>And 1.6.3 gave the following:</div>
<div style><div>#---------------------------------------------------</div><div># Benchmarking PingPong </div><div># #processes = 2 </div><div>#---------------------------------------------------</div><div>       #bytes #repetitions      t[usec]   Mbytes/sec</div>
<div>            0         1000         1.06         0.00</div><div>            1         1000         0.94         1.01</div><div>            2         1000         0.95         2.02</div><div>            4         1000         0.95         4.01</div>
<div>            8         1000         0.97         7.90</div><div>           16         1000         0.98        15.63</div><div>           32         1000         0.99        30.86</div><div>           64         1000         1.02        59.60</div>
<div>          128         1000         1.58        77.23</div><div>          256         1000         1.71       142.73</div><div>          512         1000         1.86       263.15</div><div>         1024         1000         2.13       459.35</div>
<div>         2048         1000         2.72       718.31</div><div>         4096         1000         3.27      1194.74</div><div>         8192         1000         4.33      1802.57</div><div>        16384         1000         6.20      2521.78</div>
<div>        32768         1000         8.84      3535.46</div><div>        65536          640        14.28      4376.82</div><div>       131072          320        24.97      5005.06</div><div>       262144          160        44.94      5562.46</div>
<div>       524288           80        86.76      5763.29</div><div>      1048576           40       168.73      5926.77</div><div>      2097152           20       333.65      5994.32</div><div>      4194304           10       666.09      6005.16</div>
<div><br></div><div>#---------------------------------------------------</div><div># Benchmarking PingPing </div><div># #processes = 2 </div><div>#---------------------------------------------------</div><div>       #bytes #repetitions      t[usec]   Mbytes/sec</div>
<div>            0         1000         0.93         0.00</div><div>            1         1000         0.97         0.98</div><div>            2         1000         0.97         1.97</div><div>            4         1000         0.97         3.94</div>
<div>            8         1000         0.99         7.70</div><div>           16         1000         0.99        15.34</div><div>           32         1000         1.01        30.21</div><div>           64         1000         1.05        58.13</div>
<div>          128         1000         1.61        75.82</div><div>          256         1000         1.73       141.20</div><div>          512         1000         1.88       259.87</div><div>         1024         1000         2.17       450.21</div>
<div>         2048         1000         2.83       691.13</div><div>         4096         1000         3.45      1131.26</div><div>         8192         1000         4.76      1639.88</div><div>        16384         1000         7.76      2014.01</div>
<div>        32768         1000        10.34      3021.35</div><div>        65536          640        16.29      3836.55</div><div>       131072          320        26.72      4678.40</div><div>       262144          160        48.83      5120.31</div>
<div>       524288           80        91.85      5443.61</div><div>      1048576           40       178.65      5597.63</div><div>      2097152           20       351.31      5692.98</div><div>      4194304           10       701.69      5700.53</div>
<div><br></div><div style>The sendrecv and exchange also got worse. I can send additional data if needed.</div><div style><br></div><div style>The performance on collectives generally has slightly improved comparing to 1.6.3 or remained the same. But in certain cases I got much better results with tuned_collectives. In particular those suited my system better:</div>
<div style>--mca coll_tuned_barrier_algorithm 6 (default and tuned):<br></div><div style><div>#---------------------------------------------------</div><div># Benchmarking Barrier </div><div># #processes = 160 </div><div>
#---------------------------------------------------</div><div> #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]</div><div>         1000        49.75        49.77        49.76</div><div style><div>#---------------------------------------------------</div>
<div># Benchmarking Barrier </div><div># #processes = 160 </div><div>#---------------------------------------------------</div><div> #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]</div><div>         1000        12.74        12.74        12.74</div>
<div><br></div><div style>Bcast for small messages</div><div style>--mca coll_tuned_bcast_algorithm 3 (default and tuned):<br></div><div style><div>#----------------------------------------------------------------</div><div>
# Benchmarking Bcast </div><div># #processes = 160 </div><div>#----------------------------------------------------------------</div><div>       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]</div><div>            0         1000         0.01         0.02         0.02</div>
<div>            1         1000         9.87         9.96         9.92</div><div>            2         1000        10.44        10.51        10.47</div><div>            4         1000        10.30        10.37        10.34</div>
<div>            8         1000        10.34        10.43        10.38</div><div>           16         1000        10.39        10.48        10.43</div><div>           32         1000        10.36        10.43        10.40</div>
<div>           64         1000        10.38        10.44        10.41</div><div>          128         1000        10.11        10.22        10.17</div><div>          256         1000        11.37        11.54        11.48</div>
<div>          512         1000        14.09        14.25        14.19</div><div>         1024         1000        18.77        19.03        18.94</div><div>         2048         1000        13.47        13.63        13.58</div>
<div>         4096         1000        25.39        25.60        25.55</div><div>         8192         1000        50.80        51.11        51.04</div><div>        16384         1000       102.64       103.53       103.38</div>
<div>        32768         1000       280.86       281.80       281.62</div><div>        65536          640       387.10       391.90       391.26</div><div>       131072          320       779.58       796.04       794.30</div>
<div>       262144          160      1526.52      1597.39      1590.31</div><div>       524288           80       355.67       379.06       375.27</div><div>      1048576           40       702.95       753.65       736.29</div>
<div>      2097152           20      1518.11      1580.85      1551.57</div><div>      4194304           10      3183.22      3931.81      3676.94</div><div><br></div><div><div>#----------------------------------------------------------------</div>
<div># Benchmarking Bcast </div><div># #processes = 160 </div><div>#----------------------------------------------------------------</div><div>       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]</div><div>            0         1000         0.01         0.02         0.02</div>
<div>            1         1000         4.54         5.13         4.85</div><div>            2         1000         4.50         5.11         4.81</div><div>            4         1000         4.50         5.09         4.80</div>
<div>            8         1000         4.48         5.09         4.79</div><div>           16         1000         4.49         5.09         4.79</div><div>           32         1000         4.55         5.15         4.86</div>
<div>           64         1000         4.52         5.14         4.83</div><div>          128         1000         4.66         5.28         4.98</div><div>          256         1000         4.78         5.40         5.09</div>
<div>          512         1000         4.89         5.52         5.21</div><div>         1024         1000         5.15         5.81         5.48</div><div>         2048         1000         5.60         6.30         5.94</div>
<div>         4096         1000         8.25         8.67         8.46</div><div>         8192         1000        10.49        11.01        10.76</div><div>        16384         1000        20.05        20.87        20.50</div>
<div>        32768         1000        30.11        31.41        30.80</div><div>        65536          640        46.08        48.94        47.54</div><div>       131072          320        75.53        84.98        80.26</div>
<div>       262144          160       134.26       169.44       151.92</div><div>       524288           80       240.34       372.76       307.80</div><div>      1048576           40       427.00       951.02       699.41</div>
<div>      2097152           20       933.41      3170.45      2076.21</div><div>      4194304           10      2682.40     16020.39      9718.86</div></div><div><br></div><div style>and AllGatherv:</div><div style>--mca coll_tuned_allgatherv_algorithm 5 (default and tuned):<br>
</div><div style><div>#----------------------------------------------------------------</div><div># Benchmarking Allgatherv </div><div># #processes = 160 </div><div>#----------------------------------------------------------------</div>
<div>       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]</div><div>            0         1000         0.06         0.07         0.06</div><div>            1         1000        54.11        54.15        54.13</div>
<div>            2         1000        52.74        52.78        52.76</div><div>            4         1000        55.09        55.13        55.11</div><div>            8         1000        58.48        58.52        58.50</div>
<div>           16         1000        61.99        62.03        62.01</div><div>           32         1000        69.31        69.35        69.32</div><div>           64         1000        88.13        88.18        88.16</div>
<div>          128         1000       126.62       126.71       126.68</div><div>          256         1000       215.26       215.34       215.31</div><div>          512         1000       832.54       833.01       832.57</div>
<div>         1024         1000       928.81       929.31       928.86</div><div>         2048         1000      1072.77      1073.35      1072.85</div><div>         4096         1000      1222.82      1223.42      1222.90</div>
<div>         8192         1000      1713.46      1714.13      1713.87</div><div>        16384         1000      2596.87      2598.31      2597.40</div><div>        32768         1000      4153.70      4154.09      4153.92</div>
<div>        65536          640      6795.04      6796.32      6795.83</div><div>       131072          320     12076.74     12083.04     12080.28</div><div>       262144          160     23120.98     23153.76     23138.10</div>
<div>       524288           80     49077.99     49204.79     49142.48</div><div>      1048576           40    132120.25    132675.60    132400.38</div><div>      2097152           20    240537.20    241821.05    241138.53</div>
<div>      4194304           10    457125.71    459065.10    458035.03</div><div><br></div><div><div>#----------------------------------------------------------------</div><div># Benchmarking Allgatherv </div><div># #processes = 160 </div>
<div>#----------------------------------------------------------------</div><div>       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]</div><div>            0         1000         0.06         0.07         0.06</div>
<div>            1         1000         0.47         0.56         0.52</div><div>            2         1000         0.47         0.57         0.51</div><div>            4         1000         0.48         0.56         0.52</div>
<div>            8         1000         0.46         0.56         0.51</div><div>           16         1000         0.47         0.57         0.52</div><div>           32         1000         0.47         0.56         0.52</div>
<div>           64         1000         0.47         0.57         0.52</div><div>          128         1000         0.50         0.62         0.57</div><div>          256         1000         0.58         0.68         0.63</div>
<div>          512         1000         0.62         0.81         0.70</div><div>         1024         1000         0.71         0.97         0.80</div><div>         2048         1000         0.89         1.24         1.05</div>
<div>         4096         1000         2.21         2.58         2.40</div><div>         8192         1000         3.08         3.55         3.30</div><div>        16384         1000         4.77         5.56         5.11</div>
<div>        32768         1000         7.99         9.75         8.90</div><div>        65536          640        15.81        19.35        17.69</div><div>       131072          320        34.18        39.74        36.95</div>
<div>       262144          160        71.72        80.37        76.06</div><div>       524288           80       143.64       161.81       152.36</div><div>      1048576           40       781.10       868.80       825.57</div>
<div>      2097152           20      2594.30      2795.45      2672.58</div><div>      4194304           10      5185.79      5451.20      5298.98</div></div><div><br></div><div style>This time I only ran the test on 160 processes but before I&#39;ve done more testing with 1.6 on different number of processes (from 16 to 320) and those tuned parameters helped almost each time. I don&#39;t know what are default parameters tuned for but perhaps it may be a good idea to change the defaults for the kind of system I use.</div>
<div style><br></div><div style><br></div><div style><br></div><div style>I can perform some additional tests if necessary or give more information on the problems that I&#39;ve came across.</div><div style><br></div><div style>
Regards, Pavel Mezentsev.</div></div></div></div></div></div></div></div></div></div></div></div><div class="gmail_extra"><br><br><div class="gmail_quote">2013/2/27 Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span><br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">The goal is to release 1.7 (final) by the end of this week.  New rc posted with fairly small changes:<br>
<br>
    <a href="http://www.open-mpi.org/software/ompi/v1.7/" target="_blank">http://www.open-mpi.org/software/ompi/v1.7/</a><br>
<br>
- Fix wrong header file / compilation error in bcol<br>
- Support MXM STREAM for isend and irecv<br>
- Make sure &quot;mpirun &lt;dirname&gt;&quot; fails with $status!=0<br>
- Bunches of cygwin minor fixes<br>
- Make sure the fortran compiler supports BIND(C) with LOGICAL for the F08 bindings<br>
- Fix --disable-mpi-io with the F08 bindings<br>
<span class="HOEnZb"><font color="#888888"><br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</font></span></blockquote></div><br></div>

