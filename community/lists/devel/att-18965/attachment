to use ompio<div>mpirun --mca io ompio ...</div><div>to use romio (v2.x)</div><div>mpirun --mca io romio314 ...</div><div>to use romio (v1.10)</div><div>mpirun --mca io romio ...</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles</div><div><br><br>On Wednesday, May 11, 2016, Michael Rezny &lt;<a href="mailto:michael.rezny@monash.edu">michael.rezny@monash.edu</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><div><div>Hi Sreenidhi,<br></div>you need to specify --collective as an input parameter to mpi_tile_io<br><br></div>kindest regards<br></div>Mike<br><div><div><div><br></div></div></div></div><div class="gmail_extra"><br><div class="gmail_quote">On 11 May 2016 at 12:01, Sreenidhi Bharathkar Ramesh <span dir="ltr">&lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;sreenidhi-bharathkar.ramesh@broadcom.com&#39;);" target="_blank">sreenidhi-bharathkar.ramesh@broadcom.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><br></div>Thank you so much for the details.<div><br></div><div><br></div><div>1. while running the &quot;Tile I/O&quot; benchmark, I see the following message:</div><div><br></div><div>$ mpirun -np 28 ./mpi-tile-io --nr_tiles_x 7 --nr_tiles_y 4 --sz_tile_x 100 --sz_tile_y 100 --sz_element 32 --filename file1g</div><div>...</div><div># collective I/O off<br></div><div><br></div><div>How do I enable collective I/O ?</div><div><br></div><div>2. I switched to using Open MPI v 2.0.0rc2 .  How do I know which IO is being used ?  How do I switch between OMPIO and ROMIO ? </div><div><br></div><div><br></div><div>Please let me know.</div><div><br></div><div>Thanks,</div><div>- Sreenidhi.</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Tue, May 10, 2016 at 7:14 PM, Edgar Gabriel <span dir="ltr">&lt;<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;egabriel@central.uh.edu&#39;);" target="_blank">egabriel@central.uh.edu</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
  
    
  
  <div bgcolor="#FFFFFF" text="#000000">
    <p>in the 1.7, 1.8 and 1.10 series ROMIO remains the default. In the
      upcomgin 2.x series, OMPIO will be the default, except for Lustre
      file systems, where we will stick with ROMIO as the primary
      resource.</p>
    <p>Regarding performance comparison, we ran numerous tests late last
      year and early this year. It really depends on the application
      scenario and the platform that you are using. If you want to know
      which one should you use, I would definitely suggest to stick with
      ROMIO in the 1.10 series, since many of the bug fixes of OMPIO
      that we did in the last two years could not be back-ported to the
      1.10 series for technical reasons. If you plan to switch to the
      2.x series, it might be easiest to just run a couple of tests and
      compare the performance for your application on your platform, and
      base your decision on that.<span><font color="#888888"><br>
    </font></span></p><span><font color="#888888">
    <p>Edgar<br>
    </p></font></span><div><div>
    <br>
    <div>On 5/10/2016 6:32 AM, Sreenidhi
      Bharathkar Ramesh wrote:<br>
    </div>
    <blockquote type="cite">
      <div dir="ltr">Hi,
        <div><br>
        </div>
        <div>1. During default build of OpenMPI, it looks like both <a href="http://ompio.la" target="_blank">ompio.la</a>
          and <a href="http://romio.la" target="_blank">romio.la</a>
          are built.  Which I/O MCA library is used and based on what is
          the decision taken ?</div>
        <div><br>
        </div>
        <div>2. Are there any statistics available to compare these two
          - OMPIO vs ROMIO ?</div>
        <div><br>
        </div>
        <div>I am using OpenMPI v1.10.1.</div>
        <div><br>
        </div>
        <div>Thanks,</div>
        <div>- Sreenidhi.</div><span><font color="#888888">
      </font></span></div><span><font color="#888888">
    </font></span></blockquote><span><font color="#888888">
    </font></span></div></div><span><font color="#888888"><pre cols="72">--
</pre>
  </font></span></div><span><font color="#888888">

<br>_______________________________________________<br>
devel mailing list<br>
<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;devel@open-mpi.org&#39;);" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/05/18951.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/05/18951.php</a><br></font></span></blockquote></div><br></div>
<br>_______________________________________________<br>
devel mailing list<br>
<a href="javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;devel@open-mpi.org&#39;);" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/05/18963.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/05/18963.php</a><br></blockquote></div><br></div>
</blockquote></div>

