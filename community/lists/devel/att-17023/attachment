<div dir="ltr">Dave,<div><br></div><div>The PML is the layer with the smartness (or so we hope) that moves the data around in Open MPI. The default PML is called OB1, and is the only one that supports multiplexing over multiple transports. Under certain circumstances, other, more specialized PMLs (such as yalla or mxm) take precedence, in which case it is normal that you don&#39;t see the bandwidth going up as it is limited by the most efficient of the two.</div><div><br></div><div>  George.</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Mon, Feb 23, 2015 at 9:53 PM, Dave Turner <span dir="ltr">&lt;<a href="mailto:drdaveturner@gmail.com" target="_blank">drdaveturner@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Jeff, George,<div><br></div><div>      When I try to use yalla with the dev master I get the error message below:</div><div><br></div><div>







<p><span>~/openmpi-master/bin/mpirun -np 2 --mca pml yalla ./NPmpi.master -o np.out</span></p></div><div>*** Error in `/homes/daveturner/openmpi-master/bin/orted&#39;: free(): corrupted unsorted chunks: 0x0000000002351270 ***</div><div>
<p><span>*** Error in `/homes/daveturner/openmpi-master/bin/orted&#39;: corrupted double-linked list: 0</span>x0000000002351260 ***</p><p><br></p><p>George:  I&#39;m not sure what the PML ob1 is that you refer to.  I&#39;m running with the</p><p>default settings for the most part and trying to tune those.  When we had nodes<br></p><p>with just RoCE over 10 Gbps and just QDR IB, the latencies were the same</p><p>at around 3 microseconds.  The bandwidths were around 10 Gbps and 30 Gbps.</p><span class="HOEnZb"><font color="#888888"><p><br></p><p>                                   Dave</p></font></span></div></div><div class="HOEnZb"><div class="h5"><div class="gmail_extra"><br><div class="gmail_quote">On Sun, Feb 22, 2015 at 5:37 AM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Dave --<br>
<br>
Just out of curiosity, what kind of performance do you get when you use MXM?  (e.g., the yalla PML on master)<br>
<br>
<br>
&gt; On Feb 19, 2015, at 6:41 PM, Dave Turner &lt;<a href="mailto:drdaveturner@gmail.com" target="_blank">drdaveturner@gmail.com</a>&gt; wrote:<br>
&gt;<br>
&gt;<br>
&gt;      I&#39;ve downloaded the OpenMPI master as suggested and rerun all my aggregate tests<br>
&gt; across my system with QDR IB and 10 Gbps RoCE.<br>
&gt;<br>
&gt;      The attached unidirectional.pdf graph is the ping-pong performance for 1 core<br>
&gt; on 1 machine to 1 core on the 2nd.  The red curve for OpenMPI 1.8.3 shows lower<br>
&gt; performance for small and also medium message sizes for the base test without<br>
&gt; using any tuning parameters.  The green line from the OpenMPI master shows lower<br>
&gt; performance only for small messages, but great for medium size.  Turning off the<br>
&gt; 10 Gbps card entirely produces great performance for all message sizes.  So the<br>
&gt; fixes in the master at least help, but it still seems to be choosing to use RoCE for<br>
&gt; small messages rather than QDR IB.  They both use the openib btl so I assume it<br>
&gt; just chooses one at random so this is probably not that surprising.  Since there are<br>
&gt; no tunable parameters for multiple openib btl&#39;s, this cannot be manually tuned.<br>
&gt;<br>
&gt;      The bi-directional ping-pong tests show basically the same thing with lower<br>
&gt; performance for small message sizes for 1.8.3 and the master.  However, I&#39;m<br>
&gt; also seeing the max bandwidth being limited to 44 Gbps instead of 60 Gbps<br>
&gt; for the master for some reason.<br>
&gt;<br>
&gt;      The aggregate tests in the 3rd graph are for 20 cores on one machine<br>
&gt; yelling at 20 cores on the 2nd machine (bi-directional too).  They likewise show<br>
&gt; the lower 10 Gbps RoCE performance for small messages, and also show<br>
&gt; the max bandwidth being limited to 45 Gbps for the master.<br>
&gt;<br>
&gt;      Our solution for now is to simply exclude mlx4_1 which is the 10 Gbps card<br>
&gt; which will give us QDR performance but not allow us to use the extra 10 Gbps<br>
&gt; to channel bond for large messages.  It is more worrisome that max bandwidth<br>
&gt; on the bi-directional and aggregate tests using the master are slower than they<br>
&gt; should be.<br>
&gt;<br>
&gt;                        Dave<br>
&gt;<br>
&gt; On Wed, Feb 11, 2015 at 11:00 AM, &lt;<a href="mailto:devel-request@open-mpi.org" target="_blank">devel-request@open-mpi.org</a>&gt; wrote:<br>
&gt; Send devel mailing list submissions to<br>
&gt;         <a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
&gt;<br>
&gt; To subscribe or unsubscribe via the World Wide Web, visit<br>
&gt;         <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; or, via email, send a message with subject or body &#39;help&#39; to<br>
&gt;         <a href="mailto:devel-request@open-mpi.org" target="_blank">devel-request@open-mpi.org</a><br>
&gt;<br>
&gt; You can reach the person managing the list at<br>
&gt;         <a href="mailto:devel-owner@open-mpi.org" target="_blank">devel-owner@open-mpi.org</a><br>
&gt;<br>
&gt; When replying, please edit your Subject line so it is more specific<br>
&gt; than &quot;Re: Contents of devel digest...&quot;<br>
&gt;<br>
&gt;<br>
&gt; Today&#39;s Topics:<br>
&gt;<br>
&gt;    1. Re: OMPI devel] RoCE plus QDR IB tunable parameters<br>
&gt;       (George Bosilca)<br>
&gt;    2. Re: OMPI devel] RoCE plus QDR IB tunable parameters<br>
&gt;       (Howard Pritchard)<br>
&gt;<br>
&gt;<br>
&gt; ----------------------------------------------------------------------<br>
&gt;<br>
&gt; Message: 1<br>
&gt; Date: Tue, 10 Feb 2015 20:41:30 -0500<br>
&gt; From: George Bosilca &lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank">bosilca@icl.utk.edu</a>&gt;<br>
&gt; To: <a href="mailto:DrDaveTurner@gmail.com" target="_blank">DrDaveTurner@gmail.com</a>, Open MPI Developers &lt;<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a>&gt;<br>
&gt; Subject: Re: [OMPI devel] OMPI devel] RoCE plus QDR IB tunable<br>
&gt;         parameters<br>
&gt; Message-ID:<br>
&gt;         &lt;<a href="mailto:CAMJJpkXC6e_y34fU5VeJ0uHrrJ2z4CA89mN7WfWa5dSfx52s7A@mail.gmail.com" target="_blank">CAMJJpkXC6e_y34fU5VeJ0uHrrJ2z4CA89mN7WfWa5dSfx52s7A@mail.gmail.com</a>&gt;<br>
&gt; Content-Type: text/plain; charset=&quot;utf-8&quot;<br>
&gt;<br>
&gt; Somehow one of the most basic information about the capabilities of the<br>
&gt; BTLs (bandwidth) disappeared from the MCA parameters and the one left<br>
&gt; (latency) was mislabeled. This mishap not only prevented the communication<br>
&gt; engine from correctly ordering the BTL for small messages (the latency<br>
&gt; bound part), but also introduced undesirable bias on the load-balance<br>
&gt; between multiple devices logic (the bandwidth part).<br>
&gt;<br>
&gt; I just pushed a fix  in master<br>
&gt; <a href="https://github.com/open-mpi/ompi/commit/e173f9b0c0c63c3ea24b8d8bc0ebafe1f1736acb" target="_blank">https://github.com/open-mpi/ompi/commit/e173f9b0c0c63c3ea24b8d8bc0ebafe1f1736acb</a>.<br>
&gt; Once validated this should be moved over the 1.8 branch.<br>
&gt;<br>
&gt; Dave do you think it is possible to renew your experiment with the current<br>
&gt; master ?<br>
&gt;<br>
&gt;   Thanks,<br>
&gt;     George.<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; On Mon, Feb 9, 2015 at 2:57 PM, Dave Turner &lt;<a href="mailto:drdaveturner@gmail.com" target="_blank">drdaveturner@gmail.com</a>&gt; wrote:<br>
&gt;<br>
&gt; &gt; Gilles,<br>
&gt; &gt;<br>
&gt; &gt;      I tried running with btl_openib_cpc_include rdmacm and saw no change.<br>
&gt; &gt;<br>
&gt; &gt;       Let&#39;s simplify the problem by forgetting about the channel bonding.<br>
&gt; &gt; If I just do an aggregate test of 16 cores on one machine talking to 16 on<br>
&gt; &gt; a second machine without any settings changed from the default install<br>
&gt; &gt; of OpenMPI, I see that RoCE over the 10 Gbps link is used for small<br>
&gt; &gt; messages<br>
&gt; &gt; then it switches over to QDR IB for large messages.  I don&#39;t see channel<br>
&gt; &gt; bonding<br>
&gt; &gt; for large messages, but can turn this on with the btl_tcp_exclusivity<br>
&gt; &gt; parameter.<br>
&gt; &gt;<br>
&gt; &gt;      I think there are 2 problems here, both related to the fact that QDR<br>
&gt; &gt; IB link and RoCE<br>
&gt; &gt; both use the same openib btl.  The first problem is that the slower RoCE<br>
&gt; &gt; link is being chosen<br>
&gt; &gt; for small messages, which does lower performance significantly.  The<br>
&gt; &gt; second problem<br>
&gt; &gt; is that I don&#39;t think there are parameters to allow for tuning of multiple<br>
&gt; &gt; openib btl&#39;s<br>
&gt; &gt; to manually select one over the other.<br>
&gt; &gt;<br>
&gt; &gt;                        Dave<br>
&gt; &gt;<br>
&gt; &gt; On Fri, Feb 6, 2015 at 8:24 PM, Gilles Gouaillardet &lt;<br>
&gt; &gt; <a href="mailto:gilles.gouaillardet@gmail.com" target="_blank">gilles.gouaillardet@gmail.com</a>&gt; wrote:<br>
&gt; &gt;<br>
&gt; &gt;&gt; Dave,<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; These settings tell ompi to use native infiniband on the ib qdr port and<br>
&gt; &gt;&gt; tcpo/ip on the other port.<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; From the faq, roce is implemented in the openib btl<br>
&gt; &gt;&gt; <a href="http://www.open-mpi.org/faq/?category=openfabrics#ompi-over-roce" target="_blank">http://www.open-mpi.org/faq/?category=openfabrics#ompi-over-roce</a><br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; Did you use<br>
&gt; &gt;&gt; --mca btl_openib_cpc_include rdmacm<br>
&gt; &gt;&gt; in your first tests ?<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; I had some second thougths about the bandwidth values, and imho they<br>
&gt; &gt;&gt; should be 327680 and 81920 because of the 8/10 encoding<br>
&gt; &gt;&gt; (And that being said, that should not change the measured performance)<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; Also, could you try again by forcing the same btl_tcp_latency and<br>
&gt; &gt;&gt; btl_openib_latency ?<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; Cheers,<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; Gilles<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; Dave Turner &lt;<a href="mailto:drdaveturner@gmail.com" target="_blank">drdaveturner@gmail.com</a>&gt; wrote:<br>
&gt; &gt;&gt; George,<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;      I can check with my guys on Monday but I think the bandwidth<br>
&gt; &gt;&gt; parameters<br>
&gt; &gt;&gt; are the defaults.  I did alter these to 40960 and 10240 as someone else<br>
&gt; &gt;&gt; suggested to me.  The attached graph shows the base red line, along with<br>
&gt; &gt;&gt; the manual balanced blue line and auto balanced green line (0&#39;s for both).<br>
&gt; &gt;&gt; This shift lower suggests to me that the higher TCP latency is being<br>
&gt; &gt;&gt; pulled in.<br>
&gt; &gt;&gt; I&#39;m not sure why the curves are shifted right.<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;                         Dave<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; On Fri, Feb 6, 2015 at 5:32 PM, George Bosilca &lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank">bosilca@icl.utk.edu</a>&gt;<br>
&gt; &gt;&gt; wrote:<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;&gt; Dave,<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; Based on your ompi_info.all the following bandwidth are reported on your<br>
&gt; &gt;&gt;&gt; system:<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;                 MCA btl: parameter &quot;btl_openib_bandwidth&quot; (current<br>
&gt; &gt;&gt;&gt; value: &quot;4&quot;, data source: default, level: 5 tuner/detail, type: unsigned)<br>
&gt; &gt;&gt;&gt;                           Approximate maximum bandwidth of interconnect<br>
&gt; &gt;&gt;&gt; (0 = auto-detect value at run-time [not supported in all BTL modules], &gt;= 1<br>
&gt; &gt;&gt;&gt; = bandwidth in Mbps)<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;                  MCA btl: parameter &quot;btl_tcp_bandwidth&quot; (current value:<br>
&gt; &gt;&gt;&gt; &quot;100&quot;, data source: default, level: 5 tuner/detail, type: unsigned)<br>
&gt; &gt;&gt;&gt;                           Approximate maximum bandwidth of interconnect<br>
&gt; &gt;&gt;&gt; (0 = auto-detect value at run-time [not supported in all BTL modules], &gt;= 1<br>
&gt; &gt;&gt;&gt; = bandwidth in Mbps)<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; This basically states that on your system the default values for these<br>
&gt; &gt;&gt;&gt; parameters are wrong, your TCP network being much faster than the IB. This<br>
&gt; &gt;&gt;&gt; explains the somewhat unexpected decision of OMPI.<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; As a possible solution I suggest you set these bandwidth values to<br>
&gt; &gt;&gt;&gt; something more meaningful (directly in your configuration file). As an<br>
&gt; &gt;&gt;&gt; example,<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; btl_openib_bandwidth = 40000<br>
&gt; &gt;&gt;&gt; btl_tcp_bandwidth = 10000<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; make more sense based on your HPC system description.<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;   George.<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; On Fri, Feb 6, 2015 at 5:37 PM, Dave Turner &lt;<a href="mailto:drdaveturner@gmail.com" target="_blank">drdaveturner@gmail.com</a>&gt;<br>
&gt; &gt;&gt;&gt; wrote:<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;      We have nodes in our HPC system that have 2 NIC&#39;s,<br>
&gt; &gt;&gt;&gt;&gt; one being QDR IB and the second being a slower 10 Gbps card<br>
&gt; &gt;&gt;&gt;&gt; configured for both RoCE and TCP.  Aggregate bandwidth<br>
&gt; &gt;&gt;&gt;&gt; tests with 20 cores on one node yelling at 20 cores on a second<br>
&gt; &gt;&gt;&gt;&gt; node (attached roce.ib.aggregate.pdf) show that without tuning<br>
&gt; &gt;&gt;&gt;&gt; the slower RoCE interface is being used for small messages<br>
&gt; &gt;&gt;&gt;&gt; then QDR IB is used for larger messages (red line).  Tuning<br>
&gt; &gt;&gt;&gt;&gt; the tcp_exclusivity to 1024 to match the openib_exclusivity<br>
&gt; &gt;&gt;&gt;&gt; adds another 20 Gbps of bidirectional bandwidth to the high end (green<br>
&gt; &gt;&gt;&gt;&gt; line),<br>
&gt; &gt;&gt;&gt;&gt; and I&#39;m guessing this is TCP traffic and not RoCE.<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;      So by default the slower interface is being chosen on the low end,<br>
&gt; &gt;&gt;&gt;&gt; and<br>
&gt; &gt;&gt;&gt;&gt; I don&#39;t think there are tunable parameters to allow me to choose the<br>
&gt; &gt;&gt;&gt;&gt; QDR interface as the default.  Going forward we&#39;ll probably just<br>
&gt; &gt;&gt;&gt;&gt; disable<br>
&gt; &gt;&gt;&gt;&gt; RoCE on these nodes and go with QDR IB plus 10 Gbps TCP for large<br>
&gt; &gt;&gt;&gt;&gt; messages.<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;       However, I do think these issues will come up more in the future.<br>
&gt; &gt;&gt;&gt;&gt; With the low latency of RoCE matching IB, there are more opportunities<br>
&gt; &gt;&gt;&gt;&gt; to do channel bonding or allowing multiple interfaces for aggregate<br>
&gt; &gt;&gt;&gt;&gt; traffic<br>
&gt; &gt;&gt;&gt;&gt; for even smaller message sizes.<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;                 Dave Turner<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt; --<br>
&gt; &gt;&gt;&gt;&gt; Work:     <a href="mailto:DaveTurner@ksu.edu" target="_blank">DaveTurner@ksu.edu</a>     <a href="tel:%28785%29%20532-7791" value="+17855327791" target="_blank">(785) 532-7791</a><br>
&gt; &gt;&gt;&gt;&gt;              118 Nichols Hall, Manhattan KS  66502<br>
&gt; &gt;&gt;&gt;&gt; Home:    <a href="mailto:DrDaveTurner@gmail.com" target="_blank">DrDaveTurner@gmail.com</a><br>
&gt; &gt;&gt;&gt;&gt;               cell: <a href="tel:%28785%29%20770-5929" value="+17857705929" target="_blank">(785) 770-5929</a><br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt; _______________________________________________<br>
&gt; &gt;&gt;&gt;&gt; devel mailing list<br>
&gt; &gt;&gt;&gt;&gt; <a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
&gt; &gt;&gt;&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; &gt;&gt;&gt;&gt; Link to this post:<br>
&gt; &gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/community/lists/devel/2015/02/16951.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/02/16951.php</a><br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; --<br>
&gt; &gt;&gt; Work:     <a href="mailto:DaveTurner@ksu.edu" target="_blank">DaveTurner@ksu.edu</a>     <a href="tel:%28785%29%20532-7791" value="+17855327791" target="_blank">(785) 532-7791</a><br>
&gt; &gt;&gt;              118 Nichols Hall, Manhattan KS  66502<br>
&gt; &gt;&gt; Home:    <a href="mailto:DrDaveTurner@gmail.com" target="_blank">DrDaveTurner@gmail.com</a><br>
&gt; &gt;&gt;               cell: <a href="tel:%28785%29%20770-5929" value="+17857705929" target="_blank">(785) 770-5929</a><br>
&gt; &gt;&gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; --<br>
&gt; &gt; Work:     <a href="mailto:DaveTurner@ksu.edu" target="_blank">DaveTurner@ksu.edu</a>     <a href="tel:%28785%29%20532-7791" value="+17855327791" target="_blank">(785) 532-7791</a><br>
&gt; &gt;              118 Nichols Hall, Manhattan KS  66502<br>
&gt; &gt; Home:    <a href="mailto:DrDaveTurner@gmail.com" target="_blank">DrDaveTurner@gmail.com</a><br>
&gt; &gt;               cell: <a href="tel:%28785%29%20770-5929" value="+17857705929" target="_blank">(785) 770-5929</a><br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; devel mailing list<br>
&gt; &gt; <a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
&gt; &gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; &gt; Link to this post:<br>
&gt; &gt; <a href="http://www.open-mpi.org/community/lists/devel/2015/02/16963.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/02/16963.php</a><br>
&gt; &gt;<br>
&gt; -------------- next part --------------<br>
&gt; HTML attachment scrubbed and removed<br>
&gt;<br>
&gt; ------------------------------<br>
&gt;<br>
&gt; Message: 2<br>
&gt; Date: Tue, 10 Feb 2015 20:34:59 -0700<br>
&gt; From: Howard Pritchard &lt;<a href="mailto:hppritcha@gmail.com" target="_blank">hppritcha@gmail.com</a>&gt;<br>
&gt; To: Open MPI Developers &lt;<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a>&gt;<br>
&gt; Subject: Re: [OMPI devel] OMPI devel] RoCE plus QDR IB tunable<br>
&gt;         parameters<br>
&gt; Message-ID:<br>
&gt;         &lt;CAF1Cqj5=GPfi=t8Jw6SSUBKjqut0ChgntTyXfU0diM=<a href="mailto:MXs%2B9Yw@mail.gmail.com" target="_blank">MXs+9Yw@mail.gmail.com</a>&gt;<br>
&gt; Content-Type: text/plain; charset=&quot;utf-8&quot;<br>
&gt;<br>
&gt; HI George,<br>
&gt;<br>
&gt; I&#39;d say commit cf377db82 explains the vanishing of the bandwidth metric as<br>
&gt; well as the mis-labeling of the latency metric.<br>
&gt;<br>
&gt; Howard<br>
&gt;<br>
&gt;<br>
&gt; 2015-02-10 18:41 GMT-07:00 George Bosilca &lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank">bosilca@icl.utk.edu</a>&gt;:<br>
&gt;<br>
&gt; &gt; Somehow one of the most basic information about the capabilities of the<br>
&gt; &gt; BTLs (bandwidth) disappeared from the MCA parameters and the one left<br>
&gt; &gt; (latency) was mislabeled. This mishap not only prevented the communication<br>
&gt; &gt; engine from correctly ordering the BTL for small messages (the latency<br>
&gt; &gt; bound part), but also introduced undesirable bias on the load-balance<br>
&gt; &gt; between multiple devices logic (the bandwidth part).<br>
&gt; &gt;<br>
&gt; &gt; I just pushed a fix  in master<br>
&gt; &gt; <a href="https://github.com/open-mpi/ompi/commit/e173f9b0c0c63c3ea24b8d8bc0ebafe1f1736acb" target="_blank">https://github.com/open-mpi/ompi/commit/e173f9b0c0c63c3ea24b8d8bc0ebafe1f1736acb</a>.<br>
&gt; &gt; Once validated this should be moved over the 1.8 branch.<br>
&gt; &gt;<br>
&gt; &gt; Dave do you think it is possible to renew your experiment with the current<br>
&gt; &gt; master ?<br>
&gt; &gt;<br>
&gt; &gt;   Thanks,<br>
&gt; &gt;     George.<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; On Mon, Feb 9, 2015 at 2:57 PM, Dave Turner &lt;<a href="mailto:drdaveturner@gmail.com" target="_blank">drdaveturner@gmail.com</a>&gt;<br>
&gt; &gt; wrote:<br>
&gt; &gt;<br>
&gt; &gt;&gt; Gilles,<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;      I tried running with btl_openib_cpc_include rdmacm and saw no<br>
&gt; &gt;&gt; change.<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;       Let&#39;s simplify the problem by forgetting about the channel bonding.<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; If I just do an aggregate test of 16 cores on one machine talking to 16 on<br>
&gt; &gt;&gt; a second machine without any settings changed from the default install<br>
&gt; &gt;&gt; of OpenMPI, I see that RoCE over the 10 Gbps link is used for small<br>
&gt; &gt;&gt; messages<br>
&gt; &gt;&gt; then it switches over to QDR IB for large messages.  I don&#39;t see channel<br>
&gt; &gt;&gt; bonding<br>
&gt; &gt;&gt; for large messages, but can turn this on with the btl_tcp_exclusivity<br>
&gt; &gt;&gt; parameter.<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;      I think there are 2 problems here, both related to the fact that QDR<br>
&gt; &gt;&gt; IB link and RoCE<br>
&gt; &gt;&gt; both use the same openib btl.  The first problem is that the slower RoCE<br>
&gt; &gt;&gt; link is being chosen<br>
&gt; &gt;&gt; for small messages, which does lower performance significantly.  The<br>
&gt; &gt;&gt; second problem<br>
&gt; &gt;&gt; is that I don&#39;t think there are parameters to allow for tuning of<br>
&gt; &gt;&gt; multiple openib btl&#39;s<br>
&gt; &gt;&gt; to manually select one over the other.<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;                        Dave<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; On Fri, Feb 6, 2015 at 8:24 PM, Gilles Gouaillardet &lt;<br>
&gt; &gt;&gt; <a href="mailto:gilles.gouaillardet@gmail.com" target="_blank">gilles.gouaillardet@gmail.com</a>&gt; wrote:<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;&gt; Dave,<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; These settings tell ompi to use native infiniband on the ib qdr port and<br>
&gt; &gt;&gt;&gt; tcpo/ip on the other port.<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; From the faq, roce is implemented in the openib btl<br>
&gt; &gt;&gt;&gt; <a href="http://www.open-mpi.org/faq/?category=openfabrics#ompi-over-roce" target="_blank">http://www.open-mpi.org/faq/?category=openfabrics#ompi-over-roce</a><br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; Did you use<br>
&gt; &gt;&gt;&gt; --mca btl_openib_cpc_include rdmacm<br>
&gt; &gt;&gt;&gt; in your first tests ?<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; I had some second thougths about the bandwidth values, and imho they<br>
&gt; &gt;&gt;&gt; should be 327680 and 81920 because of the 8/10 encoding<br>
&gt; &gt;&gt;&gt; (And that being said, that should not change the measured performance)<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; Also, could you try again by forcing the same btl_tcp_latency and<br>
&gt; &gt;&gt;&gt; btl_openib_latency ?<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; Cheers,<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; Gilles<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; Dave Turner &lt;<a href="mailto:drdaveturner@gmail.com" target="_blank">drdaveturner@gmail.com</a>&gt; wrote:<br>
&gt; &gt;&gt;&gt; George,<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;      I can check with my guys on Monday but I think the bandwidth<br>
&gt; &gt;&gt;&gt; parameters<br>
&gt; &gt;&gt;&gt; are the defaults.  I did alter these to 40960 and 10240 as someone else<br>
&gt; &gt;&gt;&gt; suggested to me.  The attached graph shows the base red line, along with<br>
&gt; &gt;&gt;&gt; the manual balanced blue line and auto balanced green line (0&#39;s for<br>
&gt; &gt;&gt;&gt; both).<br>
&gt; &gt;&gt;&gt; This shift lower suggests to me that the higher TCP latency is being<br>
&gt; &gt;&gt;&gt; pulled in.<br>
&gt; &gt;&gt;&gt; I&#39;m not sure why the curves are shifted right.<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;                         Dave<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; On Fri, Feb 6, 2015 at 5:32 PM, George Bosilca &lt;<a href="mailto:bosilca@icl.utk.edu" target="_blank">bosilca@icl.utk.edu</a>&gt;<br>
&gt; &gt;&gt;&gt; wrote:<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt; Dave,<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt; Based on your ompi_info.all the following bandwidth are reported on<br>
&gt; &gt;&gt;&gt;&gt; your system:<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;                 MCA btl: parameter &quot;btl_openib_bandwidth&quot; (current<br>
&gt; &gt;&gt;&gt;&gt; value: &quot;4&quot;, data source: default, level: 5 tuner/detail, type: unsigned)<br>
&gt; &gt;&gt;&gt;&gt;                           Approximate maximum bandwidth of interconnect<br>
&gt; &gt;&gt;&gt;&gt; (0 = auto-detect value at run-time [not supported in all BTL modules], &gt;= 1<br>
&gt; &gt;&gt;&gt;&gt; = bandwidth in Mbps)<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;                  MCA btl: parameter &quot;btl_tcp_bandwidth&quot; (current value:<br>
&gt; &gt;&gt;&gt;&gt; &quot;100&quot;, data source: default, level: 5 tuner/detail, type: unsigned)<br>
&gt; &gt;&gt;&gt;&gt;                           Approximate maximum bandwidth of interconnect<br>
&gt; &gt;&gt;&gt;&gt; (0 = auto-detect value at run-time [not supported in all BTL modules], &gt;= 1<br>
&gt; &gt;&gt;&gt;&gt; = bandwidth in Mbps)<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt; This basically states that on your system the default values for these<br>
&gt; &gt;&gt;&gt;&gt; parameters are wrong, your TCP network being much faster than the IB. This<br>
&gt; &gt;&gt;&gt;&gt; explains the somewhat unexpected decision of OMPI.<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt; As a possible solution I suggest you set these bandwidth values to<br>
&gt; &gt;&gt;&gt;&gt; something more meaningful (directly in your configuration file). As an<br>
&gt; &gt;&gt;&gt;&gt; example,<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt; btl_openib_bandwidth = 40000<br>
&gt; &gt;&gt;&gt;&gt; btl_tcp_bandwidth = 10000<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt; make more sense based on your HPC system description.<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;   George.<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt; On Fri, Feb 6, 2015 at 5:37 PM, Dave Turner &lt;<a href="mailto:drdaveturner@gmail.com" target="_blank">drdaveturner@gmail.com</a>&gt;<br>
&gt; &gt;&gt;&gt;&gt; wrote:<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;&gt;      We have nodes in our HPC system that have 2 NIC&#39;s,<br>
&gt; &gt;&gt;&gt;&gt;&gt; one being QDR IB and the second being a slower 10 Gbps card<br>
&gt; &gt;&gt;&gt;&gt;&gt; configured for both RoCE and TCP.  Aggregate bandwidth<br>
&gt; &gt;&gt;&gt;&gt;&gt; tests with 20 cores on one node yelling at 20 cores on a second<br>
&gt; &gt;&gt;&gt;&gt;&gt; node (attached roce.ib.aggregate.pdf) show that without tuning<br>
&gt; &gt;&gt;&gt;&gt;&gt; the slower RoCE interface is being used for small messages<br>
&gt; &gt;&gt;&gt;&gt;&gt; then QDR IB is used for larger messages (red line).  Tuning<br>
&gt; &gt;&gt;&gt;&gt;&gt; the tcp_exclusivity to 1024 to match the openib_exclusivity<br>
&gt; &gt;&gt;&gt;&gt;&gt; adds another 20 Gbps of bidirectional bandwidth to the high end (green<br>
&gt; &gt;&gt;&gt;&gt;&gt; line),<br>
&gt; &gt;&gt;&gt;&gt;&gt; and I&#39;m guessing this is TCP traffic and not RoCE.<br>
&gt; &gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;&gt;      So by default the slower interface is being chosen on the low<br>
&gt; &gt;&gt;&gt;&gt;&gt; end, and<br>
&gt; &gt;&gt;&gt;&gt;&gt; I don&#39;t think there are tunable parameters to allow me to choose the<br>
&gt; &gt;&gt;&gt;&gt;&gt; QDR interface as the default.  Going forward we&#39;ll probably just<br>
&gt; &gt;&gt;&gt;&gt;&gt; disable<br>
&gt; &gt;&gt;&gt;&gt;&gt; RoCE on these nodes and go with QDR IB plus 10 Gbps TCP for large<br>
&gt; &gt;&gt;&gt;&gt;&gt; messages.<br>
&gt; &gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;&gt;       However, I do think these issues will come up more in the future.<br>
&gt; &gt;&gt;&gt;&gt;&gt; With the low latency of RoCE matching IB, there are more opportunities<br>
&gt; &gt;&gt;&gt;&gt;&gt; to do channel bonding or allowing multiple interfaces for aggregate<br>
&gt; &gt;&gt;&gt;&gt;&gt; traffic<br>
&gt; &gt;&gt;&gt;&gt;&gt; for even smaller message sizes.<br>
&gt; &gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;&gt;                 Dave Turner<br>
&gt; &gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;&gt; --<br>
&gt; &gt;&gt;&gt;&gt;&gt; Work:     <a href="mailto:DaveTurner@ksu.edu" target="_blank">DaveTurner@ksu.edu</a>     <a href="tel:%28785%29%20532-7791" value="+17855327791" target="_blank">(785) 532-7791</a><br>
&gt; &gt;&gt;&gt;&gt;&gt;              118 Nichols Hall, Manhattan KS  66502<br>
&gt; &gt;&gt;&gt;&gt;&gt; Home:    <a href="mailto:DrDaveTurner@gmail.com" target="_blank">DrDaveTurner@gmail.com</a><br>
&gt; &gt;&gt;&gt;&gt;&gt;               cell: <a href="tel:%28785%29%20770-5929" value="+17857705929" target="_blank">(785) 770-5929</a><br>
&gt; &gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;&gt; _______________________________________________<br>
&gt; &gt;&gt;&gt;&gt;&gt; devel mailing list<br>
&gt; &gt;&gt;&gt;&gt;&gt; <a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
&gt; &gt;&gt;&gt;&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; &gt;&gt;&gt;&gt;&gt; Link to this post:<br>
&gt; &gt;&gt;&gt;&gt;&gt; <a href="http://www.open-mpi.org/community/lists/devel/2015/02/16951.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/02/16951.php</a><br>
&gt; &gt;&gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;&gt; --<br>
&gt; &gt;&gt;&gt; Work:     <a href="mailto:DaveTurner@ksu.edu" target="_blank">DaveTurner@ksu.edu</a>     <a href="tel:%28785%29%20532-7791" value="+17855327791" target="_blank">(785) 532-7791</a><br>
&gt; &gt;&gt;&gt;              118 Nichols Hall, Manhattan KS  66502<br>
&gt; &gt;&gt;&gt; Home:    <a href="mailto:DrDaveTurner@gmail.com" target="_blank">DrDaveTurner@gmail.com</a><br>
&gt; &gt;&gt;&gt;               cell: <a href="tel:%28785%29%20770-5929" value="+17857705929" target="_blank">(785) 770-5929</a><br>
&gt; &gt;&gt;&gt;<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; --<br>
&gt; &gt;&gt; Work:     <a href="mailto:DaveTurner@ksu.edu" target="_blank">DaveTurner@ksu.edu</a>     <a href="tel:%28785%29%20532-7791" value="+17855327791" target="_blank">(785) 532-7791</a><br>
&gt; &gt;&gt;              118 Nichols Hall, Manhattan KS  66502<br>
&gt; &gt;&gt; Home:    <a href="mailto:DrDaveTurner@gmail.com" target="_blank">DrDaveTurner@gmail.com</a><br>
&gt; &gt;&gt;               cell: <a href="tel:%28785%29%20770-5929" value="+17857705929" target="_blank">(785) 770-5929</a><br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; _______________________________________________<br>
&gt; &gt;&gt; devel mailing list<br>
&gt; &gt;&gt; <a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
&gt; &gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; &gt;&gt; Link to this post:<br>
&gt; &gt;&gt; <a href="http://www.open-mpi.org/community/lists/devel/2015/02/16963.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/02/16963.php</a><br>
&gt; &gt;&gt;<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; devel mailing list<br>
&gt; &gt; <a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
&gt; &gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; &gt; Link to this post:<br>
&gt; &gt; <a href="http://www.open-mpi.org/community/lists/devel/2015/02/16965.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/02/16965.php</a><br>
&gt; &gt;<br>
&gt; -------------- next part --------------<br>
&gt; HTML attachment scrubbed and removed<br>
&gt;<br>
&gt; ------------------------------<br>
&gt;<br>
&gt; Subject: Digest Footer<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt;<br>
&gt; ------------------------------<br>
&gt;<br>
&gt; End of devel Digest, Vol 2917, Issue 1<br>
&gt; **************************************<br>
&gt;<br>
&gt;<br>
<span><font color="#888888">&gt;<br>
&gt; --<br>
&gt; Work:     <a href="mailto:DaveTurner@ksu.edu" target="_blank">DaveTurner@ksu.edu</a>     <a href="tel:%28785%29%20532-7791" value="+17855327791" target="_blank">(785) 532-7791</a><br>
&gt;              118 Nichols Hall, Manhattan KS  66502<br>
&gt; Home:    <a href="mailto:DrDaveTurner@gmail.com" target="_blank">DrDaveTurner@gmail.com</a><br>
&gt;               cell: <a href="tel:%28785%29%20770-5929" value="+17857705929" target="_blank">(785) 770-5929</a><br>
&gt; &lt;unidirectional.pdf&gt;&lt;bidirectional.pdf&gt;&lt;aggregate.pdf&gt;_______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/02/17004.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/02/17004.php</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
</font></span></blockquote></div><br><br clear="all"><div><br></div>-- <br><div><div dir="ltr">Work:     <a href="mailto:DaveTurner@ksu.edu" target="_blank">DaveTurner@ksu.edu</a>     <a href="tel:%28785%29%20532-7791" value="+17855327791" target="_blank">(785) 532-7791</a><div>             118 Nichols Hall, Manhattan KS  66502<br>Home:    <a href="mailto:DrDaveTurner@gmail.com" target="_blank">DrDaveTurner@gmail.com</a><br>              cell: <a href="tel:%28785%29%20770-5929" value="+17857705929" target="_blank">(785) 770-5929</a><br></div></div></div>
</div>
</div></div><br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/02/17022.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/02/17022.php</a><br></blockquote></div><br></div>

