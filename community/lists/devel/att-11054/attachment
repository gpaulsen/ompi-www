<div dir="ltr"><div>or, lstopo lies (Im not using the latest hwloc but one which comes with distro).</div><div>The machine has two dual-code sockets, total 4 physical cores:</div><div>processor       : 0<br>vendor_id       : GenuineIntel<br>

cpu family      : 6<br>model           : 45<br>model name      : Intel(R) Xeon(R) CPU E5-2650 0 @ 2.00GHz<br>stepping        : 7<br>cpu MHz         : 1200.000<br>cache size      : 20480 KB<br>physical id     : 0<br>siblings        : 2<br>

core id         : 0<br>cpu cores       : 1<br>apicid          : 0<br>initial apicid  : 0<br>fpu             : yes<br>fpu_exception   : yes<br>cpuid level     : 13<br>wp              : yes<br>flags           : fpu vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xto<br>

pology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 x2apic popcnt aes xsave avx lahf_lm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi fle<br>xpriority ept vpid<br>

bogomips        : 4000.12<br>clflush size    : 64<br>cache_alignment : 64<br>address sizes   : 46 bits physical, 48 bits virtual<br>power management:<br></div><div>processor       : 1<br>vendor_id       : GenuineIntel<br>

cpu family      : 6<br>model           : 45<br>model name      : Intel(R) Xeon(R) CPU E5-2650 0 @ 2.00GHz<br>stepping        : 7<br>cpu MHz         : 1200.000<br>cache size      : 20480 KB<br>physical id     : 1<br>siblings        : 2<br>

core id         : 0<br>cpu cores       : 1<br>apicid          : 32<br>initial apicid  : 32<br>fpu             : yes<br>fpu_exception   : yes<br>cpuid level     : 13<br>wp              : yes<br>flags           : fpu vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xto<br>

pology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 x2apic popcnt aes xsave avx lahf_lm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi fle<br>xpriority ept vpid<br>

bogomips        : 3999.44<br>clflush size    : 64<br>cache_alignment : 64<br>address sizes   : 46 bits physical, 48 bits virtual<br>power management:<br></div><div>processor       : 2<br>vendor_id       : GenuineIntel<br>

cpu family      : 6<br>model           : 45<br>model name      : Intel(R) Xeon(R) CPU E5-2650 0 @ 2.00GHz<br>stepping        : 7<br>cpu MHz         : 1200.000<br>cache size      : 20480 KB<br>physical id     : 0<br>siblings        : 2<br>

core id         : 0<br>cpu cores       : 1<br>apicid          : 1<br>initial apicid  : 1<br>fpu             : yes<br>fpu_exception   : yes<br>cpuid level     : 13<br>wp              : yes<br>flags           : fpu vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xto<br>

pology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 x2apic popcnt aes xsave avx lahf_lm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi fle<br>xpriority ept vpid<br>

bogomips        : 3999.42<br>clflush size    : 64<br>cache_alignment : 64<br>address sizes   : 46 bits physical, 48 bits virtual<br>power management:<br></div><div>processor       : 3<br>vendor_id       : GenuineIntel<br>

cpu family      : 6<br>model           : 45<br>model name      : Intel(R) Xeon(R) CPU E5-2650 0 @ 2.00GHz<br>stepping        : 7<br>cpu MHz         : 1200.000<br>cache size      : 20480 KB<br>physical id     : 1<br>siblings        : 2<br>

core id         : 0<br>cpu cores       : 1<br>apicid          : 33<br>initial apicid  : 33<br>fpu             : yes<br>fpu_exception   : yes<br>cpuid level     : 13<br>wp              : yes<br>flags           : fpu vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 x2apic popcnt aes xsave avx lahf_lm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid<br>

bogomips        : 3999.44<br>clflush size    : 64<br>cache_alignment : 64<br>address sizes   : 46 bits physical, 48 bits virtual<br>power management:<br><br><br></div><div class="gmail_quote">On Wed, May 30, 2012 at 3:40 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>

<blockquote style="margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid" class="gmail_quote"><div style="word-wrap:break-word">Hmmm...well, from what I see, mpirun was actually giving you the right answer! I only see TWO cores on each node, yet you told it to bind FOUR processes on each node, each proc to be bound to a unique core.<div>

<br></div><div>The error message was correct - there are not enough cores on those nodes to do what you requested.</div><div><br></div><div><br><div><div><div class="h5"><div>On May 30, 2012, at 6:19 AM, Mike Dubman wrote:</div>

<br></div></div><blockquote type="cite"><div><div class="h5"><div dir="ltr">attached.<br><br><div class="gmail_quote">On Wed, May 30, 2012 at 2:32 PM, Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br>

<blockquote style="margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid" class="gmail_quote">

<div>On May 30, 2012, at 7:20 AM, Jeff Squyres wrote:<br>
<br>
&gt;&gt; $hwloc-ls --of console<br>
&gt;&gt; Machine (32GB)<br>
&gt;&gt;  NUMANode L#0 (P#0 16GB) + Socket L#0 + L3 L#0 (20MB) + L2 L#0 (256KB) + L1 L#0 (32KB) + Core L#0<br>
&gt;&gt;    PU L#0 (P#0)<br>
&gt;&gt;    PU L#1 (P#2)<br>
&gt;&gt;  NUMANode L#1 (P#1 16GB) + Socket L#1 + L3 L#1 (20MB) + L2 L#1 (256KB) + L1 L#1 (32KB) + Core L#1<br>
&gt;&gt;    PU L#2 (P#1)<br>
&gt;&gt;    PU L#3 (P#3)<br>
&gt;<br>
&gt; Is this hwloc output exactly the same on both nodes?<br>
<br>
<br>
</div>More specifically, can you send the lstopo xml output from each of the 2 nodes you ran on?<br>
<div><div><br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</div></div></blockquote></div><br></div>
</div></div><span>&lt;lstopo-out.tbz&gt;</span>_______________________________________________<div class="im"><br>devel mailing list<br><a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></div>

</blockquote></div><br></div></div><br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br></blockquote></div><br></div>

