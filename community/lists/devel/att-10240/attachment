<html xmlns:v="urn:schemas-microsoft-com:vml" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:w="urn:schemas-microsoft-com:office:word" xmlns:m="http://schemas.microsoft.com/office/2004/12/omml" xmlns="http://www.w3.org/TR/REC-html40"><head><meta http-equiv=Content-Type content="text/html; charset=us-ascii"><meta name=Generator content="Microsoft Word 14 (filtered medium)"><base href="x-msg://102/"><style><!--
/* Font Definitions */
@font-face
	{font-family:Helvetica;
	panose-1:2 11 6 4 2 2 2 2 2 4;}
@font-face
	{font-family:Wingdings;
	panose-1:5 0 0 0 0 0 0 0 0 0;}
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:Tahoma;
	panose-1:2 11 6 4 3 5 4 4 2 4;}
@font-face
	{font-family:Consolas;
	panose-1:2 11 6 9 2 2 4 3 2 4;}
/* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin:0in;
	margin-bottom:.0001pt;
	font-size:12.0pt;
	font-family:"Times New Roman","serif";}
a:link, span.MsoHyperlink
	{mso-style-priority:99;
	color:blue;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{mso-style-priority:99;
	color:purple;
	text-decoration:underline;}
pre
	{mso-style-priority:99;
	mso-style-link:"HTML Preformatted Char";
	margin:0in;
	margin-bottom:.0001pt;
	font-size:10.0pt;
	font-family:"Courier New";}
p.MsoAcetate, li.MsoAcetate, div.MsoAcetate
	{mso-style-priority:99;
	mso-style-link:"Balloon Text Char";
	margin:0in;
	margin-bottom:.0001pt;
	font-size:8.0pt;
	font-family:"Tahoma","sans-serif";}
span.apple-converted-space
	{mso-style-name:apple-converted-space;}
span.HTMLPreformattedChar
	{mso-style-name:"HTML Preformatted Char";
	mso-style-priority:99;
	mso-style-link:"HTML Preformatted";
	font-family:Consolas;}
span.EmailStyle20
	{mso-style-type:personal-reply;
	font-family:"Calibri","sans-serif";
	color:#1F497D;}
span.BalloonTextChar
	{mso-style-name:"Balloon Text Char";
	mso-style-priority:99;
	mso-style-link:"Balloon Text";
	font-family:"Tahoma","sans-serif";}
.MsoChpDefault
	{mso-style-type:export-only;
	font-size:10.0pt;}
@page WordSection1
	{size:8.5in 11.0in;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
	{page:WordSection1;}
--></style><!--[if gte mso 9]><xml>
<o:shapedefaults v:ext="edit" spidmax="1026" />
</xml><![endif]--><!--[if gte mso 9]><xml>
<o:shapelayout v:ext="edit">
<o:idmap v:ext="edit" data="1" />
</o:shapelayout></xml><![endif]--></head><body lang=EN-US link=blue vlink=purple><div class=WordSection1><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>You can tell it is working because your program does not hang anymore </span><span style='font-size:11.0pt;font-family:Wingdings;color:#1F497D'>J</span><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>&nbsp; Otherwise, there is a not a way that I am aware of.<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>Rolf<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>PS: And I assume you mean Open MPI under your third bullet below.<o:p></o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><div style='border:none;border-left:solid blue 1.5pt;padding:0in 0in 0in 4.0pt'><div><div style='border:none;border-top:solid #B5C4DF 1.0pt;padding:3.0pt 0in 0in 0in'><p class=MsoNormal><b><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'>From:</span></b><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'> devel-bounces@open-mpi.org [mailto:devel-bounces@open-mpi.org] <b>On Behalf Of </b>Sebastian Rinke<br><b>Sent:</b> Friday, January 20, 2012 12:21 PM<br><b>To:</b> Open MPI Developers<br><b>Subject:</b> Re: [OMPI devel] GPUDirect v1 issues<o:p></o:p></span></p></div></div><p class=MsoNormal><o:p>&nbsp;</o:p></p><div><p class=MsoNormal>With&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>* MLNX OFED stack tailored for GPUDirect<o:p></o:p></p></div><div><p class=MsoNormal>* RHEL + kernel patch&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal>* MVAPICH2&nbsp;<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>it is possible to monitor GPUDirect v1 activities by means of observing changes to values in<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>* /sys/module/ib_core/parameters/gpu_direct_pages<o:p></o:p></p></div><div><p class=MsoNormal>* /sys/module/ib_core/parameters/gpu_direct_shares<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>By setting CUDA_NIC_INTEROP=1 there are no changes anymore.<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>Is there a different way now to monitor if GPUDirect actually works?<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>Sebastian.<o:p></o:p></p></div><p class=MsoNormal><o:p>&nbsp;</o:p></p><div><div><p class=MsoNormal>On Jan 18, 2012, at 5:06 PM, Kenneth Lloyd wrote:<o:p></o:p></p></div><p class=MsoNormal><br><br><o:p></o:p></p><div><div><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>It is documented in<span class=apple-converted-space>&nbsp;</span><a href="http://developer.download.nvidia.com/compute/cuda/4_0/docs/GPUDirect_Technology_Overview.pdf">http://developer.download.nvidia.com/compute/cuda/4_0/docs/GPUDirect_Technology_Overview.pdf</a></span><o:p></o:p></p></div><div><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif"'>set CUDA_NIC_INTEROP=1</span><span style='font-family:"Arial","sans-serif";color:black'><o:p></o:p></span></p></div><div><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>&nbsp;</span><o:p></o:p></p></div><div><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>&nbsp;</span><o:p></o:p></p></div><div><div style='border:none;border-top:solid #B5C4DF 1.0pt;padding:3.0pt 0in 0in 0in;border-width:initial;border-color:initial'><div><p class=MsoNormal><b><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'>From:</span></b><span class=apple-converted-space><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'>&nbsp;</span></span><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'><a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a><span class=apple-converted-space>&nbsp;</span>[mailto:devel-bounces@open-mpi.org]<span class=apple-converted-space>&nbsp;</span><b>On Behalf Of<span class=apple-converted-space>&nbsp;</span></b>Sebastian Rinke<br><b>Sent:</b><span class=apple-converted-space>&nbsp;</span>Wednesday, January 18, 2012 8:15 AM<br><b>To:</b><span class=apple-converted-space>&nbsp;</span>Open MPI Developers<br><b>Subject:</b><span class=apple-converted-space>&nbsp;</span>Re: [OMPI devel] GPUDirect v1 issues</span><o:p></o:p></p></div></div></div><div><p class=MsoNormal>&nbsp;<o:p></o:p></p></div><div><div><p class=MsoNormal>Setting the environment variable fixed the problem for Open MPI with CUDA 4.0. Thanks!<o:p></o:p></p></div></div><div><div><p class=MsoNormal>&nbsp;<o:p></o:p></p></div></div><div><div><p class=MsoNormal>However, I'm wondering why this is not documented in the NVIDIA GPUDirect package.<o:p></o:p></p></div></div><div><div><p class=MsoNormal>&nbsp;<o:p></o:p></p></div></div><div><div><p class=MsoNormal>Sebastian.<o:p></o:p></p></div></div><div><p class=MsoNormal>&nbsp;<o:p></o:p></p></div><div><div><div><p class=MsoNormal>On Jan 18, 2012, at 1:28 AM, Rolf vandeVaart wrote:<o:p></o:p></p></div></div><div><p class=MsoNormal><br><br><br><o:p></o:p></p></div><div><div><div><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>Yes, the step outlined in your second bullet is no longer necessary.&nbsp;</span><o:p></o:p></p></div></div><div><div><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>&nbsp;</span><o:p></o:p></p></div></div><div><div><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>Rolf</span><o:p></o:p></p></div></div><div><div><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>&nbsp;</span><o:p></o:p></p></div></div><div><div><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>&nbsp;</span><o:p></o:p></p></div></div><div style='border:none;border-left:solid windowtext 3.0pt;padding:0in 0in 0in 4.0pt;border-width:initial;border-color:initial;border-width:initial;border-color:initial'><div><div style='border:none;border-top:solid windowtext 3.0pt;padding:3.0pt 0in 0in 0in;border-width:initial;border-color:initial;border-width:initial;border-color:initial'><div><div><p class=MsoNormal><b><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'>From:</span></b><span class=apple-converted-space><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'>&nbsp;</span></span><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'><a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a><span class=apple-converted-space>&nbsp;</span>[<a href="mailto:devel-bounces@open-mpi.org">mailto:devel-bounces@open-mpi.org</a>]<span class=apple-converted-space>&nbsp;</span><b>On Behalf Of<span class=apple-converted-space>&nbsp;</span></b>Sebastian Rinke<br><b>Sent:</b><span class=apple-converted-space>&nbsp;</span>Tuesday, January 17, 2012 5:22 PM<br><b>To:</b><span class=apple-converted-space>&nbsp;</span>Open MPI Developers<br><b>Subject:</b><span class=apple-converted-space>&nbsp;</span>Re: [OMPI devel] GPUDirect v1 issues</span><o:p></o:p></p></div></div></div></div><div><div><p class=MsoNormal><span style='color:black'>&nbsp;</span><o:p></o:p></p></div></div><div><div><p class=MsoNormal><span style='color:black'>Thank you very much. I will try setting the environment variable and if required also use the 4.1 RC2 version.<br><br>To clarify things a little bit for me, to set up my machine with GPUDirect v1 I did the following:<br><br>* Install RHEL 5.4<br>* Use the kernel with GPUDirect support<br>* Use the MLNX OFED stack with GPUDirect support<br>* Install the CUDA developer driver<br><br>Does using CUDA&nbsp; &gt;= 4.0&nbsp; make one of the above steps&nbsp; redundant?<br><br>I.e., RHEL or different kernel or MLNX OFED stack with GPUDirect support is&nbsp; not needed any more?<br><br>Sebastian.<br><br>Rolf vandeVaart wrote:</span><o:p></o:p></p></div></div><pre><span style='color:black'>I ran your test case against Open MPI 1.4.2 and CUDA 4.1 RC2 and it worked fine.&nbsp; I do not have a machine right now where I can load CUDA 4.0 drivers.</span><o:p></o:p></pre><pre><span style='color:black'>Any chance you can try CUDA 4.1 RC2?&nbsp; There were some improvements in the support (you do not need to set an environment variable for one)</span><o:p></o:p></pre><pre><span style='color:black'> <a href="http://developer.nvidia.com/cuda-toolkit-41">http://developer.nvidia.com/cuda-toolkit-41</a></span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>There is also a chance that setting the environment variable as outlined in this link may help you.</span><o:p></o:p></pre><pre><span style='color:black'><a href="http://forums.nvidia.com/index.php?showtopic=200629">http://forums.nvidia.com/index.php?showtopic=200629</a></span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>However, I cannot explain why MVAPICH would work and Open MPI would not.&nbsp; </span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>Rolf</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp; </span><o:p></o:p></pre><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><pre><span style='color:black'>-----Original Message-----</span><o:p></o:p></pre><pre><span style='color:black'>From: <a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a> [<a href="mailto:devel-bounces@open-mpi.org">mailto:devel-bounces@open-mpi.org</a>]</span><o:p></o:p></pre><pre><span style='color:black'>On Behalf Of Sebastian Rinke</span><o:p></o:p></pre><pre><span style='color:black'>Sent: Tuesday, January 17, 2012 12:08 PM</span><o:p></o:p></pre><pre><span style='color:black'>To: Open MPI Developers</span><o:p></o:p></pre><pre><span style='color:black'>Subject: Re: [OMPI devel] GPUDirect v1 issues</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>I use CUDA 4.0 with MVAPICH2 1.5.1p1 and Open MPI 1.4.2.</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>Attached you find a little test case which is based on the GPUDirect v1 test</span><o:p></o:p></pre><pre><span style='color:black'>case (mpi_pinned.c).</span><o:p></o:p></pre><pre><span style='color:black'>In that program the sender splits a message into chunks and sends them</span><o:p></o:p></pre><pre><span style='color:black'>separately to the receiver which posts the corresponding recvs. It is a kind of</span><o:p></o:p></pre><pre><span style='color:black'>pipelining.</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>In mpi_pinned.c:141 the offsets into the recv buffer are set.</span><o:p></o:p></pre><pre><span style='color:black'>For the correct offsets, i.e. increasing them, it blocks with Open MPI.</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>Using line 142 instead (offset = 0) works.</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>The tarball attached contains a Makefile where you will have to adjust</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>* CUDA_INC_DIR</span><o:p></o:p></pre><pre><span style='color:black'>* CUDA_LIB_DIR</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>Sebastian</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>On Jan 17, 2012, at 4:16 PM, Kenneth A. Lloyd wrote:</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'> &nbsp;&nbsp;&nbsp;</span><o:p></o:p></pre><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><pre><span style='color:black'>Also, which version of MVAPICH2 did you use?</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>I've been pouring over Rolf's OpenMPI CUDA RDMA 3 (using CUDA 4.1 r2)</span><o:p></o:p></pre><pre><span style='color:black'>vis MVAPICH-GPU on a small 3 node cluster. These are wickedly interesting.</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>Ken</span><o:p></o:p></pre><pre><span style='color:black'>-----Original Message-----</span><o:p></o:p></pre><pre><span style='color:black'>From: <a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a> [<a href="mailto:devel-bounces@open">mailto:devel-bounces@open</a>-</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><o:p></o:p></pre></blockquote><pre><span style='color:black'>mpi.org]</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;&nbsp;&nbsp; </span><o:p></o:p></pre><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><pre><span style='color:black'>On Behalf Of Rolf vandeVaart</span><o:p></o:p></pre><pre><span style='color:black'>Sent: Tuesday, January 17, 2012 7:54 AM</span><o:p></o:p></pre><pre><span style='color:black'>To: Open MPI Developers</span><o:p></o:p></pre><pre><span style='color:black'>Subject: Re: [OMPI devel] GPUDirect v1 issues</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>I am not aware of any issues.&nbsp; Can you send me a test program and I</span><o:p></o:p></pre><pre><span style='color:black'>can try it out?</span><o:p></o:p></pre><pre><span style='color:black'>Which version of CUDA are you using?</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>Rolf</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><o:p></o:p></pre><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><pre><span style='color:black'>-----Original Message-----</span><o:p></o:p></pre><pre><span style='color:black'>From: <a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a> [<a href="mailto:devel-bounces@open">mailto:devel-bounces@open</a>-</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><o:p></o:p></pre></blockquote></blockquote><pre><span style='color:black'>mpi.org]</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;&nbsp;&nbsp; </span><o:p></o:p></pre><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><pre><span style='color:black'>On Behalf Of Sebastian Rinke</span><o:p></o:p></pre><pre><span style='color:black'>Sent: Tuesday, January 17, 2012 8:50 AM</span><o:p></o:p></pre><pre><span style='color:black'>To: Open MPI Developers</span><o:p></o:p></pre><pre><span style='color:black'>Subject: [OMPI devel] GPUDirect v1 issues</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>Dear all,</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>I'm using GPUDirect v1 with Open MPI 1.4.3 and experience blocking</span><o:p></o:p></pre><pre><span style='color:black'>MPI_SEND/RECV to block forever.</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>For two subsequent MPI_RECV, it hangs if the recv buffer pointer of</span><o:p></o:p></pre><pre><span style='color:black'>the second recv points to somewhere, i.e. not at the beginning, in</span><o:p></o:p></pre><pre><span style='color:black'>the recv buffer (previously allocated with cudaMallocHost()).</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>I tried the same with MVAPICH2 and did not see the problem.</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>Does anybody know about issues with GPUDirect v1 using Open MPI?</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>Thanks for your help,</span><o:p></o:p></pre><pre><span style='color:black'>Sebastian</span><o:p></o:p></pre><pre><span style='color:black'>_______________________________________________</span><o:p></o:p></pre><pre><span style='color:black'>devel mailing list</span><o:p></o:p></pre><pre><span style='color:black'><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a></span><o:p></o:p></pre><pre><span style='color:black'><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><o:p></o:p></pre></blockquote></blockquote></blockquote><pre><span style='color:black'>-----------------------------------------------------------------------------------</span><o:p></o:p></pre><pre><span style='color:black'>This email message is for the sole use of the intended recipient(s) and may contain</span><o:p></o:p></pre><pre><span style='color:black'>confidential information.&nbsp; Any unauthorized review, use, disclosure or distribution</span><o:p></o:p></pre><pre><span style='color:black'>is prohibited.&nbsp; If you are not the intended recipient, please contact the sender by</span><o:p></o:p></pre><pre><span style='color:black'>reply email and destroy all copies of the original message.</span><o:p></o:p></pre><pre><span style='color:black'>-----------------------------------------------------------------------------------</span><o:p></o:p></pre><pre><span style='color:black'>&nbsp;</span><o:p></o:p></pre><pre><span style='color:black'>_______________________________________________</span><o:p></o:p></pre><pre><span style='color:black'>devel mailing list</span><o:p></o:p></pre><pre><span style='color:black'><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a></span><o:p></o:p></pre><pre><span style='color:black'><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></span><o:p></o:p></pre><pre><span style='color:black'>&nbsp; </span><o:p></o:p></pre><div><div><p class=MsoNormal><span style='color:black'>&nbsp;</span><o:p></o:p></p></div></div></div><div><p class=MsoNormal><span style='font-size:13.5pt;font-family:"Helvetica","sans-serif"'>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></span><o:p></o:p></p></div></div></div><div><p class=MsoNormal>&nbsp;<o:p></o:p></p></div><p class=MsoNormal>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><o:p></o:p></p></div></div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div></div></body></html>
