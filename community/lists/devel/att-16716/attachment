<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">It would have to be for 1.8.5 as there is no way to change that configure.m4 without re-releasing.<div class=""><br class=""></div><div class="">We still apparently have a thread-related performance issue as reported by Intel. It appears that we didnâ€™t completely manage to fix the blasted thread locks, leaving some still on by default, thereby causing a roughly 10-15% loss of performance relative to earlier in the 1.8 series. So a 1.8.5 is going to be required fairly soon anyway.</div><div class=""><br class=""></div><div class="">Sigh</div><div class=""><br class=""></div><div class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Dec 22, 2014, at 9:46 AM, Howard Pritchard &lt;<a href="mailto:hppritcha@gmail.com" class="">hppritcha@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class=""><div dir="ltr" class="">I opened an issue <a href="https://github.com/open-mpi/ompi/issues/322" class="">322</a>&nbsp;about this and gave put it on 1.8.5 milestone.<div class="">I'll submit a PR to remove the sn/xpmem.h usage in the vader</div><div class="">config file.</div><div class=""><br class=""></div><div class="">I think to do justice to SGI UV, someone would have to put in time</div><div class="">to figure out how to use their GRU api.&nbsp; I'm pretty sure that's the way the</div><div class="">sgi mpi delivers small messages efficiently.</div><div class=""><br class=""></div><div class="">Howard</div><div class=""><br class=""></div><div class=""><br class=""></div></div><div class="gmail_extra"><br class=""><div class="gmail_quote">2014-12-22 8:43 GMT-07:00 Nathan Hjelm <span dir="ltr" class="">&lt;<a href="mailto:hjelmn@lanl.gov" target="_blank" class="">hjelmn@lanl.gov</a>&gt;</span>:<br class=""><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><br class="">
Yeah, I figured out why XPMEM is failing on SGI UV but have not figured<br class="">
out a fix. If possible can we remove the check for sn/xpmem.h in the<br class="">
ompi/mca/btl/vader/configure.m4. I hopefully will have a better fix for<br class="">
1.8.5.<br class="">
<br class="">
-Nathan<br class="">
<br class="">
On Fri, Dec 19, 2014 at 11:59:29PM -0800, Paul Hargrove wrote:<br class="">
&gt;&nbsp; &nbsp; Sorry to rain on the parade, but SGI UV is still broken by default.<br class="">
&gt;&nbsp; &nbsp; I reported this as present in 1.8.4rc5 and Nathan had claimed to be<br class="">
&gt;&nbsp; &nbsp; working on it.<br class="">
&gt;&nbsp; &nbsp; A reminder that all it takes is a 1-line change in<br class="">
&gt;&nbsp; &nbsp; ompi/mca/btl/vader/configure.m4 to not search for sn/xpmem.h<br class="">
&gt;&nbsp; &nbsp; -Paul<br class="">
&gt;&nbsp; &nbsp; On Fri, Dec 19, 2014 at 7:26 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" class="">rhc@open-mpi.org</a>&gt; wrote:<br class="">
&gt;<br class="">
&gt;&nbsp; &nbsp; &nbsp; The Open MPI Team, representing a consortium of research, academic, and<br class="">
&gt;&nbsp; &nbsp; &nbsp; industry partners, is pleased to announce the release of Open MPI<br class="">
&gt;&nbsp; &nbsp; &nbsp; version 1.8.4.<br class="">
&gt;<br class="">
&gt;&nbsp; &nbsp; &nbsp; v1.8.4 is a bug fix release.&nbsp; All users are encouraged to upgrade to<br class="">
&gt;&nbsp; &nbsp; &nbsp; v1.8.4 when possible.<br class="">
&gt;<br class="">
&gt;&nbsp; &nbsp; &nbsp; Version 1.8.4 can be downloaded from the main Open MPI web site or any<br class="">
&gt;&nbsp; &nbsp; &nbsp; of its mirrors&nbsp; (mirrors will be updating shortly).<br class="">
&gt;<br class="">
&gt;&nbsp; &nbsp; &nbsp; Here is a list of changes in v1.8.4 as compared to v1.8.3:<br class="">
&gt;<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fix MPI_SIZEOF; now available in mpif.h for modern Fortran compilers<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; (see README for more details).&nbsp; Also fixed various compiler/linker<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; errors.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fixed inadvertant Fortran ABI break between v1.8.1 and v1.8.2 in the<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; mpi interface module when compiled with gfortran &gt;= v4.9.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fix various MPI_THREAD_MULTIPLE issues in the TCP BTL.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - mpirun no longer requires the --hetero-nodes switch; it will<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; automatically detect when running in heterogeneous scenarios.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Update LSF support, to include revamped affinity functionality.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Update embedded hwloc to v1.9.1.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fixed max registerable memory computation in the openib BTL.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Updated error message when debuggers are unable to find various<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; symbols/types to be more clear.&nbsp; Thanks to Dave Love for raising the<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; issue.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Added proper support for LSF and PBS/Torque libraries in static<br class="">
&gt;&nbsp; &nbsp; &nbsp; builds.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Rankfiles now support physical processor IDs.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fixed potential hang in MPI_ABORT.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fixed problems with the PSM MTL and "re-connect" scenarios, such as<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; MPI_INTERCOMM_CREATE.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fix MPI_IREDUCE_SCATTER with a single process.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fix (rare) race condition in stdout/stderr funneling to mpirun where<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; some trailing output could get lost when a process terminated.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Removed inadvertent change that set --enable-mpi-thread-multiple "on"<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; by default, thus impacting performance for non-threaded apps.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Significantly reduced startup time by optimizing internal hash table<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; implementation.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fixed OS X linking with the Fortran mpi module when used with<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; gfortran &gt;= 4.9.&nbsp; Thanks to Github user yafshar for raising the<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; issue.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fixed memory leak on Cygwin platforms.&nbsp; Thanks for Marco Atzeri for<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; reporting the issue.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fixed seg fault in neighborhood collectives when the degree of the<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; topology is higher than the communicator size.&nbsp; Thanks to Lisandro<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; Dalcin for reporting the issue.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fixed segfault in neighborhood collectives under certain use-cases.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fixed various issues regarding Solaris support.&nbsp; Thanks to Siegmar<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; Gross for patiently identifying all the issues.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fixed PMI configure tests for certain Slurm installation patterns.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fixed param registration issue in Java bindings.&nbsp; Thanks to Takahiro<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; Kawashima and Siegmar Gross for identifying the issue.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Several man page fixes.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Silence several warnings and close some memory leaks (more remain,<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; but it's better than it was).<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Re-enabled the use of CMA and knem in the shared memory BTL.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Updated mpirun manpage to correctly explain new map/rank/binding<br class="">
&gt;&nbsp; &nbsp; &nbsp; options.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fixed MPI_IALLGATHER problem with intercommunicators.&nbsp; Thanks for<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; Takahiro Kawashima for the patch.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Numerous updates and performance improvements to OpenSHMEM.<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Turned off message coalescing in the openib BTL until a proper fix<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; for that capability can be provided (tentatively expected for 1.8.5)<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fix a bug in iof output that dates back to the dinosaurs which would<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; output extra bytes if the system was very heavily loaded<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Fix a bug where specifying mca_component_show_load_errors=0 could<br class="">
&gt;&nbsp; &nbsp; &nbsp; &nbsp; cause ompi_info to segfault<br class="">
&gt;&nbsp; &nbsp; &nbsp; - Updated valgrind suppression file<br class="">
&gt;<br class="">
&gt;&nbsp; &nbsp; &nbsp; _______________________________________________<br class="">
&gt;&nbsp; &nbsp; &nbsp; announce mailing list<br class="">
&gt;&nbsp; &nbsp; &nbsp; <a href="mailto:announce@open-mpi.org" class="">announce@open-mpi.org</a><br class="">
&gt;&nbsp; &nbsp; &nbsp; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/announce" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/announce</a><br class="">
&gt;&nbsp; &nbsp; &nbsp; Searchable archives:<br class="">
&gt;&nbsp; &nbsp; &nbsp; <a href="http://www.open-mpi.org/community/lists/announce/2014/12/index.php" target="_blank" class="">http://www.open-mpi.org/community/lists/announce/2014/12/index.php</a><br class="">
&gt;<br class="">
&gt;&nbsp; &nbsp; --<br class="">
&gt;&nbsp; &nbsp; Paul H. Hargrove&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <a href="mailto:PHHargrove@lbl.gov" class="">PHHargrove@lbl.gov</a><br class="">
&gt;&nbsp; &nbsp; Computer Languages &amp; Systems Software (CLaSS) Group<br class="">
&gt;&nbsp; &nbsp; Computer Science Department&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Tel: <a href="tel:%2B1-510-495-2352" value="+15104952352" class="">+1-510-495-2352</a><br class="">
&gt;&nbsp; &nbsp; Lawrence Berkeley National Laboratory&nbsp; &nbsp; &nbsp;Fax: <a href="tel:%2B1-510-486-6900" value="+15104866900" class="">+1-510-486-6900</a><br class="">
<br class="">
&gt; _______________________________________________<br class="">
&gt; devel mailing list<br class="">
&gt; <a href="mailto:devel@open-mpi.org" class="">devel@open-mpi.org</a><br class="">
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br class="">
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/12/16704.php" target="_blank" class="">http://www.open-mpi.org/community/lists/devel/2014/12/16704.php</a><br class="">
<br class="">
<br class="">_______________________________________________<br class="">
devel mailing list<br class="">
<a href="mailto:devel@open-mpi.org" class="">devel@open-mpi.org</a><br class="">
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br class="">
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/12/16710.php" target="_blank" class="">http://www.open-mpi.org/community/lists/devel/2014/12/16710.php</a><br class=""></blockquote></div><br class=""></div>
_______________________________________________<br class="">devel mailing list<br class=""><a href="mailto:devel@open-mpi.org" class="">devel@open-mpi.org</a><br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/devel<br class="">Link to this post: http://www.open-mpi.org/community/lists/devel/2014/12/16715.php</div></blockquote></div><br class=""></div></body></html>
