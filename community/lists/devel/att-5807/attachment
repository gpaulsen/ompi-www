<p>Sorry, guys, I tested it on 1.3 branch, trunk version(1.4a1r20980) seems to be fixed.<br></p><p>BUT, </p><p>the default value of mpool_sm_min_size in 1.4a1r20980 is 67108864 </p><p>when I set it to 0, there is a performance degradation, is it OK ?</p>
<p>$LD_LIBRARY_PATH=~/work/svn/ompi/trunk/build_x86-64/install/lib/ install/bin/mpirun -np 2 -mca btl sm,self -mca mpool_sm_min_size 0 ~/work/svn/hpc/tools/benchmarks/OMB-3.1.1/osu_bw<br># OSU MPI Bandwidth Test v3.1.1<br>
# Size        Bandwidth (MB/s)<br>1                         1.20<br>2                         3.39<br>4                         6.93<br>8                        14.09<br>16                       27.80<br>32                       50.58<br>
64                      101.08<br>128                     173.23<br>256                     257.81<br>512                     436.86<br>1024                    674.51<br>2048                    856.80<br>4096                    573.87<br>
8192                    607.55<br>16384                   660.58<br>32768                   685.23<br>65536                   687.45<br>131072                  690.52<br>262144                  687.48<br>524288                  676.77<br>
1048576                 675.74<br>2097152                 676.89<br>4194304                 677.28<br>lennyb@dellix7 ~/work/svn/ompi/trunk/build_x86-64 $LD_LIBRARY_PATH=~/work/svn/ompi/trunk/build_x86-64/install/lib/ install/bin/mpirun -np 2 -mca btl sm,self  ~/work/svn/hpc/tools/benchmarks/OMB-3.1.1/osu_bw<br>
# OSU MPI Bandwidth Test v3.1.1<br># Size        Bandwidth (MB/s)<br>1                         1.72<br>2                         3.70<br>4                         7.43<br>8                        13.45<br>16                       29.83<br>
32                       52.66<br>64                      105.08<br>128                     181.16<br>256                     288.16<br>512                     426.83<br>1024                    690.21<br>2048                    867.00<br>
4096                    567.53<br>8192                    667.35<br>16384                   806.97<br>32768                   892.95<br>65536                   989.62<br>131072                 1009.25<br>262144                 1018.35<br>
524288                 1037.32<br>1048576                1048.75<br>2097152                1057.51<br>4194304                1062.16<br><br></p><p>Lenny.</p><br><div><span class="gmail_quote">On 4/12/09, <b class="gmail_sendername">Lenny Verkhovsky</b> &lt;<a href="mailto:lenny.verkhovsky@gmail.com">lenny.verkhovsky@gmail.com</a>&gt; wrote:</span><blockquote class="gmail_quote" style="margin:0;margin-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex">
<p>r20980 It still get stacked</p><p>LD_LIBRARY_PATH=~/work/svn/hpc/dev/ompi_1_3_trunk/build_x86-64/install/lib/ ~/work/svn/hpc/dev/ompi_1_3_trunk/build_x86-64/install/bin/mpirun -np 2 -mca btl self,sm ./osu_bw<br></p><p>
# OSU MPI Bandwidth Test v3.1.1<br>
# Size        Bandwidth (MB/s)<br>1                         1.46<br>2                         3.66<br>4                         7.29<br>8                        14.64<br>16                       29.44<br>32                       56.94<br>

64                      112.25<br>128                     189.02<br>256                     278.26<br>512                     448.58<br>1024                    686.25<br>2048                    865.27<br><br><br></p><div>
<span class="e" id="q_12099de2abdb7035_1"><br><div>
<span class="gmail_quote">On 4/8/09, <b class="gmail_sendername">Jeff Squyres</b> &lt;<a href="mailto:jsquyres@cisco.com" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">jsquyres@cisco.com</a>&gt; wrote:</span><blockquote class="gmail_quote" style="margin:0;margin-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex">

Ditto -- works for me too.  Huzzah!<div><span><br>
<br>
<br>
On Apr 7, 2009, at 8:39 PM, Eugene Loh wrote:<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
George Bosilca wrote:<br>
<br>
&gt; This is interesting. I cannot trigger this deadlock on my AMD cluster<br>
&gt; even when I set the sm_min_size to zero. However, on a Intel cluster<br>
&gt; this can be triggered pretty easily.<br>
&gt;<br>
&gt; Anyway, I think I finally understood where the problem is coming<br>
&gt; from.  r20952 and r20953 are commits that, in addition to the ones<br>
&gt; from  yesterday, fix the problem. Please read the log on r20953 to see<br>
&gt; how  this happens.<br>
&gt;<br>
&gt; Of course, please stress it before we move it to the 1.3 branch.<br>
<br>
Okay, this fix works for me.<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</blockquote>
<br>
<br></span></div><span>
-- <br>
Jeff Squyres<br>
Cisco Systems</span><div><span><br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank" onclick="return top.js.OpenExtLink(window,event,this)">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</span></div></blockquote></div><br>
</span></div></blockquote></div><br>

