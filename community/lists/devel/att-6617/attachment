<div dir="ltr"><div>thanks a lot!</div>
<div><br><br> </div>
<div class="gmail_quote">On Tue, Aug 11, 2009 at 8:07 PM, Eugene Loh <span dir="ltr">&lt;<a href="mailto:Eugene.Loh@sun.com">Eugene.Loh@sun.com</a>&gt;</span> wrote:<br>
<blockquote style="BORDER-LEFT: #ccc 1px solid; MARGIN: 0px 0px 0px 0.8ex; PADDING-LEFT: 1ex" class="gmail_quote">
<div text="#000000" bgcolor="#ffffff">
<div>
<div></div>
<div class="h5">Mike Dubman wrote: 
<blockquote cite="http://midb20b52800908110929s56ba85b1xcbb66bbf88a46b0e@mail.gmail.com" type="cite">
<div dir="ltr">
<div class="gmail_quote">
<div dir="ltr"><br>Hello guys,<br><br><br>When executing following command with mtt and ompi 1.3.3:<br><br>mpirun --host witch15,witch15,witch15,witch15,witch16,witch16,witch16,witch16,witch17,witch17,witch17,witch17,witch18,witch18,witch18,witch18,witch19,witch19,witch19,witch19 -np 20   --mca btl_openib_use_srq 1  --mca btl self,sm,openib  ~mtt/mtt-scratch/20090809140816_dellix8_11812/installs/mnum/tests/ibm/ibm/dynamic/loop_spawn <br>
<br>getting following errors:<br><br>parent: MPI_Comm_spawn #0 return : 0<br>parent: MPI_Comm_spawn #20 return : 0<br>parent: MPI_Comm_spawn #40 return : 0<br>parent: MPI_Comm_spawn #60 return : 0<br>parent: MPI_Comm_spawn #80 return : 0<br>
parent: MPI_Comm_spawn #100 return : 0<br>parent: MPI_Comm_spawn #120 return : 0<br>parent: MPI_Comm_spawn #140 return : 0<br>parent: MPI_Comm_spawn #160 return : 0<br>parent: MPI_Comm_spawn #180 return : 0<br>parent: MPI_Comm_spawn #200 return : 0<br>
parent: MPI_Comm_spawn #220 return : 0<br>parent: MPI_Comm_spawn #240 return : 0<br>parent: MPI_Comm_spawn #260 return : 0<br>parent: MPI_Comm_spawn #280 return : 0<br>parent: MPI_Comm_spawn #300 return : 0<br>parent: MPI_Comm_spawn #320 return : 0<br>
parent: MPI_Comm_spawn #340 return : 0<br>parent: MPI_Comm_spawn #360 return : 0<br>parent: MPI_Comm_spawn #380 return : 0<br>parent: MPI_Comm_spawn #400 return : 0<br>parent: MPI_Comm_spawn #420 return : 0<br>parent: MPI_Comm_spawn #440 return : 0<br>
parent: MPI_Comm_spawn #460 return : 0<br>parent: MPI_Comm_spawn #480 return : 0<br>parent: MPI_Comm_spawn #500 return : 0<br>parent: MPI_Comm_spawn #520 return : 0<br>parent: MPI_Comm_spawn #540 return : 0<br>parent: MPI_Comm_spawn #560 return : 0<br>
parent: MPI_Comm_spawn #580 return : 0<br>--------------------------------------------------------------------------<br>mpirun was unable to launch the specified application as it encountered an error:<br><br><span style="COLOR: rgb(255,0,0)">Error: system limit exceeded on number of pipes that can be open</span><br>
Node: witch19<br><br>when attempting to start process rank 0.<br><br>This can be resolved by setting the mca parameter opal_set_max_sys_limits to 1,<br>increasing your limit descriptor setting (using limit or ulimit commands),<br>
asking the system administrator for that node to increase the system limit, or <br>by rearranging your processes to place fewer of them on that node.<br><br><br>Do you know what OS params should be changed in order to resolve it?<br>
</div></div></div></blockquote></div></div>I thought this error message just got a makeover.  So, if it&#39;s insufficient, it should probably be improved further.  The message suggests:<br><br>1) setting opal_set_max_sys_limits to 1, which seems pretty self explanatory<br>
<br>2) increasing descriptor limit using limit or ulimit, which requires a little more OS familiarity<br><br>3) cutting a deal with sysadmin<br><br>4) rearranging processes<br><br>So, which part are you asking about?  #2?  If so, try &quot;man limit&quot; and look at the places where you see anything about &quot;descriptor[s]&quot;.  Answers depend on the shell you use.<br>
</div><br>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</blockquote></div><br></div>

