<? include("../../include/msg-header.inc"); ?>
<!-- received="Fri Oct 20 14:13:37 2006" -->
<!-- isoreceived="20061020181337" -->
<!-- sent="Fri, 20 Oct 2006 14:13:30 -0400" -->
<!-- isosent="20061020181330" -->
<!-- name="Pak Lui" -->
<!-- email="Pak.Lui_at_[hidden]" -->
<!-- subject="Re: [OMPI devel] [GE users] OpenMPI 1.2 integration and dedicated MPI networks" -->
<!-- id="453911CA.6050205_at_Sun.COM" -->
<!-- charset="ISO-8859-1" -->
<!-- inreplyto="4538EF15.6040608_at_cora.nwra.com" -->
<!-- expires="-1" -->
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<p class="headers">
<strong>From:</strong> Pak Lui (<em>Pak.Lui_at_[hidden]</em>)<br>
<strong>Date:</strong> 2006-10-20 14:13:30
</p>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1129.php">Orion Poplawski: "Re: [OMPI devel] [GE users] OpenMPI 1.2 integration and dedicated MPI networks"</a>
<li><strong>Previous message:</strong> <a href="1127.php">Orion Poplawski: "Re: [OMPI devel] [GE users] OpenMPI 1.2 integration and dedicated MPI networks"</a>
<li><strong>In reply to:</strong> <a href="1127.php">Orion Poplawski: "Re: [OMPI devel] [GE users] OpenMPI 1.2 integration and dedicated MPI networks"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1129.php">Orion Poplawski: "Re: [OMPI devel] [GE users] OpenMPI 1.2 integration and dedicated MPI networks"</a>
<li><strong>Reply:</strong> <a href="1129.php">Orion Poplawski: "Re: [OMPI devel] [GE users] OpenMPI 1.2 integration and dedicated MPI networks"</a>
<!-- reply="end" -->
</ul>
<hr>
<!-- body="start" -->
<p>
Hi Orion and Reuti,
<br>
<p>Let me see if I can understand the issue by breaking them down first:
<br>
<p>(1) First, I am curious to know why you would need to create a 
<br>
PE_HOSTFILE yourself, because that file is generated by SGE/N1GE when 
<br>
you specify you are running a parallel job under SGE/N1GE, by doing 
<br>
something like this with qsub/qsh/qrsh, etc:
<br>
<p>% qsub -pe name_of_my_pe 4
<br>
<p>Normally I wouldn't expect users who run a parallel job would need to 
<br>
create or modify that file though, and to manually set any environment 
<br>
variables.
<br>
<p>You can also find am example of a simple use case of N1GE/SGE with 
<br>
OMPI/ORTE here:
<br>
<p><a href="http://www.open-mpi.org/faq/?category=running#run-n1ge-or-sge">http://www.open-mpi.org/faq/?category=running#run-n1ge-or-sge</a>
<br>
<p>===
<br>
<p>(2) As for the following error message:
<br>
<p><span class="quotelev1"> &gt; error: commlib error: access denied (client IP resolved to host name
</span><br>
<span class="quotelev1"> &gt; &quot;coop01x.cora.nwra.com&quot;. This is not identical to clients host name
</span><br>
<span class="quotelev1"> &gt; &quot;coop01.cora.nwra.com&quot;)
</span><br>
<p>As you mentioned in your setup, each node has 2 interfaces. And this 
<br>
message is an SGE error and it seems to tell you that SGE cannot resolve 
<br>
the host name.
<br>
<p>Could you check if you run this SGE gethostbyname script to see if the 
<br>
two different hostnames can be resolved to the same host?
<br>
<p># Check that SGE can resolve all the hostnames correctly:
<br>
<p># cd /gridware/sge/utilbin/solaris64
<br>
# ./gethostbyname -aname sun-1
<br>
sun-1-grid
<br>
# ./gethostbyname -aname sun-1-grid
<br>
sun-1-grid
<br>
#
<br>
<p>If this doesn't work, you may need to follow the procedure in the 
<br>
following location to tell SGE about multiple interfaces by creating a 
<br>
host_aliases in $SGE_ROOT/$SGE_CELL/common directory, as described in 
<br>
this sunsource.net document below.
<br>
<p><a href="http://gridengine.sunsource.net/howto/multi_intrfcs.html">http://gridengine.sunsource.net/howto/multi_intrfcs.html</a>
<br>
<p>===
<br>
<p>(3) The integration would use the hostnames in the PE_HOSTFILE for 
<br>
launching the grid engine tasks. You can see the actual qrsh command 
<br>
that is used to launch the tasks by setting the mca parameter:
<br>
<p>-mca pls_gridengine_debug 1
<br>
<p>===
<br>
<p>(4) As for what you have mentioned here:
<br>
<p><span class="quotelev1"> &gt; Now, looking at the OpenMPI gridengine code, it looks like it gets the
</span><br>
<span class="quotelev1"> &gt; node name from the first entry in the pe_hostfile, and never really uses
</span><br>
<span class="quotelev1"> &gt; the queue name for anything.
</span><br>
<span class="quotelev1"> &gt;
</span><br>
<span class="quotelev1"> &gt;          ptr = strtok_r(buf, &quot; \n&quot;, &amp;tok);
</span><br>
<span class="quotelev1"> &gt;          num = strtok_r(NULL, &quot; \n&quot;, &amp;tok);
</span><br>
<span class="quotelev1"> &gt;          queue = strtok_r(NULL, &quot; \n&quot;, &amp;tok);
</span><br>
<span class="quotelev1"> &gt;          arch = strtok_r(NULL, &quot; \n&quot;, &amp;tok);
</span><br>
<span class="quotelev1"> &gt; ...
</span><br>
<span class="quotelev1"> &gt;          node-&gt;node_name = strdup(ptr);
</span><br>
<span class="quotelev1"> &gt;          node-&gt;node_arch = strdup(arch);
</span><br>
<span class="quotelev1"> &gt;
</span><br>
<span class="quotelev1"> &gt; Perhaps it can be modified it uses the queue name hostname when doing
</span><br>
<span class="quotelev1"> &gt; SGE/qrsh calls, but the first hostname when doing MPI communication.
</span><br>
<span class="quotelev1"> &gt; Not really sure what the intent of the two fields in SGE's pe_hostfile
</span><br>
<span class="quotelev1"> &gt; is, or if OpenMPI can handle the idea of two hostnames for different
</span><br>
<span class="quotelev1"> &gt; purposes.
</span><br>
<span class="quotelev1"> &gt;
</span><br>
<p>Once it is in a parallel environment of SGE (e.g. when you have started 
<br>
a parallel job with &quot;qsh/qsub/qrsh -pe name_of_pe&quot;), in ORTE would use 
<br>
the -inherit flag of qrsh to tell qrsh to start a task in a already 
<br>
scheduled parallel job, therefore we cannot assign another queue to the 
<br>
job, because SGE wouldn't allow us to do that, and I don't believe is a 
<br>
right thing to do, as N1GE/SGE would return an error like this:
<br>
<p>% /opt/sge/bin/sol-sparc64/qrsh -q new2.q -inherit -V node1 sleep 10
<br>
error: Unknown option -q
<br>
<p>===
<br>
<p>I am thinking that the #(2) is the reason why you are running into the 
<br>
error. But let me know if that works for you.
<br>
<p><p>Orion Poplawski wrote:
<br>
<span class="quotelev1">&gt; Reuti wrote:
</span><br>
<span class="quotelev2">&gt;&gt; Hi,
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev2">&gt;&gt; Am 20.10.2006 um 01:08 schrieb Orion Poplawski:
</span><br>
<span class="quotelev2">&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; I'm starting to test out OpenMPI 1.2 tight integration with SGE and 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; have run into the following issue.  Currently, my startmpi script 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; massages the hostnames in the machines file created from the SGE 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; pe_hostfile add an &quot;x&quot; suffix on machines that are connected with a 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; separate GigE network dedicated for MPI traffic.
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; With tight integration, openmpi uses the SGE pe_hostfile directly, e.g.:
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; coop00.cora.nwra.com 2 coop.q_at_[hidden] &lt;NULL&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; coop01.cora.nwra.com 2 coop.q_at_[hidden] &lt;NULL&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt;
</span><br>
<span class="quotelev3">&gt;&gt;&gt; Now, how/can I modify this so that MPI traffic speaks to coop00x and 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; coop01x?  One immediate problem that I'm running into is that the 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; startmpi script from the SGE PE runs as the user of the job so it 
</span><br>
<span class="quotelev3">&gt;&gt;&gt; can't modify pe_hostfile.
</span><br>
<span class="quotelev2">&gt;&gt; is the name of the pe_hostfile hardcoded, to point to the one in the 
</span><br>
<span class="quotelev2">&gt;&gt; nodes spool directory, or is OpenMPI using the $PE_HOSTFILE, which you 
</span><br>
<span class="quotelev2">&gt;&gt; could reset to a new name to point to a modified one? Another issue 
</span><br>
<span class="quotelev2">&gt;&gt; might be the back-channel of the communication, where sometimes simply 
</span><br>
<span class="quotelev2">&gt;&gt; the `hostname` of the sender is taken to answer.
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; (Sending this to the openmpi-devel list as well I see what insight they 
</span><br>
<span class="quotelev1">&gt; may have.  This seems like a common use case.)
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; It uses $PE_HOSTFILE, so I made a startup script that created a new 
</span><br>
<span class="quotelev1">&gt; pe_hostfile.  This requires something like the following in my job script:
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; setenv PE_HOSTFILE $TMPDIR/pe_hostfile
</span><br>
<span class="quotelev1">&gt; orterun -np $NSLOTS $*
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; which is unfortunate that it can't be handled automatically somehow.
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; First tried:
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; coop01x.cora.nwra.com 2 coop.q_at_[hidden] &lt;NULL&gt;
</span><br>
<span class="quotelev1">&gt; coop00x.cora.nwra.com 2 coop.q_at_[hidden] &lt;NULL&gt;
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; Which yielded:
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; error: commlib error: access denied (client IP resolved to host name 
</span><br>
<span class="quotelev1">&gt; &quot;coop01x.cora.nwra.com&quot;. This is not identical to clients host name 
</span><br>
<span class="quotelev1">&gt; &quot;coop01.cora.nwra.com&quot;)
</span><br>
<span class="quotelev1">&gt; error: executing task of job 41354 failed: failed sending task to 
</span><br>
<span class="quotelev1">&gt; execd_at_[hidden]: can't find connection
</span><br>
<span class="quotelev1">&gt; [coop01:27468] ERROR: A daemon on node coop00x.cora.nwra.com failed to 
</span><br>
<span class="quotelev1">&gt; start as expected.
</span><br>
<span class="quotelev1">&gt; [coop01:27468] ERROR: There may be more information available from
</span><br>
<span class="quotelev1">&gt; [coop01:27468] ERROR: the 'qstat -t' command on the Grid Engine tasks.
</span><br>
<span class="quotelev1">&gt; [coop01:27468] ERROR: If the problem persists, please restart the
</span><br>
<span class="quotelev1">&gt; [coop01:27468] ERROR: Grid Engine PE job
</span><br>
<span class="quotelev1">&gt; [coop01:27468] ERROR: The daemon exited unexpectedly with status 1.
</span><br>
<span class="quotelev1">&gt; error: commlib error: access denied (client IP resolved to host name 
</span><br>
<span class="quotelev1">&gt; &quot;coop01x.cora.nwra.com&quot;. This is not identical to clients host name 
</span><br>
<span class="quotelev1">&gt; &quot;coop01.cora.nwra.com&quot;)
</span><br>
<span class="quotelev1">&gt; error: executing task of job 41354 failed: failed sending task to 
</span><br>
<span class="quotelev1">&gt; execd_at_[hidden]: can't find connection
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; Then:
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; coop01x.cora.nwra.com 2 coop.q_at_[hidden] &lt;NULL&gt;
</span><br>
<span class="quotelev1">&gt; coop00x.cora.nwra.com 2 coop.q_at_[hidden] &lt;NULL&gt;
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; which yields:
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; error: commlib error: access denied (client IP resolved to host name 
</span><br>
<span class="quotelev1">&gt; &quot;coop01x.cora.nwra.com&quot;. This is not identical to clients host name 
</span><br>
<span class="quotelev1">&gt; &quot;coop01.cora.nwra.com&quot;)
</span><br>
<span class="quotelev1">&gt; error: executing task of job 41356 failed: failed sending task to 
</span><br>
<span class="quotelev1">&gt; execd_at_[hidden]: can't find connection
</span><br>
<span class="quotelev1">&gt; error: commlib error: access denied (client IP resolved to host name 
</span><br>
<span class="quotelev1">&gt; &quot;coop01x.cora.nwra.com&quot;. This is not identical to clients host name 
</span><br>
<span class="quotelev1">&gt; &quot;coop01.cora.nwra.com&quot;)
</span><br>
<span class="quotelev1">&gt; [coop01:27945] ERROR: A daemon on node coop01x.cora.nwra.com failed to 
</span><br>
<span class="quotelev1">&gt; start as expected.
</span><br>
<span class="quotelev1">&gt; [coop01:27945] ERROR: There may be more information available from
</span><br>
<span class="quotelev1">&gt; [coop01:27945] ERROR: the 'qstat -t' command on the Grid Engine tasks.
</span><br>
<span class="quotelev1">&gt; [coop01:27945] ERROR: If the problem persists, please restart the
</span><br>
<span class="quotelev1">&gt; [coop01:27945] ERROR: Grid Engine PE job
</span><br>
<span class="quotelev1">&gt; [coop01:27945] ERROR: The daemon exited unexpectedly with status 1.
</span><br>
<span class="quotelev1">&gt; error: executing task of job 41356 failed: failed sending task to 
</span><br>
<span class="quotelev1">&gt; execd_at_[hidden]: can't find connection
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; Now, looking at the OpenMPI gridengine code, it looks like it gets the 
</span><br>
<span class="quotelev1">&gt; node name from the first entry in the pe_hostfile, and never really uses 
</span><br>
<span class="quotelev1">&gt; the queue name for anything.
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt;          ptr = strtok_r(buf, &quot; \n&quot;, &amp;tok);
</span><br>
<span class="quotelev1">&gt;          num = strtok_r(NULL, &quot; \n&quot;, &amp;tok);
</span><br>
<span class="quotelev1">&gt;          queue = strtok_r(NULL, &quot; \n&quot;, &amp;tok);
</span><br>
<span class="quotelev1">&gt;          arch = strtok_r(NULL, &quot; \n&quot;, &amp;tok);
</span><br>
<span class="quotelev1">&gt; ...
</span><br>
<span class="quotelev1">&gt;          node-&gt;node_name = strdup(ptr);
</span><br>
<span class="quotelev1">&gt;          node-&gt;node_arch = strdup(arch);
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<span class="quotelev1">&gt; Perhaps it can be modified it uses the queue name hostname when doing 
</span><br>
<span class="quotelev1">&gt; SGE/qrsh calls, but the first hostname when doing MPI communication. 
</span><br>
<span class="quotelev1">&gt; Not really sure what the intent of the two fields in SGE's pe_hostfile 
</span><br>
<span class="quotelev1">&gt; is, or if OpenMPI can handle the idea of two hostnames for different 
</span><br>
<span class="quotelev1">&gt; purposes.
</span><br>
<span class="quotelev1">&gt; 
</span><br>
<p><p><pre>
-- 
Thanks,
- Pak Lui
pak.lui_at_[hidden]
</pre>
<!-- body="end" -->
<hr>
<ul class="links">
<!-- next="start" -->
<li><strong>Next message:</strong> <a href="1129.php">Orion Poplawski: "Re: [OMPI devel] [GE users] OpenMPI 1.2 integration and dedicated MPI networks"</a>
<li><strong>Previous message:</strong> <a href="1127.php">Orion Poplawski: "Re: [OMPI devel] [GE users] OpenMPI 1.2 integration and dedicated MPI networks"</a>
<li><strong>In reply to:</strong> <a href="1127.php">Orion Poplawski: "Re: [OMPI devel] [GE users] OpenMPI 1.2 integration and dedicated MPI networks"</a>
<!-- nextthread="start" -->
<li><strong>Next in thread:</strong> <a href="1129.php">Orion Poplawski: "Re: [OMPI devel] [GE users] OpenMPI 1.2 integration and dedicated MPI networks"</a>
<li><strong>Reply:</strong> <a href="1129.php">Orion Poplawski: "Re: [OMPI devel] [GE users] OpenMPI 1.2 integration and dedicated MPI networks"</a>
<!-- reply="end" -->
</ul>
<div class="center">
<table border="2" width="100%" class="links">
<tr>
<th><a href="date.php">Date view</a></th>
<th><a href="index.php">Thread view</a></th>
<th><a href="subject.php">Subject view</a></th>
<th><a href="author.php">Author view</a></th>
</tr>
</table>
</div>
<!-- trailer="footer" -->
<? include("../../include/msg-footer.inc") ?>
