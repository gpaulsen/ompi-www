<div dir="ltr">I opened an issue <a href="https://github.com/open-mpi/ompi/issues/322">322</a> about this and gave put it on 1.8.5 milestone.<div>I&#39;ll submit a PR to remove the sn/xpmem.h usage in the vader</div><div>config file.</div><div><br></div><div>I think to do justice to SGI UV, someone would have to put in time</div><div>to figure out how to use their GRU api.  I&#39;m pretty sure that&#39;s the way the</div><div>sgi mpi delivers small messages efficiently.</div><div><br></div><div>Howard</div><div><br></div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">2014-12-22 8:43 GMT-07:00 Nathan Hjelm <span dir="ltr">&lt;<a href="mailto:hjelmn@lanl.gov" target="_blank">hjelmn@lanl.gov</a>&gt;</span>:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><br>
Yeah, I figured out why XPMEM is failing on SGI UV but have not figured<br>
out a fix. If possible can we remove the check for sn/xpmem.h in the<br>
ompi/mca/btl/vader/configure.m4. I hopefully will have a better fix for<br>
1.8.5.<br>
<br>
-Nathan<br>
<br>
On Fri, Dec 19, 2014 at 11:59:29PM -0800, Paul Hargrove wrote:<br>
&gt;    Sorry to rain on the parade, but SGI UV is still broken by default.<br>
&gt;    I reported this as present in 1.8.4rc5 and Nathan had claimed to be<br>
&gt;    working on it.<br>
&gt;    A reminder that all it takes is a 1-line change in<br>
&gt;    ompi/mca/btl/vader/configure.m4 to not search for sn/xpmem.h<br>
&gt;    -Paul<br>
&gt;    On Fri, Dec 19, 2014 at 7:26 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt;<br>
&gt;      The Open MPI Team, representing a consortium of research, academic, and<br>
&gt;      industry partners, is pleased to announce the release of Open MPI<br>
&gt;      version 1.8.4.<br>
&gt;<br>
&gt;      v1.8.4 is a bug fix release.  All users are encouraged to upgrade to<br>
&gt;      v1.8.4 when possible.<br>
&gt;<br>
&gt;      Version 1.8.4 can be downloaded from the main Open MPI web site or any<br>
&gt;      of its mirrors  (mirrors will be updating shortly).<br>
&gt;<br>
&gt;      Here is a list of changes in v1.8.4 as compared to v1.8.3:<br>
&gt;<br>
&gt;      - Fix MPI_SIZEOF; now available in mpif.h for modern Fortran compilers<br>
&gt;        (see README for more details).  Also fixed various compiler/linker<br>
&gt;        errors.<br>
&gt;      - Fixed inadvertant Fortran ABI break between v1.8.1 and v1.8.2 in the<br>
&gt;        mpi interface module when compiled with gfortran &gt;= v4.9.<br>
&gt;      - Fix various MPI_THREAD_MULTIPLE issues in the TCP BTL.<br>
&gt;      - mpirun no longer requires the --hetero-nodes switch; it will<br>
&gt;        automatically detect when running in heterogeneous scenarios.<br>
&gt;      - Update LSF support, to include revamped affinity functionality.<br>
&gt;      - Update embedded hwloc to v1.9.1.<br>
&gt;      - Fixed max registerable memory computation in the openib BTL.<br>
&gt;      - Updated error message when debuggers are unable to find various<br>
&gt;        symbols/types to be more clear.  Thanks to Dave Love for raising the<br>
&gt;        issue.<br>
&gt;      - Added proper support for LSF and PBS/Torque libraries in static<br>
&gt;      builds.<br>
&gt;      - Rankfiles now support physical processor IDs.<br>
&gt;      - Fixed potential hang in MPI_ABORT.<br>
&gt;      - Fixed problems with the PSM MTL and &quot;re-connect&quot; scenarios, such as<br>
&gt;        MPI_INTERCOMM_CREATE.<br>
&gt;      - Fix MPI_IREDUCE_SCATTER with a single process.<br>
&gt;      - Fix (rare) race condition in stdout/stderr funneling to mpirun where<br>
&gt;        some trailing output could get lost when a process terminated.<br>
&gt;      - Removed inadvertent change that set --enable-mpi-thread-multiple &quot;on&quot;<br>
&gt;        by default, thus impacting performance for non-threaded apps.<br>
&gt;      - Significantly reduced startup time by optimizing internal hash table<br>
&gt;        implementation.<br>
&gt;      - Fixed OS X linking with the Fortran mpi module when used with<br>
&gt;        gfortran &gt;= 4.9.  Thanks to Github user yafshar for raising the<br>
&gt;        issue.<br>
&gt;      - Fixed memory leak on Cygwin platforms.  Thanks for Marco Atzeri for<br>
&gt;        reporting the issue.<br>
&gt;      - Fixed seg fault in neighborhood collectives when the degree of the<br>
&gt;        topology is higher than the communicator size.  Thanks to Lisandro<br>
&gt;        Dalcin for reporting the issue.<br>
&gt;      - Fixed segfault in neighborhood collectives under certain use-cases.<br>
&gt;      - Fixed various issues regarding Solaris support.  Thanks to Siegmar<br>
&gt;        Gross for patiently identifying all the issues.<br>
&gt;      - Fixed PMI configure tests for certain Slurm installation patterns.<br>
&gt;      - Fixed param registration issue in Java bindings.  Thanks to Takahiro<br>
&gt;        Kawashima and Siegmar Gross for identifying the issue.<br>
&gt;      - Several man page fixes.<br>
&gt;      - Silence several warnings and close some memory leaks (more remain,<br>
&gt;        but it&#39;s better than it was).<br>
&gt;      - Re-enabled the use of CMA and knem in the shared memory BTL.<br>
&gt;      - Updated mpirun manpage to correctly explain new map/rank/binding<br>
&gt;      options.<br>
&gt;      - Fixed MPI_IALLGATHER problem with intercommunicators.  Thanks for<br>
&gt;        Takahiro Kawashima for the patch.<br>
&gt;      - Numerous updates and performance improvements to OpenSHMEM.<br>
&gt;      - Turned off message coalescing in the openib BTL until a proper fix<br>
&gt;        for that capability can be provided (tentatively expected for 1.8.5)<br>
&gt;      - Fix a bug in iof output that dates back to the dinosaurs which would<br>
&gt;        output extra bytes if the system was very heavily loaded<br>
&gt;      - Fix a bug where specifying mca_component_show_load_errors=0 could<br>
&gt;        cause ompi_info to segfault<br>
&gt;      - Updated valgrind suppression file<br>
&gt;<br>
&gt;      _______________________________________________<br>
&gt;      announce mailing list<br>
&gt;      <a href="mailto:announce@open-mpi.org">announce@open-mpi.org</a><br>
&gt;      Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/announce" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/announce</a><br>
&gt;      Searchable archives:<br>
&gt;      <a href="http://www.open-mpi.org/community/lists/announce/2014/12/index.php" target="_blank">http://www.open-mpi.org/community/lists/announce/2014/12/index.php</a><br>
&gt;<br>
&gt;    --<br>
&gt;    Paul H. Hargrove                          <a href="mailto:PHHargrove@lbl.gov">PHHargrove@lbl.gov</a><br>
&gt;    Computer Languages &amp; Systems Software (CLaSS) Group<br>
&gt;    Computer Science Department               Tel: <a href="tel:%2B1-510-495-2352" value="+15104952352">+1-510-495-2352</a><br>
&gt;    Lawrence Berkeley National Laboratory     Fax: <a href="tel:%2B1-510-486-6900" value="+15104866900">+1-510-486-6900</a><br>
<br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/12/16704.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/12/16704.php</a><br>
<br>
<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/12/16710.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/12/16710.php</a><br></blockquote></div><br></div>

