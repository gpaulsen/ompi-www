<div dir="ltr">I usually run &quot;mpirun -np 2 ./test&quot;. I execute always on a single node. The message appears either with 1 or 2 GPUs on the single node.<br><div><div class="gmail_extra"><br><br><div class="gmail_quote">

2013/1/24 Rolf vandeVaart <span dir="ltr">&lt;<a href="mailto:rvandevaart@nvidia.com" target="_blank">rvandevaart@nvidia.com</a>&gt;</span><br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">

<div link="blue" vlink="purple" lang="EN-US"><div><p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d">Thanks for this report.  I will look into this.  Can you tell me what your mpirun command looked like and do you know what transport you are running over?<u></u><u></u></span></p>

<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d">Specifically, is this on a single node or multiple nodes?<u></u><u></u></span></p><p class="MsoNormal">

<span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d"><u></u> <u></u></span></p><p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d">Rolf<u></u><u></u></span></p>

<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d"><u></u> <u></u></span></p><div style="border:none;border-left:solid blue 1.5pt;padding:0in 0in 0in 4.0pt">

<div><div style="border:none;border-top:solid #b5c4df 1.0pt;padding:3.0pt 0in 0in 0in"><p class="MsoNormal"><b><span style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;">From:</span></b><span style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;"> <a href="mailto:devel-bounces@open-mpi.org" target="_blank">devel-bounces@open-mpi.org</a> [mailto:<a href="mailto:devel-bounces@open-mpi.org" target="_blank">devel-bounces@open-mpi.org</a>] <b>On Behalf Of </b>Alessandro Fanfarillo<br>

<b>Sent:</b> Thursday, January 24, 2013 4:11 AM<br><b>To:</b> <a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br><b>Subject:</b> [OMPI devel] CUDA support doesn&#39;t work starting from 1.9a1r27862<u></u><u></u></span></p>

</div></div><div><div><p class="MsoNormal"><u></u> <u></u></p><div><div><div><p class="MsoNormal">Dear all,<u></u><u></u></p></div><p class="MsoNormal">I would like to report a bug for the CUDA support on the last 5 trunk versions.<u></u><u></u></p>

</div><div><p class="MsoNormal" style="margin-bottom:12.0pt">The attached code is a simply send/receive test case which correctly works with version 1.9a1r27844. <u></u><u></u></p></div><div><p class="MsoNormal">Starting from version 1.9a1r27862 up to 1.9a1r27897 I get the following message:<br>

<br>./test: symbol lookup error: /usr/local/openmpi/lib/openmpi/mca_pml_ob1.so: undefined symbol: progress_one_cuda_htod_event<br>./test: symbol lookup error: /usr/local/openmpi/lib/openmpi/mca_pml_ob1.so: undefined symbol: progress_one_cuda_htod_event<br>

--------------------------------------------------------------------------<br>mpirun has exited due to process rank 0 with PID 21641 on<br>node ip-10-16-24-100 exiting improperly. There are three reasons this could occur:<br>

<br>1. this process did not call &quot;init&quot; before exiting, but others in<br>the job did. This can cause a job to hang indefinitely while it waits<br>for all processes to call &quot;init&quot;. By rule, if one process calls &quot;init&quot;,<br>

then ALL processes must call &quot;init&quot; prior to termination.<br><br>2. this process called &quot;init&quot;, but exited without calling &quot;finalize&quot;.<br>By rule, all processes that call &quot;init&quot; MUST call &quot;finalize&quot; prior to<br>

exiting or it will be considered an &quot;abnormal termination&quot;<br><br>3. this process called &quot;MPI_Abort&quot; or &quot;orte_abort&quot; and the mca parameter<br>orte_create_session_dirs is set to false. In this case, the run-time cannot<br>

detect that the abort call was an abnormal termination. Hence, the only<br>error message you will receive is this one.<br><br>This may have caused other processes in the application to be<br>terminated by signals sent by mpirun (as reported here).<br>

<br>You can avoid this message by specifying -quiet on the mpirun command line.<u></u><u></u></p></div><div><p class="MsoNormal" style="margin-bottom:12.0pt"><br>-----------------------------------------------------------------------------------------------------<u></u><u></u></p>

</div><div><p class="MsoNormal" style="margin-bottom:12.0pt">I&#39;m using gcc-4.7.2 and CUDA 4.2. The test fails also with CUDA 4.1.<u></u><u></u></p></div><div><p class="MsoNormal">Thanks in advance.<br><br>Best regards.<br>

<br>Alessandro Fanfarillo<u></u><u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p></div><div><p class="MsoNormal"><u></u> <u></u></p></div></div></div></div></div></div>
<div>
<hr>
</div>
<div>This email message is for the sole use of the intended recipient(s) and may 
contain confidential information.  Any unauthorized review, use, disclosure 
or distribution is prohibited.  If you are not the intended recipient, 
please contact the sender by reply email and destroy all copies of the original 
message. </div>
<div>
<hr>
</div>
<p></p>
</div>
<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br></blockquote></div><br></div></div></div>

