<div dir="ltr">Hi,<div><br></div><div>Sure: </div><div><a href="https://github.com/open-mpi/ompi-release/issues/178">https://github.com/open-mpi/ompi-release/issues/178</a></div><div><br></div><div>Thanks,</div><div>Alina.</div></div><div class="gmail_extra"><br><div class="gmail_quote">On Sat, Jan 31, 2015 at 3:39 PM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Alina --<br>
<br>
Sorry; I think this bug report got lost in the run-up to the Open MPI dev meeting last week, and that fact that Nathan (the primary one-sided maintainer) is taking a little time off.<br>
<br>
Can you file a Github issue about this?<br>
<br>
Thanks.<br>
<div><div class="h5"><br>
<br>
&gt; On Jan 19, 2015, at 4:50 AM, Alina Sklarevich &lt;<a href="mailto:alinas@dev.mellanox.co.il">alinas@dev.mellanox.co.il</a>&gt; wrote:<br>
&gt;<br>
&gt; Attaching the test for reproduction.<br>
&gt;<br>
&gt; On Mon, Jan 19, 2015 at 11:48 AM, Alina Sklarevich &lt;<a href="mailto:alinas@dev.mellanox.co.il">alinas@dev.mellanox.co.il</a>&gt; wrote:<br>
&gt; Dear OMPI community,<br>
&gt;<br>
&gt;<br>
&gt; We observe a segmentation fault in our regression testing. Our initial investigation shows that It happens for any 1.8.x release and with any PML/BTL/MTL combo on two processes, when running the MPICH one-sided test, accumulate-fence test, attached to this report with the following command line:<br>
&gt;<br>
&gt;<br>
&gt; $mpirun -np 2 --bind-to core --display-map --map-by node -mca pml ob1 -mca btl self,openib ../test/mpi/rma/accfence1<br>
&gt;<br>
&gt;<br>
&gt; The initial trace is:<br>
&gt;<br>
&gt;<br>
&gt; Data for JOB [16088,1] offset 0<br>
&gt;<br>
&gt;<br>
&gt; ========================   JOB MAP   ========================<br>
&gt;<br>
&gt;<br>
&gt; Data for node: vegas15 Num slots: 16     Max slots: 0        Num procs: 1<br>
&gt;<br>
&gt;                Process OMPI jobid: [16088,1] App: 0 Process rank: 0<br>
&gt;<br>
&gt;<br>
&gt; Data for node: vegas16 Num slots: 16     Max slots: 0        Num procs: 1<br>
&gt;<br>
&gt;                Process OMPI jobid: [16088,1] App: 0 Process rank: 1<br>
&gt;<br>
&gt;<br>
&gt; =============================================================<br>
&gt;<br>
&gt; [vegas16:22098] *** Process received signal ***<br>
&gt;<br>
&gt; [vegas16:22098] Signal: Segmentation fault (11)<br>
&gt;<br>
&gt; [vegas16:22098] Signal code: Address not mapped (1)<br>
&gt;<br>
&gt; [vegas16:22098] Failing at address: 0x34<br>
&gt;<br>
&gt; [vegas16:22098] [ 0] /lib64/libpthread.so.0[0x3f6e80f710]<br>
&gt;<br>
&gt; [vegas16:22098] [ 1] /labhome/alinas/workspace/ompi/openmpi-1.8.4/install/lib/libopen-pal.so.6(opal_memory_ptmalloc2_int_free+0x188)[0x7ffff772baa2]<br>
&gt;<br>
&gt; [vegas16:22098] [ 2] /labhome/alinas/workspace/ompi/openmpi-1.8.4/install/lib/libopen-pal.so.6(opal_memory_ptmalloc2_free+0x98)[0x7ffff772a1f5]<br>
&gt;<br>
&gt; [vegas16:22098] [ 3] /labhome/alinas/workspace/ompi/openmpi-1.8.4/install/lib/libopen-pal.so.6(+0xd6f59)[0x7ffff7728f59]<br>
&gt;<br>
&gt; [vegas16:22098] [ 4] /labhome/alinas/workspace/ompi/openmpi-1.8.4/install/lib/libmpi.so.1(+0x2f884)[0x7ffff7c92884]<br>
&gt;<br>
&gt; [vegas16:22098] [ 5] /labhome/alinas/workspace/ompi/openmpi-1.8.4/install/lib/libmpi.so.1(ompi_attr_delete_all+0x2eb)[0x7ffff7c92dbe]<br>
&gt;<br>
&gt; [vegas16:22098] [ 6] /labhome/alinas/workspace/ompi/openmpi-1.8.4/install/lib/libmpi.so.1(ompi_comm_free+0x6a)[0x7ffff7c99336]<br>
&gt;<br>
&gt; [vegas16:22098] [ 7] /labhome/alinas/workspace/ompi/openmpi-1.8.4/install/lib/openmpi/mca_osc_rdma.so(ompi_osc_rdma_free+0x921)[0x7ffff32ab3bc]<br>
&gt;<br>
&gt; [vegas16:22098] [ 8] /labhome/alinas/workspace/ompi/openmpi-1.8.4/install/lib/libmpi.so.1(ompi_win_free+0x24)[0x7ffff7cc0c87]<br>
&gt;<br>
&gt; [vegas16:22098] [ 9] /labhome/alinas/workspace/ompi/openmpi-1.8.4/install/lib/libmpi.so.1(MPI_Win_free+0xb8)[0x7ffff7d2b702]<br>
&gt;<br>
&gt; [vegas16:22098] [10] /labhome/alinas/workspace/mpich/mpich-mellanox/test/mpi/rma/accfence1[0x402447]<br>
&gt;<br>
&gt; [vegas16:22098] [11] /lib64/libc.so.6(__libc_start_main+0xfd)[0x3f6e41ed1d]<br>
&gt;<br>
&gt; [vegas16:22098] [12] /labhome/alinas/workspace/mpich/mpich-mellanox/test/mpi/rma/accfence1[0x402119]<br>
&gt;<br>
&gt; [vegas16:22098] *** End of error message ***<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; And subsequent investigation of the core file generates the following hints:<br>
&gt;<br>
&gt;<br>
&gt; (gdb) bt<br>
&gt;<br>
&gt; #0  0x00007ffff7722a96 in opal_memory_ptmalloc2_int_free (av=0x7ffff796b320, mem=0x7125a0) at malloc.c:4402<br>
&gt;<br>
&gt; #1  0x00007ffff77211f5 in opal_memory_ptmalloc2_free (mem=0x7125a0) at malloc.c:3511<br>
&gt;<br>
&gt; #2  0x00007ffff771ff59 in opal_memory_linux_free_hook (__ptr=0x7125a0, caller=0x7ffff769a8f6) at hooks.c:709<br>
&gt;<br>
&gt; #3  0x00007ffff769a8f6 in opal_datatype_destruct (datatype=0x7123b0) at opal_datatype_create.c:59<br>
&gt;<br>
&gt; #4  0x00007ffff3346ad0 in opal_obj_run_destructors (object=0x7123b0) at ../../../../opal/class/opal_object.h:448<br>
&gt;<br>
&gt; #5  0x00007ffff334af68 in process_acc (module=0x70e370, source=0, acc_header=0x70fef0) at osc_rdma_data_move.c:1184<br>
&gt;<br>
&gt; #6  0x00007ffff334c752 in process_frag (module=0x70e370, frag=0x70fee0) at osc_rdma_data_move.c:1576<br>
&gt;<br>
&gt; #7  0x00007ffff334cafb in ompi_osc_rdma_callback (request=0x700b80) at osc_rdma_data_move.c:1656<br>
&gt;<br>
&gt; #8  0x00007ffff3db3770 in ompi_request_complete (request=0x700b80, with_signal=true) at ../../../../ompi/request/request.h:402<br>
&gt;<br>
&gt; #9  0x00007ffff3db3f11 in recv_request_pml_complete (recvreq=0x700b80) at pml_ob1_recvreq.h:181<br>
&gt;<br>
&gt; #10 0x00007ffff3db5019 in mca_pml_ob1_recv_frag_callback_match (btl=0x7ffff41d9c20, tag=65 &#39;A&#39;, des=0x7fffffffd210, cbdata=0x0) at pml_ob1_recvfrag.c:243<br>
&gt;<br>
&gt; #11 0x00007ffff3fd6c4b in mca_btl_sm_component_progress () at btl_sm_component.c:1087<br>
&gt;<br>
&gt; #12 0x00007ffff7678d66 in opal_progress () at runtime/opal_progress.c:187<br>
&gt;<br>
&gt; #13 0x00007ffff3dabb44 in opal_condition_wait (c=0x7ffff7ffa120, m=0x7ffff7ffa160) at ../../../../opal/threads/condition.h:78<br>
&gt;<br>
&gt; #14 0x00007ffff3dabcc6 in ompi_request_wait_completion (req=0x7fffffffd410) at ../../../../ompi/request/request.h:381<br>
&gt;<br>
&gt; #15 0x00007ffff3dac9da in mca_pml_ob1_recv (addr=0x7fffffffd9ec, count=1, datatype=0x7ffff7fe25c0, src=0, tag=-24, comm=0x70dac0, status=0x0) at pml_ob1_irecv.c:109<br>
&gt;<br>
&gt; #16 0x00007ffff2cd2868 in ompi_coll_tuned_scatter_intra_basic_linear (sbuf=0x0, scount=1, sdtype=0x7ffff7fe25c0, rbuf=0x7fffffffd9ec, rcount=1, rdtype=0x7ffff7fe25c0, root=0, comm=0x70dac0, module=0x70fa20)<br>
&gt;<br>
&gt;     at coll_tuned_scatter.c:231<br>
&gt;<br>
&gt; #17 0x00007ffff2cbbd75 in ompi_coll_tuned_scatter_intra_dec_fixed (sbuf=0x0, scount=1, sdtype=0x7ffff7fe25c0, rbuf=0x7fffffffd9ec, rcount=1, rdtype=0x7ffff7fe25c0, root=0, comm=0x70dac0, module=0x70fa20)<br>
&gt;<br>
&gt;     at coll_tuned_decision_fixed.c:769<br>
&gt;<br>
&gt; #18 0x00007ffff3b9c16e in mca_coll_basic_reduce_scatter_block_intra (sbuf=0x70e220, rbuf=0x7fffffffd9ec, rcount=1, dtype=0x7ffff7fe25c0, op=0x60d180, comm=0x70dac0, module=0x70f230)<br>
&gt;<br>
&gt;     at coll_basic_reduce_scatter_block.c:102<br>
&gt;<br>
&gt; #19 0x00007ffff334eecc in ompi_osc_rdma_fence (assert=0, win=0x70e260) at osc_rdma_active_target.c:140<br>
&gt;<br>
&gt; #20 0x00007ffff7d2a1b5 in PMPI_Win_fence (assert=0, win=0x70e260) at pwin_fence.c:59<br>
&gt;<br>
&gt; #21 0x0000000000402405 in main ()<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; It looks to be a clear case of memory corruption hidden somewhere in the OSC code. Nathan, can you please have a look?<br>
&gt;<br>
&gt;<br>
&gt; Thanks,<br>
&gt;<br>
&gt; Alina.<br>
&gt;<br>
&gt;<br>
</div></div>&gt; &lt;accfence1.c&gt;_______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/01/16803.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/01/16803.php</a><br>
<span class="HOEnZb"><font color="#888888"><br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/01/16867.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/01/16867.php</a><br>
</font></span></blockquote></div><br></div>

