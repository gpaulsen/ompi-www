<html xmlns:v="urn:schemas-microsoft-com:vml" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:w="urn:schemas-microsoft-com:office:word" xmlns:m="http://schemas.microsoft.com/office/2004/12/omml" xmlns="http://www.w3.org/TR/REC-html40"><head><meta http-equiv=Content-Type content="text/html; charset=us-ascii"><meta name=Generator content="Microsoft Word 12 (filtered medium)"><base href="x-msg://102/"><style><!--
/* Font Definitions */
@font-face
	{font-family:Helvetica;
	panose-1:2 11 6 4 2 2 2 2 2 4;}
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:Tahoma;
	panose-1:2 11 6 4 3 5 4 4 2 4;}
@font-face
	{font-family:Consolas;
	panose-1:2 11 6 9 2 2 4 3 2 4;}
/* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin:0in;
	margin-bottom:.0001pt;
	font-size:12.0pt;
	font-family:"Times New Roman","serif";}
a:link, span.MsoHyperlink
	{mso-style-priority:99;
	color:blue;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{mso-style-priority:99;
	color:purple;
	text-decoration:underline;}
pre
	{mso-style-priority:99;
	mso-style-link:"HTML Preformatted Char";
	margin:0in;
	margin-bottom:.0001pt;
	font-size:10.0pt;
	font-family:"Courier New";}
p.MsoAcetate, li.MsoAcetate, div.MsoAcetate
	{mso-style-priority:99;
	mso-style-link:"Balloon Text Char";
	margin:0in;
	margin-bottom:.0001pt;
	font-size:8.0pt;
	font-family:"Tahoma","sans-serif";}
span.apple-style-span
	{mso-style-name:apple-style-span;}
span.apple-converted-space
	{mso-style-name:apple-converted-space;}
span.HTMLPreformattedChar
	{mso-style-name:"HTML Preformatted Char";
	mso-style-priority:99;
	mso-style-link:"HTML Preformatted";
	font-family:Consolas;}
span.BalloonTextChar
	{mso-style-name:"Balloon Text Char";
	mso-style-priority:99;
	mso-style-link:"Balloon Text";
	font-family:"Tahoma","sans-serif";}
span.EmailStyle23
	{mso-style-type:personal-reply;
	font-family:"Calibri","sans-serif";
	color:#1F497D;}
p.Default, li.Default, div.Default
	{mso-style-name:Default;
	margin:0in;
	margin-bottom:.0001pt;
	text-autospace:none;
	font-size:12.0pt;
	font-family:"Arial","sans-serif";
	color:black;}
.MsoChpDefault
	{mso-style-type:export-only;
	font-size:10.0pt;}
@page WordSection1
	{size:8.5in 11.0in;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
	{page:WordSection1;}
--></style><!--[if gte mso 9]><xml>
<o:shapedefaults v:ext="edit" spidmax="1026" />
</xml><![endif]--><!--[if gte mso 9]><xml>
<o:shapelayout v:ext="edit">
<o:idmap v:ext="edit" data="1" />
</o:shapelayout></xml><![endif]--></head><body lang=EN-US link=blue vlink=purple><div class=WordSection1><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>It is documented in http://developer.download.nvidia.com/compute/cuda/4_0/docs/GPUDirect_Technology_Overview.pdf<o:p></o:p></span></p><p class=Default><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:windowtext'>set CUDA_NIC_INTEROP=1 </span><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:windowtext'><o:p></o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'><o:p>&nbsp;</o:p></span></p><div><div style='border:none;border-top:solid #B5C4DF 1.0pt;padding:3.0pt 0in 0in 0in'><p class=MsoNormal><b><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'>From:</span></b><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'> devel-bounces@open-mpi.org [mailto:devel-bounces@open-mpi.org] <b>On Behalf Of </b>Sebastian Rinke<br><b>Sent:</b> Wednesday, January 18, 2012 8:15 AM<br><b>To:</b> Open MPI Developers<br><b>Subject:</b> Re: [OMPI devel] GPUDirect v1 issues<o:p></o:p></span></p></div></div><p class=MsoNormal><o:p>&nbsp;</o:p></p><div><p class=MsoNormal>Setting the environment variable fixed the problem for Open MPI with CUDA 4.0. Thanks!<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>However, I'm wondering why this is not documented in the NVIDIA GPUDirect package.<o:p></o:p></p></div><div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div><div><p class=MsoNormal>Sebastian.<o:p></o:p></p></div><p class=MsoNormal><o:p>&nbsp;</o:p></p><div><div><p class=MsoNormal>On Jan 18, 2012, at 1:28 AM, Rolf vandeVaart wrote:<o:p></o:p></p></div><p class=MsoNormal><br><br><o:p></o:p></p><div><div><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>Yes, the step outlined in your second bullet is no longer necessary.&nbsp;</span><span style='color:black'><o:p></o:p></span></p></div><div><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>&nbsp;</span><span style='color:black'><o:p></o:p></span></p></div><div><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>Rolf</span><span style='color:black'><o:p></o:p></span></p></div><div><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>&nbsp;</span><span style='color:black'><o:p></o:p></span></p></div><div><p class=MsoNormal><span style='font-size:11.0pt;font-family:"Calibri","sans-serif";color:#1F497D'>&nbsp;</span><span style='color:black'><o:p></o:p></span></p></div><div style='border:none;border-left:solid blue 1.5pt;padding:0in 0in 0in 4.0pt;border-width:initial;border-color:initial'><div><div style='border:none;border-top:solid #B5C4DF 1.0pt;padding:3.0pt 0in 0in 0in;border-width:initial;border-color:initial'><div><p class=MsoNormal><b><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'>From:</span></b><span class=apple-converted-space><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'>&nbsp;</span></span><span style='font-size:10.0pt;font-family:"Tahoma","sans-serif"'><a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a><span class=apple-converted-space>&nbsp;</span>[<a href="mailto:devel-bounces@open-mpi.org">mailto:devel-bounces@open-mpi.org</a>]<span class=apple-converted-space>&nbsp;</span><b>On Behalf Of<span class=apple-converted-space>&nbsp;</span></b>Sebastian Rinke<br><b>Sent:</b><span class=apple-converted-space>&nbsp;</span>Tuesday, January 17, 2012 5:22 PM<br><b>To:</b><span class=apple-converted-space>&nbsp;</span>Open MPI Developers<br><b>Subject:</b><span class=apple-converted-space>&nbsp;</span>Re: [OMPI devel] GPUDirect v1 issues</span><span style='color:black'><o:p></o:p></span></p></div></div></div><div><p class=MsoNormal><span style='color:black'>&nbsp;<o:p></o:p></span></p></div><div><p class=MsoNormal><span style='color:black'>Thank you very much. I will try setting the environment variable and if required also use the 4.1 RC2 version.<br><br>To clarify things a little bit for me, to set up my machine with GPUDirect v1 I did the following:<br><br>* Install RHEL 5.4<br>* Use the kernel with GPUDirect support<br>* Use the MLNX OFED stack with GPUDirect support<br>* Install the CUDA developer driver<br><br>Does using CUDA&nbsp; &gt;= 4.0&nbsp; make one of the above steps&nbsp; redundant?<br><br>I.e., RHEL or different kernel or MLNX OFED stack with GPUDirect support is&nbsp; not needed any more?<br><br>Sebastian.<br><br>Rolf vandeVaart wrote:<o:p></o:p></span></p></div><pre><span style='color:black'>I ran your test case against Open MPI 1.4.2 and CUDA 4.1 RC2 and it worked fine.&nbsp; I do not have a machine right now where I can load CUDA 4.0 drivers.<o:p></o:p></span></pre><pre><span style='color:black'>Any chance you can try CUDA 4.1 RC2?&nbsp; There were some improvements in the support (you do not need to set an environment variable for one)<o:p></o:p></span></pre><pre><span style='color:black'> <a href="http://developer.nvidia.com/cuda-toolkit-41">http://developer.nvidia.com/cuda-toolkit-41</a><o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>There is also a chance that setting the environment variable as outlined in this link may help you.<o:p></o:p></span></pre><pre><span style='color:black'><a href="http://forums.nvidia.com/index.php?showtopic=200629">http://forums.nvidia.com/index.php?showtopic=200629</a><o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>However, I cannot explain why MVAPICH would work and Open MPI would not.&nbsp; <o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>Rolf<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp; <o:p></o:p></span></pre><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><pre><span style='color:black'>-----Original Message-----<o:p></o:p></span></pre><pre><span style='color:black'>From: <a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a> [<a href="mailto:devel-bounces@open-mpi.org">mailto:devel-bounces@open-mpi.org</a>]<o:p></o:p></span></pre><pre><span style='color:black'>On Behalf Of Sebastian Rinke<o:p></o:p></span></pre><pre><span style='color:black'>Sent: Tuesday, January 17, 2012 12:08 PM<o:p></o:p></span></pre><pre><span style='color:black'>To: Open MPI Developers<o:p></o:p></span></pre><pre><span style='color:black'>Subject: Re: [OMPI devel] GPUDirect v1 issues<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>I use CUDA 4.0 with MVAPICH2 1.5.1p1 and Open MPI 1.4.2.<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>Attached you find a little test case which is based on the GPUDirect v1 test<o:p></o:p></span></pre><pre><span style='color:black'>case (mpi_pinned.c).<o:p></o:p></span></pre><pre><span style='color:black'>In that program the sender splits a message into chunks and sends them<o:p></o:p></span></pre><pre><span style='color:black'>separately to the receiver which posts the corresponding recvs. It is a kind of<o:p></o:p></span></pre><pre><span style='color:black'>pipelining.<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>In mpi_pinned.c:141 the offsets into the recv buffer are set.<o:p></o:p></span></pre><pre><span style='color:black'>For the correct offsets, i.e. increasing them, it blocks with Open MPI.<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>Using line 142 instead (offset = 0) works.<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>The tarball attached contains a Makefile where you will have to adjust<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>* CUDA_INC_DIR<o:p></o:p></span></pre><pre><span style='color:black'>* CUDA_LIB_DIR<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>Sebastian<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>On Jan 17, 2012, at 4:16 PM, Kenneth A. Lloyd wrote:<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'> &nbsp;&nbsp;&nbsp;<o:p></o:p></span></pre><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><pre><span style='color:black'>Also, which version of MVAPICH2 did you use?<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>I've been pouring over Rolf's OpenMPI CUDA RDMA 3 (using CUDA 4.1 r2)<o:p></o:p></span></pre><pre><span style='color:black'>vis MVAPICH-GPU on a small 3 node cluster. These are wickedly interesting.<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>Ken<o:p></o:p></span></pre><pre><span style='color:black'>-----Original Message-----<o:p></o:p></span></pre><pre><span style='color:black'>From: <a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a> [<a href="mailto:devel-bounces@open">mailto:devel-bounces@open</a>-<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <o:p></o:p></span></pre></blockquote><pre><span style='color:black'>mpi.org]<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;&nbsp;&nbsp; <o:p></o:p></span></pre><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><pre><span style='color:black'>On Behalf Of Rolf vandeVaart<o:p></o:p></span></pre><pre><span style='color:black'>Sent: Tuesday, January 17, 2012 7:54 AM<o:p></o:p></span></pre><pre><span style='color:black'>To: Open MPI Developers<o:p></o:p></span></pre><pre><span style='color:black'>Subject: Re: [OMPI devel] GPUDirect v1 issues<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>I am not aware of any issues.&nbsp; Can you send me a test program and I<o:p></o:p></span></pre><pre><span style='color:black'>can try it out?<o:p></o:p></span></pre><pre><span style='color:black'>Which version of CUDA are you using?<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>Rolf<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <o:p></o:p></span></pre><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><pre><span style='color:black'>-----Original Message-----<o:p></o:p></span></pre><pre><span style='color:black'>From: <a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a> [<a href="mailto:devel-bounces@open">mailto:devel-bounces@open</a>-<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <o:p></o:p></span></pre></blockquote></blockquote><pre><span style='color:black'>mpi.org]<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;&nbsp;&nbsp; <o:p></o:p></span></pre><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><blockquote style='margin-top:5.0pt;margin-bottom:5.0pt'><pre><span style='color:black'>On Behalf Of Sebastian Rinke<o:p></o:p></span></pre><pre><span style='color:black'>Sent: Tuesday, January 17, 2012 8:50 AM<o:p></o:p></span></pre><pre><span style='color:black'>To: Open MPI Developers<o:p></o:p></span></pre><pre><span style='color:black'>Subject: [OMPI devel] GPUDirect v1 issues<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>Dear all,<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>I'm using GPUDirect v1 with Open MPI 1.4.3 and experience blocking<o:p></o:p></span></pre><pre><span style='color:black'>MPI_SEND/RECV to block forever.<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>For two subsequent MPI_RECV, it hangs if the recv buffer pointer of<o:p></o:p></span></pre><pre><span style='color:black'>the second recv points to somewhere, i.e. not at the beginning, in<o:p></o:p></span></pre><pre><span style='color:black'>the recv buffer (previously allocated with cudaMallocHost()).<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>I tried the same with MVAPICH2 and did not see the problem.<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>Does anybody know about issues with GPUDirect v1 using Open MPI?<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>Thanks for your help,<o:p></o:p></span></pre><pre><span style='color:black'>Sebastian<o:p></o:p></span></pre><pre><span style='color:black'>_______________________________________________<o:p></o:p></span></pre><pre><span style='color:black'>devel mailing list<o:p></o:p></span></pre><pre><span style='color:black'><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><o:p></o:p></span></pre><pre><span style='color:black'><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <o:p></o:p></span></pre></blockquote></blockquote></blockquote><pre><span style='color:black'>-----------------------------------------------------------------------------------<o:p></o:p></span></pre><pre><span style='color:black'>This email message is for the sole use of the intended recipient(s) and may contain<o:p></o:p></span></pre><pre><span style='color:black'>confidential information.&nbsp; Any unauthorized review, use, disclosure or distribution<o:p></o:p></span></pre><pre><span style='color:black'>is prohibited.&nbsp; If you are not the intended recipient, please contact the sender by<o:p></o:p></span></pre><pre><span style='color:black'>reply email and destroy all copies of the original message.<o:p></o:p></span></pre><pre><span style='color:black'>-----------------------------------------------------------------------------------<o:p></o:p></span></pre><pre><span style='color:black'>&nbsp;<o:p></o:p></span></pre><pre><span style='color:black'>_______________________________________________<o:p></o:p></span></pre><pre><span style='color:black'>devel mailing list<o:p></o:p></span></pre><pre><span style='color:black'><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><o:p></o:p></span></pre><pre><span style='color:black'><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><o:p></o:p></span></pre><pre><span style='color:black'>&nbsp; <o:p></o:p></span></pre><div><p class=MsoNormal><span style='color:black'>&nbsp;<o:p></o:p></span></p></div></div><p class=MsoNormal><span style='font-size:13.5pt;font-family:"Helvetica","sans-serif"'>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><o:p></o:p></span></p></div></div><p class=MsoNormal><o:p>&nbsp;</o:p></p></div></body></html>
