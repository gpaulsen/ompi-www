<font size=2 face="sans-serif">Hi Matthias,</font>
<br>
<br><font size=2 face="sans-serif">You might want to play with process
binding to see if your problem is related to bad memory affinity.</font>
<br>
<br><font size=2 face="sans-serif">Try to launch pingpong on two CPUs of
the same socket, then on different sockets (i.e. bind each process to a
core, and try different configurations).</font>
<br>
<br><font size=2 face="sans-serif">Sylvain</font>
<br>
<br>
<br>
<br><font size=1 color=#5f5f5f face="sans-serif">De : &nbsp; &nbsp; &nbsp;
&nbsp;</font><font size=1 face="sans-serif">Matthias Jurenz &lt;matthias.jurenz@tu-dresden.de&gt;</font>
<br><font size=1 color=#5f5f5f face="sans-serif">A : &nbsp; &nbsp; &nbsp;
&nbsp;</font><font size=1 face="sans-serif">Open MPI Developers
&lt;devel@open-mpi.org&gt;</font>
<br><font size=1 color=#5f5f5f face="sans-serif">Date : &nbsp; &nbsp; &nbsp;
&nbsp;</font><font size=1 face="sans-serif">13/02/2012 12:12</font>
<br><font size=1 color=#5f5f5f face="sans-serif">Objet : &nbsp; &nbsp;
&nbsp; &nbsp;</font><font size=1 face="sans-serif">[OMPI devel]
poor btl sm latency</font>
<br><font size=1 color=#5f5f5f face="sans-serif">Envoyé par : &nbsp; &nbsp;
&nbsp; &nbsp;</font><font size=1 face="sans-serif">devel-bounces@open-mpi.org</font>
<br>
<hr noshade>
<br>
<br>
<br><tt><font size=2>Hello all,<br>
<br>
on our new AMD cluster (AMD Opteron 6274, 2,2GHz) we get very bad latencies
<br>
(~1.5us) when performing 0-byte p2p communication on one single node using
the <br>
Open MPI sm BTL. When using Platform MPI we get ~0.5us latencies which
is <br>
pretty good. The bandwidth results are similar for both MPI implementations
<br>
(~3,3GB/s) - this is okay.<br>
<br>
One node has 64 cores and 64Gb RAM where it doesn't matter how many ranks
<br>
allocated by the application. We get similar results with different number
of <br>
ranks.<br>
<br>
We are using Open MPI 1.5.4 which is built by gcc 4.3.4 without any special
<br>
configure options except the installation prefix and the location of the
LSF <br>
stuff.<br>
<br>
As mentioned at </font></tt><a href="http://www.open-mpi.org/faq/?category=sm"><tt><font size=2>http://www.open-mpi.org/faq/?category=sm</font></tt></a><tt><font size=2>
we tried to use <br>
/dev/shm instead of /tmp for the session directory, but it had no effect.
<br>
Furthermore, we tried the current release candidate 1.5.5rc1 of Open MPI
which <br>
provides an option to use the SysV shared memory (-mca shmem sysv) - also
this <br>
results in similar poor latencies.<br>
<br>
Do you have any idea? Please help!<br>
<br>
Thanks,<br>
Matthias<br>
_______________________________________________<br>
devel mailing list<br>
devel@open-mpi.org<br>
</font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/devel</font></tt></a><tt><font size=2><br>
</font></tt>
<br>
