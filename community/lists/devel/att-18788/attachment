<div dir="ltr"><div><div><div>Hi, David<br><br></div>We are looking into your report. <br><br></div>Best,<br><br></div>Josh<br></div><div class="gmail_extra"><br><div class="gmail_quote">On Tue, Apr 19, 2016 at 4:41 PM, David Shrader <span dir="ltr">&lt;<a href="mailto:dshrader@lanl.gov" target="_blank">dshrader@lanl.gov</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hello,<br>
<br>
I have been investigating using XRC on a cluster with a mellanox interconnect. I have found that in a certain situation I get a seg fault. I am using 1.10.2 compiled with gcc 5.3.0, and the simplest configure line that I have found that still results in the seg fault is as follows:<br>
<br>
$&gt; ./configure --with-hcoll --with-mxm --prefix=...<br>
<br>
I do have mxm 3.4.3065 and hcoll 3.3.768 installed in to system space (/usr/lib64). If I use &#39;--without-hcoll --without-mxm,&#39; the seg fault does not happen.<br>
<br>
The seg fault happens even when using examples/hello_c.c, so here is an example of the seg fault using it:<br>
<br>
$&gt; mpicc hello_c.c -o hello_c.x<br>
$&gt; mpirun -n 1 ./hello_c.x<br>
Hello, world, I am 0 of 1, (Open MPI v1.10.2, package: Open MPI <a href="mailto:dshrader@mu-fey.lanl.gov" target="_blank">dshrader@mu-fey.lanl.gov</a> Distribution, ident: 1.10.2, repo rev: v1.10.1-145-g799148f, Jan 21, 2016, 135)<br>
$&gt; mpirun -n 1 -mca btl_openib_receive_queues X,4096,1024:X,12288,512:X,65536,512<br>
Hello, world, I am 0 of 1, (Open MPI v1.10.2, package: Open MPI <a href="mailto:dshrader@mu-fey.lanl.gov" target="_blank">dshrader@mu-fey.lanl.gov</a> Distribution, ident: 1.10.2, repo rev: v1.10.1-145-g799148f, Jan 21, 2016, 135)<br>
--------------------------------------------------------------------------<br>
mpirun noticed that process rank 0 with PID 22819 on node mu0001 exited on signal 11 (Segmentation fault).<br>
--------------------------------------------------------------------------<br>
<br>
The seg fault happens no matter the number of ranks. I have tried the above command with &#39;-mca pml_base_verbose,&#39; and it shows that I am using the yalla pml:<br>
<br>
$&gt; mpirun -n 1 -mca btl_openib_receive_queues X,4096,1024:X,12288,512:X,65536,512 -mca pml_base_verbose 100 ./hello_c.x<br>
...output snipped...<br>
[mu0001.localdomain:22825] select: component cm not selected / finalized<br>
[mu0001.localdomain:22825] select: component ob1 not selected / finalized<br>
[mu0001.localdomain:22825] select: component yalla selected<br>
...output snipped...<br>
--------------------------------------------------------------------------<br>
mpirun noticed that process rank 0 with PID 22825 on node mu0001 exited on signal 11 (Segmentation fault).<br>
--------------------------------------------------------------------------<br>
<br>
Interestingly enough, if I tell mpirun what pml to use, the seg fault goes away. The following command does not get the seg fault:<br>
<br>
$&gt; mpirun -n 1 -mca btl_openib_receive_queues X,4096,1024:X,12288,512:X,65536,512 -mca pml yalla ./hello_c.x<br>
<br>
Passing either ob1 or cm to &#39;-mca pml&#39; also works. So it seems that the seg fault comes about when the yalla pml is chosen by default, when mxm/hcoll is involved, and using XRC. I&#39;m not sure if mxm is to blame, however, as using &#39;-mca pml cm -mca mtl mxm&#39; with the XRC parameters doesn&#39;t throw the seg fault.<br>
<br>
Other information...<br>
OS: RHEL 6.7-based (TOSS)<br>
OpenFabrics: RedHat provided<br>
Kernel: 2.6.32-573.8.1.2chaos.ch5.4.x86_64<br>
Config.log and &#39;ompi_info --all&#39; are in the tarball ompi.tar.bz2 which is attached.<br>
<br>
Is there something else I should be doing with the yalla pml when using XRC? Regardless, I hope reporting the seg fault is useful.<br>
<br>
Thanks,<br>
David<span class="HOEnZb"><font color="#888888"><br>
<br>
-- <br>
David Shrader<br>
HPC-ENV High Performance Computer Systems<br>
Los Alamos National Lab<br>
Email: dshrader &lt;at&gt; <a href="http://lanl.gov" rel="noreferrer" target="_blank">lanl.gov</a><br>
<br>
</font></span><br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/04/18786.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/04/18786.php</a><br></blockquote></div><br></div>

