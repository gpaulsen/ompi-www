<div><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">Welcome! Yes, Jeff and I have been working on the LSF support based on 
7.0<br>features in collab with the folks at Platform.</blockquote><div><br>sounds good. i&#39;m happy to be involved with such a nice active project!<br>&nbsp;</div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">

&gt; 1) it appears that you (jeff, i guess ;) are using new LSF 7.0 API features.<br>&gt; i&#39;m working to support customers in the EDA space, and it&#39;s not clear if/when<br>&gt; they will migrate to 7.0 -- not to mention that our company (cadence) doesn&#39;t
<br>&gt; appear to have LSF 7.0 yet. i&#39;m still looking in to the deatils, but it<br>&gt; appears that (from the Platform docs) lsb_getalloc is probably just a thin<br>&gt; wrapper around the LSB_MCPU_HOSTS (spelling?) environment variable. so that
<br>&gt; could be worked around fairly easily. i dunno about lsb_launch -- it seems<br>&gt; equivalent to a set of ls_rtask() calls (one per process). however, i have<br>&gt; heard that there can be significant subtleties with the semantics of these
<br>&gt; functions, in terms of compatibility across differently configured<br>&gt; LSF-controlled farms, specifically with regrads to administrators ability to<br>&gt; track and control job execution. personally, i don&#39;t see how it&#39;s really
<br>&gt; possible for LSF to prevent &#39;bad&#39; users from spamming out jobs or<br>&gt; short-cutting queues, but perhaps some of the methods they attempt to use can<br>&gt; complicate things for a library like open-rte.
<br><br>After lengthy discussions with Platform, it was deemed the best path forward<br>is to use the lsb_getalloc interface. While it currently reads the enviro<br>variable, they indicated a potential change to read a file instead for
<br>scalability. Rather than chasing any changes, we all agreed that using<br>lsb_getalloc would remain the &quot;stable&quot; interface - so that is what we used.</blockquote><div><br>understood.<br></div><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">

Similar reasons for using lsb_launch. I would really advise against making<br>any changes away from that support. Instead, we could take a lesson from our<br>bproc support and simply (a) detect if we are on a pre-7.0 release, and then
<br>(b) build our own internal wrapper that provides back-support. See the bproc<br>pls component for examples.</blockquote><div><br>that sounds fine -- should just be a matter of a little
configure magic, right? i already had to change the current configure
stuff to be able to build at all under 6.2 (since the current configure check requires 7.0 to pass), so i guess it shouldn&#39;t be too much
harder to mimic the bproc method of detecting multiple versions, assuming it&#39;s really the same sort of
thing. basically, i&#39;d keep the main LSF configure check downgraded as i have currently done in my working copy, but add a new 7.0 check that is really the current truck check. <br><br>then, i&#39;ll make signature-compatible replacements (with the same names? or add internal functions to abstract things? or just add #ifdef&#39;s inline where they are used?) for each missing LSF 
7.0 function (implemented using the 6.1 or 6.2 API), and have configure only build them if the system LSF doesn&#39;t have them. uhm, once i figure out how to do that, anyway ... i guess i&#39;ll ask for more help if the bproc code doesn&#39;t enlighten me. if successful, i should be able to track trunk easily with respect to the LSF version issue at least.
<br><br>i&#39;ll probably just continue experimenting on my own for the moment
(tracking any updates to the main trunk LSF support) to see if i can
figure it out. any advice the best way to get such back support into
trunk, if and when if exists / is working? <br>
&nbsp;</div><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">&gt;<br>&gt; 2) this brings us to point 2 -- upon talking to the author(s) of cadence&#39;s
<br>&gt; internal open-rte-like library, several key issues were raised. mainly,<br>&gt; customers want their applications to be &#39;farm-friendly&#39; in several key ways.<br>&gt; firstly, they do not want any persistent daemons running outside of a given
<br>&gt; job -- this requirement seems met by the current open-mpi default behavior, at<br>&gt; least as far i can tell. secondly, they prefer (strongly) that applications<br>&gt; acquire resources incrementally, and perform work with whatever nodes are
<br>&gt; currently available, rather than forcing a large up-front node allocation.<br>&gt; fault tolerance is nice too, although it&#39;s unclear to me if it&#39;s really<br>&gt; practically needed. in any case, many of our applications can structure their
<br>&gt; computation to use resources in just such a way, generally by dividing the<br>&gt; work into independent, restartable pieces ( i.e. they are embarrassingly ||).<br>&gt; also, MPI communication + MPI-2 process creation seems to be a reasonable
<br>&gt; interface for handling communication and dynamic process creation on the<br>&gt; application side. however, it&#39;s not clear that open-rte supports the needed<br>&gt; dynamic resource acquisition model in any of the ras/pls components i looked
<br>&gt; at. in fact, other that just folding everything in the pls component, it&#39;s not<br>&gt; clear that the entire flow via the rmgr really supports it very well.<br>&gt; specifically for LSF, the use model is that the initial job either is created
<br>&gt; with bsub/lsb_submit(),&nbsp;&nbsp;(or automatically submits itself as step zero<br>&gt; perhaps) to run initially on N machines. N should be &#39;small&#39; (1-16) -- perhaps<br>&gt; only 1 for simplicity. then, as the application runs, it will continue to
<br>&gt; consume more resources as limited by the farm status, the user selection, and<br>&gt; the max # of processes that the job can usefully support (generally &#39;large&#39; --<br>&gt; 100-1000 cpus).<br><br>OpenRTE will be undergoing some changes shortly, so I would strongly
<br>recommend you avoid making major changes without first discussing how they<br>fit into the new design with us. While cadence is a nice system, there are<br>tradeoffs in every design approach - and it isn&#39;t clear that theirs is
<br>necessarily any better than another.<br><br>We could argue for quite some time about their beliefs regarding customers<br>desires - I have heard these statements in multiple directions, with people<br>citing claims of customer &quot;demands&quot; pointing every which way. Bottom line,
<br>from what I can tell, is that customers want something that works and is<br>transparent to them - how that is done is largely irrelevant.</blockquote><div><br>yeah, i agree with that completely. <br>&nbsp;</div><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">

We have other people working on dynamic resource allocation for other<br>systems (e.g., TM), and are making some modifications to better support that<br>kind of requirement. We can discuss those with you if you like to see how
<br>they meet your needs. Not much was done in the past in that regard because<br>people weren&#39;t interested in it. Frankly, we are somewhat moving in the<br>other direction now, so supporting it in the manner you describe may
<br>possibly become harder rather than easier. You may have to accept some<br>less-than-ideal result, I fear.</blockquote><div><br>well, i guess it basically boils down to having some level of support for dynamic resource allocation, so that if an application supports or needs to structure it&#39;s computation that way, it can do so. my impression from reading the MPI-2 spec (or somewhere else?) was that a big part of the motivation behind MPI-2 dynamic processes creation was to support just such models (a la pvm) -- and it seems that the rte layer needs matching support, or it can&#39;t really work well. if there is some support at all, or if it&#39;s not too hard to add, i guess i&#39;ll be happy. 
<br><br>that said, i&#39;d like to reiterate (and skip this paragraph if you get bored) that, at a basic level, i think the ideas behind pvm and dynamic resource allocation are pretty well founded and useful. the idea is to work *with* the existing DRM, rather than only having a private allocation layer over a static allocation from the host DRM. for applications that are capable of being dynamically flexible about the number of CPUs they need, static initial allocation just doesn&#39;t work too well -- a small initial allocation may limit the || too much, whereas a large allocation may be wasteful, and may (vastly) increase the queue time to job startup. in fact, when the queue time is long, it&#39;s extra-wasteful, because the DRM has to hold a bunch of hosts idle waiting for the whole allocation to be satisfied. in all i&#39;ve heard, this seems to be the most &#39;real&#39; customer issue -- that is, i believe the other cadence distributed processing guys when they say that they are having or have had problems with various applications -- both MPI based (LAM/MPI i think -- which had other problems concerning the deamon issue) as well as custom frameworks that simply made large (&gt;100) bsub requests. the most pathological thing i&#39;ve heard internally is that for maximum portability across different LSF farms, not only do you need to acquire resources incrementally, but that you need to acquire each CPU individually using a single bsub -- that is, you shouldn&#39;t even use the -n option to bsub *at all*. this actually simplifies things in some ways, but i don&#39;t really know if i believe it. anyway, that&#39;s what i&#39;ve heard, from the cadence open-rte-alike people that are really running applications on customer farms. somehow, there are problems with accounting or something on certain farms if you bsub non-single cpu tasks. on second thought, i can actually believe this, because the EDA community really doesn&#39;t run many true scientific-computing style multiprocessor jobs at the moment -- mainly, they are running multiple separate jobs that only loosely communicate via the file system, or not at all -- there&#39;s just some script that launch all the pieces of a job, and the pieces are in charge of co-ordinating with themselves if needed. since application have evolved from this &#39;primitive&#39; form of using multiple CPUs, it&#39;s not too surprising that farms might not be well configured to support the more traditional scientific computing use models. i&#39;m continuing to investigate the issue, and i&#39;ll have more data as i start enabling farm support in my own app on some real customer farms -- assuming i can get something working with open-mpi, of course! ;)
<br><br></div><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">&gt;<br>&gt; so, i figure it&#39;s up to me to implement this stuff ;) ... clearly, i want to
<br>&gt; keep the &#39;normal&#39; style ras/pls for LSF working, but somehow add the dynamic<br>&gt; behavior as an option. my initial thought was to (in the dynamic case)<br>&gt; basically ignore/fudge the ras/rmaps(/pls?) stages and simply use
<br>&gt; bsub/lsb_submit() in pls to launch new daemons as needed/requested.<br><br>Just an FYI: this could cause unexpected behavior in the current<br>implementation as a number of subsystems depend upon the info coming from
<br>those stages. May not be as big a problem in the revised implementation<br>currently underway.</blockquote><div><br>duly noted. i don&#39;t pretend to be able to follow the current control flow at the moment. i think just running the debug version with all the printouts should help me a lot there. also, perhaps if i just make a rmgr_dyn_lsf, and don&#39;t use sds, then there might not be as many subsystems involved to complain. actually, i suspect the LSF specific part would be (very) small, so perhaps it could be rmgr_dynurm + a new component type like dynraspls to encapsulate the DRM specific part. &nbsp;  
<br></div><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">&gt; again,<br>&gt; though it&#39;s not clear that the current control flow supports this well. given
<br>&gt; that there may be a large (10sec - 15min) delay between lsb_submit() and job<br>&gt; launch, it may be necessary to both acquire minimum size blocks of new daemons<br>&gt; at a time, and to have some non-blocking way to perform spawning. for example,
<br>&gt; in the current code, the MPI-2 spawn is blocking because it needs to return a<br>&gt; communicator to the spawned process.<br><br>Actually, that is not the real reason. It is blocking because the parent<br>wants to send a message to the new children telling them where/how to
<br>rendezvous with it. The problem is that the parent doesn&#39;t know the name of<br>the child until after the spawn is completed - because we need the child&#39;s<br>OOB contact info so we can send the message. The easiest way to ensure that
<br>all the handshakes occurred correctly was to simply make comm_spawn<br>blocking.<br><br>Given that comm_spawn in our current environments is relatively fast, that<br>was deemed to be an acceptable solution. Obviously, your stated time frames
<br>are much, much longer, so that might not work in those cases.<br><br>It would be easier to change it under the revised implementation, which will<br>better support that kind of difference between environments. In the current
<br>one, it could be quite problematic. <br></blockquote><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">&gt; however, this is not really necessary for
<br>&gt; the application to continue -- it can continue with other work until the new<br>&gt; worker is up and running. perhaps some form of multi-threading could help with<br>&gt; this, but it&#39;s not totally clear. i think i would prefer some lower-level
<br>&gt; open-rte calls that perform daemon pre-allocation ( i.e. dynamic ras/daemon<br>&gt; startup), such that i know that if there are idle daemons, it is safe to spawn<br>&gt; without risk of blocking.<br><br>I&#39;ll have to leave that up to the MPI folks on the team - we have
<br>historically resisted the idea of having one environment behave differently<br>from another so as to limit &quot;user astonishment&quot;. However, if they can live<br>with that change, I personally have no problem with it.
<br><br>We just made a significant change to daemon launch procedures, and the flow<br>between the stages is going to be completely revamped over the next few<br>months. How that affects your thinking is unclear to me at the moment, but
<br>might be worth further discussion.<br><br>Just as an FYI: we already check to see if there are available daemons, and<br>we do spawn upon them if so. The issue here sounds like it is more in<br>obtaining a larger-than-immediately-needed allocation, and spawning daemons
<br>on all of it just-in-case they are needed. There is nothing in the system<br>that precludes doing so - we made a design decision early on not to do it,<br>but that&#39;s not a requirement. Again, the revised implementation would let
<br>you do that much easier than the current one.</blockquote><div><br><br>hmm, i&#39;m thinking that if there was a way to directly tell open-rte to acquire more daemons non-blockingly, that would be enough. <br>in the LSF case, i think one would bsub the
daemons themselves (with arguments sufficient to phone-home, so no sds needed?), so (node acquisition == daemon startup).<br><br>this functions could be called heuristically by MPI-2 spawn type functions, or even manually by the application (in the short term). it should not effect the semantics of the MPI-2 calls themselves.
<br><br>the goal is that one could determine (at least with some confidence) if there were any free (and ready to spawn quickly without blocking) resources before issuing a spawn call. this might just mean examining the value of the MPI universe size (and that this value could change), or it might need some new interface, i dunno.
<br></div><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">&gt;<br>&gt; oh, and at first glance there appears to be a bunch of duplicated code across
<br>&gt; the various flavors of ras (and similarly for pls, sds). is it reasonable to<br>&gt; attempt to factor things out? i seem to recall reading that some major rework<br>&gt; was in progress, so perhaps this would not be a good time?
<br><br>Definitely not a good time - I would wait awhile and let&#39;s see how much of<br>it remains. Some of it is there because of historical uncertainty over what<br>would be common and what wouldn&#39;t be - some might be there for a reason
<br>known to the original author. I would advise asking before assuming...</blockquote><div><br>okay.<br></div><br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">

&gt;<br>&gt; uhm ... well, any advice on anything here?<br>&gt;<br>&gt; thanks,<br>&gt;<br>&gt; Matt.<br>&gt;<br></blockquote></div><br>thanks again,<br><br>Matt.<br>
<br>

