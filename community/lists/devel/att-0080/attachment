<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
<HEAD>

<META NAME="Generator" CONTENT="MS Exchange Server version 6.5.7233.8">
<TITLE>Re: [O-MPI devel] Fwd: Regarding MVAPI Component in Open MPI</TITLE>
</HEAD>
<BODY>
<DIV id=idOWAReplyText17977 dir=ltr>
<DIV dir=ltr><FONT face=Arial color=#000000 size=2></FONT>&nbsp;</DIV></DIV>
<DIV dir=ltr>
<DIV><FONT face=Arial size=2><SPAN 
class=302433108-05082005>Hi,</SPAN></FONT></DIV>
<DIV><FONT face=Arial size=2><SPAN 
class=302433108-05082005></SPAN></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2><SPAN class=302433108-05082005>Thanks for the 
information.</SPAN></FONT></DIV>
<DIV><FONT face=Arial size=2><SPAN 
class=302433108-05082005></SPAN></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2><SPAN class=302433108-05082005>Here is the output 
of ompi_info</SPAN></FONT></DIV>
<DIV><FONT face=Arial size=2><SPAN 
class=302433108-05082005></SPAN></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2><SPAN class=302433108-05082005>[root@micrompi-1 
ompi]# 
ompi_info<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
Open MPI: 1.0a1r6612M<BR>&nbsp;&nbsp; Open MPI SVN revision: 
r6612M<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
Open RTE: 1.0a1r6612M<BR>&nbsp;&nbsp; Open RTE SVN revision: 
r6612M<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
OPAL: 1.0a1r6612M<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; OPAL SVN revision: 
r6612M<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
Prefix: /openmpi<BR>&nbsp;Configured architecture: 
x86_64-redhat-linux-gnu<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
Configured by: 
root<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Configured 
on: Thu Aug&nbsp; 4 23:31:51 IST 
2005<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Configure host: 
micrompi-1<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
Built by: 
root<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
Built on: Thu Aug&nbsp; 4 23:43:29 IST 
2005<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
Built host: 
micrompi-1<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
C bindings: 
yes<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C++ 
bindings: yes<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fortran77 bindings: yes 
(all)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fortran90 bindings: 
no<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
C compiler: gcc<BR>&nbsp;&nbsp;&nbsp;&nbsp; C compiler absolute: 
/usr/bin/gcc<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
C++ compiler: g++<BR>&nbsp;&nbsp; C++ compiler absolute: 
/usr/bin/g++<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fortran77 compiler: g77<BR>&nbsp; 
Fortran77 compiler abs: /usr/bin/g77<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fortran90 
compiler: none<BR>&nbsp; Fortran90 compiler abs: 
none<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
C profiling: yes<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
C++ profiling: yes<BR>&nbsp;&nbsp;&nbsp;&nbsp; Fortran77 profiling: 
yes<BR>&nbsp;&nbsp;&nbsp;&nbsp; Fortran90 profiling: 
no<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C++ exceptions: 
no<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Thread support: 
posix (mpi: no, progress: no)<BR>&nbsp; Internal debug support: 
yes<BR>&nbsp;&nbsp;&nbsp;&nbsp; MPI parameter check: runtime<BR>Memory profiling 
support: yes<BR>Memory debugging support: 
yes<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; libltdl support: 
1<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MCA allocator: 
basic (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MCA 
allocator: bucket (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA coll: basic (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA coll: self (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA io: romio (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA mpool: mvapi (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA mpool: sm (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA pml: teg (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA pml: uniq (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA ptl: self (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA ptl: sm (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA ptl: tcp (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA topo: unity (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA gpr: proxy (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA gpr: replica (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA iof: proxy (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA iof: svc (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA ns: proxy (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA ns: replica (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA oob: tcp (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA ras: host (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA rds: hostfile (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA rds: resfile (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA rmaps: round_robin (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA rmgr: proxy (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA rmgr: urm (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA rml: oob (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA pls: fork (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA pls: proxy (MCA v1.0, API v1.0, Component 
v1.0)<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
MCA pls: rsh (MCA v1.0, API v1.0, Component v1.0)<BR></SPAN></FONT></DIV>
<DIV><FONT face=Arial size=2><SPAN 
class=302433108-05082005></SPAN></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2><SPAN class=302433108-05082005><STRONG>The OpenMPI 
version that I am using r6612 (I could see from the output ompi_info command). 
There is NO btl frame where as mpool was built.</STRONG></SPAN></FONT></DIV>
<DIV><FONT face=Arial size=2><SPAN 
class=302433108-05082005></SPAN></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2><SPAN class=302433108-05082005>In the configure 
options,&nbsp;giving &nbsp;--with-btl-mvapi=/opt/topspin would sufficient as it 
has got include and lib64 directories. Therefore it will pick up the necessary 
things. Also, I have set the following flags</SPAN></FONT></DIV>
<DIV><FONT face=Arial size=2><SPAN 
class=302433108-05082005></SPAN></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2><SPAN 
class=302433108-05082005></SPAN></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial size=2><SPAN class=302433108-05082005>export 
CFLAGS="-I/optl/topspin/include -I/opt/topspin/include/vapi"</SPAN></FONT></DIV>
<DIV><FONT face=Arial size=2>export LDFLAGS="-lmosal -lvapi 
-L/opt/topspin/lib64"</FONT></DIV>
<DIV><FONT face=Arial><FONT size=2>export btl_mvapi_LDFLAGS=<SPAN 
class=302433108-05082005>$LDFLAGS</SPAN></FONT></FONT></DIV>
<DIV><FONT face=Arial><FONT size=2>export btl_mvapi_LIBS=<SPAN 
class=302433108-05082005>$LDFLAGS</SPAN></FONT></FONT></DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN 
class=302433108-05082005></SPAN></FONT></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN class=302433108-05082005>I will 
configure and build the latest code. To get the latest code,&nbsp;I have run the 
following command. Please let me know if I am not 
correct.</SPAN></FONT></FONT></DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN 
class=302433108-05082005></SPAN></FONT></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN class=302433108-05082005>svn co -r6613 
<A 
href="http://svn.open-mpi.org/svn/ompi/trunk">http://svn.open-mpi.org/svn/ompi/trunk</A> 
ompi</SPAN></FONT></FONT></DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN 
class=302433108-05082005></SPAN></FONT></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN class=302433108-05082005>Configured 
as..</SPAN></FONT></FONT></DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN 
class=302433108-05082005></SPAN></FONT></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN class=302433108-05082005>./configure 
--prefix=/openmpi --with-btl-mvapi=/opt/topspin/ </SPAN></FONT></FONT></DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN 
class=302433108-05082005></SPAN></FONT></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN class=302433108-05082005>When I gave 
make all, it is configuring again and again, I mean it is going in a loop. In my 
machine, I do not need libmpga and libmtl_common, hence I removed -lmpga and 
-lmtl_common entries from config/ompi_check_mvapi.m4 file and then ran 
autogen.sh. </SPAN></FONT></FONT></DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN 
class=302433108-05082005></SPAN></FONT></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN class=302433108-05082005>I don't have 
any clue why the configuration is going in loop even while building. I could see 
that config.status --recheck is being issued from Makefile and I feel this might 
the reason for configure to run in loop.</SPAN></FONT></FONT></DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN 
class=302433108-05082005></SPAN></FONT></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN 
class=302433108-05082005></SPAN></FONT></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN class=302433108-05082005>Can someone 
help in this?</SPAN></FONT></FONT></DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN 
class=302433108-05082005></SPAN></FONT></FONT>&nbsp;</DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN 
class=302433108-05082005>Thanks</SPAN></FONT></FONT></DIV>
<DIV><FONT face=Arial><FONT size=2><SPAN 
class=302433108-05082005>-Sridhar</SPAN></FONT></FONT></DIV><BR>
<HR tabIndex=-1>
<FONT face=Tahoma size=2><B>From:</B> devel-bounces@open-mpi.org on behalf of 
Jeff Squyres<BR><B>Sent:</B> Thu 8/4/2005 4:29 PM<BR><B>To:</B> Open MPI 
Developers<BR><B>Subject:</B> Re: [O-MPI devel] Fwd: Regarding MVAPI Component 
in Open MPI<BR></FONT><BR></DIV>
<DIV>
<P><FONT size=2>On Aug 4, 2005, at 6:43 AM, Jeff Squyres wrote:<BR><BR>&gt;&gt; 
I got OpenMPI tar ball and could configure and build on AMD x86_64<BR>&gt;&gt; 
arch.<BR><BR>Excellent.&nbsp; Note, however, that it's probably better to get 
a<BR>Subversion checkout.&nbsp; As this is the current head of our 
development<BR>tree, it's a constantly moving target -- having a Subversion 
checkout<BR>will help you keep up with our progress.<BR><BR>&gt;&gt; In our 
case, we need to enable MVAPI and disable OpenIB. For this, I<BR>&gt;&gt; have 
moved .ompi_ignore file from mvapi directory to openib directory.<BR>&gt;&gt; I 
could see that OpenIB was disabled as the entire openib tree was<BR>&gt;&gt; 
skipped by the autogen.sh script.<BR><BR>It depends on what version of the 
tarball you got -- in the version<BR>that I have, the mvapi components (both btl 
and mpool) do not have<BR>.ompi_ignore files (we recently removed them -- July 
27th, r6613).<BR><BR>Additionally, you should not need to run autogen.sh in a 
tarball (in<BR>fact, autogen.sh should warn you if you try to do this).&nbsp; 
autogen.sh is<BR>only required in a Subversion checkout.&nbsp; Please see the 
top-level<BR>HACKING file in a Subversion checkout (I don't think that it 
is<BR>included in the tarball).<BR><BR>Finally, note that you'll need to give 
additional --with options to<BR>configure to tell it where the MVAPI libraries 
and header files are<BR>located -- more on this below.<BR><BR>&gt;&gt; While 
running Pallas accross the nodes, I could see that data is<BR>&gt;&gt; passing 
over Gigbit ethernet and NOT over Infiniband.&nbsp; Does anyone has<BR>&gt;&gt; 
idea about why data is going through Gig and NOT over infiniband? Do 
I<BR>&gt;&gt; have to set any configuration options? Do I have to give any 
run-time<BR>&gt;&gt; options? I have tried with mpirun -mca btl mvapi but of no 
use.<BR><BR>What is the output of the ompi_info command?&nbsp; This will tell 
you if the<BR>mvapi component is compiled and installed (it sounds like it is 
not).<BR><BR>&gt;&gt; I could make out that TCP component is being used and in 
order to<BR>&gt;&gt; disable tcp, I have copied .ompi_ignore in to 
directories<BR>&gt;&gt; /ompi/orte/mca/oob/tcp and /ompi/ompi/mca/ptl/tcp. But 
this time<BR>&gt;&gt; program fails with segmentation fault error.<BR><BR>Right 
now, IIRC, we don't have checks to ensure that there are valid<BR>paths from one 
MPI process to another -- which is probably the seg<BR>fault.<BR><BR>Also note 
that .ompi_ignore is an autogen mechanism.&nbsp; It is really<BR>intended for 
developers who want to protect parts of the tree during<BR>development when it 
is not ready for general use.&nbsp; It is not really<BR>intended<BR><BR>&gt;&gt; 
These are the configure options that I have given while configuring<BR>&gt;&gt; 
OpenMPI.<BR>&gt;&gt; &nbsp;<BR>&gt;&gt; ./configure --prefix=/openmpi 
--with-btl-mvapi=/usr/local/topspin/<BR>&gt;&gt; 
--with-btl-mvapi-libdir=/usr/local/topspin --with-mvapi<BR><BR>Almost 
correct.&nbsp; Check out ./configure --help:<BR><BR>&nbsp;&nbsp; 
--with-btl-mvapi=MVAPI_DIR<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
Additional directory to search for 
MVAPI<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
installation<BR>&nbsp;&nbsp; 
--with-btl-mvapi-libdir=IBLIBDIR<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
directory where the IB library can be found,<BR>if 
it<BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
is not in MVAPI_DIR/lib or MVAPI_DIR/lib64<BR><BR>The --with-btl-mvapi-libdir 
flag is only necessary if the MVAPI library<BR>cannot be found the 
/usr/local/topspin/lib or /usr/local/topspin/lib64.<BR>&nbsp; There is no 
--with-mvapi flag.<BR><BR>So it's quite possible that with the wrong value 
for<BR>--with-btl-mvapi-libdir, it failed to compile the mvapi 
component<BR>(i.e., I suspect it was looking for /usr/local/topspin/libmosal.* 
when<BR>libmosal is most likely in /usr/local/topspin/lib 
or<BR>/usr/local/topspin/lib64), which resulted in Open MPI falling back 
to<BR>TCP/GigE.<BR><BR>After you install Open MPI, you can run the ompi_info 
command and it<BR>will show a list of all the installed components.&nbsp; You 
should see the<BR>mvapi component in both the btl and mpool frameworks if all 
went well.&nbsp;<BR>If it didn't, then send us the output (stdout and stderr) of 
configure,<BR>the top-level config.log file, and the output from "make all" 
(please<BR>compress!) and we can have a look to see what went wrong.<BR><BR>Once 
you have the mvapi components built, you can choose to use them at<BR>run-time 
via switches to mpirun.&nbsp; See the slides that we talked through<BR>on the 
teleconference -- I provided some examples (you can set these<BR>via command 
line arguments, environment variables, or files).<BR><BR>For one thing, you need 
to manually specify to use the 3rd generation<BR>p2p stuff in Open MPI -- our 
2nd generation is still currently the<BR>default (that will likely change in the 
near future, but it hasn't been<BR>done yet).&nbsp; For 
example:<BR><BR>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mpirun --mca pml ob1 
--mca btl mvapi,self -np 4 a.out<BR><BR>This tells the pml to use the "ob1" 
component (i.e., the 3rd generation<BR>p2p stuff) and to use the mvapi and self 
btl components (self is<BR>loopback -- one processing sending to 
itself).<BR><BR>Give that a whirl and let us know how it goes.<BR><BR>--<BR>{+} 
Jeff Squyres<BR>{+} The Open MPI Project<BR>{+} <A 
href="http://www.open-mpi.org/">http://www.open-mpi.org/</A><BR><BR><BR>_______________________________________________<BR>devel 
mailing list<BR>devel@open-mpi.org<BR><A 
href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</A><BR></FONT></P></DIV>

</BODY>
</HTML>
