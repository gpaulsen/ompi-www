<div dir="ltr"><p>I do care about performance more than memory as well, </p><p>but if 64M is the value that was before all the recent SM changings, </p><p>so let it stay :)<br></p><p>Lenny.</p><br><div class="gmail_quote">
On Mon, Apr 13, 2009 at 5:46 PM, Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
I [unfortunately] think that our benchmark performance is important.  :-(<br>
<br>
So I don&#39;t know if 64mb is too big, but it should probably be above zero to avoid the performance degregation.<div><div class="h5"><br>
<br>
<br>
On Apr 13, 2009, at 10:28 AM, Eugene Loh wrote:<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Lenny Verkhovsky wrote:<br>
<br>
&gt; Sorry, guys, I tested it on 1.3 branch, trunk version(1.4a1r20980)<br>
&gt; seems to be fixed.<br>
&gt;<br>
Great.<br>
<br>
&gt; BUT,<br>
&gt;<br>
&gt; the default value of mpool_sm_min_size in 1.4a1r20980 is 67108864<br>
&gt;<br>
&gt; when I set it to 0, there is a performance degradation, is it OK ?<br>
&gt;<br>
Depends on what matters for you!  :^)<br>
<br>
Anyhow:<br>
<br>
1)  I think many bandwidth tests won&#39;t see this problem, but osu_bw is<br>
different since it pumps so many messages into the system concurrently.<br>
<br>
2)  For the sake of osu_bw, I think leaving the default at 64M is good.<br>
<br>
&gt; $LD_LIBRARY_PATH=~/work/svn/ompi/trunk/build_x86-64/install/lib/<br>
&gt; install/bin/mpirun -np 2 -mca btl sm,self -mca mpool_sm_min_size 0<br>
&gt; ~/work/svn/hpc/tools/benchmarks/OMB-3.1.1/osu_bw<br>
&gt; # OSU MPI Bandwidth Test v3.1.1<br>
&gt; # Size Bandwidth (MB/s)<br>
&gt; 1 1.20<br>
&gt; 2 3.39<br>
&gt; 4 6.93<br>
&gt; 8 14.09<br>
&gt; 16 27.80<br>
&gt; 32 50.58<br>
&gt; 64 101.08<br>
&gt; 128 173.23<br>
&gt; 256 257.81<br>
&gt; 512 436.86<br>
&gt; 1024 674.51<br>
&gt; 2048 856.80<br>
&gt; 4096 573.87<br>
&gt; 8192 607.55<br>
&gt; 16384 660.58<br>
&gt; 32768 685.23<br>
&gt; 65536 687.45<br>
&gt; 131072 690.52<br>
&gt; 262144 687.48<br>
&gt; 524288 676.77<br>
&gt; 1048576 675.74<br>
&gt; 2097152 676.89<br>
&gt; 4194304 677.28<br>
&gt; lennyb@dellix7 ~/work/svn/ompi/trunk/build_x86-64<br>
&gt; $LD_LIBRARY_PATH=~/work/svn/ompi/trunk/build_x86-64/install/lib/<br>
&gt; install/bin/mpirun -np 2 -mca btl sm,self<br>
&gt; ~/work/svn/hpc/tools/benchmarks/OMB-3.1.1/osu_bw<br>
&gt; # OSU MPI Bandwidth Test v3.1.1<br>
&gt; # Size Bandwidth (MB/s)<br>
&gt; 1 1.72<br>
&gt; 2 3.70<br>
&gt; 4 7.43<br>
&gt; 8 13.45<br>
&gt; 16 29.83<br>
&gt; 32 52.66<br>
&gt; 64 105.08<br>
&gt; 128 181.16<br>
&gt; 256 288.16<br>
&gt; 512 426.83<br>
&gt; 1024 690.21<br>
&gt; 2048 867.00<br>
&gt; 4096 567.53<br>
&gt; 8192 667.35<br>
&gt; 16384 806.97<br>
&gt; 32768 892.95<br>
&gt; 65536 989.62<br>
&gt; 131072 1009.25<br>
&gt; 262144 1018.35<br>
&gt; 524288 1037.32<br>
&gt; 1048576 1048.75<br>
&gt; 2097152 1057.51<br>
&gt; 4194304 1062.16<br>
&gt;<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</blockquote>
<br>
<br></div></div><font color="#888888">
-- <br>
Jeff Squyres<br>
Cisco Systems</font><div><div class="h5"><br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</div></div></blockquote></div><br></div>

