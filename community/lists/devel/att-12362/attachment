<div dir="ltr">Thanks for the reply Ralph.<div><br></div><div style>I will look for a way to deal with this situation for the moment. </div><div style><br></div><div style>Regards.</div><div style><br></div><div style>Hugo</div>
</div><div class="gmail_extra"><br><br><div class="gmail_quote">2013/5/6 Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span><br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div style="word-wrap:break-word">We are working towards thread safety, but nowhere near ready yet. <div><br><div><div><div class="h5"><div>On May 6, 2013, at 3:39 AM, Hugo Daniel Meyer &lt;<a href="mailto:meyer.hugo@gmail.com" target="_blank">meyer.hugo@gmail.com</a>&gt; wrote:</div>
<br></div></div><blockquote type="cite"><div><div class="h5"><div dir="ltr">Sorry, i&#39;ve sent the message without finishing it.<div><br></div><div><span style="font-family:arial,sans-serif;font-size:13px">Hello to @ll.</span><div style="font-family:arial,sans-serif;font-size:13px">

<br></div><div style="font-family:arial,sans-serif;font-size:13px">I&#39;m not sure if this is the correct list to post this question, but maybe i&#39;m dealing with a bug.</div><div style="font-family:arial,sans-serif;font-size:13px">

<br></div><div style="font-family:arial,sans-serif;font-size:13px">I have develop an event logging mechanism where application processes connect to event loggers (using MPI_Lookup, MPI_open_port, MPI_Comm_Connect, MPI_Comm_Accept, etc) that are part of another MPI application.</div>

<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">Well, i have develop my own vprotocol where once a process receive a message try to establish a connection with an event logger which is a thread that belongs to another mpi application. </div>

<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">The event logger application consists in one mpi process per node with multiple threads waiting for connections of MPI processes from the main application. </div>

<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">I&#39;m suspecting that there is a problem with the critical regions when processes try to connect with the threads of the event logger. </div>

<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">I&#39;m attaching two short examples that i have written in order to show the problem. First, i launch the event-logger application:</div>

<div style="font-family:arial,sans-serif;font-size:13px"><br></div><div><div><font face="arial, sans-serif">mpirun -n 2 --machinefile machinefile2-th --report-uri URIFILE ./test-thread</font></div><div><br></div><div>
Then i launch the example as this:</div><div><br></div><div>mpirun -n 16 --machinefile machine16 --ompi-server file:URIFILE ./thread_logger_connect<br></div><div><br></div><div>I have obtained this output:</div>
<div><br></div><div><div><i><font color="#0000ff">Published: radic_eventlog[1,6], ret=0</font></i></div><div><i><font color="#0000ff">[clus2:16104] [[39125,1],1] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file dpm_orte.c at line 315</font></i></div>

<div><i><font color="#0000ff">[clus2:16104] [[39125,1],1] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file dpm_orte.c at line 315</font></i></div><div><i><font color="#0000ff">[clus2:16104] *** An error occurred in MPI_Comm_accept</font></i></div>

<div><i><font color="#0000ff">[clus2:16104] *** on communicator MPI_COMM_SELF</font></i></div><div><i><font color="#0000ff">[clus2:16104] *** MPI_ERR_UNKNOWN: unknown error</font></i></div><div><i><font color="#0000ff">[clus2:16104] *** MPI_ERRORS_ARE_FATAL (your MPI job will now abort)</font></i></div>

<div><i><font color="#0000ff">--------------------------------------------------------------------------</font></i></div><div><i><font color="#0000ff">mpirun has exited due to process rank 1 with PID 16104 on</font></i></div>

<div><i><font color="#0000ff">node clus2 exiting improperly. There are two reasons this could occur:</font></i></div><div><i><font color="#0000ff"><br></font></i></div><div><i><font color="#0000ff">1. this process did not call &quot;init&quot; before exiting, but others in</font></i></div>

<div><i><font color="#0000ff">the job did. This can cause a job to hang indefinitely while it waits</font></i></div><div><i><font color="#0000ff">for all processes to call &quot;init&quot;. By rule, if one process calls &quot;init&quot;,</font></i></div>

<div><i><font color="#0000ff">then ALL processes must call &quot;init&quot; prior to termination.</font></i></div><div><i><font color="#0000ff"><br></font></i></div><div><i><font color="#0000ff">2. this process called &quot;init&quot;, but exited without calling &quot;finalize&quot;.</font></i></div>

<div><i><font color="#0000ff">By rule, all processes that call &quot;init&quot; MUST call &quot;finalize&quot; prior to</font></i></div><div><i><font color="#0000ff">exiting or it will be considered an &quot;abnormal termination&quot;</font></i></div>

<div><i><font color="#0000ff"><br></font></i></div><div><i><font color="#0000ff">This may have caused other processes in the application to be</font></i></div><div><i><font color="#0000ff">terminated by signals sent by mpirun (as reported here).</font></i></div>

<div><br></div><div><br></div></div></div><div style="font-family:arial,sans-serif;font-size:13px">If i use mutex in order to serialized the access to MPI_Comm_Accept, the behavior is ok, but shoudn&#39;t the MPI_comm_accept be thread safe?</div>

</div><div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">Best regards.</div><div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">

Hugo Meyer</div><div style="font-family:arial,sans-serif;font-size:13px"><br></div><div style="font-family:arial,sans-serif;font-size:13px">P.d.: This occurs with openmpi1.5.1 and also with also with an old version of the trunk (1.7).</div>

</div><div class="gmail_extra"><br><br><div class="gmail_quote">2013/5/6 Hugo Daniel Meyer <span dir="ltr">&lt;<a href="mailto:meyer.hugo@gmail.com" target="_blank">meyer.hugo@gmail.com</a>&gt;</span><br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">

<div dir="ltr">Hello to @ll.<div><br></div><div>I&#39;m not sure if this is the correct list to post this question, but maybe i&#39;m dealing with a bug.</div><div><br></div><div>I have develop an event logging mechanism where application processes connect to event loggers (using MPI_Lookup, MPI_open_port, MPI_Comm_Connect, MPI_Comm_Accept, etc) that are part of another MPI application.</div>



<div><br></div><div>Well, i have develop my own vprotocol where once a process receive a message try to establish a connection with an event logger which is a thread that belongs to another mpi application. </div>
<div><br></div><div>The event logger application consists in one mpi process per node with multiple threads waiting for connections of MPI processes from the main application. </div><div><br></div><div>I&#39;m suspecting that there is a problem with the critical regions when processes try to connect with the threads of the event logger. </div>


<div><br></div><div>I&#39;m attaching two short examples that i have written in order to show the problem. First, i launch the event-logger application:</div><div><br></div><div><br></div><div><br>
</div><div>If i use mutex in order to serialized the access to MPI_Comm_Accept,</div><div><br></div>
<div><br></div><div><br></div></div>
</blockquote></div><br></div>
</div></div><span>&lt;event_logger.c&gt;</span><span>&lt;main-mpi-app.c&gt;</span>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></blockquote></div><br></div></div><br>_______________________________________________<br>

devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br></blockquote></div><br></div>

