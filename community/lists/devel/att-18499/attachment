Federico,<div><br></div><div>unless you already took care of that, I would guess all 16 orted bound their children MPI tasks on socket 0</div><div><br></div><div>can you try</div><div>mpirun --bind-to none ...</div><div><br></div><div>btw, is your benchmark application cpu bound ? memory bound ? MPI bound ?</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<br><br>On Monday, January 25, 2016, Federico Reghenzani &lt;<a href="mailto:federico1.reghenzani@mail.polimi.it">federico1.reghenzani@mail.polimi.it</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Hello,<div><br></div><div>we have executed a benchmark (SkaMPI) on the same machine (32 core Intel Xeon 86_64) with these two configurations:</div><div>- 1 orted with 16 processes with BTL forced to TCP (--mca btl self,tcp)</div><div>- 16 orted with each 1 process (that uses TCP)</div><div><br></div><div>We use a custom RAS to allow multiple orted on the same machine (I know that it seems non-sense to have multiple orteds on the same machine for the same application, but we are doing some experiments for migration).</div><div><br></div><div>Initially we have expected approximately the same performance in both cases (we have 16 processes communicating via TCP in both cases), but we have a degradation of 50%, and we are sure that is not an overhead due to orteds initialization.<br></div><div><br></div><div>Do you have any idea how can multiple orteds influence the processess performance?</div><div><br></div><div><br></div><div>Cheers,</div><div>Federico<br clear="all"><div><div><div dir="ltr"><div><div dir="ltr"><div><div dir="ltr"><div>__</div><div><span style="font-size:12.8px">Federico Reghenzani</span></div><div><font size="1">M.Eng. Student @ Politecnico di Milano</font></div><div><span style="font-size:x-small">Computer Science and Engineering</span></div><div><br></div><div><br></div></div></div></div></div></div></div></div>
</div></div>
</blockquote></div>

