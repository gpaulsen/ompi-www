<div dir="ltr">with this fix - no failure.<div>Thanks!</div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Mon, Jun 2, 2014 at 8:52 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">Yep, that&#39;s the one. Should have fixed that problem<div><br></div><div><br><div><div>
<div class="h5"><div>On Jun 2, 2014, at 10:30 AM, Mike Dubman &lt;<a href="mailto:miked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt; wrote:</div><br></div></div><blockquote type="cite"><div><div class="h5">
<div dir="ltr">This one? &quot;<span style="font-family:Verdana,Helvetica,&#39;sans serif&#39;;font-size:11px">Fix typo that would cause a segfault if orte_startup_timeout was set&quot;</span><div><span style="font-family:Verdana,Helvetica,&#39;sans serif&#39;;font-size:11px">If so, it is still running.</span></div>

</div><div class="gmail_extra"><br><br><div class="gmail_quote">On Mon, Jun 2, 2014 at 8:16 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">You&#39;re still missing a commit that fixed this problem<div><br><div><div><div>
<div>On Jun 2, 2014, at 9:44 AM, Mike Dubman &lt;<a href="mailto:miked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt; wrote:</div><br></div></div><blockquote type="cite"><div><div><div dir="ltr">
The jenkins still failed (hang and killed by timeout after 3m) as below. No env. mca params were used.<div><br></div><div>Changes:</div><div><table style="font-family:Verdana,Helvetica,&#39;sans serif&#39;;font-size:11px;margin-top:1em;margin-left:1em">


<tbody><tr><td style="font-family:Verdana,Helvetica,&#39;sans serif&#39;;vertical-align:middle"><ol><li>Revert r31926 and replace it with a more complete checking of availability and accessibility of the required freq control paths. </li>


<li>Break the loop caused by retrying to send a message to a hop that is unknown by the TCP oob component. We attempt to provide a way for other components to try, but need to mark that the TCP component is not able to reach that process so the OOB base will know to give up.</li>


</ol></td></tr><tr></tr></tbody></table><br><div><br></div><div><pre style="white-space:pre-wrap;word-wrap:break-word;margin-top:0px;margin-bottom:0px;font-size:11px"><span><b>19:36:19</b> </span>+ timeout -s SIGSEGV 3m /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/bin/mpirun -np 8 /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/examples/hello_usempi
<span><b>19:36:19</b> </span>[vegas12:03383] *** Process received signal ***
<span><b>19:36:19</b> </span>[vegas12:03383] Signal: Segmentation fault (11)
<span><b>19:36:19</b> </span>[vegas12:03383] Signal code: Address not mapped (1)
<span><b>19:36:19</b> </span>[vegas12:03383] Failing at address: 0x20
<span><b>19:36:19</b> </span>[vegas12:03383] [ 0] /lib64/libpthread.so.0[0x3937c0f500]
<span><b>19:36:19</b> </span>[vegas12:03383] [ 1] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/lib/libopen-rte.so.0(orte_plm_base_post_launch+0x90)[0x7ffff7dcbe50]
<span><b>19:36:19</b> </span>[vegas12:03383] [ 2] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/lib/libopen-pal.so.0(opal_libevent2021_event_base_loop+0x8bc)[0x7ffff7b1076c]
<span><b>19:36:19</b> </span>[vegas12:03383] [ 3] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/bin/mpirun(orterun+0x126d)[0x40501d]
<span><b>19:36:19</b> </span>[vegas12:03383] [ 4] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/bin/mpirun(main+0x20)[0x4039e4]
<span><b>19:36:19</b> </span>[vegas12:03383] [ 5] /lib64/libc.so.6(__libc_start_main+0xfd)[0x393741ecdd]
<span><b>19:36:19</b> </span>[vegas12:03383] [ 6] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/bin/mpirun[0x403909]
<span><b>19:36:19</b> </span>[vegas12:03383] *** End of error message ***
<span><b>19:36:20</b> </span>Build step &#39;Execute shell&#39; marked build as failure
<span><b>19:36:21</b> </span>[BFA] Scanning build for known causes...</pre></div></div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Mon, Jun 2, 2014 at 7:00 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>


<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">I fixed this - key was that it only would happen if the MCA param orte_startup_timeout was set.<div>


<br></div><div>It really does help, folks, if you include info on what MCA params were set when you get these failures. Otherwise, it is impossible to replicate the problem.</div><div><div><br></div><div><br>
<div><div>On Jun 2, 2014, at 6:49 AM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt; wrote:</div><br><blockquote type="cite"><div style="word-wrap:break-word">Hi guys<div><br>


</div><div>I&#39;m awake now and will take a look at this - thanks</div><div>Ralph</div><div><br><div><div>On Jun 2, 2014, at 6:34 AM, Mike Dubman &lt;<a href="mailto:miked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt; wrote:</div>


<br><blockquote type="cite"><div dir="ltr"><div>/scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/bin/mpirun -np 8 -mca btl sm,tcp --mca rtc_freq_priority 0 /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/examples/hello_usempiProgram terminated with signal 11, Segmentation fault.<br>



</div><div><br></div><div><br></div><div>#0  orte_plm_base_post_launch (fd=&lt;value optimized out&gt;, args=&lt;value optimized out&gt;, cbdata=0x7393b0) at base/plm_base_launch_support.c:607</div><div>607             opal_event_evtimer_del(timer-&gt;ev);</div>



<div>Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.107.el6.x86_64 libgcc-4.4.7-3.el6.x86_64 libpciaccess-0.13.1-2.el6.x86_64 numactl-2.0.7-6.el6.x86_64</div><div>(gdb) bt</div><div>#0  orte_plm_base_post_launch (fd=&lt;value optimized out&gt;, args=&lt;value optimized out&gt;, cbdata=0x7393b0) at base/plm_base_launch_support.c:607</div>



<div>#1  0x00007ffff7b1076c in event_process_active_single_queue (base=0x630d30, flags=&lt;value optimized out&gt;) at event.c:1367</div><div>#2  event_process_active (base=0x630d30, flags=&lt;value optimized out&gt;) at event.c:1437</div>



<div>#3  opal_libevent2021_event_base_loop (base=0x630d30, flags=&lt;value optimized out&gt;) at event.c:1645</div><div>#4  0x000000000040501d in orterun (argc=10, argv=0x7fffffffe208) at orterun.c:1080</div><div>#5  0x00000000004039e4 in main (argc=10, argv=0x7fffffffe208) at main.c:13</div>



</div><div class="gmail_extra"><br><br><div class="gmail_quote">On Mon, Jun 2, 2014 at 3:31 PM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles.gouaillardet@gmail.com" target="_blank">gilles.gouaillardet@gmail.com</a>&gt;</span> wrote:<br>



<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">OK,<div><br></div><div>please send me a clean gdb backtrace :</div><div>ulimit -c unlimited</div><div>/* this should generate a core */</div>



<div>mpirun ...</div><div>gdb mpirun core...</div><div>bt</div><div>
<br></div><div>if no core</div><div>gdb mpirun</div><div>r -np ... --mca ... ...</div><div>and after the crash</div><div>bt</div><div><br></div><div>then i can only review the code and hope i can find the root cause of the error i am unable to reproduce in my environment</div>




<div><br></div><div>Cheers,</div><div><br></div><div>Gilles</div><div><br></div><div><br></div></div><div class="gmail_extra"><br><br><div class="gmail_quote"><div>On Mon, Jun 2, 2014 at 9:03 PM, Mike Dubman <span dir="ltr">&lt;<a href="mailto:miked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt;</span> wrote:<br>




</div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div dir="ltr">Hi,<div>The jenkins took your commit and applied automatically, I tried with mca flag later.</div>



<div>Also, we don`t have /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor in our system, the cpuspeed daemon is off by default on all our nodes.</div>

<div><br></div><div><br></div><div>Regards</div><div>M</div></div><div class="gmail_extra"><br><br><div class="gmail_quote"><div>On Mon, Jun 2, 2014 at 3:00 PM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles.gouaillardet@gmail.com" target="_blank">gilles.gouaillardet@gmail.com</a>&gt;</span> wrote:<br>





</div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div dir="ltr">Mike,<div><br></div><div>did you apply the patch *and* mpirun --mca rtc_freq_priority 0 ?</div>




<div><br>
</div><div>*both* are required (--mca rtc_freq_priority 0 is not enough without the patch)</div><div><br>
</div><div>can you please confirm there is no <span style="font-size:13px;font-family:arial,sans-serif">/sys/devices/system/cpu/cpu0/</span><span style="font-size:13px;font-family:arial,sans-serif">cpufreq/scaling_governor (pseudo) file on your system ?</span></div>






<div><span style="font-size:13px;font-family:arial,sans-serif"><br></span></div><div><font face="arial, sans-serif">if this still does not work for you, then this might be a different issue i was unable to reproduce.</font></div>






<div><font face="arial, sans-serif">in this case, could you run mpirun under gdb and send a gdb stack trace ?</font></div><div><font face="arial, sans-serif"><br></font></div><div><font face="arial, sans-serif"><br></font></div>






<div><font face="arial, sans-serif">Cheers,</font></div><div><font face="arial, sans-serif"><br></font></div><div><font face="arial, sans-serif">Gilles</font></div><div><br></div><div><br></div><div class="gmail_extra"><br>






<br><div class="gmail_quote"><div>On Mon, Jun 2, 2014 at 8:26 PM, Mike Dubman <span dir="ltr">&lt;<a href="mailto:miked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt;</span> wrote:<br>
</div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div>
<div dir="ltr">more info, specifying --mca rtc_freq_priority 0 explicitly, generates different kind of fail:<div><br><div><div>$/scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/bin/mpirun -np 8 -mca btl sm,tcp --mca rtc_freq_priority 0 /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/examples/hello_usempi</div>







<div>[vegas12:13887] *** Process received signal ***</div><div>[vegas12:13887] Signal: Segmentation fault (11)</div><div>[vegas12:13887] Signal code: Address not mapped (1)</div><div>[vegas12:13887] Failing at address: 0x20</div>







<div>[vegas12:13887] [ 0] /lib64/libpthread.so.0[0x3937c0f500]</div><div>[vegas12:13887] [ 1] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/lib/libopen-rte.so.0(orte_plm_base_post_launch+0x90)[0x7ffff7dcbe50]</div>







<div>[vegas12:13887] [ 2] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/lib/libopen-pal.so.0(opal_libevent2021_event_base_loop+0x8bc)[0x7ffff7b1076c]</div><div>[vegas12:13887] [ 3] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/bin/mpirun(orterun+0x126d)[0x40501d]</div>







<div>[vegas12:13887] [ 4] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/bin/mpirun(main+0x20)[0x4039e4]</div><div>[vegas12:13887] [ 5] /lib64/libc.so.6(__libc_start_main+0xfd)[0x393741ecdd]</div>







<div>[vegas12:13887] [ 6] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/bin/mpirun[0x403909]</div><div>[vegas12:13887] *** End of error message ***</div><div>Segmentation fault (core dumped)</div>







</div></div></div><div><div class="gmail_extra"><br><br><div class="gmail_quote">On Mon, Jun 2, 2014 at 2:24 PM, Mike Dubman <span dir="ltr">&lt;<a href="mailto:miked@dev.mellanox.co.il" target="_blank">miked@dev.mellanox.co.il</a>&gt;</span> wrote:<br>







<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr">Hi,<div>This fix &quot;<span style="font-family:Verdana,Helvetica,&#39;sans serif&#39;;font-size:11px">orte_rtc_base_select: skip a RTC module if it has a zero priority&quot; did not help and jenkins stilll fails as before.</span></div>








<div><span style="font-family:Verdana,Helvetica,&#39;sans serif&#39;;font-size:11px">The ompi was configured:</span></div><div><font face="Verdana, Helvetica, sans serif"><span style="font-size:11px">--with-platform=contrib/platform/mellanox/optimized --with-ompi-param-check --enable-picky --with-knem --with-mxm --with-fca</span></font><br>








</div><div><font face="Verdana, Helvetica, sans serif"><span style="font-size:11px"><br></span></font></div><div><span style="font-family:Verdana,Helvetica,&#39;sans serif&#39;;font-size:11px">The run was on single node:</span></div>








<div><pre style="word-wrap:break-word;margin-top:0px;margin-bottom:0px"><font><span style="font-size:11px;white-space:pre-wrap">$/scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/bin/mpirun -np 8 -mca btl sm,tcp /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/examples/hello_usempi
[vegas12:13834] *** Process received signal ***
[vegas12:13834] Signal: Segmentation fault (11)
[vegas12:13834] Signal code: Address not mapped (1)
[vegas12:13834] Failing at address: (nil)
[vegas12:13834] [ 0] /lib64/libpthread.so.0[0x3937c0f500]
[vegas12:13834] [ 1] /lib64/libc.so.6(fgets+0x2d)[0x3937466f2d]
[vegas12:13834] [ 2] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/lib/openmpi/mca_rtc_freq.so(+0x1f3f)[0x7ffff41f5f3f]
[vegas12:13834] [ 3] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/lib/openmpi/mca_rtc_freq.so(+0x279b)[0x7ffff41f679b]
[vegas12:13834] [ 4] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/lib/libopen-rte.so.0(orte_rtc_base_select+0xe6)[0x7ffff7ddc036]
[vegas12:13834] [ 5] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/lib/openmpi/mca_ess_hnp.so(+0x4056)[0x7ffff725b056]
[vegas12:13834] [ 6] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/lib/libopen-rte.so.0(orte_init+0x174)[0x7ffff7d97254]
[vegas12:13834] [ 7] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/bin/mpirun(orterun+0x863)[0x404613]
[vegas12:13834] [ 8] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/bin/mpirun(main+0x20)[0x4039e4]
[vegas12:13834] [ 9] /lib64/libc.so.6(__libc_start_main+0xfd)[0x393741ecdd]
[vegas12:13834] [10] /scrap/jenkins/scrap/workspace/hpc-ompi-shmem/label/hpc-test-node/ompi_install1/bin/mpirun[0x403909]
[vegas12:13834] *** End of error message ***
Segmentation fault (core dumped)</span></font><span style="font-size:11px;white-space:pre-wrap">
</span></pre></div><div><br></div></div><div class="gmail_extra"><br><br><div class="gmail_quote"><div>On Mon, Jun 2, 2014 at 10:19 AM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles.gouaillardet@gmail.com" target="_blank">gilles.gouaillardet@gmail.com</a>&gt;</span> wrote:<br>








</div><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div dir="ltr"><div><div><div><div>Mike and Ralph,<br><br></div>i could not find a simple workaround.<br>




<br>for the time being, i commited r31926 and invite those who face a similar issue to use the following workaround :<br>

</div>export OMPI_MCA_rtc_freq_priority=0<br></div>/* or mpirun --mca rtc_freq_priority 0 ... */<br><br></div>Cheers,<br><br>Gilles<div><br><br><div class="gmail_extra"><br><br><div class="gmail_quote">On Mon, Jun 2, 2014 at 3:45 PM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles.gouaillardet@gmail.com" target="_blank">gilles.gouaillardet@gmail.com</a>&gt;</span> wrote:<br>









<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><div><div>in orte/mca/rtc/freq/rtc_freq.c at line 187<br></div>fp = fopen(filename, &quot;r&quot;);<br>









</div>and filename is &quot;/sys/devices/system/cpu/cpu0/cpufreq/scaling_governor&quot;<br>
<br></div>there is no error check, so if fp is NULL, orte_getline() will call fgets() that will crash.<br></div></blockquote></div></div></div></div>
<br></div><div>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/06/14939.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/06/14939.php</a><br></blockquote></div><br></div>
</blockquote></div><br></div>
</div><br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/06/14945.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/06/14945.php</a><br></blockquote></div><br></div></div>
<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/06/14947.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/06/14947.php</a><br></blockquote></div><br></div>
<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br></div>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/06/14948.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/06/14948.php</a><br></blockquote></div><br></div>
<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/06/14949.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/06/14949.php</a><br></blockquote></div><br></div>
_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>


Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/06/14950.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/06/14950.php</a></blockquote></div><br></div></div></blockquote>


</div><br></div></div></div><br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/06/14956.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/06/14956.php</a><br></blockquote></div><br></div>
_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>

</div></div>Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/06/14957.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/06/14957.php</a></blockquote></div><br></div></div>

<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/06/14958.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/06/14958.php</a><br></blockquote></div><br></div>
_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</div></div>Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/06/14959.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/06/14959.php</a></blockquote></div><br></div></div>
<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/06/14960.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/06/14960.php</a><br></blockquote></div><br></div>

