Index: ompi/mca/btl/openib/connect/btl_openib_connect_udcm.c
===================================================================
--- ompi/mca/btl/openib/connect/btl_openib_connect_udcm.c	(revision 28724)
+++ ompi/mca/btl/openib/connect/btl_openib_connect_udcm.c	(working copy)
@@ -828,7 +828,6 @@
 
 static void udcm_module_destroy_listen_qp (udcm_module_t *m)
 {
-    enum ibv_qp_attr_mask attr_mask;
     struct ibv_qp_attr attr;
     struct ibv_wc wc;
     mca_btl_openib_async_cmd_t async_command;
@@ -858,7 +857,6 @@
 	memset(&attr, 0, sizeof(attr));
 	attr.qp_state = IBV_QPS_ERR;
 	attr.sq_psn = 0;
-	attr_mask = IBV_QP_STATE | IBV_QP_SQ_PSN;
 
 	BTL_VERBOSE(("Setting qp to err state %p", (void *)m->listen_qp));
 
Index: ompi/mca/coll/inter/coll_inter.c
===================================================================
--- ompi/mca/coll/inter/coll_inter.c	(revision 28724)
+++ ompi/mca/coll/inter/coll_inter.c	(working copy)
@@ -144,12 +144,8 @@
 mca_coll_inter_module_enable(mca_coll_base_module_t *module,
                              struct ompi_communicator_t *comm)
 {
-    int size, rank;
     mca_coll_inter_module_t *inter_module = (mca_coll_inter_module_t*) module;
     
-    rank = ompi_comm_rank(comm);
-    size = ompi_comm_size(comm);
-    
     inter_module->inter_comm = comm;
     
 #if 0
Index: ompi/mca/coll/inter/coll_inter_allreduce.c
===================================================================
--- ompi/mca/coll/inter/coll_inter_allreduce.c	(revision 28724)
+++ ompi/mca/coll/inter/coll_inter_allreduce.c	(working copy)
@@ -44,13 +44,12 @@
                                struct ompi_communicator_t *comm,
                                mca_coll_base_module_t *module)
 {
-    int err, rank, root = 0, rsize;
+    int err, rank, root = 0;
     ptrdiff_t lb, extent;
     char *tmpbuf = NULL, *pml_buffer = NULL;
     ompi_request_t *req[2];
 
     rank = ompi_comm_rank(comm);
-    rsize = ompi_comm_remote_size(comm);
     
     /* Perform the reduction locally */
     err = ompi_datatype_get_extent(dtype, &lb, &extent);
Index: ompi/mca/coll/inter/coll_inter_bcast.c
===================================================================
--- ompi/mca/coll/inter/coll_inter_bcast.c	(revision 28724)
+++ ompi/mca/coll/inter/coll_inter_bcast.c	(working copy)
@@ -41,11 +41,9 @@
                            struct ompi_communicator_t *comm,
                            mca_coll_base_module_t *module)
 {
-    int rsize;
     int rank;
     int err;
 
-    rsize = ompi_comm_remote_size(comm);
     rank = ompi_comm_rank(comm);
 
     if (MPI_PROC_NULL == root) {
Index: ompi/mca/coll/inter/coll_inter_reduce.c
===================================================================
--- ompi/mca/coll/inter/coll_inter_reduce.c	(revision 28724)
+++ ompi/mca/coll/inter/coll_inter_reduce.c	(working copy)
@@ -43,14 +43,13 @@
                             int root, struct ompi_communicator_t *comm,
                             mca_coll_base_module_t *module)
 {
-    int rank, err, size;
+    int rank, err;
     ptrdiff_t true_lb, true_extent, lb, extent;
     char *free_buffer = NULL;
     char *pml_buffer = NULL;
 
     /* Initialize */
     rank = ompi_comm_rank(comm);
-    size = ompi_comm_remote_size(comm);
 
     if (MPI_PROC_NULL == root) {
         /* do nothing */
Index: ompi/mca/coll/ml/coll_ml_module.c
===================================================================
--- ompi/mca/coll/ml/coll_ml_module.c	(revision 28724)
+++ ompi/mca/coll/ml/coll_ml_module.c	(working copy)
@@ -624,7 +624,6 @@
 static void ml_init_k_nomial_trees(mca_coll_ml_topology_t *topo, int *list_of_ranks_in_all_subgroups, int my_rank_in_list)
 {
     int *list_n_connected;
-    int *list;
     int group_size, rank, i, j, knt, offset, k, my_sbgp = 0;
     int my_root;
     int level_one_knt;
@@ -689,8 +688,6 @@
            on the bcol module
          */
         list_n_connected = (int *) malloc(sizeof(int)*group_size);
-        /* pointer to group list */
-        list = pair->subgroup_module->group_list;
         /* next thing to do is to find out which subgroup I'm in
          * at this particular level
          */
@@ -1939,7 +1936,6 @@
                 *my_proc = NULL;
 
     const mca_sbgp_base_component_2_0_0_t *sbgp_component = NULL;
-    const mca_bcol_base_component_2_0_0_t *bcol_component = NULL;
 
 
     int i_hier = 0, n_hier = 0, ll_p1,
@@ -2074,7 +2070,6 @@
          */
 
         sbgp_component = (mca_sbgp_base_component_2_0_0_t *) sbgp_cli->component.cli_component;
-        bcol_component = (mca_bcol_base_component_2_0_0_t *) bcol_cli->cli_component;
 
         /* Skip excluded levels */
         if (NULL != exclude_sbgp_name) {
@@ -2702,7 +2697,6 @@
     int32_t **route_table = NULL;
     int32_t *all_reachable_ranks = NULL;
 
-    struct ompi_proc_t **ompi_procs = NULL;
     struct ompi_proc_t **sbgp_procs = NULL;
 
     mca_sbgp_base_module_t *sbgp_group = NULL;
@@ -2735,7 +2729,6 @@
     }
 
     all_reachable_ranks[my_rank] = IS_RECHABLE;
-    ompi_procs = comm->c_local_group->grp_proc_pointers;
 
     for (level = 0; level < topo->n_levels; ++level) {
         sbgp_group = topo->component_pairs[level].subgroup_module;
@@ -3247,7 +3240,6 @@
 
     mca_coll_ml_module_t *ml_module = NULL;
     mca_coll_ml_component_t *cs = &mca_coll_ml_component;
-    double start, end, tic;
     bool iboffload_was_requested = mca_coll_ml_check_if_bcol_is_requested("iboffload");
 
     ML_VERBOSE(10, ("ML comm query start.\n"));
@@ -3315,15 +3307,11 @@
      * setup communicator hierarchy - the ml component is available for
      * caching information about the sbgp modules selected.
      */
-    start = ret_us();
     ret = ml_discover_hierarchy(ml_module);
     if (OMPI_SUCCESS != ret) {
         ML_ERROR(("ml_discover_hierarchy exited with error.\n"));
         goto CLEANUP;
     }
-    end = ret_us();
-    tic = end - start;
-    /*fprintf(stderr,"discover hierarchy %1.8f\n",tic);*/
 
     /* gvm Disabled for debuggin */
     ret = mca_coll_ml_build_filtered_fn_table(ml_module);
Index: ompi/mca/coll/sm/coll_sm_reduce.c
===================================================================
--- ompi/mca/coll/sm/coll_sm_reduce.c	(revision 28724)
+++ ompi/mca/coll/sm/coll_sm_reduce.c	(working copy)
@@ -230,7 +230,6 @@
         size_t count_left = (size_t)count;
         int frag_num = 0;
         bool first_operation = true;
-        bool sbuf_copied_to_rbuf = false;
 
         /* If the datatype is the same packed as it is unpacked, we
            can save a memory copy and just do the reduction operation
@@ -362,7 +361,6 @@
                             ompi_datatype_copy_content_same_ddt(dtype, count,
                                                reduce_target, (char*)sbuf);
                         }
-                        sbuf_copied_to_rbuf = true;
                     }
                 } 
 
Index: ompi/mca/sbgp/basesmsocket/sbgp_basesmsocket_component.c
===================================================================
--- ompi/mca/sbgp/basesmsocket/sbgp_basesmsocket_component.c	(revision 28724)
+++ ompi/mca/sbgp/basesmsocket/sbgp_basesmsocket_component.c	(working copy)
@@ -259,7 +259,7 @@
     mca_sbgp_basesmsocket_module_t *module;
     int ret;
     int my_socket_index;
-    int proc, cnt, local, n_local_peers, my_index, my_rank;
+    int proc, cnt, local, n_local_peers, my_rank;
     ompi_proc_t* my_proc;
     int *local_ranks_in_comm=NULL;
     int *socket_info=NULL, my_socket_info;
@@ -269,11 +269,6 @@
     output_data=NULL;
     my_rank=ompi_comm_rank(comm);
     my_proc=ompi_comm_peer_lookup(comm,my_rank);
-    for( proc=0 ; proc < n_procs_in ; proc++) {
-        if( procs[proc]==my_proc){
-            my_index=proc;
-        }
-    }
 
     /*create a new module*/
     module=OBJ_NEW(mca_sbgp_basesmsocket_module_t);
Index: orte/mca/routed/binomial/routed_binomial.c
===================================================================
--- orte/mca/routed/binomial/routed_binomial.c	(revision 28724)
+++ orte/mca/routed/binomial/routed_binomial.c	(working copy)
@@ -840,7 +840,6 @@
     int i, bitmap, peer, hibit, mask, found;
     orte_routed_tree_t *child;
     opal_bitmap_t *relations;
-    orte_process_name_t proc_name;
 
     OPAL_OUTPUT_VERBOSE((3, orte_routed_base_framework.framework_output,
                          "%s routed:binomial rank %d parent %d me %d num_procs %d",
@@ -865,12 +864,6 @@
                                      rank,
                                      ORTE_VPID_PRINT(child->vpid)));
 
-                /* If the process we are looking at next is already dead, then
-                 * we inherit its children. Keep up with the process name of
-                 * that process so we can check it's state.
-                 */
-                proc_name.vpid = peer;
-
                 if (mine) {
                     /* this is a direct child - add it to my list */
                     opal_list_append(childrn, &child->super);
@@ -913,8 +906,6 @@
                                  ORTE_NAME_PRINT(ORTE_PROC_MY_NAME)));
             /* execute compute on this child */
             if (0 <= (found = binomial_tree(peer, rank, me, num_procs, nchildren, childrn, relatives, mine))) {
-                proc_name.vpid = found;
-
                 OPAL_OUTPUT_VERBOSE((5, orte_routed_base_framework.framework_output,
                                      "%s routed:binomial find children returning found value %d",
                                      ORTE_NAME_PRINT(ORTE_PROC_MY_NAME), found));
