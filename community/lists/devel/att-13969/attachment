<div dir="ltr">Ralph,<div><br></div><div>As I said this is NOT a cluster - it is a 4k-core shared memory machine.</div><div>TORQUE is allocating cpus (time-shared mode, IIRC), not nodes.</div><div>So, there is always exactly one line in $PBS_NODESFILE.</div>
<div><br></div><div>The system runs as 2 partitions of 2k-cores each.</div><div>So, the contents odf$PBS_NODESFILE has exactly 2 possible values, each 1 line.</div><div><br></div><div>The values of PBS_PPN and PBS_NCPUS both reflect the size of the allocation.</div>
<div><br></div><div>At a minimum, shouldn&#39;t Open MPI be multiplying the lines in $PBS_NODESFILE by the value of $PBS_PPN?</div><div><br></div><div>Additionally, when I try &quot;mpirun -npernode 16 ./ring_c&quot; I am still told there are not enough slots.</div>
<div>Shouldn&#39;t that be working with 1 line is $PBS_NODESFILE?</div><div><br></div><div>-Paul</div><div><br></div><div><br></div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Fri, Jan 31, 2014 at 2:47 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">We read the nodes from the PBS_NODEFILE, Paul - can you pass that along?<div><br><div>
<div>On Jan 31, 2014, at 2:33 PM, Paul Hargrove &lt;<a href="mailto:phhargrove@lbl.gov" target="_blank">phhargrove@lbl.gov</a>&gt; wrote:</div><br><blockquote type="cite"><div dir="ltr">I am trying to test the trunk on an SGI UV (to validate Nathan&#39;s port of btl:vader to SGI&#39;s variant of xpmem).<div>
<br></div><div>At configure time, PBS&#39;s TM support was correctly located.</div><div>
<br></div><div>My PBS batch script includes</div><div><font face="courier new, monospace">  #PBS -l ncpus=16</font></div><div>because that is what this installation requires (not nodes, mppnodes, or anything like that).</div>

<div>One is allocating cpus on a large shared-memory machine, not a set of nodes in a cluster.</div><div><br></div><div>However, this appears to be causing mpirun to think I have just 1 slot:</div><div><br></div><div><div>

<font face="courier new, monospace">+ mpirun -np 2 ./ring_c</font></div><div><font face="courier new, monospace">--------------------------------------------------------------------------</font></div><div><font face="courier new, monospace">There are not enough slots available in the system to satisfy the 2 slots </font></div>

<div><font face="courier new, monospace">that were requested by the application:</font></div><div><font face="courier new, monospace">  ./ring_c</font></div><div><font face="courier new, monospace"><br></font></div><div>
<font face="courier new, monospace">Either request fewer slots for your application, or make more slots available</font></div>
<div><font face="courier new, monospace">for use.</font></div><div><font face="courier new, monospace">--------------------------------------------------------------------------</font></div></div><div><br></div><div>In case they contain useful info, here are the PBS env vars in the job:</div>

<div><br></div><div><div><font face="courier new, monospace">PBS_HT_NCPUS=32</font></div><div><font face="courier new, monospace">PBS_VERSION=TORQUE-2.3.13</font></div><div><font face="courier new, monospace">PBS_JOBNAME=qs</font></div>

<div><font face="courier new, monospace">PBS_ENVIRONMENT=PBS_BATCH</font></div><div><font face="courier new, monospace">PBS_HOME=/var/spool/torque</font></div><div><font face="courier new, monospace">PBS_O_WORKDIR=/usr/users/6/hargrove/SCRATCH/OMPI/openmpi-trunk-linux-x86_64-uv-trunk/BLD/examples</font></div>

<div><font face="courier new, monospace">PBS_PPN=16</font></div><div><font face="courier new, monospace">PBS_TASKNUM=1</font></div><div><font face="courier new, monospace">PBS_O_HOME=/usr/users/6/hargrove</font></div><div>

<font face="courier new, monospace">PBS_MOMPORT=15003</font></div><div><font face="courier new, monospace">PBS_O_QUEUE=debug</font></div><div><font face="courier new, monospace">PBS_O_LOGNAME=hargrove</font></div><div><font face="courier new, monospace">PBS_O_LANG=en_US.UTF-8</font></div>

<div><font face="courier new, monospace">PBS_JOBCOOKIE=9EEF5DF75FA705A241FEF66EDFE01C5B</font></div><div><font face="courier new, monospace">PBS_NODENUM=0</font></div><div><font face="courier new, monospace">PBS_O_SHELL=/usr/psc/shells/bash</font></div>

<div><font face="courier new, monospace">PBS_SERVER=<a href="http://tg-login1.blacklight.psc.teragrid.org/" target="_blank">tg-login1.blacklight.psc.teragrid.org</a></font></div><div><font face="courier new, monospace">PBS_JOBID=<a href="http://314827.tg-login1.blacklight.psc.teragrid.org/" target="_blank">314827.tg-login1.blacklight.psc.teragrid.org</a></font></div>

<div><font face="courier new, monospace">PBS_NCPUS=16</font></div><div><font face="courier new, monospace">PBS_O_HOST=<a href="http://tg-login1.blacklight.psc.teragrid.org/" target="_blank">tg-login1.blacklight.psc.teragrid.org</a></font></div>

<div><font face="courier new, monospace">PBS_VNODENUM=0</font></div><div><font face="courier new, monospace">PBS_QUEUE=debug_r1</font></div><div><font face="courier new, monospace">PBS_O_MAIL=/var/mail/hargrove</font></div>

<div><font face="courier new, monospace">PBS_NODEFILE=/var/spool/torque/aux//<a href="http://314827.tg-login1.blacklight.psc.teragrid.org/" target="_blank">314827.tg-login1.blacklight.psc.teragrid.org</a></font></div><div>
<font face="courier new, monospace">PBS_O_PATH=[...removed...]</font></div>
</div><div><br clear="all"><div>If any additional info is needed to help make mpirun &quot;just work&quot;, please let me know.</div><div><br></div><div>However, at this point I am mostly interested in any work-arounds that will let me run something other than a singleton on this system.</div>

<div><br></div><div>-Paul</div><span class="HOEnZb"><font color="#888888"><div><br></div>-- <br><font face="courier new, monospace"><div>Paul H. Hargrove                          <a href="mailto:PHHargrove@lbl.gov" target="_blank">PHHargrove@lbl.gov</a></div>
<div>
Future Technologies Group</div><div>Computer and Data Sciences Department     Tel: <a href="tel:%2B1-510-495-2352" value="+15104952352" target="_blank">+1-510-495-2352</a></div><div>Lawrence Berkeley National Laboratory     Fax: <a href="tel:%2B1-510-486-6900" value="+15104866900" target="_blank">+1-510-486-6900</a></div>
</font>
</font></span></div></div><span class="HOEnZb"><font color="#888888">
_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></font></span></blockquote>
</div><br></div></div><br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br></blockquote></div><br><br clear="all"><div><br></div>-- <br><font face="courier new, monospace"><div>
Paul H. Hargrove                          <a href="mailto:PHHargrove@lbl.gov" target="_blank">PHHargrove@lbl.gov</a></div><div>Future Technologies Group</div><div>Computer and Data Sciences Department     Tel: +1-510-495-2352</div>
<div>Lawrence Berkeley National Laboratory     Fax: +1-510-486-6900</div></font>
</div>

