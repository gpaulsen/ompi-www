<div dir="ltr">hi Howard,<div><br></div><div>Was this issue resolved ?  If so, what is the solution ?</div><div>Please let me know.</div><div>Curious to know , since we are also experimenting with these limits.</div><div><br></div><div>Thanks,</div><div>- Sreenidhi.</div><div><br></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Tue, Jul 19, 2016 at 10:50 AM, Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles@rist.or.jp" target="_blank">gilles@rist.or.jp</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
  
    
  
  <div bgcolor="#FFFFFF" text="#000000">
    <p>Howard,</p>
    <p><br>
    </p>
    <p>did you bump both btl_tcp_rndv_eager_limit and
      btl_tcp_eager_limit ?</p>
    <p>you might also need to bump btl_tcp_sndbuf, btl_tcp_rcvbuf and
      btl_tcp_max_send_size to get the max performance out of your 100Gb
      ethernet cards</p>
    <p>last but not least, you might also need to bump btl_tcp_links to
      saturate your network (that is likely a good thing when running 1
      task per node, but that can lead to decreased performance when
      running several tasks per node)<br>
    </p>
    <p>Cheers,</p>
    <p><br>
    </p>
    <p>Gilles<br>
    </p><div><div class="h5">
    <br>
    <div>On 7/19/2016 6:57 AM, Howard Pritchard
      wrote:<br>
    </div>
    </div></div><blockquote type="cite"><div><div class="h5">
      <div dir="ltr">Hi Folks,
        <div><br>
        </div>
        <div>I have a cluster with some 100 Gb ethernet cards</div>
        <div>installed.  What we are noticing if we force Open MPI
          1.10.3</div>
        <div>to go through the TCP BTL (rather than yalla)  is that</div>
        <div>the performance of osu_bw once the TCP BTL switches</div>
        <div>from eager to rendezvous (&gt; 32KB)</div>
        <div>falls off a cliff, going from about 1.6 GB/sec to 233
          MB/sec</div>
        <div>and stays that way out to 4 MB message lengths.</div>
        <div><br>
        </div>
        <div>There&#39;s nothing wrong with the IP stack (iperf -P4 gives </div>
        <div>63 Gb/sec).</div>
        <div><br>
        </div>
        <div>So, my questions are </div>
        <div><br>
        </div>
        <div>1) is this performance expected for the TCP BTL when in</div>
        <div>rendezvous mode?</div>
        <div>2) is there some way to get more like the single socket</div>
        <div>performance obtained with iperf for large messages (~16
          Gb/sec).</div>
        <div><br>
        </div>
        <div>We tried adjusting the tcp_btl_rendezvous threshold but
          that doesn&#39;t</div>
        <div>appear to actually be adjustable from the mpirun command
          line.</div>
        <div><br>
        </div>
        <div>Thanks for any suggestions,</div>
        <div><br>
        </div>
        <div>Howard</div>
        <div><br>
        </div>
        <div><br>
        </div>
        <div><br>
        </div>
      </div>
      <br>
      <fieldset></fieldset>
      <br>
      </div></div><pre>_______________________________________________
devel mailing list
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/07/19237.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/07/19237.php</a></pre>
    </blockquote>
    <br>
  </div>

<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="https://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">https://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/07/19240.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/07/19240.php</a><br></blockquote></div><br></div>

