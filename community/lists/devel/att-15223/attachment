<div dir="ltr">My understanding is that both of these clauses are based on the fact that there are ongoing communications between two processes when one of them decide to shut down. From an MPI perspective, I can hardly see a case where this is legit.<div>
<br></div><div>  George.</div><div><br></div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Wed, Jul 23, 2014 at 8:33 AM, Yossi Etigin <span dir="ltr">&lt;<a href="mailto:yosefe@mellanox.com" target="_blank">yosefe@mellanox.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">





<div lang="EN-US" link="blue" vlink="purple">
<div>
<p><u></u><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d"><span>1.<span style="font:7.0pt &quot;Times New Roman&quot;">      
</span></span></span><u></u><span dir="LTR"></span><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d">If the barrier is before del_proc, it does guarantee all MPI calls have been completed by all other ranks, but it does not
 guarantee all ACKs have been delivered. For MXM, closing the connection (del_procs call completed) guarantees that my rank got all ACKs. So we need a barrier between del_procs and pml_finalize, because only when all other ranks closed their connection it’s
 safe to destroy the global pml resources.<u></u><u></u></span></p>
<p><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d"><u></u> <u></u></span></p>
<p><u></u><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d"><span>2.<span style="font:7.0pt &quot;Times New Roman&quot;">      
</span></span></span><u></u><span dir="LTR"></span><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d">In order to avoid a situation when rankA starts disconnecting from rankB, while rankB is still doing MPI work. In this case
 rankB will not be able to communicate with rankA any more, while it still has work to do.<u></u><u></u></span></p>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d"><u></u> <u></u></span></p>
<p class="MsoNormal"><b><span style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;">From:</span></b><span style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;"> devel [mailto:<a href="mailto:devel-bounces@open-mpi.org" target="_blank">devel-bounces@open-mpi.org</a>]
<b>On Behalf Of </b>George Bosilca<br>
<b>Sent:</b> Monday, July 21, 2014 9:11 PM</span></p><div><div class="h5"><br>
<b>To:</b> Open MPI Developers<br>
<b>Subject:</b> Re: [OMPI devel] barrier before calling del_procs<u></u><u></u></div></div><p></p><div><div class="h5">
<p class="MsoNormal"><u></u> <u></u></p>
<div>
<div>
<div>
<p class="MsoNormal">On Mon, Jul 21, 2014 at 1:41 PM, Yossi Etigin &lt;<a href="mailto:yosefe@mellanox.com" target="_blank">yosefe@mellanox.com</a>&gt; wrote:<u></u><u></u></p>
<div>
<div>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d">Right, but:</span><u></u><u></u></p>
<p><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d">1.</span><span style="font-size:7.0pt;color:#1f497d">      
</span><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d">IMHO the rte_barrier in the wrong place (in the trunk)</span><u></u><u></u></p>
</div>
</div>
<div>
<p class="MsoNormal"><u></u> <u></u></p>
</div>
<div>
<p class="MsoNormal">In the trunk we have the rte_barrier prior to del_proc, which is what I would have expected: quiescence the BTLs by reaching a point where everybody agree that no more MPI messages will be exchanged, and then delete the BTLs.<u></u><u></u></p>

</div>
<div>
<p class="MsoNormal"> <u></u><u></u></p>
</div>
<blockquote style="border:none;border-left:solid #cccccc 1.0pt;padding:0in 0in 0in 6.0pt;margin-left:4.8pt;margin-right:0in">
<div>
<div>
<p><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d">2.</span><span style="font-size:7.0pt;color:#1f497d">      
</span><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d">In addition to the rte_barrier, need also mpi_barrier</span><u></u><u></u></p>
</div>
</div>
</blockquote>
<div>
<p class="MsoNormal">Care for providing a reasoning for this barrier? Why and where should it be placed?<u></u><u></u></p>
</div>
<div>
<p class="MsoNormal"><u></u> <u></u></p>
</div>
<div>
<p class="MsoNormal">  George.<u></u><u></u></p>
</div>
<div>
<p class="MsoNormal"><u></u> <u></u></p>
</div>
<div>
<p class="MsoNormal"><u></u> <u></u></p>
</div>
<div>
<p class="MsoNormal"> <u></u><u></u></p>
</div>
<blockquote style="border:none;border-left:solid #cccccc 1.0pt;padding:0in 0in 0in 6.0pt;margin-left:4.8pt;margin-right:0in">
<div>
<div>
<p class="MsoNormal"><span style="font-size:11.0pt;font-family:&quot;Calibri&quot;,&quot;sans-serif&quot;;color:#1f497d"> </span><u></u><u></u></p>
<p class="MsoNormal"><b><span style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;">From:</span></b><span style="font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;"> devel [mailto:<a href="mailto:devel-bounces@open-mpi.org" target="_blank">devel-bounces@open-mpi.org</a>]
<b>On Behalf Of </b>George Bosilca<br>
<b>Sent:</b> Monday, July 21, 2014 8:19 PM<br>
<b>To:</b> Open MPI Developers</span><u></u><u></u></p>
<div>
<div>
<p class="MsoNormal"><br>
<b>Subject:</b> Re: [OMPI devel] barrier before calling del_procs<u></u><u></u></p>
</div>
</div>
<div>
<div>
<p class="MsoNormal"> <u></u><u></u></p>
<div>
<p class="MsoNormal">There was a long thread of discussion on why we must use an rte_barrier and not an mpi_barrier during the finalize. Basically, we long as we have connectionless unreliable BTLs
 we need an external mechanism to ensure complete tear-down of the entire infrastructure. Thus, we need to rely on an rte_barrier not because it guarantees the correctness of the code, but because it provides enough time to all processes to flush all HPC traffic.<u></u><u></u></p>

<div>
<p class="MsoNormal"> <u></u><u></u></p>
</div>
<div>
<p class="MsoNormal">  George.<u></u><u></u></p>
</div>
<div>
<p class="MsoNormal"> <u></u><u></u></p>
</div>
</div>
<div>
<p class="MsoNormal" style="margin-bottom:12.0pt"> <u></u><u></u></p>
<div>
<p class="MsoNormal">On Mon, Jul 21, 2014 at 1:10 PM, Yossi Etigin &lt;<a href="mailto:yosefe@mellanox.com" target="_blank">yosefe@mellanox.com</a>&gt; wrote:<u></u><u></u></p>
<p class="MsoNormal">I see. But in branch v1.8, in 31869, Ralph reverted the commit which moved del_procs after the barrier:<br>
  &quot;Revert r31851 until we can resolve how to close these leaks without causing the usnic BTL to fail during disconnect of intercommunicators<br>
   Refs #4643&quot;<br>
Also, we need an rte barrier after del_procs - because otherwise rankA could call pml_finalize() before rankB finishes disconnecting from rankA.<br>
<br>
I think the order in finalize should be like this:<br>
        1. mpi_barrier(world)<br>
        2. del_procs()<br>
        3. rte_barrier()<br>
        4. pml_finalize()<u></u><u></u></p>
<div>
<div>
<p class="MsoNormal"><br>
-----Original Message-----<br>
From: Nathan Hjelm [mailto:<a href="mailto:hjelmn@lanl.gov" target="_blank">hjelmn@lanl.gov</a>]<br>
Sent: Monday, July 21, 2014 8:01 PM<br>
To: Open MPI Developers<br>
Cc: Yossi Etigin<br>
Subject: Re: [OMPI devel] barrier before calling del_procs<br>
<br>
I should add that it is an rte barrier and not an MPI barrier for technical reasons.<br>
<br>
-Nathan<br>
<br>
On Mon, Jul 21, 2014 at 09:42:53AM -0700, Ralph Castain wrote:<br>
&gt;    We already have an rte barrier before del procs<br>
&gt;<br>
&gt;    Sent from my iPhone<br>
&gt;    On Jul 21, 2014, at 8:21 AM, Yossi Etigin &lt;<a href="mailto:yosefe@mellanox.com" target="_blank">yosefe@mellanox.com</a>&gt; wrote:<br>
&gt;<br>
&gt;      Hi,<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;      We get occasional hangs with MTL/MXM during finalize, because a global<br>
&gt;      synchronization is needed before calling del_procs.<br>
&gt;<br>
&gt;      e.g rank A may call del_procs() and disconnect from rank B, while rank B<br>
&gt;      is still working.<br>
&gt;<br>
&gt;      What do you think about adding an MPI barrier on COMM_WORLD before<br>
&gt;      calling del_procs()?<br>
&gt;<br>
&gt;<br>
<br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; Link to this post:<br>
&gt; <a href="http://www.open-mpi.org/community/lists/devel/2014/07/15204.php" target="_blank">
http://www.open-mpi.org/community/lists/devel/2014/07/15204.php</a><br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><u></u><u></u></p>
</div>
</div>
<p class="MsoNormal">Link to this post:
<a href="http://www.open-mpi.org/community/lists/devel/2014/07/15206.php" target="_blank">
http://www.open-mpi.org/community/lists/devel/2014/07/15206.php</a><u></u><u></u></p>
</div>
<p class="MsoNormal"> <u></u><u></u></p>
</div>
</div>
</div>
</div>
</div>
<p class="MsoNormal"><br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">
http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/07/15208.php" target="_blank">
http://www.open-mpi.org/community/lists/devel/2014/07/15208.php</a><u></u><u></u></p>
</blockquote>
</div>
<p class="MsoNormal"><u></u> <u></u></p>
</div>
</div>
</div></div></div>
</div>

<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/07/15213.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/07/15213.php</a><br></blockquote></div><br></div>

