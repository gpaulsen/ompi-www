Thanks Josh.<div><br></div><div>I&#39;ve already check te prelink and is set to &quot;no&quot;.</div><div><br></div><div>I&#39;m going to try with the trunk head, and then i&#39;ll let you know how it goes.</div><div><br></div>
<div>Best regards.</div><div><br></div><div>Hugo Meyer<br><br><div class="gmail_quote">2011/1/25 Joshua Hursey <span dir="ltr">&lt;<a href="mailto:jjhursey@open-mpi.org">jjhursey@open-mpi.org</a>&gt;</span><br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
Can you try with the current trunk head (r24296)?<br>
I just committed a fix for the C/R functionality in which restarts were getting stuck. This will likely affect the migration functionality, but I have not had an opportunity to test just yet.<br>
<br>
Another thing to check is that prelink is turned off on all of your machines.<br>
  <a href="https://upc-bugs.lbl.gov//blcr/doc/html/FAQ.html#prelink" target="_blank">https://upc-bugs.lbl.gov//blcr/doc/html/FAQ.html#prelink</a><br>
<br>
Let me know if the problem persists, and I&#39;ll dig into a bit more.<br>
<br>
Thanks,<br>
Josh<br>
<div><div></div><div class="h5"><br>
On Jan 24, 2011, at 11:37 AM, Hugo Meyer wrote:<br>
<br>
&gt; Hello @ll<br>
&gt;<br>
&gt; I&#39;ve got a problem when i try to use the ompi-migrate command.<br>
&gt;<br>
&gt; What i&#39;m doing is execute for example the next application in one node of a cluster (both process wil run on the same node):<br>
&gt;<br>
&gt; mpirun -np 2 -am ft-enable-cr ./whoami 10 10<br>
&gt;<br>
&gt; Then in the same node i try to migrate the processes to another node:<br>
&gt;<br>
&gt; ompi-migrate -x node9 -t node3 14914<br>
&gt;<br>
&gt; And then i get this message:<br>
&gt;<br>
&gt; [clus9:15620] *** Process received signal ***<br>
&gt; [clus9:15620] Signal: Segmentation fault (11)<br>
&gt; [clus9:15620] Signal code: Address not mapped (1)<br>
&gt; [clus9:15620] Failing at address: (nil)<br>
&gt; [clus9:15620] [ 0] /lib64/libpthread.so.0 [0x2aaaac0b8d40]<br>
&gt; [clus9:15620] *** End of error message ***<br>
&gt; Segmentation fault<br>
&gt;<br>
&gt; I assume that maybe there is something wrong with the thread level, but i have configured the open-mpi like this:<br>
&gt;<br>
&gt; ../configure --prefix=/home/hmeyer/desarrollo/ompi-code/binarios/ --enable-debug --enable-debug-symbols --enable-trace --with-ft=cr --disable-ipv6 --enable-opal-multi-threads --enable-ft-thread --without-hwloc --disable-vt --with-blcr=/soft/blcr-0.8.2/ --with-blcr-libdir=/soft/blcr-0.8.2/lib/<br>

&gt;<br>
&gt; The checkpoint and restart works fine, but when i restore an application that has more than one process, this one is restored and executed until the last line before MPI_FINALIZE(), but the processes never finalize, i assume that they never call the MPI_FINALIZE(), but with one process ompi-checkpoint and ompi-restart work great.<br>

&gt;<br>
&gt; Best regards.<br>
&gt;<br>
&gt; Hugo Meyer<br>
</div></div>&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
<br>
------------------------------------<br>
Joshua Hursey<br>
Postdoctoral Research Associate<br>
Oak Ridge National Laboratory<br>
<a href="http://users.nccs.gov/~jjhursey" target="_blank">http://users.nccs.gov/~jjhursey</a><br>
<br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</blockquote></div><br></div>

