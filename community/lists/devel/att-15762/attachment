<html><head><meta http-equiv="Content-Type" content="text/html charset=us-ascii"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">I think the problem is that the MCA params need to be set at startup, along with the flag indicating where they came from, but also need to be changeable via the MPI_T interface at a later point. So we are tripping over the issues of when to release and replace the param values, ensuring they are properly handled and don't cause the MCA param system to crash upon finalize, etc.<div><br></div><div><br><div><div>On Aug 29, 2014, at 8:22 AM, Shamis, Pavel &lt;<a href="mailto:shamisp@ornl.gov">shamisp@ornl.gov</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="font-size: 12px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;">I was under impression that mca_tl_openib_tune_endpoint supposed to handle the miss-match between tunings of different devices.<br>Few years ago we did some "extreme" inter-operability testing and ompi handled all cases really well.<br><br>I'm not sure if I understand correctly what is the "core" issue.<br><br><br>Pavel (Pasha) Shamis<br>---<br>Computer Science Research Group<br>Computer Science and Math Division<br>Oak Ridge National Laboratory<br><br><br><br><br><br><br>On Aug 29, 2014, at 4:12 AM, Gilles Gouaillardet &lt;<a href="mailto:gilles.gouaillardet@iferc.org">gilles.gouaillardet@iferc.org</a>&lt;<a href="mailto:gilles.gouaillardet@iferc.org">mailto:gilles.gouaillardet@iferc.org</a>&gt;&gt; wrote:<br><br>Ralph,<br><br><br>r32639 and r32642 fixes bugs that do exist in both trunk and v1.8, and they can be considered as independent of the issue that is discussed in this thread and the one you pointed.<br><br>so imho, they should land v1.8 even if they do not fix the issue we are now discussing here<br><br>Cheers,<br><br>Gilles<br><br><br>On 2014/08/29 16:42, Ralph Castain wrote:<br><br>This is the email thread which sparked the problem:<br><br><a href="http://www.open-mpi.org/community/lists/devel/2014/07/15329.php">http://www.open-mpi.org/community/lists/devel/2014/07/15329.php</a><br><br>I actually tried to apply the original CMR and couldn't get it to work in the 1.8 branch - just kept having problems, so I pushed it off to 1.8.3. I'm leery to accept either of the current CMRs for two reasons: (a) none of the preceding changes is in the 1.8 series yet, and (b) it doesn't sound like we still have a complete solution.<br><br>Anyway, I just wanted to point to the original problem that was trying to be addressed.<br><br><br>On Aug 28, 2014, at 10:01 PM, Gilles Gouaillardet &lt;<a href="mailto:gilles.gouaillardet@iferc.org">gilles.gouaillardet@iferc.org</a>&gt;&lt;<a href="mailto:gilles.gouaillardet@iferc.org">mailto:gilles.gouaillardet@iferc.org</a>&gt; wrote:<br><br><br><br>Howard and Edgar,<br><br>i fixed a few bugs (r32639 and r32642)<br><br>the bug is trivial to reproduce with any mpi hello world program<br><br>mpirun -np 2 --mca btl openib,self hello_world<br><br>after setting the mca param in the $HOME/.openmpi/mca-params.conf<br><br>$ cat ~/.openmpi/mca-params.conf<br>btl_openib_receive_queues = S,12288,128,64,32:S,65536,128,64,3<br><br>good news is the program does not crash with a glory SIGSEGV any more<br>bad news is the program will (nicely) abort for an incorrect reason :<br><br>--------------------------------------------------------------------------<br>The Open MPI receive queue configuration for the OpenFabrics devices<br>on two nodes are incompatible, meaning that MPI processes on two<br>specific nodes were unable to communicate with each other. &nbsp;This<br>generally happens when you are using OpenFabrics devices from<br>different vendors on the same network. &nbsp;You should be able to use the<br>mca_btl_openib_receive_queues MCA parameter to set a uniform receive<br>queue configuration for all the devices in the MPI job, and therefore<br>be able to run successfully.<br><br>Local host: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;node0<br>Local adapter: &nbsp;&nbsp;&nbsp;mlx4_0 (vendor 0x2c9, part ID 4099)<br>Local queues: &nbsp;&nbsp;&nbsp;&nbsp;S,12288,128,64,32:S,65536,128,64,3<br><br>Remote host: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;node0<br>Remote adapter: &nbsp;&nbsp;(vendor 0x2c9, part ID 4099)<br>Remote queues:<br>P,128,256,192,128:S,2048,1024,1008,64:S,12288,1024,1008,64:S,65536,1024,1008,64<br><br>the root cause is the remote host did not send its receive_queues to the<br>local host<br>(and hence the local host believes the remote hosts uses the default value)<br><br>the logic was revamped vs v1.8, that is why v1.8 does not have such issue.<br><br>i am still thinking what should be the right fix :<br>- one option is to send the receive queues<br>- an other option would be to differenciate value overrided in<br>mca-params.conf (should be always ok) of value overrided in the .ini<br>(might want to double check local and remote values match)<br><br>Cheers,<br><br>Gilles<br><br>On 2014/08/29 7:02, Pritchard Jr., Howard wrote:<br><br><br>Hi Edgar,<br><br>Could you send me your conf file? &nbsp;I'll try to reproduce it.<br><br>Maybe run with --mca btl_base_verbose 20 or something to<br>see what the code that is parsing this field in the conf file<br>is finding.<br><br><br>Howard<br><br><br>-----Original Message-----<br>From: devel [<a href="mailto:devel-bounces@open-mpi.org">mailto:devel-bounces@open-mpi.org</a>] On Behalf Of Edgar Gabriel<br>Sent: Thursday, August 28, 2014 3:40 PM<br>To: Open MPI Developers<br>Subject: Re: [OMPI devel] segfault in openib component on trunk<br><br>to add another piece of information that I just found, the segfault only occurs if I have a particular mca parameter set in my mca-params.conf file, namely<br><br>btl_openib_receive_queues = S,12288,128,64,32:S,65536,128,64,3<br><br>Has the syntax for this parameter changed, or should/can I get rid of it?<br><br>Thanks<br>Edgar<br><br>On 08/28/2014 04:19 PM, Edgar Gabriel wrote:<br><br><br>we are having recently problems running trunk with openib component<br>enabled on one of our clusters. The problem occurs right in the<br>initialization part, here is the stack right before the segfault:<br><br>---snip---<br>(gdb) where<br>#0 &nbsp;mca_btl_openib_tune_endpoint (openib_btl=0x762a40,<br>endpoint=0x7d9660) at btl_openib.c:470<br>#1 &nbsp;0x00007f1062f105c4 in mca_btl_openib_add_procs (btl=0x762a40,<br>nprocs=2, procs=0x759be0, peers=0x762440, reachable=0x7fff22dd16f0) at<br>btl_openib.c:1093<br>#2 &nbsp;0x00007f106316102c in mca_bml_r2_add_procs (nprocs=2,<br>procs=0x759be0, reachable=0x7fff22dd16f0) at bml_r2.c:201<br>#3 &nbsp;0x00007f10615c0dd5 in mca_pml_ob1_add_procs (procs=0x70dc00,<br>nprocs=2) at pml_ob1.c:334<br>#4 &nbsp;0x00007f106823ed84 in ompi_mpi_init (argc=1, argv=0x7fff22dd1da8,<br>requested=0, provided=0x7fff22dd184c) at runtime/ompi_mpi_init.c:790<br>#5 &nbsp;0x00007f1068273a2c in MPI_Init (argc=0x7fff22dd188c,<br>argv=0x7fff22dd1880) at init.c:84<br>#6 &nbsp;0x00000000004008e7 in main (argc=1, argv=0x7fff22dd1da8) at<br>hello_world.c:13<br>---snip---<br><br><br>in line 538 of the file containing the mca_btl_openib_tune_endpoint<br>routine, the strcmp operation fails, because &nbsp;recv_qps is a NULL pointer.<br><br><br>---snip---<br><br>if(0 != strcmp(mca_btl_openib_component.receive_queues, recv_qps)) {<br><br>---snip---<br><br>Does anybody have an idea on what might be going wrong and how to<br>resolve it? Just to confirm, everything works perfectly with the 1.8<br>series on that very same &nbsp;cluster<br><br>Thanks<br>Edgar<br>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>&lt;<a href="mailto:devel@open-mpi.org">mailto:devel@open-mpi.org</a>&gt;<br>Subscription:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>Link to this post:<br><a href="http://www.open-mpi.org/community/lists/devel/2014/08/15746.php">http://www.open-mpi.org/community/lists/devel/2014/08/15746.php</a><br><br><br>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>&lt;<a href="mailto:devel@open-mpi.org">mailto:devel@open-mpi.org</a>&gt;<br>Subscription:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>Link to this post:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/devel/2014/08/15747.php">http://www.open-mpi.org/community/lists/devel/2014/08/15747.php</a><br>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>&lt;<a href="mailto:devel@open-mpi.org">mailto:devel@open-mpi.org</a>&gt;<br>Subscription:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>Link to this post:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/devel/2014/08/15748.php">http://www.open-mpi.org/community/lists/devel/2014/08/15748.php</a><br><br><br>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>&lt;<a href="mailto:devel@open-mpi.org">mailto:devel@open-mpi.org</a>&gt;<br>Subscription:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>Link to this post:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/devel/2014/08/15749.php">http://www.open-mpi.org/community/lists/devel/2014/08/15749.php</a><br><br><br><br><br><br><br><br>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>&lt;<a href="mailto:devel@open-mpi.org">mailto:devel@open-mpi.org</a>&gt;<br>Subscription:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>Link to this post:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/devel/2014/08/15750.php">http://www.open-mpi.org/community/lists/devel/2014/08/15750.php</a><br><br>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>&lt;<a href="mailto:devel@open-mpi.org">mailto:devel@open-mpi.org</a>&gt;<br>Subscription:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>Link to this post:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/devel/2014/08/15752.php">http://www.open-mpi.org/community/lists/devel/2014/08/15752.php</a><br><br>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>Subscription:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>Link to this post:<span class="Apple-converted-space">&nbsp;</span><a href="http://www.open-mpi.org/community/lists/devel/2014/08/15757.php">http://www.open-mpi.org/community/lists/devel/2014/08/15757.php</a></div></blockquote></div><br></div></body></html>
