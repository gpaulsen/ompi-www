<div dir="ltr">Do you mean the logs from failed attempts? They are enclosed. If you need the successful logs I&#39;ll need to make them again since the files from successful builds are deleted.<div><br></div><div style>I am not using  MXM. The results with the option you suggested are the same as before:</div>
<div>#---------------------------------------------------</div><div># Benchmarking PingPong </div><div># #processes = 2 </div><div>#---------------------------------------------------</div><div>       #bytes #repetitions      t[usec]   Mbytes/sec</div>
<div>            0         1000         1.49         0.00</div><div>            1         1000         1.58         0.61</div><div>            2         1000         1.12         1.71</div><div>            4         1000         1.10         3.48</div>
<div>            8         1000         1.11         6.90</div><div>           16         1000         1.11        13.69</div><div>           32         1000         1.12        27.21</div><div>           64         1000         1.16        52.52</div>
<div>          128         1000         1.72        70.83</div><div>          256         1000         1.84       132.72</div><div>          512         1000         1.99       245.74</div><div>         1024         1000         2.25       433.92</div>
<div>         2048         1000         2.87       680.54</div><div>         4096         1000         3.52      1109.13</div><div>         8192         1000         4.68      1670.60</div><div>        16384         1000         9.66      1617.91</div>
<div>        32768         1000        14.30      2185.24</div><div>        65536          640        23.45      2665.33</div><div>       131072          320        35.99      3473.15</div><div>       262144          160        58.05      4306.65</div>
<div>       524288           80       101.94      4904.69</div><div>      1048576           40       188.65      5300.86</div><div>      2097152           20       526.05      3801.94</div><div>      4194304           10      1096.09      3649.32</div>
<div><br></div><div>#---------------------------------------------------</div><div># Benchmarking PingPing </div><div># #processes = 2 </div><div>#---------------------------------------------------</div><div>       #bytes #repetitions      t[usec]   Mbytes/sec</div>
<div>            0         1000         1.10         0.00</div><div>            1         1000         1.24         0.77</div><div>            2         1000         1.23         1.55</div><div>            4         1000         1.23         3.10</div>
<div>            8         1000         1.25         6.09</div><div>           16         1000         1.14        13.41</div><div>           32         1000         1.11        27.40</div><div>           64         1000         1.16        52.75</div>
<div>          128         1000         1.71        71.34</div><div>          256         1000         1.84       132.33</div><div>          512         1000         1.98       246.63</div><div>         1024         1000         2.27       429.26</div>
<div>         2048         1000         2.91       672.30</div><div>         4096         1000         3.52      1109.43</div><div>         8192         1000         4.80      1627.25</div><div>        16384         1000         9.98      1565.64</div>
<div>        32768         1000        14.70      2125.14</div><div>        65536          640        24.18      2584.97</div><div>       131072          320        37.33      3348.95</div><div>       262144          160        60.59      4125.82</div>
<div>       524288           80       105.83      4724.78</div><div>      1048576           40       197.82      5055.05</div><div>      2097152           20       791.35      2527.34</div><div style>      4194304           10      1820.30      2197.44 </div>
<div style><br></div><div style>Regards, Pavel Mezentsev.</div></div><div class="gmail_extra"><br><br><div class="gmail_quote">2013/2/28 Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span><br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div class="im">On Feb 27, 2013, at 7:36 PM, Pavel Mezentsev &lt;<a href="mailto:pavel.mezentsev@gmail.com">pavel.mezentsev@gmail.com</a>&gt; wrote:<br>

<br>
&gt; I&#39;ve tried the new rc. Here is what I got:<br>
<br>
</div>Thanks for testing.<br>
<div><div class="h5"><br>
&gt; 1) I&#39;ve successfully built it with intel-13.1 and gcc-4.7.2. But I&#39;ve failed while using open64-4.5.2 and ekopath-5.0.0 (pathscale). The problems are in the fortran part. In each case I&#39;ve used the following configuration line:<br>

&gt; CC=$CC CXX=$CXX F77=$F77 FC=$FC ./configure --prefix=$prefix --with-knem=$knem_path<br>
&gt; Open64 failed during configuration with the following:<br>
&gt; *** Fortran compiler<br>
&gt; checking whether we are using the GNU Fortran compiler... yes<br>
&gt; checking whether openf95 accepts -g... yes<br>
&gt; configure: WARNING: Open MPI now ignores the F77 and FFLAGS environment variables; only the FC and FCFLAGS environment variables are used.<br>
&gt; checking whether ln -s works... yes<br>
&gt; checking if Fortran compiler works... yes<br>
&gt; checking for extra arguments to build a shared library... none needed<br>
&gt; checking for Fortran flag to compile .f files... none<br>
&gt; checking for Fortran flag to compile .f90 files... none<br>
&gt; checking to see if Fortran compilers need additional linker flags... none<br>
&gt; checking  external symbol convention... double underscore<br>
&gt; checking if C and Fortran are link compatible... yes<br>
&gt; checking to see if Fortran compiler likes the C++ exception flags... skipped (no C++ exceptions flags)<br>
&gt; checking to see if mpifort compiler needs additional linker flags... none<br>
&gt; checking if Fortran compiler supports CHARACTER... yes<br>
&gt; checking size of Fortran CHARACTER... 1<br>
&gt; checking for C type corresponding to CHARACTER... char<br>
&gt; checking alignment of Fortran CHARACTER... 1<br>
&gt; checking for corresponding KIND value of CHARACTER... C_SIGNED_CHAR<br>
&gt; checking KIND value of Fortran C_SIGNED_CHAR... no ISO_C_BINDING -- fallback<br>
&gt; checking Fortran value of selected_int_kind(4)... no<br>
&gt; configure: WARNING: Could not determine KIND value of C_SIGNED_CHAR<br>
&gt; configure: WARNING: See config.log for more details<br>
&gt; configure: error: Cannot continue<br>
<br>
</div></div>Please send the full configure output as well as the config.log file (please compress).<br>
<div class="im"><br>
&gt; Ekopath failed during make with the following error:<br>
&gt;  PPFC     mpi-f08-sizeof.lo<br>
&gt;   PPFC     mpi-f08.lo<br>
&gt; In file included from mpi-f08.F90:37:<br>
&gt; mpi-f-interfaces-bind.h:1908: warning: extra tokens at end of #endif directive<br>
&gt; mpi-f-interfaces-bind.h:2957: warning: extra tokens at end of #endif directive<br>
&gt; In file included from mpi-f08.F90:38:<br>
&gt; pmpi-f-interfaces-bind.h:1911: warning: extra tokens at end of #endif directive<br>
&gt; pmpi-f-interfaces-bind.h:2963: warning: extra tokens at end of #endif directive<br>
&gt; pathf95-1044 pathf95: INTERNAL OMPI_OP_CREATE_F, File = mpi-f-interfaces-bind.h, Line = 955, Column = 29<br>
&gt;   Internal : Unexpected ATP_PGM_UNIT in check_interoperable_pgm_unit()<br>
<br>
</div>I&#39;ve pinged Pathscale about this.<br>
<div class="im"><br>
&gt; 2) I&#39;ve ran a couple of tests (IMB) with the new version. I ran this on a system consisting of 10 nodes with Intel SB processor and fdr ConnectX3 infiniband adapters.<br>
&gt; First I&#39;ve tried the following parameters:<br>
&gt; mpirun -np $NP -hostfile hosts --mca btl openib,sm,self --bind-to-core -npernode 16 --mca mpi_leave_pinned 1 ./IMB-MPI1 -npmin $NP -mem 4G $COLL<br>
&gt; This combination complained about mca_leave_pinned. The same line works for 1.6.3. Is something different in the new release and I&#39;ve missed it?<br>
&gt; --------------------------------------------------------------------------<br>
&gt; A process attempted to use the &quot;leave pinned&quot; MPI feature, but no<br>
&gt; memory registration hooks were found on the system at run time.  This<br>
&gt; may be the result of running on a system that does not support memory<br>
&gt; hooks or having some other software subvert Open MPI&#39;s use of the<br>
&gt; memory hooks.  You can disable Open MPI&#39;s use of memory hooks by<br>
&gt; setting both the mpi_leave_pinned and mpi_leave_pinned_pipeline MCA<br>
&gt; parameters to 0.<br>
<br>
</div>This should not be, and might explain your lower performance on the IMB results.<br>
<br>
Nathan -- you reported that you saw something like this before, but were then unable to reproduce.  Any ideas what&#39;s going on here?  Mellanox?<br>
<br>
(although the short message latency is troubling...)<br>
<br>
Can you ensure that you aren&#39;t using MXM in 1.7?  I understand that its short message latency is worse than RC verbs.  You&#39;ll need to add &quot;--mca pml ob1&quot; in your command line.<br>
<div class="HOEnZb"><div class="h5"><br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</div></div></blockquote></div><br></div>

