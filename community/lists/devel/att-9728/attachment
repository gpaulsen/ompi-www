Thanks for the links.  <div><br></div><div>I found a link(below) to compare TIPC, TCP and SCTP. But it uses some old version of TIPC(1.7.3).  Do you have any similar tests but on the latest version of TIPC, TCP and SCTP.  That will be more helpful to convince people to use TIPC.  </div>
<div><br></div><div>Another thing I am interested is whether or not TIPC has some internal/underlying mechanism  to manage multiple network interfaces on single node, e.g. automatic load balance between multiple network cards.  </div>
<div><br></div><div><meta http-equiv="content-type" content="text/html; charset=utf-8"><font class="Apple-style-span" face="&#39;Times New Roman&#39;" size="3"><meta http-equiv="content-type" content="text/html; charset=utf-8"><a href="http://www.strlen.de/tipc/">http://www.strlen.de/tipc/</a></font></div>
<div><br>Teng<br><div class="gmail_quote">On Tue, Aug 30, 2011 at 7:57 AM, Xin He <span dir="ltr">&lt;<a href="mailto:xin.i.he@ericsson.com">xin.i.he@ericsson.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
<u></u>

  
    
    
  
  <div bgcolor="#ffffff" text="#000000">
    Yes, it is Gigabytes Ethernet. I configure ompi again using
    &quot;./configure --disable-mpi-f90 --disable-mpi-f77 --disable-mpi-cxx
    --disable-vt --disable-io-romio --prefix=/usr
    --with-platform=optimized&quot;<br>
    and run IMB-MPI1 again with &quot;mpirun --mca btl tcp,self -n 2
    --hostfile my_hostfile --bynode ./IMB-MPI1&quot; again, the result does
    not seem very different though...<br>
    <br>
    About TIPC, maybe this article can explains more: <a href="http://www.kernel.org/doc/ols/2004/ols2004v2-pages-61-70.pdf" target="_blank">http://www.kernel.org/doc/ols/2004/ols2004v2-pages-61-70.pdf</a><br>
    <br>
    To use TIPC, you use &quot;tipcutil&quot; to configure:<br>
    I first connect 2 machines directly with wires<br>
    1. set tipc address of 2 PC. Say &lt;1.1.1&gt; and &lt;1.1.2&gt;
    respectively and with the same Network ID<br>
    2. <span style="border-collapse:separate;color:rgb(0, 0, 0);font-family:&#39;Times New Roman&#39;;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:normal;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;font-size:medium"><span style="font-family:&#39;Helvetica Neue&#39;,Helvetica,Arial,sans-serif;font-size:13px;line-height:20px">&quot;tipc-config -v -i -be=eth:eth0,eth:eth1&quot;</span></span>
    at each machine to set the bears. Check <a href="http://tipc.sourceforge.net/doc/tipc_1.7_users_guide.html#installation" target="_blank">http://tipc.sourceforge.net/doc/tipc_1.7_users_guide.html#installation</a>
    for more information<br>
    3. &quot;tipc-config -l&quot; to check links. If successful, you should see:<br>
    <a href="mailto:ehhexxn@node2:~/git/test_ompi/IMB_3.2/src$" target="_blank">ehhexxn@node2:~/git/test_ompi/IMB_3.2/src$</a> tipc-config -l<br>
    Links:<br>
    multicast-link: up<br>
    1.1.2:eth0-1.1.1:eth0: up<br>
    1.1.2:eth1-1.1.1:eth1: up<br>
    <br>
    In the attachment, there are sample programs using TIPC that can be
    used to test TIPC environment :)<br><font color="#888888">
    <br>
    /Xin</font><div><div></div><div class="h5"><br>
     <br>
     <br>
    On 08/29/2011 03:22 PM, teng ma wrote:
    <blockquote type="cite">Is your interconnect Gigabytes Ethernet? It&#39;s very
      surprised to see TCP BTL just got 33MBytes peak BW on your
      cluster.  I did a similar test on an amd cluster with gigabytes
      Ethernet. As following shows, the TCP BTL&#39;s BW is similar with
      your tipc(112MBytes/s).   Could you redo the test with 2 processes
      spawned, 2 nodes in your machinefile and enabling --bynode?  <br>
      <br>
      It looks like your tipc BTL is pretty good at message size between
      8K and 512K.  Can you tell us more about difference between TIPC
      and TCP protocol stacks?  Any special configure needed to enable
      your tipc?  Maybe you can write a module in Netpipe( similar to
      NPTcp )to test raw performance on both TCP and TIPC without MPI. <br>
      <br>
      TCP BTL on a Gigbytes ethernet<br>
      #---------------------------------------------------<br>
      # Benchmarking PingPong <br>
      # #processes = 2 <br>
      #---------------------------------------------------<br>
             #bytes #repetitions      t[usec]   Mbytes/sec<br>
                  0         1000        23.27         0.00<br>
                  1         1000        23.78         0.04<br>
                  2         1000        23.77         0.08<br>
                  4         1000        25.47         0.15<br>
                  8         1000        23.94         0.32<br>
                 16         1000        24.36         0.63<br>
                 32         1000        24.83         1.23<br>
                 64         1000        25.76         2.37<br>
                128         1000        27.25         4.48<br>
                256         1000        30.66         7.96<br>
                512         1000        36.86        13.25<br>
               1024         1000        49.00        19.93<br>
               2048         1000        77.83        25.10<br>
               4096         1000        82.42        47.39<br>
               8192         1000       165.28        47.27<br>
              16384         1000       325.01        48.08<br>
              32768         1000       440.75        70.90<br>
              65536          640      1060.00        58.96<br>
             131072          320      1674.71        74.64<br>
             262144          160      2814.13        88.84<br>
             524288           80      4975.11       100.50<br>
            1048576           40      9526.94       104.97<br>
            2097152           20     18419.33       108.58<br>
            4194304           10     36150.05       110.65<br>
            8388608            5     71880.79       111.30<br>
      <br>
      <br>
      <br>
      Teng<br>
      <br>
      <div class="gmail_quote">On Mon, Aug 29, 2011 at 3:51 AM, Xin He <span dir="ltr">&lt;<a href="mailto:xin.i.he@ericsson.com" target="_blank">xin.i.he@ericsson.com</a>&gt;</span>
        wrote:<br>
        <blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204, 204, 204);padding-left:1ex">
          <div><br>
            <br>
            <br>
            On 08/25/2011 03:14 PM, Jeff Squyres wrote:<br>
            <blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204, 204, 204);padding-left:1ex">
              On Aug 25, 2011, at 8:25 AM, Xin He wrote:<br>
              <br>
              <blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204, 204, 204);padding-left:1ex">
                <blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204, 204, 204);padding-left:1ex">
                  Can you edit your configure.m4 directly and test it
                  and whatnot?  I provided the configure.m4 as a
                  starting point for you.  :-)  It shouldn&#39;t be hard to
                  make it check linux/tipc.h instead of tipc.h.  I&#39;m
                  happy to give you direct write access to the
                  bitbucket, if you want it.<br>
                </blockquote>
                I think me having write access is convenient for both of
                us :)<br>
              </blockquote>
              Sure -- what&#39;s your bitbucket account ID?<br>
            </blockquote>
          </div>
          It&#39;s &quot;letter113&quot;
          <div><br>
            <blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204, 204, 204);padding-left:1ex">
              <blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204, 204, 204);padding-left:1ex">
                <blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204, 204, 204);padding-left:1ex">
                  As we&#39;ve discussed off-list, we can&#39;t take the code
                  upstream until the contributor agreement is signed,
                  unfortunately.<br>
                  <br>
                </blockquote>
                The agreement thing is ongoing right now, but it may
                take some time.<br>
              </blockquote>
              No worries.  Lawyers tend to take time when reviewing this
              stuff; we&#39;ve seen this pattern in most organizations who
              sign the OMPI agreement.<br>
              <br>
              <blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204, 204, 204);padding-left:1ex">
                But to save time, can you guys do some test on TIPC BTL,
                so that<br>
                when the agreement is ready, the code can be used?<br>
              </blockquote>
              I don&#39;t know if any of us has the TIPC support libraries
              installed.<br>
            </blockquote>
            <br>
          </div>
          It is easy to have TIPC support. It is within the kernel
          actually. To get TIPC working, you only have to configure it
          by using &quot;tipc-config&quot;. Maybe you<br>
          can check this doc for information: <a href="http://tipc.sourceforge.net/doc/Users_Guide.txt" target="_blank">http://tipc.sourceforge.net/doc/Users_Guide.txt</a>
          <div><br>
            <blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204, 204, 204);padding-left:1ex">
              So... what *is* TIPC?  Is there a writeup anywhere that we
              can read about what it is / how it works?  For example,
              what makes TIPC perform better than TCP?<br>
            </blockquote>
            <br>
          </div>
          Sure.  Search &quot;TIPC: Providing Communication for Linux
          Clusters&quot;. It is a paper written by the author of TIPC,
          explaining basic stuff about TIPC,<br>
          should be very useful. And you can visit TIPC homepage: <a href="http://tipc.sourceforge.net/" target="_blank">http://tipc.sourceforge.net/</a> .
          <div>
            <div><br>
              <blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204, 204, 204);padding-left:1ex">
                <blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204, 204, 204);padding-left:1ex">
                  <blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204, 204, 204);padding-left:1ex">
                    <blockquote class="gmail_quote" style="margin:0pt 0pt 0pt 0.8ex;border-left:1px solid rgb(204, 204, 204);padding-left:1ex">
                      I have done some tests using tools like NetPIPE,
                      osu and IMB and the result shows that TIPC BTL has
                      a better performance<br>
                      than TCP BTL.<br>
                    </blockquote>
                    Great!  Can you share any results?<br>
                  </blockquote>
                  Yes, please check the appendix for the results using
                  IMB 3.2.<br>
                  <br>
                  I have done the tests on 2 computers. Dell SC1435<br>
                  Dual-Core AMD Opteron(tm) Processor 2212 HE x 2<br>
                  4 GB Mem<br>
                  Ubuntu Server 10.04 LTS 32-bit Linux 2.6.32-24<br>
                </blockquote>
                I&#39;m not familiar with the Dell or Opteron lines -- how
                recent are those models?<br>
                <br>
                I ask because your TCP latency is a bit high (about 85us
                in 2-process IMB PingPong); it might suggest older
                hardware.  Or you may have built a debugging version of
                Open MPI (if you have a .svn or .hg checkout, that&#39;s the
                default). See the HACKING top-level file for how to get
                an optimized build.<br>
                <br>
                For example, with my debug build of Open MPI on fairly
                old Xeons with 1GB ethernet, I&#39;m getting the following
                PingPong results (note: this is a debug build; it&#39;s not
                even an optimized build):<br>
                <br>
                -----<br>
                $ mpirun --mca btl tcp,self --bynode -np 2 --mca
                btl_tcp_if_include eth0 hostname<br>
                svbu-mpi008<br>
                svbu-mpi009<br>
                $ mpirun --mca btl tcp,self --bynode -np 2 --mca
                btl_tcp_if_include eth0 IMB-MPI1 PingPong<br>
                #---------------------------------------------------<br>
                #    Intel (R) MPI Benchmark Suite V3.2, MPI-1 part<br>
                #---------------------------------------------------<br>
                ...<br>
                #---------------------------------------------------<br>
                # Benchmarking PingPong<br>
                # #processes = 2<br>
                #---------------------------------------------------<br>
                       #bytes #repetitions      t[usec]   Mbytes/sec<br>
                            0         1000        57.31         0.00<br>
                            1         1000        57.71         0.02<br>
                            2         1000        57.73         0.03<br>
                            4         1000        57.81         0.07<br>
                            8         1000        57.78         0.13<br>
                -----<br>
                <br>
                With an optimized build, it shaves off only a few us
                (which isn&#39;t too important in this case, but it does
                matter in the low-latency transport cases):<br>
                <br>
                -----<br>
                #---------------------------------------------------<br>
                # Benchmarking PingPong<br>
                # #processes = 2<br>
                #---------------------------------------------------<br>
                       #bytes #repetitions      t[usec]   Mbytes/sec<br>
                            0         1000        54.62         0.00<br>
                            1         1000        54.92         0.02<br>
                            2         1000        55.15         0.03<br>
                            4         1000        55.16         0.07<br>
                            8         1000        55.15         0.14<br>
                -----<br>
                <br>
              </blockquote>
            </div>
          </div>
          Hi, I think these models are reasonably new :)<br>
          The result I gave you, they are tested on 2 processes but on 2
          different servers. I get that the result you showed is 2
          processes on one machine?<br>
          But I did build with debug enabled, I will try optimize then
          :)<br>
          <br>
          BTW, I forgot to tell you about SM &amp; TIPC. Unfortunately,
          TIPC does not beat SM...<br>
          <font color="#888888">
            <br>
            /Xin</font>
          <div>
            <div><br>
              <br>
              _______________________________________________<br>
              devel mailing list<br>
              <a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
              <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
            </div>
          </div>
        </blockquote>
      </div>
      <br>
    </blockquote>
    <br>
  </div></div></div>

<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br></blockquote></div><br></div>

