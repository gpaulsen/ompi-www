<div dir="ltr">There was a long thread of discussion on why we must use an rte_barrier and not an mpi_barrier during the finalize. Basically, we long as we have connectionless unreliable BTLs we need an external mechanism to ensure complete tear-down of the entire infrastructure. Thus, we need to rely on an rte_barrier not because it guarantees the correctness of the code, but because it provides enough time to all processes to flush all HPC traffic.<div>
<br></div><div>  George.<br></div><div><br></div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Mon, Jul 21, 2014 at 1:10 PM, Yossi Etigin <span dir="ltr">&lt;<a href="mailto:yosefe@mellanox.com" target="_blank">yosefe@mellanox.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">I see. But in branch v1.8, in 31869, Ralph reverted the commit which moved del_procs after the barrier:<br>
  &quot;Revert r31851 until we can resolve how to close these leaks without causing the usnic BTL to fail during disconnect of intercommunicators<br>
   Refs #4643&quot;<br>
Also, we need an rte barrier after del_procs - because otherwise rankA could call pml_finalize() before rankB finishes disconnecting from rankA.<br>
<br>
I think the order in finalize should be like this:<br>
        1. mpi_barrier(world)<br>
        2. del_procs()<br>
        3. rte_barrier()<br>
        4. pml_finalize()<br>
<div><div class="h5"><br>
-----Original Message-----<br>
From: Nathan Hjelm [mailto:<a href="mailto:hjelmn@lanl.gov">hjelmn@lanl.gov</a>]<br>
Sent: Monday, July 21, 2014 8:01 PM<br>
To: Open MPI Developers<br>
Cc: Yossi Etigin<br>
Subject: Re: [OMPI devel] barrier before calling del_procs<br>
<br>
I should add that it is an rte barrier and not an MPI barrier for technical reasons.<br>
<br>
-Nathan<br>
<br>
On Mon, Jul 21, 2014 at 09:42:53AM -0700, Ralph Castain wrote:<br>
&gt;    We already have an rte barrier before del procs<br>
&gt;<br>
&gt;    Sent from my iPhone<br>
&gt;    On Jul 21, 2014, at 8:21 AM, Yossi Etigin &lt;<a href="mailto:yosefe@mellanox.com">yosefe@mellanox.com</a>&gt; wrote:<br>
&gt;<br>
&gt;      Hi,<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;      We get occasional hangs with MTL/MXM during finalize, because a global<br>
&gt;      synchronization is needed before calling del_procs.<br>
&gt;<br>
&gt;      e.g rank A may call del_procs() and disconnect from rank B, while rank B<br>
&gt;      is still working.<br>
&gt;<br>
&gt;      What do you think about adding an MPI barrier on COMM_WORLD before<br>
&gt;      calling del_procs()?<br>
&gt;<br>
&gt;<br>
<br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; Link to this post:<br>
&gt; <a href="http://www.open-mpi.org/community/lists/devel/2014/07/15204.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/07/15204.php</a><br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</div></div>Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/07/15206.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/07/15206.php</a><br>
</blockquote></div><br></div>

