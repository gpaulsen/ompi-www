<p dir="ltr">Interestingly enough the 4MB latency actually improved significantly relative to the initial numbers. </p>
<p dir="ltr">-Paul [Sent from my phone] </p>
<div class="gmail_quote">On Jan 8, 2014 8:50 AM, &quot;George Bosilca&quot; &lt;<a href="mailto:bosilca@icl.utk.edu">bosilca@icl.utk.edu</a>&gt; wrote:<br type="attribution"><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
These results are way worst that the one you send on your previous email? What is the reason?<br>
<br>
  George.<br>
<br>
On Jan 8, 2014, at 17:33 , Nathan Hjelm &lt;<a href="mailto:hjelmn@lanl.gov">hjelmn@lanl.gov</a>&gt; wrote:<br>
<br>
&gt; Ah, good catch. A new version is attached that should eliminate the race<br>
&gt; window for the multi-threaded case. Performance numbers are still<br>
&gt; looking really good. We beat mvapich2 in the small message ping-pong by<br>
&gt; a good margin. See the results below. The large message latency<br>
&gt; difference for large messages is probably due to a difference in the max<br>
&gt; send size for vader vs mvapich.<br>
&gt;<br>
&gt; To answer Pasha&#39;s question. I don&#39;t see a noticiable difference in<br>
&gt; performance for btl&#39;s with no sendi function (this includes<br>
&gt; ugni). OpenIB should get a boost. I will test that once I get an<br>
&gt; allocation.<br>
&gt;<br>
&gt; CPU: Xeon E5-2670 @ 2.60 GHz<br>
&gt;<br>
&gt; Open MPI (-mca btl vader,self):<br>
&gt; # OSU MPI Latency Test v4.1<br>
&gt; # Size          Latency (us)<br>
&gt; 0                       0.17<br>
&gt; 1                       0.19<br>
&gt; 2                       0.19<br>
&gt; 4                       0.19<br>
&gt; 8                       0.19<br>
&gt; 16                      0.19<br>
&gt; 32                      0.19<br>
&gt; 64                      0.40<br>
&gt; 128                     0.40<br>
&gt; 256                     0.43<br>
&gt; 512                     0.52<br>
&gt; 1024                    0.67<br>
&gt; 2048                    0.94<br>
&gt; 4096                    1.44<br>
&gt; 8192                    2.04<br>
&gt; 16384                   3.47<br>
&gt; 32768                   6.10<br>
&gt; 65536                   9.38<br>
&gt; 131072                 16.47<br>
&gt; 262144                 29.63<br>
&gt; 524288                 54.81<br>
&gt; 1048576               106.63<br>
&gt; 2097152               206.84<br>
&gt; 4194304               421.26<br>
&gt;<br>
&gt;<br>
&gt; mvapich2 1.9:<br>
&gt; # OSU MPI Latency Test<br>
&gt; # Size            Latency (us)<br>
&gt; 0                         0.23<br>
&gt; 1                         0.23<br>
&gt; 2                         0.23<br>
&gt; 4                         0.23<br>
&gt; 8                         0.23<br>
&gt; 16                        0.28<br>
&gt; 32                        0.28<br>
&gt; 64                        0.39<br>
&gt; 128                       0.40<br>
&gt; 256                       0.40<br>
&gt; 512                       0.42<br>
&gt; 1024                      0.51<br>
&gt; 2048                      0.71<br>
&gt; 4096                      1.02<br>
&gt; 8192                      1.60<br>
&gt; 16384                     3.47<br>
&gt; 32768                     5.05<br>
&gt; 65536                     8.06<br>
&gt; 131072                   14.82<br>
&gt; 262144                   28.15<br>
&gt; 524288                   53.69<br>
&gt; 1048576                 127.47<br>
&gt; 2097152                 235.58<br>
&gt; 4194304                 683.90<br>
&gt;<br>
&gt;<br>
&gt; -Nathan<br>
&gt;<br>
&gt; On Tue, Jan 07, 2014 at 06:23:13PM -0700, George Bosilca wrote:<br>
&gt;&gt;   The local request is not correctly released, leading to assert in debug<br>
&gt;&gt;   mode. This is because you avoid calling MCA_PML_BASE_RECV_REQUEST_FINI,<br>
&gt;&gt;   fact that leaves the request in an ACTIVE state, condition carefully<br>
&gt;&gt;   checked during the call to destructor.<br>
&gt;&gt;<br>
&gt;&gt;   I attached a second patch that fixes the issue above, and implement a<br>
&gt;&gt;   similar optimization for the blocking send.<br>
&gt;&gt;<br>
&gt;&gt;   Unfortunately, this is not enough. The mca_pml_ob1_send_inline<br>
&gt;&gt;   optimization is horribly wrong in a multithreaded case as it alter the<br>
&gt;&gt;   send_sequence without storing it. If you create a gap in the send_sequence<br>
&gt;&gt;   a deadlock will __definitively__ occur. I strongly suggest you turn off<br>
&gt;&gt;   the mca_pml_ob1_send_inline optimization on the multithreaded case. All<br>
&gt;&gt;   the others optimizations should be safe in all cases.<br>
&gt;&gt;<br>
&gt;&gt;     George.<br>
&gt;&gt;<br>
&gt;&gt;   On Jan 8, 2014, at 01:15 , Shamis, Pavel &lt;<a href="mailto:shamisp@ornl.gov">shamisp@ornl.gov</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt;&gt; Overall it looks good. It would be helpful to validate performance<br>
&gt;&gt;   numbers for other interconnects as well.<br>
&gt;&gt;&gt; -Pasha<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; -----Original Message-----<br>
&gt;&gt;&gt;&gt; From: devel [mailto:<a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a>] On Behalf Of Nathan<br>
&gt;&gt;&gt;&gt; Hjelm<br>
&gt;&gt;&gt;&gt; Sent: Tuesday, January 07, 2014 6:45 PM<br>
&gt;&gt;&gt;&gt; To: Open MPI Developers List<br>
&gt;&gt;&gt;&gt; Subject: [OMPI devel] RFC: OB1 optimizations<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; What: Push some ob1 optimizations to the trunk and 1.7.5.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; What: This patch contains two optimizations:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; - Introduce a fast send path for blocking send calls. This path uses<br>
&gt;&gt;&gt;&gt;   the btl sendi function to put the data on the wire without the need<br>
&gt;&gt;&gt;&gt;   for setting up a send request. In the case of btl/vader this can<br>
&gt;&gt;&gt;&gt;   also avoid allocating/initializing a new fragment. With btl/vader<br>
&gt;&gt;&gt;&gt;   this optimization improves small message latency by 50-200ns in<br>
&gt;&gt;&gt;&gt;   ping-pong type benchmarks. Larger messages may take a small hit in<br>
&gt;&gt;&gt;&gt;   the range of 10-20ns.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; - Use a stack-allocated receive request for blocking recieves. This<br>
&gt;&gt;&gt;&gt;   optimization saves the extra instructions associated with accessing<br>
&gt;&gt;&gt;&gt;   the receive request free list. I was able to get another 50-200ns<br>
&gt;&gt;&gt;&gt;   improvement in the small-message ping-pong with this optimization. I<br>
&gt;&gt;&gt;&gt;   see no hit for larger messages.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; When: These changes touch the critical path in ob1 and are targeted for<br>
&gt;&gt;&gt;&gt; 1.7.5. As such I will set a moderately long timeout. Timeout set for<br>
&gt;&gt;&gt;&gt; next Friday (Jan 17).<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Some results from osu_latency on haswell:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; hjelmn@cn143 pt2pt]$ mpirun -n 2 --bind-to core -mca btl vader,self<br>
&gt;&gt;&gt;&gt; ./osu_latency<br>
&gt;&gt;&gt;&gt; # OSU MPI Latency Test v4.0.1<br>
&gt;&gt;&gt;&gt; # Size          Latency (us)<br>
&gt;&gt;&gt;&gt; 0                       0.11<br>
&gt;&gt;&gt;&gt; 1                       0.14<br>
&gt;&gt;&gt;&gt; 2                       0.14<br>
&gt;&gt;&gt;&gt; 4                       0.14<br>
&gt;&gt;&gt;&gt; 8                       0.14<br>
&gt;&gt;&gt;&gt; 16                      0.14<br>
&gt;&gt;&gt;&gt; 32                      0.15<br>
&gt;&gt;&gt;&gt; 64                      0.18<br>
&gt;&gt;&gt;&gt; 128                     0.36<br>
&gt;&gt;&gt;&gt; 256                     0.37<br>
&gt;&gt;&gt;&gt; 512                     0.46<br>
&gt;&gt;&gt;&gt; 1024                    0.56<br>
&gt;&gt;&gt;&gt; 2048                    0.80<br>
&gt;&gt;&gt;&gt; 4096                    1.12<br>
&gt;&gt;&gt;&gt; 8192                    1.68<br>
&gt;&gt;&gt;&gt; 16384                   2.98<br>
&gt;&gt;&gt;&gt; 32768                   5.10<br>
&gt;&gt;&gt;&gt; 65536                   8.12<br>
&gt;&gt;&gt;&gt; 131072                 14.07<br>
&gt;&gt;&gt;&gt; 262144                 25.30<br>
&gt;&gt;&gt;&gt; 524288                 47.40<br>
&gt;&gt;&gt;&gt; 1048576                91.71<br>
&gt;&gt;&gt;&gt; 2097152               195.56<br>
&gt;&gt;&gt;&gt; 4194304               487.05<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Patch Attached.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; -Nathan<br>
&gt;&gt;&gt; _______________________________________________<br>
&gt;&gt;&gt; devel mailing list<br>
&gt;&gt;&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt;&gt;&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt;&gt;<br>
&gt;&gt;   _______________________________________________<br>
&gt;&gt;   devel mailing list<br>
&gt;&gt;   <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt;&gt;   <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt;<br>
&gt;<br>
&gt; &lt;ob1_optimization_take3.patch&gt;_______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</blockquote></div>

