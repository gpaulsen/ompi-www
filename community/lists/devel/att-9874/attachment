<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; ">This problem resurfaced on the user list, so I dug around a bit and think I've figured it out using George's test code. The problem lies in the fact that the intercomm "merge" function can create a linkage between procs that was not reflected anywhere in a modex, and so at least some of the procs in the resulting communicator don't know how to talk to some of the new communicator's peers.<div><br></div><div>For example, consider the case where:</div><div><br></div><div>1. parent job A comm_spawns a process (job B) - these processes exchange modex and can communicate</div><div><br></div><div>2. parent job A now comm_spawns another process (job C) - again, these can communicate, but the proc in C knows nothing of B</div><div><br></div><div>3. do an intercomm merge across the communicators created by the two comm_spawns. This puts B and C into the same communicator, but they know nothing about how to talk to each other as they were not involved in any exchange of contact info. Hence, collectives on that communicator now fail.</div><div><br></div><div>I tried adding all known contact info (not just your own) into the modex, but that doesn't resolve the problem. It resulted in C knowing how to talk to B (because A knew when the comm_spawn was done), but B still has no idea how to talk to C as it didn't participate in the modex associated with step 2.</div><div><br></div><div>It seems to me that the solution is to have intercomm "merge" actually execute a modex to ensure that all procs in the new communicator know how to communicate with each other, but I readily admit I might be missing something.</div><div><br></div><div>Anyone have thoughts on this? It has come up twice now, so probably something worth addressing.</div><div><br><div><br><div>Begin forwarded message:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px;"><span style="font-family:'Helvetica'; font-size:medium; color:rgba(0, 0, 0, 1);"><b>From: </b></span><span style="font-family:'Helvetica'; font-size:medium;">Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;<br></span></div><div style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px;"><span style="font-family:'Helvetica'; font-size:medium; color:rgba(0, 0, 0, 1);"><b>Date: </b></span><span style="font-family:'Helvetica'; font-size:medium;">October 25, 2011 10:08:00 AM MDT<br></span></div><div style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px;"><span style="font-family:'Helvetica'; font-size:medium; color:rgba(0, 0, 0, 1);"><b>To: </b></span><span style="font-family:'Helvetica'; font-size:medium;">Open MPI Users &lt;<a href="mailto:users@open-mpi.org">users@open-mpi.org</a>&gt;<br></span></div><div style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px;"><span style="font-family:'Helvetica'; font-size:medium; color:rgba(0, 0, 0, 1);"><b>Subject: </b></span><span style="font-family:'Helvetica'; font-size:medium;"><b>Re: [OMPI users] Problem-Bug with MPI_Intercomm_create()</b><br></span></div><br><div>FWIW: I have tracked this problem down. The fix is a little more complicated then I'd like, so I'm going to have to ping some other folks to ensure we concur on the approach before doing something.<br><br>On Oct 25, 2011, at 8:20 AM, Ralph Castain wrote:<br><br><blockquote type="cite">I still see it failing the test George provided on the trunk. I'm unaware of anyone looking further into it, though, as the prior discussion seemed to just end.<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite">On Oct 25, 2011, at 7:01 AM, orel wrote:<br></blockquote><blockquote type="cite"><br></blockquote><blockquote type="cite"><blockquote type="cite">Dears,<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">I try from several days to use advanced MPI2 features in the following scenario :<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">1) a master code A (of size NPA) spawns (MPI_Comm_spawn()) two slave<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"> &nbsp;&nbsp;codes B (of size NPB) and C (of size NPC), providing intercomms A-B and A-C ;<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">2) i create intracomm AB and AC by merging intercomms ;<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">3) then i create intercomm AB-C by calling MPI_Intercomm_create() by using AC as bridge...<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"> &nbsp;MPI_Comm intercommABC; A: MPI_Intercomm_create(intracommAB, 0, intracommAC, NPA, TAG,&amp;intercommABC);<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">B: MPI_Intercomm_create(intracommAB, 0, MPI_COMM_NULL, 0,TAG,&amp;intercommABC);<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">C: MPI_Intercomm_create(intracommC, 0, intracommAC, 0, TAG,&amp;intercommABC);<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"> &nbsp;&nbsp;&nbsp;In these calls, A0 and C0 play the role of local leader for AB and C respectively.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"> &nbsp;&nbsp;&nbsp;C0 and A0 play the roles of remote leader in bridge intracomm AC.<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">3) &nbsp;MPI_Barrier(intercommABC);<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">4) &nbsp;i merge intercomm AB-C into intracomm ABC$<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">5) &nbsp;MPI_Barrier(intracommABC);<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">My BUG: These calls success, but when i try to use intracommABC for a collective communication like MPI_Barrier(),<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i got the following error :<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">*** An error occurred in MPI_Barrier<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">*** on communicator<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">*** MPI_ERR_INTERN: internal error<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">*** MPI_ERRORS_ARE_FATAL: your MPI job will now abort<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">I try with OpenMPI trunk, 1.5.3, 1.5.4 and Mpich2-1.4.1p1<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">My code works perfectly if intracomm A, B and C are obtained by MPI_Comm_split() instead of MPI_Comm_spawn() !!!!<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">I found same problem in a previous thread of the OMPI Users mailing list :<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">=&gt; <a href="http://www.open-mpi.org/community/lists/users/2011/06/16711.php">http://www.open-mpi.org/community/lists/users/2011/06/16711.php</a><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Is that bug/problem is currently under investigation ? :-)<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">i can give detailed code, but the one provided by George Bosilca in this previous thread provides same error...<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Thank you to help me...<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">-- <br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">Aurélien Esnard<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">University Bordeaux 1 / LaBRI / INRIA (France)<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">_______________________________________________<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite">users mailing list<br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><a href="mailto:users@open-mpi.org">users@open-mpi.org</a><br></blockquote></blockquote><blockquote type="cite"><blockquote type="cite"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/users">http://www.open-mpi.org/mailman/listinfo.cgi/users</a><br></blockquote></blockquote><blockquote type="cite"><br></blockquote><br></div></blockquote></div><br></div></body></html>
