<html><head><meta http-equiv="Content-Type" content="text/html charset=us-ascii"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;"><br><div><div>On May 7, 2014, at 7:56 AM, Joshua Ladd &lt;<a href="mailto:jladd.mlnx@gmail.com">jladd.mlnx@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr"><div>Ah, I see. Sorry for the reactionary comment - but this feature falls squarely within my "jurisdiction", and we've invested a lot in improving OMPI jobstart under srun. <br><br>That being said (now that I've taken some deep breaths and carefully read your original email :)), what you're proposing isn't a bad idea. I think it would be good to maybe add a "--with-pmi2" flag to configure since "--with-pmi" automagically uses PMI2 if it finds the header and lib. This way, we could experiment with PMI1/PMI2 without having to rebuild SLURM or hack the installation. <br></div></div></blockquote><div><br></div>That would be a much simpler solution than what Artem proposed (off-list) where we would try PMI2 and then if it didn't work try to figure out how to fall back to PMI1. I'll add this for now, and if Artem wants to try his more automagic solution and can make it work, then we can reconsider that option.</div><div><br></div><div>Thanks</div><div>Ralph</div><div><br><blockquote type="cite"><div dir="ltr"><div>
<br></div>Josh&nbsp; <br></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Wed, May 7, 2014 at 10:45 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">Okay, then we'll just have to develop a workaround for all those Slurm releases where PMI-2 is borked :-(<div>
<br></div><div>FWIW: I think people misunderstood my statement. I specifically did *not* propose to *lose* PMI-2 support. I suggested that we change it to "on-by-request" instead of the current "on-by-default" so we wouldn't keep getting asked about PMI-2 bugs in Slurm. Once the Slurm implementation stabilized, then we could reverse that policy.</div>
<div><br></div><div>However, given that both you and Chris appear to prefer to keep it "on-by-default", we'll see if we can find a way to detect that PMI-2 is broken and then fall back to PMI-1.</div><div><br>
</div><div><br><div><div><div class="h5"><div>On May 7, 2014, at 7:39 AM, Joshua Ladd &lt;<a href="mailto:jladd.mlnx@gmail.com" target="_blank">jladd.mlnx@gmail.com</a>&gt; wrote:</div><br></div></div><blockquote type="cite">
<div><div class="h5"><div dir="ltr"><div><div><div><div>Just saw this thread, and I second Chris' observations: at scale we are seeing huge gains in jobstart performance with PMI2 over PMI1. We <b>CANNOT</b> loose this functionality. For competitive reasons, I cannot provide exact numbers, but let's say the difference is in the ballpark of a full order-of-magnitude on 20K ranks versus PMI1. PMI1 is completely unacceptable/unusable at scale. Certainly PMI2 still has scaling issues, but there is no contest between PMI1 and PMI2.&nbsp; We (MLNX) are actively working to resolve some of the scalability issues in PMI2. <br>

<br></div>Josh<br><br></div><div>Joshua S. Ladd<br></div>Staff Engineer, HPC Software<br></div>Mellanox Technologies<br><br></div>Email: <a href="mailto:joshual@mellanox.com" target="_blank">joshual@mellanox.com</a><br></div>
<div class="gmail_extra">
<br><br><div class="gmail_quote">On Wed, May 7, 2014 at 4:00 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">

Interesting - how many nodes were involved? As I said, the bad scaling becomes more evident at a fairly high node count.<br>
<div><br>
On May 7, 2014, at 12:07 AM, Christopher Samuel &lt;<a href="mailto:samuel@unimelb.edu.au" target="_blank">samuel@unimelb.edu.au</a>&gt; wrote:<br>
<br>
&gt; -----BEGIN PGP SIGNED MESSAGE-----<br>
&gt; Hash: SHA1<br>
&gt;<br>
&gt; Hiya Ralph,<br>
&gt;<br>
&gt; On 07/05/14 14:49, Ralph Castain wrote:<br>
&gt;<br>
&gt;&gt; I should have looked closer to see the numbers you posted, Chris -<br>
&gt;&gt; those include time for MPI wireup. So what you are seeing is that<br>
&gt;&gt; mpirun is much more efficient at exchanging the MPI endpoint info<br>
&gt;&gt; than PMI. I suspect that PMI2 is not much better as the primary<br>
&gt;&gt; reason for the difference is that mpriun sends blobs, while PMI<br>
&gt;&gt; requires that everything be encoded into strings and sent in little<br>
&gt;&gt; pieces.<br>
&gt;&gt;<br>
&gt;&gt; Hence, mpirun can exchange the endpoint info (the dreaded "modex"<br>
&gt;&gt; operation) much faster, and MPI_Init completes faster. Rest of the<br>
&gt;&gt; computation should be the same, so long compute apps will see the<br>
&gt;&gt; difference narrow considerably.<br>
&gt;<br>
&gt; Unfortunately it looks like I had an enthusiastic cleanup at some point<br>
&gt; and so I cannot find the out files from those runs at the moment, but<br>
&gt; I did find some comparisons from around that time.<br>
&gt;<br>
&gt; This first pair are comparing running NAMD with OMPI 1.7.3a1r29103<br>
&gt; run with mpirun and srun successively from inside the same Slurm job.<br>
&gt;<br>
&gt; mpirun namd2 macpf.conf<br>
&gt; srun --mpi=pmi2 namd2 macpf.conf<br>
&gt;<br>
&gt; Firstly the mpirun output (grep'ing the interesting bits):<br>
&gt;<br>
&gt; Charm++&gt; Running on MPI version: 2.1<br>
&gt; Info: Benchmark time: 512 CPUs 0.0959179 s/step 0.555081 days/ns 1055.19 MB memory<br>
&gt; Info: Benchmark time: 512 CPUs 0.0929002 s/step 0.537617 days/ns 1055.19 MB memory<br>
&gt; Info: Benchmark time: 512 CPUs 0.0727373 s/step 0.420933 days/ns 1055.19 MB memory<br>
&gt; Info: Benchmark time: 512 CPUs 0.0779532 s/step 0.451118 days/ns 1055.19 MB memory<br>
&gt; Info: Benchmark time: 512 CPUs 0.0785246 s/step 0.454425 days/ns 1055.19 MB memory<br>
&gt; WallClock: 1403.388550 &nbsp;CPUTime: 1403.388550 &nbsp;Memory: 1119.085938 MB<br>
&gt;<br>
&gt; Now the srun output:<br>
&gt;<br>
&gt; Charm++&gt; Running on MPI version: 2.1<br>
&gt; Info: Benchmark time: 512 CPUs 0.0906865 s/step 0.524806 days/ns 1036.75 MB memory<br>
&gt; Info: Benchmark time: 512 CPUs 0.0874809 s/step 0.506255 days/ns 1036.75 MB memory<br>
&gt; Info: Benchmark time: 512 CPUs 0.0746328 s/step 0.431903 days/ns 1036.75 MB memory<br>
&gt; Info: Benchmark time: 512 CPUs 0.0726161 s/step 0.420232 days/ns 1036.75 MB memory<br>
&gt; Info: Benchmark time: 512 CPUs 0.0710574 s/step 0.411212 days/ns 1036.75 MB memory<br>
&gt; WallClock: 1230.784424 &nbsp;CPUTime: 1230.784424 &nbsp;Memory: 1100.648438 MB<br>
&gt;<br>
&gt;<br>
&gt; The next two pairs are first launched using mpirun from 1.6.x and then with srun<br>
&gt; from 1.7.3a1r29103. &nbsp;Again each pair inside the same Slurm job with the same inputs.<br>
&gt;<br>
&gt; First pair mpirun:<br>
&gt;<br>
&gt; Charm++&gt; Running on MPI version: 2.1<br>
&gt; Info: Benchmark time: 64 CPUs 0.410424 s/step 2.37514 days/ns 909.57 MB memory<br>
&gt; Info: Benchmark time: 64 CPUs 0.392106 s/step 2.26913 days/ns 909.57 MB memory<br>
&gt; Info: Benchmark time: 64 CPUs 0.313136 s/step 1.81213 days/ns 909.57 MB memory<br>
&gt; Info: Benchmark time: 64 CPUs 0.316792 s/step 1.83329 days/ns 909.57 MB memory<br>
&gt; Info: Benchmark time: 64 CPUs 0.313867 s/step 1.81636 days/ns 909.57 MB memory<br>
&gt; WallClock: 8341.524414 &nbsp;CPUTime: 8341.524414 &nbsp;Memory: 975.015625 MB<br>
&gt;<br>
&gt; First pair srun:<br>
&gt;<br>
&gt; Charm++&gt; Running on MPI version: 2.1<br>
&gt; Info: Benchmark time: 64 CPUs 0.341967 s/step 1.97897 days/ns 903.883 MB memory<br>
&gt; Info: Benchmark time: 64 CPUs 0.339644 s/step 1.96553 days/ns 903.883 MB memory<br>
&gt; Info: Benchmark time: 64 CPUs 0.284424 s/step 1.64597 days/ns 903.883 MB memory<br>
&gt; Info: Benchmark time: 64 CPUs 0.28115 s/step 1.62702 days/ns 903.883 MB memory<br>
&gt; Info: Benchmark time: 64 CPUs 0.279536 s/step 1.61769 days/ns 903.883 MB memory<br>
&gt; WallClock: <a href="tel:7476.643555" value="+17476643555" target="_blank">7476.643555</a> &nbsp;CPUTime: <a href="tel:7476.643555" value="+17476643555" target="_blank">7476.643555</a> &nbsp;Memory: 968.867188 MB<br>
&gt;<br>
&gt;<br>
&gt; Second pair mpirun:<br>
&gt;<br>
&gt; Charm++&gt; Running on MPI version: 2.1<br>
&gt; Info: Benchmark time: 64 CPUs 0.366327 s/step 2.11995 days/ns 939.527 MB memory<br>
&gt; Info: Benchmark time: 64 CPUs 0.359805 s/step 2.0822 days/ns 939.527 MB memory<br>
&gt; Info: Benchmark time: 64 CPUs 0.292342 s/step 1.69179 days/ns 939.527 MB memory<br>
&gt; Info: Benchmark time: 64 CPUs 0.293499 s/step 1.69849 days/ns 939.527 MB memory<br>
&gt; Info: Benchmark time: 64 CPUs 0.292355 s/step 1.69187 days/ns 939.527 MB memory<br>
&gt; WallClock: 7842.831543 &nbsp;CPUTime: 7842.831543 &nbsp;Memory: 1004.050781 MB<br>
&gt;<br>
&gt; Second pair srun:<br>
&gt;<br>
&gt; Charm++&gt; Running on MPI version: 2.1<br>
&gt; Info: Benchmark time: 64 CPUs 0.347864 s/step 2.0131 days/ns 904.91 MB memory<br>
&gt; Info: Benchmark time: 64 CPUs 0.346367 s/step 2.00444 days/ns 904.91 MB memory<br>
&gt; Info: Benchmark time: 64 CPUs 0.29007 s/step 1.67865 days/ns 904.91 MB memory<br>
&gt; Info: Benchmark time: 64 CPUs 0.279447 s/step 1.61717 days/ns 904.91 MB memory<br>
&gt; Info: Benchmark time: 64 CPUs 0.280824 s/step 1.62514 days/ns 904.91 MB memory<br>
&gt; WallClock: 7522.677246 &nbsp;CPUTime: 7522.677246 &nbsp;Memory: 969.433594 MB<br>
&gt;<br>
&gt;<br>
&gt; So to me it looks like (for NAMD on our system at least) that<br>
&gt; PMI2 does seem to give better scalability.<br>
&gt;<br>
&gt; All the best!<br>
&gt; Chris<br>
&gt; - --<br>
&gt; Christopher Samuel &nbsp; &nbsp; &nbsp; &nbsp;Senior Systems Administrator<br>
&gt; VLSCI - Victorian Life Sciences Computation Initiative<br>
&gt; Email: <a href="mailto:samuel@unimelb.edu.au" target="_blank">samuel@unimelb.edu.au</a> Phone: <a href="tel:%2B61%20%280%293%20903%2055545" value="+61390355545" target="_blank">+61 (0)3 903 55545</a><br>
&gt; <a href="http://www.vlsci.org.au/" target="_blank">http://www.vlsci.org.au/</a> &nbsp; &nbsp; &nbsp;<a href="http://twitter.com/vlsci" target="_blank">http://twitter.com/vlsci</a><br>
&gt;<br>
&gt; -----BEGIN PGP SIGNATURE-----<br>
&gt; Version: GnuPG v1.4.14 (GNU/Linux)<br>
&gt; Comment: Using GnuPG with Thunderbird - <a href="http://www.enigmail.net/" target="_blank">http://www.enigmail.net/</a><br>
&gt;<br>
&gt; iEYEARECAAYFAlNp28UACgkQO2KABBYQAh8hagCfewbbxUR6grg5R40GrwjtIZV0<br>
&gt; 1KYAn2uX0yKLdOEbkHARKouzwFilaTTD<br>
&gt; =A/Yw<br>
&gt; -----END PGP SIGNATURE-----<br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/05/14697.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/05/14697.php</a><br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</div>Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/05/14698.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/05/14698.php</a><br>
</blockquote></div><br></div>
_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</div></div>Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/05/14707.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/05/14707.php</a></blockquote></div><br></div></div>
<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/05/14708.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/05/14708.php</a><br></blockquote></div><br></div>
_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/devel<br>Link to this post: http://www.open-mpi.org/community/lists/devel/2014/05/14711.php</blockquote></div><br></body></html>
