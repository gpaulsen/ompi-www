<html><head><meta http-equiv="Content-Type" content="text/html charset=us-ascii"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">Well, the problem is that we can't simply decide that anything called "ib.." is an IB port and should be ignored. There is no naming rule regarding IP interfaces that I've ever heard about that would allow us to make such an assumption, though I admit most people let the system create default names and thus would get something like an "ib..".<div><br></div><div>So we leave it up to the sys admin to configure the system based on their knowledge of what they want to use. On the big clusters at the labs, we commonly put MCA params in the default param file for this purpose as we *don't* want OOB traffic going over the IB fabric.</div><div><br></div><div>But that's the sys admin's choice, not a requirement. I've seen organizations that do it the other way because their Ethernet is really slow.</div><div><br></div><div>In this case, the problem is really in the OOB itself. The local proc is connecting to its local daemon via eth0, which is fine. When it sends a message to mpirun on a different proc, that message goes from the app to the daemon via eth0. The daemon looks for mpirun in its contact list, and sees that it has a direct link to mpirun via this nifty "ib0" interface - and so it uses that one to relay the message along.</div><div><br></div><div>This is where we are hitting the problem - the OOB isn't correctly doing the transfer between those two interfaces like it should. So it is a bug that we need to fix, regardless of any other actions (e.g., if it was an eth1 that was the direct connection, we would still want to transfer the message to the other interface).</div><div><br></div><div>HTH</div><div>Ralph</div><div><br><div><div><div>On Jun 4, 2014, at 7:32 PM, Gilles Gouaillardet &lt;<a href="mailto:gilles.gouaillardet@gmail.com">gilles.gouaillardet@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite"><div dir="ltr"><div><div>Thanks Ralf,<br><br></div>for the time being, i just found a workaround<br></div>--mca oob_tcp_if_include eth0<br><div><div class="gmail_extra"><br></div><div class="gmail_extra">Generally speaking, is openmpi doing the wiser thing ?<br>
</div><div class="gmail_extra">here is what i mean :<br></div><div class="gmail_extra">the cluster i work on (4k+ nodes) each node has two ip interfaces :<br></div><div class="gmail_extra">&nbsp;* eth0 (gigabit ethernet) : because of the cluster size, several subnets are used.<br>
</div><div class="gmail_extra">&nbsp;* ib0 (IP over IB) : only one subnet<br></div><div class="gmail_extra">i can easily understand such a large cluster is not so common, but on the other hand i do not believe the IP configuration (subnetted gigE and single subnet IPoIB) can be called exotic.<br>
<br></div><div class="gmail_extra">if nodes from different eth0 subnets are used, and if i understand correctly your previous replies, orte will "discard" eth0 because nodes cannot contact each other "directly".<br>
</div><div class="gmail_extra">directly means the nodes are not on the same subnet. that being said, they can communicate via IP thanks to IP routing (i mean IP routing, i do *not* mean orte routing).<br>that means orte communications will use IPoIB which might not be the best thing to do since establishing an IPoIB connection can be long (especially at scale *and* if the arp table is not populated)<br>
<br></div><div class="gmail_extra">is my understanding correct so far ?<br><br></div><div class="gmail_extra">bottom line, i would have expected openmpi uses eth0 regardless IP routing is required, and ib0 is simply not used (or eventually used as a fallback option)<br>
</div><div class="gmail_extra"><br></div><div class="gmail_extra">this leads to my next question : is the current default ok ? if not should we change it and how ?<br></div><div class="gmail_extra">/*<br>imho :<br>&nbsp;- IP routing is not always a bad/slow thing<br>
&nbsp;- gigE can sometimes be better than IPoIB)<br>*/<br><br></div><div class="gmail_extra">i am fine if at the end :<br>- this issue is fixed<br>- we decide it is up to the sysadmin to make --mca oob_tcp_if_include eth0 the default if this is really thought to be best for the cluster. (and i can try to draft a faq if needed)<br>
<br></div><div class="gmail_extra">Cheers,<br><br>Gilles<br><br></div><div class="gmail_extra"><div class="gmail_quote">On Wed, Jun 4, 2014 at 11:50 PM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word"><br><div>I'll work on it - may take a day or two to really fix. Only impacts systems with mismatched interfaces, which is why we aren't generally seeing it.</div>
<br></div></blockquote></div></div></div></div>
_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/devel<br>Link to this post: http://www.open-mpi.org/community/lists/devel/2014/06/14972.php</blockquote></div><br></div></div></body></html>
