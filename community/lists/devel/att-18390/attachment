<html><head><meta http-equiv="Content-Type" content="text/html charset=utf-8"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;" class="">I agree about the limitation. However, what Howard is doing helps resolve it by breaking out the Jenkins runs into categories. So instead of one massive test session, setup one Jenkins server for each category. Then we can set the specific tags according to the test category.<div class=""><br class=""></div><div class="">Make sense?</div><div class="">Ralph</div><div class=""><br class=""><div><blockquote type="cite" class=""><div class="">On Nov 25, 2015, at 3:54 AM, Gilles Gouaillardet &lt;<a href="mailto:gilles.gouaillardet@gmail.com" class="">gilles.gouaillardet@gmail.com</a>&gt; wrote:</div><br class="Apple-interchange-newline"><div class="">Ralph and all,<div class=""><br class=""></div><div class="">My 0.02US$</div><div class=""><br class=""></div><div class="">We are kind of limited by the github API&nbsp;<a href="https://developer.github.com/v3/repos/statuses/" class="">https://developer.github.com/v3/repos/statuses/</a></div><div class="">Basically, a status is pending, success, error or failure plus a string.</div><div class=""><br class=""></div><div class="">A possible work around is to have Jenkins set labels on the PR.</div><div class=""><br class=""></div><div class="">If only valgrind fails, the status could be succes, and the valgrind failure cold be reported via the status string (that no one might bother reading, but this is an other story) or via a label</div><div class="">(Should the label be for success or failure ?)</div><div class=""><br class=""></div><div class="">I agree it is not obvious (not to say impossible) to fully understand what Jenkins is doing under the hood. That could/should be documented, or at least, the Jenkins plugin could be made published</div><div class="">(Public repository like the bot used to set labels/milestones/assignees, or private in the ompi-tests repository)</div><div class=""><br class=""></div><div class="">I will give some more thoughts to the testing part.</div><div class=""><br class=""></div><div class="">Cheers,</div><div class=""><br class=""></div><div class="">Gilles</div><div class=""><br class="">On Wednesday, November 25, 2015, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org" class="">rhc@open-mpi.org</a>&gt; wrote:<br class=""><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi folks<br class="">
<br class="">
I wanted to pull this conversation out from the specific issue where it was being conducted as I think it merits a broader discussion. I understand and appreciate the role of the Jenkins testing - what I am trying to find is a way to make that testing more usable.<br class="">
<br class="">
There are two things that I think would help:<br class="">
<br class="">
1. separating the tests being conducted into different “buckets”. We now have several types of testing being conducted:<br class="">
<br class="">
&nbsp; &nbsp; * simple build tests. These don’t involve any execution. If something fails a build test, it would be<br class="">
&nbsp; &nbsp; &nbsp; very helpful to clearly state exactly what configure options were being used, and what compiler.<br class="">
&nbsp; &nbsp; &nbsp; Ideally, such failures would be labeled as “build”.<br class="">
<br class="">
&nbsp; &nbsp; * valgrind tests. These are problematic in that they are not necessarily PR-specific - if anything<br class="">
&nbsp; &nbsp; &nbsp; &nbsp;causes a leak or valgrind issue, every PR is marked as “failed” and can lead to wasted time<br class="">
&nbsp; &nbsp; &nbsp; &nbsp;chasing non-existent problems with a specific PR. Unfortunately, I can’t think of a way to get<br class="">
&nbsp; &nbsp; &nbsp; &nbsp;Jenkins to properly deal with the issue other than to mark such test results as “valgrind” so<br class="">
&nbsp; &nbsp; &nbsp; &nbsp;they are clearly called out as being in that category<br class="">
<br class="">
&nbsp; &nbsp; * distribution tests that build tarballs, run “make distcheck”, etc. These usually fail due to something<br class="">
&nbsp; &nbsp; &nbsp; not being included in the tarball, or some directory not being completely cleaned. This is<br class="">
&nbsp; &nbsp; &nbsp; another case where it is really important to know, for example, that someone used a platform<br class="">
&nbsp; &nbsp; &nbsp; file when building the tarball, so it would really help to know exactly how this test was conducted.<br class="">
&nbsp; &nbsp; &nbsp; Ideally, any distribution test failure would be marked as “distribution” so we know what happened.<br class="">
<br class="">
&nbsp; &nbsp; * run tests that execute various programs. Lots of things can go wrong here, many of them<br class="">
&nbsp; &nbsp; &nbsp; dependent on exactly how the code was built (so we know which components were<br class="">
&nbsp; &nbsp; &nbsp; around) and how it was run (e.g., default MCA param file). Ideally, these failures would<br class="">
&nbsp; &nbsp; &nbsp; be marked as “run”.<br class="">
<br class="">
Please note: when I ask for a clear statement of configuration options etc, what I’m saying is that it is very hard to sift thru hundreds of lines of output to find, for example, the cmd line that failed. A more concise test output would make debugging much faster and easier, and therefore make Jenkins testing much more usable.<br class="">
<br class="">
<br class="">
2. having the Jenkins testers clearly tell us what tests they are expecting us to pass. Perhaps a list of these could be posted somewhere, and some notification given as to when those lists are being changed? It would help avoid surprises and allow developers a chance to test things themselves before posting PRs.<br class="">
<br class="">
I know I’m asking for some effort on behalf of those running these servers. However, I think it would make those efforts much more useful.<br class="">
Ralph<br class="">
<br class="">
_______________________________________________<br class="">
devel mailing list<br class="">
<a href="javascript:;" onclick="_e(event, 'cvml', 'devel@open-mpi.org')" class="">devel@open-mpi.org</a><br class="">
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank" class="">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br class="">
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/11/18388.php" target="_blank" class="">http://www.open-mpi.org/community/lists/devel/2015/11/18388.php</a></blockquote></div>
_______________________________________________<br class="">devel mailing list<br class=""><a href="mailto:devel@open-mpi.org" class="">devel@open-mpi.org</a><br class="">Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/devel<br class="">Link to this post: http://www.open-mpi.org/community/lists/devel/2015/11/18389.php</div></blockquote></div><br class=""></div></body></html>
