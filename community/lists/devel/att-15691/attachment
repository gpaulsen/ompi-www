<html><head><meta http-equiv="Content-Type" content="text/html charset=iso-8859-1"></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space;">You might want to try again with current head of trunk as something seems off in what you are seeing - more below<div><br></div><div><br><div><div>On Aug 22, 2014, at 3:12 AM, Gilles Gouaillardet &lt;<a href="mailto:gilles.gouaillardet@iferc.org">gilles.gouaillardet@iferc.org</a>&gt; wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite">Ralph,<br><br>i tried again after the merge and found the same behaviour, though the<br>internals are very different.<br><br>i run without any batch manager<br><br>from node0:<br>mpirun -np 1 --mca btl tcp,self -host node1 ./abort<br><br>exit with exit code zero :-(<br></blockquote><div><br></div>Hmmm...it works fine for me, without your patch:</div><div><br></div><div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);"><span style="color: #7c7c7c">07:35:41&nbsp;</span> $ mpirun -n 1 -mca btl tcp,self -host bend002 ./abort</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">Hello, World, I am 0 of 1</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">--------------------------------------------------------------------------</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD&nbsp;</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">with errorcode 2.</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156); min-height: 13px;"><br></div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">You may or may not see output from other processes, depending on</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">exactly when Open MPI kills them.</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">--------------------------------------------------------------------------</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">--------------------------------------------------------------------------</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">mpirun noticed that process rank 0 with PID 24382 on node bend002 exited on signal 0 (Unknown signal 0).</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">--------------------------------------------------------------------------</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; color: rgb(206, 51, 204); background-color: rgb(254, 244, 156);"><span style="color: #7c7c7c">07:35:56&nbsp;</span><span style="color: #000000"> </span><span style="color: #000000">$ showcode</span></div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">130</div><div><br></div><blockquote type="cite"><br>short story : i applied pmix.2.patch and that fixed my problem<br>could you please review this ?<br><br>long story :<br>i initially applied pmix.1.patch and it solved my problem<br>then i ran<br>mpirun -np 1 --mca btl openib,self -host node1 ./abort<br>and i came back to square one : exit code is zero<br>so i used the debugger and was unable to reproduce the issue<br>(one more race condition, yeah !)<br>finally, i wrote pmix.2.patch, fixed my issue and realized that<br>pmix.1.patch was no more needed.<br>currently, and assuming pmix.2.patch is correct, i cannot tell wether<br>pmix.1.patch is needed or not<br>since this part of the code is no more executed.<br><br>i also found one hang with the following trivial program within one node :<br><br>int main (int argc, char *argv[]) {<br> &nbsp;&nbsp;&nbsp;&nbsp;MPI_Init(&amp;argc, &amp;argv);<br> &nbsp;&nbsp;&nbsp;MPI_Finalize();<br> &nbsp;&nbsp;&nbsp;return 3;<br>}<br><br>from node0 :<br>$ mpirun -np 1 ./test<br>-------------------------------------------------------<br>Primary job &nbsp;terminated normally, but 1 process returned<br>a non-zero exit code.. Per user-direction, the job has been aborted.<br>-------------------------------------------------------<br><br>AND THE PROGRAM HANGS<br></blockquote><div><br></div>This also works fine for me:</div><div><br></div><div><div style="margin: 0px; font-size: 11px; font-family: Menlo; color: rgb(206, 51, 204); background-color: rgb(254, 244, 156);"><span style="color: #7c7c7c">07:37:27&nbsp;</span><span style="color: #000000"> </span><span style="color: #000000">$ mpirun -n 1 ./mpi_no_op</span></div><div style="margin: 0px; font-size: 11px; font-family: Menlo; color: rgb(206, 51, 204); background-color: rgb(254, 244, 156);"><span style="color: #7c7c7c">07:37:36&nbsp;</span><span style="color: #000000"> </span><span style="color: #000000">$ cat mpi_no_op.c</span></div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">/* -*- C -*-</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">&nbsp;*</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">&nbsp;* $HEADER$</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">&nbsp;*</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">&nbsp;* The most basic of MPI applications</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">&nbsp;*/</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156); min-height: 13px;"><br></div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">#include &lt;stdio.h&gt;</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">#include "mpi.h"</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156); min-height: 13px;"><br></div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">int main(int argc, char* argv[])</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">{</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">&nbsp; &nbsp; MPI_Init(&amp;argc, &amp;argv);</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156); min-height: 13px;"><br></div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">&nbsp; &nbsp; MPI_Finalize();</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">&nbsp; &nbsp; return 0;</div><div style="margin: 0px; font-size: 11px; font-family: Menlo; background-color: rgb(254, 244, 156);">}</div><div><br></div><div><br></div><blockquote type="cite"><br>*but*<br>$ mpirun -np 1 -host node1 ./test<br>-------------------------------------------------------<br>Primary job &nbsp;terminated normally, but 1 process returned<br>a non-zero exit code.. Per user-direction, the job has been aborted.<br>-------------------------------------------------------<br>--------------------------------------------------------------------------<br>mpirun detected that one or more processes exited with non-zero status,<br>thus causing<br>the job to be terminated. The first process to do so was:<br><br> &nbsp;Process name: [[22080,1],0]<br> &nbsp;Exit code: &nbsp;&nbsp;&nbsp;3<br>--------------------------------------------------------------------------<br><br>return with exit code 3.<br></blockquote><div><br></div>Likewise here - works just fine for me</div><div><br></div><div><br><blockquote type="cite"><br>then i found a strange behaviour with helloworld if only the self btl is<br>used :<br>$ mpirun -np 1 --mca btl self ./hw<br>[helios91:23319] OPAL dss:unpack: got type 12 when expecting type 3<br>[helios91:23319] [[22303,0],0] ORTE_ERROR_LOG: Pack data mismatch in<br>file ../../../src/ompi-trunk/orte/orted/pmix/pmix_server_sendrecv.c at<br>line 722<br><br>the program returns with exit code zero, but display an error message.<br><br>Cheers,<br><br>Gilles<br><br>On 2014/08/21 6:21, Ralph Castain wrote:<br><blockquote type="cite">I'm aware of the problem, but it will be fixed when the PMIx branch is merged later this week.<br><br>On Aug 19, 2014, at 10:00 PM, Gilles Gouaillardet &lt;<a href="mailto:gilles.gouaillardet@iferc.org">gilles.gouaillardet@iferc.org</a>&gt; wrote:<br><br><blockquote type="cite">Folks,<br><br>let's look at the following trivial test program :<br><br>#include &lt;mpi.h&gt;<br>#include &lt;stdio.h&gt;<br><br>int main (int argc, char * argv[]) {<br> &nbsp;&nbsp;int rank, size;<br> &nbsp;&nbsp;MPI_Init(&amp;argc, &amp;argv);<br> &nbsp;&nbsp;MPI_Comm_size(MPI_COMM_WORLD, &amp;size);<br> &nbsp;&nbsp;MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);<br> &nbsp;&nbsp;printf ("I am %d/%d and i abort\n", rank, size);<br> &nbsp;&nbsp;MPI_Abort(MPI_COMM_WORLD, 2);<br> &nbsp;&nbsp;printf ("%d/%d aborted !\n", rank, size);<br> &nbsp;&nbsp;return 3;<br>}<br><br>and let's run mpirun (trunk) on node0 and ask the mpi task to run on<br>task 1 :<br>with two tasks or more :<br><br>node0 $ mpirun --mca btl tcp,self -host node1 -np 2 ./abort<br>--------------------------------------------------------------------------<br>MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD<br>with errorcode 2.<br><br>NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.<br>You may or may not see output from other processes, depending on<br>exactly when Open MPI kills them.<br>--------------------------------------------------------------------------<br>I am 1/2 and i abort<br>I am 0/2 and i abort<br>[node0:00740] 1 more process has sent help message help-mpi-api.txt /<br>mpi-abort<br>[node0:00740] Set MCA parameter "orte_base_help_aggregate" to 0 to see<br>all help / error messages<br><br>node0 $ echo $?<br>0<br><br>the exit status of mpirun is zero<br>/* this is why the MPI_Errhandler_fatal_c test fails in mtt */<br><br>now if we run only one task :<br><br>node0 $ mpirun --mca btl tcp,self -host node1 -np 1 ./abort<br>I am 0/1 and i abort<br>--------------------------------------------------------------------------<br>MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD<br>with errorcode 2.<br><br>NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.<br>You may or may not see output from other processes, depending on<br>exactly when Open MPI kills them.<br>--------------------------------------------------------------------------<br>--------------------------------------------------------------------------<br>mpirun has exited due to process rank 0 with PID 15884 on<br>node node1 exiting improperly. There are three reasons this could occur:<br><br>1. this process did not call "init" before exiting, but others in<br>the job did. This can cause a job to hang indefinitely while it waits<br>for all processes to call "init". By rule, if one process calls "init",<br>then ALL processes must call "init" prior to termination.<br><br>2. this process called "init", but exited without calling "finalize".<br>By rule, all processes that call "init" MUST call "finalize" prior to<br>exiting or it will be considered an "abnormal termination"<br><br>3. this process called "MPI_Abort" or "orte_abort" and the mca parameter<br>orte_create_session_dirs is set to false. In this case, the run-time cannot<br>detect that the abort call was an abnormal termination. Hence, the only<br>error message you will receive is this one.<br><br>This may have caused other processes in the application to be<br>terminated by signals sent by mpirun (as reported here).<br><br>You can avoid this message by specifying -quiet on the mpirun command line.<br><br>--------------------------------------------------------------------------<br>node0 $ echo $?<br>1<br><br>the program displayed a misleading error message and mpirun exited with<br>error code 1<br>/* i would have expected 2, or 3 in the worst case scenario */<br><br><br>i digged it a bit and found a kind of race condition in orted (running<br>on node 1)<br>basically, when the process dies, it writes stuff in the openmpi session<br>directory and exits.<br>exiting send a SIGCHLD to orted and close the socket/pipe connected to<br>orted.<br>on orted, the loss of connection is generally processed before the<br>SIGCHLD by libevent,<br>and as a consequence, the exit code is not correctly set (e.g. it is<br>left to zero).<br>i did not see any kind of communication between the mpi task and orted<br>(except writing a file in the openmpi session directory) as i would have<br>expected<br>/* but this was just my initial guess, the truth is i do not know what<br>is supposed to happen */<br><br>i wrote the attached abort.patch patch to basically get it working.<br>i highly suspect this is not the right thing to do so i did not commit it.<br><br>it works fine with two tasks or more.<br>with only one task, mpirun display a misleading error message but the<br>exit status is ok.<br><br>could someone (Ralph ?) have a look at this ?<br><br>Cheers,<br><br>Gilles<br><br><br>node0 $ mpirun --mca btl tcp,self -host node1 -np 2 ./abort<br>I am 1/2 and i abort<br>I am 0/2 and i abort<br>--------------------------------------------------------------------------<br>MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD<br>with errorcode 2.<br><br>NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.<br>You may or may not see output from other processes, depending on<br>exactly when Open MPI kills them.<br>--------------------------------------------------------------------------<br>[node0:00920] 1 more process has sent help message help-mpi-api.txt /<br>mpi-abort<br>[node0:00920] Set MCA parameter "orte_base_help_aggregate" to 0 to see<br>all help / error messages<br>node0 $ echo $?<br>2<br><br><br><br>node0 $ mpirun --mca btl tcp,self -host node1 -np 1 ./abort<br>I am 0/1 and i abort<br>--------------------------------------------------------------------------<br>MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD<br>with errorcode 2.<br><br>NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.<br>You may or may not see output from other processes, depending on<br>exactly when Open MPI kills them.<br>--------------------------------------------------------------------------<br>-------------------------------------------------------<br>Primary job &nbsp;terminated normally, but 1 process returned<br>a non-zero exit code.. Per user-direction, the job has been aborted.<br>-------------------------------------------------------<br>--------------------------------------------------------------------------<br>mpirun detected that one or more processes exited with non-zero status,<br>thus causing<br>the job to be terminated. The first process to do so was:<br><br> Process name: [[7955,1],0]<br> Exit code: &nbsp;&nbsp;&nbsp;2<br>--------------------------------------------------------------------------<br>node0 $ echo $?<br>2<br><br><br><br>&lt;abort.patch&gt;_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/devel<br>Link to this post: http://www.open-mpi.org/community/lists/devel/2014/08/15666.php<br></blockquote>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/devel<br>Link to this post: http://www.open-mpi.org/community/lists/devel/2014/08/15672.php<br></blockquote><br><span>&lt;pmix.1.patch&gt;</span><span>&lt;pmix.2.patch&gt;</span>_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>Subscription: http://www.open-mpi.org/mailman/listinfo.cgi/devel<br>Link to this post: http://www.open-mpi.org/community/lists/devel/2014/08/15689.php</blockquote></div><br></div></body></html>
