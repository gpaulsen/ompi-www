<div dir="ltr">Ralph,<div><br></div><div>in the case of intercomm_create, the children free all the communicators and then MPI_Disconnect() and then MPI_Finalize() and exits.</div><div>the parent only MPI_Disconnect() without freeing all the communicators. MPI_Finalize() tries to disconnect and communicate with already exited processes.</div>
<div><br></div><div>my understanding is that there are two ways of seeing things :</div><div>a) the &quot;R-way&quot; : the problem is the parent should not try to communicate to already exited processes</div><div>b) the &quot;J-way&quot; : the problem is the children should have waited either in MPI_Comm_free() or MPI_Finalize()</div>
<div><br></div><div>i did not investigate the loop_spawn test yet, and will do today.</div><div><br></div><div>as far as i am concerned, i have no opinion on which of a) or b) is the correct/most appropriate approach.</div>
<div><br></div><div>Cheers,</div><div><br></div><div>Gilles</div></div><div class="gmail_extra"><br><br><div class="gmail_quote">On Wed, May 28, 2014 at 9:46 AM, Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org" target="_blank">rhc@open-mpi.org</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Since you ignored my response, I&#39;ll reiterate and clarify it here. The problem in the case of loop_spawn is that the parent process remains &quot;connected&quot; to children after the child has finalized and died. Hence, when the parent attempts to finalize, it tries to &quot;disconnect&quot; itself from processes that no longer exist - and that is what generates the error message.<br>

<br>
So the issue in that case appears to be that &quot;finalize&quot; is not marking the child process as &quot;disconnected&quot;, thus leaving the parent thinking that it needs to disconnect when it finally ends.<br>
<div><div class="h5"><br>
<br>
On May 27, 2014, at 5:33 PM, Jeff Squyres (jsquyres) &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt; wrote:<br>
<br>
&gt; Note that MPI says that COMM_DISCONNECT simply disconnects that individual communicator.  It does *not* guarantee that the processes involved will be fully disconnected.<br>
&gt;<br>
&gt; So I think that the freeing of communicators is good app behavior, but it is not required by the MPI spec.<br>
&gt;<br>
&gt; If OMPI is requiring this for correct termination, then something is wrong.  MPI_FINALIZE is supposed to be collective across all connected MPI procs -- and if the parent and spawned procs in this test are still connected (because they have not disconnected all communicators between them), the FINALIZE is supposed to be collective across all of them.<br>

&gt;<br>
&gt; This means that FINALIZE is allowed to block if it needs to, such that OMPI sending control messages to procs that are still &quot;connected&quot; (in the MPI sense) should never cause a race condition.<br>
&gt;<br>
&gt; As such, this sounds like an OMPI bug.<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; On May 27, 2014, at 2:27 AM, Gilles Gouaillardet &lt;<a href="mailto:gilles.gouaillardet@gmail.com">gilles.gouaillardet@gmail.com</a>&gt; wrote:<br>
&gt;<br>
&gt;&gt; Folks,<br>
&gt;&gt;<br>
&gt;&gt; currently, the dynamic/intercomm_create test from the ibm test suite output the following messages :<br>
&gt;&gt;<br>
&gt;&gt; dpm_base_disconnect_init: error -12 in isend to process 1<br>
&gt;&gt;<br>
&gt;&gt; the root cause it task 0 tries to send messages to already exited tasks.<br>
&gt;&gt;<br>
&gt;&gt; one way of seeing things is that this is an application issue :<br>
&gt;&gt; task 0 should have MPI_Comm_free&#39;d all its communicator before calling MPI_Comm_disconnect.<br>
&gt;&gt; This can be achieved via the attached patch<br>
&gt;&gt;<br>
&gt;&gt; an other way of seeing things is that this is a bug in OpenMPI.<br>
&gt;&gt; In this case, what would be the the right approach ?<br>
&gt;&gt; - automatically free communicators (if needed) when MPI_Comm_disconnect is invoked ?<br>
&gt;&gt; - simply remove communicators (if needed) from ompi_mpi_communicators when MPI_Comm_disconnect is invoked ?<br>
&gt;&gt;  /* this causes a memory leak, but the application can be seen as responsible of it */<br>
&gt;&gt; - other ?<br>
&gt;&gt;<br>
&gt;&gt; Thanks in advance for your feedback,<br>
&gt;&gt;<br>
&gt;&gt; Gilles<br>
&gt;&gt; &lt;intercomm_create.patch&gt;_______________________________________________<br>
&gt;&gt; devel mailing list<br>
&gt;&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt;&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt;&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/05/14847.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/05/14847.php</a><br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Jeff Squyres<br>
&gt; <a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
&gt; For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt; Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/05/14875.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/05/14875.php</a><br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</div></div>Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2014/05/14876.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2014/05/14876.php</a><br>
</blockquote></div><br></div>

