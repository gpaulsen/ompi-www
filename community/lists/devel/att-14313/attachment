<div dir="ltr"><div>Hello,</div><div><br></div>I think I am ready to return to mpirun affinity handling discussion. I have more general solution now. It is beta-tested (just one cluster with SLURM using cgroups plugin). But it shows my main idea and if it is worth to be included into mainstream I am ready to polish or improove it.<div>
<br></div><div>The code respects SLURM cpu allocation through SLURM_JOB_CPUS_PER_NODE and handles such cases correctly:</div><div><div>SLURM_JOB_CPUS_PER_NODE=&#39;12(x3),7&#39;</div><div>SLURM_JOB_NODELIST=node[0-3]<br></div>
</div><div><br></div><div>It splits the node lists into groups having equal number of cpus. In the example above we will have 2 groups:</div><div>1) node0, node1, node2 with 12 cpus</div><div>2) node3 with 7 cpus.</div><div>
<br></div><div>then it uses separate srun&#39;s for each group.</div><div><br></div><div>The weakness of this patch is that we need to deal with several srun&#39;s and I am not sure that cleanup will perform correctly. I plan to test this case additionaly.</div>
</div><div class="gmail_extra"><br><br><div class="gmail_quote">2014-02-12 17:42 GMT+07:00 Artem Polyakov <span dir="ltr">&lt;<a href="mailto:artpol84@gmail.com" target="_blank">artpol84@gmail.com</a>&gt;</span>:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<div dir="ltr">Hello<div><br></div><div>I found that SLURM installations that use cgroup plugin and have TaskAffinity=yes in cgroup.conf have problems with OpenMPI: all processes on non-launch node are assigned on one core. This leads to quite poor performance.</div>

<div>The problem can be seen only if using mpirun to start parallel application in batch script. For example: <b>mpirun ./mympi</b></div><div>If using srun with PMI affinity is setted properly: <b>srun ./mympi.</b><br></div>

<div><br></div><div>Close look shows that the reason lies in the way Open MPI use srun to launch ORTE daemons. Here is example of the command line:</div><div><b>srun</b> <b>--nodes=1</b> <b>--ntasks=1</b> --kill-on-bad-exit --nodelist=node02 <b>orted</b> -mca ess slurm -mca orte_ess_jobid 3799121920 -mca orte_ess_vpid <br>

</div><div> </div><div>Saying <b>--nodes=1</b> <b>--ntasks=1</b> to SLURM means that you want to start one task and (with TaskAffinity=yes) it will be binded to one core. Next orted use this affinity as base for all spawned branch processes. If I understand correctly the problem behind using srun is that if you say <b>srun</b> <b>--nodes=1</b> <b>--ntasks=4</b> - then SLURM will spawn 4 independent orted processes binded to different cores which is not what we really need.</div>

<div><br></div><div>I found that disabling of cpu binding as a fast hack works good for cgroup plugin. Since job runs inside a group which has core access restrictions, spawned branch processes are executed under nodes scheduler control on all allocated cores. The command line looks like this:</div>

<div><div>srun <b>--cpu_bind=none</b> --nodes=1 --ntasks=1 --kill-on-bad-exit --nodelist=node02 orted -mca ess slurm -mca orte_ess_jobid 3799121920 -mca orte_ess_vpid </div></div><div><br></div><div>This solution will probably won&#39;t work with SLURM task/affinity plugin. Also it may be a bad idea when strong affinity desirable.</div>

<div><br></div><div>My patch to stable Open MPI version (1.6.5) is attached to this e-mail. I will try to make more reliable solution but I need more time and beforehand would like to know opinion of Open MPI developers.</div>
<span class="HOEnZb"><font color="#888888">
<div><div><br></div>-- <br>С Уважением, Поляков Артем Юрьевич<br>Best regards, Artem Y. Polyakov
</div></font></span></div>
</blockquote></div><br><br clear="all"><div><br></div>-- <br>С Уважением, Поляков Артем Юрьевич<br>Best regards, Artem Y. Polyakov
</div>

