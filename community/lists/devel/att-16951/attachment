<div dir="ltr"><div><br></div>     We have nodes in our HPC system that have 2 NIC&#39;s, <div>one being QDR IB and the second being a slower 10 Gbps card</div><div>configured for both RoCE and TCP.  Aggregate bandwidth </div><div>tests with 20 cores on one node yelling at 20 cores on a second</div><div>node (attached roce.ib.aggregate.pdf) show that without tuning</div><div>the slower RoCE interface is being used for small messages</div><div>then QDR IB is used for larger messages (red line).  Tuning</div><div>the tcp_exclusivity to 1024 to match the openib_exclusivity </div><div>adds another 20 Gbps of bidirectional bandwidth to the high end (green line),</div><div>and I&#39;m guessing this is TCP traffic and not RoCE.</div><div><br></div><div>     So by default the slower interface is being chosen on the low end, and</div><div>I don&#39;t think there are tunable parameters to allow me to choose the </div><div>QDR interface as the default.  Going forward we&#39;ll probably just disable </div><div>RoCE on these nodes and go with QDR IB plus 10 Gbps TCP for large messages.   </div><div><br></div><div>      However, I do think these issues will come up more in the future.</div><div>With the low latency of RoCE matching IB, there are more opportunities</div><div>to do channel bonding or allowing multiple interfaces for aggregate traffic</div><div>for even smaller message sizes. </div><div><br></div><div>                Dave Turner</div><div><div><br></div>-- <br><div class="gmail_signature"><div dir="ltr">Work:     <a href="mailto:DaveTurner@ksu.edu" target="_blank">DaveTurner@ksu.edu</a>     (785) 532-7791<div>             118 Nichols Hall, Manhattan KS  66502<br>Home:    <a href="mailto:DrDaveTurner@gmail.com" target="_blank">DrDaveTurner@gmail.com</a><br>              cell: (785) 770-5929<br></div></div></div>
</div></div>

