<div dir="ltr"><div><div><div><div><div>Hi Gilles,<br><br></div>I enhanced my simple test program to dump the contents of the buffer:<br></div><div>If I am not mistaken, it appears that the unpack is not doing the endian conversion.<br><br></div><div>kindest regards<br></div><div>Mike<br></div><div><br></div>Good:<br><span style="font-family:monospace,monospace">send data 000004d2 0000162e <br>MPI_Pack_external: 0<br>buffer size: 8<br>Buffer contents<br>d2, 04, 00, 00, 2e, 16, 00, 00, <br>MPI_unpack_external: 0<br>recv data 000004d2 0000162e <br></span><br></div>Bad: --enable-heterogeneous<br><span style="font-family:monospace,monospace">send data 000004d2 0000162e <br>MPI_Pack_external: 0<br>buffer size: 8<br>Buffer contents<br>d2, 04, 00, 00, 2e, 16, 00, 00, <br>MPI_unpack_external: 0<br>recv data d2040000 2e160000 <br></span><br></div>kindest regards<br></div>Mike<br></div><div class="gmail_extra"><br><div class="gmail_quote">On 11 February 2016 at 19:31, Gilles Gouaillardet <span dir="ltr">&lt;<a href="mailto:gilles.gouaillardet@gmail.com" target="_blank">gilles.gouaillardet@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Michael,<div><br></div><div>I think it is worst than that ...</div><div><br></div><div>without --enable-heterogeneous, it seems the data is not correctly packed</div><div>(e.g. it is not converted to big endian), at least on a x86_64 arch.</div><div>unpack looks broken too, but pack followed by unpack does work.</div><div>that means if you are reading data correctly written in external32e format,</div><div>it will not be correctly unpacked.</div><div><br></div><div>with --enable-heterogeneous, it is only half broken</div><div>(I do not know yet whether pack or unpack is broken ...)</div><div>and pack followed by unpack does not work.</div><div><br></div><div>I will double check that tomorrow</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles<br><br>On Thursday, February 11, 2016, Michael Rezny &lt;<a href="mailto:michael.rezny@monash.edu" target="_blank">michael.rezny@monash.edu</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><div><div><div><div><div><div><div><div>Hi Ralph,<br></div>you are indeed correct. However, many of our users<br></div>have workstations such as me, with OpenMPI provided by installing a package.<br></div><div>So we don&#39;t know what has been configured.<br></div><div><br></div>Then we have failures, since, for instance, Ubuntu 14.04 by default appears to have been built<br></div>with heterogeneous support! The other (working) machine is a large HPC, and it seems OpenMPI was built<br></div><div>without heterogeneous support.<br></div><div><br></div>Currently we work around the problem for packing and unpacking by having a compiler switch<br></div>that will switch between calls to pack/unpack_external and pac/unpack.<br><br></div>It is only now we started to track down what the problem actually is.<br><br></div>kindest regards<br></div>Mike<br></div><div class="gmail_extra"><br><div class="gmail_quote">On 11 February 2016 at 15:54, Ralph Castain <span dir="ltr">&lt;<a>rhc@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style="word-wrap:break-word">Out of curiosity: if both systems are Intel, they why are you enabling hetero? You don’t need it in that scenario.<div><br></div><div>Admittedly, we do need to fix the bug - just trying to understand why you are configuring that way.</div><div><br></div><div><br><div><blockquote type="cite"><div>On Feb 10, 2016, at 8:46 PM, Michael Rezny &lt;<a>michael.rezny@monash.edu</a>&gt; wrote:</div><br><div><div dir="ltr"><div><div><div><div><div><div>Hi Gilles,<br></div>I can confirm that with a fresh download and build from source for OpenMPI 1.10.2<br></div><div>with --enable-heterogeneous<br></div>the unpacked ints are the wrong endian.<br><br></div>However, without --enable-heterogeneous, the unpacked ints are correct.<br><br></div>So, this problem still exists in heterogeneous builds with OpenMPI version 1.10.2.<br><br></div>kindest regards<br></div>Mike<br></div><div class="gmail_extra"><br><div class="gmail_quote">On 11 February 2016 at 14:48, Gilles Gouaillardet <span dir="ltr">&lt;<a>gilles.gouaillardet@gmail.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Michael,<div><br></div><div>does your two systems have the same endianness ?</div><div><br></div><div>do you know how openmpi was configure&#39;d on both systems ?</div><div>(is --enable-heterogeneous enabled or disabled on both systems ?)</div><div><br></div><div>fwiw, openmpi 1.6.5 is old now and no more maintained.</div><div>I strongly encourage you to use openmpi 1.10.2</div><div><br></div><div>Cheers,</div><div><br></div><div>Gilles</div><div><br>On Thursday, February 11, 2016, Michael Rezny &lt;<a>michael.rezny@monash.edu</a>&gt; wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir="ltr"><div><div><div><div><div><div>Hi,<br></div>I am running Ubuntu 14.04 LTS with OpenMPI 1.6.5 and gcc 4.8.4<br><br></div>On a single rank program which just packs and unpacks two ints using MPI_Pack_external and MPI_Unpack_external<br></div>the unpacked ints are in the wrong endian order.<br><br></div>However, on a HPC, (not Ubuntu), using OpenMPI 1.6.5 and gcc 4.8.4 the unpacked ints are correct.<br><br></div><div>Is it possible to get some assistance to track down what is going on?<br></div><div><br></div>Here is the output from the program:<br><br> <span style="font-family:monospace,monospace">~/tests/mpi/Pack test1<br>send data 000004d2 0000162e <br>MPI_Pack_external: 0<br>buffer size: 8<br>MPI_unpack_external: 0<br>recv data d2040000 2e160000 </span><br><br></div>And here is the source code:<br><br><span style="font-family:monospace,monospace">#include &lt;stdio.h&gt;<br>#include &lt;mpi.h&gt;<br><br>int main(int argc, char *argv[]) {<br>  int numRanks, myRank, error;<br><br>  int send_data[2] = {1234, 5678};<br>  int recv_data[2];<br><br>  MPI_Aint buffer_size = 1000;<br>  char buffer[buffer_size];<br><br>  MPI_Init(&amp;argc, &amp;argv);<br>  MPI_Comm_size(MPI_COMM_WORLD, &amp;numRanks);<br>  MPI_Comm_rank(MPI_COMM_WORLD, &amp;myRank);<br><br>  printf(&quot;send data %08x %08x \n&quot;, send_data[0], send_data[1]);<br><br>  MPI_Aint position = 0;<br>  error = MPI_Pack_external(&quot;external32&quot;, (void*) send_data, 2, MPI_INT,<br>          buffer, buffer_size, &amp;position);<br>  printf(&quot;MPI_Pack_external: %d\n&quot;, error);<br><br>  printf(&quot;buffer size: %d\n&quot;, (int) position);<br><br>  position = 0;<br>  error = MPI_Unpack_external(&quot;external32&quot;, buffer, buffer_size, &amp;position,<br>          recv_data, 2, MPI_INT);<br>  printf(&quot;MPI_unpack_external: %d\n&quot;, error);<br><br>  printf(&quot;recv data %08x %08x \n&quot;, recv_data[0], recv_data[1]);<br><br>  MPI_Finalize();<br><br>  return 0;<br>}<br></span><br><br></div>
</blockquote></div>
<br>_______________________________________________<br>
devel mailing list<br>
<a>devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/02/18573.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/02/18573.php</a><br></blockquote></div><br></div>
_______________________________________________<br>devel mailing list<br><a>devel@open-mpi.org</a><br>Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/02/18575.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/02/18575.php</a></div></blockquote></div><br></div></div><br>_______________________________________________<br>
devel mailing list<br>
<a>devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/02/18576.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/02/18576.php</a><br></blockquote></div><br></div>
</blockquote></div>
<br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2016/02/18579.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2016/02/18579.php</a><br></blockquote></div><br></div>

