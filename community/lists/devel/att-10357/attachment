<font size=3>&nbsp;</font>
<br>
<br><tt><font size=2>devel-bounces@open-mpi.org wrote on 02/09/2012 12:20:41
PM:<br>
<br>
&gt; De : Brice Goglin &lt;Brice.Goglin@inria.fr&gt;</font></tt>
<br><tt><font size=2>&gt; A : Open MPI Developers &lt;devel@open-mpi.org&gt;</font></tt>
<br><tt><font size=2>&gt; Date : 02/09/2012 12:20 PM</font></tt>
<br><tt><font size=2>&gt; Objet : Re: [OMPI devel] btl/openib: get_ib_dev_distance
doesn't see<br>
&gt; processes as bound if the job has been launched by srun</font></tt>
<br><tt><font size=2>&gt; Envoyé par : devel-bounces@open-mpi.org</font></tt>
<br><tt><font size=2>&gt; <br>
&gt; By default, hwloc only shows what's inside the current cpuset. There's<br>
&gt; an option to show everything instead (topology flag).</font></tt>
<br>
<br><tt><font size=2>So may be using that flag inside opal_paffinity_base_get_processor_info()
would be a better fix than the one I'm proposing in my patch.</font></tt>
<br>
<br><tt><font size=2>I found a bunch of other places where things are managed
as in get_ib_dev_distance().</font></tt>
<br>
<br><tt><font size=2>Just doing a grep in the sources, I could find:</font></tt>
<br><tt><font size=2>&nbsp; . init_maffinity() in btl/sm/btl_sm.c</font></tt>
<br><tt><font size=2>&nbsp; . vader_init_maffinity() in btl/vader/btl_vader.c</font></tt>
<br><tt><font size=2>&nbsp; . get_ib_dev_distance() in btl/wv/btl_wv_component.c</font></tt>
<br>
<br><tt><font size=2>So I think the flag Brice is talking about should
definitely be the fix.</font></tt>
<br>
<br><tt><font size=2>Regards,</font></tt>
<br><tt><font size=2>Nadia</font></tt>
<br><tt><font size=2><br>
&gt; <br>
&gt; Brice<br>
&gt; <br>
&gt; <br>
&gt; <br>
&gt; Le 09/02/2012 12:18, Jeff Squyres a écrit :<br>
&gt; &gt; Just so that I understand this better -- if a process is bound
in <br>
&gt; a cpuset, will tools like hwloc's lstopo only show the Linux <br>
&gt; processors *in that cpuset*? &nbsp;I.e., does it not have any visibility
<br>
&gt; of the processors outside of its cpuset?<br>
&gt; &gt;<br>
&gt; &gt;<br>
&gt; &gt; On Jan 27, 2012, at 11:38 AM, nadia.derbey wrote:<br>
&gt; &gt;<br>
&gt; &gt;&gt; Hi,<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; If a job is launched using &quot;srun --resv-ports --cpu_bind:...&quot;
and slurm<br>
&gt; &gt;&gt; is configured with:<br>
&gt; &gt;&gt; &nbsp; TaskPlugin=task/affinity<br>
&gt; &gt;&gt; &nbsp; TaskPluginParam=Cpusets<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; each rank of that job is in a cpuset that contains a single
CPU.<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; Now, if we use carto on top of this, the following happens
in<br>
&gt; &gt;&gt; get_ib_dev_distance() (in btl/openib/btl_openib_component.c):<br>
&gt; &gt;&gt; &nbsp; . opal_paffinity_base_get_processor_info() is called
to get the<br>
&gt; &gt;&gt; &nbsp; &nbsp; number of logical processors (we get 1 due
to the singleton cpuset)<br>
&gt; &gt;&gt; &nbsp; . we loop over that # of processors to check whether
our process is<br>
&gt; &gt;&gt; &nbsp; &nbsp; bound to one of them. In our case the loop
will be executed only<br>
&gt; &gt;&gt; &nbsp; &nbsp; once and we will never get the correct binding
information.<br>
&gt; &gt;&gt; &nbsp; . if the process is bound actually get the distance
to the device.<br>
&gt; &gt;&gt; &nbsp; &nbsp; in our case we won't execute that part of the
code.<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; The attached patch is a proposal to fix the issue.<br>
&gt; &gt;&gt;<br>
&gt; &gt;&gt; Regards,<br>
&gt; &gt;&gt; Nadia<br>
&gt; &gt;&gt; &lt;get_ib_dev_distance.patch&gt;_______________________________________________<br>
&gt; &gt;&gt; devel mailing list<br>
&gt; &gt;&gt; devel@open-mpi.org<br>
&gt; &gt;&gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/devel</font></tt></a><tt><font size=2><br>
&gt; &gt;<br>
&gt; <br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; devel@open-mpi.org<br>
&gt; </font></tt><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel"><tt><font size=2>http://www.open-mpi.org/mailman/listinfo.cgi/devel</font></tt></a><tt><font size=2><br>
</font></tt>
