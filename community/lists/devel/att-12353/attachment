<div dir="ltr">Jeff,<div><br></div><div>I know Intel MPI (MPICH based) &quot;just works&quot; with Phi, but you need to do things like:</div><div>   mpirun –n 2 –host cpu host.exe : –n 4 –host mic0 mic.exe</div><div>if you want to use the Phi for more than just kernel-offload (in which case they won&#39;t have/need an MPI rank).</div>

<div>So, launch procs is PART of the problem, but certainty not all of it.</div><div><br></div><div style>At least, unlike RR, the processing elements all share the same endianness!</div><div><br></div><div>-Paul</div></div>
<div class="gmail_extra"><br><br><div class="gmail_quote">On Thu, May 2, 2013 at 6:28 PM, Jeff Squyres (jsquyres) <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span> wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">I know the MPICH guys did a bunch of work to support the Phi&#39;s.  I don&#39;t know exactly what that means (I haven&#39;t read their docs about this stuff), but I suspect that it&#39;s more than just launching MPI processes on them...<br>

<br>
<br>
On May 2, 2013, at 8:54 PM, Paul Hargrove &lt;<a href="mailto:phhargrove@lbl.gov">phhargrove@lbl.gov</a>&gt; wrote:<br>
<br>
&gt; Ralph,<br>
&gt;<br>
&gt; I am not an expert, by any means, but based on a presentation I heard 4 hours ago:<br>
&gt;<br>
&gt; The Xeon and Phi instruction sets have a large intersection, but neither is a subset of the other.<br>
&gt; In particular, Phi has its own SIMD instructions *instead* of Xeon&#39;s MMX, SSEn, etc.<br>
&gt; There is also on CMPXCHG16B instruction on Phi, among others.<br>
&gt; So, there will need to be different binaries, or &quot;fat&quot; binaries that branch based on CPU type.<br>
&gt;<br>
&gt; -Paul<br>
&gt;<br>
&gt;<br>
&gt; On Thu, May 2, 2013 at 5:47 PM, Ralph Castain &lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt; wrote:<br>
&gt;<br>
&gt; On May 2, 2013, at 5:12 PM, Christopher Samuel &lt;<a href="mailto:samuel@unimelb.edu.au">samuel@unimelb.edu.au</a>&gt; wrote:<br>
&gt;<br>
&gt; &gt; -----BEGIN PGP SIGNED MESSAGE-----<br>
&gt; &gt; Hash: SHA1<br>
&gt; &gt;<br>
&gt; &gt; Hi folks,<br>
&gt; &gt;<br>
&gt; &gt; The new system we&#39;re bringing up has 10 nodes with dual Xeon Phi MIC<br>
&gt; &gt; cards, are there any plans to support them by launching MPI tasks<br>
&gt; &gt; directly on the Phis themselves (rather than just as offload devices<br>
&gt; &gt; for code on the hosts)?<br>
&gt;<br>
&gt; We had something similar at one time - I developed it for the Roadrunner cluster so you could run MPI tasks on the GPUs. Worked well, but eventually fell into disrepair due to lack of use.<br>
&gt;<br>
&gt; In this case, I suspect it will be much easier to do as the Phis appear to be a lot more visible to the host than the GPU did on RR. Looking at the documentation, the Phis just sit directly on the PCIe bus, so they should look just like any other processor, and they are Xeon binary compatible - so there is no issue with tracking which binary to run on which processor.<br>

&gt;<br>
&gt; Brice: do the Phis appear in the hwloc topology object?<br>
&gt;<br>
&gt; Chris: can you run lstopo on one of the nodes and send me the output (off-list)?<br>
&gt;<br>
&gt;<br>
&gt; &gt;<br>
&gt; &gt; All the best,<br>
&gt; &gt; Chris<br>
&gt; &gt; - --<br>
&gt; &gt; Christopher Samuel        Senior Systems Administrator<br>
&gt; &gt; VLSCI - Victorian Life Sciences Computation Initiative<br>
&gt; &gt; Email: <a href="mailto:samuel@unimelb.edu.au">samuel@unimelb.edu.au</a> Phone: <a href="tel:%2B61%20%280%293%20903%2055545" value="+61390355545">+61 (0)3 903 55545</a><br>
&gt; &gt; <a href="http://www.vlsci.org.au/" target="_blank">http://www.vlsci.org.au/</a>      <a href="http://twitter.com/vlsci" target="_blank">http://twitter.com/vlsci</a><br>
&gt; &gt;<br>
&gt; &gt; -----BEGIN PGP SIGNATURE-----<br>
&gt; &gt; Version: GnuPG v1.4.11 (GNU/Linux)<br>
&gt; &gt; Comment: Using GnuPG with Thunderbird - <a href="http://www.enigmail.net/" target="_blank">http://www.enigmail.net/</a><br>
&gt; &gt;<br>
&gt; &gt; iEYEARECAAYFAlGDAPYACgkQO2KABBYQAh+y9ACfZ0SdqDuV7Euq3B0ANtxPhH1D<br>
&gt; &gt; 3h4An1Zlhu2Ut+OFvbTa9xbLBkspwwPY<br>
&gt; &gt; =TbIy<br>
&gt; &gt; -----END PGP SIGNATURE-----<br>
&gt; &gt; _______________________________________________<br>
&gt; &gt; devel mailing list<br>
&gt; &gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; &gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
&gt;<br>
&gt;<br>
<span class="HOEnZb"><font color="#888888">&gt;<br>
&gt; --<br>
&gt; Paul H. Hargrove                          <a href="mailto:PHHargrove@lbl.gov">PHHargrove@lbl.gov</a><br>
&gt; Future Technologies Group<br>
&gt; Computer and Data Sciences Department     Tel: <a href="tel:%2B1-510-495-2352" value="+15104952352">+1-510-495-2352</a><br>
&gt; Lawrence Berkeley National Laboratory     Fax: <a href="tel:%2B1-510-486-6900" value="+15104866900">+1-510-486-6900</a><br>
&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
<br>
<br>
--<br>
Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a><br>
For corporate legal information go to: <a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</font></span></blockquote></div><br><br clear="all"><div><br></div>-- <br><font face="courier new, monospace"><div>Paul H. Hargrove                          <a href="mailto:PHHargrove@lbl.gov" target="_blank">PHHargrove@lbl.gov</a></div>
<div>Future Technologies Group</div><div>Computer and Data Sciences Department     Tel: +1-510-495-2352</div><div>Lawrence Berkeley National Laboratory     Fax: +1-510-486-6900</div></font>
</div>

