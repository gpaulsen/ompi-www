<br><br><div class="gmail_quote">On Thu, May 14, 2009 at 10:47 AM, Terry Dontje <span dir="ltr">&lt;<a href="mailto:Terry.Dontje@sun.com">Terry.Dontje@sun.com</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<div class="im">Ralph Castain wrote:<br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Hi folks<br>
<br>
I encourage people to please look at your MTT outputs. As we are preparing to roll the 1.3.3 release, I am seeing a lot of problems on the branch:<br>
<br>
1. timeouts, coming in two forms: (a) MPI_Abort hanging, and (b) collectives hanging (this is mostly on Solaris)<br>
<br>
</blockquote></div>
Can you clarify or send me a link that makes you believe b is mostly solaris.  Looking at last night&#39;s Sun&#39;s MTT 1.3 nightly runs I see 47 timeouts on Linux and 24 timeouts on Solaris.  That doesn&#39;t constitute mostly Solaris to me.  Also how are you determining these timeouts are Collective based?  I have a theory they are but I don&#39;t have a clear smoking gun as of yet.</blockquote>
<div><br>I looked at this MTT report, which showed it hanging in a whole bunch of collective tests:<br><br><a href="http://www.open-mpi.org/mtt/index.php?limit=&amp;wrap=&amp;trial=&amp;enable_drilldowns=&amp;yaxis_scale=&amp;xaxis_scale=&amp;hide_subtitle=&amp;split_graphs=&amp;remote_go=&amp;do_cookies=&amp;phase=test_run&amp;text_start_timestamp=2009-05-13+15%3A15%3A25+-+2009-05-14+15%3A15%3A25&amp;text_platform_hardware=">http://www.open-mpi.org/mtt/index.php?limit=&amp;wrap=&amp;trial=&amp;enable_drilldowns=&amp;yaxis_scale=&amp;xaxis_scale=&amp;hide_subtitle=&amp;split_graphs=&amp;remote_go=&amp;do_cookies=&amp;phase=test_run&amp;text_start_timestamp=2009-05-13+15%3A15%3A25+-+2009-05-14+15%3A15%3A25&amp;text_platform_hardware=</a>^x86_64%24&amp;show_platform_hardware=show&amp;text_os_name=^Linux%24&amp;show_os_name=show&amp;text_mpi_name=^ompi-nightly-v1.3%24&amp;show_mpi_name=show&amp;text_mpi_version=^1.3.3a1r21173%24&amp;show_mpi_version=show&amp;text_suite_name=all&amp;show_suite_name=show&amp;text_test_name=all&amp;show_test_name=hide&amp;text_np=all&amp;show_np=show&amp;text_full_command=&amp;show_full_command=show&amp;text_http_username=^sun%24&amp;show_http_username=show&amp;text_local_username=all&amp;show_local_username=hide&amp;text_platform_name=^burl-ct-v20z-10%24&amp;show_platform_name=show&amp;click=Detail&amp;phase=test_run&amp;test_result=_rt&amp;text_os_version=&amp;show_os_version=&amp;text_platform_type=&amp;show_platform_type=&amp;text_hostname=&amp;show_hostname=&amp;text_compiler_name=&amp;show_compiler_name=&amp;text_compiler_version=&amp;show_compiler_version=&amp;text_vpath_mode=&amp;show_vpath_mode=&amp;text_endian=&amp;show_endian=&amp;text_bitness=&amp;show_bitness=&amp;text_configure_arguments=&amp;text_exit_value=&amp;show_exit_value=&amp;text_exit_signal=&amp;show_exit_signal=&amp;text_duration=&amp;show_duration=&amp;text_client_serial=&amp;show_client_serial=&amp;text_result_message=&amp;text_result_stdout=&amp;text_result_stderr=&amp;text_environment=&amp;text_description=&amp;text_launcher=&amp;show_launcher=&amp;text_resource_mgr=&amp;show_resource_mgr=&amp;text_network=&amp;show_network=&amp;text_parameters=&amp;show_parameters=&amp;lastgo=summary<br>
<br>When I look at the hangs on other systems, they are in non-collective tests. I&#39;m not sure what that really means, though - it was just an observation based on this one set of tests.<br> <br></div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
<br>
<br>
I&#39;ve been looking at some collective hangs and segv&#39;s.  These seem to happen across different platform and OS (Linux and Solaris).  I&#39;ve been finding it really hard to reproduce.  I ran MPI_Allreduce_loc_c on a three clusters for 2 days without a hang or segv.  I am really concerned whether we&#39;ll even be able to get this to fail with debugging on. <br>

I have not been able to get a core or time with a hung run in order to get more information. <br><div class="im">
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
2. segfaults - mostly on sif, but occasionally elsewhere<br>
<br>
3. daemon failed to report back - this was only on sif<br>
<br>
We will need to correct many of these for the release - unless it proves to be due to trivial errors, I don&#39;t see how we will be ready to roll release candidates next week.<br>
<br>
So let&#39;s please start taking a look at these?!<br>
<br>
</blockquote></div>
I&#39;ve actually been looking at ours though I have not been extremely vocal.  I was hoping to get more info on our timeouts before requesting help.</blockquote><div><br>No problem - I wasn&#39;t pointing a finger at anyone in particular. Just wanted to highlight that the branch is not in great shape since we had talked on the telecon about trying to do a release next week.<br>
<br><br></div><blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;"><br>
<blockquote class="gmail_quote" style="border-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-left: 1ex;">
Ralph<br>
<br>
------------------------------------------------------------------------<br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
  <br>
</blockquote>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</blockquote></div><br>

