<div dir="ltr">This seems to be the case only with the TKR interface. All the others are either calling the OMPI version directly (mpif-h), or are calling some other internal (or weak symbol function).<div><div><br></div><div>  George.</div><div><br></div></div></div><div class="gmail_extra"><br><div class="gmail_quote">On Tue, Aug 25, 2015 at 9:04 AM, Bert Wesarg <span dir="ltr">&lt;<a href="mailto:bert.wesarg@tu-dresden.de" target="_blank">bert.wesarg@tu-dresden.de</a>&gt;</span> wrote:<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><span class="">On 08/25/2015 02:44 PM, Gilles Gouaillardet wrote:<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
Folks,<br>
<br>
I ran some basic tests with IPM profiler-like<br>
<a href="https://github.com/nerscadmin/IPM" rel="noreferrer" target="_blank">https://github.com/nerscadmin/IPM</a> and found that when fortran calls an<br>
mpi subroutine, this is accounted twice.<br>
IPM defines both MPI_* subroutines and their fortran mpi_*_ counterpart.<br>
since the ompi fortran calls the MPI_* symbol (and not the PMPI_* one), and<br>
IPM does nothing to prevent double accounting, all subroutines are<br>
accounted twice<br>
<br>
what is the rationale for calling MPI_* from fortran instead of PMPI_* ?<br>
<br>
basically, I can see three options<br>
1. we do nothing, this is an IPM problem, not an Open MPI one<br>
2. we change ompi to call the PMPI_* symbols<br>
3. we add a configure option to call PMPI_* symbols instead of the MPI_*<br>
ones<br>
<br>
any thoughts ?<br>
</blockquote>
<br></span>
One more datapoint, also from a monitor tool (Score-P, as some of you know): The Open SHMEM part of Open MPI also calls the MPI interface, not the PMPI. That may result in performance data from MPI calls in SHMEM applications, which seems weird too.<br>
<br>
Bert<br>
<br>
<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
<br>
Cheers,<br>
<br>
Gilles<br>
<br>
<br><span class="HOEnZb"><font color="#888888">
</font></span></blockquote><span class="HOEnZb"><font color="#888888">
<br>
-- <br>
Dipl.-Inf. Bert Wesarg<br>
wiss. Mitarbeiter<br>
<br>
Technische Universität Dresden<br>
Zentrum für Informationsdienste und Hochleistungsrechnen (ZIH)<br>
01062 Dresden<br>
Tel.: <a href="tel:%2B49%20%28351%29%20463-42451" value="+4935146342451" target="_blank">+49 (351) 463-42451</a><br>
Fax: <a href="tel:%2B49%20%28351%29%20463-37773" value="+4935146337773" target="_blank">+49 (351) 463-37773</a><br>
E-Mail: <a href="mailto:bert.wesarg@tu-dresden.de" target="_blank">bert.wesarg@tu-dresden.de</a><br>
<br>
</font></span><br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
Subscription: <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
Link to this post: <a href="http://www.open-mpi.org/community/lists/devel/2015/08/17843.php" rel="noreferrer" target="_blank">http://www.open-mpi.org/community/lists/devel/2015/08/17843.php</a><br></blockquote></div><br></div>

