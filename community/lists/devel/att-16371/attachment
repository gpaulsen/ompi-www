<div dir="ltr"><br><div class="gmail_quote">Sorry, Jeff, missed your msg about sending it to the dev list.</div><div class="gmail_quote"><br></div><div class="gmail_quote">Background:</div><div class="gmail_quote">I wanted to be able to easily generate communicators based on locality of PU used in MPI.</div><div class="gmail_quote">My initial idea is to use MPI_Win_Create to create shared memory based on locality.</div><div class="gmail_quote">In my line of ideas I have a few arrays which are rarely needed, and when they are I need all information from all processors.</div><div class="gmail_quote">Instead of performing full AllGather I could use a shared memory base and skip the overhead of communication and only have overhead of memory locality. Ok, this might be too specific, but I wanted to test it to learn something about shared memory in MPI ;)</div><div class="gmail_quote"><br></div><div class="gmail_quote">This functionality is already existing in the hwloc base, it contains all the information that is needed.</div><div class="gmail_quote"><br></div><div class="gmail_quote">So I worked on the idea and got MPI to recognize a few more flags based on the locality provided by hwloc.</div><div class="gmail_quote">The function MPI_Comm_Split_Type already provides this type of splitting:</div><div class="gmail_quote">MPI_COMM_TYPE_SHARED</div><div class="gmail_quote">which pretty much does what I wanted.Â </div><div class="gmail_quote">But it fell short of the general scheme to all levels of control.</div><div class="gmail_quote"><br></div><div class="gmail_quote">So I added different communicator splittings based on these locality segments:</div><div class="gmail_quote">OMPI_COMM_TYPE_CU<br>OMPI_COMM_TYPE_HOST<br>OMPI_COMM_TYPE_BOARD<br>OMPI_COMM_TYPE_NODE // same as MPI_COMM_TYPE_SHARED<br>MPI_COMM_TYPE_SHARED // same as OMPI_COMM_TYPE_NODE<br>OMPI_COMM_TYPE_NUMA<br>OMPI_COMM_TYPE_SOCKET<br>OMPI_COMM_TYPE_L3CACHE<br>OMPI_COMM_TYPE_L2CACHE<br>OMPI_COMM_TYPE_L1CACHE<br>OMPI_COMM_TYPE_CORE<br>OMPI_COMM_TYPE_HWTHREAD<br></div><div class="gmail_quote"><br></div><div class="gmail_quote">My branch can be found at: <a href="https://github.com/zerothi/ompi" target="_blank">https://github.com/zerothi/ompi</a></div><div class="gmail_quote"><br></div><div class="gmail_quote">First a small &quot;bug&quot; report on the compilation:</div><div class="gmail_quote"><div><div class="h5">I had problems right after the <a href="http://autogen.pl" target="_blank">autogen.pl</a> script.</div></div><div dir="ltr"><div><div class="h5"><div>Procedure:</div><div>$&gt; git clone .. ompi</div><div>$&gt; cd ompi</div><div>$&gt; ./<a href="http://autogen.pl" target="_blank">autogen.pl</a></div><div>My build versions:</div><div>m4: 1.4.17</div><div>automake: 1.14</div><div>autoconf: 2.69</div><div>libtool: 2.4.3</div><div>the autogen completes successfully (attached is the autogen output if needed)</div><div>$&gt; mkdir build</div><div>$&gt; cd build</div><div>$&gt; ../configure --with-platform=optimized</div><div>I have attached the config.log (note that I have tested it with both the shipped 1.9.1 and 1.10.0 hwloc)</div><div>$&gt; make all</div><div>Error message is:</div><div>make[2]: Entering directory &#39;/home/nicpa/test/build/opal/libltdl&#39;<br>CDPATH=&quot;${ZSH_VERSION+.}:&quot; &amp;&amp; cd ../../../opal/libltdl &amp;&amp; /bin/bash /home/nicpa/test/config/missing aclocal-1.14 -I ../../config<br>aclocal-1.14: error: ../../config/autogen_found_items.m4:308: file &#39;opal/mca/backtrace/configure.m4&#39; does not exist<br></div><div>this error message is the same as found:</div><div><a href="http://www.open-mpi.org/community/lists/devel/2013/07/12504.php" target="_blank">http://www.open-mpi.org/community/lists/devel/2013/07/12504.php</a></div><div>My work-around is simple</div><div>It has to do with the created ACLOCAL_AMFLAGS variable</div><div>in build/opal/libltdl/Makefile</div><div>OLD:</div><div>ACLOCAL_AMFLAGS = -I ../../config<br></div><div>CORRECT:</div><div>ACLOCAL_AMFLAGS = -I ../../</div><div>Either the configure script creates the wrong include paths for the m4 scripts, or the m4 scripts are not copied fully to the config directory.</div><div>Ok, it works and the fix is simple. I just wonder why?</div><div><br></div><div><br></div></div></div><div><div class="h5"><div>First here is my test system 1:<br></div><div>$&gt; hwloc-info</div><div>depth 0:	1 Machine (type #1)<br> depth 1:	1 Socket (type #3)<br>  depth 2:	1 L3Cache (type #4)<br>   depth 3:	2 L2Cache (type #4)<br>    depth 4:	2 L1dCache (type #4)<br>     depth 5:	2 L1iCache (type #4)<br>      depth 6:	2 Core (type #5)<br>       depth 7:	4 PU (type #6)<br>Special depth -3:	2 Bridge (type #9)<br>Special depth -4:	4 PCI Device (type #10)<br>Special depth -5:	5 OS Device (type #11)<br></div><div><br></div><div>and my test system 2:</div><div>depth 0:	1 Machine (type #1)<br> depth 1:	1 Socket (type #3)<br>  depth 2:	1 L3Cache (type #4)<br>   depth 3:	4 L2Cache (type #4)<br>    depth 4:	4 L1dCache (type #4)<br>     depth 5:	4 L1iCache (type #4)<br>      depth 6:	4 Core (type #5)<br>       depth 7:	8 PU (type #6)<br>Special depth -3:	3 Bridge (type #9)<br>Special depth -4:	3 PCI Device (type #10)<br>Special depth -5:	4 OS Device (type #11)<br></div><div><br></div><div>Here is an excerpt of what it can do (I have attached a fortran program that creates a communicator using all types):</div><div><br></div></div></div><div>$&gt; mpirun -np 4 ./comm_split<br></div><div><div class="h5"><div>Example of MPI_Comm_Split_Type<br><br>Currently using 4 nodes.<br><br>Comm using CU Node:   2 local rank:   2 out of   4 ranks<br>Comm using CU Node:   3 local rank:   3 out of   4 ranks<br>Comm using CU Node:   1 local rank:   1 out of   4 ranks<br>Comm using CU Node:   0 local rank:   0 out of   4 ranks<br> <br>Comm using Host Node:   0 local rank:   0 out of   4 ranks<br>Comm using Host Node:   2 local rank:   2 out of   4 ranks<br>Comm using Host Node:   3 local rank:   3 out of   4 ranks<br>Comm using Host Node:   1 local rank:   1 out of   4 ranks<br> <br>Comm using Board Node:   2 local rank:   2 out of   4 ranks<br>Comm using Board Node:   3 local rank:   3 out of   4 ranks<br>Comm using Board Node:   1 local rank:   1 out of   4 ranks<br>Comm using Board Node:   0 local rank:   0 out of   4 ranks<br> <br>Comm using Node Node:   0 local rank:   0 out of   4 ranks<br>Comm using Node Node:   1 local rank:   1 out of   4 ranks<br>Comm using Node Node:   2 local rank:   2 out of   4 ranks<br>Comm using Node Node:   3 local rank:   3 out of   4 ranks<br> <br>Comm using Shared Node:   0 local rank:   0 out of   4 ranks<br>Comm using Shared Node:   3 local rank:   3 out of   4 ranks<br>Comm using Shared Node:   1 local rank:   1 out of   4 ranks<br>Comm using Shared Node:   2 local rank:   2 out of   4 ranks<br> <br>Comm using Numa Node:   0 local rank:   0 out of   1 ranks<br>Comm using Numa Node:   2 local rank:   0 out of   1 ranks<br>Comm using Numa Node:   3 local rank:   0 out of   1 ranks<br>Comm using Numa Node:   1 local rank:   0 out of   1 ranks<br> <br>Comm using Socket Node:   1 local rank:   0 out of   1 ranks<br>Comm using Socket Node:   2 local rank:   0 out of   1 ranks<br>Comm using Socket Node:   3 local rank:   0 out of   1 ranks<br>Comm using Socket Node:   0 local rank:   0 out of   1 ranks<br> <br>Comm using L3 Node:   0 local rank:   0 out of   1 ranks<br>Comm using L3 Node:   3 local rank:   0 out of   1 ranks<br>Comm using L3 Node:   1 local rank:   0 out of   1 ranks<br>Comm using L3 Node:   2 local rank:   0 out of   1 ranks<br> <br>Comm using L2 Node:   2 local rank:   0 out of   1 ranks<br>Comm using L2 Node:   3 local rank:   0 out of   1 ranks<br>Comm using L2 Node:   1 local rank:   0 out of   1 ranks<br>Comm using L2 Node:   0 local rank:   0 out of   1 ranks<br> <br>Comm using L1 Node:   0 local rank:   0 out of   1 ranks<br>Comm using L1 Node:   1 local rank:   0 out of   1 ranks<br>Comm using L1 Node:   2 local rank:   0 out of   1 ranks<br>Comm using L1 Node:   3 local rank:   0 out of   1 ranks<br> <br>Comm using Core Node:   0 local rank:   0 out of   1 ranks<br>Comm using Core Node:   3 local rank:   0 out of   1 ranks<br>Comm using Core Node:   1 local rank:   0 out of   1 ranks<br>Comm using Core Node:   2 local rank:   0 out of   1 ranks<br> <br>Comm using HW Node:   2 local rank:   0 out of   1 ranks<br>Comm using HW Node:   3 local rank:   0 out of   1 ranks<br>Comm using HW Node:   1 local rank:   0 out of   1 ranks<br>Comm using HW Node:   0 local rank:   0 out of   1 ranks<br></div><div><br></div></div></div><div>This is the output on both systems (note that I in the first system oversubscribe the node). I have not tested it on a cluster :(.<br></div><div><div class="h5"><div>One thing that worries me is that the SOCKET and L3 cache split types are not of size 4? I only have one socket, and one L3 cache, so they must be sharing?</div><div>I am not so sure about NUMA in this case. If you need any more information about my setup to debug this, please let me know.</div><div>Or am I completely missing something?</div><div><br></div><div>I tried looking into the opal/mca/hwloc/hwloc.h, but I have no idea whether they are related to the problem or not.</div><div><br></div><div>If you think, I can make a pull request at its current stage?</div><div><br></div><div>--</div><div>Kind regards Nick</div></div></div></div></div><div class="gmail_extra">
</div></div>

