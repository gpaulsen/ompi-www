I&#39;ve been comparing 1.5.4 and 1.5.5rc3 with the same parameters that&#39;s why I didn&#39;t use --bind-to-core. I checked and the usage of --bind-to-core improved the result comparing to 1.5.4:<div><div>#repetitions  t_min[usec]  t_max[usec]  t_avg[usec]</div>
<div>         1000        84.96        85.08        85.02</div><div><br></div><div>So I guess with 1.5.5 the processes move from core to core within node even though I use all cores, right? Then why 1.5.4 behaves differently?</div>
<div><br></div><div>I need --bind-to-core in some cases and that&#39;s why I need 1.5.5rc3 instead of more stable 1.5.4. I know that I can use numactl explicitly but --bind-to-core is more convinient :)<br><br><div class="gmail_quote">
2012/3/23 Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;</span><br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">I don&#39;t see where you told OMPI to --bind-to-core. We don&#39;t automatically bind, so you have to explicitly tell us to do so.<br>

<div><div class="h5"><br>
On Mar 23, 2012, at 6:20 AM, Pavel Mezentsev wrote:<br>
<br>
&gt; Hello<br>
&gt;<br>
&gt; I&#39;m doing some testing with IMB and dicovered a strange thing:<br>
&gt;<br>
&gt; Since I have a system with new AMD opteron 6276 processors I&#39;m using 1.5.5rc3 since it supports binding to cores.<br>
&gt;<br>
&gt; But when I run the barrier test form intel mpi benchmarks, the best I get is:<br>
&gt; #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]<br>
&gt;           598     15159.56     15211.05     15184.70<br>
&gt;  (/opt/openmpi-1.5.5rc3/intel12/bin/mpirun -x OMP_NUM_THREADS=1  -hostfile hosts_all2all_2 -npernode 32 --mca btl openib,sm,self -mca coll_tuned_use_dynamic_rules 1 -mca coll_tuned_barrier_algorithm 1 -np 256 openmpi-1.5.5rc3/intel12/IMB-MPI1 -off_cache 16,64 -msglog 1:16 -npmin 256 barrier)<br>

&gt;<br>
&gt; And with openmpi 1.5.4 the result is much better:<br>
&gt; #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]<br>
&gt;          1000       113.23       113.33       113.28<br>
&gt;<br>
&gt; (/opt/openmpi-1.5.4/intel12/bin/mpirun -x OMP_NUM_THREADS=1  -hostfile hosts_all2all_2 -npernode 32 --mca btl openib,sm,self -mca coll_tuned_use_dynamic_rules 1 -mca coll_tuned_barrier_algorithm 3 -np 256 openmpi-1.5.4/intel12/IMB-MPI1 -off_cache 16,64 -msglog 1:16 -npmin 256 barrier)<br>

&gt;<br>
&gt; and still I couldn&#39;t come close to the result I got with mvapich:<br>
&gt; #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]<br>
&gt;          1000        17.51        17.53        17.53<br>
&gt;<br>
&gt; (/opt/mvapich2-1.8/intel12/bin/mpiexec.hydra -env OMP_NUM_THREADS 1 -hostfile hosts_all2all_2 -np 256 mvapich2-1.8/intel12/IMB-MPI1 -mem 2 -off_cache 16,64 -msglog 1:16 -npmin 256 barrier)<br>
&gt;<br>
&gt; I dunno if this is a bug or me doing something not the way I should. So is there a way to improve my results?<br>
&gt;<br>
&gt; Best regards,<br>
&gt; Pavel Mezentsev<br>
&gt;<br>
&gt;<br>
</div></div>&gt; _______________________________________________<br>
&gt; devel mailing list<br>
&gt; <a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
&gt; <a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
<br>
<br>
_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br>
</blockquote></div><br></div></div>

