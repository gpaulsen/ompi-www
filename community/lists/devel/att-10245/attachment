<html><head></head><body style="word-wrap: break-word; -webkit-nbsp-mode: space; -webkit-line-break: after-white-space; "><div>Ok, thank you Ken and Rolf. I will have a look into the 4.1 version.</div><div><br></div><div>@ Rolf:</div><div>I actually meant MVAPICH2, since Open MPI requires to have&nbsp;CUDA_NIC_INTEROP=1 set.</div><div>However, setting the environment variable does not show any changes in the files previously mentioned.</div><div><br></div><div>Nevertheless, you already answered my question. Thanks.</div><blockquote type="cite"><div><blockquote type="CITE" style="position: static; z-index: auto; "></blockquote></div></blockquote><div><br></div><div>Sebastian.&nbsp;</div><br><div><div>On Jan 21, 2012, at 4:03 PM, Kenneth Lloyd wrote:</div><br class="Apple-interchange-newline"><blockquote type="cite">


  <meta http-equiv="Content-Type" content="text/html; CHARSET=UTF-8">
  <meta name="GENERATOR" content="GtkHTML/3.28.3">

<div>
Sebastian,<br>
<br>
If possible, I strongly suggest you look into CUDA 4.1 r2 and using Rolf vandeVaart's MPI CUDA RDMA 3).&nbsp; Your life will be MUCH easier.<br>
<br>
Having used GPUDirect1 in the last half of 2010, I can say it is a pain for the 9 - 14% gain in efficiency we saw.<br>
<br>
Ken<br>
<br>
On Fri, 2012-01-20 at 18:20 +0100, Sebastian Rinke wrote:
<blockquote type="CITE">
    With&nbsp;
</blockquote>
<blockquote type="CITE">
    <br>
    <br>
</blockquote>
<blockquote type="CITE">
    * MLNX OFED stack tailored for GPUDirect
</blockquote>
<blockquote type="CITE">
    * RHEL + kernel patch&nbsp;
</blockquote>
<blockquote type="CITE">
    * MVAPICH2&nbsp;
</blockquote>
<blockquote type="CITE">
    <br>
    <br>
</blockquote>
<blockquote type="CITE" style="position: static; z-index: auto; ">
    it is possible to monitor GPUDirect v1 activities by means of observing changes to values in
</blockquote>
<blockquote type="CITE">
    <br>
    <br>
</blockquote>
<blockquote type="CITE" style="position: static; z-index: auto; ">
    * /sys/module/ib_core/parameters/gpu_direct_pages
</blockquote>
<blockquote type="CITE" style="position: static; z-index: auto; ">
    * /sys/module/ib_core/parameters/gpu_direct_shares
</blockquote>
<blockquote type="CITE">
    <br>
    <br>
</blockquote>
<blockquote type="CITE" style="position: static; z-index: auto; ">
    By setting CUDA_NIC_INTEROP=1 there are no changes anymore.
</blockquote>
<blockquote type="CITE">
    <br>
    <br>
</blockquote>
<blockquote type="CITE">
    Is there a different way now to monitor if GPUDirect actually works?
</blockquote>
<blockquote type="CITE">
    <br>
    <br>
</blockquote>
<blockquote type="CITE">
    Sebastian.
</blockquote>
<blockquote type="CITE">
    <br>
</blockquote>
<blockquote type="CITE">
    On Jan 18, 2012, at 5:06 PM, Kenneth Lloyd wrote:
</blockquote>
<blockquote type="CITE">
    <br>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        It is documented in&nbsp;<font color="#0000ff"><a href="http://developer.download.nvidia.com/compute/cuda/4_0/docs/GPUDirect_Technology_Overview.pdf">http://developer.download.nvidia.com/compute/cuda/4_0/docs/GPUDirect_Technology_Overview.pdf</a></font>
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        <font color="#000000">set CUDA_NIC_INTEROP=1</font>
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        &nbsp;
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        &nbsp;
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        <b>From:</b>&nbsp;<font color="#0000ff"><a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a></font>&nbsp;[mailto:devel-bounces@open-mpi.org]&nbsp;<b>On Behalf Of&nbsp;</b>Sebastian Rinke<br>
        <b>Sent:</b>&nbsp;Wednesday, January 18, 2012 8:15 AM<br>
        <b>To:</b>&nbsp;Open MPI Developers<br>
        <b>Subject:</b>&nbsp;Re: [OMPI devel] GPUDirect v1 issues
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        &nbsp;
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        Setting the environment variable fixed the problem for Open MPI with CUDA 4.0. Thanks!
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        &nbsp;
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        However, I'm wondering why this is not documented in the NVIDIA GPUDirect package.
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        &nbsp;
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        Sebastian.
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        &nbsp;
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        On Jan 18, 2012, at 1:28 AM, Rolf vandeVaart wrote:
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        <br>
        <br>
        <br>
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        Yes, the step outlined in your second bullet is no longer necessary.&nbsp;
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        &nbsp;
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        Rolf
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        &nbsp;
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        &nbsp;
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        <b>From:</b>&nbsp;<font color="#0000ff"><a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a></font>&nbsp;[<font color="#0000ff"><a href="mailto:devel-bounces@open-mpi.org">mailto:devel-bounces@open-mpi.org</a></font>]&nbsp;<b>On Behalf Of&nbsp;</b>Sebastian Rinke<br>
        <b>Sent:</b>&nbsp;Tuesday, January 17, 2012 5:22 PM<br>
        <b>To:</b>&nbsp;Open MPI Developers<br>
        <b>Subject:</b>&nbsp;Re: [OMPI devel] GPUDirect v1 issues
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        <font color="#000000">&nbsp;</font>
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        <font color="#000000">Thank you very much. I will try setting the environment variable and if required also use the 4.1 RC2 version.</font><br>
        <br>
        <font color="#000000">To clarify things a little bit for me, to set up my machine with GPUDirect v1 I did the following:</font><br>
        <br>
        <font color="#000000">* Install RHEL 5.4</font><br>
        <font color="#000000">* Use the kernel with GPUDirect support</font><br>
        <font color="#000000">* Use the MLNX OFED stack with GPUDirect support</font><br>
        <font color="#000000">* Install the CUDA developer driver</font><br>
        <br>
        <font color="#000000">Does using CUDA&nbsp; &gt;= 4.0&nbsp; make one of the above steps&nbsp; redundant?</font><br>
        <br>
        <font color="#000000">I.e., RHEL or different kernel or MLNX OFED stack with GPUDirect support is&nbsp; not needed any more?</font><br>
        <br>
        <font color="#000000">Sebastian.</font><br>
        <br>
        <font color="#000000">Rolf vandeVaart wrote:</font>
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
<pre><font color="#000000">I ran your test case against Open MPI 1.4.2 and CUDA 4.1 RC2 and it worked fine.&nbsp; I do not have a machine right now where I can load CUDA 4.0 drivers.</font>
<font color="#000000">Any chance you can try CUDA 4.1 RC2?&nbsp; There were some improvements in the support (you do not need to set an environment variable for one)</font>
<font color="#0000ff"><a href="http://developer.nvidia.com/cuda-toolkit-41">http://developer.nvidia.com/cuda-toolkit-41</a></font>
<font color="#000000">&nbsp;</font>
<font color="#000000">There is also a chance that setting the environment variable as outlined in this link may help you.</font>
<font color="#0000ff"><a href="http://forums.nvidia.com/index.php?showtopic=200629">http://forums.nvidia.com/index.php?showtopic=200629</a></font>
<font color="#000000">&nbsp;</font>
<font color="#000000">However, I cannot explain why MVAPICH would work and Open MPI would not.&nbsp; </font>
<font color="#000000">&nbsp;</font>
<font color="#000000">Rolf</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">&nbsp; </font>
</pre>
        <blockquote>
<pre><font color="#000000">-----Original Message-----</font>
<font color="#000000">From: </font><font color="#0000ff"><a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a></font><font color="#000000"> [</font><font color="#0000ff"><a href="mailto:devel-bounces@open-mpi.org">mailto:devel-bounces@open-mpi.org</a></font><font color="#000000">]</font>
<font color="#000000">On Behalf Of Sebastian Rinke</font>
<font color="#000000">Sent: Tuesday, January 17, 2012 12:08 PM</font>
<font color="#000000">To: Open MPI Developers</font>
<font color="#000000">Subject: Re: [OMPI devel] GPUDirect v1 issues</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">I use CUDA 4.0 with MVAPICH2 1.5.1p1 and Open MPI 1.4.2.</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">Attached you find a little test case which is based on the GPUDirect v1 test</font>
<font color="#000000">case (mpi_pinned.c).</font>
<font color="#000000">In that program the sender splits a message into chunks and sends them</font>
<font color="#000000">separately to the receiver which posts the corresponding recvs. It is a kind of</font>
<font color="#000000">pipelining.</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">In mpi_pinned.c:141 the offsets into the recv buffer are set.</font>
<font color="#000000">For the correct offsets, i.e. increasing them, it blocks with Open MPI.</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">Using line 142 instead (offset = 0) works.</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">The tarball attached contains a Makefile where you will have to adjust</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">* CUDA_INC_DIR</font>
<font color="#000000">* CUDA_LIB_DIR</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">Sebastian</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">On Jan 17, 2012, at 4:16 PM, Kenneth A. Lloyd wrote:</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">&nbsp;&nbsp;&nbsp;</font>
</pre>
            <blockquote>
<pre><font color="#000000">Also, which version of MVAPICH2 did you use?</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">I've been pouring over Rolf's OpenMPI CUDA RDMA 3 (using CUDA 4.1 r2)</font>
<font color="#000000">vis MVAPICH-GPU on a small 3 node cluster. These are wickedly interesting.</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">Ken</font>
<font color="#000000">-----Original Message-----</font>
<font color="#000000">From: </font><font color="#0000ff"><a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a></font><font color="#000000"> [</font><font color="#0000ff"><a href="mailto:devel-bounces@open">mailto:devel-bounces@open</a></font><font color="#000000">-</font>
<font color="#000000">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </font>
</pre>
            </blockquote>
<pre><font color="#000000">mpi.org]</font>
<font color="#000000">&nbsp;&nbsp;&nbsp; </font>
</pre>
            <blockquote>
<pre><font color="#000000">On Behalf Of Rolf vandeVaart</font>
<font color="#000000">Sent: Tuesday, January 17, 2012 7:54 AM</font>
<font color="#000000">To: Open MPI Developers</font>
<font color="#000000">Subject: Re: [OMPI devel] GPUDirect v1 issues</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">I am not aware of any issues.&nbsp; Can you send me a test program and I</font>
<font color="#000000">can try it out?</font>
<font color="#000000">Which version of CUDA are you using?</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">Rolf</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </font>
</pre>
                <blockquote>
<pre><font color="#000000">-----Original Message-----</font>
<font color="#000000">From: </font><font color="#0000ff"><a href="mailto:devel-bounces@open-mpi.org">devel-bounces@open-mpi.org</a></font><font color="#000000"> [</font><font color="#0000ff"><a href="mailto:devel-bounces@open">mailto:devel-bounces@open</a></font><font color="#000000">-</font>
<font color="#000000">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </font>
</pre>
                </blockquote>
            </blockquote>
<pre><font color="#000000">mpi.org]</font>
<font color="#000000">&nbsp;&nbsp;&nbsp; </font>
</pre>
            <blockquote>
                <blockquote>
<pre><font color="#000000">On Behalf Of Sebastian Rinke</font>
<font color="#000000">Sent: Tuesday, January 17, 2012 8:50 AM</font>
<font color="#000000">To: Open MPI Developers</font>
<font color="#000000">Subject: [OMPI devel] GPUDirect v1 issues</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">Dear all,</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">I'm using GPUDirect v1 with Open MPI 1.4.3 and experience blocking</font>
<font color="#000000">MPI_SEND/RECV to block forever.</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">For two subsequent MPI_RECV, it hangs if the recv buffer pointer of</font>
<font color="#000000">the second recv points to somewhere, i.e. not at the beginning, in</font>
<font color="#000000">the recv buffer (previously allocated with cudaMallocHost()).</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">I tried the same with MVAPICH2 and did not see the problem.</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">Does anybody know about issues with GPUDirect v1 using Open MPI?</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">Thanks for your help,</font>
<font color="#000000">Sebastian</font>
<font color="#000000">_______________________________________________</font>
<font color="#000000">devel mailing list</font>
<font color="#0000ff"><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a></font>
<font color="#0000ff"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></font>
<font color="#000000">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </font>
</pre>
                </blockquote>
            </blockquote>
        </blockquote>
<pre><font color="#000000">-----------------------------------------------------------------------------------</font>
<font color="#000000">This email message is for the sole use of the intended recipient(s) and may contain</font>
<font color="#000000">confidential information.&nbsp; Any unauthorized review, use, disclosure or distribution</font>
<font color="#000000">is prohibited.&nbsp; If you are not the intended recipient, please contact the sender by</font>
<font color="#000000">reply email and destroy all copies of the original message.</font>
<font color="#000000">-----------------------------------------------------------------------------------</font>
<font color="#000000">&nbsp;</font>
<font color="#000000">_______________________________________________</font>
<font color="#000000">devel mailing list</font>
<font color="#0000ff"><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a></font>
<font color="#0000ff"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></font>
<font color="#000000">&nbsp; </font>
</pre>
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        <font color="#000000">&nbsp;</font>
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        _______________________________________________<br>
        devel mailing list<br>
        <font color="#0000ff"><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a></font><br>
        <font color="#0000ff"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></font>
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        &nbsp;
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <blockquote type="CITE">
        _______________________________________________<br>
        devel mailing list<br>
        <font color="#0000ff"><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a></font><br>
        <font color="#0000ff"><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></font>
    </blockquote>
</blockquote>
<blockquote type="CITE">
    <br>
<pre>_______________________________________________
devel mailing list
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a>
</pre>
</blockquote>
<br>
<table cellspacing="0" cellpadding="0" width="100%">
<tbody><tr>
<td>
==============<br>
<b>Kenneth A. Lloyd, Jr.</b><br>
CEO - Director of Systems Science<br>
Watt Systems Technologies Inc.<br>
Albuquerque, NM US<br>
<br>
<tt><font size="2"><font color="#737373">This e-mail is covered by the Electronic Communications Privacy Act, 18 U.S.C. 2510-2521 and is intended only for the addressee named above. It may contain privileged or confidential information. If you are not the addressee you must not copy, distribute, disclose or use any of the information in it. If you have received it in error please delete it and immediately notify the sender.</font></font></tt><br>
<br>
<br>
</td>
</tr>
</tbody></table>
</div>

_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>http://www.open-mpi.org/mailman/listinfo.cgi/devel</blockquote></div><br></body></html>

