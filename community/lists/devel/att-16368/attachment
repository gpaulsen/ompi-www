<html>
  <head>
    <meta content="text/html; charset=ISO-8859-1"
      http-equiv="Content-Type">
  </head>
  <body text="#000000" bgcolor="#FFFFFF">
    <div class="moz-cite-prefix">Ralph,<br>
      <br>
      let me correct and enhance my previous statement :<br>
      <br>
      - i cannot reproduce your crash in my environment (RHEL6 like vs
      your RHEL7 like)<br>
      (i configured with --enable-debug --enable-picky)<br>
      <br>
      - i can reproduce the crash with<br>
      mpirun --mca mpi_param_check false<br>
      <br>
      - if you configured with --without-mpi-param-check, i assume you
      would get the same crash<br>
      (and if i understand correctly, there would be no way to --mca
      mpi_param_check true)<br>
      <br>
      here is the relevant part of my config.status :<br>
      $ grep MPI_PARAM_CHECK config.status <br>
      D["MPI_PARAM_CHECK"]=" ompi_mpi_param_check"<br>
      D["OMPI_PARAM_CHECK"]=" 1"<br>
      <br>
      i will try on a centos7 box from now.<br>
      in the mean time, can you check you config.status and try again
      with <br>
      mpirun --mca mpi_param_check true<br>
      <br>
      Cheers,<br>
      <br>
      Gilles<br>
      <br>
      On 2014/11/27 10:06, Gilles Gouaillardet wrote:<br>
    </div>
    <blockquote
      cite="mid:4l0ednln3sn9845ijdtdp4ig.1417050385842@email.android.com"
      type="cite">
      <pre wrap="">I will double check this(afk right now)
Are you running on a rhel6 like distro with gcc ?

Iirc, crash vs mpi error is ruled by --with-param-check or something like this...

Cheers,

Gilles 

Ralph Castain <a class="moz-txt-link-rfc2396E" href="mailto:rhc@open-mpi.org">&lt;rhc@open-mpi.org&gt;</a>&#12373;&#12435;&#12398;&#12513;&#12540;&#12523;:
</pre>
      <blockquote type="cite">
        <pre wrap="">I tried it with both the fortran and c versions - got the same result.


This was indeed with a debug build. I wouldn&#8217;t expect a segfault even with an optimized build, though - I would expect an MPI error, yes?




On Nov 26, 2014, at 4:26 PM, Gilles Gouaillardet <a class="moz-txt-link-rfc2396E" href="mailto:gilles.gouaillardet@gmail.com">&lt;gilles.gouaillardet@gmail.com&gt;</a> wrote:


I will have a look

Btw, i was running the fortran version, not the c one.
Did you confgure with --enable--debug ?
The program sends to a rank *not* in the communicator, so this behavior could make some sense on an optimized build.

Cheers,

Gilles

Ralph Castain <a class="moz-txt-link-rfc2396E" href="mailto:rhc@open-mpi.org">&lt;rhc@open-mpi.org&gt;</a>&#12373;&#12435;&#12398;&#12513;&#12540;&#12523;:
Ick - I&#8217;m getting a segfault when trying to run that test:


MPITEST info&nbsp; (0): Starting MPI_Errhandler_fatal test

MPITEST info&nbsp; (0): This test will abort after printing the results message

MPITEST info&nbsp; (0): If it does not, then a f.a.i.l.u.r.e will be noted

[bend001:07714] *** Process received signal ***

[bend001:07714] Signal: Segmentation fault (11)

[bend001:07714] Signal code: Address not mapped (1)

[bend001:07714] Failing at address: 0x50

[bend001:07715] *** Process received signal ***

[bend001:07715] Signal: Segmentation fault (11)

[bend001:07715] Signal code: Address not mapped (1)

[bend001:07715] Failing at address: 0x50

[bend001:07714] ompi_comm_peer_lookup: invalid peer index (3)

[bend001:07713] ompi_comm_peer_lookup: invalid peer index (3)

[bend001:07715] ompi_comm_peer_lookup: invalid peer index (3)

[bend001:07713] *** Process received signal ***

[bend001:07713] Signal: Segmentation fault (11)

[bend001:07713] Signal code: Address not mapped (1)

[bend001:07713] Failing at address: 0x50

[bend001:07713] [ 0] /usr/lib64/libpthread.so.0(+0xf130)[0x7f4485ecb130]

[bend001:07713] [ 1] /home/common/openmpi/build/ompi-release/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_send+0x5d)[0x7f4480f74ca6]

[bend001:07713] [ 2] [bend001:07714] [ 0] /usr/lib64/libpthread.so.0(+0xf130)[0x7ff457885130]

[bend001:07714] [ 1] /home/common/openmpi/build/ompi-release/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_send+0x5d)[0x7ff44e8dbca6]

[bend001:07714] [ 2] [bend001:07715] [ 0] /usr/lib64/libpthread.so.0(+0xf130)[0x7ffa97ff6130]

[bend001:07715] [ 1] /home/common/openmpi/build/ompi-release/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_send+0x5d)[0x7ffa8eeeeca6]

[bend001:07715] [ 2] MPITEST_results: MPI_Errhandler_fatal all tests PASSED (3)



This is with the head of the 1.8 branch. Any suggestions?

Ralph



On Nov 26, 2014, at 8:46 AM, Ralph Castain <a class="moz-txt-link-rfc2396E" href="mailto:rhc@open-mpi.org">&lt;rhc@open-mpi.org&gt;</a> wrote:


Hmmm&#8230;.yeah, I know we saw this and resolved it in the trunk, but it looks like the fix indeed failed to come over to 1.8. I&#8217;ll take a gander (pretty sure I remember how I fixed it) - thanks!

On Nov 26, 2014, at 12:03 AM, Gilles Gouaillardet <a class="moz-txt-link-rfc2396E" href="mailto:gilles.gouaillardet@iferc.org">&lt;gilles.gouaillardet@iferc.org&gt;</a> wrote:

Ralph,

i noted several hangs in mtt with the v1.8 branch.

a simple way to reproduce it is to use the MPI_Errhandler_fatal_f test
</pre>
      </blockquote>
      <pre wrap="">&gt;from the intel_tests suite,
</pre>
      <blockquote type="cite">
        <pre wrap="">invoke mpirun on one node and run the taks on an other node :

node0$ mpirun -np 3 -host node1 --mca btl tcp,self ./MPI_Errhandler_fatal_f

/* since this is a race condition, you might need to run this in a loop
in order to hit the bug */

the attached tarball contains a patch (add debug + temporary hack) and
some log files obtained with
--mca errmgr_base_verbose 100 --mca odls_base_verbose 100

without the hack, i can reproduce the bug with -np 3 (log.ko.txt) , with
the hack, i can still reproduce the hang (though it might
be a different one) with -np 16 (log.ko.2.txt)

i remember some similar hangs were fixed on the trunk/master a few
monthes ago.
i tried to backport some commits but it did not help :-(

could you please have a look at this ?

Cheers,

Gilles
&lt;abort_hang.tar.gz&gt;_______________________________________________
devel mailing list
<a class="moz-txt-link-abbreviated" href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>
Subscription:&nbsp;<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a>
Link to this post:&nbsp;<a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/devel/2014/11/16357.php">http://www.open-mpi.org/community/lists/devel/2014/11/16357.php</a>


_______________________________________________
devel mailing list
<a class="moz-txt-link-abbreviated" href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/devel/2014/11/16364.php">http://www.open-mpi.org/community/lists/devel/2014/11/16364.php</a>


</pre>
        <br>
        <fieldset class="mimeAttachmentHeader"></fieldset>
        <br>
        <pre wrap="">_______________________________________________
devel mailing list
<a class="moz-txt-link-abbreviated" href="mailto:devel@open-mpi.org">devel@open-mpi.org</a>
Subscription: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/mailman/listinfo.cgi/devel">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a>
Link to this post: <a class="moz-txt-link-freetext" href="http://www.open-mpi.org/community/lists/devel/2014/11/16366.php">http://www.open-mpi.org/community/lists/devel/2014/11/16366.php</a></pre>
      </blockquote>
    </blockquote>
    <br>
  </body>
</html>

