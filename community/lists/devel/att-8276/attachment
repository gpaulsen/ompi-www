<span class="Apple-style-span" style="font-family: arial, sans-serif; font-size: 12.5px; border-collapse: collapse; "><span class="Apple-style-span" style="font-size: small;">&nbsp;Hi Ralph,</span><div><span class="Apple-style-span" style="font-size: small;"><br>
</span></div><div><span class="Apple-style-span" style="font-size: small;">I wonder if I disable the btl-sm component ,the performance of message passing will degrade.I need to test how much performance are lost. The code quality now is far from to be integrated into trunk, &nbsp;I will try to clean up my code before sending these changes to you </span>.</div>
</span><br><div class="gmail_quote">在 2010年8月13日 下午12:00，Ralph Castain <span dir="ltr">&lt;<a href="mailto:rhc@open-mpi.org">rhc@open-mpi.org</a>&gt;</span>写道：<br><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;">
<div style="word-wrap:break-word">This sounds like excellent progress! Jeff and others know much more about MTT than I do, so I&#39;ll leave that question to them.<div><br></div><div>You have two approaches to the mmap issue. Easiest for now would be to simply disable the shared memory component - you can either turn it off at run-time with -mca btl ^sm, or you can direct that it not even be built with -enable-mca-no-build=btl-sm when configuring OMPI.</div>
<div><br></div><div>I would think your TCP comm would then allow the two procs sharing a host to communicate. Can you give that a try?</div><div><br></div><div>I&#39;d be happy to begin reviewing the changes, and can help integrate them back into the OMPI trunk, when you feel ready.</div>
<div><div></div><div class="h5"><div><br></div><div><br><div><div>On Aug 12, 2010, at 9:35 PM, 张晶 wrote:</div><br><blockquote type="cite">Hi Ralph,Jeff and all<div><br></div><div>&nbsp;&nbsp; &nbsp; &nbsp; It is a good news that I can almost run the openmpi on the vxworks ,but there are also still some bugs.The final test which has passed is:</div>
<div>&nbsp;&nbsp; &nbsp; &nbsp;Rank 0 process &nbsp;calls mpi_send running on the host 0,rank 1 process calls mpi_recv running on the host 1. It works well .For the absence of &nbsp;the mmap in the vxworks &nbsp;,which is used in the btl sm component , it still fails running two processes in the same host.</div>

<div>&nbsp;&nbsp; &nbsp; &nbsp;The difference between the vxworks and unix is the real trouble .For example pipe(),fork(),exec(),socketpair(),fcntl() ,sshd &nbsp;and so no are not implemented in the vxworks .Replacing these lost with the correspond functions is the key work of the migration.After having a clear understanding of the function of rsh component ,I write a &nbsp;simple daemon and client to launch the orted for the calling of the rlogin() in the user space of the vxworks complain.&nbsp;</div>

<div>&nbsp;&nbsp; &nbsp; &nbsp;I think there are still many test needed to be launching .Maybe I&#39;d better to look into &nbsp;MTT.<br><br><div class="gmail_quote">在 2010年7月8日 上午9:54，张晶 <span dir="ltr">&lt;<a href="mailto:iam.chilli@gmail.com" target="_blank">iam.chilli@gmail.com</a>&gt;</span>写道：<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Thank you ,Squyres , it is really useful !<br><br><div class="gmail_quote">在 2010年7月7日 下午7:22，Jeff Squyres <span dir="ltr">&lt;<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a>&gt;</span>写道：<br>

<blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div></div><div>
<div>On Jul 6, 2010, at 10:48 PM, 张晶 wrote:<br>
<br>
&gt; 1.If I write a rlogin component ,<br>
<br>
</div>Is the command line of rlogin that much different than that of rsh/ssh? &nbsp;For example, can you just s/rsh/rlogin/ on the overall command line and have it just work?<br>
<br>
If so, I suspect that tweaking the rsh plm might be far simpler than having your own component.<br>
<div><br>
&gt; can I just login in the node in the cluster and launch the process . &nbsp;If it is ,what the role the odls plays ??<br>
<br>
</div>ODLS = ORTE Daemon Local launch Subsystem.<br>
<br>
PLM = Process Lifecycle Management.<br>
<br>
Meaning: the PLM is used to launch orteds (more on this below) across multiple nodes. &nbsp;The ODLS is used to launch processes locally from the orted (e.g., via POSIX fork/exec).<br>
<div><br>
&gt; 2.what is orted? Should the orted exists in every node and functions as a node process launch proxy ?<br>
<br>
</div>Yes. &nbsp;The orted = ORTE daemon. &nbsp;It is almost always the first thing launched on each node and acts as a proxy for launching, killing, and monitoring the user&#39;s applications on each node. &nbsp;It also does other control kinds of things, like relay stdout/stderr back up to the HNP (more below), etc.<br>



<div><br>
&gt; 3,what is hnp ? Is every job has only one hnp ,and when I use mpirun , the mpirun process is hnp ??<br>
<br>
</div>HNP = head node process, meaning mpirun (or actually, orterun -- mpirun is a symlink to orterun). &nbsp;The HNP functions as an orted as well, so it can use the ODLS to launch processes locally, etc.<br>
<br>
Ralph can provide more detail on all of the above, but these are the basics.<br>
<font color="#888888"><br>
--<br>
</font><div><div></div><div>Jeff Squyres<br>
<a href="mailto:jsquyres@cisco.com" target="_blank">jsquyres@cisco.com</a><br>
For corporate legal information go to:<br>
<a href="http://www.cisco.com/web/about/doing_business/legal/cri/" target="_blank">http://www.cisco.com/web/about/doing_business/legal/cri/</a><br>
<br>
<br>
</div></div><br></div></div><div>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br></div></blockquote></div><br><br clear="all"><br>-- <br><font color="#888888">张晶<br>
</font></blockquote></div><br><br clear="all"><br>-- <br>张晶<br>
</div>
_______________________________________________<br>devel mailing list<br><a href="mailto:devel@open-mpi.org" target="_blank">devel@open-mpi.org</a><br><a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a></blockquote>
</div><br></div></div></div></div><br>_______________________________________________<br>
devel mailing list<br>
<a href="mailto:devel@open-mpi.org">devel@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/devel" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/devel</a><br></blockquote></div><br>

