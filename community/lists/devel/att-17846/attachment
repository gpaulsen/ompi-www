<p dir="ltr">is this going in to v2.x?</p>
<p dir="ltr">----------</p>
<p dir="ltr">sent from my smart phonr so no good type.</p>
<p dir="ltr">Howard</p>
<div class="gmail_quote">On Aug 25, 2015 7:54 AM,  &lt;<a href="mailto:gitdub@crest.iu.edu">gitdub@crest.iu.edu</a>&gt; wrote:<br type="attribution"><blockquote class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">This is an automated email from the git hooks/post-receive script. It was<br>
generated because a ref change was pushed to the repository containing<br>
the project &quot;open-mpi/ompi&quot;.<br>
<br>
The branch, master has been updated<br>
       via  e2124c61fee7bd5a156c90d559ba15f6ded34d53 (commit)<br>
      from  6f2e8d20737907b474a401d041b5c0b1059e7d3f (commit)<br>
<br>
Those revisions listed above that are new to this repository have<br>
not appeared on any other notification email; so we list those<br>
revisions in full, below.<br>
<br>
- Log -----------------------------------------------------------------<br>
<a href="https://github.com/open-mpi/ompi/commit/e2124c61fee7bd5a156c90d559ba15f6ded34d53" rel="noreferrer" target="_blank">https://github.com/open-mpi/ompi/commit/e2124c61fee7bd5a156c90d559ba15f6ded34d53</a><br>
<br>
commit e2124c61fee7bd5a156c90d559ba15f6ded34d53<br>
Author: Jeff Squyres &lt;<a href="mailto:jsquyres@cisco.com">jsquyres@cisco.com</a>&gt;<br>
Date:   Tue Aug 25 09:53:25 2015 -0400<br>
<br>
    README: minor re-flowing on extra-long lines<br>
<br>
    No other content changes; just re-flowing of long lines.<br>
<br>
diff --git a/README b/README<br>
index 70f251d..6883d1f 100644<br>
--- a/README<br>
+++ b/README<br>
@@ -436,8 +436,8 @@ General Run-Time Support Notes<br>
 MPI Functionality and Features<br>
 ------------------------------<br>
<br>
-- Rank reordering support is available using the TreeMatch library. It is activated<br>
-  for the graph and dist_graph topologies.<br>
+- Rank reordering support is available using the TreeMatch library. It<br>
+  is activated for the graph and dist_graph topologies.<br>
<br>
 - All MPI-3 functionality is supported.<br>
<br>
@@ -532,37 +532,39 @@ MPI Collectives<br>
   MPI process onto Mellanox QDR InfiniBand switch CPUs and HCAs.<br>
<br>
 - The &quot;ML&quot; coll component is an implementation of MPI collective<br>
-  operations that takes advantage of communication hierarchies<br>
-  in modern systems. A ML collective operation is implemented by<br>
+  operations that takes advantage of communication hierarchies in<br>
+  modern systems. A ML collective operation is implemented by<br>
   combining multiple independently progressing collective primitives<br>
   implemented over different communication hierarchies, hence a ML<br>
-  collective operation is also referred to as a hierarchical collective<br>
-  operation. The number of collective primitives that are included in a<br>
-  ML collective operation is a function of subgroups(hierarchies).<br>
-  Typically, MPI processes in a single communication hierarchy such as<br>
-  CPU socket, node, or subnet are grouped together into a single subgroup<br>
-  (hierarchy). The number of subgroups are configurable at runtime,<br>
-  and each different collective operation could be configured to have<br>
-  a different of number of subgroups.<br>
+  collective operation is also referred to as a hierarchical<br>
+  collective operation. The number of collective primitives that are<br>
+  included in a ML collective operation is a function of<br>
+  subgroups(hierarchies).  Typically, MPI processes in a single<br>
+  communication hierarchy such as CPU socket, node, or subnet are<br>
+  grouped together into a single subgroup (hierarchy). The number of<br>
+  subgroups are configurable at runtime, and each different collective<br>
+  operation could be configured to have a different of number of<br>
+  subgroups.<br>
<br>
   The component frameworks and components used by/required for a<br>
   &quot;ML&quot; collective operation.<br>
<br>
   Frameworks:<br>
-  * &quot;sbgp&quot; - Provides functionality for grouping processes into subgroups<br>
+  * &quot;sbgp&quot; - Provides functionality for grouping processes into<br>
+             subgroups<br>
   * &quot;bcol&quot; - Provides collective primitives optimized for a particular<br>
              communication hierarchy<br>
<br>
   Components:<br>
-  * sbgp components     - Provides grouping functionality over a CPU socket<br>
-                          (&quot;basesocket&quot;), shared memory (&quot;basesmuma&quot;),<br>
-                          Mellanox&#39;s ConnectX HCA (&quot;ibnet&quot;), and other<br>
-                          interconnects supported by PML (&quot;p2p&quot;)<br>
-<br>
-  * BCOL components     - Provides optimized collective primitives for<br>
-                          shared memory (&quot;basesmuma&quot;), Mellanox&#39;s ConnectX<br>
-                          HCA (&quot;iboffload&quot;), and other interconnects supported<br>
-                          by PML (&quot;ptpcoll&quot;)<br>
+  * sbgp components - Provides grouping functionality over a CPU<br>
+                      socket (&quot;basesocket&quot;), shared memory<br>
+                      (&quot;basesmuma&quot;), Mellanox&#39;s ConnectX HCA<br>
+                      (&quot;ibnet&quot;), and other interconnects supported by<br>
+                      PML (&quot;p2p&quot;)<br>
+  * BCOL components - Provides optimized collective primitives for<br>
+                      shared memory (&quot;basesmuma&quot;), Mellanox&#39;s ConnectX<br>
+                      HCA (&quot;iboffload&quot;), and other interconnects<br>
+                      supported by PML (&quot;ptpcoll&quot;)<br>
<br>
 - The &quot;cuda&quot; coll component provides CUDA-aware support for the<br>
   reduction type collectives with GPU buffers. This component is only<br>
@@ -1002,10 +1004,11 @@ RUN-TIME SYSTEM SUPPORT<br>
   most cases.  This option is only needed for special configurations.<br>
<br>
 --with-pmi<br>
-  Build PMI support (by default on non-Cray XE/XC systems, it is not built).<br>
-  On Cray XE/XC systems, the location of pmi is detected automatically as<br>
-  part of the configure process.  For non-Cray systems, if the pmi2.h header<br>
-  is found in addition to pmi.h, then support for PMI2 will be built.<br>
+  Build PMI support (by default on non-Cray XE/XC systems, it is not<br>
+  built).  On Cray XE/XC systems, the location of pmi is detected<br>
+  automatically as part of the configure process.  For non-Cray<br>
+  systems, if the pmi2.h header is found in addition to pmi.h, then<br>
+  support for PMI2 will be built.<br>
<br>
 --with-slurm<br>
   Force the building of SLURM scheduler support.<br>
@@ -1635,9 +1638,9 @@ Open MPI API Extensions<br>
 -----------------------<br>
<br>
 Open MPI contains a framework for extending the MPI API that is<br>
-available to applications.  Each extension is usually a standalone set of<br>
-functionality that is distinct from other extensions (similar to how<br>
-Open MPI&#39;s plugins are usually unrelated to each other).  These<br>
+available to applications.  Each extension is usually a standalone set<br>
+of functionality that is distinct from other extensions (similar to<br>
+how Open MPI&#39;s plugins are usually unrelated to each other).  These<br>
 extensions provide new functions and/or constants that are available<br>
 to MPI applications.<br>
<br>
@@ -1955,9 +1958,9 @@ Here&#39;s how the three sub-groups are defined:<br>
     get their MPI/OSHMEM application to run correctly.<br>
  2. Application tuner: Generally, these are parameters that can be<br>
     used to tweak MPI application performance.<br>
- 3. MPI/OSHMEM developer: Parameters that either don&#39;t fit in the other two,<br>
-    or are specifically intended for debugging / development of Open<br>
-    MPI itself.<br>
+ 3. MPI/OSHMEM developer: Parameters that either don&#39;t fit in the<br>
+    other two, or are specifically intended for debugging /<br>
+    development of Open MPI itself.<br>
<br>
 Each sub-group is broken down into three classifications:<br>
<br>
<br>
<br>
-----------------------------------------------------------------------<br>
<br>
Summary of changes:<br>
 README | 67 ++++++++++++++++++++++++++++++++++--------------------------------<br>
 1 file changed, 35 insertions(+), 32 deletions(-)<br>
<br>
<br>
hooks/post-receive<br>
--<br>
open-mpi/ompi<br>
_______________________________________________<br>
ompi-commits mailing list<br>
<a href="mailto:ompi-commits@open-mpi.org">ompi-commits@open-mpi.org</a><br>
<a href="http://www.open-mpi.org/mailman/listinfo.cgi/ompi-commits" rel="noreferrer" target="_blank">http://www.open-mpi.org/mailman/listinfo.cgi/ompi-commits</a><br>
</blockquote></div>

